Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 772.8525 - loglik: -7.7132e+02 - logprior: -1.5285e+00
Epoch 2/10
39/39 - 31s - loss: 646.0856 - loglik: -6.4414e+02 - logprior: -1.9426e+00
Epoch 3/10
39/39 - 32s - loss: 635.7000 - loglik: -6.3382e+02 - logprior: -1.8839e+00
Epoch 4/10
39/39 - 32s - loss: 632.8640 - loglik: -6.3106e+02 - logprior: -1.8002e+00
Epoch 5/10
39/39 - 33s - loss: 631.3694 - loglik: -6.2957e+02 - logprior: -1.7972e+00
Epoch 6/10
39/39 - 34s - loss: 631.0458 - loglik: -6.2923e+02 - logprior: -1.8137e+00
Epoch 7/10
39/39 - 35s - loss: 630.7538 - loglik: -6.2892e+02 - logprior: -1.8317e+00
Epoch 8/10
39/39 - 35s - loss: 630.9350 - loglik: -6.2910e+02 - logprior: -1.8379e+00
Fitted a model with MAP estimate = -630.1991
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (106, 1), (112, 1), (117, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (173, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (213, 1), (214, 1), (217, 1), (220, 1), (229, 1), (233, 1), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (265, 3), (266, 2), (267, 1), (268, 2), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 620.8578 - loglik: -6.1932e+02 - logprior: -1.5417e+00
Epoch 2/2
39/39 - 59s - loss: 608.6628 - loglik: -6.0823e+02 - logprior: -4.3143e-01
Fitted a model with MAP estimate = -605.6487
expansions: [(78, 1)]
discards: [249 335 336 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 611.9616 - loglik: -6.1080e+02 - logprior: -1.1609e+00
Epoch 2/2
39/39 - 64s - loss: 607.9722 - loglik: -6.0795e+02 - logprior: -1.7927e-02
Fitted a model with MAP estimate = -605.7423
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 611.0125 - loglik: -6.1001e+02 - logprior: -1.0023e+00
Epoch 2/10
39/39 - 63s - loss: 607.4385 - loglik: -6.0762e+02 - logprior: 0.1768
Epoch 3/10
39/39 - 62s - loss: 605.2486 - loglik: -6.0553e+02 - logprior: 0.2811
Epoch 4/10
39/39 - 61s - loss: 602.7345 - loglik: -6.0316e+02 - logprior: 0.4301
Epoch 5/10
39/39 - 61s - loss: 601.3262 - loglik: -6.0183e+02 - logprior: 0.5005
Epoch 6/10
39/39 - 59s - loss: 600.5681 - loglik: -6.0122e+02 - logprior: 0.6489
Epoch 7/10
39/39 - 58s - loss: 600.6898 - loglik: -6.0146e+02 - logprior: 0.7671
Fitted a model with MAP estimate = -599.9621
Time for alignment: 1179.3732
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 769.6734 - loglik: -7.6815e+02 - logprior: -1.5264e+00
Epoch 2/10
39/39 - 39s - loss: 644.6063 - loglik: -6.4267e+02 - logprior: -1.9409e+00
Epoch 3/10
39/39 - 40s - loss: 634.9096 - loglik: -6.3303e+02 - logprior: -1.8793e+00
Epoch 4/10
39/39 - 42s - loss: 632.5417 - loglik: -6.3073e+02 - logprior: -1.8107e+00
Epoch 5/10
39/39 - 44s - loss: 630.2354 - loglik: -6.2841e+02 - logprior: -1.8257e+00
Epoch 6/10
39/39 - 45s - loss: 630.0894 - loglik: -6.2824e+02 - logprior: -1.8520e+00
Epoch 7/10
39/39 - 46s - loss: 630.2305 - loglik: -6.2836e+02 - logprior: -1.8748e+00
Fitted a model with MAP estimate = -629.4238
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (99, 1), (100, 1), (101, 1), (103, 1), (106, 1), (118, 1), (121, 1), (134, 1), (142, 1), (144, 1), (145, 1), (150, 1), (159, 1), (160, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (175, 1), (186, 1), (189, 1), (193, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 1), (202, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 2), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 620.8430 - loglik: -6.1930e+02 - logprior: -1.5472e+00
Epoch 2/2
39/39 - 72s - loss: 609.0854 - loglik: -6.0864e+02 - logprior: -4.4079e-01
Fitted a model with MAP estimate = -606.3089
expansions: [(16, 1)]
discards: [248 294 335 336]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 612.2892 - loglik: -6.1113e+02 - logprior: -1.1587e+00
Epoch 2/2
39/39 - 74s - loss: 608.3470 - loglik: -6.0829e+02 - logprior: -5.8394e-02
Fitted a model with MAP estimate = -606.1694
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 77s - loss: 611.7246 - loglik: -6.1072e+02 - logprior: -1.0028e+00
Epoch 2/10
39/39 - 72s - loss: 607.8578 - loglik: -6.0804e+02 - logprior: 0.1803
Epoch 3/10
39/39 - 68s - loss: 605.8527 - loglik: -6.0612e+02 - logprior: 0.2684
Epoch 4/10
39/39 - 57s - loss: 603.0943 - loglik: -6.0351e+02 - logprior: 0.4126
Epoch 5/10
39/39 - 54s - loss: 601.4857 - loglik: -6.0196e+02 - logprior: 0.4768
Epoch 6/10
39/39 - 54s - loss: 601.1606 - loglik: -6.0174e+02 - logprior: 0.5751
Epoch 7/10
39/39 - 55s - loss: 601.1286 - loglik: -6.0186e+02 - logprior: 0.7334
Epoch 8/10
39/39 - 60s - loss: 600.9174 - loglik: -6.0175e+02 - logprior: 0.8310
Epoch 9/10
39/39 - 64s - loss: 600.0981 - loglik: -6.0106e+02 - logprior: 0.9665
Epoch 10/10
39/39 - 68s - loss: 600.7582 - loglik: -6.0178e+02 - logprior: 1.0194
Fitted a model with MAP estimate = -599.8058
Time for alignment: 1504.2428
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 770.0569 - loglik: -7.6852e+02 - logprior: -1.5393e+00
Epoch 2/10
39/39 - 48s - loss: 645.0391 - loglik: -6.4309e+02 - logprior: -1.9522e+00
Epoch 3/10
39/39 - 50s - loss: 635.7297 - loglik: -6.3388e+02 - logprior: -1.8475e+00
Epoch 4/10
39/39 - 50s - loss: 632.9497 - loglik: -6.3117e+02 - logprior: -1.7810e+00
Epoch 5/10
39/39 - 49s - loss: 631.7010 - loglik: -6.2991e+02 - logprior: -1.7920e+00
Epoch 6/10
39/39 - 49s - loss: 631.6559 - loglik: -6.2986e+02 - logprior: -1.7970e+00
Epoch 7/10
39/39 - 48s - loss: 631.3096 - loglik: -6.2950e+02 - logprior: -1.8134e+00
Epoch 8/10
39/39 - 49s - loss: 631.2150 - loglik: -6.2939e+02 - logprior: -1.8216e+00
Epoch 9/10
39/39 - 51s - loss: 630.3110 - loglik: -6.2849e+02 - logprior: -1.8165e+00
Epoch 10/10
39/39 - 49s - loss: 631.9622 - loglik: -6.3015e+02 - logprior: -1.8168e+00
Fitted a model with MAP estimate = -630.7005
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (84, 1), (85, 2), (86, 1), (91, 1), (92, 1), (95, 1), (98, 1), (100, 1), (103, 1), (106, 1), (118, 1), (120, 1), (122, 1), (135, 1), (141, 1), (143, 1), (144, 1), (159, 1), (160, 1), (161, 1), (163, 2), (164, 3), (165, 1), (173, 1), (184, 1), (188, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 1), (199, 1), (214, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 2), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 621.2504 - loglik: -6.1969e+02 - logprior: -1.5638e+00
Epoch 2/2
39/39 - 55s - loss: 609.3531 - loglik: -6.0889e+02 - logprior: -4.6064e-01
Fitted a model with MAP estimate = -606.4994
expansions: []
discards: [ 56 294]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 611.9778 - loglik: -6.1081e+02 - logprior: -1.1718e+00
Epoch 2/2
39/39 - 55s - loss: 608.6594 - loglik: -6.0863e+02 - logprior: -2.7501e-02
Fitted a model with MAP estimate = -605.9766
expansions: [(56, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 611.1162 - loglik: -6.1019e+02 - logprior: -9.3108e-01
Epoch 2/10
39/39 - 54s - loss: 607.5372 - loglik: -6.0775e+02 - logprior: 0.2098
Epoch 3/10
39/39 - 54s - loss: 605.0609 - loglik: -6.0540e+02 - logprior: 0.3360
Epoch 4/10
39/39 - 55s - loss: 603.5623 - loglik: -6.0401e+02 - logprior: 0.4433
Epoch 5/10
39/39 - 53s - loss: 601.2924 - loglik: -6.0186e+02 - logprior: 0.5671
Epoch 6/10
39/39 - 53s - loss: 600.7038 - loglik: -6.0138e+02 - logprior: 0.6791
Epoch 7/10
39/39 - 53s - loss: 600.2751 - loglik: -6.0105e+02 - logprior: 0.7710
Epoch 8/10
39/39 - 54s - loss: 600.2955 - loglik: -6.0117e+02 - logprior: 0.8705
Fitted a model with MAP estimate = -599.7647
Time for alignment: 1380.6968
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 770.4273 - loglik: -7.6890e+02 - logprior: -1.5285e+00
Epoch 2/10
39/39 - 37s - loss: 644.3006 - loglik: -6.4236e+02 - logprior: -1.9444e+00
Epoch 3/10
39/39 - 37s - loss: 635.1043 - loglik: -6.3323e+02 - logprior: -1.8696e+00
Epoch 4/10
39/39 - 38s - loss: 632.3671 - loglik: -6.3056e+02 - logprior: -1.8080e+00
Epoch 5/10
39/39 - 39s - loss: 630.6801 - loglik: -6.2886e+02 - logprior: -1.8206e+00
Epoch 6/10
39/39 - 41s - loss: 630.9536 - loglik: -6.2910e+02 - logprior: -1.8569e+00
Fitted a model with MAP estimate = -629.8324
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (65, 1), (67, 1), (68, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (99, 1), (100, 1), (101, 1), (107, 1), (119, 1), (121, 1), (123, 1), (136, 1), (140, 1), (141, 1), (143, 1), (144, 1), (159, 1), (160, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (224, 1), (229, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 620.4365 - loglik: -6.1885e+02 - logprior: -1.5897e+00
Epoch 2/2
39/39 - 65s - loss: 608.8535 - loglik: -6.0845e+02 - logprior: -4.0331e-01
Fitted a model with MAP estimate = -606.2926
expansions: [(78, 1)]
discards: [248 334 335]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 612.4720 - loglik: -6.1124e+02 - logprior: -1.2304e+00
Epoch 2/2
39/39 - 67s - loss: 608.2970 - loglik: -6.0819e+02 - logprior: -1.0807e-01
Fitted a model with MAP estimate = -606.2234
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 611.7193 - loglik: -6.1071e+02 - logprior: -1.0129e+00
Epoch 2/10
39/39 - 59s - loss: 608.0962 - loglik: -6.0823e+02 - logprior: 0.1354
Epoch 3/10
39/39 - 58s - loss: 605.4661 - loglik: -6.0573e+02 - logprior: 0.2645
Epoch 4/10
39/39 - 56s - loss: 603.5833 - loglik: -6.0398e+02 - logprior: 0.3931
Epoch 5/10
39/39 - 57s - loss: 601.6865 - loglik: -6.0217e+02 - logprior: 0.4870
Epoch 6/10
39/39 - 56s - loss: 601.4685 - loglik: -6.0206e+02 - logprior: 0.5933
Epoch 7/10
39/39 - 55s - loss: 600.7608 - loglik: -6.0145e+02 - logprior: 0.6844
Epoch 8/10
39/39 - 55s - loss: 601.5479 - loglik: -6.0231e+02 - logprior: 0.7608
Fitted a model with MAP estimate = -600.3555
Time for alignment: 1220.5621
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 772.1420 - loglik: -7.7058e+02 - logprior: -1.5610e+00
Epoch 2/10
39/39 - 38s - loss: 645.0153 - loglik: -6.4303e+02 - logprior: -1.9885e+00
Epoch 3/10
39/39 - 38s - loss: 635.1168 - loglik: -6.3321e+02 - logprior: -1.9086e+00
Epoch 4/10
39/39 - 39s - loss: 632.4527 - loglik: -6.3063e+02 - logprior: -1.8247e+00
Epoch 5/10
39/39 - 38s - loss: 631.1484 - loglik: -6.2932e+02 - logprior: -1.8248e+00
Epoch 6/10
39/39 - 39s - loss: 630.6400 - loglik: -6.2879e+02 - logprior: -1.8512e+00
Epoch 7/10
39/39 - 39s - loss: 630.2117 - loglik: -6.2832e+02 - logprior: -1.8878e+00
Epoch 8/10
39/39 - 39s - loss: 630.1927 - loglik: -6.2829e+02 - logprior: -1.9009e+00
Epoch 9/10
39/39 - 39s - loss: 630.1518 - loglik: -6.2825e+02 - logprior: -1.8978e+00
Epoch 10/10
39/39 - 41s - loss: 630.5052 - loglik: -6.2860e+02 - logprior: -1.9039e+00
Fitted a model with MAP estimate = -629.7126
expansions: [(13, 1), (14, 2), (15, 1), (34, 1), (46, 1), (51, 3), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (106, 1), (112, 1), (117, 1), (120, 1), (135, 1), (139, 1), (140, 1), (143, 1), (146, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (174, 1), (184, 1), (188, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (203, 1), (213, 1), (217, 1), (219, 1), (229, 1), (234, 2), (235, 2), (240, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 620.5292 - loglik: -6.1905e+02 - logprior: -1.4791e+00
Epoch 2/2
39/39 - 65s - loss: 608.2227 - loglik: -6.0791e+02 - logprior: -3.1518e-01
Fitted a model with MAP estimate = -605.7337
expansions: []
discards: [250]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 611.4189 - loglik: -6.1024e+02 - logprior: -1.1796e+00
Epoch 2/2
39/39 - 65s - loss: 607.4747 - loglik: -6.0748e+02 - logprior: 0.0017
Fitted a model with MAP estimate = -605.3675
expansions: [(143, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 610.3935 - loglik: -6.0948e+02 - logprior: -9.1390e-01
Epoch 2/10
39/39 - 75s - loss: 606.8055 - loglik: -6.0708e+02 - logprior: 0.2761
Epoch 3/10
39/39 - 75s - loss: 604.7881 - loglik: -6.0515e+02 - logprior: 0.3622
Epoch 4/10
39/39 - 72s - loss: 601.8215 - loglik: -6.0232e+02 - logprior: 0.5025
Epoch 5/10
39/39 - 74s - loss: 600.5972 - loglik: -6.0123e+02 - logprior: 0.6314
Epoch 6/10
39/39 - 76s - loss: 600.0504 - loglik: -6.0077e+02 - logprior: 0.7162
Epoch 7/10
39/39 - 72s - loss: 599.6125 - loglik: -6.0043e+02 - logprior: 0.8178
Epoch 8/10
39/39 - 75s - loss: 599.9116 - loglik: -6.0085e+02 - logprior: 0.9402
Fitted a model with MAP estimate = -599.0611
Time for alignment: 1504.5078
Computed alignments with likelihoods: ['-599.9621', '-599.8058', '-599.7647', '-600.3555', '-599.0611']
Best model has likelihood: -599.0611  (prior= 0.9787 )
time for generating output: 0.3099
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9291938997821351
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9812 - loglik: -1.4678e+02 - logprior: -3.2046e+00
Epoch 2/10
19/19 - 1s - loss: 128.7822 - loglik: -1.2745e+02 - logprior: -1.3281e+00
Epoch 3/10
19/19 - 1s - loss: 119.6156 - loglik: -1.1810e+02 - logprior: -1.5196e+00
Epoch 4/10
19/19 - 1s - loss: 117.9935 - loglik: -1.1657e+02 - logprior: -1.4260e+00
Epoch 5/10
19/19 - 1s - loss: 117.5943 - loglik: -1.1621e+02 - logprior: -1.3876e+00
Epoch 6/10
19/19 - 1s - loss: 117.2986 - loglik: -1.1592e+02 - logprior: -1.3789e+00
Epoch 7/10
19/19 - 1s - loss: 117.1585 - loglik: -1.1580e+02 - logprior: -1.3582e+00
Epoch 8/10
19/19 - 1s - loss: 117.1010 - loglik: -1.1575e+02 - logprior: -1.3528e+00
Epoch 9/10
19/19 - 1s - loss: 117.1935 - loglik: -1.1585e+02 - logprior: -1.3477e+00
Fitted a model with MAP estimate = -116.9350
expansions: [(6, 3), (15, 1), (19, 2), (23, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 121.1034 - loglik: -1.1703e+02 - logprior: -4.0749e+00
Epoch 2/2
19/19 - 1s - loss: 113.6614 - loglik: -1.1167e+02 - logprior: -1.9950e+00
Fitted a model with MAP estimate = -112.2911
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 113.5556 - loglik: -1.1055e+02 - logprior: -3.0084e+00
Epoch 2/2
19/19 - 1s - loss: 110.6550 - loglik: -1.0945e+02 - logprior: -1.2003e+00
Fitted a model with MAP estimate = -110.0265
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.2371 - loglik: -1.1148e+02 - logprior: -3.7615e+00
Epoch 2/10
19/19 - 1s - loss: 110.9505 - loglik: -1.0962e+02 - logprior: -1.3273e+00
Epoch 3/10
19/19 - 1s - loss: 110.2263 - loglik: -1.0907e+02 - logprior: -1.1514e+00
Epoch 4/10
19/19 - 1s - loss: 109.7995 - loglik: -1.0869e+02 - logprior: -1.1090e+00
Epoch 5/10
19/19 - 1s - loss: 109.5211 - loglik: -1.0843e+02 - logprior: -1.0950e+00
Epoch 6/10
19/19 - 1s - loss: 109.2804 - loglik: -1.0819e+02 - logprior: -1.0911e+00
Epoch 7/10
19/19 - 1s - loss: 109.1954 - loglik: -1.0813e+02 - logprior: -1.0699e+00
Epoch 8/10
19/19 - 1s - loss: 109.0542 - loglik: -1.0799e+02 - logprior: -1.0594e+00
Epoch 9/10
19/19 - 1s - loss: 109.0466 - loglik: -1.0800e+02 - logprior: -1.0500e+00
Epoch 10/10
19/19 - 1s - loss: 109.0386 - loglik: -1.0801e+02 - logprior: -1.0309e+00
Fitted a model with MAP estimate = -108.9514
Time for alignment: 40.9413
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0732 - loglik: -1.4687e+02 - logprior: -3.2070e+00
Epoch 2/10
19/19 - 1s - loss: 129.0687 - loglik: -1.2769e+02 - logprior: -1.3742e+00
Epoch 3/10
19/19 - 1s - loss: 119.6278 - loglik: -1.1805e+02 - logprior: -1.5801e+00
Epoch 4/10
19/19 - 1s - loss: 117.5127 - loglik: -1.1604e+02 - logprior: -1.4743e+00
Epoch 5/10
19/19 - 1s - loss: 117.1953 - loglik: -1.1575e+02 - logprior: -1.4441e+00
Epoch 6/10
19/19 - 1s - loss: 116.8946 - loglik: -1.1547e+02 - logprior: -1.4255e+00
Epoch 7/10
19/19 - 1s - loss: 116.7400 - loglik: -1.1533e+02 - logprior: -1.4126e+00
Epoch 8/10
19/19 - 1s - loss: 116.6651 - loglik: -1.1526e+02 - logprior: -1.4060e+00
Epoch 9/10
19/19 - 1s - loss: 116.6946 - loglik: -1.1529e+02 - logprior: -1.4030e+00
Fitted a model with MAP estimate = -116.4709
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.3941 - loglik: -1.1730e+02 - logprior: -4.0903e+00
Epoch 2/2
19/19 - 1s - loss: 113.6754 - loglik: -1.1170e+02 - logprior: -1.9775e+00
Fitted a model with MAP estimate = -112.2513
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 113.6489 - loglik: -1.1065e+02 - logprior: -3.0029e+00
Epoch 2/2
19/19 - 1s - loss: 110.6182 - loglik: -1.0941e+02 - logprior: -1.2047e+00
Fitted a model with MAP estimate = -110.0384
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.0583 - loglik: -1.1134e+02 - logprior: -3.7191e+00
Epoch 2/10
19/19 - 1s - loss: 110.9447 - loglik: -1.0962e+02 - logprior: -1.3222e+00
Epoch 3/10
19/19 - 1s - loss: 110.2562 - loglik: -1.0911e+02 - logprior: -1.1508e+00
Epoch 4/10
19/19 - 1s - loss: 109.7755 - loglik: -1.0866e+02 - logprior: -1.1105e+00
Epoch 5/10
19/19 - 1s - loss: 109.5374 - loglik: -1.0844e+02 - logprior: -1.0992e+00
Epoch 6/10
19/19 - 1s - loss: 109.2053 - loglik: -1.0812e+02 - logprior: -1.0847e+00
Epoch 7/10
19/19 - 1s - loss: 109.0957 - loglik: -1.0802e+02 - logprior: -1.0714e+00
Epoch 8/10
19/19 - 1s - loss: 109.2327 - loglik: -1.0817e+02 - logprior: -1.0625e+00
Fitted a model with MAP estimate = -109.0046
Time for alignment: 38.9453
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0441 - loglik: -1.4684e+02 - logprior: -3.2004e+00
Epoch 2/10
19/19 - 1s - loss: 129.0568 - loglik: -1.2777e+02 - logprior: -1.2837e+00
Epoch 3/10
19/19 - 1s - loss: 120.7287 - loglik: -1.1929e+02 - logprior: -1.4340e+00
Epoch 4/10
19/19 - 1s - loss: 119.1205 - loglik: -1.1773e+02 - logprior: -1.3947e+00
Epoch 5/10
19/19 - 1s - loss: 118.6676 - loglik: -1.1730e+02 - logprior: -1.3684e+00
Epoch 6/10
19/19 - 1s - loss: 118.2158 - loglik: -1.1685e+02 - logprior: -1.3648e+00
Epoch 7/10
19/19 - 1s - loss: 117.9970 - loglik: -1.1664e+02 - logprior: -1.3561e+00
Epoch 8/10
19/19 - 1s - loss: 117.9949 - loglik: -1.1665e+02 - logprior: -1.3435e+00
Epoch 9/10
19/19 - 1s - loss: 117.9065 - loglik: -1.1657e+02 - logprior: -1.3413e+00
Epoch 10/10
19/19 - 1s - loss: 117.8567 - loglik: -1.1652e+02 - logprior: -1.3350e+00
Fitted a model with MAP estimate = -117.6672
expansions: [(6, 3), (7, 1), (19, 2), (23, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.4630 - loglik: -1.1738e+02 - logprior: -4.0795e+00
Epoch 2/2
19/19 - 1s - loss: 113.6902 - loglik: -1.1171e+02 - logprior: -1.9823e+00
Fitted a model with MAP estimate = -112.2947
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6199 - loglik: -1.1061e+02 - logprior: -3.0053e+00
Epoch 2/2
19/19 - 1s - loss: 110.6143 - loglik: -1.0941e+02 - logprior: -1.2020e+00
Fitted a model with MAP estimate = -110.0423
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.1507 - loglik: -1.1143e+02 - logprior: -3.7245e+00
Epoch 2/10
19/19 - 1s - loss: 110.9767 - loglik: -1.0966e+02 - logprior: -1.3169e+00
Epoch 3/10
19/19 - 1s - loss: 110.1753 - loglik: -1.0902e+02 - logprior: -1.1523e+00
Epoch 4/10
19/19 - 1s - loss: 109.7531 - loglik: -1.0864e+02 - logprior: -1.1084e+00
Epoch 5/10
19/19 - 1s - loss: 109.5408 - loglik: -1.0844e+02 - logprior: -1.0981e+00
Epoch 6/10
19/19 - 1s - loss: 109.2277 - loglik: -1.0814e+02 - logprior: -1.0834e+00
Epoch 7/10
19/19 - 1s - loss: 109.2029 - loglik: -1.0813e+02 - logprior: -1.0693e+00
Epoch 8/10
19/19 - 1s - loss: 109.0423 - loglik: -1.0798e+02 - logprior: -1.0622e+00
Epoch 9/10
19/19 - 1s - loss: 109.1113 - loglik: -1.0807e+02 - logprior: -1.0435e+00
Fitted a model with MAP estimate = -108.9720
Time for alignment: 40.7162
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.8684 - loglik: -1.4666e+02 - logprior: -3.2084e+00
Epoch 2/10
19/19 - 1s - loss: 128.4583 - loglik: -1.2709e+02 - logprior: -1.3677e+00
Epoch 3/10
19/19 - 1s - loss: 119.3259 - loglik: -1.1775e+02 - logprior: -1.5740e+00
Epoch 4/10
19/19 - 1s - loss: 117.5201 - loglik: -1.1604e+02 - logprior: -1.4799e+00
Epoch 5/10
19/19 - 1s - loss: 116.9734 - loglik: -1.1552e+02 - logprior: -1.4583e+00
Epoch 6/10
19/19 - 1s - loss: 116.8593 - loglik: -1.1542e+02 - logprior: -1.4372e+00
Epoch 7/10
19/19 - 1s - loss: 116.7153 - loglik: -1.1530e+02 - logprior: -1.4190e+00
Epoch 8/10
19/19 - 1s - loss: 116.6464 - loglik: -1.1523e+02 - logprior: -1.4120e+00
Epoch 9/10
19/19 - 1s - loss: 116.5460 - loglik: -1.1514e+02 - logprior: -1.4065e+00
Epoch 10/10
19/19 - 1s - loss: 116.6005 - loglik: -1.1520e+02 - logprior: -1.4016e+00
Fitted a model with MAP estimate = -116.3892
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 121.5192 - loglik: -1.1743e+02 - logprior: -4.0939e+00
Epoch 2/2
19/19 - 1s - loss: 113.6376 - loglik: -1.1165e+02 - logprior: -1.9909e+00
Fitted a model with MAP estimate = -112.2735
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6484 - loglik: -1.1065e+02 - logprior: -3.0013e+00
Epoch 2/2
19/19 - 1s - loss: 110.6267 - loglik: -1.0943e+02 - logprior: -1.2011e+00
Fitted a model with MAP estimate = -110.0400
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 115.1566 - loglik: -1.1142e+02 - logprior: -3.7367e+00
Epoch 2/10
19/19 - 1s - loss: 110.9095 - loglik: -1.0959e+02 - logprior: -1.3201e+00
Epoch 3/10
19/19 - 1s - loss: 110.2425 - loglik: -1.0909e+02 - logprior: -1.1483e+00
Epoch 4/10
19/19 - 1s - loss: 109.7313 - loglik: -1.0862e+02 - logprior: -1.1099e+00
Epoch 5/10
19/19 - 1s - loss: 109.5462 - loglik: -1.0845e+02 - logprior: -1.0942e+00
Epoch 6/10
19/19 - 1s - loss: 109.3108 - loglik: -1.0823e+02 - logprior: -1.0833e+00
Epoch 7/10
19/19 - 1s - loss: 109.1637 - loglik: -1.0809e+02 - logprior: -1.0730e+00
Epoch 8/10
19/19 - 1s - loss: 109.0817 - loglik: -1.0803e+02 - logprior: -1.0557e+00
Epoch 9/10
19/19 - 1s - loss: 108.9721 - loglik: -1.0792e+02 - logprior: -1.0474e+00
Epoch 10/10
19/19 - 1s - loss: 108.9767 - loglik: -1.0795e+02 - logprior: -1.0314e+00
Fitted a model with MAP estimate = -108.9497
Time for alignment: 41.7353
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9580 - loglik: -1.4675e+02 - logprior: -3.2066e+00
Epoch 2/10
19/19 - 1s - loss: 129.0804 - loglik: -1.2772e+02 - logprior: -1.3642e+00
Epoch 3/10
19/19 - 1s - loss: 120.8875 - loglik: -1.1933e+02 - logprior: -1.5579e+00
Epoch 4/10
19/19 - 1s - loss: 118.6880 - loglik: -1.1724e+02 - logprior: -1.4472e+00
Epoch 5/10
19/19 - 1s - loss: 118.1574 - loglik: -1.1675e+02 - logprior: -1.4122e+00
Epoch 6/10
19/19 - 1s - loss: 117.7501 - loglik: -1.1636e+02 - logprior: -1.3936e+00
Epoch 7/10
19/19 - 1s - loss: 117.8546 - loglik: -1.1648e+02 - logprior: -1.3792e+00
Fitted a model with MAP estimate = -117.5102
expansions: [(6, 1), (7, 1), (8, 1), (14, 1), (15, 1), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.8368 - loglik: -1.1774e+02 - logprior: -4.0966e+00
Epoch 2/2
19/19 - 1s - loss: 113.6460 - loglik: -1.1168e+02 - logprior: -1.9655e+00
Fitted a model with MAP estimate = -112.1608
expansions: [(0, 2)]
discards: [ 0 33 35 39 45]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 113.8101 - loglik: -1.1081e+02 - logprior: -3.0030e+00
Epoch 2/2
19/19 - 1s - loss: 110.6122 - loglik: -1.0941e+02 - logprior: -1.2019e+00
Fitted a model with MAP estimate = -110.0311
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 115.0430 - loglik: -1.1132e+02 - logprior: -3.7244e+00
Epoch 2/10
19/19 - 1s - loss: 110.9142 - loglik: -1.0959e+02 - logprior: -1.3209e+00
Epoch 3/10
19/19 - 1s - loss: 110.2840 - loglik: -1.0913e+02 - logprior: -1.1504e+00
Epoch 4/10
19/19 - 1s - loss: 109.7774 - loglik: -1.0866e+02 - logprior: -1.1181e+00
Epoch 5/10
19/19 - 1s - loss: 109.4391 - loglik: -1.0834e+02 - logprior: -1.1032e+00
Epoch 6/10
19/19 - 1s - loss: 109.3607 - loglik: -1.0828e+02 - logprior: -1.0834e+00
Epoch 7/10
19/19 - 1s - loss: 109.0431 - loglik: -1.0796e+02 - logprior: -1.0788e+00
Epoch 8/10
19/19 - 1s - loss: 109.2265 - loglik: -1.0817e+02 - logprior: -1.0594e+00
Fitted a model with MAP estimate = -109.0132
Time for alignment: 36.8024
Computed alignments with likelihoods: ['-108.9514', '-109.0046', '-108.9720', '-108.9497', '-109.0132']
Best model has likelihood: -108.9497  (prior= -1.0233 )
time for generating output: 0.0970
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6338412097213854
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.1352 - loglik: -3.5714e+02 - logprior: -2.9921e+00
Epoch 2/10
19/19 - 3s - loss: 302.4807 - loglik: -3.0135e+02 - logprior: -1.1260e+00
Epoch 3/10
19/19 - 3s - loss: 277.1888 - loglik: -2.7598e+02 - logprior: -1.2137e+00
Epoch 4/10
19/19 - 3s - loss: 273.4503 - loglik: -2.7228e+02 - logprior: -1.1733e+00
Epoch 5/10
19/19 - 3s - loss: 269.8579 - loglik: -2.6873e+02 - logprior: -1.1252e+00
Epoch 6/10
19/19 - 3s - loss: 269.0958 - loglik: -2.6800e+02 - logprior: -1.0940e+00
Epoch 7/10
19/19 - 3s - loss: 268.6847 - loglik: -2.6763e+02 - logprior: -1.0513e+00
Epoch 8/10
19/19 - 3s - loss: 268.8508 - loglik: -2.6780e+02 - logprior: -1.0521e+00
Fitted a model with MAP estimate = -268.3082
expansions: [(12, 2), (14, 4), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.8177 - loglik: -2.6599e+02 - logprior: -3.8258e+00
Epoch 2/2
19/19 - 4s - loss: 259.9784 - loglik: -2.5818e+02 - logprior: -1.7975e+00
Fitted a model with MAP estimate = -258.3869
expansions: [(0, 3)]
discards: [ 0 12 17 38 76]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.2934 - loglik: -2.5855e+02 - logprior: -2.7407e+00
Epoch 2/2
19/19 - 5s - loss: 257.7815 - loglik: -2.5686e+02 - logprior: -9.1778e-01
Fitted a model with MAP estimate = -257.0897
expansions: []
discards: [  0   1   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 263.7733 - loglik: -2.6012e+02 - logprior: -3.6485e+00
Epoch 2/10
19/19 - 4s - loss: 260.5547 - loglik: -2.5904e+02 - logprior: -1.5101e+00
Epoch 3/10
19/19 - 4s - loss: 258.6801 - loglik: -2.5797e+02 - logprior: -7.1163e-01
Epoch 4/10
19/19 - 5s - loss: 257.4738 - loglik: -2.5708e+02 - logprior: -3.9367e-01
Epoch 5/10
19/19 - 5s - loss: 256.5342 - loglik: -2.5624e+02 - logprior: -2.9646e-01
Epoch 6/10
19/19 - 4s - loss: 256.3946 - loglik: -2.5617e+02 - logprior: -2.2707e-01
Epoch 7/10
19/19 - 5s - loss: 255.8030 - loglik: -2.5563e+02 - logprior: -1.7474e-01
Epoch 8/10
19/19 - 4s - loss: 255.6947 - loglik: -2.5556e+02 - logprior: -1.3916e-01
Epoch 9/10
19/19 - 5s - loss: 256.0214 - loglik: -2.5592e+02 - logprior: -9.7973e-02
Fitted a model with MAP estimate = -255.6304
Time for alignment: 120.3607
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.5957 - loglik: -3.5760e+02 - logprior: -2.9915e+00
Epoch 2/10
19/19 - 3s - loss: 305.4348 - loglik: -3.0431e+02 - logprior: -1.1253e+00
Epoch 3/10
19/19 - 3s - loss: 279.8703 - loglik: -2.7866e+02 - logprior: -1.2064e+00
Epoch 4/10
19/19 - 3s - loss: 272.1096 - loglik: -2.7088e+02 - logprior: -1.2331e+00
Epoch 5/10
19/19 - 3s - loss: 270.3556 - loglik: -2.6918e+02 - logprior: -1.1804e+00
Epoch 6/10
19/19 - 3s - loss: 269.2926 - loglik: -2.6815e+02 - logprior: -1.1411e+00
Epoch 7/10
19/19 - 3s - loss: 268.4860 - loglik: -2.6737e+02 - logprior: -1.1153e+00
Epoch 8/10
19/19 - 3s - loss: 268.6139 - loglik: -2.6751e+02 - logprior: -1.1011e+00
Fitted a model with MAP estimate = -268.2702
expansions: [(12, 2), (14, 3), (16, 1), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 270.1080 - loglik: -2.6628e+02 - logprior: -3.8249e+00
Epoch 2/2
19/19 - 5s - loss: 259.8014 - loglik: -2.5800e+02 - logprior: -1.7995e+00
Fitted a model with MAP estimate = -257.9871
expansions: [(0, 3)]
discards: [ 0 12 17 38 76]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.5694 - loglik: -2.5882e+02 - logprior: -2.7468e+00
Epoch 2/2
19/19 - 4s - loss: 257.9706 - loglik: -2.5704e+02 - logprior: -9.3391e-01
Fitted a model with MAP estimate = -257.1458
expansions: []
discards: [  0   1   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 263.9253 - loglik: -2.6028e+02 - logprior: -3.6438e+00
Epoch 2/10
19/19 - 5s - loss: 260.3433 - loglik: -2.5882e+02 - logprior: -1.5203e+00
Epoch 3/10
19/19 - 5s - loss: 258.6947 - loglik: -2.5800e+02 - logprior: -6.9840e-01
Epoch 4/10
19/19 - 5s - loss: 257.3481 - loglik: -2.5693e+02 - logprior: -4.1838e-01
Epoch 5/10
19/19 - 5s - loss: 256.9447 - loglik: -2.5662e+02 - logprior: -3.2255e-01
Epoch 6/10
19/19 - 5s - loss: 256.1625 - loglik: -2.5590e+02 - logprior: -2.5815e-01
Epoch 7/10
19/19 - 5s - loss: 256.1131 - loglik: -2.5591e+02 - logprior: -2.0134e-01
Epoch 8/10
19/19 - 5s - loss: 255.3622 - loglik: -2.5520e+02 - logprior: -1.5739e-01
Epoch 9/10
19/19 - 4s - loss: 255.9959 - loglik: -2.5588e+02 - logprior: -1.1453e-01
Fitted a model with MAP estimate = -255.6572
Time for alignment: 123.7718
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.1760 - loglik: -3.5719e+02 - logprior: -2.9905e+00
Epoch 2/10
19/19 - 4s - loss: 305.3761 - loglik: -3.0424e+02 - logprior: -1.1360e+00
Epoch 3/10
19/19 - 4s - loss: 279.1156 - loglik: -2.7787e+02 - logprior: -1.2487e+00
Epoch 4/10
19/19 - 4s - loss: 273.1939 - loglik: -2.7197e+02 - logprior: -1.2216e+00
Epoch 5/10
19/19 - 3s - loss: 270.6985 - loglik: -2.6952e+02 - logprior: -1.1783e+00
Epoch 6/10
19/19 - 4s - loss: 268.9200 - loglik: -2.6778e+02 - logprior: -1.1382e+00
Epoch 7/10
19/19 - 3s - loss: 269.5444 - loglik: -2.6843e+02 - logprior: -1.1133e+00
Fitted a model with MAP estimate = -268.6468
expansions: [(12, 2), (14, 3), (16, 1), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 270.3390 - loglik: -2.6651e+02 - logprior: -3.8319e+00
Epoch 2/2
19/19 - 5s - loss: 259.7052 - loglik: -2.5791e+02 - logprior: -1.7985e+00
Fitted a model with MAP estimate = -258.0692
expansions: [(0, 3)]
discards: [  0  12  17  38  76 138 140]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.9941 - loglik: -2.5921e+02 - logprior: -2.7891e+00
Epoch 2/2
19/19 - 5s - loss: 258.3670 - loglik: -2.5742e+02 - logprior: -9.4839e-01
Fitted a model with MAP estimate = -257.5647
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 263.9026 - loglik: -2.6026e+02 - logprior: -3.6458e+00
Epoch 2/10
19/19 - 5s - loss: 260.7430 - loglik: -2.5921e+02 - logprior: -1.5313e+00
Epoch 3/10
19/19 - 5s - loss: 259.1283 - loglik: -2.5841e+02 - logprior: -7.2034e-01
Epoch 4/10
19/19 - 5s - loss: 257.6336 - loglik: -2.5723e+02 - logprior: -4.0799e-01
Epoch 5/10
19/19 - 5s - loss: 257.0476 - loglik: -2.5675e+02 - logprior: -2.9760e-01
Epoch 6/10
19/19 - 5s - loss: 256.3523 - loglik: -2.5611e+02 - logprior: -2.4376e-01
Epoch 7/10
19/19 - 5s - loss: 256.3819 - loglik: -2.5619e+02 - logprior: -1.9395e-01
Fitted a model with MAP estimate = -256.1258
Time for alignment: 116.5393
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 359.9353 - loglik: -3.5694e+02 - logprior: -2.9973e+00
Epoch 2/10
19/19 - 3s - loss: 305.2310 - loglik: -3.0411e+02 - logprior: -1.1229e+00
Epoch 3/10
19/19 - 3s - loss: 279.1948 - loglik: -2.7801e+02 - logprior: -1.1799e+00
Epoch 4/10
19/19 - 3s - loss: 272.3957 - loglik: -2.7121e+02 - logprior: -1.1886e+00
Epoch 5/10
19/19 - 3s - loss: 270.6153 - loglik: -2.6949e+02 - logprior: -1.1245e+00
Epoch 6/10
19/19 - 4s - loss: 269.3143 - loglik: -2.6821e+02 - logprior: -1.1016e+00
Epoch 7/10
19/19 - 4s - loss: 268.7421 - loglik: -2.6767e+02 - logprior: -1.0725e+00
Epoch 8/10
19/19 - 4s - loss: 268.7172 - loglik: -2.6765e+02 - logprior: -1.0644e+00
Epoch 9/10
19/19 - 4s - loss: 268.8128 - loglik: -2.6776e+02 - logprior: -1.0526e+00
Fitted a model with MAP estimate = -268.2899
expansions: [(12, 2), (14, 4), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (57, 2), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 270.4760 - loglik: -2.6662e+02 - logprior: -3.8555e+00
Epoch 2/2
19/19 - 5s - loss: 260.5610 - loglik: -2.5873e+02 - logprior: -1.8274e+00
Fitted a model with MAP estimate = -258.8213
expansions: [(0, 3)]
discards: [  0  12  17  38  76 139]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 262.1258 - loglik: -2.5936e+02 - logprior: -2.7682e+00
Epoch 2/2
19/19 - 4s - loss: 258.5365 - loglik: -2.5762e+02 - logprior: -9.1496e-01
Fitted a model with MAP estimate = -257.6809
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 264.2812 - loglik: -2.6065e+02 - logprior: -3.6347e+00
Epoch 2/10
19/19 - 4s - loss: 260.6488 - loglik: -2.5918e+02 - logprior: -1.4705e+00
Epoch 3/10
19/19 - 4s - loss: 258.6707 - loglik: -2.5804e+02 - logprior: -6.3433e-01
Epoch 4/10
19/19 - 4s - loss: 257.7013 - loglik: -2.5733e+02 - logprior: -3.7314e-01
Epoch 5/10
19/19 - 5s - loss: 257.1749 - loglik: -2.5691e+02 - logprior: -2.6739e-01
Epoch 6/10
19/19 - 4s - loss: 256.2003 - loglik: -2.5599e+02 - logprior: -2.0574e-01
Epoch 7/10
19/19 - 5s - loss: 256.2227 - loglik: -2.5607e+02 - logprior: -1.5633e-01
Fitted a model with MAP estimate = -256.0028
Time for alignment: 118.5460
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.2850 - loglik: -3.5729e+02 - logprior: -2.9990e+00
Epoch 2/10
19/19 - 3s - loss: 303.2033 - loglik: -3.0204e+02 - logprior: -1.1653e+00
Epoch 3/10
19/19 - 3s - loss: 276.6450 - loglik: -2.7531e+02 - logprior: -1.3340e+00
Epoch 4/10
19/19 - 4s - loss: 270.4481 - loglik: -2.6915e+02 - logprior: -1.2970e+00
Epoch 5/10
19/19 - 3s - loss: 269.4127 - loglik: -2.6820e+02 - logprior: -1.2146e+00
Epoch 6/10
19/19 - 3s - loss: 267.8396 - loglik: -2.6666e+02 - logprior: -1.1791e+00
Epoch 7/10
19/19 - 3s - loss: 267.6296 - loglik: -2.6649e+02 - logprior: -1.1414e+00
Epoch 8/10
19/19 - 4s - loss: 267.2886 - loglik: -2.6616e+02 - logprior: -1.1329e+00
Epoch 9/10
19/19 - 3s - loss: 267.4911 - loglik: -2.6636e+02 - logprior: -1.1315e+00
Fitted a model with MAP estimate = -267.0912
expansions: [(12, 2), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (66, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.0199 - loglik: -2.6518e+02 - logprior: -3.8418e+00
Epoch 2/2
19/19 - 5s - loss: 259.1207 - loglik: -2.5731e+02 - logprior: -1.8103e+00
Fitted a model with MAP estimate = -257.3256
expansions: [(0, 2)]
discards: [  0  12  17  38  76 139]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.7195 - loglik: -2.5797e+02 - logprior: -2.7472e+00
Epoch 2/2
19/19 - 5s - loss: 257.5065 - loglik: -2.5658e+02 - logprior: -9.2737e-01
Fitted a model with MAP estimate = -256.4972
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 261.7753 - loglik: -2.5808e+02 - logprior: -3.6922e+00
Epoch 2/10
19/19 - 5s - loss: 257.9690 - loglik: -2.5677e+02 - logprior: -1.2006e+00
Epoch 3/10
19/19 - 5s - loss: 256.2795 - loglik: -2.5569e+02 - logprior: -5.8813e-01
Epoch 4/10
19/19 - 5s - loss: 255.0864 - loglik: -2.5459e+02 - logprior: -4.9510e-01
Epoch 5/10
19/19 - 5s - loss: 255.3540 - loglik: -2.5490e+02 - logprior: -4.5709e-01
Fitted a model with MAP estimate = -254.5206
Time for alignment: 112.2647
Computed alignments with likelihoods: ['-255.6304', '-255.6572', '-256.1258', '-256.0028', '-254.5206']
Best model has likelihood: -254.5206  (prior= -0.4042 )
time for generating output: 0.1340
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8052980132450331
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.7871 - loglik: -2.7173e+02 - logprior: -3.0597e+00
Epoch 2/10
19/19 - 2s - loss: 236.3294 - loglik: -2.3513e+02 - logprior: -1.1975e+00
Epoch 3/10
19/19 - 2s - loss: 221.0424 - loglik: -2.1975e+02 - logprior: -1.2920e+00
Epoch 4/10
19/19 - 2s - loss: 217.5802 - loglik: -2.1627e+02 - logprior: -1.3123e+00
Epoch 5/10
19/19 - 2s - loss: 216.7573 - loglik: -2.1551e+02 - logprior: -1.2507e+00
Epoch 6/10
19/19 - 2s - loss: 216.5242 - loglik: -2.1531e+02 - logprior: -1.2153e+00
Epoch 7/10
19/19 - 2s - loss: 216.1333 - loglik: -2.1493e+02 - logprior: -1.1986e+00
Epoch 8/10
19/19 - 2s - loss: 215.9074 - loglik: -2.1472e+02 - logprior: -1.1862e+00
Epoch 9/10
19/19 - 2s - loss: 216.0538 - loglik: -2.1487e+02 - logprior: -1.1873e+00
Fitted a model with MAP estimate = -215.5514
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 1), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.4286 - loglik: -2.1453e+02 - logprior: -3.8968e+00
Epoch 2/2
19/19 - 3s - loss: 208.1971 - loglik: -2.0694e+02 - logprior: -1.2580e+00
Fitted a model with MAP estimate = -206.4599
expansions: []
discards: [ 0 45 74 81 82 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.6764 - loglik: -2.0872e+02 - logprior: -3.9566e+00
Epoch 2/2
19/19 - 3s - loss: 208.2847 - loglik: -2.0689e+02 - logprior: -1.3929e+00
Fitted a model with MAP estimate = -206.9474
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.6283 - loglik: -2.0677e+02 - logprior: -2.8539e+00
Epoch 2/10
19/19 - 3s - loss: 207.0303 - loglik: -2.0598e+02 - logprior: -1.0544e+00
Epoch 3/10
19/19 - 3s - loss: 206.2431 - loglik: -2.0525e+02 - logprior: -9.8969e-01
Epoch 4/10
19/19 - 3s - loss: 206.1043 - loglik: -2.0517e+02 - logprior: -9.3094e-01
Epoch 5/10
19/19 - 3s - loss: 205.4003 - loglik: -2.0449e+02 - logprior: -9.1458e-01
Epoch 6/10
19/19 - 3s - loss: 205.1642 - loglik: -2.0427e+02 - logprior: -8.8988e-01
Epoch 7/10
19/19 - 3s - loss: 205.0994 - loglik: -2.0423e+02 - logprior: -8.6833e-01
Epoch 8/10
19/19 - 3s - loss: 205.1835 - loglik: -2.0433e+02 - logprior: -8.5715e-01
Fitted a model with MAP estimate = -204.8584
Time for alignment: 84.1415
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.4131 - loglik: -2.7136e+02 - logprior: -3.0532e+00
Epoch 2/10
19/19 - 2s - loss: 236.0333 - loglik: -2.3483e+02 - logprior: -1.2006e+00
Epoch 3/10
19/19 - 2s - loss: 221.2105 - loglik: -2.1987e+02 - logprior: -1.3427e+00
Epoch 4/10
19/19 - 2s - loss: 217.5062 - loglik: -2.1617e+02 - logprior: -1.3399e+00
Epoch 5/10
19/19 - 2s - loss: 216.3251 - loglik: -2.1505e+02 - logprior: -1.2725e+00
Epoch 6/10
19/19 - 2s - loss: 216.0327 - loglik: -2.1481e+02 - logprior: -1.2235e+00
Epoch 7/10
19/19 - 2s - loss: 216.0410 - loglik: -2.1483e+02 - logprior: -1.2155e+00
Fitted a model with MAP estimate = -215.4570
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 1), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.0210 - loglik: -2.1419e+02 - logprior: -3.8299e+00
Epoch 2/2
19/19 - 3s - loss: 208.0004 - loglik: -2.0674e+02 - logprior: -1.2585e+00
Fitted a model with MAP estimate = -206.3518
expansions: []
discards: [ 0 45 74 79 82 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.4478 - loglik: -2.0851e+02 - logprior: -3.9378e+00
Epoch 2/2
19/19 - 3s - loss: 208.3210 - loglik: -2.0688e+02 - logprior: -1.4370e+00
Fitted a model with MAP estimate = -206.8472
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 209.4768 - loglik: -2.0663e+02 - logprior: -2.8441e+00
Epoch 2/10
19/19 - 3s - loss: 206.8891 - loglik: -2.0584e+02 - logprior: -1.0441e+00
Epoch 3/10
19/19 - 3s - loss: 206.1492 - loglik: -2.0516e+02 - logprior: -9.8627e-01
Epoch 4/10
19/19 - 3s - loss: 205.6602 - loglik: -2.0473e+02 - logprior: -9.3312e-01
Epoch 5/10
19/19 - 3s - loss: 205.7432 - loglik: -2.0484e+02 - logprior: -9.0365e-01
Fitted a model with MAP estimate = -205.0865
Time for alignment: 69.3704
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2931 - loglik: -2.7124e+02 - logprior: -3.0569e+00
Epoch 2/10
19/19 - 2s - loss: 237.2861 - loglik: -2.3607e+02 - logprior: -1.2140e+00
Epoch 3/10
19/19 - 2s - loss: 221.6178 - loglik: -2.2032e+02 - logprior: -1.3001e+00
Epoch 4/10
19/19 - 2s - loss: 217.6585 - loglik: -2.1636e+02 - logprior: -1.3035e+00
Epoch 5/10
19/19 - 2s - loss: 216.8255 - loglik: -2.1557e+02 - logprior: -1.2548e+00
Epoch 6/10
19/19 - 2s - loss: 216.2473 - loglik: -2.1503e+02 - logprior: -1.2205e+00
Epoch 7/10
19/19 - 2s - loss: 216.1017 - loglik: -2.1490e+02 - logprior: -1.2027e+00
Epoch 8/10
19/19 - 2s - loss: 215.7459 - loglik: -2.1454e+02 - logprior: -1.2075e+00
Epoch 9/10
19/19 - 2s - loss: 215.8177 - loglik: -2.1462e+02 - logprior: -1.1998e+00
Fitted a model with MAP estimate = -215.3662
expansions: [(0, 2), (4, 1), (6, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (19, 1), (30, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.5154 - loglik: -2.1369e+02 - logprior: -3.8275e+00
Epoch 2/2
19/19 - 3s - loss: 207.3660 - loglik: -2.0613e+02 - logprior: -1.2376e+00
Fitted a model with MAP estimate = -205.7852
expansions: []
discards: [ 0 74 81 83 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.0604 - loglik: -2.0810e+02 - logprior: -3.9565e+00
Epoch 2/2
19/19 - 3s - loss: 207.7366 - loglik: -2.0626e+02 - logprior: -1.4816e+00
Fitted a model with MAP estimate = -206.2992
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.0271 - loglik: -2.0617e+02 - logprior: -2.8607e+00
Epoch 2/10
19/19 - 3s - loss: 206.3656 - loglik: -2.0531e+02 - logprior: -1.0584e+00
Epoch 3/10
19/19 - 3s - loss: 205.5763 - loglik: -2.0458e+02 - logprior: -9.9193e-01
Epoch 4/10
19/19 - 3s - loss: 205.4232 - loglik: -2.0449e+02 - logprior: -9.3767e-01
Epoch 5/10
19/19 - 3s - loss: 204.8453 - loglik: -2.0393e+02 - logprior: -9.1442e-01
Epoch 6/10
19/19 - 3s - loss: 204.7633 - loglik: -2.0387e+02 - logprior: -8.9059e-01
Epoch 7/10
19/19 - 3s - loss: 204.3792 - loglik: -2.0350e+02 - logprior: -8.7581e-01
Epoch 8/10
19/19 - 3s - loss: 204.4438 - loglik: -2.0359e+02 - logprior: -8.4996e-01
Fitted a model with MAP estimate = -204.3084
Time for alignment: 81.8098
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.5935 - loglik: -2.7153e+02 - logprior: -3.0667e+00
Epoch 2/10
19/19 - 2s - loss: 238.1918 - loglik: -2.3697e+02 - logprior: -1.2179e+00
Epoch 3/10
19/19 - 2s - loss: 221.3397 - loglik: -2.2002e+02 - logprior: -1.3186e+00
Epoch 4/10
19/19 - 2s - loss: 217.3756 - loglik: -2.1606e+02 - logprior: -1.3158e+00
Epoch 5/10
19/19 - 2s - loss: 215.9355 - loglik: -2.1467e+02 - logprior: -1.2627e+00
Epoch 6/10
19/19 - 2s - loss: 215.7879 - loglik: -2.1457e+02 - logprior: -1.2207e+00
Epoch 7/10
19/19 - 2s - loss: 215.1894 - loglik: -2.1398e+02 - logprior: -1.2118e+00
Epoch 8/10
19/19 - 2s - loss: 215.6424 - loglik: -2.1444e+02 - logprior: -1.1987e+00
Fitted a model with MAP estimate = -214.8130
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (47, 1), (61, 2), (62, 2), (63, 2), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 216.9756 - loglik: -2.1313e+02 - logprior: -3.8501e+00
Epoch 2/2
19/19 - 3s - loss: 207.4450 - loglik: -2.0622e+02 - logprior: -1.2275e+00
Fitted a model with MAP estimate = -205.8270
expansions: []
discards: [ 0 46 79 82 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 211.9469 - loglik: -2.0800e+02 - logprior: -3.9453e+00
Epoch 2/2
19/19 - 3s - loss: 207.6827 - loglik: -2.0628e+02 - logprior: -1.4024e+00
Fitted a model with MAP estimate = -206.2968
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 208.9194 - loglik: -2.0607e+02 - logprior: -2.8455e+00
Epoch 2/10
19/19 - 3s - loss: 206.4029 - loglik: -2.0535e+02 - logprior: -1.0537e+00
Epoch 3/10
19/19 - 3s - loss: 205.7524 - loglik: -2.0476e+02 - logprior: -9.8868e-01
Epoch 4/10
19/19 - 3s - loss: 205.3131 - loglik: -2.0438e+02 - logprior: -9.3591e-01
Epoch 5/10
19/19 - 3s - loss: 205.1269 - loglik: -2.0423e+02 - logprior: -8.9401e-01
Epoch 6/10
19/19 - 3s - loss: 204.4595 - loglik: -2.0358e+02 - logprior: -8.7892e-01
Epoch 7/10
19/19 - 3s - loss: 204.4760 - loglik: -2.0362e+02 - logprior: -8.5551e-01
Fitted a model with MAP estimate = -204.3569
Time for alignment: 76.0268
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.3415 - loglik: -2.7128e+02 - logprior: -3.0603e+00
Epoch 2/10
19/19 - 2s - loss: 235.9711 - loglik: -2.3476e+02 - logprior: -1.2120e+00
Epoch 3/10
19/19 - 2s - loss: 220.5466 - loglik: -2.1924e+02 - logprior: -1.3021e+00
Epoch 4/10
19/19 - 2s - loss: 216.6249 - loglik: -2.1530e+02 - logprior: -1.3233e+00
Epoch 5/10
19/19 - 2s - loss: 215.7780 - loglik: -2.1450e+02 - logprior: -1.2738e+00
Epoch 6/10
19/19 - 2s - loss: 215.2265 - loglik: -2.1400e+02 - logprior: -1.2279e+00
Epoch 7/10
19/19 - 2s - loss: 215.2555 - loglik: -2.1404e+02 - logprior: -1.2170e+00
Fitted a model with MAP estimate = -214.6688
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 3), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (74, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 215.9347 - loglik: -2.1216e+02 - logprior: -3.7773e+00
Epoch 2/2
19/19 - 3s - loss: 207.0464 - loglik: -2.0584e+02 - logprior: -1.2043e+00
Fitted a model with MAP estimate = -205.7301
expansions: []
discards: [ 0 74 78]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 210.8416 - loglik: -2.0689e+02 - logprior: -3.9558e+00
Epoch 2/2
19/19 - 3s - loss: 207.0528 - loglik: -2.0564e+02 - logprior: -1.4129e+00
Fitted a model with MAP estimate = -205.7598
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 208.4394 - loglik: -2.0559e+02 - logprior: -2.8527e+00
Epoch 2/10
19/19 - 3s - loss: 205.8422 - loglik: -2.0478e+02 - logprior: -1.0635e+00
Epoch 3/10
19/19 - 3s - loss: 205.3370 - loglik: -2.0434e+02 - logprior: -9.9573e-01
Epoch 4/10
19/19 - 3s - loss: 204.8252 - loglik: -2.0388e+02 - logprior: -9.4120e-01
Epoch 5/10
19/19 - 3s - loss: 204.3745 - loglik: -2.0346e+02 - logprior: -9.1908e-01
Epoch 6/10
19/19 - 3s - loss: 204.3331 - loglik: -2.0344e+02 - logprior: -8.9326e-01
Epoch 7/10
19/19 - 3s - loss: 204.1114 - loglik: -2.0324e+02 - logprior: -8.7478e-01
Epoch 8/10
19/19 - 3s - loss: 204.1112 - loglik: -2.0325e+02 - logprior: -8.5996e-01
Epoch 9/10
19/19 - 3s - loss: 203.4250 - loglik: -2.0258e+02 - logprior: -8.4022e-01
Epoch 10/10
19/19 - 3s - loss: 204.1628 - loglik: -2.0334e+02 - logprior: -8.2178e-01
Fitted a model with MAP estimate = -203.7437
Time for alignment: 82.5177
Computed alignments with likelihoods: ['-204.8584', '-205.0865', '-204.3084', '-204.3569', '-203.7437']
Best model has likelihood: -203.7437  (prior= -0.8008 )
time for generating output: 0.1731
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6681750473955042
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.1530 - loglik: -1.7295e+02 - logprior: -3.2051e+00
Epoch 2/10
19/19 - 1s - loss: 133.4364 - loglik: -1.3209e+02 - logprior: -1.3445e+00
Epoch 3/10
19/19 - 1s - loss: 117.2815 - loglik: -1.1593e+02 - logprior: -1.3524e+00
Epoch 4/10
19/19 - 1s - loss: 114.7171 - loglik: -1.1337e+02 - logprior: -1.3443e+00
Epoch 5/10
19/19 - 1s - loss: 113.7219 - loglik: -1.1243e+02 - logprior: -1.2883e+00
Epoch 6/10
19/19 - 1s - loss: 113.6554 - loglik: -1.1239e+02 - logprior: -1.2621e+00
Epoch 7/10
19/19 - 1s - loss: 113.1357 - loglik: -1.1188e+02 - logprior: -1.2519e+00
Epoch 8/10
19/19 - 1s - loss: 113.1717 - loglik: -1.1193e+02 - logprior: -1.2398e+00
Fitted a model with MAP estimate = -113.0335
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8581 - loglik: -1.0971e+02 - logprior: -4.1488e+00
Epoch 2/2
19/19 - 1s - loss: 105.2036 - loglik: -1.0398e+02 - logprior: -1.2218e+00
Fitted a model with MAP estimate = -103.8433
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2472 - loglik: -1.0520e+02 - logprior: -4.0431e+00
Epoch 2/2
19/19 - 1s - loss: 104.9932 - loglik: -1.0357e+02 - logprior: -1.4228e+00
Fitted a model with MAP estimate = -104.1488
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5047 - loglik: -1.0432e+02 - logprior: -3.1819e+00
Epoch 2/10
19/19 - 1s - loss: 104.3443 - loglik: -1.0302e+02 - logprior: -1.3244e+00
Epoch 3/10
19/19 - 1s - loss: 103.8221 - loglik: -1.0262e+02 - logprior: -1.2048e+00
Epoch 4/10
19/19 - 1s - loss: 103.1546 - loglik: -1.0199e+02 - logprior: -1.1631e+00
Epoch 5/10
19/19 - 1s - loss: 102.9072 - loglik: -1.0179e+02 - logprior: -1.1218e+00
Epoch 6/10
19/19 - 1s - loss: 102.3778 - loglik: -1.0127e+02 - logprior: -1.1111e+00
Epoch 7/10
19/19 - 1s - loss: 102.1760 - loglik: -1.0109e+02 - logprior: -1.0900e+00
Epoch 8/10
19/19 - 1s - loss: 102.6070 - loglik: -1.0153e+02 - logprior: -1.0750e+00
Fitted a model with MAP estimate = -102.1347
Time for alignment: 43.4359
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2327 - loglik: -1.7302e+02 - logprior: -3.2078e+00
Epoch 2/10
19/19 - 1s - loss: 132.6243 - loglik: -1.3129e+02 - logprior: -1.3294e+00
Epoch 3/10
19/19 - 1s - loss: 116.5141 - loglik: -1.1515e+02 - logprior: -1.3642e+00
Epoch 4/10
19/19 - 1s - loss: 114.2433 - loglik: -1.1285e+02 - logprior: -1.3924e+00
Epoch 5/10
19/19 - 1s - loss: 113.4870 - loglik: -1.1216e+02 - logprior: -1.3234e+00
Epoch 6/10
19/19 - 1s - loss: 112.7263 - loglik: -1.1143e+02 - logprior: -1.2954e+00
Epoch 7/10
19/19 - 1s - loss: 113.2186 - loglik: -1.1193e+02 - logprior: -1.2898e+00
Fitted a model with MAP estimate = -112.7529
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.0023 - loglik: -1.0987e+02 - logprior: -4.1318e+00
Epoch 2/2
19/19 - 1s - loss: 104.8835 - loglik: -1.0365e+02 - logprior: -1.2310e+00
Fitted a model with MAP estimate = -103.6995
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1394 - loglik: -1.0512e+02 - logprior: -4.0228e+00
Epoch 2/2
19/19 - 1s - loss: 105.0541 - loglik: -1.0363e+02 - logprior: -1.4242e+00
Fitted a model with MAP estimate = -104.0637
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.3307 - loglik: -1.0414e+02 - logprior: -3.1869e+00
Epoch 2/10
19/19 - 1s - loss: 104.3814 - loglik: -1.0305e+02 - logprior: -1.3274e+00
Epoch 3/10
19/19 - 1s - loss: 103.6645 - loglik: -1.0245e+02 - logprior: -1.2175e+00
Epoch 4/10
19/19 - 1s - loss: 103.2606 - loglik: -1.0209e+02 - logprior: -1.1694e+00
Epoch 5/10
19/19 - 1s - loss: 102.9287 - loglik: -1.0179e+02 - logprior: -1.1362e+00
Epoch 6/10
19/19 - 1s - loss: 102.4838 - loglik: -1.0137e+02 - logprior: -1.1145e+00
Epoch 7/10
19/19 - 1s - loss: 102.1907 - loglik: -1.0109e+02 - logprior: -1.1029e+00
Epoch 8/10
19/19 - 1s - loss: 102.3245 - loglik: -1.0125e+02 - logprior: -1.0764e+00
Fitted a model with MAP estimate = -102.1343
Time for alignment: 41.0927
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.3695 - loglik: -1.7316e+02 - logprior: -3.2075e+00
Epoch 2/10
19/19 - 1s - loss: 133.6058 - loglik: -1.3226e+02 - logprior: -1.3436e+00
Epoch 3/10
19/19 - 1s - loss: 117.0618 - loglik: -1.1568e+02 - logprior: -1.3800e+00
Epoch 4/10
19/19 - 1s - loss: 114.6118 - loglik: -1.1321e+02 - logprior: -1.4000e+00
Epoch 5/10
19/19 - 1s - loss: 113.5851 - loglik: -1.1224e+02 - logprior: -1.3433e+00
Epoch 6/10
19/19 - 1s - loss: 113.3686 - loglik: -1.1205e+02 - logprior: -1.3168e+00
Epoch 7/10
19/19 - 1s - loss: 113.2951 - loglik: -1.1199e+02 - logprior: -1.3016e+00
Epoch 8/10
19/19 - 1s - loss: 112.8410 - loglik: -1.1155e+02 - logprior: -1.2917e+00
Epoch 9/10
19/19 - 1s - loss: 112.7056 - loglik: -1.1142e+02 - logprior: -1.2850e+00
Epoch 10/10
19/19 - 1s - loss: 112.5997 - loglik: -1.1132e+02 - logprior: -1.2821e+00
Fitted a model with MAP estimate = -112.7111
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8773 - loglik: -1.0968e+02 - logprior: -4.1970e+00
Epoch 2/2
19/19 - 1s - loss: 105.0775 - loglik: -1.0385e+02 - logprior: -1.2244e+00
Fitted a model with MAP estimate = -103.7024
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1637 - loglik: -1.0513e+02 - logprior: -4.0355e+00
Epoch 2/2
19/19 - 1s - loss: 104.9487 - loglik: -1.0352e+02 - logprior: -1.4294e+00
Fitted a model with MAP estimate = -104.0988
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.2642 - loglik: -1.0407e+02 - logprior: -3.1965e+00
Epoch 2/10
19/19 - 1s - loss: 104.4793 - loglik: -1.0315e+02 - logprior: -1.3276e+00
Epoch 3/10
19/19 - 1s - loss: 103.5342 - loglik: -1.0232e+02 - logprior: -1.2138e+00
Epoch 4/10
19/19 - 1s - loss: 103.4749 - loglik: -1.0232e+02 - logprior: -1.1572e+00
Epoch 5/10
19/19 - 1s - loss: 102.8074 - loglik: -1.0168e+02 - logprior: -1.1319e+00
Epoch 6/10
19/19 - 1s - loss: 102.3302 - loglik: -1.0122e+02 - logprior: -1.1136e+00
Epoch 7/10
19/19 - 1s - loss: 102.0577 - loglik: -1.0096e+02 - logprior: -1.0958e+00
Epoch 8/10
19/19 - 1s - loss: 102.5595 - loglik: -1.0148e+02 - logprior: -1.0770e+00
Fitted a model with MAP estimate = -102.1026
Time for alignment: 44.1178
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.3850 - loglik: -1.7318e+02 - logprior: -3.2027e+00
Epoch 2/10
19/19 - 1s - loss: 133.7022 - loglik: -1.3236e+02 - logprior: -1.3444e+00
Epoch 3/10
19/19 - 1s - loss: 117.4952 - loglik: -1.1614e+02 - logprior: -1.3521e+00
Epoch 4/10
19/19 - 1s - loss: 114.5301 - loglik: -1.1316e+02 - logprior: -1.3659e+00
Epoch 5/10
19/19 - 1s - loss: 113.9627 - loglik: -1.1265e+02 - logprior: -1.3165e+00
Epoch 6/10
19/19 - 1s - loss: 113.2378 - loglik: -1.1195e+02 - logprior: -1.2879e+00
Epoch 7/10
19/19 - 1s - loss: 113.3289 - loglik: -1.1205e+02 - logprior: -1.2819e+00
Fitted a model with MAP estimate = -113.0746
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.5776 - loglik: -1.0947e+02 - logprior: -4.1088e+00
Epoch 2/2
19/19 - 1s - loss: 104.8468 - loglik: -1.0364e+02 - logprior: -1.2038e+00
Fitted a model with MAP estimate = -103.7529
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1322 - loglik: -1.0509e+02 - logprior: -4.0381e+00
Epoch 2/2
19/19 - 1s - loss: 104.9326 - loglik: -1.0351e+02 - logprior: -1.4192e+00
Fitted a model with MAP estimate = -104.0788
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.2782 - loglik: -1.0418e+02 - logprior: -3.1002e+00
Epoch 2/10
19/19 - 1s - loss: 104.5649 - loglik: -1.0323e+02 - logprior: -1.3315e+00
Epoch 3/10
19/19 - 1s - loss: 103.7614 - loglik: -1.0254e+02 - logprior: -1.2241e+00
Epoch 4/10
19/19 - 1s - loss: 102.9414 - loglik: -1.0177e+02 - logprior: -1.1696e+00
Epoch 5/10
19/19 - 1s - loss: 102.8254 - loglik: -1.0169e+02 - logprior: -1.1357e+00
Epoch 6/10
19/19 - 1s - loss: 102.4615 - loglik: -1.0134e+02 - logprior: -1.1184e+00
Epoch 7/10
19/19 - 1s - loss: 102.0108 - loglik: -1.0091e+02 - logprior: -1.0969e+00
Epoch 8/10
19/19 - 1s - loss: 102.1877 - loglik: -1.0111e+02 - logprior: -1.0815e+00
Fitted a model with MAP estimate = -102.0249
Time for alignment: 40.6955
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2157 - loglik: -1.7301e+02 - logprior: -3.2060e+00
Epoch 2/10
19/19 - 1s - loss: 132.5335 - loglik: -1.3120e+02 - logprior: -1.3376e+00
Epoch 3/10
19/19 - 1s - loss: 116.7023 - loglik: -1.1536e+02 - logprior: -1.3447e+00
Epoch 4/10
19/19 - 1s - loss: 114.6640 - loglik: -1.1333e+02 - logprior: -1.3363e+00
Epoch 5/10
19/19 - 1s - loss: 113.7720 - loglik: -1.1249e+02 - logprior: -1.2858e+00
Epoch 6/10
19/19 - 1s - loss: 113.1318 - loglik: -1.1187e+02 - logprior: -1.2610e+00
Epoch 7/10
19/19 - 1s - loss: 113.3323 - loglik: -1.1208e+02 - logprior: -1.2492e+00
Fitted a model with MAP estimate = -113.1372
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.7420 - loglik: -1.0963e+02 - logprior: -4.1121e+00
Epoch 2/2
19/19 - 1s - loss: 105.0024 - loglik: -1.0377e+02 - logprior: -1.2307e+00
Fitted a model with MAP estimate = -103.8307
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2287 - loglik: -1.0521e+02 - logprior: -4.0203e+00
Epoch 2/2
19/19 - 1s - loss: 104.9791 - loglik: -1.0355e+02 - logprior: -1.4267e+00
Fitted a model with MAP estimate = -104.1556
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.3982 - loglik: -1.0422e+02 - logprior: -3.1823e+00
Epoch 2/10
19/19 - 1s - loss: 104.3493 - loglik: -1.0302e+02 - logprior: -1.3272e+00
Epoch 3/10
19/19 - 1s - loss: 103.9345 - loglik: -1.0272e+02 - logprior: -1.2156e+00
Epoch 4/10
19/19 - 1s - loss: 102.9051 - loglik: -1.0174e+02 - logprior: -1.1657e+00
Epoch 5/10
19/19 - 1s - loss: 102.7284 - loglik: -1.0160e+02 - logprior: -1.1277e+00
Epoch 6/10
19/19 - 1s - loss: 102.5319 - loglik: -1.0142e+02 - logprior: -1.1088e+00
Epoch 7/10
19/19 - 1s - loss: 102.2344 - loglik: -1.0114e+02 - logprior: -1.0967e+00
Epoch 8/10
19/19 - 1s - loss: 102.0275 - loglik: -1.0095e+02 - logprior: -1.0768e+00
Epoch 9/10
19/19 - 1s - loss: 102.2983 - loglik: -1.0124e+02 - logprior: -1.0576e+00
Fitted a model with MAP estimate = -101.9919
Time for alignment: 41.5696
Computed alignments with likelihoods: ['-102.1347', '-102.1343', '-102.1026', '-102.0249', '-101.9919']
Best model has likelihood: -101.9919  (prior= -1.0698 )
time for generating output: 0.1044
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8142414860681114
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 568.7345 - loglik: -5.6691e+02 - logprior: -1.8245e+00
Epoch 2/10
39/39 - 14s - loss: 488.7275 - loglik: -4.8764e+02 - logprior: -1.0871e+00
Epoch 3/10
39/39 - 15s - loss: 484.3369 - loglik: -4.8338e+02 - logprior: -9.5324e-01
Epoch 4/10
39/39 - 15s - loss: 482.5723 - loglik: -4.8164e+02 - logprior: -9.3416e-01
Epoch 5/10
39/39 - 16s - loss: 482.0006 - loglik: -4.8109e+02 - logprior: -9.1541e-01
Epoch 6/10
39/39 - 17s - loss: 481.6476 - loglik: -4.8075e+02 - logprior: -9.0112e-01
Epoch 7/10
39/39 - 17s - loss: 481.8128 - loglik: -4.8091e+02 - logprior: -9.0472e-01
Fitted a model with MAP estimate = -479.7209
expansions: [(25, 1), (30, 4), (57, 1), (106, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 484.2262 - loglik: -4.8230e+02 - logprior: -1.9272e+00
Epoch 2/2
39/39 - 19s - loss: 479.2520 - loglik: -4.7832e+02 - logprior: -9.3354e-01
Fitted a model with MAP estimate = -476.3313
expansions: [(110, 2)]
discards: [  0  33 114]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 484.4536 - loglik: -4.8174e+02 - logprior: -2.7173e+00
Epoch 2/2
39/39 - 18s - loss: 479.3349 - loglik: -4.7819e+02 - logprior: -1.1456e+00
Fitted a model with MAP estimate = -475.9819
expansions: [(0, 2), (109, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 478.7512 - loglik: -4.7707e+02 - logprior: -1.6769e+00
Epoch 2/10
39/39 - 18s - loss: 475.3067 - loglik: -4.7472e+02 - logprior: -5.8826e-01
Epoch 3/10
39/39 - 18s - loss: 474.3192 - loglik: -4.7383e+02 - logprior: -4.9266e-01
Epoch 4/10
39/39 - 19s - loss: 472.5316 - loglik: -4.7212e+02 - logprior: -4.1431e-01
Epoch 5/10
39/39 - 18s - loss: 472.0498 - loglik: -4.7173e+02 - logprior: -3.2182e-01
Epoch 6/10
39/39 - 17s - loss: 471.3890 - loglik: -4.7115e+02 - logprior: -2.4361e-01
Epoch 7/10
39/39 - 18s - loss: 472.0464 - loglik: -4.7188e+02 - logprior: -1.6329e-01
Fitted a model with MAP estimate = -471.2981
Time for alignment: 393.2883
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 567.8214 - loglik: -5.6598e+02 - logprior: -1.8459e+00
Epoch 2/10
39/39 - 16s - loss: 484.2928 - loglik: -4.8315e+02 - logprior: -1.1461e+00
Epoch 3/10
39/39 - 16s - loss: 476.4997 - loglik: -4.7544e+02 - logprior: -1.0637e+00
Epoch 4/10
39/39 - 16s - loss: 473.7229 - loglik: -4.7267e+02 - logprior: -1.0527e+00
Epoch 5/10
39/39 - 16s - loss: 472.6773 - loglik: -4.7163e+02 - logprior: -1.0484e+00
Epoch 6/10
39/39 - 17s - loss: 472.2326 - loglik: -4.7119e+02 - logprior: -1.0443e+00
Epoch 7/10
39/39 - 17s - loss: 472.1384 - loglik: -4.7109e+02 - logprior: -1.0437e+00
Epoch 8/10
39/39 - 17s - loss: 472.0915 - loglik: -4.7104e+02 - logprior: -1.0506e+00
Epoch 9/10
39/39 - 18s - loss: 472.1298 - loglik: -4.7108e+02 - logprior: -1.0462e+00
Fitted a model with MAP estimate = -470.3072
expansions: [(25, 1), (55, 1), (56, 1), (57, 1), (75, 1), (81, 1), (100, 1), (102, 1), (137, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 477.3741 - loglik: -4.7459e+02 - logprior: -2.7875e+00
Epoch 2/2
39/39 - 19s - loss: 471.1813 - loglik: -4.7027e+02 - logprior: -9.1571e-01
Fitted a model with MAP estimate = -467.5532
expansions: [(0, 2), (109, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 471.9353 - loglik: -4.7011e+02 - logprior: -1.8260e+00
Epoch 2/2
39/39 - 19s - loss: 469.0323 - loglik: -4.6829e+02 - logprior: -7.4166e-01
Fitted a model with MAP estimate = -466.2323
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 472.1589 - loglik: -4.6991e+02 - logprior: -2.2486e+00
Epoch 2/10
39/39 - 18s - loss: 466.8897 - loglik: -4.6636e+02 - logprior: -5.2634e-01
Epoch 3/10
39/39 - 18s - loss: 466.5191 - loglik: -4.6606e+02 - logprior: -4.6127e-01
Epoch 4/10
39/39 - 18s - loss: 464.7934 - loglik: -4.6440e+02 - logprior: -3.9075e-01
Epoch 5/10
39/39 - 17s - loss: 464.1244 - loglik: -4.6381e+02 - logprior: -3.1719e-01
Epoch 6/10
39/39 - 17s - loss: 463.4581 - loglik: -4.6324e+02 - logprior: -2.2104e-01
Epoch 7/10
39/39 - 18s - loss: 463.9273 - loglik: -4.6377e+02 - logprior: -1.5323e-01
Fitted a model with MAP estimate = -463.4071
Time for alignment: 433.9942
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 563.8625 - loglik: -5.6201e+02 - logprior: -1.8486e+00
Epoch 2/10
39/39 - 16s - loss: 479.1350 - loglik: -4.7791e+02 - logprior: -1.2264e+00
Epoch 3/10
39/39 - 16s - loss: 472.3848 - loglik: -4.7126e+02 - logprior: -1.1290e+00
Epoch 4/10
39/39 - 16s - loss: 470.8511 - loglik: -4.6976e+02 - logprior: -1.0881e+00
Epoch 5/10
39/39 - 17s - loss: 469.9017 - loglik: -4.6883e+02 - logprior: -1.0746e+00
Epoch 6/10
39/39 - 18s - loss: 469.9285 - loglik: -4.6886e+02 - logprior: -1.0703e+00
Fitted a model with MAP estimate = -467.8145
expansions: [(25, 1), (28, 1), (29, 4), (56, 1), (57, 1), (75, 1), (77, 1), (82, 1), (102, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 469.5548 - loglik: -4.6769e+02 - logprior: -1.8674e+00
Epoch 2/2
39/39 - 20s - loss: 464.6456 - loglik: -4.6383e+02 - logprior: -8.1626e-01
Fitted a model with MAP estimate = -461.9266
expansions: []
discards: [ 0 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 471.1674 - loglik: -4.6853e+02 - logprior: -2.6371e+00
Epoch 2/2
39/39 - 19s - loss: 467.4907 - loglik: -4.6645e+02 - logprior: -1.0382e+00
Fitted a model with MAP estimate = -463.9754
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 466.8263 - loglik: -4.6520e+02 - logprior: -1.6256e+00
Epoch 2/10
39/39 - 19s - loss: 463.4313 - loglik: -4.6290e+02 - logprior: -5.3511e-01
Epoch 3/10
39/39 - 19s - loss: 463.0925 - loglik: -4.6265e+02 - logprior: -4.3992e-01
Epoch 4/10
39/39 - 19s - loss: 461.5345 - loglik: -4.6118e+02 - logprior: -3.5013e-01
Epoch 5/10
39/39 - 19s - loss: 461.0392 - loglik: -4.6076e+02 - logprior: -2.8096e-01
Epoch 6/10
39/39 - 18s - loss: 460.5124 - loglik: -4.6032e+02 - logprior: -1.9431e-01
Epoch 7/10
39/39 - 18s - loss: 460.4276 - loglik: -4.6031e+02 - logprior: -1.1888e-01
Epoch 8/10
39/39 - 17s - loss: 459.9726 - loglik: -4.5993e+02 - logprior: -4.1052e-02
Epoch 9/10
39/39 - 17s - loss: 460.6408 - loglik: -4.6068e+02 - logprior: 0.0396
Fitted a model with MAP estimate = -459.9228
Time for alignment: 429.7311
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 569.0544 - loglik: -5.6721e+02 - logprior: -1.8429e+00
Epoch 2/10
39/39 - 16s - loss: 484.8719 - loglik: -4.8367e+02 - logprior: -1.2032e+00
Epoch 3/10
39/39 - 16s - loss: 475.5400 - loglik: -4.7438e+02 - logprior: -1.1561e+00
Epoch 4/10
39/39 - 16s - loss: 473.4084 - loglik: -4.7229e+02 - logprior: -1.1173e+00
Epoch 5/10
39/39 - 16s - loss: 472.9531 - loglik: -4.7184e+02 - logprior: -1.1096e+00
Epoch 6/10
39/39 - 16s - loss: 472.3327 - loglik: -4.7122e+02 - logprior: -1.1085e+00
Epoch 7/10
39/39 - 16s - loss: 472.1502 - loglik: -4.7104e+02 - logprior: -1.1083e+00
Epoch 8/10
39/39 - 16s - loss: 472.4462 - loglik: -4.7134e+02 - logprior: -1.1095e+00
Fitted a model with MAP estimate = -470.4622
expansions: [(29, 1), (30, 1), (56, 1), (57, 1), (60, 2), (79, 1), (80, 1), (81, 1), (82, 1), (101, 1), (102, 1), (105, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 473.2068 - loglik: -4.7130e+02 - logprior: -1.9088e+00
Epoch 2/2
39/39 - 17s - loss: 468.1517 - loglik: -4.6726e+02 - logprior: -8.8982e-01
Fitted a model with MAP estimate = -465.1993
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 473.1517 - loglik: -4.7042e+02 - logprior: -2.7274e+00
Epoch 2/2
39/39 - 18s - loss: 469.2729 - loglik: -4.6812e+02 - logprior: -1.1519e+00
Fitted a model with MAP estimate = -465.9292
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 468.9529 - loglik: -4.6724e+02 - logprior: -1.7138e+00
Epoch 2/10
39/39 - 17s - loss: 465.7619 - loglik: -4.6514e+02 - logprior: -6.1810e-01
Epoch 3/10
39/39 - 18s - loss: 464.9639 - loglik: -4.6445e+02 - logprior: -5.1712e-01
Epoch 4/10
39/39 - 18s - loss: 463.1237 - loglik: -4.6269e+02 - logprior: -4.3389e-01
Epoch 5/10
39/39 - 19s - loss: 463.3700 - loglik: -4.6302e+02 - logprior: -3.5470e-01
Fitted a model with MAP estimate = -462.7202
Time for alignment: 369.6559
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 566.7952 - loglik: -5.6494e+02 - logprior: -1.8583e+00
Epoch 2/10
39/39 - 16s - loss: 483.6996 - loglik: -4.8254e+02 - logprior: -1.1551e+00
Epoch 3/10
39/39 - 17s - loss: 477.2956 - loglik: -4.7626e+02 - logprior: -1.0390e+00
Epoch 4/10
39/39 - 17s - loss: 475.3652 - loglik: -4.7434e+02 - logprior: -1.0288e+00
Epoch 5/10
39/39 - 17s - loss: 473.8149 - loglik: -4.7278e+02 - logprior: -1.0309e+00
Epoch 6/10
39/39 - 16s - loss: 474.1588 - loglik: -4.7312e+02 - logprior: -1.0432e+00
Fitted a model with MAP estimate = -471.7961
expansions: [(29, 1), (30, 1), (54, 1), (57, 1), (58, 1), (61, 2), (80, 1), (81, 1), (82, 1), (102, 1), (137, 1), (138, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 474.3921 - loglik: -4.7250e+02 - logprior: -1.8932e+00
Epoch 2/2
39/39 - 17s - loss: 469.2057 - loglik: -4.6833e+02 - logprior: -8.7124e-01
Fitted a model with MAP estimate = -466.4571
expansions: [(109, 1), (152, 1)]
discards: [ 0 67]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 473.7361 - loglik: -4.7107e+02 - logprior: -2.6644e+00
Epoch 2/2
39/39 - 17s - loss: 469.1654 - loglik: -4.6810e+02 - logprior: -1.0691e+00
Fitted a model with MAP estimate = -466.0104
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 468.8583 - loglik: -4.6718e+02 - logprior: -1.6745e+00
Epoch 2/10
39/39 - 18s - loss: 466.0833 - loglik: -4.6550e+02 - logprior: -5.8422e-01
Epoch 3/10
39/39 - 17s - loss: 465.0407 - loglik: -4.6455e+02 - logprior: -4.8638e-01
Epoch 4/10
39/39 - 18s - loss: 462.9534 - loglik: -4.6255e+02 - logprior: -4.0830e-01
Epoch 5/10
39/39 - 18s - loss: 463.4948 - loglik: -4.6317e+02 - logprior: -3.2858e-01
Fitted a model with MAP estimate = -462.9007
Time for alignment: 341.7411
Computed alignments with likelihoods: ['-471.2981', '-463.4071', '-459.9228', '-462.7202', '-462.9007']
Best model has likelihood: -459.9228  (prior= 0.0880 )
time for generating output: 0.2228
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7593696285778342
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.8394 - loglik: -1.3962e+02 - logprior: -3.2179e+00
Epoch 2/10
19/19 - 1s - loss: 113.9865 - loglik: -1.1255e+02 - logprior: -1.4316e+00
Epoch 3/10
19/19 - 1s - loss: 103.7042 - loglik: -1.0210e+02 - logprior: -1.6066e+00
Epoch 4/10
19/19 - 1s - loss: 101.8610 - loglik: -1.0041e+02 - logprior: -1.4527e+00
Epoch 5/10
19/19 - 1s - loss: 101.2505 - loglik: -9.9817e+01 - logprior: -1.4333e+00
Epoch 6/10
19/19 - 1s - loss: 101.0499 - loglik: -9.9625e+01 - logprior: -1.4246e+00
Epoch 7/10
19/19 - 1s - loss: 100.7798 - loglik: -9.9369e+01 - logprior: -1.4108e+00
Epoch 8/10
19/19 - 1s - loss: 100.8669 - loglik: -9.9462e+01 - logprior: -1.4046e+00
Fitted a model with MAP estimate = -100.7350
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.6706 - loglik: -1.0354e+02 - logprior: -4.1341e+00
Epoch 2/2
19/19 - 1s - loss: 99.6410 - loglik: -9.7589e+01 - logprior: -2.0520e+00
Fitted a model with MAP estimate = -98.0760
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5811 - loglik: -9.7320e+01 - logprior: -3.2607e+00
Epoch 2/2
19/19 - 1s - loss: 96.6651 - loglik: -9.5253e+01 - logprior: -1.4123e+00
Fitted a model with MAP estimate = -96.2335
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5707 - loglik: -9.6322e+01 - logprior: -3.2487e+00
Epoch 2/10
19/19 - 1s - loss: 96.9274 - loglik: -9.5547e+01 - logprior: -1.3804e+00
Epoch 3/10
19/19 - 1s - loss: 96.5079 - loglik: -9.5222e+01 - logprior: -1.2856e+00
Epoch 4/10
19/19 - 1s - loss: 96.3747 - loglik: -9.5140e+01 - logprior: -1.2347e+00
Epoch 5/10
19/19 - 1s - loss: 96.0756 - loglik: -9.4877e+01 - logprior: -1.1981e+00
Epoch 6/10
19/19 - 1s - loss: 95.9237 - loglik: -9.4743e+01 - logprior: -1.1811e+00
Epoch 7/10
19/19 - 1s - loss: 96.0942 - loglik: -9.4927e+01 - logprior: -1.1676e+00
Fitted a model with MAP estimate = -95.8643
Time for alignment: 39.4375
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.6759 - loglik: -1.3946e+02 - logprior: -3.2169e+00
Epoch 2/10
19/19 - 1s - loss: 113.9590 - loglik: -1.1253e+02 - logprior: -1.4283e+00
Epoch 3/10
19/19 - 1s - loss: 104.1205 - loglik: -1.0252e+02 - logprior: -1.5984e+00
Epoch 4/10
19/19 - 1s - loss: 102.0242 - loglik: -1.0057e+02 - logprior: -1.4503e+00
Epoch 5/10
19/19 - 1s - loss: 101.4413 - loglik: -1.0001e+02 - logprior: -1.4316e+00
Epoch 6/10
19/19 - 1s - loss: 101.1064 - loglik: -9.9685e+01 - logprior: -1.4215e+00
Epoch 7/10
19/19 - 1s - loss: 101.0991 - loglik: -9.9691e+01 - logprior: -1.4081e+00
Epoch 8/10
19/19 - 1s - loss: 100.9375 - loglik: -9.9537e+01 - logprior: -1.4009e+00
Epoch 9/10
19/19 - 1s - loss: 100.9717 - loglik: -9.9578e+01 - logprior: -1.3941e+00
Fitted a model with MAP estimate = -100.8934
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.6271 - loglik: -1.0350e+02 - logprior: -4.1319e+00
Epoch 2/2
19/19 - 1s - loss: 99.7886 - loglik: -9.7719e+01 - logprior: -2.0693e+00
Fitted a model with MAP estimate = -98.2301
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 101.0809 - loglik: -9.7836e+01 - logprior: -3.2448e+00
Epoch 2/2
19/19 - 1s - loss: 97.1256 - loglik: -9.5736e+01 - logprior: -1.3901e+00
Fitted a model with MAP estimate = -96.6982
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.3791 - loglik: -9.6129e+01 - logprior: -3.2505e+00
Epoch 2/10
19/19 - 1s - loss: 96.9504 - loglik: -9.5567e+01 - logprior: -1.3836e+00
Epoch 3/10
19/19 - 1s - loss: 96.5540 - loglik: -9.5267e+01 - logprior: -1.2870e+00
Epoch 4/10
19/19 - 1s - loss: 96.2965 - loglik: -9.5060e+01 - logprior: -1.2361e+00
Epoch 5/10
19/19 - 1s - loss: 96.1724 - loglik: -9.4973e+01 - logprior: -1.1997e+00
Epoch 6/10
19/19 - 1s - loss: 95.9153 - loglik: -9.4733e+01 - logprior: -1.1818e+00
Epoch 7/10
19/19 - 1s - loss: 95.8365 - loglik: -9.4670e+01 - logprior: -1.1664e+00
Epoch 8/10
19/19 - 1s - loss: 96.0098 - loglik: -9.4855e+01 - logprior: -1.1546e+00
Fitted a model with MAP estimate = -95.8422
Time for alignment: 40.0533
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7041 - loglik: -1.3949e+02 - logprior: -3.2151e+00
Epoch 2/10
19/19 - 1s - loss: 113.9790 - loglik: -1.1255e+02 - logprior: -1.4326e+00
Epoch 3/10
19/19 - 1s - loss: 104.2426 - loglik: -1.0265e+02 - logprior: -1.5948e+00
Epoch 4/10
19/19 - 1s - loss: 102.6095 - loglik: -1.0117e+02 - logprior: -1.4436e+00
Epoch 5/10
19/19 - 1s - loss: 101.7944 - loglik: -1.0036e+02 - logprior: -1.4326e+00
Epoch 6/10
19/19 - 1s - loss: 101.5938 - loglik: -1.0017e+02 - logprior: -1.4231e+00
Epoch 7/10
19/19 - 1s - loss: 101.1980 - loglik: -9.9787e+01 - logprior: -1.4108e+00
Epoch 8/10
19/19 - 1s - loss: 101.2038 - loglik: -9.9804e+01 - logprior: -1.4000e+00
Fitted a model with MAP estimate = -101.1497
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 3), (26, 1), (28, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.7661 - loglik: -1.0363e+02 - logprior: -4.1397e+00
Epoch 2/2
19/19 - 1s - loss: 99.6914 - loglik: -9.7618e+01 - logprior: -2.0733e+00
Fitted a model with MAP estimate = -98.1443
expansions: [(0, 1)]
discards: [ 0  8 17 29 43]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5398 - loglik: -9.7259e+01 - logprior: -3.2812e+00
Epoch 2/2
19/19 - 1s - loss: 96.6251 - loglik: -9.5203e+01 - logprior: -1.4219e+00
Fitted a model with MAP estimate = -96.1896
expansions: []
discards: [24 37]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.6442 - loglik: -9.6394e+01 - logprior: -3.2502e+00
Epoch 2/10
19/19 - 1s - loss: 97.0014 - loglik: -9.5620e+01 - logprior: -1.3810e+00
Epoch 3/10
19/19 - 1s - loss: 96.6157 - loglik: -9.5333e+01 - logprior: -1.2826e+00
Epoch 4/10
19/19 - 1s - loss: 96.3656 - loglik: -9.5136e+01 - logprior: -1.2295e+00
Epoch 5/10
19/19 - 1s - loss: 95.9635 - loglik: -9.4768e+01 - logprior: -1.1956e+00
Epoch 6/10
19/19 - 1s - loss: 96.1538 - loglik: -9.4976e+01 - logprior: -1.1775e+00
Fitted a model with MAP estimate = -95.9358
Time for alignment: 36.2032
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7942 - loglik: -1.3957e+02 - logprior: -3.2196e+00
Epoch 2/10
19/19 - 1s - loss: 113.8278 - loglik: -1.1239e+02 - logprior: -1.4386e+00
Epoch 3/10
19/19 - 1s - loss: 103.8775 - loglik: -1.0228e+02 - logprior: -1.5987e+00
Epoch 4/10
19/19 - 1s - loss: 101.9986 - loglik: -1.0055e+02 - logprior: -1.4489e+00
Epoch 5/10
19/19 - 1s - loss: 101.3309 - loglik: -9.9895e+01 - logprior: -1.4357e+00
Epoch 6/10
19/19 - 1s - loss: 100.9386 - loglik: -9.9518e+01 - logprior: -1.4209e+00
Epoch 7/10
19/19 - 1s - loss: 100.9027 - loglik: -9.9492e+01 - logprior: -1.4105e+00
Epoch 8/10
19/19 - 1s - loss: 100.7158 - loglik: -9.9316e+01 - logprior: -1.4003e+00
Epoch 9/10
19/19 - 1s - loss: 100.8298 - loglik: -9.9433e+01 - logprior: -1.3965e+00
Fitted a model with MAP estimate = -100.6969
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.5457 - loglik: -1.0341e+02 - logprior: -4.1365e+00
Epoch 2/2
19/19 - 1s - loss: 99.7466 - loglik: -9.7690e+01 - logprior: -2.0569e+00
Fitted a model with MAP estimate = -98.1619
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.6659 - loglik: -9.7406e+01 - logprior: -3.2601e+00
Epoch 2/2
19/19 - 1s - loss: 96.7053 - loglik: -9.5301e+01 - logprior: -1.4044e+00
Fitted a model with MAP estimate = -96.2725
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.5235 - loglik: -9.6275e+01 - logprior: -3.2480e+00
Epoch 2/10
19/19 - 1s - loss: 96.9623 - loglik: -9.5582e+01 - logprior: -1.3801e+00
Epoch 3/10
19/19 - 1s - loss: 96.5175 - loglik: -9.5232e+01 - logprior: -1.2852e+00
Epoch 4/10
19/19 - 1s - loss: 96.1897 - loglik: -9.4959e+01 - logprior: -1.2310e+00
Epoch 5/10
19/19 - 1s - loss: 96.1782 - loglik: -9.4983e+01 - logprior: -1.1950e+00
Epoch 6/10
19/19 - 1s - loss: 95.9414 - loglik: -9.4763e+01 - logprior: -1.1786e+00
Epoch 7/10
19/19 - 1s - loss: 95.9928 - loglik: -9.4826e+01 - logprior: -1.1672e+00
Fitted a model with MAP estimate = -95.8689
Time for alignment: 38.3233
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7591 - loglik: -1.3955e+02 - logprior: -3.2125e+00
Epoch 2/10
19/19 - 1s - loss: 113.8044 - loglik: -1.1238e+02 - logprior: -1.4253e+00
Epoch 3/10
19/19 - 1s - loss: 103.7045 - loglik: -1.0210e+02 - logprior: -1.5995e+00
Epoch 4/10
19/19 - 1s - loss: 101.9167 - loglik: -1.0047e+02 - logprior: -1.4474e+00
Epoch 5/10
19/19 - 1s - loss: 101.3414 - loglik: -9.9913e+01 - logprior: -1.4283e+00
Epoch 6/10
19/19 - 1s - loss: 101.1046 - loglik: -9.9686e+01 - logprior: -1.4191e+00
Epoch 7/10
19/19 - 1s - loss: 101.3288 - loglik: -9.9922e+01 - logprior: -1.4064e+00
Fitted a model with MAP estimate = -100.9751
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (27, 2), (28, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.4949 - loglik: -1.0336e+02 - logprior: -4.1386e+00
Epoch 2/2
19/19 - 1s - loss: 99.7493 - loglik: -9.7721e+01 - logprior: -2.0285e+00
Fitted a model with MAP estimate = -98.0063
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0208 - loglik: -9.7776e+01 - logprior: -3.2447e+00
Epoch 2/2
19/19 - 1s - loss: 97.2869 - loglik: -9.5892e+01 - logprior: -1.3948e+00
Fitted a model with MAP estimate = -96.6879
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.4095 - loglik: -9.6155e+01 - logprior: -3.2546e+00
Epoch 2/10
19/19 - 1s - loss: 96.9642 - loglik: -9.5580e+01 - logprior: -1.3841e+00
Epoch 3/10
19/19 - 1s - loss: 96.4072 - loglik: -9.5120e+01 - logprior: -1.2867e+00
Epoch 4/10
19/19 - 1s - loss: 96.4064 - loglik: -9.5172e+01 - logprior: -1.2340e+00
Epoch 5/10
19/19 - 1s - loss: 95.9543 - loglik: -9.4754e+01 - logprior: -1.2003e+00
Epoch 6/10
19/19 - 1s - loss: 96.0149 - loglik: -9.4830e+01 - logprior: -1.1851e+00
Fitted a model with MAP estimate = -95.8970
Time for alignment: 35.4587
Computed alignments with likelihoods: ['-95.8643', '-95.8422', '-95.9358', '-95.8689', '-95.8970']
Best model has likelihood: -95.8422  (prior= -1.1448 )
time for generating output: 0.1026
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7670830271858927
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.6995 - loglik: -4.5485e+02 - logprior: -2.8455e+00
Epoch 2/10
19/19 - 5s - loss: 381.2953 - loglik: -3.7997e+02 - logprior: -1.3256e+00
Epoch 3/10
19/19 - 6s - loss: 349.4116 - loglik: -3.4772e+02 - logprior: -1.6923e+00
Epoch 4/10
19/19 - 6s - loss: 342.5428 - loglik: -3.4080e+02 - logprior: -1.7456e+00
Epoch 5/10
19/19 - 6s - loss: 340.2617 - loglik: -3.3859e+02 - logprior: -1.6695e+00
Epoch 6/10
19/19 - 6s - loss: 338.7726 - loglik: -3.3714e+02 - logprior: -1.6343e+00
Epoch 7/10
19/19 - 6s - loss: 338.4337 - loglik: -3.3683e+02 - logprior: -1.6083e+00
Epoch 8/10
19/19 - 6s - loss: 337.3160 - loglik: -3.3571e+02 - logprior: -1.6051e+00
Epoch 9/10
19/19 - 6s - loss: 338.7129 - loglik: -3.3712e+02 - logprior: -1.5909e+00
Fitted a model with MAP estimate = -334.5734
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (90, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 325.5271 - loglik: -3.2317e+02 - logprior: -2.3549e+00
Epoch 2/2
39/39 - 10s - loss: 313.7660 - loglik: -3.1285e+02 - logprior: -9.1329e-01
Fitted a model with MAP estimate = -309.5054
expansions: []
discards: [  0 113 145]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 319.2471 - loglik: -3.1649e+02 - logprior: -2.7528e+00
Epoch 2/2
39/39 - 10s - loss: 314.6985 - loglik: -3.1395e+02 - logprior: -7.4984e-01
Fitted a model with MAP estimate = -310.3580
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 314.1535 - loglik: -3.1252e+02 - logprior: -1.6314e+00
Epoch 2/10
39/39 - 10s - loss: 311.1145 - loglik: -3.1063e+02 - logprior: -4.8108e-01
Epoch 3/10
39/39 - 10s - loss: 310.3375 - loglik: -3.0995e+02 - logprior: -3.8432e-01
Epoch 4/10
39/39 - 10s - loss: 308.7505 - loglik: -3.0843e+02 - logprior: -3.2166e-01
Epoch 5/10
39/39 - 10s - loss: 308.0704 - loglik: -3.0782e+02 - logprior: -2.5391e-01
Epoch 6/10
39/39 - 10s - loss: 307.5523 - loglik: -3.0737e+02 - logprior: -1.8505e-01
Epoch 7/10
39/39 - 10s - loss: 306.9792 - loglik: -3.0687e+02 - logprior: -1.1009e-01
Epoch 8/10
39/39 - 10s - loss: 307.4673 - loglik: -3.0744e+02 - logprior: -2.4811e-02
Fitted a model with MAP estimate = -306.8539
Time for alignment: 233.1262
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.2397 - loglik: -4.5439e+02 - logprior: -2.8451e+00
Epoch 2/10
19/19 - 5s - loss: 378.8268 - loglik: -3.7751e+02 - logprior: -1.3129e+00
Epoch 3/10
19/19 - 6s - loss: 346.9818 - loglik: -3.4534e+02 - logprior: -1.6424e+00
Epoch 4/10
19/19 - 6s - loss: 341.6402 - loglik: -3.3991e+02 - logprior: -1.7346e+00
Epoch 5/10
19/19 - 6s - loss: 338.9820 - loglik: -3.3732e+02 - logprior: -1.6612e+00
Epoch 6/10
19/19 - 6s - loss: 338.5762 - loglik: -3.3697e+02 - logprior: -1.6107e+00
Epoch 7/10
19/19 - 6s - loss: 338.1153 - loglik: -3.3654e+02 - logprior: -1.5801e+00
Epoch 8/10
19/19 - 6s - loss: 338.2204 - loglik: -3.3665e+02 - logprior: -1.5750e+00
Fitted a model with MAP estimate = -334.4477
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 2), (87, 2), (88, 2), (89, 1), (99, 1), (100, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 325.8566 - loglik: -3.2352e+02 - logprior: -2.3367e+00
Epoch 2/2
39/39 - 9s - loss: 314.8511 - loglik: -3.1390e+02 - logprior: -9.5077e-01
Fitted a model with MAP estimate = -310.6656
expansions: [(143, 1)]
discards: [  0  93 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 319.8362 - loglik: -3.1708e+02 - logprior: -2.7564e+00
Epoch 2/2
39/39 - 9s - loss: 315.0260 - loglik: -3.1423e+02 - logprior: -7.9294e-01
Fitted a model with MAP estimate = -310.7703
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 314.4543 - loglik: -3.1278e+02 - logprior: -1.6717e+00
Epoch 2/10
39/39 - 9s - loss: 311.5308 - loglik: -3.1101e+02 - logprior: -5.2264e-01
Epoch 3/10
39/39 - 9s - loss: 310.0990 - loglik: -3.0965e+02 - logprior: -4.4710e-01
Epoch 4/10
39/39 - 9s - loss: 309.3191 - loglik: -3.0895e+02 - logprior: -3.7246e-01
Epoch 5/10
39/39 - 10s - loss: 308.2704 - loglik: -3.0798e+02 - logprior: -2.9367e-01
Epoch 6/10
39/39 - 9s - loss: 307.3291 - loglik: -3.0710e+02 - logprior: -2.2570e-01
Epoch 7/10
39/39 - 9s - loss: 307.9351 - loglik: -3.0778e+02 - logprior: -1.5592e-01
Fitted a model with MAP estimate = -307.5108
Time for alignment: 208.7870
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 457.9270 - loglik: -4.5509e+02 - logprior: -2.8355e+00
Epoch 2/10
19/19 - 6s - loss: 380.4289 - loglik: -3.7913e+02 - logprior: -1.3024e+00
Epoch 3/10
19/19 - 6s - loss: 352.1614 - loglik: -3.5053e+02 - logprior: -1.6285e+00
Epoch 4/10
19/19 - 6s - loss: 345.3077 - loglik: -3.4360e+02 - logprior: -1.7118e+00
Epoch 5/10
19/19 - 6s - loss: 341.2055 - loglik: -3.3955e+02 - logprior: -1.6579e+00
Epoch 6/10
19/19 - 6s - loss: 341.0511 - loglik: -3.3944e+02 - logprior: -1.6118e+00
Epoch 7/10
19/19 - 6s - loss: 339.8734 - loglik: -3.3830e+02 - logprior: -1.5762e+00
Epoch 8/10
19/19 - 6s - loss: 339.6013 - loglik: -3.3804e+02 - logprior: -1.5612e+00
Epoch 9/10
19/19 - 6s - loss: 339.1046 - loglik: -3.3756e+02 - logprior: -1.5491e+00
Epoch 10/10
19/19 - 6s - loss: 339.3497 - loglik: -3.3782e+02 - logprior: -1.5345e+00
Fitted a model with MAP estimate = -335.9546
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (51, 1), (57, 1), (65, 1), (66, 3), (68, 1), (71, 2), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (103, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 326.6078 - loglik: -3.2427e+02 - logprior: -2.3351e+00
Epoch 2/2
39/39 - 10s - loss: 315.2265 - loglik: -3.1432e+02 - logprior: -9.1106e-01
Fitted a model with MAP estimate = -310.5594
expansions: []
discards: [  0  54  85  93 114]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 320.1331 - loglik: -3.1740e+02 - logprior: -2.7313e+00
Epoch 2/2
39/39 - 10s - loss: 315.3948 - loglik: -3.1465e+02 - logprior: -7.4467e-01
Fitted a model with MAP estimate = -310.7532
expansions: [(23, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 314.3498 - loglik: -3.1271e+02 - logprior: -1.6420e+00
Epoch 2/10
39/39 - 9s - loss: 311.2099 - loglik: -3.1070e+02 - logprior: -5.0832e-01
Epoch 3/10
39/39 - 9s - loss: 309.6804 - loglik: -3.0926e+02 - logprior: -4.1877e-01
Epoch 4/10
39/39 - 10s - loss: 308.5637 - loglik: -3.0820e+02 - logprior: -3.6089e-01
Epoch 5/10
39/39 - 10s - loss: 307.9398 - loglik: -3.0765e+02 - logprior: -2.8801e-01
Epoch 6/10
39/39 - 9s - loss: 307.1073 - loglik: -3.0690e+02 - logprior: -2.0978e-01
Epoch 7/10
39/39 - 10s - loss: 307.4980 - loglik: -3.0736e+02 - logprior: -1.4253e-01
Fitted a model with MAP estimate = -306.8441
Time for alignment: 226.5115
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 457.6017 - loglik: -4.5477e+02 - logprior: -2.8270e+00
Epoch 2/10
19/19 - 6s - loss: 379.7629 - loglik: -3.7846e+02 - logprior: -1.3012e+00
Epoch 3/10
19/19 - 7s - loss: 351.2054 - loglik: -3.4961e+02 - logprior: -1.5996e+00
Epoch 4/10
19/19 - 7s - loss: 343.0445 - loglik: -3.4136e+02 - logprior: -1.6886e+00
Epoch 5/10
19/19 - 7s - loss: 340.8917 - loglik: -3.3924e+02 - logprior: -1.6503e+00
Epoch 6/10
19/19 - 7s - loss: 339.9532 - loglik: -3.3836e+02 - logprior: -1.5895e+00
Epoch 7/10
19/19 - 7s - loss: 340.1263 - loglik: -3.3856e+02 - logprior: -1.5650e+00
Fitted a model with MAP estimate = -336.0226
expansions: [(0, 2), (14, 1), (15, 3), (17, 1), (18, 1), (22, 1), (23, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (103, 1), (113, 1), (114, 2), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 328.3166 - loglik: -3.2601e+02 - logprior: -2.3074e+00
Epoch 2/2
39/39 - 9s - loss: 317.8699 - loglik: -3.1694e+02 - logprior: -9.2880e-01
Fitted a model with MAP estimate = -313.5806
expansions: [(20, 1)]
discards: [  0 111 131]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 323.8498 - loglik: -3.2109e+02 - logprior: -2.7559e+00
Epoch 2/2
39/39 - 9s - loss: 318.6320 - loglik: -3.1780e+02 - logprior: -8.2924e-01
Fitted a model with MAP estimate = -313.9561
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 317.8398 - loglik: -3.1616e+02 - logprior: -1.6811e+00
Epoch 2/10
39/39 - 9s - loss: 314.8602 - loglik: -3.1432e+02 - logprior: -5.4310e-01
Epoch 3/10
39/39 - 9s - loss: 313.6218 - loglik: -3.1318e+02 - logprior: -4.4371e-01
Epoch 4/10
39/39 - 10s - loss: 312.8818 - loglik: -3.1249e+02 - logprior: -3.8879e-01
Epoch 5/10
39/39 - 9s - loss: 311.7220 - loglik: -3.1141e+02 - logprior: -3.1169e-01
Epoch 6/10
39/39 - 9s - loss: 311.1743 - loglik: -3.1093e+02 - logprior: -2.4397e-01
Epoch 7/10
39/39 - 10s - loss: 310.8968 - loglik: -3.1072e+02 - logprior: -1.7391e-01
Epoch 8/10
39/39 - 9s - loss: 310.1172 - loglik: -3.1002e+02 - logprior: -9.2762e-02
Epoch 9/10
39/39 - 9s - loss: 310.0010 - loglik: -3.0999e+02 - logprior: -1.5937e-02
Epoch 10/10
39/39 - 9s - loss: 310.1194 - loglik: -3.1018e+02 - logprior: 0.0561
Fitted a model with MAP estimate = -309.7244
Time for alignment: 235.7255
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.8041 - loglik: -4.5496e+02 - logprior: -2.8394e+00
Epoch 2/10
19/19 - 6s - loss: 380.9816 - loglik: -3.7966e+02 - logprior: -1.3258e+00
Epoch 3/10
19/19 - 6s - loss: 348.5827 - loglik: -3.4689e+02 - logprior: -1.6889e+00
Epoch 4/10
19/19 - 6s - loss: 342.3073 - loglik: -3.4055e+02 - logprior: -1.7602e+00
Epoch 5/10
19/19 - 6s - loss: 340.2011 - loglik: -3.3853e+02 - logprior: -1.6723e+00
Epoch 6/10
19/19 - 6s - loss: 338.5732 - loglik: -3.3695e+02 - logprior: -1.6268e+00
Epoch 7/10
19/19 - 6s - loss: 338.2575 - loglik: -3.3666e+02 - logprior: -1.5963e+00
Epoch 8/10
19/19 - 6s - loss: 338.6426 - loglik: -3.3706e+02 - logprior: -1.5869e+00
Fitted a model with MAP estimate = -335.0732
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (22, 1), (36, 1), (38, 1), (44, 1), (48, 1), (50, 2), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (70, 1), (79, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 326.7162 - loglik: -3.2441e+02 - logprior: -2.3091e+00
Epoch 2/2
39/39 - 9s - loss: 314.7521 - loglik: -3.1385e+02 - logprior: -9.0019e-01
Fitted a model with MAP estimate = -310.1685
expansions: [(24, 1)]
discards: [  0 113]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 318.6959 - loglik: -3.1597e+02 - logprior: -2.7298e+00
Epoch 2/2
39/39 - 9s - loss: 314.1516 - loglik: -3.1340e+02 - logprior: -7.4969e-01
Fitted a model with MAP estimate = -309.8120
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 313.6697 - loglik: -3.1202e+02 - logprior: -1.6471e+00
Epoch 2/10
39/39 - 9s - loss: 310.7298 - loglik: -3.1023e+02 - logprior: -4.9695e-01
Epoch 3/10
39/39 - 9s - loss: 309.7262 - loglik: -3.0932e+02 - logprior: -4.1112e-01
Epoch 4/10
39/39 - 9s - loss: 308.0271 - loglik: -3.0769e+02 - logprior: -3.4206e-01
Epoch 5/10
39/39 - 9s - loss: 307.2898 - loglik: -3.0701e+02 - logprior: -2.7601e-01
Epoch 6/10
39/39 - 10s - loss: 307.3765 - loglik: -3.0717e+02 - logprior: -2.0487e-01
Fitted a model with MAP estimate = -306.7264
Time for alignment: 196.2046
Computed alignments with likelihoods: ['-306.8539', '-307.5108', '-306.8441', '-309.7244', '-306.7264']
Best model has likelihood: -306.7264  (prior= -0.1806 )
time for generating output: 0.3666
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.4322403277665744
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 902.7805 - loglik: -9.0142e+02 - logprior: -1.3648e+00
Epoch 2/10
39/39 - 52s - loss: 724.8838 - loglik: -7.2321e+02 - logprior: -1.6697e+00
Epoch 3/10
39/39 - 53s - loss: 709.8019 - loglik: -7.0820e+02 - logprior: -1.6057e+00
Epoch 4/10
39/39 - 55s - loss: 704.8900 - loglik: -7.0322e+02 - logprior: -1.6656e+00
Epoch 5/10
39/39 - 54s - loss: 702.5923 - loglik: -7.0087e+02 - logprior: -1.7175e+00
Epoch 6/10
39/39 - 51s - loss: 701.6547 - loglik: -6.9991e+02 - logprior: -1.7437e+00
Epoch 7/10
39/39 - 52s - loss: 700.5128 - loglik: -6.9880e+02 - logprior: -1.7130e+00
Epoch 8/10
39/39 - 49s - loss: 700.4495 - loglik: -6.9876e+02 - logprior: -1.6941e+00
Epoch 9/10
39/39 - 50s - loss: 700.1854 - loglik: -6.9838e+02 - logprior: -1.8046e+00
Epoch 10/10
39/39 - 49s - loss: 700.4993 - loglik: -6.9878e+02 - logprior: -1.7154e+00
Fitted a model with MAP estimate = -699.9710
expansions: [(0, 5), (43, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (194, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (229, 1), (230, 4), (256, 4), (258, 2), (259, 2), (260, 2), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 680.5836 - loglik: -6.7837e+02 - logprior: -2.2175e+00
Epoch 2/2
39/39 - 64s - loss: 658.4918 - loglik: -6.5768e+02 - logprior: -8.1041e-01
Fitted a model with MAP estimate = -654.3920
expansions: []
discards: [  1   2 148 153 224 233 261 262 292 330 333 354 370 371]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 664.3764 - loglik: -6.6302e+02 - logprior: -1.3521e+00
Epoch 2/2
39/39 - 57s - loss: 658.6556 - loglik: -6.5839e+02 - logprior: -2.6297e-01
Fitted a model with MAP estimate = -655.2715
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 662.1135 - loglik: -6.6098e+02 - logprior: -1.1367e+00
Epoch 2/10
39/39 - 58s - loss: 658.1791 - loglik: -6.5832e+02 - logprior: 0.1441
Epoch 3/10
39/39 - 58s - loss: 654.3619 - loglik: -6.5467e+02 - logprior: 0.3120
Epoch 4/10
39/39 - 58s - loss: 651.6918 - loglik: -6.5218e+02 - logprior: 0.4852
Epoch 5/10
39/39 - 58s - loss: 649.3686 - loglik: -6.5000e+02 - logprior: 0.6331
Epoch 6/10
39/39 - 63s - loss: 648.5520 - loglik: -6.4924e+02 - logprior: 0.6928
Epoch 7/10
39/39 - 72s - loss: 648.0173 - loglik: -6.4867e+02 - logprior: 0.6576
Epoch 8/10
39/39 - 73s - loss: 647.4397 - loglik: -6.4834e+02 - logprior: 0.8994
Epoch 9/10
39/39 - 74s - loss: 648.0012 - loglik: -6.4910e+02 - logprior: 1.0948
Fitted a model with MAP estimate = -646.9264
Time for alignment: 1638.4310
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 902.6968 - loglik: -9.0131e+02 - logprior: -1.3828e+00
Epoch 2/10
39/39 - 46s - loss: 723.1644 - loglik: -7.2147e+02 - logprior: -1.6983e+00
Epoch 3/10
39/39 - 49s - loss: 707.7996 - loglik: -7.0611e+02 - logprior: -1.6901e+00
Epoch 4/10
39/39 - 53s - loss: 703.1321 - loglik: -7.0149e+02 - logprior: -1.6465e+00
Epoch 5/10
39/39 - 48s - loss: 700.7092 - loglik: -6.9900e+02 - logprior: -1.7108e+00
Epoch 6/10
39/39 - 48s - loss: 699.1633 - loglik: -6.9746e+02 - logprior: -1.7076e+00
Epoch 7/10
39/39 - 54s - loss: 699.1143 - loglik: -6.9734e+02 - logprior: -1.7704e+00
Epoch 8/10
39/39 - 56s - loss: 698.2856 - loglik: -6.9650e+02 - logprior: -1.7891e+00
Epoch 9/10
39/39 - 55s - loss: 698.3054 - loglik: -6.9652e+02 - logprior: -1.7886e+00
Fitted a model with MAP estimate = -697.5678
expansions: [(0, 5), (41, 1), (45, 2), (60, 1), (62, 1), (67, 1), (73, 1), (74, 1), (82, 1), (84, 3), (85, 3), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (141, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (276, 1), (278, 2), (279, 1), (281, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 681.3418 - loglik: -6.7903e+02 - logprior: -2.3127e+00
Epoch 2/2
39/39 - 80s - loss: 657.8318 - loglik: -6.5689e+02 - logprior: -9.4037e-01
Fitted a model with MAP estimate = -654.0197
expansions: []
discards: [  1   2   3  99 102 149 154 225 234 290 327 352 368 369]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 664.8600 - loglik: -6.6358e+02 - logprior: -1.2807e+00
Epoch 2/2
39/39 - 71s - loss: 659.4946 - loglik: -6.5956e+02 - logprior: 0.0645
Fitted a model with MAP estimate = -656.5277
expansions: [(0, 17)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 92s - loss: 664.1794 - loglik: -6.6231e+02 - logprior: -1.8675e+00
Epoch 2/10
39/39 - 90s - loss: 658.3855 - loglik: -6.5804e+02 - logprior: -3.4838e-01
Epoch 3/10
39/39 - 90s - loss: 655.6373 - loglik: -6.5549e+02 - logprior: -1.4836e-01
Epoch 4/10
39/39 - 80s - loss: 652.4719 - loglik: -6.5245e+02 - logprior: -1.9128e-02
Epoch 5/10
39/39 - 88s - loss: 649.2302 - loglik: -6.4952e+02 - logprior: 0.2922
Epoch 6/10
39/39 - 74s - loss: 648.4896 - loglik: -6.4903e+02 - logprior: 0.5417
Epoch 7/10
39/39 - 86s - loss: 646.9360 - loglik: -6.4758e+02 - logprior: 0.6428
Epoch 8/10
39/39 - 75s - loss: 647.1179 - loglik: -6.4807e+02 - logprior: 0.9509
Fitted a model with MAP estimate = -646.6580
Time for alignment: 1819.0865
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 904.3759 - loglik: -9.0293e+02 - logprior: -1.4422e+00
Epoch 2/10
39/39 - 57s - loss: 723.9950 - loglik: -7.2186e+02 - logprior: -2.1362e+00
Epoch 3/10
39/39 - 59s - loss: 709.1746 - loglik: -7.0703e+02 - logprior: -2.1446e+00
Epoch 4/10
39/39 - 58s - loss: 705.4097 - loglik: -7.0327e+02 - logprior: -2.1358e+00
Epoch 5/10
39/39 - 57s - loss: 702.7249 - loglik: -7.0053e+02 - logprior: -2.1907e+00
Epoch 6/10
39/39 - 58s - loss: 701.2655 - loglik: -6.9909e+02 - logprior: -2.1724e+00
Epoch 7/10
39/39 - 59s - loss: 701.0043 - loglik: -6.9887e+02 - logprior: -2.1384e+00
Epoch 8/10
39/39 - 53s - loss: 700.9562 - loglik: -6.9885e+02 - logprior: -2.1031e+00
Epoch 9/10
39/39 - 52s - loss: 700.4196 - loglik: -6.9831e+02 - logprior: -2.1063e+00
Epoch 10/10
39/39 - 55s - loss: 700.7499 - loglik: -6.9859e+02 - logprior: -2.1588e+00
Fitted a model with MAP estimate = -700.0122
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (40, 1), (42, 1), (45, 1), (60, 1), (62, 1), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (141, 1), (144, 1), (146, 1), (149, 1), (156, 1), (159, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (228, 1), (229, 1), (232, 3), (233, 1), (257, 1), (258, 3), (259, 2), (260, 2), (261, 1), (277, 1), (279, 2), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 679.8428 - loglik: -6.7809e+02 - logprior: -1.7578e+00
Epoch 2/2
39/39 - 75s - loss: 658.8069 - loglik: -6.5812e+02 - logprior: -6.9180e-01
Fitted a model with MAP estimate = -654.9077
expansions: []
discards: [146 151 198 223 232 323 349]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 663.0137 - loglik: -6.6156e+02 - logprior: -1.4542e+00
Epoch 2/2
39/39 - 85s - loss: 657.8340 - loglik: -6.5759e+02 - logprior: -2.3971e-01
Fitted a model with MAP estimate = -654.4832
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 82s - loss: 662.1445 - loglik: -6.6102e+02 - logprior: -1.1279e+00
Epoch 2/10
39/39 - 70s - loss: 657.3988 - loglik: -6.5744e+02 - logprior: 0.0449
Epoch 3/10
39/39 - 79s - loss: 654.6667 - loglik: -6.5480e+02 - logprior: 0.1367
Epoch 4/10
39/39 - 71s - loss: 650.9290 - loglik: -6.5139e+02 - logprior: 0.4618
Epoch 5/10
39/39 - 76s - loss: 649.1879 - loglik: -6.4982e+02 - logprior: 0.6323
Epoch 6/10
39/39 - 86s - loss: 647.7597 - loglik: -6.4830e+02 - logprior: 0.5377
Epoch 7/10
39/39 - 85s - loss: 647.8514 - loglik: -6.4862e+02 - logprior: 0.7675
Fitted a model with MAP estimate = -647.1428
Time for alignment: 1801.3224
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 902.3354 - loglik: -9.0096e+02 - logprior: -1.3794e+00
Epoch 2/10
39/39 - 46s - loss: 723.2090 - loglik: -7.2151e+02 - logprior: -1.7012e+00
Epoch 3/10
39/39 - 45s - loss: 708.2108 - loglik: -7.0660e+02 - logprior: -1.6141e+00
Epoch 4/10
39/39 - 46s - loss: 703.0743 - loglik: -7.0154e+02 - logprior: -1.5303e+00
Epoch 5/10
39/39 - 46s - loss: 699.9017 - loglik: -6.9833e+02 - logprior: -1.5683e+00
Epoch 6/10
39/39 - 46s - loss: 699.2521 - loglik: -6.9763e+02 - logprior: -1.6227e+00
Epoch 7/10
39/39 - 45s - loss: 698.6461 - loglik: -6.9705e+02 - logprior: -1.6004e+00
Epoch 8/10
39/39 - 44s - loss: 698.6511 - loglik: -6.9682e+02 - logprior: -1.8269e+00
Fitted a model with MAP estimate = -697.3993
expansions: [(0, 5), (20, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (83, 2), (84, 3), (90, 1), (91, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (149, 4), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (164, 2), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 679.8182 - loglik: -6.7759e+02 - logprior: -2.2313e+00
Epoch 2/2
39/39 - 69s - loss: 659.0816 - loglik: -6.5822e+02 - logprior: -8.6628e-01
Fitted a model with MAP estimate = -655.2581
expansions: []
discards: [  1   2 101 148 153 183 184 324 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 664.8863 - loglik: -6.6346e+02 - logprior: -1.4227e+00
Epoch 2/2
39/39 - 66s - loss: 659.5322 - loglik: -6.5930e+02 - logprior: -2.3542e-01
Fitted a model with MAP estimate = -656.0187
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 663.0067 - loglik: -6.6181e+02 - logprior: -1.1929e+00
Epoch 2/10
39/39 - 65s - loss: 658.3387 - loglik: -6.5840e+02 - logprior: 0.0650
Epoch 3/10
39/39 - 63s - loss: 655.3422 - loglik: -6.5547e+02 - logprior: 0.1275
Epoch 4/10
39/39 - 56s - loss: 652.1954 - loglik: -6.5252e+02 - logprior: 0.3231
Epoch 5/10
39/39 - 55s - loss: 650.5220 - loglik: -6.5103e+02 - logprior: 0.5104
Epoch 6/10
39/39 - 56s - loss: 649.1593 - loglik: -6.4989e+02 - logprior: 0.7356
Epoch 7/10
39/39 - 59s - loss: 648.3770 - loglik: -6.4926e+02 - logprior: 0.8848
Epoch 8/10
39/39 - 60s - loss: 648.1246 - loglik: -6.4912e+02 - logprior: 0.9966
Epoch 9/10
39/39 - 60s - loss: 648.3334 - loglik: -6.4930e+02 - logprior: 0.9675
Fitted a model with MAP estimate = -647.8507
Time for alignment: 1495.6107
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 903.0524 - loglik: -9.0169e+02 - logprior: -1.3601e+00
Epoch 2/10
39/39 - 41s - loss: 724.6960 - loglik: -7.2298e+02 - logprior: -1.7128e+00
Epoch 3/10
39/39 - 42s - loss: 709.5850 - loglik: -7.0790e+02 - logprior: -1.6873e+00
Epoch 4/10
39/39 - 43s - loss: 703.9525 - loglik: -7.0229e+02 - logprior: -1.6592e+00
Epoch 5/10
39/39 - 43s - loss: 700.4951 - loglik: -6.9879e+02 - logprior: -1.7074e+00
Epoch 6/10
39/39 - 42s - loss: 699.2758 - loglik: -6.9737e+02 - logprior: -1.9055e+00
Epoch 7/10
39/39 - 43s - loss: 699.0348 - loglik: -6.9713e+02 - logprior: -1.9002e+00
Epoch 8/10
39/39 - 43s - loss: 698.5222 - loglik: -6.9663e+02 - logprior: -1.8917e+00
Epoch 9/10
39/39 - 42s - loss: 698.4128 - loglik: -6.9658e+02 - logprior: -1.8351e+00
Epoch 10/10
39/39 - 43s - loss: 698.1052 - loglik: -6.9627e+02 - logprior: -1.8365e+00
Fitted a model with MAP estimate = -697.7712
expansions: [(0, 5), (36, 1), (46, 1), (55, 1), (60, 1), (62, 2), (66, 1), (67, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 3), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 679.9863 - loglik: -6.7794e+02 - logprior: -2.0441e+00
Epoch 2/2
39/39 - 73s - loss: 659.4554 - loglik: -6.5883e+02 - logprior: -6.2698e-01
Fitted a model with MAP estimate = -655.6374
expansions: []
discards: [  1   2   3  72 149 154 233 325 327]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 664.5737 - loglik: -6.6324e+02 - logprior: -1.3314e+00
Epoch 2/2
39/39 - 69s - loss: 658.9709 - loglik: -6.5886e+02 - logprior: -1.1069e-01
Fitted a model with MAP estimate = -655.8447
expansions: [(0, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 381 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 663.6321 - loglik: -6.6175e+02 - logprior: -1.8862e+00
Epoch 2/10
39/39 - 72s - loss: 658.1745 - loglik: -6.5829e+02 - logprior: 0.1153
Epoch 3/10
39/39 - 70s - loss: 654.7046 - loglik: -6.5500e+02 - logprior: 0.2964
Epoch 4/10
39/39 - 63s - loss: 651.9865 - loglik: -6.5241e+02 - logprior: 0.4250
Epoch 5/10
39/39 - 57s - loss: 649.0466 - loglik: -6.4964e+02 - logprior: 0.5896
Epoch 6/10
39/39 - 56s - loss: 648.7787 - loglik: -6.4958e+02 - logprior: 0.8020
Epoch 7/10
39/39 - 56s - loss: 647.7883 - loglik: -6.4885e+02 - logprior: 1.0590
Epoch 8/10
39/39 - 57s - loss: 647.3355 - loglik: -6.4830e+02 - logprior: 0.9677
Epoch 9/10
39/39 - 60s - loss: 647.5259 - loglik: -6.4870e+02 - logprior: 1.1702
Fitted a model with MAP estimate = -646.7722
Time for alignment: 1600.3145
Computed alignments with likelihoods: ['-646.9264', '-646.6580', '-647.1428', '-647.8507', '-646.7722']
Best model has likelihood: -646.6580  (prior= 1.0580 )
time for generating output: 0.3650
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9414294367050272
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 644.7794 - loglik: -6.4297e+02 - logprior: -1.8115e+00
Epoch 2/10
39/39 - 16s - loss: 554.5904 - loglik: -5.5312e+02 - logprior: -1.4695e+00
Epoch 3/10
39/39 - 16s - loss: 546.5272 - loglik: -5.4501e+02 - logprior: -1.5193e+00
Epoch 4/10
39/39 - 16s - loss: 544.4650 - loglik: -5.4290e+02 - logprior: -1.5630e+00
Epoch 5/10
39/39 - 16s - loss: 543.4910 - loglik: -5.4186e+02 - logprior: -1.6321e+00
Epoch 6/10
39/39 - 16s - loss: 542.5807 - loglik: -5.4093e+02 - logprior: -1.6478e+00
Epoch 7/10
39/39 - 16s - loss: 541.8076 - loglik: -5.4015e+02 - logprior: -1.6583e+00
Epoch 8/10
39/39 - 16s - loss: 542.1561 - loglik: -5.4049e+02 - logprior: -1.6636e+00
Fitted a model with MAP estimate = -541.2541
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (77, 1), (81, 6), (89, 1), (90, 1), (117, 1), (118, 8), (119, 2), (120, 1), (121, 2), (128, 1), (129, 1), (131, 3), (132, 2), (135, 1), (136, 1), (139, 1), (141, 3), (142, 8), (143, 1), (145, 1), (146, 1), (157, 1), (160, 6), (161, 1), (170, 1), (171, 1), (173, 1), (174, 2)]
discards: [  0  79 149 150 151 152 153 175 176 177 178 179 180 181 182 183 184 185
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 559.0634 - loglik: -5.5650e+02 - logprior: -2.5650e+00
Epoch 2/2
39/39 - 19s - loss: 548.2593 - loglik: -5.4730e+02 - logprior: -9.5592e-01
Fitted a model with MAP estimate = -545.4194
expansions: [(83, 1), (226, 8), (232, 5)]
discards: [  0   1 132 133 134 135 161 182 183 184 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 549.7709 - loglik: -5.4740e+02 - logprior: -2.3662e+00
Epoch 2/2
39/39 - 19s - loss: 541.9507 - loglik: -5.4159e+02 - logprior: -3.6014e-01
Fitted a model with MAP estimate = -539.7455
expansions: [(0, 2), (216, 8), (234, 5)]
discards: [218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 548.0811 - loglik: -5.4657e+02 - logprior: -1.5077e+00
Epoch 2/10
39/39 - 19s - loss: 542.5912 - loglik: -5.4234e+02 - logprior: -2.5475e-01
Epoch 3/10
39/39 - 19s - loss: 541.4578 - loglik: -5.4134e+02 - logprior: -1.2184e-01
Epoch 4/10
39/39 - 20s - loss: 539.9234 - loglik: -5.3988e+02 - logprior: -4.5886e-02
Epoch 5/10
39/39 - 21s - loss: 539.1475 - loglik: -5.3916e+02 - logprior: 0.0146
Epoch 6/10
39/39 - 21s - loss: 539.0154 - loglik: -5.3909e+02 - logprior: 0.0697
Epoch 7/10
39/39 - 22s - loss: 538.2769 - loglik: -5.3844e+02 - logprior: 0.1591
Epoch 8/10
39/39 - 22s - loss: 538.7387 - loglik: -5.3896e+02 - logprior: 0.2217
Fitted a model with MAP estimate = -538.2474
Time for alignment: 468.1669
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 644.2303 - loglik: -6.4241e+02 - logprior: -1.8222e+00
Epoch 2/10
39/39 - 18s - loss: 554.6940 - loglik: -5.5322e+02 - logprior: -1.4699e+00
Epoch 3/10
39/39 - 18s - loss: 546.3428 - loglik: -5.4473e+02 - logprior: -1.6137e+00
Epoch 4/10
39/39 - 19s - loss: 544.2236 - loglik: -5.4259e+02 - logprior: -1.6366e+00
Epoch 5/10
39/39 - 18s - loss: 543.5778 - loglik: -5.4194e+02 - logprior: -1.6409e+00
Epoch 6/10
39/39 - 18s - loss: 542.8730 - loglik: -5.4122e+02 - logprior: -1.6488e+00
Epoch 7/10
39/39 - 18s - loss: 542.5745 - loglik: -5.4092e+02 - logprior: -1.6524e+00
Epoch 8/10
39/39 - 19s - loss: 542.7484 - loglik: -5.4109e+02 - logprior: -1.6609e+00
Fitted a model with MAP estimate = -542.1382
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (44, 1), (81, 3), (87, 1), (89, 1), (90, 1), (92, 1), (116, 1), (117, 10), (118, 2), (119, 2), (128, 1), (130, 3), (131, 2), (135, 1), (138, 1), (141, 12), (142, 1), (144, 1), (146, 1), (155, 1), (158, 1), (159, 2), (161, 1), (172, 4), (173, 1), (186, 1)]
discards: [  0 148 149 150 151 162 163 164 165 166 167 168 169 170 175 176 177 178
 179 180 181 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 565.3719 - loglik: -5.6279e+02 - logprior: -2.5848e+00
Epoch 2/2
39/39 - 20s - loss: 551.0781 - loglik: -5.4996e+02 - logprior: -1.1188e+00
Fitted a model with MAP estimate = -547.9619
expansions: [(87, 1), (205, 1), (210, 3), (211, 1), (226, 3), (227, 20)]
discards: [  0  81 132 133 134 140 214 215 216 217 218 219 220]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 549.4896 - loglik: -5.4702e+02 - logprior: -2.4732e+00
Epoch 2/2
39/39 - 25s - loss: 539.1265 - loglik: -5.3846e+02 - logprior: -6.6369e-01
Fitted a model with MAP estimate = -535.9120
expansions: [(0, 2), (85, 1), (87, 2), (90, 1), (243, 5)]
discards: [  0  80  81  82 131 155 177 178 221 222 223 224 225 226 227 228 229 230
 231 232 233 234 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 551.4234 - loglik: -5.4984e+02 - logprior: -1.5792e+00
Epoch 2/10
39/39 - 23s - loss: 546.0172 - loglik: -5.4576e+02 - logprior: -2.6214e-01
Epoch 3/10
39/39 - 22s - loss: 543.8226 - loglik: -5.4371e+02 - logprior: -1.0820e-01
Epoch 4/10
39/39 - 22s - loss: 542.9504 - loglik: -5.4293e+02 - logprior: -2.2609e-02
Epoch 5/10
39/39 - 21s - loss: 541.7201 - loglik: -5.4177e+02 - logprior: 0.0503
Epoch 6/10
39/39 - 21s - loss: 541.6217 - loglik: -5.4174e+02 - logprior: 0.1173
Epoch 7/10
39/39 - 21s - loss: 541.3726 - loglik: -5.4155e+02 - logprior: 0.1777
Epoch 8/10
39/39 - 20s - loss: 541.0023 - loglik: -5.4124e+02 - logprior: 0.2363
Epoch 9/10
39/39 - 20s - loss: 542.3350 - loglik: -5.4265e+02 - logprior: 0.3112
Fitted a model with MAP estimate = -541.0498
Time for alignment: 540.5545
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 645.3190 - loglik: -6.4349e+02 - logprior: -1.8306e+00
Epoch 2/10
39/39 - 18s - loss: 557.4293 - loglik: -5.5604e+02 - logprior: -1.3874e+00
Epoch 3/10
39/39 - 18s - loss: 549.6949 - loglik: -5.4822e+02 - logprior: -1.4776e+00
Epoch 4/10
39/39 - 18s - loss: 547.4365 - loglik: -5.4593e+02 - logprior: -1.5059e+00
Epoch 5/10
39/39 - 19s - loss: 545.7664 - loglik: -5.4421e+02 - logprior: -1.5612e+00
Epoch 6/10
39/39 - 19s - loss: 544.5622 - loglik: -5.4298e+02 - logprior: -1.5781e+00
Epoch 7/10
39/39 - 20s - loss: 543.9018 - loglik: -5.4233e+02 - logprior: -1.5727e+00
Epoch 8/10
39/39 - 20s - loss: 543.9189 - loglik: -5.4234e+02 - logprior: -1.5808e+00
Fitted a model with MAP estimate = -543.1455
expansions: [(4, 1), (6, 1), (31, 1), (37, 1), (83, 9), (90, 1), (91, 1), (115, 1), (116, 9), (117, 3), (118, 1), (119, 2), (125, 1), (126, 1), (128, 3), (129, 2), (134, 1), (139, 1), (140, 2), (141, 8), (142, 1), (144, 1), (145, 2), (152, 1), (155, 1), (158, 1), (159, 4), (160, 1), (161, 1), (170, 1), (171, 3), (173, 2)]
discards: [  0  77  78  79 149 174 175 176 177 178 179 180 181 182 183 184 185 186
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 555.9875 - loglik: -5.5343e+02 - logprior: -2.5575e+00
Epoch 2/2
39/39 - 25s - loss: 543.4669 - loglik: -5.4235e+02 - logprior: -1.1167e+00
Fitted a model with MAP estimate = -541.0970
expansions: [(4, 1), (88, 1), (238, 21)]
discards: [  0   1 132 133 134 135 159 177 181 182 197 198 199 200 201 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 545.9513 - loglik: -5.4342e+02 - logprior: -2.5285e+00
Epoch 2/2
39/39 - 25s - loss: 535.9866 - loglik: -5.3516e+02 - logprior: -8.3053e-01
Fitted a model with MAP estimate = -533.3764
expansions: [(0, 2), (3, 1), (218, 5), (243, 3)]
discards: [  0 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 547.2173 - loglik: -5.4562e+02 - logprior: -1.5953e+00
Epoch 2/10
39/39 - 22s - loss: 542.1313 - loglik: -5.4187e+02 - logprior: -2.5856e-01
Epoch 3/10
39/39 - 22s - loss: 540.3732 - loglik: -5.4022e+02 - logprior: -1.5657e-01
Epoch 4/10
39/39 - 21s - loss: 538.9374 - loglik: -5.3886e+02 - logprior: -7.6813e-02
Epoch 5/10
39/39 - 21s - loss: 538.8561 - loglik: -5.3885e+02 - logprior: -4.2567e-03
Epoch 6/10
39/39 - 21s - loss: 537.8751 - loglik: -5.3794e+02 - logprior: 0.0611
Epoch 7/10
39/39 - 21s - loss: 537.6415 - loglik: -5.3777e+02 - logprior: 0.1289
Epoch 8/10
39/39 - 21s - loss: 538.3530 - loglik: -5.3856e+02 - logprior: 0.2022
Fitted a model with MAP estimate = -537.6080
Time for alignment: 540.6803
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 645.6708 - loglik: -6.4387e+02 - logprior: -1.7988e+00
Epoch 2/10
39/39 - 18s - loss: 555.5432 - loglik: -5.5428e+02 - logprior: -1.2605e+00
Epoch 3/10
39/39 - 19s - loss: 547.9392 - loglik: -5.4657e+02 - logprior: -1.3652e+00
Epoch 4/10
39/39 - 20s - loss: 545.8966 - loglik: -5.4449e+02 - logprior: -1.4088e+00
Epoch 5/10
39/39 - 20s - loss: 544.9725 - loglik: -5.4354e+02 - logprior: -1.4326e+00
Epoch 6/10
39/39 - 21s - loss: 544.6250 - loglik: -5.4318e+02 - logprior: -1.4459e+00
Epoch 7/10
39/39 - 21s - loss: 544.0801 - loglik: -5.4262e+02 - logprior: -1.4600e+00
Epoch 8/10
39/39 - 20s - loss: 542.7988 - loglik: -5.4133e+02 - logprior: -1.4646e+00
Epoch 9/10
39/39 - 20s - loss: 542.6790 - loglik: -5.4121e+02 - logprior: -1.4658e+00
Epoch 10/10
39/39 - 20s - loss: 542.9707 - loglik: -5.4150e+02 - logprior: -1.4693e+00
Fitted a model with MAP estimate = -542.0995
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (78, 1), (81, 9), (83, 2), (88, 1), (89, 1), (90, 1), (93, 1), (113, 14), (114, 1), (121, 2), (125, 3), (126, 2), (131, 1), (133, 1), (136, 2), (137, 9), (145, 1), (153, 1), (156, 1), (159, 3), (169, 1), (170, 5), (172, 1), (185, 1)]
discards: [  0 150 151 160 161 162 163 164 165 166 167 174 175 176 177 178 179 180
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 560.4410 - loglik: -5.5786e+02 - logprior: -2.5786e+00
Epoch 2/2
39/39 - 23s - loss: 545.9026 - loglik: -5.4483e+02 - logprior: -1.0728e+00
Fitted a model with MAP estimate = -542.8211
expansions: [(209, 1), (224, 1), (227, 9), (236, 1), (237, 16)]
discards: [  0  80  92  93  96 136 137 138 163 185 186 187 205 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 249 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 544.7966 - loglik: -5.4235e+02 - logprior: -2.4485e+00
Epoch 2/2
39/39 - 23s - loss: 534.0948 - loglik: -5.3339e+02 - logprior: -7.0664e-01
Fitted a model with MAP estimate = -530.9574
expansions: [(0, 2), (193, 1), (194, 1), (205, 3), (206, 2), (249, 5)]
discards: [  0 175 191 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234
 235 236 237 238 239 240 241 242 243 244 245 246 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 547.1893 - loglik: -5.4565e+02 - logprior: -1.5422e+00
Epoch 2/10
39/39 - 21s - loss: 541.4722 - loglik: -5.4117e+02 - logprior: -3.0008e-01
Epoch 3/10
39/39 - 21s - loss: 539.8626 - loglik: -5.3971e+02 - logprior: -1.4976e-01
Epoch 4/10
39/39 - 22s - loss: 538.6526 - loglik: -5.3857e+02 - logprior: -8.3837e-02
Epoch 5/10
39/39 - 23s - loss: 537.6636 - loglik: -5.3765e+02 - logprior: -9.6773e-03
Epoch 6/10
39/39 - 23s - loss: 537.5560 - loglik: -5.3761e+02 - logprior: 0.0550
Epoch 7/10
39/39 - 22s - loss: 536.9041 - loglik: -5.3702e+02 - logprior: 0.1174
Epoch 8/10
39/39 - 23s - loss: 537.8169 - loglik: -5.3801e+02 - logprior: 0.1912
Fitted a model with MAP estimate = -536.8359
Time for alignment: 580.5382
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 644.4021 - loglik: -6.4255e+02 - logprior: -1.8472e+00
Epoch 2/10
39/39 - 19s - loss: 552.2443 - loglik: -5.5078e+02 - logprior: -1.4635e+00
Epoch 3/10
39/39 - 19s - loss: 545.3104 - loglik: -5.4380e+02 - logprior: -1.5112e+00
Epoch 4/10
39/39 - 19s - loss: 543.2043 - loglik: -5.4170e+02 - logprior: -1.5015e+00
Epoch 5/10
39/39 - 19s - loss: 542.5777 - loglik: -5.4107e+02 - logprior: -1.5076e+00
Epoch 6/10
39/39 - 19s - loss: 542.3391 - loglik: -5.4082e+02 - logprior: -1.5159e+00
Epoch 7/10
39/39 - 18s - loss: 541.8846 - loglik: -5.4038e+02 - logprior: -1.5069e+00
Epoch 8/10
39/39 - 19s - loss: 542.1210 - loglik: -5.4059e+02 - logprior: -1.5269e+00
Fitted a model with MAP estimate = -541.4161
expansions: [(4, 1), (6, 1), (33, 1), (52, 1), (93, 1), (120, 1), (121, 10), (122, 3), (123, 2), (129, 1), (130, 1), (132, 5), (135, 1), (136, 1), (138, 1), (141, 3), (142, 9), (143, 1), (145, 1), (146, 1), (157, 1), (160, 5), (161, 1)]
discards: [  0 149 150 151 152 153 162 163 164 165 166 167 168 169 170 175 176 177
 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195
 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 567.3216 - loglik: -5.6479e+02 - logprior: -2.5307e+00
Epoch 2/2
39/39 - 18s - loss: 556.5554 - loglik: -5.5542e+02 - logprior: -1.1339e+00
Fitted a model with MAP estimate = -554.2707
expansions: [(4, 1), (137, 1), (205, 1), (209, 1), (212, 1), (213, 36)]
discards: [  0   1 128 129 130 131 132 133 178 179 180]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 548.9973 - loglik: -5.4661e+02 - logprior: -2.3871e+00
Epoch 2/2
39/39 - 21s - loss: 536.2542 - loglik: -5.3548e+02 - logprior: -7.7653e-01
Fitted a model with MAP estimate = -533.5138
expansions: [(0, 2), (217, 1), (243, 5)]
discards: [  0   1 172 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232
 233 234 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 548.5851 - loglik: -5.4706e+02 - logprior: -1.5240e+00
Epoch 2/10
39/39 - 17s - loss: 543.4861 - loglik: -5.4319e+02 - logprior: -2.9460e-01
Epoch 3/10
39/39 - 17s - loss: 541.9843 - loglik: -5.4182e+02 - logprior: -1.6664e-01
Epoch 4/10
39/39 - 17s - loss: 541.2121 - loglik: -5.4111e+02 - logprior: -9.8322e-02
Epoch 5/10
39/39 - 17s - loss: 540.1290 - loglik: -5.4009e+02 - logprior: -4.0670e-02
Epoch 6/10
39/39 - 17s - loss: 540.0831 - loglik: -5.4010e+02 - logprior: 0.0204
Epoch 7/10
39/39 - 17s - loss: 540.2712 - loglik: -5.4036e+02 - logprior: 0.0847
Fitted a model with MAP estimate = -539.6469
Time for alignment: 449.4147
Computed alignments with likelihoods: ['-538.2474', '-535.9120', '-533.3764', '-530.9574', '-533.5138']
Best model has likelihood: -530.9574  (prior= -0.3140 )
time for generating output: 0.2595
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.8837179972253753
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.8267 - loglik: -2.6671e+02 - logprior: -3.1164e+00
Epoch 2/10
19/19 - 2s - loss: 200.6377 - loglik: -1.9931e+02 - logprior: -1.3250e+00
Epoch 3/10
19/19 - 2s - loss: 176.9587 - loglik: -1.7531e+02 - logprior: -1.6521e+00
Epoch 4/10
19/19 - 2s - loss: 172.8336 - loglik: -1.7116e+02 - logprior: -1.6735e+00
Epoch 5/10
19/19 - 2s - loss: 171.2122 - loglik: -1.6964e+02 - logprior: -1.5721e+00
Epoch 6/10
19/19 - 2s - loss: 170.3764 - loglik: -1.6880e+02 - logprior: -1.5752e+00
Epoch 7/10
19/19 - 2s - loss: 170.2972 - loglik: -1.6875e+02 - logprior: -1.5513e+00
Epoch 8/10
19/19 - 2s - loss: 170.2864 - loglik: -1.6874e+02 - logprior: -1.5441e+00
Epoch 9/10
19/19 - 2s - loss: 169.6823 - loglik: -1.6814e+02 - logprior: -1.5469e+00
Epoch 10/10
19/19 - 2s - loss: 170.1709 - loglik: -1.6862e+02 - logprior: -1.5510e+00
Fitted a model with MAP estimate = -169.5189
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 164.9517 - loglik: -1.6200e+02 - logprior: -2.9532e+00
Epoch 2/2
19/19 - 3s - loss: 153.7865 - loglik: -1.5260e+02 - logprior: -1.1903e+00
Fitted a model with MAP estimate = -152.6088
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.3030 - loglik: -1.5339e+02 - logprior: -2.9151e+00
Epoch 2/2
19/19 - 3s - loss: 152.9349 - loglik: -1.5182e+02 - logprior: -1.1148e+00
Fitted a model with MAP estimate = -152.3680
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.3705 - loglik: -1.5546e+02 - logprior: -3.9118e+00
Epoch 2/10
19/19 - 3s - loss: 154.6043 - loglik: -1.5245e+02 - logprior: -2.1562e+00
Epoch 3/10
19/19 - 3s - loss: 153.3885 - loglik: -1.5138e+02 - logprior: -2.0052e+00
Epoch 4/10
19/19 - 3s - loss: 152.2237 - loglik: -1.5071e+02 - logprior: -1.5156e+00
Epoch 5/10
19/19 - 3s - loss: 152.1033 - loglik: -1.5111e+02 - logprior: -9.8853e-01
Epoch 6/10
19/19 - 3s - loss: 151.1977 - loglik: -1.5022e+02 - logprior: -9.7651e-01
Epoch 7/10
19/19 - 3s - loss: 151.0658 - loglik: -1.5013e+02 - logprior: -9.3668e-01
Epoch 8/10
19/19 - 3s - loss: 150.8404 - loglik: -1.4991e+02 - logprior: -9.2537e-01
Epoch 9/10
19/19 - 3s - loss: 151.1212 - loglik: -1.5022e+02 - logprior: -8.9755e-01
Fitted a model with MAP estimate = -150.7702
Time for alignment: 83.3733
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.7612 - loglik: -2.6665e+02 - logprior: -3.1124e+00
Epoch 2/10
19/19 - 2s - loss: 200.3524 - loglik: -1.9903e+02 - logprior: -1.3211e+00
Epoch 3/10
19/19 - 2s - loss: 176.3012 - loglik: -1.7465e+02 - logprior: -1.6554e+00
Epoch 4/10
19/19 - 2s - loss: 172.5151 - loglik: -1.7084e+02 - logprior: -1.6757e+00
Epoch 5/10
19/19 - 2s - loss: 171.0552 - loglik: -1.6948e+02 - logprior: -1.5726e+00
Epoch 6/10
19/19 - 2s - loss: 170.3030 - loglik: -1.6872e+02 - logprior: -1.5848e+00
Epoch 7/10
19/19 - 2s - loss: 170.2299 - loglik: -1.6867e+02 - logprior: -1.5616e+00
Epoch 8/10
19/19 - 2s - loss: 169.9621 - loglik: -1.6841e+02 - logprior: -1.5543e+00
Epoch 9/10
19/19 - 2s - loss: 169.6337 - loglik: -1.6808e+02 - logprior: -1.5510e+00
Epoch 10/10
19/19 - 2s - loss: 169.9749 - loglik: -1.6842e+02 - logprior: -1.5519e+00
Fitted a model with MAP estimate = -169.5068
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 164.8774 - loglik: -1.6192e+02 - logprior: -2.9532e+00
Epoch 2/2
19/19 - 3s - loss: 153.7444 - loglik: -1.5256e+02 - logprior: -1.1882e+00
Fitted a model with MAP estimate = -152.5239
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 156.0739 - loglik: -1.5316e+02 - logprior: -2.9127e+00
Epoch 2/2
19/19 - 3s - loss: 153.1734 - loglik: -1.5206e+02 - logprior: -1.1101e+00
Fitted a model with MAP estimate = -152.3695
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 159.5036 - loglik: -1.5559e+02 - logprior: -3.9154e+00
Epoch 2/10
19/19 - 3s - loss: 154.7046 - loglik: -1.5253e+02 - logprior: -2.1777e+00
Epoch 3/10
19/19 - 3s - loss: 153.7534 - loglik: -1.5175e+02 - logprior: -2.0045e+00
Epoch 4/10
19/19 - 3s - loss: 152.6000 - loglik: -1.5104e+02 - logprior: -1.5607e+00
Epoch 5/10
19/19 - 3s - loss: 151.7195 - loglik: -1.5073e+02 - logprior: -9.8798e-01
Epoch 6/10
19/19 - 3s - loss: 151.2798 - loglik: -1.5031e+02 - logprior: -9.7201e-01
Epoch 7/10
19/19 - 3s - loss: 150.8327 - loglik: -1.4989e+02 - logprior: -9.4232e-01
Epoch 8/10
19/19 - 3s - loss: 150.7914 - loglik: -1.4987e+02 - logprior: -9.2593e-01
Epoch 9/10
19/19 - 3s - loss: 151.1082 - loglik: -1.5021e+02 - logprior: -9.0115e-01
Fitted a model with MAP estimate = -150.8294
Time for alignment: 82.5379
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.7035 - loglik: -2.6658e+02 - logprior: -3.1198e+00
Epoch 2/10
19/19 - 2s - loss: 200.2977 - loglik: -1.9897e+02 - logprior: -1.3286e+00
Epoch 3/10
19/19 - 2s - loss: 176.6708 - loglik: -1.7500e+02 - logprior: -1.6756e+00
Epoch 4/10
19/19 - 2s - loss: 173.0620 - loglik: -1.7141e+02 - logprior: -1.6530e+00
Epoch 5/10
19/19 - 2s - loss: 172.1169 - loglik: -1.7054e+02 - logprior: -1.5766e+00
Epoch 6/10
19/19 - 2s - loss: 171.8934 - loglik: -1.7033e+02 - logprior: -1.5605e+00
Epoch 7/10
19/19 - 2s - loss: 171.1693 - loglik: -1.6963e+02 - logprior: -1.5376e+00
Epoch 8/10
19/19 - 2s - loss: 170.7787 - loglik: -1.6924e+02 - logprior: -1.5358e+00
Epoch 9/10
19/19 - 2s - loss: 171.3920 - loglik: -1.6987e+02 - logprior: -1.5249e+00
Fitted a model with MAP estimate = -170.7953
expansions: [(5, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 169.2733 - loglik: -1.6534e+02 - logprior: -3.9319e+00
Epoch 2/2
19/19 - 3s - loss: 157.0406 - loglik: -1.5508e+02 - logprior: -1.9622e+00
Fitted a model with MAP estimate = -154.9836
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 157.0505 - loglik: -1.5414e+02 - logprior: -2.9102e+00
Epoch 2/2
19/19 - 3s - loss: 152.9876 - loglik: -1.5187e+02 - logprior: -1.1128e+00
Fitted a model with MAP estimate = -152.0243
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 158.8978 - loglik: -1.5505e+02 - logprior: -3.8471e+00
Epoch 2/10
19/19 - 3s - loss: 154.1007 - loglik: -1.5267e+02 - logprior: -1.4333e+00
Epoch 3/10
19/19 - 3s - loss: 152.2032 - loglik: -1.5124e+02 - logprior: -9.6304e-01
Epoch 4/10
19/19 - 3s - loss: 151.4195 - loglik: -1.5047e+02 - logprior: -9.4753e-01
Epoch 5/10
19/19 - 3s - loss: 150.6942 - loglik: -1.4979e+02 - logprior: -9.0676e-01
Epoch 6/10
19/19 - 3s - loss: 150.9159 - loglik: -1.5002e+02 - logprior: -8.9677e-01
Fitted a model with MAP estimate = -150.3704
Time for alignment: 71.9181
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.6778 - loglik: -2.6656e+02 - logprior: -3.1145e+00
Epoch 2/10
19/19 - 2s - loss: 200.4056 - loglik: -1.9907e+02 - logprior: -1.3381e+00
Epoch 3/10
19/19 - 2s - loss: 177.1098 - loglik: -1.7541e+02 - logprior: -1.6999e+00
Epoch 4/10
19/19 - 2s - loss: 173.0254 - loglik: -1.7138e+02 - logprior: -1.6456e+00
Epoch 5/10
19/19 - 2s - loss: 171.9690 - loglik: -1.7040e+02 - logprior: -1.5654e+00
Epoch 6/10
19/19 - 2s - loss: 171.5648 - loglik: -1.7002e+02 - logprior: -1.5462e+00
Epoch 7/10
19/19 - 2s - loss: 171.0107 - loglik: -1.6947e+02 - logprior: -1.5409e+00
Epoch 8/10
19/19 - 2s - loss: 170.6819 - loglik: -1.6914e+02 - logprior: -1.5390e+00
Epoch 9/10
19/19 - 2s - loss: 170.5887 - loglik: -1.6905e+02 - logprior: -1.5404e+00
Epoch 10/10
19/19 - 2s - loss: 170.3868 - loglik: -1.6885e+02 - logprior: -1.5367e+00
Fitted a model with MAP estimate = -170.2474
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 170.0817 - loglik: -1.6614e+02 - logprior: -3.9380e+00
Epoch 2/2
19/19 - 3s - loss: 156.9693 - loglik: -1.5496e+02 - logprior: -2.0102e+00
Fitted a model with MAP estimate = -155.0630
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.9854 - loglik: -1.5406e+02 - logprior: -2.9296e+00
Epoch 2/2
19/19 - 3s - loss: 152.7345 - loglik: -1.5161e+02 - logprior: -1.1217e+00
Fitted a model with MAP estimate = -151.8656
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 159.0821 - loglik: -1.5524e+02 - logprior: -3.8418e+00
Epoch 2/10
19/19 - 3s - loss: 154.1744 - loglik: -1.5275e+02 - logprior: -1.4230e+00
Epoch 3/10
19/19 - 3s - loss: 152.4079 - loglik: -1.5144e+02 - logprior: -9.6611e-01
Epoch 4/10
19/19 - 3s - loss: 151.4296 - loglik: -1.5049e+02 - logprior: -9.4003e-01
Epoch 5/10
19/19 - 3s - loss: 150.9095 - loglik: -1.4999e+02 - logprior: -9.1564e-01
Epoch 6/10
19/19 - 3s - loss: 150.7324 - loglik: -1.4983e+02 - logprior: -9.0208e-01
Epoch 7/10
19/19 - 3s - loss: 150.4396 - loglik: -1.4955e+02 - logprior: -8.8555e-01
Epoch 8/10
19/19 - 3s - loss: 150.1885 - loglik: -1.4932e+02 - logprior: -8.6568e-01
Epoch 9/10
19/19 - 3s - loss: 150.0537 - loglik: -1.4921e+02 - logprior: -8.4306e-01
Epoch 10/10
19/19 - 3s - loss: 149.9840 - loglik: -1.4915e+02 - logprior: -8.3139e-01
Fitted a model with MAP estimate = -149.9907
Time for alignment: 85.1134
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.7789 - loglik: -2.6666e+02 - logprior: -3.1144e+00
Epoch 2/10
19/19 - 2s - loss: 201.0156 - loglik: -1.9969e+02 - logprior: -1.3228e+00
Epoch 3/10
19/19 - 2s - loss: 177.4477 - loglik: -1.7577e+02 - logprior: -1.6734e+00
Epoch 4/10
19/19 - 2s - loss: 173.3770 - loglik: -1.7173e+02 - logprior: -1.6435e+00
Epoch 5/10
19/19 - 2s - loss: 172.1165 - loglik: -1.7056e+02 - logprior: -1.5605e+00
Epoch 6/10
19/19 - 2s - loss: 171.3901 - loglik: -1.6985e+02 - logprior: -1.5438e+00
Epoch 7/10
19/19 - 2s - loss: 171.2938 - loglik: -1.6976e+02 - logprior: -1.5356e+00
Epoch 8/10
19/19 - 2s - loss: 170.3815 - loglik: -1.6884e+02 - logprior: -1.5409e+00
Epoch 9/10
19/19 - 2s - loss: 170.5111 - loglik: -1.6897e+02 - logprior: -1.5415e+00
Fitted a model with MAP estimate = -170.3097
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 169.9881 - loglik: -1.6605e+02 - logprior: -3.9411e+00
Epoch 2/2
19/19 - 3s - loss: 156.9237 - loglik: -1.5493e+02 - logprior: -1.9889e+00
Fitted a model with MAP estimate = -154.9493
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 157.0729 - loglik: -1.5414e+02 - logprior: -2.9288e+00
Epoch 2/2
19/19 - 3s - loss: 152.6679 - loglik: -1.5154e+02 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -151.7900
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.0045 - loglik: -1.5519e+02 - logprior: -3.8126e+00
Epoch 2/10
19/19 - 3s - loss: 154.3398 - loglik: -1.5294e+02 - logprior: -1.4046e+00
Epoch 3/10
19/19 - 3s - loss: 152.3004 - loglik: -1.5132e+02 - logprior: -9.8523e-01
Epoch 4/10
19/19 - 3s - loss: 151.3732 - loglik: -1.5041e+02 - logprior: -9.5968e-01
Epoch 5/10
19/19 - 3s - loss: 151.4717 - loglik: -1.5056e+02 - logprior: -9.1533e-01
Fitted a model with MAP estimate = -150.8417
Time for alignment: 70.1407
Computed alignments with likelihoods: ['-150.7702', '-150.8294', '-150.3704', '-149.9907', '-150.8417']
Best model has likelihood: -149.9907  (prior= -0.8432 )
time for generating output: 0.1245
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9231205673758865
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.9794 - loglik: -2.2886e+02 - logprior: -3.1177e+00
Epoch 2/10
19/19 - 1s - loss: 196.8453 - loglik: -1.9557e+02 - logprior: -1.2739e+00
Epoch 3/10
19/19 - 1s - loss: 185.2747 - loglik: -1.8399e+02 - logprior: -1.2814e+00
Epoch 4/10
19/19 - 1s - loss: 182.1791 - loglik: -1.8089e+02 - logprior: -1.2888e+00
Epoch 5/10
19/19 - 1s - loss: 180.6913 - loglik: -1.7941e+02 - logprior: -1.2805e+00
Epoch 6/10
19/19 - 1s - loss: 180.1066 - loglik: -1.7884e+02 - logprior: -1.2715e+00
Epoch 7/10
19/19 - 1s - loss: 180.0051 - loglik: -1.7875e+02 - logprior: -1.2589e+00
Epoch 8/10
19/19 - 1s - loss: 179.7854 - loglik: -1.7853e+02 - logprior: -1.2566e+00
Epoch 9/10
19/19 - 1s - loss: 179.7728 - loglik: -1.7852e+02 - logprior: -1.2491e+00
Epoch 10/10
19/19 - 1s - loss: 179.4955 - loglik: -1.7825e+02 - logprior: -1.2468e+00
Fitted a model with MAP estimate = -179.5303
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (34, 1), (35, 2), (37, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 182.9554 - loglik: -1.7903e+02 - logprior: -3.9268e+00
Epoch 2/2
19/19 - 1s - loss: 176.0268 - loglik: -1.7399e+02 - logprior: -2.0329e+00
Fitted a model with MAP estimate = -174.7527
expansions: [(0, 2)]
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 176.6410 - loglik: -1.7372e+02 - logprior: -2.9188e+00
Epoch 2/2
19/19 - 1s - loss: 173.6658 - loglik: -1.7249e+02 - logprior: -1.1738e+00
Fitted a model with MAP estimate = -173.0029
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.7655 - loglik: -1.7406e+02 - logprior: -3.7024e+00
Epoch 2/10
19/19 - 1s - loss: 173.8652 - loglik: -1.7264e+02 - logprior: -1.2224e+00
Epoch 3/10
19/19 - 1s - loss: 173.0136 - loglik: -1.7200e+02 - logprior: -1.0139e+00
Epoch 4/10
19/19 - 1s - loss: 172.2060 - loglik: -1.7123e+02 - logprior: -9.7637e-01
Epoch 5/10
19/19 - 1s - loss: 171.6928 - loglik: -1.7075e+02 - logprior: -9.4208e-01
Epoch 6/10
19/19 - 1s - loss: 171.5473 - loglik: -1.7061e+02 - logprior: -9.3834e-01
Epoch 7/10
19/19 - 1s - loss: 171.2211 - loglik: -1.7030e+02 - logprior: -9.1866e-01
Epoch 8/10
19/19 - 1s - loss: 170.9030 - loglik: -1.6999e+02 - logprior: -9.1010e-01
Epoch 9/10
19/19 - 1s - loss: 170.8413 - loglik: -1.6994e+02 - logprior: -8.9707e-01
Epoch 10/10
19/19 - 1s - loss: 170.6856 - loglik: -1.6981e+02 - logprior: -8.7575e-01
Fitted a model with MAP estimate = -170.8088
Time for alignment: 56.2922
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.0610 - loglik: -2.2894e+02 - logprior: -3.1179e+00
Epoch 2/10
19/19 - 1s - loss: 198.3950 - loglik: -1.9712e+02 - logprior: -1.2751e+00
Epoch 3/10
19/19 - 1s - loss: 186.2854 - loglik: -1.8501e+02 - logprior: -1.2791e+00
Epoch 4/10
19/19 - 1s - loss: 181.7630 - loglik: -1.8043e+02 - logprior: -1.3328e+00
Epoch 5/10
19/19 - 1s - loss: 180.3837 - loglik: -1.7908e+02 - logprior: -1.3046e+00
Epoch 6/10
19/19 - 1s - loss: 179.3881 - loglik: -1.7810e+02 - logprior: -1.2927e+00
Epoch 7/10
19/19 - 1s - loss: 180.1226 - loglik: -1.7884e+02 - logprior: -1.2848e+00
Fitted a model with MAP estimate = -179.4950
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 1), (35, 2), (37, 1), (49, 1), (50, 1), (53, 1), (56, 2), (57, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 182.1919 - loglik: -1.7830e+02 - logprior: -3.8907e+00
Epoch 2/2
19/19 - 1s - loss: 175.1095 - loglik: -1.7315e+02 - logprior: -1.9565e+00
Fitted a model with MAP estimate = -173.8404
expansions: [(0, 2), (10, 1)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 176.3294 - loglik: -1.7346e+02 - logprior: -2.8697e+00
Epoch 2/2
19/19 - 1s - loss: 173.1001 - loglik: -1.7199e+02 - logprior: -1.1062e+00
Fitted a model with MAP estimate = -172.3865
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 177.0118 - loglik: -1.7340e+02 - logprior: -3.6141e+00
Epoch 2/10
19/19 - 1s - loss: 173.4185 - loglik: -1.7227e+02 - logprior: -1.1441e+00
Epoch 3/10
19/19 - 1s - loss: 172.3034 - loglik: -1.7135e+02 - logprior: -9.5167e-01
Epoch 4/10
19/19 - 1s - loss: 171.6137 - loglik: -1.7070e+02 - logprior: -9.1235e-01
Epoch 5/10
19/19 - 1s - loss: 171.1920 - loglik: -1.7030e+02 - logprior: -8.8706e-01
Epoch 6/10
19/19 - 1s - loss: 170.5004 - loglik: -1.6963e+02 - logprior: -8.6767e-01
Epoch 7/10
19/19 - 1s - loss: 170.4473 - loglik: -1.6959e+02 - logprior: -8.5982e-01
Epoch 8/10
19/19 - 1s - loss: 170.6007 - loglik: -1.6974e+02 - logprior: -8.5643e-01
Fitted a model with MAP estimate = -170.2178
Time for alignment: 48.1638
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.5826 - loglik: -2.2846e+02 - logprior: -3.1187e+00
Epoch 2/10
19/19 - 1s - loss: 196.4949 - loglik: -1.9523e+02 - logprior: -1.2692e+00
Epoch 3/10
19/19 - 1s - loss: 184.6696 - loglik: -1.8339e+02 - logprior: -1.2795e+00
Epoch 4/10
19/19 - 1s - loss: 181.7134 - loglik: -1.8042e+02 - logprior: -1.2956e+00
Epoch 5/10
19/19 - 1s - loss: 180.4923 - loglik: -1.7923e+02 - logprior: -1.2593e+00
Epoch 6/10
19/19 - 1s - loss: 179.7642 - loglik: -1.7852e+02 - logprior: -1.2445e+00
Epoch 7/10
19/19 - 1s - loss: 179.2297 - loglik: -1.7800e+02 - logprior: -1.2288e+00
Epoch 8/10
19/19 - 1s - loss: 179.8177 - loglik: -1.7859e+02 - logprior: -1.2298e+00
Fitted a model with MAP estimate = -179.3704
expansions: [(7, 2), (8, 2), (9, 2), (21, 1), (32, 2), (33, 1), (34, 2), (35, 1), (47, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.7519 - loglik: -1.7982e+02 - logprior: -3.9322e+00
Epoch 2/2
19/19 - 1s - loss: 176.4265 - loglik: -1.7438e+02 - logprior: -2.0501e+00
Fitted a model with MAP estimate = -174.9711
expansions: [(0, 2)]
discards: [ 0 39 43 65 70]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.1005 - loglik: -1.7420e+02 - logprior: -2.8998e+00
Epoch 2/2
19/19 - 1s - loss: 173.7366 - loglik: -1.7258e+02 - logprior: -1.1580e+00
Fitted a model with MAP estimate = -172.8940
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 177.5335 - loglik: -1.7380e+02 - logprior: -3.7356e+00
Epoch 2/10
19/19 - 1s - loss: 173.9467 - loglik: -1.7271e+02 - logprior: -1.2357e+00
Epoch 3/10
19/19 - 1s - loss: 172.5188 - loglik: -1.7152e+02 - logprior: -1.0029e+00
Epoch 4/10
19/19 - 1s - loss: 172.0583 - loglik: -1.7110e+02 - logprior: -9.5433e-01
Epoch 5/10
19/19 - 1s - loss: 171.6065 - loglik: -1.7068e+02 - logprior: -9.2795e-01
Epoch 6/10
19/19 - 1s - loss: 171.2304 - loglik: -1.7030e+02 - logprior: -9.2542e-01
Epoch 7/10
19/19 - 1s - loss: 170.6769 - loglik: -1.6977e+02 - logprior: -9.0924e-01
Epoch 8/10
19/19 - 1s - loss: 170.3476 - loglik: -1.6945e+02 - logprior: -9.0088e-01
Epoch 9/10
19/19 - 1s - loss: 170.7260 - loglik: -1.6984e+02 - logprior: -8.8384e-01
Fitted a model with MAP estimate = -170.5371
Time for alignment: 51.8103
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.6542 - loglik: -2.2853e+02 - logprior: -3.1210e+00
Epoch 2/10
19/19 - 1s - loss: 196.7898 - loglik: -1.9553e+02 - logprior: -1.2581e+00
Epoch 3/10
19/19 - 1s - loss: 186.3796 - loglik: -1.8513e+02 - logprior: -1.2523e+00
Epoch 4/10
19/19 - 1s - loss: 182.6965 - loglik: -1.8141e+02 - logprior: -1.2821e+00
Epoch 5/10
19/19 - 1s - loss: 180.7509 - loglik: -1.7949e+02 - logprior: -1.2576e+00
Epoch 6/10
19/19 - 1s - loss: 180.3181 - loglik: -1.7906e+02 - logprior: -1.2602e+00
Epoch 7/10
19/19 - 1s - loss: 180.0688 - loglik: -1.7883e+02 - logprior: -1.2391e+00
Epoch 8/10
19/19 - 1s - loss: 179.8924 - loglik: -1.7865e+02 - logprior: -1.2377e+00
Epoch 9/10
19/19 - 1s - loss: 179.9488 - loglik: -1.7872e+02 - logprior: -1.2286e+00
Fitted a model with MAP estimate = -179.7320
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (35, 2), (37, 1), (47, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 182.6577 - loglik: -1.7875e+02 - logprior: -3.9084e+00
Epoch 2/2
19/19 - 1s - loss: 176.0725 - loglik: -1.7408e+02 - logprior: -1.9964e+00
Fitted a model with MAP estimate = -174.7301
expansions: [(0, 2), (10, 1)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.0917 - loglik: -1.7422e+02 - logprior: -2.8745e+00
Epoch 2/2
19/19 - 1s - loss: 173.8926 - loglik: -1.7277e+02 - logprior: -1.1210e+00
Fitted a model with MAP estimate = -173.2176
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 177.8861 - loglik: -1.7425e+02 - logprior: -3.6349e+00
Epoch 2/10
19/19 - 1s - loss: 174.1699 - loglik: -1.7301e+02 - logprior: -1.1589e+00
Epoch 3/10
19/19 - 1s - loss: 172.9375 - loglik: -1.7198e+02 - logprior: -9.5406e-01
Epoch 4/10
19/19 - 1s - loss: 172.5884 - loglik: -1.7167e+02 - logprior: -9.1417e-01
Epoch 5/10
19/19 - 1s - loss: 171.9754 - loglik: -1.7109e+02 - logprior: -8.8082e-01
Epoch 6/10
19/19 - 1s - loss: 171.1866 - loglik: -1.7032e+02 - logprior: -8.6824e-01
Epoch 7/10
19/19 - 1s - loss: 171.0984 - loglik: -1.7025e+02 - logprior: -8.5291e-01
Epoch 8/10
19/19 - 1s - loss: 170.9167 - loglik: -1.7007e+02 - logprior: -8.4249e-01
Epoch 9/10
19/19 - 1s - loss: 171.0699 - loglik: -1.7024e+02 - logprior: -8.2841e-01
Fitted a model with MAP estimate = -170.9459
Time for alignment: 53.2853
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.7513 - loglik: -2.2863e+02 - logprior: -3.1204e+00
Epoch 2/10
19/19 - 1s - loss: 196.5882 - loglik: -1.9531e+02 - logprior: -1.2825e+00
Epoch 3/10
19/19 - 1s - loss: 184.4995 - loglik: -1.8322e+02 - logprior: -1.2753e+00
Epoch 4/10
19/19 - 1s - loss: 181.6776 - loglik: -1.8039e+02 - logprior: -1.2857e+00
Epoch 5/10
19/19 - 1s - loss: 180.4410 - loglik: -1.7919e+02 - logprior: -1.2485e+00
Epoch 6/10
19/19 - 1s - loss: 179.9582 - loglik: -1.7872e+02 - logprior: -1.2351e+00
Epoch 7/10
19/19 - 1s - loss: 179.7776 - loglik: -1.7856e+02 - logprior: -1.2160e+00
Epoch 8/10
19/19 - 1s - loss: 179.6848 - loglik: -1.7846e+02 - logprior: -1.2236e+00
Epoch 9/10
19/19 - 1s - loss: 179.6241 - loglik: -1.7841e+02 - logprior: -1.2170e+00
Epoch 10/10
19/19 - 1s - loss: 179.2828 - loglik: -1.7806e+02 - logprior: -1.2207e+00
Fitted a model with MAP estimate = -179.3747
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (26, 2), (31, 2), (34, 1), (35, 2), (37, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.9484 - loglik: -1.8000e+02 - logprior: -3.9438e+00
Epoch 2/2
19/19 - 1s - loss: 176.1516 - loglik: -1.7406e+02 - logprior: -2.0941e+00
Fitted a model with MAP estimate = -174.6832
expansions: [(0, 2)]
discards: [ 0 34 41 46 67 71 76]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 177.5177 - loglik: -1.7462e+02 - logprior: -2.8978e+00
Epoch 2/2
19/19 - 1s - loss: 174.0441 - loglik: -1.7291e+02 - logprior: -1.1378e+00
Fitted a model with MAP estimate = -173.3582
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.5922 - loglik: -1.7493e+02 - logprior: -3.6577e+00
Epoch 2/10
19/19 - 1s - loss: 174.5655 - loglik: -1.7340e+02 - logprior: -1.1698e+00
Epoch 3/10
19/19 - 1s - loss: 173.6990 - loglik: -1.7274e+02 - logprior: -9.5902e-01
Epoch 4/10
19/19 - 1s - loss: 173.0587 - loglik: -1.7214e+02 - logprior: -9.2260e-01
Epoch 5/10
19/19 - 1s - loss: 172.3339 - loglik: -1.7144e+02 - logprior: -8.9855e-01
Epoch 6/10
19/19 - 1s - loss: 172.1652 - loglik: -1.7128e+02 - logprior: -8.8374e-01
Epoch 7/10
19/19 - 1s - loss: 171.5858 - loglik: -1.7070e+02 - logprior: -8.8086e-01
Epoch 8/10
19/19 - 1s - loss: 171.9679 - loglik: -1.7110e+02 - logprior: -8.6546e-01
Fitted a model with MAP estimate = -171.5648
Time for alignment: 53.0089
Computed alignments with likelihoods: ['-170.8088', '-170.2178', '-170.5371', '-170.9459', '-171.5648']
Best model has likelihood: -170.2178  (prior= -0.8202 )
time for generating output: 0.1167
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.7795918367346939
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 644.3644 - loglik: -6.4263e+02 - logprior: -1.7342e+00
Epoch 2/10
39/39 - 15s - loss: 594.8320 - loglik: -5.9373e+02 - logprior: -1.0971e+00
Epoch 3/10
39/39 - 15s - loss: 589.2424 - loglik: -5.8811e+02 - logprior: -1.1353e+00
Epoch 4/10
39/39 - 16s - loss: 585.1475 - loglik: -5.8404e+02 - logprior: -1.1108e+00
Epoch 5/10
39/39 - 16s - loss: 581.1982 - loglik: -5.8006e+02 - logprior: -1.1341e+00
Epoch 6/10
39/39 - 16s - loss: 578.4468 - loglik: -5.7729e+02 - logprior: -1.1538e+00
Epoch 7/10
39/39 - 16s - loss: 577.3350 - loglik: -5.7615e+02 - logprior: -1.1841e+00
Epoch 8/10
39/39 - 16s - loss: 576.8140 - loglik: -5.7563e+02 - logprior: -1.1845e+00
Epoch 9/10
39/39 - 16s - loss: 576.1581 - loglik: -5.7495e+02 - logprior: -1.2041e+00
Epoch 10/10
39/39 - 16s - loss: 576.0651 - loglik: -5.7485e+02 - logprior: -1.2159e+00
Fitted a model with MAP estimate = -575.0160
expansions: [(23, 1), (24, 6), (28, 1), (49, 5), (63, 2), (64, 2), (66, 1), (82, 1), (83, 1), (84, 2), (85, 2), (97, 1), (105, 1), (106, 3), (109, 1), (110, 1), (121, 1), (124, 1), (126, 1), (127, 1), (135, 1), (145, 1), (150, 1), (156, 3), (160, 1), (161, 1), (162, 1), (164, 1), (167, 1), (170, 1), (176, 2), (181, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 612.4621 - loglik: -6.1039e+02 - logprior: -2.0742e+00
Epoch 2/2
39/39 - 22s - loss: 584.7755 - loglik: -5.8419e+02 - logprior: -5.8956e-01
Fitted a model with MAP estimate = -581.9241
expansions: [(32, 2), (54, 1), (241, 10)]
discards: [ 26  27  28  29  55  61  77  80 105 196 224 225 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 587.9755 - loglik: -5.8636e+02 - logprior: -1.6132e+00
Epoch 2/2
39/39 - 22s - loss: 583.3528 - loglik: -5.8290e+02 - logprior: -4.4895e-01
Fitted a model with MAP estimate = -580.9496
expansions: [(26, 3), (29, 1), (186, 1)]
discards: [ 33  34  35  52 124 230 231 232 233 234 235 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 587.1694 - loglik: -5.8576e+02 - logprior: -1.4108e+00
Epoch 2/10
39/39 - 21s - loss: 583.4369 - loglik: -5.8334e+02 - logprior: -1.0089e-01
Epoch 3/10
39/39 - 21s - loss: 580.8761 - loglik: -5.8089e+02 - logprior: 0.0094
Epoch 4/10
39/39 - 21s - loss: 576.4865 - loglik: -5.7656e+02 - logprior: 0.0778
Epoch 5/10
39/39 - 21s - loss: 571.0896 - loglik: -5.7125e+02 - logprior: 0.1648
Epoch 6/10
39/39 - 21s - loss: 567.9907 - loglik: -5.6827e+02 - logprior: 0.2808
Epoch 7/10
39/39 - 21s - loss: 565.2168 - loglik: -5.6549e+02 - logprior: 0.2719
Epoch 8/10
39/39 - 21s - loss: 564.0675 - loglik: -5.6435e+02 - logprior: 0.2873
Epoch 9/10
39/39 - 20s - loss: 563.4025 - loglik: -5.6370e+02 - logprior: 0.2965
Epoch 10/10
39/39 - 20s - loss: 562.5916 - loglik: -5.6291e+02 - logprior: 0.3151
Fitted a model with MAP estimate = -561.9067
Time for alignment: 555.1996
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 643.6275 - loglik: -6.4188e+02 - logprior: -1.7427e+00
Epoch 2/10
39/39 - 16s - loss: 594.7186 - loglik: -5.9364e+02 - logprior: -1.0818e+00
Epoch 3/10
39/39 - 16s - loss: 588.8508 - loglik: -5.8772e+02 - logprior: -1.1312e+00
Epoch 4/10
39/39 - 16s - loss: 584.8361 - loglik: -5.8370e+02 - logprior: -1.1324e+00
Epoch 5/10
39/39 - 16s - loss: 580.4456 - loglik: -5.7928e+02 - logprior: -1.1636e+00
Epoch 6/10
39/39 - 16s - loss: 577.7112 - loglik: -5.7652e+02 - logprior: -1.1876e+00
Epoch 7/10
39/39 - 16s - loss: 576.6508 - loglik: -5.7544e+02 - logprior: -1.2120e+00
Epoch 8/10
39/39 - 16s - loss: 576.3745 - loglik: -5.7515e+02 - logprior: -1.2252e+00
Epoch 9/10
39/39 - 16s - loss: 575.7685 - loglik: -5.7454e+02 - logprior: -1.2322e+00
Epoch 10/10
39/39 - 16s - loss: 575.1258 - loglik: -5.7388e+02 - logprior: -1.2434e+00
Fitted a model with MAP estimate = -574.4814
expansions: [(23, 1), (24, 6), (28, 1), (47, 1), (48, 4), (60, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 1), (84, 1), (97, 1), (105, 1), (106, 3), (109, 1), (110, 1), (124, 1), (125, 1), (127, 1), (136, 1), (156, 1), (157, 4), (161, 1), (162, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (176, 3), (181, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 612.4080 - loglik: -6.1041e+02 - logprior: -1.9987e+00
Epoch 2/2
39/39 - 22s - loss: 584.6115 - loglik: -5.8408e+02 - logprior: -5.3086e-01
Fitted a model with MAP estimate = -581.9035
expansions: [(32, 3), (184, 1), (241, 10)]
discards: [ 26  27  28  29  57 194 195 224 225 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 587.6790 - loglik: -5.8606e+02 - logprior: -1.6197e+00
Epoch 2/2
39/39 - 22s - loss: 583.1480 - loglik: -5.8270e+02 - logprior: -4.4723e-01
Fitted a model with MAP estimate = -580.7643
expansions: [(26, 3), (53, 2)]
discards: [ 34  35  36  54  76 127 219 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 587.0155 - loglik: -5.8558e+02 - logprior: -1.4345e+00
Epoch 2/10
39/39 - 21s - loss: 583.3976 - loglik: -5.8324e+02 - logprior: -1.6095e-01
Epoch 3/10
39/39 - 21s - loss: 580.6279 - loglik: -5.8047e+02 - logprior: -1.5391e-01
Epoch 4/10
39/39 - 21s - loss: 576.1133 - loglik: -5.7599e+02 - logprior: -1.2109e-01
Epoch 5/10
39/39 - 21s - loss: 571.0118 - loglik: -5.7093e+02 - logprior: -8.2828e-02
Epoch 6/10
39/39 - 21s - loss: 567.5775 - loglik: -5.6751e+02 - logprior: -6.9417e-02
Epoch 7/10
39/39 - 21s - loss: 565.1911 - loglik: -5.6514e+02 - logprior: -4.6250e-02
Epoch 8/10
39/39 - 21s - loss: 564.5272 - loglik: -5.6449e+02 - logprior: -3.6741e-02
Epoch 9/10
39/39 - 21s - loss: 563.4764 - loglik: -5.6346e+02 - logprior: -1.4854e-02
Epoch 10/10
39/39 - 21s - loss: 562.6061 - loglik: -5.6262e+02 - logprior: 0.0137
Fitted a model with MAP estimate = -562.0787
Time for alignment: 551.2970
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 643.8783 - loglik: -6.4214e+02 - logprior: -1.7409e+00
Epoch 2/10
39/39 - 16s - loss: 595.8480 - loglik: -5.9479e+02 - logprior: -1.0613e+00
Epoch 3/10
39/39 - 16s - loss: 589.0968 - loglik: -5.8803e+02 - logprior: -1.0715e+00
Epoch 4/10
39/39 - 16s - loss: 585.1206 - loglik: -5.8406e+02 - logprior: -1.0647e+00
Epoch 5/10
39/39 - 16s - loss: 580.3124 - loglik: -5.7921e+02 - logprior: -1.1049e+00
Epoch 6/10
39/39 - 16s - loss: 578.3607 - loglik: -5.7724e+02 - logprior: -1.1255e+00
Epoch 7/10
39/39 - 16s - loss: 577.1663 - loglik: -5.7600e+02 - logprior: -1.1657e+00
Epoch 8/10
39/39 - 16s - loss: 576.2828 - loglik: -5.7510e+02 - logprior: -1.1817e+00
Epoch 9/10
39/39 - 16s - loss: 575.9657 - loglik: -5.7475e+02 - logprior: -1.2137e+00
Epoch 10/10
39/39 - 16s - loss: 575.5757 - loglik: -5.7435e+02 - logprior: -1.2218e+00
Fitted a model with MAP estimate = -574.7768
expansions: [(23, 1), (24, 6), (28, 1), (47, 1), (48, 5), (49, 1), (56, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (96, 1), (104, 2), (105, 2), (108, 1), (109, 1), (124, 1), (126, 1), (132, 1), (135, 1), (148, 1), (150, 1), (156, 3), (161, 1), (163, 1), (167, 1), (168, 1), (169, 1), (170, 1), (176, 2), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 612.1354 - loglik: -6.1011e+02 - logprior: -2.0264e+00
Epoch 2/2
39/39 - 21s - loss: 584.4125 - loglik: -5.8385e+02 - logprior: -5.6364e-01
Fitted a model with MAP estimate = -581.7488
expansions: [(32, 2), (243, 11)]
discards: [ 26  27  28  29  79  82 107 133 197 198 225 226 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 587.9027 - loglik: -5.8630e+02 - logprior: -1.6019e+00
Epoch 2/2
39/39 - 21s - loss: 583.3745 - loglik: -5.8293e+02 - logprior: -4.4101e-01
Fitted a model with MAP estimate = -580.8352
expansions: [(26, 3), (29, 1), (188, 1), (190, 1), (216, 1)]
discards: [ 33  34  53  55 231 232 233 234 235 236 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 586.9190 - loglik: -5.8554e+02 - logprior: -1.3817e+00
Epoch 2/10
39/39 - 20s - loss: 583.2682 - loglik: -5.8319e+02 - logprior: -7.7907e-02
Epoch 3/10
39/39 - 21s - loss: 580.3716 - loglik: -5.8039e+02 - logprior: 0.0233
Epoch 4/10
39/39 - 21s - loss: 576.4202 - loglik: -5.7651e+02 - logprior: 0.0889
Epoch 5/10
39/39 - 22s - loss: 570.9222 - loglik: -5.7105e+02 - logprior: 0.1289
Epoch 6/10
39/39 - 23s - loss: 566.6541 - loglik: -5.6690e+02 - logprior: 0.2454
Epoch 7/10
39/39 - 24s - loss: 565.0580 - loglik: -5.6532e+02 - logprior: 0.2660
Epoch 8/10
39/39 - 24s - loss: 563.6039 - loglik: -5.6387e+02 - logprior: 0.2698
Epoch 9/10
39/39 - 24s - loss: 562.7324 - loglik: -5.6304e+02 - logprior: 0.3029
Epoch 10/10
39/39 - 25s - loss: 561.7415 - loglik: -5.6206e+02 - logprior: 0.3187
Fitted a model with MAP estimate = -561.2549
Time for alignment: 564.2483
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 644.0297 - loglik: -6.4229e+02 - logprior: -1.7353e+00
Epoch 2/10
39/39 - 17s - loss: 595.1614 - loglik: -5.9414e+02 - logprior: -1.0193e+00
Epoch 3/10
39/39 - 17s - loss: 588.8655 - loglik: -5.8783e+02 - logprior: -1.0386e+00
Epoch 4/10
39/39 - 17s - loss: 585.3508 - loglik: -5.8431e+02 - logprior: -1.0453e+00
Epoch 5/10
39/39 - 17s - loss: 580.8062 - loglik: -5.7973e+02 - logprior: -1.0732e+00
Epoch 6/10
39/39 - 17s - loss: 578.2923 - loglik: -5.7719e+02 - logprior: -1.1026e+00
Epoch 7/10
39/39 - 17s - loss: 577.4124 - loglik: -5.7627e+02 - logprior: -1.1458e+00
Epoch 8/10
39/39 - 18s - loss: 576.8534 - loglik: -5.7569e+02 - logprior: -1.1642e+00
Epoch 9/10
39/39 - 18s - loss: 576.4888 - loglik: -5.7530e+02 - logprior: -1.1885e+00
Epoch 10/10
39/39 - 18s - loss: 575.6963 - loglik: -5.7451e+02 - logprior: -1.1905e+00
Fitted a model with MAP estimate = -575.3097
expansions: [(23, 1), (24, 4), (27, 1), (29, 1), (48, 5), (50, 1), (54, 1), (60, 1), (62, 2), (63, 2), (65, 1), (81, 2), (82, 3), (83, 2), (103, 1), (104, 4), (105, 2), (107, 1), (108, 1), (122, 1), (123, 1), (125, 1), (134, 1), (147, 1), (149, 1), (154, 1), (155, 2), (160, 1), (162, 1), (166, 1), (167, 1), (169, 1), (176, 2), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 611.2621 - loglik: -6.0928e+02 - logprior: -1.9835e+00
Epoch 2/2
39/39 - 23s - loss: 584.0020 - loglik: -5.8343e+02 - logprior: -5.6830e-01
Fitted a model with MAP estimate = -581.4623
expansions: [(198, 1), (244, 10)]
discards: [ 25  26  27  81 101 134 135 199 227 228 241 242 243]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 587.7247 - loglik: -5.8612e+02 - logprior: -1.6002e+00
Epoch 2/2
39/39 - 25s - loss: 583.5984 - loglik: -5.8317e+02 - logprior: -4.3211e-01
Fitted a model with MAP estimate = -580.9650
expansions: [(218, 1)]
discards: [ 53  56  73 125 233 234 235 236 237 238 239 240 241]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 587.1146 - loglik: -5.8572e+02 - logprior: -1.3974e+00
Epoch 2/10
39/39 - 22s - loss: 583.7228 - loglik: -5.8364e+02 - logprior: -8.0944e-02
Epoch 3/10
39/39 - 21s - loss: 581.2803 - loglik: -5.8131e+02 - logprior: 0.0331
Epoch 4/10
39/39 - 22s - loss: 576.7367 - loglik: -5.7682e+02 - logprior: 0.0860
Epoch 5/10
39/39 - 22s - loss: 571.7496 - loglik: -5.7192e+02 - logprior: 0.1712
Epoch 6/10
39/39 - 23s - loss: 567.5873 - loglik: -5.6787e+02 - logprior: 0.2873
Epoch 7/10
39/39 - 24s - loss: 565.5208 - loglik: -5.6581e+02 - logprior: 0.2870
Epoch 8/10
39/39 - 25s - loss: 564.4250 - loglik: -5.6473e+02 - logprior: 0.3017
Epoch 9/10
39/39 - 24s - loss: 562.9213 - loglik: -5.6325e+02 - logprior: 0.3255
Epoch 10/10
39/39 - 25s - loss: 562.9156 - loglik: -5.6326e+02 - logprior: 0.3454
Fitted a model with MAP estimate = -561.9805
Time for alignment: 607.0169
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 643.9824 - loglik: -6.4225e+02 - logprior: -1.7374e+00
Epoch 2/10
39/39 - 19s - loss: 594.8931 - loglik: -5.9384e+02 - logprior: -1.0569e+00
Epoch 3/10
39/39 - 19s - loss: 588.6559 - loglik: -5.8757e+02 - logprior: -1.0815e+00
Epoch 4/10
39/39 - 19s - loss: 584.5439 - loglik: -5.8347e+02 - logprior: -1.0734e+00
Epoch 5/10
39/39 - 19s - loss: 579.8967 - loglik: -5.7879e+02 - logprior: -1.1113e+00
Epoch 6/10
39/39 - 20s - loss: 577.4094 - loglik: -5.7627e+02 - logprior: -1.1431e+00
Epoch 7/10
39/39 - 20s - loss: 576.6656 - loglik: -5.7549e+02 - logprior: -1.1806e+00
Epoch 8/10
39/39 - 20s - loss: 575.9615 - loglik: -5.7477e+02 - logprior: -1.1963e+00
Epoch 9/10
39/39 - 19s - loss: 575.7111 - loglik: -5.7449e+02 - logprior: -1.2205e+00
Epoch 10/10
39/39 - 19s - loss: 575.1689 - loglik: -5.7394e+02 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -574.5122
expansions: [(21, 1), (22, 1), (23, 1), (24, 4), (28, 1), (47, 5), (49, 1), (60, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (104, 1), (105, 4), (106, 2), (108, 1), (109, 1), (124, 1), (127, 1), (134, 1), (140, 1), (146, 1), (148, 1), (150, 1), (156, 4), (161, 1), (163, 1), (165, 1), (168, 1), (170, 2), (174, 1), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 611.5378 - loglik: -6.0950e+02 - logprior: -2.0406e+00
Epoch 2/2
39/39 - 26s - loss: 583.9382 - loglik: -5.8334e+02 - logprior: -5.9426e-01
Fitted a model with MAP estimate = -581.1459
expansions: [(32, 3), (245, 9)]
discards: [ 26  27  28  29  81 106 134 135 180 200 201 242 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 587.5234 - loglik: -5.8594e+02 - logprior: -1.5859e+00
Epoch 2/2
39/39 - 28s - loss: 583.1990 - loglik: -5.8278e+02 - logprior: -4.1742e-01
Fitted a model with MAP estimate = -580.6784
expansions: [(26, 3), (190, 1)]
discards: [ 34  35  36  56 127 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 586.8028 - loglik: -5.8540e+02 - logprior: -1.4032e+00
Epoch 2/10
39/39 - 24s - loss: 583.3923 - loglik: -5.8331e+02 - logprior: -7.8696e-02
Epoch 3/10
39/39 - 24s - loss: 580.4631 - loglik: -5.8048e+02 - logprior: 0.0195
Epoch 4/10
39/39 - 23s - loss: 576.4614 - loglik: -5.7656e+02 - logprior: 0.0956
Epoch 5/10
39/39 - 23s - loss: 571.1315 - loglik: -5.7129e+02 - logprior: 0.1560
Epoch 6/10
39/39 - 24s - loss: 566.7191 - loglik: -5.6697e+02 - logprior: 0.2523
Epoch 7/10
39/39 - 25s - loss: 565.3496 - loglik: -5.6565e+02 - logprior: 0.2994
Epoch 8/10
39/39 - 25s - loss: 563.4241 - loglik: -5.6372e+02 - logprior: 0.2952
Epoch 9/10
39/39 - 24s - loss: 562.3464 - loglik: -5.6268e+02 - logprior: 0.3335
Epoch 10/10
39/39 - 23s - loss: 562.0150 - loglik: -5.6236e+02 - logprior: 0.3490
Fitted a model with MAP estimate = -561.1718
Time for alignment: 654.9625
Computed alignments with likelihoods: ['-561.9067', '-562.0787', '-561.2549', '-561.9805', '-561.1718']
Best model has likelihood: -561.1718  (prior= 0.3974 )
time for generating output: 0.2311
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6974169741697417
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 153.9558 - loglik: -1.5073e+02 - logprior: -3.2254e+00
Epoch 2/10
19/19 - 1s - loss: 123.1116 - loglik: -1.2159e+02 - logprior: -1.5259e+00
Epoch 3/10
19/19 - 1s - loss: 111.0953 - loglik: -1.0955e+02 - logprior: -1.5438e+00
Epoch 4/10
19/19 - 1s - loss: 108.2979 - loglik: -1.0680e+02 - logprior: -1.5014e+00
Epoch 5/10
19/19 - 1s - loss: 107.1559 - loglik: -1.0569e+02 - logprior: -1.4701e+00
Epoch 6/10
19/19 - 1s - loss: 106.8839 - loglik: -1.0543e+02 - logprior: -1.4534e+00
Epoch 7/10
19/19 - 1s - loss: 106.6003 - loglik: -1.0517e+02 - logprior: -1.4299e+00
Epoch 8/10
19/19 - 1s - loss: 106.4847 - loglik: -1.0507e+02 - logprior: -1.4178e+00
Epoch 9/10
19/19 - 1s - loss: 106.3325 - loglik: -1.0492e+02 - logprior: -1.4144e+00
Epoch 10/10
19/19 - 1s - loss: 106.4007 - loglik: -1.0499e+02 - logprior: -1.4132e+00
Fitted a model with MAP estimate = -106.3000
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.5361 - loglik: -1.0426e+02 - logprior: -3.2809e+00
Epoch 2/2
19/19 - 1s - loss: 100.0441 - loglik: -9.8655e+01 - logprior: -1.3896e+00
Fitted a model with MAP estimate = -99.1860
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.3561 - loglik: -9.9192e+01 - logprior: -3.1638e+00
Epoch 2/2
19/19 - 1s - loss: 99.4385 - loglik: -9.8165e+01 - logprior: -1.2734e+00
Fitted a model with MAP estimate = -98.8370
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.9812 - loglik: -9.8856e+01 - logprior: -3.1248e+00
Epoch 2/10
19/19 - 1s - loss: 99.2226 - loglik: -9.7971e+01 - logprior: -1.2511e+00
Epoch 3/10
19/19 - 1s - loss: 98.6034 - loglik: -9.7473e+01 - logprior: -1.1303e+00
Epoch 4/10
19/19 - 1s - loss: 98.3757 - loglik: -9.7289e+01 - logprior: -1.0870e+00
Epoch 5/10
19/19 - 1s - loss: 97.7634 - loglik: -9.6709e+01 - logprior: -1.0543e+00
Epoch 6/10
19/19 - 1s - loss: 97.8526 - loglik: -9.6806e+01 - logprior: -1.0464e+00
Fitted a model with MAP estimate = -97.5698
Time for alignment: 40.6581
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0456 - loglik: -1.5081e+02 - logprior: -3.2317e+00
Epoch 2/10
19/19 - 1s - loss: 124.0330 - loglik: -1.2249e+02 - logprior: -1.5402e+00
Epoch 3/10
19/19 - 1s - loss: 112.3351 - loglik: -1.1077e+02 - logprior: -1.5700e+00
Epoch 4/10
19/19 - 1s - loss: 108.4211 - loglik: -1.0686e+02 - logprior: -1.5636e+00
Epoch 5/10
19/19 - 1s - loss: 107.3486 - loglik: -1.0581e+02 - logprior: -1.5370e+00
Epoch 6/10
19/19 - 1s - loss: 107.2142 - loglik: -1.0571e+02 - logprior: -1.5092e+00
Epoch 7/10
19/19 - 1s - loss: 106.9703 - loglik: -1.0549e+02 - logprior: -1.4820e+00
Epoch 8/10
19/19 - 1s - loss: 106.7057 - loglik: -1.0524e+02 - logprior: -1.4674e+00
Epoch 9/10
19/19 - 1s - loss: 106.6821 - loglik: -1.0522e+02 - logprior: -1.4574e+00
Epoch 10/10
19/19 - 1s - loss: 107.0267 - loglik: -1.0558e+02 - logprior: -1.4513e+00
Fitted a model with MAP estimate = -106.6477
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1292 - loglik: -1.0484e+02 - logprior: -3.2879e+00
Epoch 2/2
19/19 - 1s - loss: 100.2455 - loglik: -9.8835e+01 - logprior: -1.4109e+00
Fitted a model with MAP estimate = -99.1949
expansions: []
discards: [21 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.5068 - loglik: -9.9332e+01 - logprior: -3.1751e+00
Epoch 2/2
19/19 - 1s - loss: 99.4219 - loglik: -9.8157e+01 - logprior: -1.2644e+00
Fitted a model with MAP estimate = -98.8419
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 101.9198 - loglik: -9.8785e+01 - logprior: -3.1347e+00
Epoch 2/10
19/19 - 1s - loss: 99.3025 - loglik: -9.8052e+01 - logprior: -1.2502e+00
Epoch 3/10
19/19 - 1s - loss: 98.5621 - loglik: -9.7437e+01 - logprior: -1.1254e+00
Epoch 4/10
19/19 - 1s - loss: 98.5890 - loglik: -9.7504e+01 - logprior: -1.0851e+00
Fitted a model with MAP estimate = -98.0082
Time for alignment: 40.1678
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0415 - loglik: -1.5081e+02 - logprior: -3.2297e+00
Epoch 2/10
19/19 - 1s - loss: 123.8229 - loglik: -1.2230e+02 - logprior: -1.5251e+00
Epoch 3/10
19/19 - 1s - loss: 112.1486 - loglik: -1.1059e+02 - logprior: -1.5621e+00
Epoch 4/10
19/19 - 1s - loss: 108.8302 - loglik: -1.0730e+02 - logprior: -1.5341e+00
Epoch 5/10
19/19 - 1s - loss: 107.9290 - loglik: -1.0643e+02 - logprior: -1.5031e+00
Epoch 6/10
19/19 - 1s - loss: 107.5626 - loglik: -1.0608e+02 - logprior: -1.4788e+00
Epoch 7/10
19/19 - 1s - loss: 106.8269 - loglik: -1.0536e+02 - logprior: -1.4662e+00
Epoch 8/10
19/19 - 1s - loss: 106.1353 - loglik: -1.0467e+02 - logprior: -1.4655e+00
Epoch 9/10
19/19 - 1s - loss: 106.3435 - loglik: -1.0488e+02 - logprior: -1.4626e+00
Fitted a model with MAP estimate = -105.8373
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.6041 - loglik: -1.0431e+02 - logprior: -3.2967e+00
Epoch 2/2
19/19 - 1s - loss: 99.9496 - loglik: -9.8550e+01 - logprior: -1.4000e+00
Fitted a model with MAP estimate = -99.1462
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.2912 - loglik: -9.9120e+01 - logprior: -3.1713e+00
Epoch 2/2
19/19 - 1s - loss: 99.4686 - loglik: -9.8202e+01 - logprior: -1.2665e+00
Fitted a model with MAP estimate = -98.8536
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.9104 - loglik: -9.8776e+01 - logprior: -3.1341e+00
Epoch 2/10
19/19 - 1s - loss: 99.2169 - loglik: -9.7972e+01 - logprior: -1.2451e+00
Epoch 3/10
19/19 - 1s - loss: 98.7700 - loglik: -9.7639e+01 - logprior: -1.1309e+00
Epoch 4/10
19/19 - 1s - loss: 98.3718 - loglik: -9.7285e+01 - logprior: -1.0872e+00
Epoch 5/10
19/19 - 1s - loss: 97.8270 - loglik: -9.6764e+01 - logprior: -1.0631e+00
Epoch 6/10
19/19 - 1s - loss: 97.8151 - loglik: -9.6771e+01 - logprior: -1.0445e+00
Epoch 7/10
19/19 - 1s - loss: 97.6535 - loglik: -9.6621e+01 - logprior: -1.0322e+00
Epoch 8/10
19/19 - 1s - loss: 97.4930 - loglik: -9.6474e+01 - logprior: -1.0186e+00
Epoch 9/10
19/19 - 1s - loss: 97.3840 - loglik: -9.6381e+01 - logprior: -1.0030e+00
Epoch 10/10
19/19 - 1s - loss: 97.3774 - loglik: -9.6382e+01 - logprior: -9.9534e-01
Fitted a model with MAP estimate = -97.3623
Time for alignment: 43.7784
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 153.9901 - loglik: -1.5076e+02 - logprior: -3.2303e+00
Epoch 2/10
19/19 - 1s - loss: 123.1738 - loglik: -1.2164e+02 - logprior: -1.5355e+00
Epoch 3/10
19/19 - 1s - loss: 110.7276 - loglik: -1.0919e+02 - logprior: -1.5362e+00
Epoch 4/10
19/19 - 1s - loss: 108.3051 - loglik: -1.0680e+02 - logprior: -1.5065e+00
Epoch 5/10
19/19 - 1s - loss: 107.3342 - loglik: -1.0587e+02 - logprior: -1.4672e+00
Epoch 6/10
19/19 - 1s - loss: 106.6214 - loglik: -1.0517e+02 - logprior: -1.4505e+00
Epoch 7/10
19/19 - 1s - loss: 106.8231 - loglik: -1.0538e+02 - logprior: -1.4442e+00
Fitted a model with MAP estimate = -106.4463
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.4136 - loglik: -1.0412e+02 - logprior: -3.2944e+00
Epoch 2/2
19/19 - 1s - loss: 100.0341 - loglik: -9.8639e+01 - logprior: -1.3954e+00
Fitted a model with MAP estimate = -99.1874
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4502 - loglik: -9.9281e+01 - logprior: -3.1695e+00
Epoch 2/2
19/19 - 1s - loss: 99.3323 - loglik: -9.8055e+01 - logprior: -1.2771e+00
Fitted a model with MAP estimate = -98.8296
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.8674 - loglik: -9.8736e+01 - logprior: -3.1313e+00
Epoch 2/10
19/19 - 1s - loss: 99.2433 - loglik: -9.7989e+01 - logprior: -1.2541e+00
Epoch 3/10
19/19 - 1s - loss: 98.8908 - loglik: -9.7757e+01 - logprior: -1.1338e+00
Epoch 4/10
19/19 - 1s - loss: 98.3158 - loglik: -9.7229e+01 - logprior: -1.0865e+00
Epoch 5/10
19/19 - 1s - loss: 97.8576 - loglik: -9.6791e+01 - logprior: -1.0666e+00
Epoch 6/10
19/19 - 1s - loss: 97.5919 - loglik: -9.6542e+01 - logprior: -1.0496e+00
Epoch 7/10
19/19 - 1s - loss: 97.6765 - loglik: -9.6644e+01 - logprior: -1.0320e+00
Fitted a model with MAP estimate = -97.4704
Time for alignment: 39.3675
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.0053 - loglik: -1.5078e+02 - logprior: -3.2269e+00
Epoch 2/10
19/19 - 1s - loss: 123.6113 - loglik: -1.2208e+02 - logprior: -1.5299e+00
Epoch 3/10
19/19 - 1s - loss: 111.2269 - loglik: -1.0964e+02 - logprior: -1.5862e+00
Epoch 4/10
19/19 - 1s - loss: 107.9771 - loglik: -1.0643e+02 - logprior: -1.5502e+00
Epoch 5/10
19/19 - 1s - loss: 106.4504 - loglik: -1.0492e+02 - logprior: -1.5315e+00
Epoch 6/10
19/19 - 1s - loss: 106.4014 - loglik: -1.0489e+02 - logprior: -1.5089e+00
Epoch 7/10
19/19 - 1s - loss: 105.8858 - loglik: -1.0439e+02 - logprior: -1.4911e+00
Epoch 8/10
19/19 - 1s - loss: 106.0134 - loglik: -1.0453e+02 - logprior: -1.4787e+00
Fitted a model with MAP estimate = -105.8101
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.5912 - loglik: -1.0430e+02 - logprior: -3.2943e+00
Epoch 2/2
19/19 - 1s - loss: 99.9646 - loglik: -9.8563e+01 - logprior: -1.4013e+00
Fitted a model with MAP estimate = -99.1623
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.2355 - loglik: -9.9066e+01 - logprior: -3.1694e+00
Epoch 2/2
19/19 - 1s - loss: 99.4634 - loglik: -9.8192e+01 - logprior: -1.2715e+00
Fitted a model with MAP estimate = -98.8460
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.9598 - loglik: -9.8824e+01 - logprior: -3.1359e+00
Epoch 2/10
19/19 - 1s - loss: 99.2537 - loglik: -9.7997e+01 - logprior: -1.2563e+00
Epoch 3/10
19/19 - 1s - loss: 98.6969 - loglik: -9.7568e+01 - logprior: -1.1288e+00
Epoch 4/10
19/19 - 1s - loss: 98.2369 - loglik: -9.7147e+01 - logprior: -1.0896e+00
Epoch 5/10
19/19 - 1s - loss: 97.9446 - loglik: -9.6882e+01 - logprior: -1.0623e+00
Epoch 6/10
19/19 - 1s - loss: 97.7647 - loglik: -9.6719e+01 - logprior: -1.0457e+00
Epoch 7/10
19/19 - 1s - loss: 97.5867 - loglik: -9.6550e+01 - logprior: -1.0371e+00
Epoch 8/10
19/19 - 1s - loss: 97.6473 - loglik: -9.6618e+01 - logprior: -1.0296e+00
Fitted a model with MAP estimate = -97.4354
Time for alignment: 40.8043
Computed alignments with likelihoods: ['-97.5698', '-98.0082', '-97.3623', '-97.4704', '-97.4354']
Best model has likelihood: -97.3623  (prior= -0.9811 )
time for generating output: 0.1040
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9837962962962963
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.9201 - loglik: -3.6294e+02 - logprior: -2.9812e+00
Epoch 2/10
19/19 - 3s - loss: 338.1917 - loglik: -3.3718e+02 - logprior: -1.0135e+00
Epoch 3/10
19/19 - 3s - loss: 326.2228 - loglik: -3.2497e+02 - logprior: -1.2547e+00
Epoch 4/10
19/19 - 3s - loss: 322.0086 - loglik: -3.2086e+02 - logprior: -1.1502e+00
Epoch 5/10
19/19 - 3s - loss: 318.9969 - loglik: -3.1782e+02 - logprior: -1.1745e+00
Epoch 6/10
19/19 - 3s - loss: 317.4477 - loglik: -3.1625e+02 - logprior: -1.1927e+00
Epoch 7/10
19/19 - 3s - loss: 315.7851 - loglik: -3.1460e+02 - logprior: -1.1823e+00
Epoch 8/10
19/19 - 3s - loss: 315.3831 - loglik: -3.1420e+02 - logprior: -1.1832e+00
Epoch 9/10
19/19 - 3s - loss: 314.2496 - loglik: -3.1307e+02 - logprior: -1.1842e+00
Epoch 10/10
19/19 - 3s - loss: 313.9309 - loglik: -3.1274e+02 - logprior: -1.1934e+00
Fitted a model with MAP estimate = -313.9683
expansions: [(15, 1), (17, 1), (19, 1), (20, 1), (21, 4), (24, 3), (26, 1), (29, 1), (32, 1), (51, 1), (53, 3), (73, 1), (74, 1), (75, 1), (76, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 333.4477 - loglik: -3.2974e+02 - logprior: -3.7113e+00
Epoch 2/2
19/19 - 4s - loss: 318.3972 - loglik: -3.1656e+02 - logprior: -1.8400e+00
Fitted a model with MAP estimate = -315.8565
expansions: [(0, 2)]
discards: [  0 109]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 317.5546 - loglik: -3.1479e+02 - logprior: -2.7684e+00
Epoch 2/2
19/19 - 4s - loss: 314.2880 - loglik: -3.1334e+02 - logprior: -9.4589e-01
Fitted a model with MAP estimate = -313.0780
expansions: []
discards: [ 1 25 26 27]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.8685 - loglik: -3.1522e+02 - logprior: -2.6461e+00
Epoch 2/10
19/19 - 4s - loss: 314.5592 - loglik: -3.1371e+02 - logprior: -8.4860e-01
Epoch 3/10
19/19 - 4s - loss: 313.5643 - loglik: -3.1284e+02 - logprior: -7.2904e-01
Epoch 4/10
19/19 - 4s - loss: 312.1402 - loglik: -3.1140e+02 - logprior: -7.3538e-01
Epoch 5/10
19/19 - 4s - loss: 310.5107 - loglik: -3.0971e+02 - logprior: -7.9807e-01
Epoch 6/10
19/19 - 5s - loss: 309.0592 - loglik: -3.0830e+02 - logprior: -7.5602e-01
Epoch 7/10
19/19 - 5s - loss: 308.0463 - loglik: -3.0731e+02 - logprior: -7.3731e-01
Epoch 8/10
19/19 - 4s - loss: 307.1943 - loglik: -3.0647e+02 - logprior: -7.2740e-01
Epoch 9/10
19/19 - 5s - loss: 306.4494 - loglik: -3.0573e+02 - logprior: -7.1628e-01
Epoch 10/10
19/19 - 4s - loss: 305.7928 - loglik: -3.0509e+02 - logprior: -7.0495e-01
Fitted a model with MAP estimate = -305.5278
Time for alignment: 124.9056
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.8080 - loglik: -3.6283e+02 - logprior: -2.9786e+00
Epoch 2/10
19/19 - 3s - loss: 338.7964 - loglik: -3.3778e+02 - logprior: -1.0144e+00
Epoch 3/10
19/19 - 3s - loss: 326.5402 - loglik: -3.2531e+02 - logprior: -1.2343e+00
Epoch 4/10
19/19 - 4s - loss: 322.0026 - loglik: -3.2084e+02 - logprior: -1.1667e+00
Epoch 5/10
19/19 - 3s - loss: 319.0421 - loglik: -3.1784e+02 - logprior: -1.2010e+00
Epoch 6/10
19/19 - 3s - loss: 317.5100 - loglik: -3.1633e+02 - logprior: -1.1793e+00
Epoch 7/10
19/19 - 4s - loss: 316.6203 - loglik: -3.1545e+02 - logprior: -1.1741e+00
Epoch 8/10
19/19 - 4s - loss: 315.4177 - loglik: -3.1424e+02 - logprior: -1.1763e+00
Epoch 9/10
19/19 - 4s - loss: 314.8929 - loglik: -3.1371e+02 - logprior: -1.1854e+00
Epoch 10/10
19/19 - 3s - loss: 314.5650 - loglik: -3.1337e+02 - logprior: -1.1974e+00
Fitted a model with MAP estimate = -314.1441
expansions: [(15, 1), (17, 1), (19, 1), (20, 1), (21, 4), (23, 2), (26, 1), (29, 1), (32, 1), (51, 1), (53, 3), (68, 1), (74, 1), (75, 1), (76, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 333.1218 - loglik: -3.2939e+02 - logprior: -3.7279e+00
Epoch 2/2
19/19 - 4s - loss: 318.6564 - loglik: -3.1682e+02 - logprior: -1.8399e+00
Fitted a model with MAP estimate = -315.8154
expansions: [(0, 2)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 317.6475 - loglik: -3.1488e+02 - logprior: -2.7638e+00
Epoch 2/2
19/19 - 4s - loss: 314.2085 - loglik: -3.1325e+02 - logprior: -9.5546e-01
Fitted a model with MAP estimate = -312.8155
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 316.7794 - loglik: -3.1413e+02 - logprior: -2.6531e+00
Epoch 2/10
19/19 - 4s - loss: 313.7509 - loglik: -3.1289e+02 - logprior: -8.6538e-01
Epoch 3/10
19/19 - 5s - loss: 312.7619 - loglik: -3.1202e+02 - logprior: -7.3811e-01
Epoch 4/10
19/19 - 5s - loss: 312.0310 - loglik: -3.1122e+02 - logprior: -8.1089e-01
Epoch 5/10
19/19 - 5s - loss: 309.9132 - loglik: -3.0913e+02 - logprior: -7.8662e-01
Epoch 6/10
19/19 - 5s - loss: 308.3029 - loglik: -3.0753e+02 - logprior: -7.7705e-01
Epoch 7/10
19/19 - 5s - loss: 307.2427 - loglik: -3.0649e+02 - logprior: -7.5226e-01
Epoch 8/10
19/19 - 5s - loss: 306.5581 - loglik: -3.0581e+02 - logprior: -7.5247e-01
Epoch 9/10
19/19 - 5s - loss: 305.5998 - loglik: -3.0486e+02 - logprior: -7.4302e-01
Epoch 10/10
19/19 - 5s - loss: 305.2317 - loglik: -3.0450e+02 - logprior: -7.2695e-01
Fitted a model with MAP estimate = -304.6917
Time for alignment: 134.4843
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.9433 - loglik: -3.6296e+02 - logprior: -2.9792e+00
Epoch 2/10
19/19 - 4s - loss: 338.4387 - loglik: -3.3742e+02 - logprior: -1.0144e+00
Epoch 3/10
19/19 - 4s - loss: 326.3391 - loglik: -3.2508e+02 - logprior: -1.2630e+00
Epoch 4/10
19/19 - 3s - loss: 321.2483 - loglik: -3.2007e+02 - logprior: -1.1825e+00
Epoch 5/10
19/19 - 4s - loss: 318.0063 - loglik: -3.1680e+02 - logprior: -1.2072e+00
Epoch 6/10
19/19 - 4s - loss: 316.3345 - loglik: -3.1515e+02 - logprior: -1.1874e+00
Epoch 7/10
19/19 - 3s - loss: 315.1006 - loglik: -3.1392e+02 - logprior: -1.1812e+00
Epoch 8/10
19/19 - 3s - loss: 314.6924 - loglik: -3.1351e+02 - logprior: -1.1844e+00
Epoch 9/10
19/19 - 3s - loss: 313.8627 - loglik: -3.1267e+02 - logprior: -1.1931e+00
Epoch 10/10
19/19 - 4s - loss: 313.6962 - loglik: -3.1249e+02 - logprior: -1.2016e+00
Fitted a model with MAP estimate = -313.1460
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (24, 2), (25, 1), (32, 1), (35, 2), (52, 1), (54, 4), (75, 1), (76, 3), (82, 1), (84, 1), (86, 1), (87, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 333.1542 - loglik: -3.2945e+02 - logprior: -3.7040e+00
Epoch 2/2
19/19 - 5s - loss: 317.9776 - loglik: -3.1612e+02 - logprior: -1.8585e+00
Fitted a model with MAP estimate = -315.3659
expansions: [(0, 2), (95, 1), (104, 2)]
discards: [ 0 25 26 34 49]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 318.0368 - loglik: -3.1528e+02 - logprior: -2.7548e+00
Epoch 2/2
19/19 - 5s - loss: 314.1440 - loglik: -3.1319e+02 - logprior: -9.5293e-01
Fitted a model with MAP estimate = -312.9861
expansions: [(27, 2)]
discards: [  1  65 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.3936 - loglik: -3.1474e+02 - logprior: -2.6517e+00
Epoch 2/10
19/19 - 5s - loss: 314.2390 - loglik: -3.1337e+02 - logprior: -8.6705e-01
Epoch 3/10
19/19 - 5s - loss: 313.3676 - loglik: -3.1257e+02 - logprior: -7.9693e-01
Epoch 4/10
19/19 - 5s - loss: 311.5183 - loglik: -3.1067e+02 - logprior: -8.5108e-01
Epoch 5/10
19/19 - 5s - loss: 310.6068 - loglik: -3.0982e+02 - logprior: -7.8922e-01
Epoch 6/10
19/19 - 5s - loss: 308.2320 - loglik: -3.0745e+02 - logprior: -7.8316e-01
Epoch 7/10
19/19 - 5s - loss: 307.6243 - loglik: -3.0686e+02 - logprior: -7.6397e-01
Epoch 8/10
19/19 - 5s - loss: 307.0156 - loglik: -3.0626e+02 - logprior: -7.5282e-01
Epoch 9/10
19/19 - 5s - loss: 305.6581 - loglik: -3.0491e+02 - logprior: -7.5166e-01
Epoch 10/10
19/19 - 5s - loss: 305.7943 - loglik: -3.0506e+02 - logprior: -7.3399e-01
Fitted a model with MAP estimate = -305.0487
Time for alignment: 140.6969
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.8557 - loglik: -3.6288e+02 - logprior: -2.9787e+00
Epoch 2/10
19/19 - 3s - loss: 337.7385 - loglik: -3.3672e+02 - logprior: -1.0204e+00
Epoch 3/10
19/19 - 3s - loss: 325.7708 - loglik: -3.2451e+02 - logprior: -1.2575e+00
Epoch 4/10
19/19 - 3s - loss: 321.6406 - loglik: -3.2049e+02 - logprior: -1.1523e+00
Epoch 5/10
19/19 - 3s - loss: 319.3705 - loglik: -3.1821e+02 - logprior: -1.1577e+00
Epoch 6/10
19/19 - 3s - loss: 317.5505 - loglik: -3.1641e+02 - logprior: -1.1395e+00
Epoch 7/10
19/19 - 3s - loss: 316.0279 - loglik: -3.1490e+02 - logprior: -1.1326e+00
Epoch 8/10
19/19 - 3s - loss: 314.5298 - loglik: -3.1340e+02 - logprior: -1.1344e+00
Epoch 9/10
19/19 - 3s - loss: 314.9957 - loglik: -3.1385e+02 - logprior: -1.1440e+00
Fitted a model with MAP estimate = -314.1301
expansions: [(18, 1), (20, 2), (21, 4), (24, 3), (26, 1), (29, 1), (32, 1), (53, 3), (73, 1), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 331.0350 - loglik: -3.2731e+02 - logprior: -3.7228e+00
Epoch 2/2
19/19 - 4s - loss: 317.9806 - loglik: -3.1617e+02 - logprior: -1.8118e+00
Fitted a model with MAP estimate = -315.8910
expansions: [(0, 2), (64, 1)]
discards: [  0  91  92  93 107]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 318.3731 - loglik: -3.1560e+02 - logprior: -2.7755e+00
Epoch 2/2
19/19 - 4s - loss: 314.9577 - loglik: -3.1399e+02 - logprior: -9.6914e-01
Fitted a model with MAP estimate = -313.8078
expansions: [(94, 3)]
discards: [25 26 33]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 318.1498 - loglik: -3.1548e+02 - logprior: -2.6709e+00
Epoch 2/10
19/19 - 4s - loss: 314.8710 - loglik: -3.1398e+02 - logprior: -8.9206e-01
Epoch 3/10
19/19 - 4s - loss: 313.4008 - loglik: -3.1257e+02 - logprior: -8.2704e-01
Epoch 4/10
19/19 - 4s - loss: 312.8875 - loglik: -3.1212e+02 - logprior: -7.7236e-01
Epoch 5/10
19/19 - 4s - loss: 311.0119 - loglik: -3.1027e+02 - logprior: -7.4533e-01
Epoch 6/10
19/19 - 5s - loss: 309.2293 - loglik: -3.0851e+02 - logprior: -7.2212e-01
Epoch 7/10
19/19 - 4s - loss: 308.1674 - loglik: -3.0746e+02 - logprior: -7.0722e-01
Epoch 8/10
19/19 - 5s - loss: 306.9959 - loglik: -3.0630e+02 - logprior: -6.9426e-01
Epoch 9/10
19/19 - 4s - loss: 306.9760 - loglik: -3.0629e+02 - logprior: -6.8389e-01
Epoch 10/10
19/19 - 5s - loss: 305.9699 - loglik: -3.0529e+02 - logprior: -6.7760e-01
Fitted a model with MAP estimate = -305.6854
Time for alignment: 129.1000
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.7751 - loglik: -3.6279e+02 - logprior: -2.9816e+00
Epoch 2/10
19/19 - 4s - loss: 337.7116 - loglik: -3.3668e+02 - logprior: -1.0271e+00
Epoch 3/10
19/19 - 3s - loss: 325.3178 - loglik: -3.2407e+02 - logprior: -1.2470e+00
Epoch 4/10
19/19 - 3s - loss: 321.4043 - loglik: -3.2026e+02 - logprior: -1.1467e+00
Epoch 5/10
19/19 - 3s - loss: 319.3386 - loglik: -3.1818e+02 - logprior: -1.1587e+00
Epoch 6/10
19/19 - 3s - loss: 317.4248 - loglik: -3.1627e+02 - logprior: -1.1570e+00
Epoch 7/10
19/19 - 3s - loss: 316.2954 - loglik: -3.1512e+02 - logprior: -1.1764e+00
Epoch 8/10
19/19 - 4s - loss: 315.3930 - loglik: -3.1421e+02 - logprior: -1.1871e+00
Epoch 9/10
19/19 - 4s - loss: 314.5063 - loglik: -3.1332e+02 - logprior: -1.1882e+00
Epoch 10/10
19/19 - 4s - loss: 314.6156 - loglik: -3.1342e+02 - logprior: -1.1969e+00
Fitted a model with MAP estimate = -314.1049
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 2), (29, 1), (32, 1), (51, 1), (53, 2), (73, 1), (74, 1), (76, 3), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 333.4448 - loglik: -3.2971e+02 - logprior: -3.7334e+00
Epoch 2/2
19/19 - 5s - loss: 318.3014 - loglik: -3.1643e+02 - logprior: -1.8728e+00
Fitted a model with MAP estimate = -315.8194
expansions: [(0, 2), (69, 1)]
discards: [  0  24  25  26  37 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 318.4965 - loglik: -3.1573e+02 - logprior: -2.7623e+00
Epoch 2/2
19/19 - 5s - loss: 314.7033 - loglik: -3.1377e+02 - logprior: -9.3617e-01
Fitted a model with MAP estimate = -313.7135
expansions: []
discards: [ 1 29]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.5700 - loglik: -3.1492e+02 - logprior: -2.6456e+00
Epoch 2/10
19/19 - 4s - loss: 314.5472 - loglik: -3.1370e+02 - logprior: -8.4534e-01
Epoch 3/10
19/19 - 4s - loss: 313.7852 - loglik: -3.1305e+02 - logprior: -7.3721e-01
Epoch 4/10
19/19 - 4s - loss: 312.3718 - loglik: -3.1153e+02 - logprior: -8.3866e-01
Epoch 5/10
19/19 - 4s - loss: 310.5613 - loglik: -3.0980e+02 - logprior: -7.5863e-01
Epoch 6/10
19/19 - 4s - loss: 309.4423 - loglik: -3.0869e+02 - logprior: -7.5101e-01
Epoch 7/10
19/19 - 4s - loss: 308.0718 - loglik: -3.0735e+02 - logprior: -7.1914e-01
Epoch 8/10
19/19 - 4s - loss: 307.2144 - loglik: -3.0650e+02 - logprior: -7.1163e-01
Epoch 9/10
19/19 - 4s - loss: 306.3370 - loglik: -3.0565e+02 - logprior: -6.9067e-01
Epoch 10/10
19/19 - 4s - loss: 306.1552 - loglik: -3.0547e+02 - logprior: -6.8441e-01
Fitted a model with MAP estimate = -305.5933
Time for alignment: 133.5363
Computed alignments with likelihoods: ['-305.5278', '-304.6917', '-305.0487', '-305.6854', '-305.5933']
Best model has likelihood: -304.6917  (prior= -0.7289 )
time for generating output: 0.1409
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8824681684622919
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 918.6703 - loglik: -9.1725e+02 - logprior: -1.4178e+00
Epoch 2/10
39/39 - 36s - loss: 824.5417 - loglik: -8.2324e+02 - logprior: -1.3047e+00
Epoch 3/10
39/39 - 38s - loss: 813.4440 - loglik: -8.1204e+02 - logprior: -1.4019e+00
Epoch 4/10
39/39 - 39s - loss: 808.6625 - loglik: -8.0725e+02 - logprior: -1.4113e+00
Epoch 5/10
39/39 - 36s - loss: 806.0080 - loglik: -8.0455e+02 - logprior: -1.4563e+00
Epoch 6/10
39/39 - 35s - loss: 804.6592 - loglik: -8.0319e+02 - logprior: -1.4732e+00
Epoch 7/10
39/39 - 36s - loss: 804.6057 - loglik: -8.0311e+02 - logprior: -1.4913e+00
Epoch 8/10
39/39 - 38s - loss: 804.5280 - loglik: -8.0302e+02 - logprior: -1.5103e+00
Epoch 9/10
39/39 - 39s - loss: 804.7201 - loglik: -8.0318e+02 - logprior: -1.5380e+00
Fitted a model with MAP estimate = -795.9673
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (30, 1), (40, 2), (41, 2), (42, 2), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 1), (102, 1), (103, 1), (119, 1), (124, 2), (130, 1), (142, 1), (144, 1), (147, 1), (149, 1), (154, 1), (155, 2), (166, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (226, 2), (228, 1), (240, 2), (241, 1), (243, 1), (245, 1), (258, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 809.7954 - loglik: -8.0782e+02 - logprior: -1.9777e+00
Epoch 2/2
39/39 - 60s - loss: 789.9658 - loglik: -7.8946e+02 - logprior: -5.0320e-01
Fitted a model with MAP estimate = -779.7912
expansions: [(119, 1), (159, 1), (255, 1)]
discards: [  1  32  33  54  62 286 303 343]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 793.5127 - loglik: -7.9233e+02 - logprior: -1.1870e+00
Epoch 2/2
39/39 - 65s - loss: 788.8766 - loglik: -7.8867e+02 - logprior: -2.0409e-01
Fitted a model with MAP estimate = -779.0511
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 785.3239 - loglik: -7.8412e+02 - logprior: -1.1994e+00
Epoch 2/10
39/39 - 62s - loss: 780.6364 - loglik: -7.8053e+02 - logprior: -1.1075e-01
Epoch 3/10
39/39 - 57s - loss: 778.3809 - loglik: -7.7848e+02 - logprior: 0.1020
Epoch 4/10
39/39 - 55s - loss: 774.3892 - loglik: -7.7457e+02 - logprior: 0.1854
Epoch 5/10
39/39 - 57s - loss: 770.5511 - loglik: -7.7080e+02 - logprior: 0.2454
Epoch 6/10
39/39 - 52s - loss: 769.8190 - loglik: -7.7012e+02 - logprior: 0.3021
Epoch 7/10
39/39 - 52s - loss: 769.8019 - loglik: -7.7019e+02 - logprior: 0.3847
Epoch 8/10
39/39 - 56s - loss: 768.0908 - loglik: -7.6856e+02 - logprior: 0.4728
Epoch 9/10
39/39 - 54s - loss: 767.7490 - loglik: -7.6832e+02 - logprior: 0.5677
Epoch 10/10
39/39 - 51s - loss: 767.5840 - loglik: -7.6823e+02 - logprior: 0.6487
Fitted a model with MAP estimate = -766.5871
Time for alignment: 1440.5637
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 918.1200 - loglik: -9.1669e+02 - logprior: -1.4320e+00
Epoch 2/10
39/39 - 40s - loss: 824.0944 - loglik: -8.2278e+02 - logprior: -1.3152e+00
Epoch 3/10
39/39 - 42s - loss: 813.9197 - loglik: -8.1264e+02 - logprior: -1.2848e+00
Epoch 4/10
39/39 - 42s - loss: 809.8824 - loglik: -8.0859e+02 - logprior: -1.2923e+00
Epoch 5/10
39/39 - 44s - loss: 807.5670 - loglik: -8.0623e+02 - logprior: -1.3404e+00
Epoch 6/10
39/39 - 43s - loss: 806.4218 - loglik: -8.0506e+02 - logprior: -1.3574e+00
Epoch 7/10
39/39 - 40s - loss: 806.1785 - loglik: -8.0478e+02 - logprior: -1.3950e+00
Epoch 8/10
39/39 - 40s - loss: 806.5366 - loglik: -8.0512e+02 - logprior: -1.4180e+00
Fitted a model with MAP estimate = -797.8059
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (101, 1), (103, 2), (121, 1), (130, 1), (145, 1), (148, 2), (153, 2), (154, 1), (155, 2), (182, 1), (185, 1), (186, 1), (187, 1), (188, 1), (190, 1), (198, 1), (205, 1), (206, 3), (207, 1), (208, 1), (211, 1), (220, 1), (223, 1), (224, 2), (228, 2), (240, 1), (241, 2), (242, 1), (245, 1), (255, 1), (261, 2), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 812.4768 - loglik: -8.1046e+02 - logprior: -2.0193e+00
Epoch 2/2
39/39 - 65s - loss: 792.6497 - loglik: -7.9209e+02 - logprior: -5.5543e-01
Fitted a model with MAP estimate = -782.1492
expansions: [(157, 2)]
discards: [  0   1  30 105 133 192 256 257 283 341]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 795.5589 - loglik: -7.9406e+02 - logprior: -1.5028e+00
Epoch 2/2
39/39 - 52s - loss: 789.8634 - loglik: -7.8976e+02 - logprior: -1.0144e-01
Fitted a model with MAP estimate = -779.5313
expansions: [(0, 2), (115, 1), (154, 1)]
discards: [304]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 785.5376 - loglik: -7.8411e+02 - logprior: -1.4244e+00
Epoch 2/10
39/39 - 53s - loss: 781.0010 - loglik: -7.8094e+02 - logprior: -6.1070e-02
Epoch 3/10
39/39 - 52s - loss: 777.7480 - loglik: -7.7781e+02 - logprior: 0.0605
Epoch 4/10
39/39 - 57s - loss: 773.9970 - loglik: -7.7412e+02 - logprior: 0.1272
Epoch 5/10
39/39 - 62s - loss: 771.0817 - loglik: -7.7125e+02 - logprior: 0.1697
Epoch 6/10
39/39 - 65s - loss: 769.0641 - loglik: -7.6932e+02 - logprior: 0.2547
Epoch 7/10
39/39 - 64s - loss: 769.4667 - loglik: -7.6977e+02 - logprior: 0.3038
Fitted a model with MAP estimate = -768.0004
Time for alignment: 1256.1521
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 920.0995 - loglik: -9.1868e+02 - logprior: -1.4227e+00
Epoch 2/10
39/39 - 32s - loss: 827.1205 - loglik: -8.2580e+02 - logprior: -1.3244e+00
Epoch 3/10
39/39 - 31s - loss: 815.2313 - loglik: -8.1381e+02 - logprior: -1.4189e+00
Epoch 4/10
39/39 - 30s - loss: 810.5867 - loglik: -8.0919e+02 - logprior: -1.3918e+00
Epoch 5/10
39/39 - 30s - loss: 807.0359 - loglik: -8.0560e+02 - logprior: -1.4346e+00
Epoch 6/10
39/39 - 30s - loss: 805.8336 - loglik: -8.0433e+02 - logprior: -1.5001e+00
Epoch 7/10
39/39 - 30s - loss: 805.7381 - loglik: -8.0422e+02 - logprior: -1.5224e+00
Epoch 8/10
39/39 - 30s - loss: 805.3679 - loglik: -8.0384e+02 - logprior: -1.5253e+00
Epoch 9/10
39/39 - 31s - loss: 805.3746 - loglik: -8.0382e+02 - logprior: -1.5499e+00
Fitted a model with MAP estimate = -796.9786
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (30, 1), (36, 1), (39, 1), (41, 1), (42, 2), (43, 2), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 2), (85, 1), (87, 1), (94, 3), (102, 1), (104, 2), (122, 1), (123, 1), (124, 1), (130, 1), (142, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (180, 2), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (213, 1), (220, 1), (222, 1), (226, 1), (228, 1), (240, 2), (241, 1), (243, 1), (245, 1), (246, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 361 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 811.2111 - loglik: -8.0931e+02 - logprior: -1.8966e+00
Epoch 2/2
39/39 - 52s - loss: 790.6073 - loglik: -7.9015e+02 - logprior: -4.5247e-01
Fitted a model with MAP estimate = -779.9665
expansions: []
discards: [  1  32  33 105 122 136 195 227 262 307 345 348]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 795.1102 - loglik: -7.9389e+02 - logprior: -1.2202e+00
Epoch 2/2
39/39 - 46s - loss: 790.0091 - loglik: -7.8983e+02 - logprior: -1.8268e-01
Fitted a model with MAP estimate = -779.9610
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 786.0180 - loglik: -7.8499e+02 - logprior: -1.0276e+00
Epoch 2/10
39/39 - 44s - loss: 781.8057 - loglik: -7.8183e+02 - logprior: 0.0197
Epoch 3/10
39/39 - 44s - loss: 779.5197 - loglik: -7.7962e+02 - logprior: 0.0979
Epoch 4/10
39/39 - 43s - loss: 775.4066 - loglik: -7.7557e+02 - logprior: 0.1584
Epoch 5/10
39/39 - 43s - loss: 772.1772 - loglik: -7.7239e+02 - logprior: 0.2088
Epoch 6/10
39/39 - 44s - loss: 771.3829 - loglik: -7.7164e+02 - logprior: 0.2592
Epoch 7/10
39/39 - 44s - loss: 770.1132 - loglik: -7.7046e+02 - logprior: 0.3459
Epoch 8/10
39/39 - 45s - loss: 770.0398 - loglik: -7.7052e+02 - logprior: 0.4779
Epoch 9/10
39/39 - 46s - loss: 769.3075 - loglik: -7.6985e+02 - logprior: 0.5403
Epoch 10/10
39/39 - 45s - loss: 768.3902 - loglik: -7.6902e+02 - logprior: 0.6337
Fitted a model with MAP estimate = -767.8234
Time for alignment: 1157.6670
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 919.0264 - loglik: -9.1760e+02 - logprior: -1.4296e+00
Epoch 2/10
39/39 - 32s - loss: 824.5221 - loglik: -8.2315e+02 - logprior: -1.3766e+00
Epoch 3/10
39/39 - 32s - loss: 813.7901 - loglik: -8.1238e+02 - logprior: -1.4097e+00
Epoch 4/10
39/39 - 33s - loss: 809.6955 - loglik: -8.0831e+02 - logprior: -1.3867e+00
Epoch 5/10
39/39 - 33s - loss: 807.3115 - loglik: -8.0584e+02 - logprior: -1.4734e+00
Epoch 6/10
39/39 - 33s - loss: 806.3882 - loglik: -8.0491e+02 - logprior: -1.4816e+00
Epoch 7/10
39/39 - 35s - loss: 806.2474 - loglik: -8.0476e+02 - logprior: -1.4850e+00
Epoch 8/10
39/39 - 37s - loss: 806.3646 - loglik: -8.0485e+02 - logprior: -1.5128e+00
Fitted a model with MAP estimate = -798.1836
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (32, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (56, 1), (58, 1), (61, 1), (78, 1), (80, 1), (82, 1), (86, 1), (88, 1), (95, 2), (103, 1), (120, 1), (121, 1), (130, 1), (145, 1), (148, 1), (150, 1), (155, 1), (156, 2), (167, 1), (182, 1), (185, 1), (186, 1), (188, 1), (189, 2), (201, 1), (206, 3), (209, 1), (210, 1), (211, 1), (213, 1), (219, 1), (220, 1), (228, 2), (240, 2), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (269, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 809.3952 - loglik: -8.0747e+02 - logprior: -1.9294e+00
Epoch 2/2
39/39 - 53s - loss: 792.2908 - loglik: -7.9183e+02 - logprior: -4.6212e-01
Fitted a model with MAP estimate = -781.9175
expansions: [(299, 1)]
discards: [  1  32 253 311 337]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 796.2830 - loglik: -7.9512e+02 - logprior: -1.1666e+00
Epoch 2/2
39/39 - 58s - loss: 792.3063 - loglik: -7.9206e+02 - logprior: -2.4671e-01
Fitted a model with MAP estimate = -782.3587
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 787.8906 - loglik: -7.8684e+02 - logprior: -1.0490e+00
Epoch 2/10
39/39 - 53s - loss: 784.1293 - loglik: -7.8411e+02 - logprior: -2.3914e-02
Epoch 3/10
39/39 - 52s - loss: 781.1093 - loglik: -7.8120e+02 - logprior: 0.0906
Epoch 4/10
39/39 - 52s - loss: 779.2351 - loglik: -7.7936e+02 - logprior: 0.1214
Epoch 5/10
39/39 - 53s - loss: 773.6075 - loglik: -7.7380e+02 - logprior: 0.1963
Epoch 6/10
39/39 - 57s - loss: 773.3321 - loglik: -7.7354e+02 - logprior: 0.2073
Epoch 7/10
39/39 - 58s - loss: 773.8703 - loglik: -7.7422e+02 - logprior: 0.3491
Fitted a model with MAP estimate = -771.9325
Time for alignment: 1132.3404
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 919.5201 - loglik: -9.1809e+02 - logprior: -1.4294e+00
Epoch 2/10
39/39 - 40s - loss: 823.8488 - loglik: -8.2252e+02 - logprior: -1.3307e+00
Epoch 3/10
39/39 - 38s - loss: 812.4529 - loglik: -8.1109e+02 - logprior: -1.3635e+00
Epoch 4/10
39/39 - 38s - loss: 807.4468 - loglik: -8.0606e+02 - logprior: -1.3914e+00
Epoch 5/10
39/39 - 38s - loss: 804.7300 - loglik: -8.0331e+02 - logprior: -1.4200e+00
Epoch 6/10
39/39 - 36s - loss: 803.7485 - loglik: -8.0230e+02 - logprior: -1.4533e+00
Epoch 7/10
39/39 - 37s - loss: 803.2935 - loglik: -8.0181e+02 - logprior: -1.4835e+00
Epoch 8/10
39/39 - 36s - loss: 803.6642 - loglik: -8.0219e+02 - logprior: -1.4770e+00
Fitted a model with MAP estimate = -795.0836
expansions: [(0, 2), (14, 1), (15, 2), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (30, 1), (40, 2), (41, 2), (42, 2), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 2), (85, 1), (87, 1), (94, 2), (103, 2), (119, 1), (122, 1), (123, 1), (124, 1), (130, 1), (142, 1), (148, 2), (149, 1), (154, 1), (155, 2), (168, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 2), (211, 1), (220, 1), (222, 1), (228, 2), (240, 2), (242, 1), (245, 1), (250, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 810.6105 - loglik: -8.0859e+02 - logprior: -2.0208e+00
Epoch 2/2
39/39 - 58s - loss: 790.8638 - loglik: -7.9030e+02 - logprior: -5.6299e-01
Fitted a model with MAP estimate = -780.3256
expansions: [(309, 1)]
discards: [  0   1  16  33  55  63 108 137 262 269 321 345]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 795.1707 - loglik: -7.9375e+02 - logprior: -1.4170e+00
Epoch 2/2
39/39 - 53s - loss: 789.8743 - loglik: -7.8984e+02 - logprior: -3.6869e-02
Fitted a model with MAP estimate = -779.9131
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 786.1976 - loglik: -7.8483e+02 - logprior: -1.3665e+00
Epoch 2/10
39/39 - 56s - loss: 781.8181 - loglik: -7.8181e+02 - logprior: -6.0372e-03
Epoch 3/10
39/39 - 54s - loss: 778.1834 - loglik: -7.7830e+02 - logprior: 0.1185
Epoch 4/10
39/39 - 54s - loss: 775.9124 - loglik: -7.7605e+02 - logprior: 0.1384
Epoch 5/10
39/39 - 53s - loss: 770.6169 - loglik: -7.7085e+02 - logprior: 0.2325
Epoch 6/10
39/39 - 52s - loss: 771.0157 - loglik: -7.7130e+02 - logprior: 0.2824
Fitted a model with MAP estimate = -769.4137
Time for alignment: 1127.5900
Computed alignments with likelihoods: ['-766.5871', '-768.0004', '-767.8234', '-771.9325', '-769.4137']
Best model has likelihood: -766.5871  (prior= 0.7286 )
time for generating output: 0.9876
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.44343317649670516
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 891.6330 - loglik: -8.9018e+02 - logprior: -1.4524e+00
Epoch 2/10
39/39 - 56s - loss: 753.8360 - loglik: -7.5244e+02 - logprior: -1.3958e+00
Epoch 3/10
39/39 - 51s - loss: 743.2708 - loglik: -7.4175e+02 - logprior: -1.5183e+00
Epoch 4/10
39/39 - 50s - loss: 740.9644 - loglik: -7.3950e+02 - logprior: -1.4687e+00
Epoch 5/10
39/39 - 48s - loss: 738.9865 - loglik: -7.3752e+02 - logprior: -1.4688e+00
Epoch 6/10
39/39 - 49s - loss: 739.4385 - loglik: -7.3790e+02 - logprior: -1.5339e+00
Fitted a model with MAP estimate = -738.0371
expansions: [(9, 1), (19, 1), (21, 1), (68, 1), (70, 1), (102, 1), (106, 2), (107, 1), (110, 1), (117, 1), (122, 1), (141, 1), (143, 1), (144, 1), (145, 3), (146, 3), (161, 1), (163, 2), (164, 2), (168, 1), (177, 3), (178, 4), (181, 1), (182, 5), (183, 1), (185, 1), (186, 1), (188, 1), (189, 1), (190, 2), (191, 2), (192, 1), (203, 2), (204, 1), (210, 3), (218, 2), (219, 1), (220, 1), (225, 1), (227, 1), (233, 2), (236, 1), (239, 1), (241, 1), (245, 1), (248, 2), (249, 1), (255, 1), (276, 1), (277, 1), (278, 1), (281, 1), (282, 5), (290, 1), (296, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 412 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 726.2355 - loglik: -7.2466e+02 - logprior: -1.5781e+00
Epoch 2/2
39/39 - 67s - loss: 713.6378 - loglik: -7.1332e+02 - logprior: -3.1599e-01
Fitted a model with MAP estimate = -710.4831
expansions: [(306, 1)]
discards: [161 206 207 218 239 240 241 242 243 244 245 246 247 248 249 263 273 357
 358]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 721.3468 - loglik: -7.2015e+02 - logprior: -1.1928e+00
Epoch 2/2
39/39 - 68s - loss: 714.8392 - loglik: -7.1488e+02 - logprior: 0.0410
Fitted a model with MAP estimate = -712.5850
expansions: []
discards: [215 216 235 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 718.2234 - loglik: -7.1721e+02 - logprior: -1.0113e+00
Epoch 2/10
39/39 - 69s - loss: 714.8969 - loglik: -7.1518e+02 - logprior: 0.2828
Epoch 3/10
39/39 - 70s - loss: 712.5796 - loglik: -7.1306e+02 - logprior: 0.4823
Epoch 4/10
39/39 - 69s - loss: 711.8815 - loglik: -7.1238e+02 - logprior: 0.4997
Epoch 5/10
39/39 - 63s - loss: 711.2667 - loglik: -7.1185e+02 - logprior: 0.5866
Epoch 6/10
39/39 - 61s - loss: 710.5241 - loglik: -7.1141e+02 - logprior: 0.8863
Epoch 7/10
39/39 - 60s - loss: 710.8909 - loglik: -7.1190e+02 - logprior: 1.0093
Fitted a model with MAP estimate = -709.9785
Time for alignment: 1332.5100
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 892.5886 - loglik: -8.9113e+02 - logprior: -1.4592e+00
Epoch 2/10
39/39 - 45s - loss: 755.4603 - loglik: -7.5396e+02 - logprior: -1.4988e+00
Epoch 3/10
39/39 - 45s - loss: 743.6606 - loglik: -7.4201e+02 - logprior: -1.6532e+00
Epoch 4/10
39/39 - 46s - loss: 740.8486 - loglik: -7.3916e+02 - logprior: -1.6931e+00
Epoch 5/10
39/39 - 46s - loss: 739.5433 - loglik: -7.3788e+02 - logprior: -1.6636e+00
Epoch 6/10
39/39 - 47s - loss: 739.2072 - loglik: -7.3750e+02 - logprior: -1.7092e+00
Epoch 7/10
39/39 - 47s - loss: 739.1885 - loglik: -7.3748e+02 - logprior: -1.7125e+00
Epoch 8/10
39/39 - 47s - loss: 737.9798 - loglik: -7.3625e+02 - logprior: -1.7284e+00
Epoch 9/10
39/39 - 47s - loss: 738.9251 - loglik: -7.3722e+02 - logprior: -1.7077e+00
Fitted a model with MAP estimate = -737.6974
expansions: [(9, 1), (19, 1), (21, 1), (67, 1), (102, 2), (105, 1), (107, 1), (110, 1), (117, 1), (122, 1), (141, 1), (143, 1), (144, 1), (145, 3), (146, 3), (161, 2), (162, 2), (163, 1), (168, 1), (169, 1), (177, 1), (178, 5), (181, 1), (182, 5), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 2), (205, 2), (212, 1), (220, 2), (221, 1), (222, 1), (227, 1), (235, 1), (236, 2), (239, 1), (242, 1), (244, 1), (246, 2), (248, 1), (249, 1), (251, 1), (252, 1), (254, 1), (257, 1), (267, 1), (277, 1), (278, 4), (282, 1), (283, 2), (297, 1), (298, 1), (307, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1   2 192 193 194 195 196 197 198 199 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 732.4659 - loglik: -7.3080e+02 - logprior: -1.6627e+00
Epoch 2/2
39/39 - 68s - loss: 717.7404 - loglik: -7.1731e+02 - logprior: -4.3015e-01
Fitted a model with MAP estimate = -713.9962
expansions: [(1, 1), (4, 1), (237, 4), (341, 1)]
discards: [206 207 216 241 242 243 244 245 246 247 260 282 297 298 348]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 721.3353 - loglik: -7.2012e+02 - logprior: -1.2114e+00
Epoch 2/2
39/39 - 67s - loss: 715.8201 - loglik: -7.1590e+02 - logprior: 0.0762
Fitted a model with MAP estimate = -713.3262
expansions: [(291, 2)]
discards: [216 237 238 335 336]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 719.8720 - loglik: -7.1890e+02 - logprior: -9.7225e-01
Epoch 2/10
39/39 - 72s - loss: 716.2516 - loglik: -7.1660e+02 - logprior: 0.3459
Epoch 3/10
39/39 - 80s - loss: 713.8287 - loglik: -7.1431e+02 - logprior: 0.4769
Epoch 4/10
39/39 - 79s - loss: 712.1959 - loglik: -7.1280e+02 - logprior: 0.6000
Epoch 5/10
39/39 - 77s - loss: 711.7799 - loglik: -7.1244e+02 - logprior: 0.6572
Epoch 6/10
39/39 - 77s - loss: 711.5776 - loglik: -7.1241e+02 - logprior: 0.8362
Epoch 7/10
39/39 - 82s - loss: 711.5312 - loglik: -7.1248e+02 - logprior: 0.9443
Epoch 8/10
39/39 - 86s - loss: 711.2466 - loglik: -7.1224e+02 - logprior: 0.9916
Epoch 9/10
39/39 - 89s - loss: 711.1973 - loglik: -7.1246e+02 - logprior: 1.2658
Epoch 10/10
39/39 - 82s - loss: 710.0388 - loglik: -7.1139e+02 - logprior: 1.3506
Fitted a model with MAP estimate = -710.3433
Time for alignment: 1776.8325
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 893.2424 - loglik: -8.9178e+02 - logprior: -1.4584e+00
Epoch 2/10
39/39 - 57s - loss: 757.0428 - loglik: -7.5564e+02 - logprior: -1.4041e+00
Epoch 3/10
39/39 - 60s - loss: 745.1378 - loglik: -7.4359e+02 - logprior: -1.5445e+00
Epoch 4/10
39/39 - 64s - loss: 740.9998 - loglik: -7.3946e+02 - logprior: -1.5398e+00
Epoch 5/10
39/39 - 64s - loss: 739.7568 - loglik: -7.3819e+02 - logprior: -1.5635e+00
Epoch 6/10
39/39 - 63s - loss: 739.2934 - loglik: -7.3769e+02 - logprior: -1.5999e+00
Epoch 7/10
39/39 - 65s - loss: 738.9182 - loglik: -7.3730e+02 - logprior: -1.6148e+00
Epoch 8/10
39/39 - 64s - loss: 738.9623 - loglik: -7.3736e+02 - logprior: -1.6069e+00
Fitted a model with MAP estimate = -737.6704
expansions: [(9, 1), (19, 2), (20, 1), (66, 1), (77, 1), (100, 1), (103, 1), (104, 1), (105, 1), (108, 1), (115, 1), (120, 1), (139, 1), (141, 1), (142, 1), (143, 3), (144, 2), (145, 1), (160, 2), (161, 2), (162, 1), (163, 1), (167, 1), (176, 1), (177, 5), (180, 5), (181, 1), (182, 1), (184, 1), (185, 1), (189, 1), (190, 4), (218, 2), (219, 1), (220, 1), (227, 1), (233, 1), (234, 3), (236, 1), (239, 1), (241, 2), (242, 2), (251, 1), (252, 2), (255, 1), (268, 1), (277, 1), (278, 1), (281, 1), (282, 2), (283, 3), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 198 199 200 201 202 203 204 205 206 215]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 728.6006 - loglik: -7.2702e+02 - logprior: -1.5845e+00
Epoch 2/2
39/39 - 83s - loss: 714.7886 - loglik: -7.1447e+02 - logprior: -3.2034e-01
Fitted a model with MAP estimate = -711.6107
expansions: [(251, 1)]
discards: [160 162 208 209 218 219 256 278 293 343 344 345]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 720.5529 - loglik: -7.1940e+02 - logprior: -1.1540e+00
Epoch 2/2
39/39 - 78s - loss: 715.8336 - loglik: -7.1595e+02 - logprior: 0.1184
Fitted a model with MAP estimate = -713.6711
expansions: [(5, 1)]
discards: [213 287]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 86s - loss: 718.1768 - loglik: -7.1721e+02 - logprior: -9.7093e-01
Epoch 2/10
39/39 - 86s - loss: 714.2357 - loglik: -7.1459e+02 - logprior: 0.3579
Epoch 3/10
39/39 - 81s - loss: 713.8389 - loglik: -7.1424e+02 - logprior: 0.4024
Epoch 4/10
39/39 - 79s - loss: 711.9096 - loglik: -7.1253e+02 - logprior: 0.6211
Epoch 5/10
39/39 - 78s - loss: 710.2546 - loglik: -7.1102e+02 - logprior: 0.7621
Epoch 6/10
39/39 - 79s - loss: 710.3641 - loglik: -7.1118e+02 - logprior: 0.8138
Fitted a model with MAP estimate = -710.4514
Time for alignment: 1668.5200
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 893.6636 - loglik: -8.9222e+02 - logprior: -1.4486e+00
Epoch 2/10
39/39 - 44s - loss: 756.5019 - loglik: -7.5509e+02 - logprior: -1.4117e+00
Epoch 3/10
39/39 - 45s - loss: 745.4766 - loglik: -7.4397e+02 - logprior: -1.5020e+00
Epoch 4/10
39/39 - 47s - loss: 742.0970 - loglik: -7.4059e+02 - logprior: -1.5076e+00
Epoch 5/10
39/39 - 49s - loss: 741.5858 - loglik: -7.4010e+02 - logprior: -1.4895e+00
Epoch 6/10
39/39 - 51s - loss: 741.8013 - loglik: -7.4028e+02 - logprior: -1.5199e+00
Fitted a model with MAP estimate = -740.0055
expansions: [(9, 1), (19, 1), (21, 1), (64, 2), (102, 1), (106, 1), (110, 1), (118, 1), (124, 1), (143, 2), (144, 1), (145, 1), (147, 3), (148, 2), (163, 2), (164, 2), (165, 2), (169, 1), (176, 1), (177, 3), (178, 4), (181, 1), (182, 5), (183, 2), (185, 1), (186, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 2), (211, 3), (218, 2), (219, 1), (220, 1), (227, 1), (233, 3), (236, 1), (239, 1), (241, 2), (242, 2), (251, 1), (252, 2), (255, 1), (265, 1), (274, 1), (277, 3), (281, 1), (282, 2), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 192 193 194 195 196 197 198 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 401 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 734.8649 - loglik: -7.3239e+02 - logprior: -2.4757e+00
Epoch 2/2
39/39 - 73s - loss: 720.0976 - loglik: -7.1891e+02 - logprior: -1.1901e+00
Fitted a model with MAP estimate = -716.4909
expansions: [(4, 1), (5, 1), (234, 4), (235, 1), (244, 1)]
discards: [  0   1  67 205 206 217 218 251 260 295 296 297 340 346]
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 725.6567 - loglik: -7.2364e+02 - logprior: -2.0179e+00
Epoch 2/2
39/39 - 73s - loss: 718.8229 - loglik: -7.1853e+02 - logprior: -2.9021e-01
Fitted a model with MAP estimate = -714.9437
expansions: [(4, 1), (69, 1), (231, 3), (243, 2), (292, 1)]
discards: [  0   1   2 161 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 722.9277 - loglik: -7.2117e+02 - logprior: -1.7558e+00
Epoch 2/10
39/39 - 63s - loss: 717.8569 - loglik: -7.1724e+02 - logprior: -6.1832e-01
Epoch 3/10
39/39 - 63s - loss: 716.3588 - loglik: -7.1648e+02 - logprior: 0.1207
Epoch 4/10
39/39 - 65s - loss: 714.7211 - loglik: -7.1511e+02 - logprior: 0.3858
Epoch 5/10
39/39 - 68s - loss: 713.5373 - loglik: -7.1413e+02 - logprior: 0.5933
Epoch 6/10
39/39 - 70s - loss: 713.2230 - loglik: -7.1393e+02 - logprior: 0.7045
Epoch 7/10
39/39 - 70s - loss: 713.3386 - loglik: -7.1428e+02 - logprior: 0.9432
Fitted a model with MAP estimate = -712.4630
Time for alignment: 1366.5236
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 893.8311 - loglik: -8.9236e+02 - logprior: -1.4702e+00
Epoch 2/10
39/39 - 48s - loss: 756.9433 - loglik: -7.5548e+02 - logprior: -1.4625e+00
Epoch 3/10
39/39 - 47s - loss: 745.6920 - loglik: -7.4400e+02 - logprior: -1.6906e+00
Epoch 4/10
39/39 - 46s - loss: 742.0125 - loglik: -7.4036e+02 - logprior: -1.6484e+00
Epoch 5/10
39/39 - 46s - loss: 740.8090 - loglik: -7.3910e+02 - logprior: -1.7126e+00
Epoch 6/10
39/39 - 46s - loss: 741.0984 - loglik: -7.3937e+02 - logprior: -1.7280e+00
Fitted a model with MAP estimate = -739.4077
expansions: [(9, 1), (19, 2), (20, 1), (67, 1), (69, 1), (101, 2), (104, 1), (106, 1), (107, 1), (108, 1), (114, 1), (119, 1), (120, 1), (138, 2), (139, 1), (140, 1), (142, 3), (143, 2), (158, 2), (159, 1), (160, 1), (161, 1), (165, 1), (175, 1), (176, 5), (179, 5), (180, 1), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 2), (202, 2), (203, 1), (210, 2), (218, 2), (219, 1), (220, 1), (227, 1), (233, 1), (234, 2), (237, 1), (240, 1), (242, 1), (246, 1), (252, 1), (254, 1), (275, 1), (278, 1), (279, 3), (280, 1), (281, 2), (283, 1), (284, 1), (289, 1), (290, 2), (296, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195 196 197 205 206]
Re-initialized the encoder parameters.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 731.6332 - loglik: -7.2996e+02 - logprior: -1.6716e+00
Epoch 2/2
39/39 - 77s - loss: 716.6920 - loglik: -7.1628e+02 - logprior: -4.1003e-01
Fitted a model with MAP estimate = -713.4792
expansions: [(5, 1), (249, 1), (250, 1), (297, 1), (342, 1)]
discards: [208 209 218 241 242 243 257 263 285 343 344 345 346 360]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 720.8300 - loglik: -7.1959e+02 - logprior: -1.2374e+00
Epoch 2/2
39/39 - 82s - loss: 715.6095 - loglik: -7.1565e+02 - logprior: 0.0447
Fitted a model with MAP estimate = -712.6501
expansions: [(252, 2), (254, 1)]
discards: [104 213 214 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 718.1437 - loglik: -7.1714e+02 - logprior: -9.9998e-01
Epoch 2/10
39/39 - 84s - loss: 714.0874 - loglik: -7.1440e+02 - logprior: 0.3094
Epoch 3/10
39/39 - 83s - loss: 712.5193 - loglik: -7.1297e+02 - logprior: 0.4462
Epoch 4/10
39/39 - 79s - loss: 710.5732 - loglik: -7.1116e+02 - logprior: 0.5863
Epoch 5/10
39/39 - 83s - loss: 710.7164 - loglik: -7.1141e+02 - logprior: 0.6890
Fitted a model with MAP estimate = -709.6216
Time for alignment: 1345.9891
Computed alignments with likelihoods: ['-709.9785', '-710.3433', '-710.4514', '-712.4630', '-709.6216']
Best model has likelihood: -709.6216  (prior= 0.7459 )
time for generating output: 0.4668
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8021306160259379
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.7763 - loglik: -2.4972e+02 - logprior: -3.0605e+00
Epoch 2/10
19/19 - 2s - loss: 222.7547 - loglik: -2.2150e+02 - logprior: -1.2509e+00
Epoch 3/10
19/19 - 2s - loss: 208.8781 - loglik: -2.0743e+02 - logprior: -1.4446e+00
Epoch 4/10
19/19 - 2s - loss: 205.6900 - loglik: -2.0433e+02 - logprior: -1.3623e+00
Epoch 5/10
19/19 - 2s - loss: 204.7679 - loglik: -2.0340e+02 - logprior: -1.3678e+00
Epoch 6/10
19/19 - 2s - loss: 204.4773 - loglik: -2.0313e+02 - logprior: -1.3478e+00
Epoch 7/10
19/19 - 2s - loss: 204.4621 - loglik: -2.0312e+02 - logprior: -1.3415e+00
Epoch 8/10
19/19 - 2s - loss: 204.3646 - loglik: -2.0303e+02 - logprior: -1.3327e+00
Epoch 9/10
19/19 - 2s - loss: 204.3636 - loglik: -2.0303e+02 - logprior: -1.3310e+00
Epoch 10/10
19/19 - 2s - loss: 204.3766 - loglik: -2.0305e+02 - logprior: -1.3258e+00
Fitted a model with MAP estimate = -204.1480
expansions: [(9, 2), (10, 1), (11, 1), (13, 1), (19, 1), (20, 1), (23, 1), (26, 1), (37, 2), (39, 1), (40, 1), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5295 - loglik: -2.0366e+02 - logprior: -3.8693e+00
Epoch 2/2
19/19 - 2s - loss: 200.2549 - loglik: -1.9828e+02 - logprior: -1.9781e+00
Fitted a model with MAP estimate = -198.8700
expansions: [(0, 2), (55, 1)]
discards: [ 0  9 45 66]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 200.3274 - loglik: -1.9747e+02 - logprior: -2.8556e+00
Epoch 2/2
19/19 - 2s - loss: 197.0874 - loglik: -1.9601e+02 - logprior: -1.0771e+00
Fitted a model with MAP estimate = -196.4521
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.5065 - loglik: -1.9783e+02 - logprior: -3.6736e+00
Epoch 2/10
19/19 - 2s - loss: 197.4205 - loglik: -1.9626e+02 - logprior: -1.1570e+00
Epoch 3/10
19/19 - 2s - loss: 196.3784 - loglik: -1.9547e+02 - logprior: -9.1190e-01
Epoch 4/10
19/19 - 2s - loss: 195.9678 - loglik: -1.9509e+02 - logprior: -8.7775e-01
Epoch 5/10
19/19 - 2s - loss: 195.7588 - loglik: -1.9489e+02 - logprior: -8.6559e-01
Epoch 6/10
19/19 - 2s - loss: 195.3268 - loglik: -1.9448e+02 - logprior: -8.4507e-01
Epoch 7/10
19/19 - 2s - loss: 195.2437 - loglik: -1.9441e+02 - logprior: -8.3525e-01
Epoch 8/10
19/19 - 2s - loss: 195.0420 - loglik: -1.9422e+02 - logprior: -8.1887e-01
Epoch 9/10
19/19 - 2s - loss: 195.1665 - loglik: -1.9436e+02 - logprior: -8.0535e-01
Fitted a model with MAP estimate = -194.9390
Time for alignment: 66.8086
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.3176 - loglik: -2.4926e+02 - logprior: -3.0592e+00
Epoch 2/10
19/19 - 2s - loss: 220.5492 - loglik: -2.1931e+02 - logprior: -1.2355e+00
Epoch 3/10
19/19 - 2s - loss: 208.1088 - loglik: -2.0670e+02 - logprior: -1.4101e+00
Epoch 4/10
19/19 - 2s - loss: 205.6416 - loglik: -2.0430e+02 - logprior: -1.3389e+00
Epoch 5/10
19/19 - 2s - loss: 204.7715 - loglik: -2.0342e+02 - logprior: -1.3487e+00
Epoch 6/10
19/19 - 2s - loss: 204.7726 - loglik: -2.0345e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -204.3455
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (14, 1), (15, 1), (35, 1), (40, 2), (41, 3), (43, 2), (47, 3), (48, 1), (49, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 206.9843 - loglik: -2.0311e+02 - logprior: -3.8773e+00
Epoch 2/2
19/19 - 2s - loss: 199.9638 - loglik: -1.9799e+02 - logprior: -1.9714e+00
Fitted a model with MAP estimate = -198.4961
expansions: [(0, 2)]
discards: [ 0 10 49 52 63]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.4713 - loglik: -1.9760e+02 - logprior: -2.8672e+00
Epoch 2/2
19/19 - 2s - loss: 197.3871 - loglik: -1.9630e+02 - logprior: -1.0897e+00
Fitted a model with MAP estimate = -196.7019
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 201.7620 - loglik: -1.9805e+02 - logprior: -3.7089e+00
Epoch 2/10
19/19 - 2s - loss: 197.3429 - loglik: -1.9616e+02 - logprior: -1.1790e+00
Epoch 3/10
19/19 - 2s - loss: 196.5632 - loglik: -1.9564e+02 - logprior: -9.2265e-01
Epoch 4/10
19/19 - 2s - loss: 195.9069 - loglik: -1.9502e+02 - logprior: -8.9142e-01
Epoch 5/10
19/19 - 2s - loss: 195.7429 - loglik: -1.9487e+02 - logprior: -8.7711e-01
Epoch 6/10
19/19 - 2s - loss: 195.1878 - loglik: -1.9433e+02 - logprior: -8.5689e-01
Epoch 7/10
19/19 - 2s - loss: 195.3797 - loglik: -1.9453e+02 - logprior: -8.4910e-01
Fitted a model with MAP estimate = -195.0549
Time for alignment: 55.1215
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.8434 - loglik: -2.4978e+02 - logprior: -3.0620e+00
Epoch 2/10
19/19 - 2s - loss: 220.9385 - loglik: -2.1966e+02 - logprior: -1.2794e+00
Epoch 3/10
19/19 - 2s - loss: 206.3536 - loglik: -2.0487e+02 - logprior: -1.4875e+00
Epoch 4/10
19/19 - 2s - loss: 203.8870 - loglik: -2.0249e+02 - logprior: -1.3926e+00
Epoch 5/10
19/19 - 2s - loss: 203.2185 - loglik: -2.0182e+02 - logprior: -1.3948e+00
Epoch 6/10
19/19 - 1s - loss: 202.9812 - loglik: -2.0162e+02 - logprior: -1.3641e+00
Epoch 7/10
19/19 - 2s - loss: 202.9109 - loglik: -2.0155e+02 - logprior: -1.3601e+00
Epoch 8/10
19/19 - 2s - loss: 202.6524 - loglik: -2.0130e+02 - logprior: -1.3527e+00
Epoch 9/10
19/19 - 2s - loss: 202.8718 - loglik: -2.0152e+02 - logprior: -1.3501e+00
Fitted a model with MAP estimate = -202.5551
expansions: [(9, 2), (10, 1), (11, 1), (14, 1), (15, 1), (20, 1), (21, 1), (35, 1), (37, 2), (39, 1), (40, 2), (43, 4), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5970 - loglik: -2.0372e+02 - logprior: -3.8805e+00
Epoch 2/2
19/19 - 2s - loss: 200.0381 - loglik: -1.9803e+02 - logprior: -2.0032e+00
Fitted a model with MAP estimate = -198.7140
expansions: [(0, 2)]
discards: [ 0  9 45 52 67 69]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.1706 - loglik: -1.9732e+02 - logprior: -2.8514e+00
Epoch 2/2
19/19 - 2s - loss: 197.1115 - loglik: -1.9603e+02 - logprior: -1.0811e+00
Fitted a model with MAP estimate = -196.4617
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 201.4813 - loglik: -1.9780e+02 - logprior: -3.6776e+00
Epoch 2/10
19/19 - 2s - loss: 197.4014 - loglik: -1.9624e+02 - logprior: -1.1573e+00
Epoch 3/10
19/19 - 2s - loss: 196.3932 - loglik: -1.9548e+02 - logprior: -9.1265e-01
Epoch 4/10
19/19 - 2s - loss: 196.0969 - loglik: -1.9522e+02 - logprior: -8.7715e-01
Epoch 5/10
19/19 - 2s - loss: 195.5884 - loglik: -1.9473e+02 - logprior: -8.6162e-01
Epoch 6/10
19/19 - 2s - loss: 195.4283 - loglik: -1.9458e+02 - logprior: -8.4843e-01
Epoch 7/10
19/19 - 2s - loss: 195.1355 - loglik: -1.9430e+02 - logprior: -8.3791e-01
Epoch 8/10
19/19 - 2s - loss: 195.1017 - loglik: -1.9428e+02 - logprior: -8.1915e-01
Epoch 9/10
19/19 - 2s - loss: 195.0518 - loglik: -1.9425e+02 - logprior: -8.0646e-01
Epoch 10/10
19/19 - 2s - loss: 194.9585 - loglik: -1.9416e+02 - logprior: -7.9485e-01
Fitted a model with MAP estimate = -194.9035
Time for alignment: 65.3047
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.6392 - loglik: -2.4958e+02 - logprior: -3.0613e+00
Epoch 2/10
19/19 - 2s - loss: 221.4194 - loglik: -2.2017e+02 - logprior: -1.2500e+00
Epoch 3/10
19/19 - 2s - loss: 206.6150 - loglik: -2.0515e+02 - logprior: -1.4602e+00
Epoch 4/10
19/19 - 2s - loss: 204.2275 - loglik: -2.0284e+02 - logprior: -1.3861e+00
Epoch 5/10
19/19 - 2s - loss: 203.0055 - loglik: -2.0159e+02 - logprior: -1.4189e+00
Epoch 6/10
19/19 - 2s - loss: 202.8806 - loglik: -2.0148e+02 - logprior: -1.3973e+00
Epoch 7/10
19/19 - 2s - loss: 202.6699 - loglik: -2.0128e+02 - logprior: -1.3863e+00
Epoch 8/10
19/19 - 2s - loss: 202.5229 - loglik: -2.0115e+02 - logprior: -1.3763e+00
Epoch 9/10
19/19 - 2s - loss: 202.6254 - loglik: -2.0125e+02 - logprior: -1.3726e+00
Fitted a model with MAP estimate = -202.3589
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (24, 1), (34, 1), (39, 1), (40, 1), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.3642 - loglik: -2.0350e+02 - logprior: -3.8682e+00
Epoch 2/2
19/19 - 2s - loss: 200.2475 - loglik: -1.9827e+02 - logprior: -1.9741e+00
Fitted a model with MAP estimate = -198.7775
expansions: [(0, 2)]
discards: [ 0 10 64 83]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.5036 - loglik: -1.9765e+02 - logprior: -2.8550e+00
Epoch 2/2
19/19 - 2s - loss: 197.3443 - loglik: -1.9626e+02 - logprior: -1.0888e+00
Fitted a model with MAP estimate = -196.7123
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.7336 - loglik: -1.9803e+02 - logprior: -3.7054e+00
Epoch 2/10
19/19 - 2s - loss: 197.4705 - loglik: -1.9629e+02 - logprior: -1.1800e+00
Epoch 3/10
19/19 - 2s - loss: 196.4682 - loglik: -1.9556e+02 - logprior: -9.1104e-01
Epoch 4/10
19/19 - 2s - loss: 195.9846 - loglik: -1.9510e+02 - logprior: -8.8424e-01
Epoch 5/10
19/19 - 2s - loss: 195.7482 - loglik: -1.9488e+02 - logprior: -8.6398e-01
Epoch 6/10
19/19 - 2s - loss: 195.5195 - loglik: -1.9467e+02 - logprior: -8.4722e-01
Epoch 7/10
19/19 - 2s - loss: 195.0825 - loglik: -1.9425e+02 - logprior: -8.3210e-01
Epoch 8/10
19/19 - 2s - loss: 195.1244 - loglik: -1.9430e+02 - logprior: -8.2515e-01
Fitted a model with MAP estimate = -194.9896
Time for alignment: 62.0477
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.6560 - loglik: -2.4959e+02 - logprior: -3.0611e+00
Epoch 2/10
19/19 - 2s - loss: 221.0554 - loglik: -2.1977e+02 - logprior: -1.2851e+00
Epoch 3/10
19/19 - 1s - loss: 206.0068 - loglik: -2.0448e+02 - logprior: -1.5284e+00
Epoch 4/10
19/19 - 2s - loss: 203.6841 - loglik: -2.0225e+02 - logprior: -1.4341e+00
Epoch 5/10
19/19 - 1s - loss: 203.1296 - loglik: -2.0170e+02 - logprior: -1.4331e+00
Epoch 6/10
19/19 - 2s - loss: 202.8014 - loglik: -2.0140e+02 - logprior: -1.3981e+00
Epoch 7/10
19/19 - 1s - loss: 202.6739 - loglik: -2.0128e+02 - logprior: -1.3900e+00
Epoch 8/10
19/19 - 2s - loss: 202.5206 - loglik: -2.0114e+02 - logprior: -1.3827e+00
Epoch 9/10
19/19 - 1s - loss: 202.5408 - loglik: -2.0116e+02 - logprior: -1.3767e+00
Fitted a model with MAP estimate = -202.3637
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (20, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (50, 2), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.8309 - loglik: -2.0393e+02 - logprior: -3.8988e+00
Epoch 2/2
19/19 - 2s - loss: 200.2413 - loglik: -1.9822e+02 - logprior: -2.0196e+00
Fitted a model with MAP estimate = -198.8074
expansions: [(0, 2)]
discards: [ 0 10 65 67 85]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.4664 - loglik: -1.9760e+02 - logprior: -2.8646e+00
Epoch 2/2
19/19 - 2s - loss: 197.2534 - loglik: -1.9616e+02 - logprior: -1.0946e+00
Fitted a model with MAP estimate = -196.6800
expansions: []
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.6783 - loglik: -1.9800e+02 - logprior: -3.6822e+00
Epoch 2/10
19/19 - 2s - loss: 197.6083 - loglik: -1.9646e+02 - logprior: -1.1440e+00
Epoch 3/10
19/19 - 2s - loss: 196.6972 - loglik: -1.9577e+02 - logprior: -9.2834e-01
Epoch 4/10
19/19 - 2s - loss: 196.2942 - loglik: -1.9541e+02 - logprior: -8.8826e-01
Epoch 5/10
19/19 - 2s - loss: 195.8421 - loglik: -1.9497e+02 - logprior: -8.6983e-01
Epoch 6/10
19/19 - 2s - loss: 195.5699 - loglik: -1.9471e+02 - logprior: -8.5836e-01
Epoch 7/10
19/19 - 2s - loss: 195.4451 - loglik: -1.9460e+02 - logprior: -8.4561e-01
Epoch 8/10
19/19 - 2s - loss: 195.4038 - loglik: -1.9458e+02 - logprior: -8.2513e-01
Epoch 9/10
19/19 - 2s - loss: 195.3274 - loglik: -1.9451e+02 - logprior: -8.1628e-01
Epoch 10/10
19/19 - 2s - loss: 195.2888 - loglik: -1.9449e+02 - logprior: -7.9951e-01
Fitted a model with MAP estimate = -195.1583
Time for alignment: 65.0885
Computed alignments with likelihoods: ['-194.9390', '-195.0549', '-194.9035', '-194.9896', '-195.1583']
Best model has likelihood: -194.9035  (prior= -0.7843 )
time for generating output: 0.1276
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.8272425249169435
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.5832 - loglik: -2.9857e+02 - logprior: -3.0145e+00
Epoch 2/10
19/19 - 2s - loss: 271.1256 - loglik: -2.6993e+02 - logprior: -1.1927e+00
Epoch 3/10
19/19 - 2s - loss: 259.8533 - loglik: -2.5876e+02 - logprior: -1.0907e+00
Epoch 4/10
19/19 - 2s - loss: 257.3873 - loglik: -2.5639e+02 - logprior: -9.9900e-01
Epoch 5/10
19/19 - 2s - loss: 256.3118 - loglik: -2.5536e+02 - logprior: -9.5110e-01
Epoch 6/10
19/19 - 2s - loss: 255.6822 - loglik: -2.5474e+02 - logprior: -9.4695e-01
Epoch 7/10
19/19 - 2s - loss: 256.0096 - loglik: -2.5507e+02 - logprior: -9.4119e-01
Fitted a model with MAP estimate = -254.8715
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (39, 2), (41, 1), (42, 1), (54, 4), (74, 1), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.2717 - loglik: -2.5620e+02 - logprior: -4.0739e+00
Epoch 2/2
19/19 - 3s - loss: 253.8150 - loglik: -2.5261e+02 - logprior: -1.2075e+00
Fitted a model with MAP estimate = -251.9352
expansions: [(69, 1), (70, 2), (71, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.0448 - loglik: -2.5214e+02 - logprior: -2.9075e+00
Epoch 2/2
19/19 - 3s - loss: 251.8815 - loglik: -2.5087e+02 - logprior: -1.0138e+00
Fitted a model with MAP estimate = -250.5035
expansions: []
discards: [ 71  97 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.2820 - loglik: -2.5138e+02 - logprior: -2.9062e+00
Epoch 2/10
19/19 - 2s - loss: 251.3670 - loglik: -2.5040e+02 - logprior: -9.6470e-01
Epoch 3/10
19/19 - 3s - loss: 250.6013 - loglik: -2.4984e+02 - logprior: -7.6189e-01
Epoch 4/10
19/19 - 3s - loss: 249.6223 - loglik: -2.4890e+02 - logprior: -7.1778e-01
Epoch 5/10
19/19 - 3s - loss: 249.0142 - loglik: -2.4833e+02 - logprior: -6.7972e-01
Epoch 6/10
19/19 - 3s - loss: 248.7556 - loglik: -2.4809e+02 - logprior: -6.6337e-01
Epoch 7/10
19/19 - 3s - loss: 248.3866 - loglik: -2.4773e+02 - logprior: -6.5621e-01
Epoch 8/10
19/19 - 3s - loss: 248.0590 - loglik: -2.4742e+02 - logprior: -6.4099e-01
Epoch 9/10
19/19 - 3s - loss: 247.7073 - loglik: -2.4708e+02 - logprior: -6.2749e-01
Epoch 10/10
19/19 - 3s - loss: 247.8445 - loglik: -2.4724e+02 - logprior: -6.0298e-01
Fitted a model with MAP estimate = -247.7136
Time for alignment: 81.2011
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 301.6033 - loglik: -2.9858e+02 - logprior: -3.0228e+00
Epoch 2/10
19/19 - 2s - loss: 269.8199 - loglik: -2.6864e+02 - logprior: -1.1799e+00
Epoch 3/10
19/19 - 2s - loss: 259.5129 - loglik: -2.5843e+02 - logprior: -1.0798e+00
Epoch 4/10
19/19 - 2s - loss: 256.6889 - loglik: -2.5570e+02 - logprior: -9.8886e-01
Epoch 5/10
19/19 - 2s - loss: 255.4453 - loglik: -2.5445e+02 - logprior: -9.9670e-01
Epoch 6/10
19/19 - 2s - loss: 254.8644 - loglik: -2.5387e+02 - logprior: -9.9039e-01
Epoch 7/10
19/19 - 2s - loss: 254.1499 - loglik: -2.5315e+02 - logprior: -9.9621e-01
Epoch 8/10
19/19 - 2s - loss: 254.2743 - loglik: -2.5329e+02 - logprior: -9.8485e-01
Fitted a model with MAP estimate = -253.5852
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (39, 2), (42, 2), (44, 1), (45, 1), (52, 2), (53, 2), (54, 1), (74, 1), (75, 1), (76, 2), (77, 4), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.1028 - loglik: -2.5501e+02 - logprior: -4.0928e+00
Epoch 2/2
19/19 - 3s - loss: 252.9888 - loglik: -2.5181e+02 - logprior: -1.1782e+00
Fitted a model with MAP estimate = -251.1827
expansions: []
discards: [ 1 50 72 99]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.8590 - loglik: -2.5203e+02 - logprior: -2.8276e+00
Epoch 2/2
19/19 - 3s - loss: 252.0115 - loglik: -2.5102e+02 - logprior: -9.8957e-01
Fitted a model with MAP estimate = -250.9342
expansions: []
discards: [26]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 253.8963 - loglik: -2.5113e+02 - logprior: -2.7618e+00
Epoch 2/10
19/19 - 2s - loss: 251.3894 - loglik: -2.5044e+02 - logprior: -9.4729e-01
Epoch 3/10
19/19 - 3s - loss: 250.9241 - loglik: -2.5013e+02 - logprior: -7.9275e-01
Epoch 4/10
19/19 - 3s - loss: 249.8763 - loglik: -2.4914e+02 - logprior: -7.3740e-01
Epoch 5/10
19/19 - 3s - loss: 249.6543 - loglik: -2.4895e+02 - logprior: -7.0932e-01
Epoch 6/10
19/19 - 3s - loss: 248.4641 - loglik: -2.4780e+02 - logprior: -6.6531e-01
Epoch 7/10
19/19 - 3s - loss: 248.3773 - loglik: -2.4774e+02 - logprior: -6.4137e-01
Epoch 8/10
19/19 - 3s - loss: 248.2382 - loglik: -2.4762e+02 - logprior: -6.1445e-01
Epoch 9/10
19/19 - 3s - loss: 247.7618 - loglik: -2.4715e+02 - logprior: -6.1568e-01
Epoch 10/10
19/19 - 3s - loss: 248.2218 - loglik: -2.4764e+02 - logprior: -5.8677e-01
Fitted a model with MAP estimate = -247.8034
Time for alignment: 82.0430
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.4320 - loglik: -2.9840e+02 - logprior: -3.0276e+00
Epoch 2/10
19/19 - 2s - loss: 271.4762 - loglik: -2.7027e+02 - logprior: -1.2105e+00
Epoch 3/10
19/19 - 2s - loss: 258.5913 - loglik: -2.5747e+02 - logprior: -1.1224e+00
Epoch 4/10
19/19 - 2s - loss: 255.9381 - loglik: -2.5492e+02 - logprior: -1.0176e+00
Epoch 5/10
19/19 - 2s - loss: 254.9654 - loglik: -2.5399e+02 - logprior: -9.8024e-01
Epoch 6/10
19/19 - 2s - loss: 254.4230 - loglik: -2.5346e+02 - logprior: -9.6323e-01
Epoch 7/10
19/19 - 2s - loss: 253.9113 - loglik: -2.5295e+02 - logprior: -9.5835e-01
Epoch 8/10
19/19 - 2s - loss: 254.2601 - loglik: -2.5330e+02 - logprior: -9.5815e-01
Fitted a model with MAP estimate = -253.2535
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 2), (41, 2), (42, 2), (43, 1), (51, 1), (52, 1), (53, 1), (75, 3), (76, 2), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.3480 - loglik: -2.5524e+02 - logprior: -4.1126e+00
Epoch 2/2
19/19 - 3s - loss: 253.1435 - loglik: -2.5194e+02 - logprior: -1.2016e+00
Fitted a model with MAP estimate = -251.3913
expansions: []
discards: [ 1 27 56]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.9658 - loglik: -2.5205e+02 - logprior: -2.9165e+00
Epoch 2/2
19/19 - 2s - loss: 252.0425 - loglik: -2.5103e+02 - logprior: -1.0086e+00
Fitted a model with MAP estimate = -250.8677
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.1623 - loglik: -2.5125e+02 - logprior: -2.9170e+00
Epoch 2/10
19/19 - 2s - loss: 251.1761 - loglik: -2.5021e+02 - logprior: -9.6543e-01
Epoch 3/10
19/19 - 2s - loss: 251.0335 - loglik: -2.5027e+02 - logprior: -7.6582e-01
Epoch 4/10
19/19 - 2s - loss: 250.0709 - loglik: -2.4936e+02 - logprior: -7.1395e-01
Epoch 5/10
19/19 - 2s - loss: 249.2036 - loglik: -2.4852e+02 - logprior: -6.8291e-01
Epoch 6/10
19/19 - 3s - loss: 248.7771 - loglik: -2.4812e+02 - logprior: -6.5541e-01
Epoch 7/10
19/19 - 2s - loss: 248.4583 - loglik: -2.4780e+02 - logprior: -6.5767e-01
Epoch 8/10
19/19 - 3s - loss: 248.3210 - loglik: -2.4768e+02 - logprior: -6.4230e-01
Epoch 9/10
19/19 - 3s - loss: 248.2544 - loglik: -2.4763e+02 - logprior: -6.2020e-01
Epoch 10/10
19/19 - 3s - loss: 247.9617 - loglik: -2.4735e+02 - logprior: -6.0883e-01
Fitted a model with MAP estimate = -247.9417
Time for alignment: 80.6535
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7643 - loglik: -2.9874e+02 - logprior: -3.0259e+00
Epoch 2/10
19/19 - 2s - loss: 271.3111 - loglik: -2.7011e+02 - logprior: -1.2057e+00
Epoch 3/10
19/19 - 2s - loss: 259.3182 - loglik: -2.5820e+02 - logprior: -1.1185e+00
Epoch 4/10
19/19 - 2s - loss: 256.8119 - loglik: -2.5580e+02 - logprior: -1.0076e+00
Epoch 5/10
19/19 - 2s - loss: 255.5367 - loglik: -2.5455e+02 - logprior: -9.8919e-01
Epoch 6/10
19/19 - 2s - loss: 255.2558 - loglik: -2.5427e+02 - logprior: -9.8805e-01
Epoch 7/10
19/19 - 2s - loss: 254.3533 - loglik: -2.5337e+02 - logprior: -9.8228e-01
Epoch 8/10
19/19 - 2s - loss: 254.6721 - loglik: -2.5369e+02 - logprior: -9.8417e-01
Fitted a model with MAP estimate = -253.8284
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (41, 3), (42, 1), (44, 1), (45, 1), (51, 1), (52, 1), (53, 1), (73, 1), (74, 1), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.4561 - loglik: -2.5537e+02 - logprior: -4.0897e+00
Epoch 2/2
19/19 - 3s - loss: 253.3988 - loglik: -2.5223e+02 - logprior: -1.1732e+00
Fitted a model with MAP estimate = -251.8273
expansions: []
discards: [ 1  2 96]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.6677 - loglik: -2.5276e+02 - logprior: -2.9086e+00
Epoch 2/2
19/19 - 2s - loss: 252.5209 - loglik: -2.5153e+02 - logprior: -9.8974e-01
Fitted a model with MAP estimate = -251.2894
expansions: [(0, 1), (50, 1)]
discards: [25 57]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 256.0209 - loglik: -2.5157e+02 - logprior: -4.4504e+00
Epoch 2/10
19/19 - 3s - loss: 251.7876 - loglik: -2.5058e+02 - logprior: -1.2060e+00
Epoch 3/10
19/19 - 3s - loss: 250.9070 - loglik: -2.5008e+02 - logprior: -8.2992e-01
Epoch 4/10
19/19 - 3s - loss: 249.7195 - loglik: -2.4897e+02 - logprior: -7.5188e-01
Epoch 5/10
19/19 - 3s - loss: 249.6406 - loglik: -2.4895e+02 - logprior: -6.9529e-01
Epoch 6/10
19/19 - 3s - loss: 248.6049 - loglik: -2.4792e+02 - logprior: -6.8007e-01
Epoch 7/10
19/19 - 3s - loss: 248.1740 - loglik: -2.4751e+02 - logprior: -6.6293e-01
Epoch 8/10
19/19 - 3s - loss: 248.4184 - loglik: -2.4777e+02 - logprior: -6.4635e-01
Fitted a model with MAP estimate = -247.9866
Time for alignment: 76.3632
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.6991 - loglik: -2.9867e+02 - logprior: -3.0257e+00
Epoch 2/10
19/19 - 2s - loss: 273.8254 - loglik: -2.7262e+02 - logprior: -1.2027e+00
Epoch 3/10
19/19 - 2s - loss: 259.6645 - loglik: -2.5857e+02 - logprior: -1.0956e+00
Epoch 4/10
19/19 - 2s - loss: 256.8401 - loglik: -2.5585e+02 - logprior: -9.9142e-01
Epoch 5/10
19/19 - 2s - loss: 255.4333 - loglik: -2.5447e+02 - logprior: -9.6519e-01
Epoch 6/10
19/19 - 2s - loss: 254.6150 - loglik: -2.5365e+02 - logprior: -9.6320e-01
Epoch 7/10
19/19 - 2s - loss: 254.6147 - loglik: -2.5366e+02 - logprior: -9.5275e-01
Epoch 8/10
19/19 - 2s - loss: 254.4519 - loglik: -2.5349e+02 - logprior: -9.5836e-01
Epoch 9/10
19/19 - 2s - loss: 254.1151 - loglik: -2.5316e+02 - logprior: -9.5718e-01
Epoch 10/10
19/19 - 2s - loss: 254.6172 - loglik: -2.5367e+02 - logprior: -9.4473e-01
Fitted a model with MAP estimate = -253.5527
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (41, 3), (42, 1), (44, 2), (53, 3), (75, 3), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 260.1974 - loglik: -2.5605e+02 - logprior: -4.1491e+00
Epoch 2/2
19/19 - 3s - loss: 253.2832 - loglik: -2.5208e+02 - logprior: -1.2020e+00
Fitted a model with MAP estimate = -251.6358
expansions: [(52, 1)]
discards: [  1   2  58 101]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.3728 - loglik: -2.5244e+02 - logprior: -2.9372e+00
Epoch 2/2
19/19 - 2s - loss: 252.5739 - loglik: -2.5156e+02 - logprior: -1.0116e+00
Fitted a model with MAP estimate = -251.2285
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.9887 - loglik: -2.5154e+02 - logprior: -4.4459e+00
Epoch 2/10
19/19 - 2s - loss: 251.9084 - loglik: -2.5067e+02 - logprior: -1.2379e+00
Epoch 3/10
19/19 - 2s - loss: 250.7961 - loglik: -2.4991e+02 - logprior: -8.8404e-01
Epoch 4/10
19/19 - 2s - loss: 250.1718 - loglik: -2.4939e+02 - logprior: -7.7989e-01
Epoch 5/10
19/19 - 2s - loss: 249.5837 - loglik: -2.4884e+02 - logprior: -7.3898e-01
Epoch 6/10
19/19 - 2s - loss: 248.6790 - loglik: -2.4797e+02 - logprior: -7.0968e-01
Epoch 7/10
19/19 - 2s - loss: 248.4806 - loglik: -2.4778e+02 - logprior: -6.9945e-01
Epoch 8/10
19/19 - 2s - loss: 248.2183 - loglik: -2.4753e+02 - logprior: -6.8420e-01
Epoch 9/10
19/19 - 2s - loss: 248.0194 - loglik: -2.4736e+02 - logprior: -6.6369e-01
Epoch 10/10
19/19 - 2s - loss: 248.1545 - loglik: -2.4751e+02 - logprior: -6.4924e-01
Fitted a model with MAP estimate = -247.9213
Time for alignment: 81.7004
Computed alignments with likelihoods: ['-247.7136', '-247.8034', '-247.9417', '-247.9866', '-247.9213']
Best model has likelihood: -247.7136  (prior= -0.6082 )
time for generating output: 0.1402
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8849597471973516
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3051 - loglik: -1.8310e+02 - logprior: -3.2010e+00
Epoch 2/10
19/19 - 1s - loss: 155.4902 - loglik: -1.5399e+02 - logprior: -1.5012e+00
Epoch 3/10
19/19 - 1s - loss: 142.0355 - loglik: -1.4045e+02 - logprior: -1.5810e+00
Epoch 4/10
19/19 - 1s - loss: 138.2187 - loglik: -1.3666e+02 - logprior: -1.5586e+00
Epoch 5/10
19/19 - 1s - loss: 137.3220 - loglik: -1.3580e+02 - logprior: -1.5179e+00
Epoch 6/10
19/19 - 1s - loss: 136.8529 - loglik: -1.3533e+02 - logprior: -1.5201e+00
Epoch 7/10
19/19 - 1s - loss: 136.4527 - loglik: -1.3494e+02 - logprior: -1.5119e+00
Epoch 8/10
19/19 - 1s - loss: 136.5191 - loglik: -1.3502e+02 - logprior: -1.5010e+00
Fitted a model with MAP estimate = -136.1908
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 135.9097 - loglik: -1.3261e+02 - logprior: -3.2987e+00
Epoch 2/2
19/19 - 1s - loss: 128.2395 - loglik: -1.2692e+02 - logprior: -1.3149e+00
Fitted a model with MAP estimate = -127.3083
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.6808 - loglik: -1.2751e+02 - logprior: -3.1751e+00
Epoch 2/2
19/19 - 1s - loss: 127.8272 - loglik: -1.2659e+02 - logprior: -1.2373e+00
Fitted a model with MAP estimate = -127.0131
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.5029 - loglik: -1.2736e+02 - logprior: -3.1400e+00
Epoch 2/10
19/19 - 1s - loss: 127.6115 - loglik: -1.2639e+02 - logprior: -1.2165e+00
Epoch 3/10
19/19 - 1s - loss: 126.7047 - loglik: -1.2563e+02 - logprior: -1.0734e+00
Epoch 4/10
19/19 - 1s - loss: 126.3159 - loglik: -1.2529e+02 - logprior: -1.0260e+00
Epoch 5/10
19/19 - 1s - loss: 125.3849 - loglik: -1.2437e+02 - logprior: -1.0193e+00
Epoch 6/10
19/19 - 1s - loss: 125.2637 - loglik: -1.2426e+02 - logprior: -1.0034e+00
Epoch 7/10
19/19 - 1s - loss: 125.0606 - loglik: -1.2406e+02 - logprior: -9.9736e-01
Epoch 8/10
19/19 - 1s - loss: 124.7077 - loglik: -1.2373e+02 - logprior: -9.7828e-01
Epoch 9/10
19/19 - 1s - loss: 124.6343 - loglik: -1.2366e+02 - logprior: -9.7186e-01
Epoch 10/10
19/19 - 1s - loss: 124.7321 - loglik: -1.2378e+02 - logprior: -9.5283e-01
Fitted a model with MAP estimate = -124.5233
Time for alignment: 46.9667
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.2153 - loglik: -1.8301e+02 - logprior: -3.2062e+00
Epoch 2/10
19/19 - 1s - loss: 152.4860 - loglik: -1.5086e+02 - logprior: -1.6225e+00
Epoch 3/10
19/19 - 1s - loss: 140.7597 - loglik: -1.3923e+02 - logprior: -1.5251e+00
Epoch 4/10
19/19 - 1s - loss: 137.8395 - loglik: -1.3626e+02 - logprior: -1.5811e+00
Epoch 5/10
19/19 - 1s - loss: 136.5653 - loglik: -1.3501e+02 - logprior: -1.5583e+00
Epoch 6/10
19/19 - 1s - loss: 135.9472 - loglik: -1.3438e+02 - logprior: -1.5626e+00
Epoch 7/10
19/19 - 1s - loss: 135.4621 - loglik: -1.3392e+02 - logprior: -1.5397e+00
Epoch 8/10
19/19 - 1s - loss: 135.7660 - loglik: -1.3424e+02 - logprior: -1.5232e+00
Fitted a model with MAP estimate = -135.3356
expansions: [(16, 1), (17, 1), (18, 1), (21, 2), (22, 1), (23, 1), (25, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 135.7649 - loglik: -1.3247e+02 - logprior: -3.2974e+00
Epoch 2/2
19/19 - 1s - loss: 128.2040 - loglik: -1.2690e+02 - logprior: -1.3022e+00
Fitted a model with MAP estimate = -127.2667
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.5429 - loglik: -1.2736e+02 - logprior: -3.1853e+00
Epoch 2/10
19/19 - 1s - loss: 127.5747 - loglik: -1.2632e+02 - logprior: -1.2576e+00
Epoch 3/10
19/19 - 1s - loss: 126.8530 - loglik: -1.2574e+02 - logprior: -1.1167e+00
Epoch 4/10
19/19 - 1s - loss: 126.1901 - loglik: -1.2512e+02 - logprior: -1.0735e+00
Epoch 5/10
19/19 - 1s - loss: 125.6462 - loglik: -1.2458e+02 - logprior: -1.0710e+00
Epoch 6/10
19/19 - 1s - loss: 125.2125 - loglik: -1.2417e+02 - logprior: -1.0471e+00
Epoch 7/10
19/19 - 1s - loss: 124.9028 - loglik: -1.2386e+02 - logprior: -1.0450e+00
Epoch 8/10
19/19 - 1s - loss: 124.6212 - loglik: -1.2359e+02 - logprior: -1.0343e+00
Epoch 9/10
19/19 - 1s - loss: 124.7000 - loglik: -1.2368e+02 - logprior: -1.0211e+00
Fitted a model with MAP estimate = -124.5659
Time for alignment: 36.7420
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.4001 - loglik: -1.8320e+02 - logprior: -3.2039e+00
Epoch 2/10
19/19 - 1s - loss: 153.0998 - loglik: -1.5149e+02 - logprior: -1.6116e+00
Epoch 3/10
19/19 - 1s - loss: 141.6071 - loglik: -1.4011e+02 - logprior: -1.4970e+00
Epoch 4/10
19/19 - 1s - loss: 138.0391 - loglik: -1.3649e+02 - logprior: -1.5508e+00
Epoch 5/10
19/19 - 1s - loss: 136.9502 - loglik: -1.3542e+02 - logprior: -1.5270e+00
Epoch 6/10
19/19 - 1s - loss: 136.1454 - loglik: -1.3461e+02 - logprior: -1.5338e+00
Epoch 7/10
19/19 - 1s - loss: 135.6872 - loglik: -1.3415e+02 - logprior: -1.5345e+00
Epoch 8/10
19/19 - 1s - loss: 135.8967 - loglik: -1.3438e+02 - logprior: -1.5132e+00
Fitted a model with MAP estimate = -135.4830
expansions: [(14, 1), (16, 1), (17, 1), (22, 2), (23, 2), (25, 1), (33, 1), (36, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 135.6224 - loglik: -1.3231e+02 - logprior: -3.3098e+00
Epoch 2/2
19/19 - 1s - loss: 128.1689 - loglik: -1.2686e+02 - logprior: -1.3121e+00
Fitted a model with MAP estimate = -127.2649
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.5544 - loglik: -1.2738e+02 - logprior: -3.1727e+00
Epoch 2/2
19/19 - 1s - loss: 127.8193 - loglik: -1.2657e+02 - logprior: -1.2477e+00
Fitted a model with MAP estimate = -127.0404
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.3741 - loglik: -1.2723e+02 - logprior: -3.1469e+00
Epoch 2/10
19/19 - 1s - loss: 127.6349 - loglik: -1.2642e+02 - logprior: -1.2198e+00
Epoch 3/10
19/19 - 1s - loss: 126.9649 - loglik: -1.2589e+02 - logprior: -1.0739e+00
Epoch 4/10
19/19 - 1s - loss: 126.0752 - loglik: -1.2505e+02 - logprior: -1.0292e+00
Epoch 5/10
19/19 - 1s - loss: 125.6971 - loglik: -1.2468e+02 - logprior: -1.0158e+00
Epoch 6/10
19/19 - 1s - loss: 125.1520 - loglik: -1.2415e+02 - logprior: -1.0026e+00
Epoch 7/10
19/19 - 1s - loss: 124.8732 - loglik: -1.2388e+02 - logprior: -9.9794e-01
Epoch 8/10
19/19 - 1s - loss: 124.6231 - loglik: -1.2364e+02 - logprior: -9.8593e-01
Epoch 9/10
19/19 - 1s - loss: 124.8746 - loglik: -1.2390e+02 - logprior: -9.7143e-01
Fitted a model with MAP estimate = -124.5656
Time for alignment: 44.5592
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.2021 - loglik: -1.8300e+02 - logprior: -3.2011e+00
Epoch 2/10
19/19 - 1s - loss: 153.2427 - loglik: -1.5165e+02 - logprior: -1.5879e+00
Epoch 3/10
19/19 - 1s - loss: 142.1752 - loglik: -1.4068e+02 - logprior: -1.4939e+00
Epoch 4/10
19/19 - 1s - loss: 139.7296 - loglik: -1.3822e+02 - logprior: -1.5088e+00
Epoch 5/10
19/19 - 1s - loss: 138.2075 - loglik: -1.3672e+02 - logprior: -1.4851e+00
Epoch 6/10
19/19 - 1s - loss: 137.7401 - loglik: -1.3625e+02 - logprior: -1.4882e+00
Epoch 7/10
19/19 - 1s - loss: 137.3423 - loglik: -1.3585e+02 - logprior: -1.4892e+00
Epoch 8/10
19/19 - 1s - loss: 137.2384 - loglik: -1.3575e+02 - logprior: -1.4838e+00
Epoch 9/10
19/19 - 1s - loss: 136.9933 - loglik: -1.3551e+02 - logprior: -1.4808e+00
Epoch 10/10
19/19 - 1s - loss: 137.3227 - loglik: -1.3585e+02 - logprior: -1.4760e+00
Fitted a model with MAP estimate = -136.9614
expansions: [(16, 1), (17, 1), (18, 1), (21, 3), (22, 1), (23, 1), (24, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 136.2250 - loglik: -1.3294e+02 - logprior: -3.2882e+00
Epoch 2/2
19/19 - 1s - loss: 128.1310 - loglik: -1.2682e+02 - logprior: -1.3118e+00
Fitted a model with MAP estimate = -127.2624
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.6575 - loglik: -1.2749e+02 - logprior: -3.1718e+00
Epoch 2/2
19/19 - 1s - loss: 127.6156 - loglik: -1.2637e+02 - logprior: -1.2413e+00
Fitted a model with MAP estimate = -127.0126
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.4457 - loglik: -1.2730e+02 - logprior: -3.1468e+00
Epoch 2/10
19/19 - 1s - loss: 127.5666 - loglik: -1.2635e+02 - logprior: -1.2208e+00
Epoch 3/10
19/19 - 1s - loss: 126.7547 - loglik: -1.2568e+02 - logprior: -1.0797e+00
Epoch 4/10
19/19 - 1s - loss: 126.2870 - loglik: -1.2525e+02 - logprior: -1.0361e+00
Epoch 5/10
19/19 - 1s - loss: 125.6883 - loglik: -1.2467e+02 - logprior: -1.0139e+00
Epoch 6/10
19/19 - 1s - loss: 124.9868 - loglik: -1.2398e+02 - logprior: -1.0109e+00
Epoch 7/10
19/19 - 1s - loss: 125.1460 - loglik: -1.2415e+02 - logprior: -9.9731e-01
Fitted a model with MAP estimate = -124.7055
Time for alignment: 44.5768
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3747 - loglik: -1.8317e+02 - logprior: -3.2042e+00
Epoch 2/10
19/19 - 1s - loss: 154.0364 - loglik: -1.5244e+02 - logprior: -1.6013e+00
Epoch 3/10
19/19 - 1s - loss: 141.6983 - loglik: -1.4021e+02 - logprior: -1.4919e+00
Epoch 4/10
19/19 - 1s - loss: 138.3113 - loglik: -1.3676e+02 - logprior: -1.5515e+00
Epoch 5/10
19/19 - 1s - loss: 137.1000 - loglik: -1.3558e+02 - logprior: -1.5241e+00
Epoch 6/10
19/19 - 1s - loss: 136.7865 - loglik: -1.3526e+02 - logprior: -1.5301e+00
Epoch 7/10
19/19 - 1s - loss: 136.6231 - loglik: -1.3510e+02 - logprior: -1.5230e+00
Epoch 8/10
19/19 - 1s - loss: 136.5202 - loglik: -1.3501e+02 - logprior: -1.5121e+00
Epoch 9/10
19/19 - 1s - loss: 136.2755 - loglik: -1.3477e+02 - logprior: -1.5032e+00
Epoch 10/10
19/19 - 1s - loss: 136.0126 - loglik: -1.3451e+02 - logprior: -1.4987e+00
Fitted a model with MAP estimate = -136.0927
expansions: [(16, 1), (17, 1), (18, 1), (22, 2), (23, 2), (25, 1), (33, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.0051 - loglik: -1.3270e+02 - logprior: -3.3007e+00
Epoch 2/2
19/19 - 1s - loss: 128.3771 - loglik: -1.2707e+02 - logprior: -1.3048e+00
Fitted a model with MAP estimate = -127.3110
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.7938 - loglik: -1.2762e+02 - logprior: -3.1729e+00
Epoch 2/2
19/19 - 1s - loss: 127.6544 - loglik: -1.2642e+02 - logprior: -1.2367e+00
Fitted a model with MAP estimate = -126.9901
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.3852 - loglik: -1.2725e+02 - logprior: -3.1387e+00
Epoch 2/10
19/19 - 1s - loss: 127.5913 - loglik: -1.2638e+02 - logprior: -1.2156e+00
Epoch 3/10
19/19 - 1s - loss: 126.8564 - loglik: -1.2578e+02 - logprior: -1.0764e+00
Epoch 4/10
19/19 - 1s - loss: 126.2732 - loglik: -1.2524e+02 - logprior: -1.0341e+00
Epoch 5/10
19/19 - 1s - loss: 125.5343 - loglik: -1.2452e+02 - logprior: -1.0099e+00
Epoch 6/10
19/19 - 1s - loss: 125.3611 - loglik: -1.2436e+02 - logprior: -1.0021e+00
Epoch 7/10
19/19 - 1s - loss: 124.7829 - loglik: -1.2378e+02 - logprior: -9.9857e-01
Epoch 8/10
19/19 - 1s - loss: 124.6115 - loglik: -1.2363e+02 - logprior: -9.7706e-01
Epoch 9/10
19/19 - 1s - loss: 124.7072 - loglik: -1.2374e+02 - logprior: -9.6745e-01
Fitted a model with MAP estimate = -124.5756
Time for alignment: 45.4495
Computed alignments with likelihoods: ['-124.5233', '-124.5659', '-124.5656', '-124.7055', '-124.5756']
Best model has likelihood: -124.5233  (prior= -0.9460 )
time for generating output: 0.1002
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.968432780391186
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 731.3432 - loglik: -7.2970e+02 - logprior: -1.6419e+00
Epoch 2/10
39/39 - 21s - loss: 619.7520 - loglik: -6.1822e+02 - logprior: -1.5342e+00
Epoch 3/10
39/39 - 22s - loss: 607.9386 - loglik: -6.0639e+02 - logprior: -1.5533e+00
Epoch 4/10
39/39 - 22s - loss: 604.6742 - loglik: -6.0314e+02 - logprior: -1.5296e+00
Epoch 5/10
39/39 - 22s - loss: 603.0249 - loglik: -6.0148e+02 - logprior: -1.5494e+00
Epoch 6/10
39/39 - 22s - loss: 600.8729 - loglik: -5.9929e+02 - logprior: -1.5814e+00
Epoch 7/10
39/39 - 22s - loss: 599.5582 - loglik: -5.9797e+02 - logprior: -1.5899e+00
Epoch 8/10
39/39 - 22s - loss: 598.6112 - loglik: -5.9701e+02 - logprior: -1.6029e+00
Epoch 9/10
39/39 - 23s - loss: 598.7451 - loglik: -5.9714e+02 - logprior: -1.6026e+00
Fitted a model with MAP estimate = -597.6300
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 3), (33, 1), (34, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (86, 1), (115, 1), (116, 1), (117, 1), (121, 5), (131, 1), (132, 1), (135, 1), (148, 1), (152, 1), (154, 2), (163, 1), (164, 1), (165, 1), (166, 1), (175, 1), (184, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 593.1810 - loglik: -5.9077e+02 - logprior: -2.4142e+00
Epoch 2/2
39/39 - 35s - loss: 578.4680 - loglik: -5.7722e+02 - logprior: -1.2457e+00
Fitted a model with MAP estimate = -573.8235
expansions: [(0, 3), (148, 1), (237, 1)]
discards: [  0  84 185 260]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 578.2401 - loglik: -5.7684e+02 - logprior: -1.3993e+00
Epoch 2/2
39/39 - 35s - loss: 573.3470 - loglik: -5.7304e+02 - logprior: -3.0734e-01
Fitted a model with MAP estimate = -570.4173
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 579.0861 - loglik: -5.7714e+02 - logprior: -1.9418e+00
Epoch 2/10
39/39 - 36s - loss: 572.7384 - loglik: -5.7269e+02 - logprior: -4.4855e-02
Epoch 3/10
39/39 - 38s - loss: 569.9585 - loglik: -5.7009e+02 - logprior: 0.1364
Epoch 4/10
39/39 - 39s - loss: 567.8654 - loglik: -5.6813e+02 - logprior: 0.2610
Epoch 5/10
39/39 - 40s - loss: 565.3336 - loglik: -5.6570e+02 - logprior: 0.3663
Epoch 6/10
39/39 - 40s - loss: 565.0521 - loglik: -5.6553e+02 - logprior: 0.4760
Epoch 7/10
39/39 - 38s - loss: 564.2491 - loglik: -5.6487e+02 - logprior: 0.6233
Epoch 8/10
39/39 - 38s - loss: 563.7249 - loglik: -5.6446e+02 - logprior: 0.7313
Epoch 9/10
39/39 - 39s - loss: 565.0240 - loglik: -5.6586e+02 - logprior: 0.8322
Fitted a model with MAP estimate = -563.5638
Time for alignment: 848.8391
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 731.6493 - loglik: -7.3000e+02 - logprior: -1.6445e+00
Epoch 2/10
39/39 - 26s - loss: 616.8128 - loglik: -6.1522e+02 - logprior: -1.5878e+00
Epoch 3/10
39/39 - 26s - loss: 605.5386 - loglik: -6.0396e+02 - logprior: -1.5829e+00
Epoch 4/10
39/39 - 26s - loss: 601.7072 - loglik: -6.0015e+02 - logprior: -1.5615e+00
Epoch 5/10
39/39 - 25s - loss: 600.0575 - loglik: -5.9848e+02 - logprior: -1.5775e+00
Epoch 6/10
39/39 - 26s - loss: 599.0283 - loglik: -5.9743e+02 - logprior: -1.5983e+00
Epoch 7/10
39/39 - 26s - loss: 598.7266 - loglik: -5.9713e+02 - logprior: -1.5967e+00
Epoch 8/10
39/39 - 25s - loss: 598.9114 - loglik: -5.9731e+02 - logprior: -1.6007e+00
Fitted a model with MAP estimate = -598.2392
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 3), (21, 1), (32, 1), (33, 1), (35, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 2), (72, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 2), (136, 2), (137, 1), (147, 1), (154, 2), (163, 1), (164, 1), (165, 1), (166, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [ 0 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 596.9818 - loglik: -5.9451e+02 - logprior: -2.4715e+00
Epoch 2/2
39/39 - 39s - loss: 580.1575 - loglik: -5.7883e+02 - logprior: -1.3250e+00
Fitted a model with MAP estimate = -574.8540
expansions: [(0, 3), (20, 1), (21, 1), (147, 1), (235, 1)]
discards: [  0  85 166 263]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 578.7853 - loglik: -5.7735e+02 - logprior: -1.4359e+00
Epoch 2/2
39/39 - 40s - loss: 573.8010 - loglik: -5.7346e+02 - logprior: -3.4297e-01
Fitted a model with MAP estimate = -570.9323
expansions: [(197, 5)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 578.0244 - loglik: -5.7602e+02 - logprior: -2.0007e+00
Epoch 2/10
39/39 - 38s - loss: 570.2469 - loglik: -5.7012e+02 - logprior: -1.2718e-01
Epoch 3/10
39/39 - 38s - loss: 567.3762 - loglik: -5.6745e+02 - logprior: 0.0780
Epoch 4/10
39/39 - 37s - loss: 564.9510 - loglik: -5.6514e+02 - logprior: 0.1847
Epoch 5/10
39/39 - 37s - loss: 562.8644 - loglik: -5.6317e+02 - logprior: 0.3066
Epoch 6/10
39/39 - 37s - loss: 562.4962 - loglik: -5.6291e+02 - logprior: 0.4181
Epoch 7/10
39/39 - 37s - loss: 562.0570 - loglik: -5.6259e+02 - logprior: 0.5359
Epoch 8/10
39/39 - 40s - loss: 561.0810 - loglik: -5.6172e+02 - logprior: 0.6435
Epoch 9/10
39/39 - 41s - loss: 561.7739 - loglik: -5.6255e+02 - logprior: 0.7723
Fitted a model with MAP estimate = -561.1773
Time for alignment: 888.6377
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 732.9078 - loglik: -7.3128e+02 - logprior: -1.6306e+00
Epoch 2/10
39/39 - 27s - loss: 618.0515 - loglik: -6.1648e+02 - logprior: -1.5677e+00
Epoch 3/10
39/39 - 27s - loss: 605.4432 - loglik: -6.0375e+02 - logprior: -1.6947e+00
Epoch 4/10
39/39 - 28s - loss: 602.4300 - loglik: -6.0076e+02 - logprior: -1.6697e+00
Epoch 5/10
39/39 - 28s - loss: 600.2800 - loglik: -5.9859e+02 - logprior: -1.6886e+00
Epoch 6/10
39/39 - 28s - loss: 599.9639 - loglik: -5.9826e+02 - logprior: -1.7057e+00
Epoch 7/10
39/39 - 27s - loss: 599.4044 - loglik: -5.9769e+02 - logprior: -1.7136e+00
Epoch 8/10
39/39 - 26s - loss: 599.7967 - loglik: -5.9808e+02 - logprior: -1.7141e+00
Fitted a model with MAP estimate = -598.4911
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (14, 1), (15, 1), (16, 1), (17, 2), (29, 1), (33, 1), (35, 1), (41, 1), (45, 1), (46, 1), (51, 1), (68, 1), (69, 2), (70, 2), (76, 1), (81, 1), (101, 1), (114, 1), (115, 1), (117, 1), (119, 4), (121, 1), (132, 1), (134, 1), (136, 2), (137, 1), (149, 1), (153, 2), (164, 3), (173, 2), (176, 1), (177, 1), (179, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [ 0 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 595.1293 - loglik: -5.9272e+02 - logprior: -2.4076e+00
Epoch 2/2
39/39 - 37s - loss: 578.9523 - loglik: -5.7767e+02 - logprior: -1.2792e+00
Fitted a model with MAP estimate = -575.3985
expansions: [(0, 3), (148, 1)]
discards: [  0 241 266]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 578.5161 - loglik: -5.7711e+02 - logprior: -1.4072e+00
Epoch 2/2
39/39 - 39s - loss: 571.7487 - loglik: -5.7140e+02 - logprior: -3.4385e-01
Fitted a model with MAP estimate = -568.1222
expansions: []
discards: [  0   1 218]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 577.1548 - loglik: -5.7508e+02 - logprior: -2.0739e+00
Epoch 2/10
39/39 - 41s - loss: 570.5059 - loglik: -5.7030e+02 - logprior: -2.0600e-01
Epoch 3/10
39/39 - 39s - loss: 567.7097 - loglik: -5.6786e+02 - logprior: 0.1457
Epoch 4/10
39/39 - 38s - loss: 565.4823 - loglik: -5.6570e+02 - logprior: 0.2219
Epoch 5/10
39/39 - 38s - loss: 563.4033 - loglik: -5.6372e+02 - logprior: 0.3188
Epoch 6/10
39/39 - 37s - loss: 562.2728 - loglik: -5.6272e+02 - logprior: 0.4477
Epoch 7/10
39/39 - 38s - loss: 562.0515 - loglik: -5.6262e+02 - logprior: 0.5719
Epoch 8/10
39/39 - 37s - loss: 562.6304 - loglik: -5.6333e+02 - logprior: 0.6986
Fitted a model with MAP estimate = -561.5411
Time for alignment: 859.3650
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 731.1392 - loglik: -7.2949e+02 - logprior: -1.6480e+00
Epoch 2/10
39/39 - 26s - loss: 616.3081 - loglik: -6.1465e+02 - logprior: -1.6533e+00
Epoch 3/10
39/39 - 27s - loss: 602.6111 - loglik: -6.0089e+02 - logprior: -1.7172e+00
Epoch 4/10
39/39 - 28s - loss: 599.3441 - loglik: -5.9767e+02 - logprior: -1.6767e+00
Epoch 5/10
39/39 - 27s - loss: 597.8885 - loglik: -5.9621e+02 - logprior: -1.6835e+00
Epoch 6/10
39/39 - 27s - loss: 597.0671 - loglik: -5.9536e+02 - logprior: -1.7049e+00
Epoch 7/10
39/39 - 27s - loss: 596.7539 - loglik: -5.9504e+02 - logprior: -1.7150e+00
Epoch 8/10
39/39 - 25s - loss: 596.5676 - loglik: -5.9485e+02 - logprior: -1.7211e+00
Epoch 9/10
39/39 - 25s - loss: 597.2982 - loglik: -5.9557e+02 - logprior: -1.7235e+00
Fitted a model with MAP estimate = -595.5349
expansions: [(12, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (149, 1), (153, 2), (162, 1), (163, 1), (164, 1), (165, 1), (173, 2), (174, 1), (183, 1), (185, 1), (188, 1), (192, 1), (193, 1), (196, 1), (210, 1), (211, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 591.6599 - loglik: -5.8922e+02 - logprior: -2.4352e+00
Epoch 2/2
39/39 - 31s - loss: 577.0120 - loglik: -5.7573e+02 - logprior: -1.2847e+00
Fitted a model with MAP estimate = -572.6674
expansions: [(0, 3), (147, 1), (239, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 576.8006 - loglik: -5.7538e+02 - logprior: -1.4256e+00
Epoch 2/2
39/39 - 36s - loss: 572.0624 - loglik: -5.7171e+02 - logprior: -3.4811e-01
Fitted a model with MAP estimate = -569.3839
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 578.0695 - loglik: -5.7608e+02 - logprior: -1.9931e+00
Epoch 2/10
39/39 - 33s - loss: 571.9795 - loglik: -5.7177e+02 - logprior: -2.0710e-01
Epoch 3/10
39/39 - 33s - loss: 568.6036 - loglik: -5.6875e+02 - logprior: 0.1450
Epoch 4/10
39/39 - 32s - loss: 566.2432 - loglik: -5.6649e+02 - logprior: 0.2498
Epoch 5/10
39/39 - 33s - loss: 563.9842 - loglik: -5.6431e+02 - logprior: 0.3224
Epoch 6/10
39/39 - 35s - loss: 563.5145 - loglik: -5.6398e+02 - logprior: 0.4688
Epoch 7/10
39/39 - 35s - loss: 563.6271 - loglik: -5.6419e+02 - logprior: 0.5672
Fitted a model with MAP estimate = -562.8509
Time for alignment: 773.6440
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 733.0840 - loglik: -7.3144e+02 - logprior: -1.6439e+00
Epoch 2/10
39/39 - 25s - loss: 615.6854 - loglik: -6.1406e+02 - logprior: -1.6272e+00
Epoch 3/10
39/39 - 24s - loss: 603.5441 - loglik: -6.0184e+02 - logprior: -1.7040e+00
Epoch 4/10
39/39 - 24s - loss: 600.1224 - loglik: -5.9847e+02 - logprior: -1.6493e+00
Epoch 5/10
39/39 - 24s - loss: 598.4358 - loglik: -5.9677e+02 - logprior: -1.6658e+00
Epoch 6/10
39/39 - 24s - loss: 597.8585 - loglik: -5.9618e+02 - logprior: -1.6797e+00
Epoch 7/10
39/39 - 23s - loss: 597.3766 - loglik: -5.9568e+02 - logprior: -1.6938e+00
Epoch 8/10
39/39 - 23s - loss: 597.5154 - loglik: -5.9581e+02 - logprior: -1.7055e+00
Fitted a model with MAP estimate = -596.4782
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 2), (19, 1), (33, 1), (34, 1), (42, 2), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 5), (132, 2), (134, 1), (137, 1), (147, 1), (154, 3), (163, 1), (164, 1), (165, 1), (166, 1), (177, 1), (178, 1), (180, 1), (184, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 592.0883 - loglik: -5.8962e+02 - logprior: -2.4652e+00
Epoch 2/2
39/39 - 35s - loss: 577.0092 - loglik: -5.7568e+02 - logprior: -1.3285e+00
Fitted a model with MAP estimate = -572.3883
expansions: [(0, 3), (147, 1), (148, 1), (169, 2)]
discards: [  0  50  85 162 241]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 576.3465 - loglik: -5.7487e+02 - logprior: -1.4796e+00
Epoch 2/2
39/39 - 34s - loss: 571.4525 - loglik: -5.7103e+02 - logprior: -4.2411e-01
Fitted a model with MAP estimate = -568.4485
expansions: []
discards: [  0   2 192]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 577.2997 - loglik: -5.7522e+02 - logprior: -2.0766e+00
Epoch 2/10
39/39 - 34s - loss: 570.8521 - loglik: -5.7074e+02 - logprior: -1.1425e-01
Epoch 3/10
39/39 - 35s - loss: 568.0359 - loglik: -5.6810e+02 - logprior: 0.0613
Epoch 4/10
39/39 - 36s - loss: 565.5934 - loglik: -5.6578e+02 - logprior: 0.1882
Epoch 5/10
39/39 - 36s - loss: 563.8756 - loglik: -5.6417e+02 - logprior: 0.2977
Epoch 6/10
39/39 - 35s - loss: 562.6187 - loglik: -5.6305e+02 - logprior: 0.4358
Epoch 7/10
39/39 - 34s - loss: 562.5671 - loglik: -5.6313e+02 - logprior: 0.5616
Epoch 8/10
39/39 - 33s - loss: 562.4394 - loglik: -5.6311e+02 - logprior: 0.6718
Epoch 9/10
39/39 - 33s - loss: 563.0677 - loglik: -5.6387e+02 - logprior: 0.8019
Fitted a model with MAP estimate = -561.6765
Time for alignment: 804.8322
Computed alignments with likelihoods: ['-563.5638', '-561.1773', '-561.5411', '-562.8509', '-561.6765']
Best model has likelihood: -561.1773  (prior= 0.8239 )
time for generating output: 0.3053
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.34944732987082167
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 971.5339 - loglik: -9.7024e+02 - logprior: -1.2965e+00
Epoch 2/10
39/39 - 47s - loss: 809.8615 - loglik: -8.0889e+02 - logprior: -9.7558e-01
Epoch 3/10
39/39 - 48s - loss: 797.1212 - loglik: -7.9608e+02 - logprior: -1.0459e+00
Epoch 4/10
39/39 - 52s - loss: 794.1350 - loglik: -7.9309e+02 - logprior: -1.0476e+00
Epoch 5/10
39/39 - 55s - loss: 793.3428 - loglik: -7.9230e+02 - logprior: -1.0478e+00
Epoch 6/10
39/39 - 58s - loss: 793.1821 - loglik: -7.9211e+02 - logprior: -1.0748e+00
Epoch 7/10
39/39 - 58s - loss: 792.7343 - loglik: -7.9163e+02 - logprior: -1.1080e+00
Epoch 8/10
39/39 - 58s - loss: 792.3134 - loglik: -7.9126e+02 - logprior: -1.0576e+00
Epoch 9/10
39/39 - 60s - loss: 792.8091 - loglik: -7.9173e+02 - logprior: -1.0764e+00
Fitted a model with MAP estimate = -784.1966
expansions: [(0, 3), (34, 1), (51, 1), (53, 1), (57, 1), (70, 1), (71, 1), (75, 2), (99, 1), (120, 2), (121, 1), (122, 1), (123, 1), (146, 3), (147, 1), (148, 1), (167, 1), (170, 1), (176, 1), (177, 1), (178, 1), (193, 2), (196, 1), (197, 3), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (262, 1), (266, 2), (285, 1), (286, 2), (287, 8), (296, 4), (309, 1), (313, 1), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 776.7029 - loglik: -7.7477e+02 - logprior: -1.9351e+00
Epoch 2/2
39/39 - 79s - loss: 760.1609 - loglik: -7.5984e+02 - logprior: -3.1600e-01
Fitted a model with MAP estimate = -750.0199
expansions: [(0, 2), (345, 1), (346, 1)]
discards: [133 167 220 228 318]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 763.0529 - loglik: -7.6116e+02 - logprior: -1.8958e+00
Epoch 2/2
39/39 - 75s - loss: 758.0515 - loglik: -7.5804e+02 - logprior: -1.4472e-02
Fitted a model with MAP estimate = -748.5157
expansions: []
discards: [  1   2 335 337 338 339]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 755.0206 - loglik: -7.5403e+02 - logprior: -9.9204e-01
Epoch 2/10
39/39 - 73s - loss: 751.0052 - loglik: -7.5136e+02 - logprior: 0.3582
Epoch 3/10
39/39 - 80s - loss: 749.5479 - loglik: -7.5009e+02 - logprior: 0.5405
Epoch 4/10
39/39 - 84s - loss: 748.2111 - loglik: -7.4887e+02 - logprior: 0.6541
Epoch 5/10
39/39 - 85s - loss: 746.7803 - loglik: -7.4753e+02 - logprior: 0.7498
Epoch 6/10
39/39 - 83s - loss: 747.7360 - loglik: -7.4863e+02 - logprior: 0.8942
Fitted a model with MAP estimate = -746.3211
Time for alignment: 1619.6496
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 970.5087 - loglik: -9.6918e+02 - logprior: -1.3333e+00
Epoch 2/10
39/39 - 59s - loss: 809.8972 - loglik: -8.0880e+02 - logprior: -1.1006e+00
Epoch 3/10
39/39 - 59s - loss: 798.9526 - loglik: -7.9776e+02 - logprior: -1.1916e+00
Epoch 4/10
39/39 - 55s - loss: 796.0713 - loglik: -7.9492e+02 - logprior: -1.1486e+00
Epoch 5/10
39/39 - 56s - loss: 795.1287 - loglik: -7.9396e+02 - logprior: -1.1660e+00
Epoch 6/10
39/39 - 58s - loss: 794.4483 - loglik: -7.9327e+02 - logprior: -1.1770e+00
Epoch 7/10
39/39 - 59s - loss: 794.4078 - loglik: -7.9322e+02 - logprior: -1.1890e+00
Epoch 8/10
39/39 - 60s - loss: 794.8737 - loglik: -7.9363e+02 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -785.7518
expansions: [(0, 2), (6, 1), (20, 1), (42, 1), (51, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (100, 3), (120, 2), (121, 1), (122, 1), (123, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (262, 1), (266, 2), (285, 2), (286, 2), (287, 8), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 777.3969 - loglik: -7.7578e+02 - logprior: -1.6126e+00
Epoch 2/2
39/39 - 69s - loss: 760.0279 - loglik: -7.5954e+02 - logprior: -4.9024e-01
Fitted a model with MAP estimate = -749.7657
expansions: [(0, 2)]
discards: [  1   2   3 111 134 168 315 361 387]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 764.7414 - loglik: -7.6308e+02 - logprior: -1.6649e+00
Epoch 2/2
39/39 - 65s - loss: 759.7236 - loglik: -7.5972e+02 - logprior: -2.3675e-03
Fitted a model with MAP estimate = -749.9811
expansions: [(0, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 755.0975 - loglik: -7.5351e+02 - logprior: -1.5866e+00
Epoch 2/10
39/39 - 64s - loss: 751.3755 - loglik: -7.5170e+02 - logprior: 0.3208
Epoch 3/10
39/39 - 71s - loss: 749.7116 - loglik: -7.5019e+02 - logprior: 0.4813
Epoch 4/10
39/39 - 79s - loss: 747.8925 - loglik: -7.4850e+02 - logprior: 0.6048
Epoch 5/10
39/39 - 84s - loss: 747.2515 - loglik: -7.4795e+02 - logprior: 0.6947
Epoch 6/10
39/39 - 81s - loss: 746.3356 - loglik: -7.4724e+02 - logprior: 0.9028
Epoch 7/10
39/39 - 76s - loss: 746.7292 - loglik: -7.4777e+02 - logprior: 1.0412
Fitted a model with MAP estimate = -745.8447
Time for alignment: 1678.0736
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 975.7189 - loglik: -9.7442e+02 - logprior: -1.3039e+00
Epoch 2/10
39/39 - 57s - loss: 815.5987 - loglik: -8.1460e+02 - logprior: -9.9605e-01
Epoch 3/10
39/39 - 59s - loss: 800.3765 - loglik: -7.9923e+02 - logprior: -1.1439e+00
Epoch 4/10
39/39 - 59s - loss: 796.4326 - loglik: -7.9527e+02 - logprior: -1.1669e+00
Epoch 5/10
39/39 - 58s - loss: 795.7209 - loglik: -7.9452e+02 - logprior: -1.1973e+00
Epoch 6/10
39/39 - 53s - loss: 794.8353 - loglik: -7.9366e+02 - logprior: -1.1791e+00
Epoch 7/10
39/39 - 48s - loss: 794.7750 - loglik: -7.9358e+02 - logprior: -1.1944e+00
Epoch 8/10
39/39 - 46s - loss: 794.1799 - loglik: -7.9300e+02 - logprior: -1.1827e+00
Epoch 9/10
39/39 - 45s - loss: 794.3868 - loglik: -7.9321e+02 - logprior: -1.1765e+00
Fitted a model with MAP estimate = -785.9993
expansions: [(0, 3), (20, 1), (51, 1), (52, 1), (53, 1), (59, 1), (64, 1), (70, 1), (71, 1), (75, 2), (121, 1), (122, 2), (123, 1), (124, 1), (144, 1), (146, 3), (147, 1), (148, 1), (167, 1), (174, 1), (176, 1), (177, 1), (178, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 2), (226, 3), (246, 1), (250, 1), (251, 1), (254, 1), (255, 1), (256, 1), (259, 1), (261, 3), (262, 1), (263, 1), (265, 2), (284, 7), (285, 3), (286, 1), (287, 1), (295, 2), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 777.7266 - loglik: -7.7545e+02 - logprior: -2.2724e+00
Epoch 2/2
39/39 - 65s - loss: 759.9946 - loglik: -7.5927e+02 - logprior: -7.2076e-01
Fitted a model with MAP estimate = -749.7433
expansions: [(0, 2), (350, 2)]
discards: [  1   2  86 137 169 262 267 312 321 391]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 763.4026 - loglik: -7.6146e+02 - logprior: -1.9426e+00
Epoch 2/2
39/39 - 85s - loss: 758.5461 - loglik: -7.5862e+02 - logprior: 0.0744
Fitted a model with MAP estimate = -748.8936
expansions: [(0, 2)]
discards: [  1   2   4   5 336 337 338 339 344]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 758.1129 - loglik: -7.5605e+02 - logprior: -2.0590e+00
Epoch 2/10
39/39 - 77s - loss: 752.0286 - loglik: -7.5234e+02 - logprior: 0.3098
Epoch 3/10
39/39 - 80s - loss: 750.4293 - loglik: -7.5097e+02 - logprior: 0.5408
Epoch 4/10
39/39 - 81s - loss: 749.4219 - loglik: -7.5008e+02 - logprior: 0.6576
Epoch 5/10
39/39 - 82s - loss: 747.3963 - loglik: -7.4819e+02 - logprior: 0.7905
Epoch 6/10
39/39 - 75s - loss: 746.9510 - loglik: -7.4786e+02 - logprior: 0.9103
Epoch 7/10
39/39 - 65s - loss: 749.1452 - loglik: -7.5020e+02 - logprior: 1.0573
Fitted a model with MAP estimate = -746.8023
Time for alignment: 1734.4055
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 973.0530 - loglik: -9.7173e+02 - logprior: -1.3253e+00
Epoch 2/10
39/39 - 50s - loss: 810.1028 - loglik: -8.0891e+02 - logprior: -1.1967e+00
Epoch 3/10
39/39 - 55s - loss: 797.7875 - loglik: -7.9650e+02 - logprior: -1.2838e+00
Epoch 4/10
39/39 - 51s - loss: 794.6155 - loglik: -7.9332e+02 - logprior: -1.2955e+00
Epoch 5/10
39/39 - 47s - loss: 793.8330 - loglik: -7.9249e+02 - logprior: -1.3404e+00
Epoch 6/10
39/39 - 45s - loss: 793.1788 - loglik: -7.9183e+02 - logprior: -1.3477e+00
Epoch 7/10
39/39 - 47s - loss: 792.6888 - loglik: -7.9132e+02 - logprior: -1.3652e+00
Epoch 8/10
39/39 - 52s - loss: 793.1940 - loglik: -7.9182e+02 - logprior: -1.3701e+00
Fitted a model with MAP estimate = -784.4414
expansions: [(0, 2), (6, 1), (15, 1), (44, 1), (51, 1), (53, 1), (57, 1), (61, 1), (69, 1), (70, 1), (73, 2), (99, 1), (104, 1), (119, 2), (120, 1), (121, 1), (122, 1), (146, 3), (147, 1), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (191, 1), (192, 2), (195, 1), (196, 1), (199, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 3), (243, 1), (250, 1), (251, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (264, 1), (285, 6), (286, 4), (287, 1), (288, 1), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: [  0 135]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 777.5733 - loglik: -7.7589e+02 - logprior: -1.6876e+00
Epoch 2/2
39/39 - 79s - loss: 759.6340 - loglik: -7.5917e+02 - logprior: -4.6539e-01
Fitted a model with MAP estimate = -749.2359
expansions: [(0, 2), (344, 2)]
discards: [  1   2   3  83 134 168 221 363 389]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 763.9505 - loglik: -7.6218e+02 - logprior: -1.7744e+00
Epoch 2/2
39/39 - 86s - loss: 758.7463 - loglik: -7.5864e+02 - logprior: -1.0281e-01
Fitted a model with MAP estimate = -748.7889
expansions: [(0, 2)]
discards: [  2 258 332 334 340]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 756.2360 - loglik: -7.5449e+02 - logprior: -1.7442e+00
Epoch 2/10
39/39 - 80s - loss: 751.2399 - loglik: -7.5144e+02 - logprior: 0.2024
Epoch 3/10
39/39 - 68s - loss: 748.9661 - loglik: -7.4946e+02 - logprior: 0.4938
Epoch 4/10
39/39 - 64s - loss: 748.4437 - loglik: -7.4908e+02 - logprior: 0.6383
Epoch 5/10
39/39 - 70s - loss: 746.2783 - loglik: -7.4704e+02 - logprior: 0.7617
Epoch 6/10
39/39 - 78s - loss: 747.0620 - loglik: -7.4796e+02 - logprior: 0.8937
Fitted a model with MAP estimate = -745.9925
Time for alignment: 1566.0664
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 970.9061 - loglik: -9.6960e+02 - logprior: -1.3061e+00
Epoch 2/10
39/39 - 45s - loss: 808.3165 - loglik: -8.0731e+02 - logprior: -1.0084e+00
Epoch 3/10
39/39 - 46s - loss: 797.4081 - loglik: -7.9636e+02 - logprior: -1.0523e+00
Epoch 4/10
39/39 - 50s - loss: 794.9870 - loglik: -7.9394e+02 - logprior: -1.0476e+00
Epoch 5/10
39/39 - 55s - loss: 794.1301 - loglik: -7.9301e+02 - logprior: -1.1220e+00
Epoch 6/10
39/39 - 58s - loss: 793.1923 - loglik: -7.9210e+02 - logprior: -1.0959e+00
Epoch 7/10
39/39 - 58s - loss: 793.0172 - loglik: -7.9185e+02 - logprior: -1.1680e+00
Epoch 8/10
39/39 - 58s - loss: 793.0973 - loglik: -7.9198e+02 - logprior: -1.1221e+00
Fitted a model with MAP estimate = -784.4625
expansions: [(0, 3), (5, 1), (23, 1), (50, 1), (51, 1), (52, 1), (56, 1), (61, 1), (68, 1), (69, 1), (72, 2), (98, 1), (103, 1), (118, 2), (119, 1), (120, 1), (121, 1), (144, 3), (146, 1), (165, 1), (172, 1), (174, 1), (175, 1), (187, 1), (191, 2), (194, 1), (195, 1), (217, 1), (218, 1), (219, 1), (222, 2), (223, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (261, 3), (262, 1), (266, 2), (285, 1), (286, 2), (287, 8), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 778.7057 - loglik: -7.7649e+02 - logprior: -2.2185e+00
Epoch 2/2
39/39 - 71s - loss: 760.0970 - loglik: -7.5937e+02 - logprior: -7.2504e-01
Fitted a model with MAP estimate = -749.6189
expansions: [(0, 2), (347, 1), (348, 2)]
discards: [  1   2 135 221 262 311 312 320 343 366 391]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 764.1854 - loglik: -7.6217e+02 - logprior: -2.0182e+00
Epoch 2/2
39/39 - 67s - loss: 758.5396 - loglik: -7.5854e+02 - logprior: -3.6881e-03
Fitted a model with MAP estimate = -748.7539
expansions: [(0, 2), (308, 1)]
discards: [  1   2   4   5 334 335 336 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 757.0197 - loglik: -7.5502e+02 - logprior: -1.9987e+00
Epoch 2/10
39/39 - 65s - loss: 751.4653 - loglik: -7.5177e+02 - logprior: 0.3085
Epoch 3/10
39/39 - 65s - loss: 749.7128 - loglik: -7.5023e+02 - logprior: 0.5215
Epoch 4/10
39/39 - 65s - loss: 748.2204 - loglik: -7.4884e+02 - logprior: 0.6168
Epoch 5/10
39/39 - 65s - loss: 746.7995 - loglik: -7.4753e+02 - logprior: 0.7339
Epoch 6/10
39/39 - 63s - loss: 746.3815 - loglik: -7.4729e+02 - logprior: 0.9041
Epoch 7/10
39/39 - 64s - loss: 746.4827 - loglik: -7.4748e+02 - logprior: 0.9947
Fitted a model with MAP estimate = -746.2529
Time for alignment: 1506.8534
Computed alignments with likelihoods: ['-746.3211', '-745.8447', '-746.8023', '-745.9925', '-746.2529']
Best model has likelihood: -745.8447  (prior= 1.1500 )
time for generating output: 1.2908
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.4652948379133409
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.6688 - loglik: -1.3044e+02 - logprior: -3.2251e+00
Epoch 2/10
19/19 - 1s - loss: 109.3252 - loglik: -1.0793e+02 - logprior: -1.3959e+00
Epoch 3/10
19/19 - 1s - loss: 101.7193 - loglik: -1.0019e+02 - logprior: -1.5245e+00
Epoch 4/10
19/19 - 1s - loss: 99.4422 - loglik: -9.8015e+01 - logprior: -1.4267e+00
Epoch 5/10
19/19 - 1s - loss: 98.6520 - loglik: -9.7255e+01 - logprior: -1.3972e+00
Epoch 6/10
19/19 - 1s - loss: 98.2663 - loglik: -9.6884e+01 - logprior: -1.3821e+00
Epoch 7/10
19/19 - 1s - loss: 98.1608 - loglik: -9.6794e+01 - logprior: -1.3665e+00
Epoch 8/10
19/19 - 1s - loss: 98.1305 - loglik: -9.6772e+01 - logprior: -1.3586e+00
Epoch 9/10
19/19 - 1s - loss: 98.2487 - loglik: -9.6899e+01 - logprior: -1.3495e+00
Fitted a model with MAP estimate = -98.0123
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.3344 - loglik: -9.8166e+01 - logprior: -4.1688e+00
Epoch 2/2
19/19 - 1s - loss: 95.2563 - loglik: -9.3195e+01 - logprior: -2.0610e+00
Fitted a model with MAP estimate = -93.8685
expansions: [(0, 2)]
discards: [ 0  9 12 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.6353 - loglik: -9.2564e+01 - logprior: -3.0713e+00
Epoch 2/2
19/19 - 1s - loss: 92.8214 - loglik: -9.1556e+01 - logprior: -1.2658e+00
Fitted a model with MAP estimate = -92.3714
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.7770 - loglik: -9.3188e+01 - logprior: -3.5887e+00
Epoch 2/10
19/19 - 1s - loss: 93.2038 - loglik: -9.1814e+01 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 1s - loss: 92.7426 - loglik: -9.1443e+01 - logprior: -1.2995e+00
Epoch 4/10
19/19 - 1s - loss: 92.5110 - loglik: -9.1262e+01 - logprior: -1.2490e+00
Epoch 5/10
19/19 - 1s - loss: 92.3199 - loglik: -9.1101e+01 - logprior: -1.2192e+00
Epoch 6/10
19/19 - 1s - loss: 92.1189 - loglik: -9.0914e+01 - logprior: -1.2049e+00
Epoch 7/10
19/19 - 1s - loss: 92.2116 - loglik: -9.1021e+01 - logprior: -1.1901e+00
Fitted a model with MAP estimate = -92.0478
Time for alignment: 36.9916
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5569 - loglik: -1.3033e+02 - logprior: -3.2277e+00
Epoch 2/10
19/19 - 1s - loss: 108.4070 - loglik: -1.0699e+02 - logprior: -1.4164e+00
Epoch 3/10
19/19 - 1s - loss: 100.8819 - loglik: -9.9559e+01 - logprior: -1.3233e+00
Epoch 4/10
19/19 - 1s - loss: 98.9567 - loglik: -9.7695e+01 - logprior: -1.2619e+00
Epoch 5/10
19/19 - 1s - loss: 98.2435 - loglik: -9.7009e+01 - logprior: -1.2345e+00
Epoch 6/10
19/19 - 1s - loss: 97.9780 - loglik: -9.6759e+01 - logprior: -1.2193e+00
Epoch 7/10
19/19 - 1s - loss: 97.7860 - loglik: -9.6581e+01 - logprior: -1.2051e+00
Epoch 8/10
19/19 - 1s - loss: 97.8746 - loglik: -9.6679e+01 - logprior: -1.1955e+00
Fitted a model with MAP estimate = -97.7440
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.7404 - loglik: -9.6426e+01 - logprior: -4.3148e+00
Epoch 2/2
19/19 - 1s - loss: 93.3258 - loglik: -9.1929e+01 - logprior: -1.3969e+00
Fitted a model with MAP estimate = -92.3525
expansions: []
discards: [ 0 10 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.8293 - loglik: -9.3718e+01 - logprior: -4.1114e+00
Epoch 2/2
19/19 - 1s - loss: 93.5292 - loglik: -9.1982e+01 - logprior: -1.5471e+00
Fitted a model with MAP estimate = -92.8463
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.5624 - loglik: -9.2288e+01 - logprior: -3.2744e+00
Epoch 2/10
19/19 - 1s - loss: 93.0506 - loglik: -9.1647e+01 - logprior: -1.4035e+00
Epoch 3/10
19/19 - 1s - loss: 92.5472 - loglik: -9.1248e+01 - logprior: -1.2991e+00
Epoch 4/10
19/19 - 1s - loss: 92.4498 - loglik: -9.1202e+01 - logprior: -1.2477e+00
Epoch 5/10
19/19 - 1s - loss: 92.3489 - loglik: -9.1137e+01 - logprior: -1.2115e+00
Epoch 6/10
19/19 - 1s - loss: 92.1632 - loglik: -9.0964e+01 - logprior: -1.1987e+00
Epoch 7/10
19/19 - 1s - loss: 92.1594 - loglik: -9.0979e+01 - logprior: -1.1799e+00
Epoch 8/10
19/19 - 1s - loss: 92.0229 - loglik: -9.0853e+01 - logprior: -1.1694e+00
Epoch 9/10
19/19 - 1s - loss: 92.0694 - loglik: -9.0915e+01 - logprior: -1.1544e+00
Fitted a model with MAP estimate = -92.0023
Time for alignment: 37.6961
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.6287 - loglik: -1.3041e+02 - logprior: -3.2222e+00
Epoch 2/10
19/19 - 1s - loss: 109.6766 - loglik: -1.0828e+02 - logprior: -1.3931e+00
Epoch 3/10
19/19 - 1s - loss: 101.5186 - loglik: -1.0004e+02 - logprior: -1.4787e+00
Epoch 4/10
19/19 - 1s - loss: 99.1238 - loglik: -9.7677e+01 - logprior: -1.4468e+00
Epoch 5/10
19/19 - 1s - loss: 98.5178 - loglik: -9.7124e+01 - logprior: -1.3942e+00
Epoch 6/10
19/19 - 1s - loss: 98.2922 - loglik: -9.6908e+01 - logprior: -1.3846e+00
Epoch 7/10
19/19 - 1s - loss: 98.2094 - loglik: -9.6842e+01 - logprior: -1.3673e+00
Epoch 8/10
19/19 - 1s - loss: 98.1880 - loglik: -9.6830e+01 - logprior: -1.3575e+00
Epoch 9/10
19/19 - 1s - loss: 98.1422 - loglik: -9.6791e+01 - logprior: -1.3513e+00
Epoch 10/10
19/19 - 1s - loss: 98.1228 - loglik: -9.6776e+01 - logprior: -1.3472e+00
Fitted a model with MAP estimate = -97.9831
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.3556 - loglik: -9.8189e+01 - logprior: -4.1662e+00
Epoch 2/2
19/19 - 1s - loss: 95.2323 - loglik: -9.3153e+01 - logprior: -2.0789e+00
Fitted a model with MAP estimate = -93.9068
expansions: [(0, 2)]
discards: [ 0  9 12 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.6872 - loglik: -9.2616e+01 - logprior: -3.0711e+00
Epoch 2/2
19/19 - 1s - loss: 92.7819 - loglik: -9.1519e+01 - logprior: -1.2631e+00
Fitted a model with MAP estimate = -92.3670
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.7489 - loglik: -9.3177e+01 - logprior: -3.5724e+00
Epoch 2/10
19/19 - 1s - loss: 93.2766 - loglik: -9.1885e+01 - logprior: -1.3912e+00
Epoch 3/10
19/19 - 1s - loss: 92.6505 - loglik: -9.1352e+01 - logprior: -1.2983e+00
Epoch 4/10
19/19 - 1s - loss: 92.3785 - loglik: -9.1130e+01 - logprior: -1.2485e+00
Epoch 5/10
19/19 - 1s - loss: 92.3860 - loglik: -9.1169e+01 - logprior: -1.2172e+00
Fitted a model with MAP estimate = -92.1916
Time for alignment: 36.6668
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5859 - loglik: -1.3036e+02 - logprior: -3.2267e+00
Epoch 2/10
19/19 - 1s - loss: 108.8282 - loglik: -1.0742e+02 - logprior: -1.4122e+00
Epoch 3/10
19/19 - 1s - loss: 101.0040 - loglik: -9.9670e+01 - logprior: -1.3343e+00
Epoch 4/10
19/19 - 1s - loss: 99.0021 - loglik: -9.7737e+01 - logprior: -1.2654e+00
Epoch 5/10
19/19 - 1s - loss: 98.2383 - loglik: -9.7003e+01 - logprior: -1.2357e+00
Epoch 6/10
19/19 - 1s - loss: 98.0184 - loglik: -9.6797e+01 - logprior: -1.2216e+00
Epoch 7/10
19/19 - 1s - loss: 97.9285 - loglik: -9.6723e+01 - logprior: -1.2054e+00
Epoch 8/10
19/19 - 1s - loss: 97.8040 - loglik: -9.6608e+01 - logprior: -1.1963e+00
Epoch 9/10
19/19 - 1s - loss: 98.0038 - loglik: -9.6812e+01 - logprior: -1.1922e+00
Fitted a model with MAP estimate = -97.7203
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.8037 - loglik: -9.6446e+01 - logprior: -4.3576e+00
Epoch 2/2
19/19 - 1s - loss: 93.3806 - loglik: -9.1992e+01 - logprior: -1.3884e+00
Fitted a model with MAP estimate = -92.3297
expansions: []
discards: [ 0 11 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 97.8193 - loglik: -9.3716e+01 - logprior: -4.1029e+00
Epoch 2/2
19/19 - 1s - loss: 93.5093 - loglik: -9.1970e+01 - logprior: -1.5394e+00
Fitted a model with MAP estimate = -92.8502
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 95.5337 - loglik: -9.2261e+01 - logprior: -3.2730e+00
Epoch 2/10
19/19 - 1s - loss: 93.0523 - loglik: -9.1647e+01 - logprior: -1.4050e+00
Epoch 3/10
19/19 - 1s - loss: 92.6873 - loglik: -9.1386e+01 - logprior: -1.3012e+00
Epoch 4/10
19/19 - 1s - loss: 92.3870 - loglik: -9.1138e+01 - logprior: -1.2487e+00
Epoch 5/10
19/19 - 1s - loss: 92.2696 - loglik: -9.1055e+01 - logprior: -1.2149e+00
Epoch 6/10
19/19 - 1s - loss: 92.2306 - loglik: -9.1027e+01 - logprior: -1.2033e+00
Epoch 7/10
19/19 - 1s - loss: 92.1110 - loglik: -9.0927e+01 - logprior: -1.1845e+00
Epoch 8/10
19/19 - 1s - loss: 92.0591 - loglik: -9.0887e+01 - logprior: -1.1716e+00
Epoch 9/10
19/19 - 1s - loss: 92.0006 - loglik: -9.0841e+01 - logprior: -1.1595e+00
Epoch 10/10
19/19 - 1s - loss: 92.1153 - loglik: -9.0968e+01 - logprior: -1.1477e+00
Fitted a model with MAP estimate = -91.9815
Time for alignment: 38.0200
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6404 - loglik: -1.3041e+02 - logprior: -3.2256e+00
Epoch 2/10
19/19 - 1s - loss: 108.5635 - loglik: -1.0715e+02 - logprior: -1.4091e+00
Epoch 3/10
19/19 - 1s - loss: 100.8202 - loglik: -9.9474e+01 - logprior: -1.3460e+00
Epoch 4/10
19/19 - 1s - loss: 98.6847 - loglik: -9.7406e+01 - logprior: -1.2788e+00
Epoch 5/10
19/19 - 1s - loss: 98.2791 - loglik: -9.7029e+01 - logprior: -1.2502e+00
Epoch 6/10
19/19 - 1s - loss: 97.9245 - loglik: -9.6688e+01 - logprior: -1.2366e+00
Epoch 7/10
19/19 - 1s - loss: 97.8386 - loglik: -9.6616e+01 - logprior: -1.2229e+00
Epoch 8/10
19/19 - 1s - loss: 97.8265 - loglik: -9.6612e+01 - logprior: -1.2144e+00
Epoch 9/10
19/19 - 1s - loss: 97.7743 - loglik: -9.6565e+01 - logprior: -1.2094e+00
Epoch 10/10
19/19 - 1s - loss: 97.7361 - loglik: -9.6531e+01 - logprior: -1.2051e+00
Fitted a model with MAP estimate = -97.6429
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.7588 - loglik: -9.6367e+01 - logprior: -4.3918e+00
Epoch 2/2
19/19 - 1s - loss: 93.3676 - loglik: -9.1998e+01 - logprior: -1.3693e+00
Fitted a model with MAP estimate = -92.3343
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.7709 - loglik: -9.3669e+01 - logprior: -4.1014e+00
Epoch 2/2
19/19 - 1s - loss: 93.3612 - loglik: -9.1821e+01 - logprior: -1.5397e+00
Fitted a model with MAP estimate = -92.8409
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.5380 - loglik: -9.2263e+01 - logprior: -3.2747e+00
Epoch 2/10
19/19 - 1s - loss: 93.0760 - loglik: -9.1669e+01 - logprior: -1.4066e+00
Epoch 3/10
19/19 - 1s - loss: 92.6232 - loglik: -9.1323e+01 - logprior: -1.2998e+00
Epoch 4/10
19/19 - 1s - loss: 92.4146 - loglik: -9.1167e+01 - logprior: -1.2480e+00
Epoch 5/10
19/19 - 1s - loss: 92.3030 - loglik: -9.1085e+01 - logprior: -1.2177e+00
Epoch 6/10
19/19 - 1s - loss: 92.2548 - loglik: -9.1057e+01 - logprior: -1.1973e+00
Epoch 7/10
19/19 - 1s - loss: 92.0813 - loglik: -9.0896e+01 - logprior: -1.1854e+00
Epoch 8/10
19/19 - 1s - loss: 92.0295 - loglik: -9.0862e+01 - logprior: -1.1677e+00
Epoch 9/10
19/19 - 1s - loss: 91.9367 - loglik: -9.0774e+01 - logprior: -1.1622e+00
Epoch 10/10
19/19 - 1s - loss: 92.0712 - loglik: -9.0927e+01 - logprior: -1.1438e+00
Fitted a model with MAP estimate = -91.9805
Time for alignment: 40.0871
Computed alignments with likelihoods: ['-92.0478', '-92.0023', '-92.1916', '-91.9815', '-91.9805']
Best model has likelihood: -91.9805  (prior= -1.1390 )
time for generating output: 0.0924
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8367653690064633
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 472.4126 - loglik: -4.7060e+02 - logprior: -1.8125e+00
Epoch 2/10
39/39 - 9s - loss: 372.9308 - loglik: -3.7114e+02 - logprior: -1.7886e+00
Epoch 3/10
39/39 - 9s - loss: 363.9102 - loglik: -3.6206e+02 - logprior: -1.8493e+00
Epoch 4/10
39/39 - 9s - loss: 361.0429 - loglik: -3.5925e+02 - logprior: -1.7955e+00
Epoch 5/10
39/39 - 9s - loss: 359.9462 - loglik: -3.5818e+02 - logprior: -1.7699e+00
Epoch 6/10
39/39 - 9s - loss: 359.2405 - loglik: -3.5747e+02 - logprior: -1.7689e+00
Epoch 7/10
39/39 - 9s - loss: 358.6713 - loglik: -3.5691e+02 - logprior: -1.7637e+00
Epoch 8/10
39/39 - 9s - loss: 358.3541 - loglik: -3.5660e+02 - logprior: -1.7586e+00
Epoch 9/10
39/39 - 9s - loss: 358.4736 - loglik: -3.5672e+02 - logprior: -1.7562e+00
Fitted a model with MAP estimate = -358.0601
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 1), (80, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 342.4289 - loglik: -3.4063e+02 - logprior: -1.8035e+00
Epoch 2/2
39/39 - 12s - loss: 328.9083 - loglik: -3.2814e+02 - logprior: -7.6353e-01
Fitted a model with MAP estimate = -326.9156
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 332.2710 - loglik: -3.3067e+02 - logprior: -1.5976e+00
Epoch 2/2
39/39 - 12s - loss: 328.0671 - loglik: -3.2755e+02 - logprior: -5.1939e-01
Fitted a model with MAP estimate = -326.5397
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 331.8350 - loglik: -3.3042e+02 - logprior: -1.4177e+00
Epoch 2/10
39/39 - 12s - loss: 327.8430 - loglik: -3.2749e+02 - logprior: -3.5607e-01
Epoch 3/10
39/39 - 13s - loss: 326.3586 - loglik: -3.2612e+02 - logprior: -2.3406e-01
Epoch 4/10
39/39 - 13s - loss: 324.4975 - loglik: -3.2435e+02 - logprior: -1.4948e-01
Epoch 5/10
39/39 - 13s - loss: 322.8691 - loglik: -3.2284e+02 - logprior: -3.3808e-02
Epoch 6/10
39/39 - 13s - loss: 321.8810 - loglik: -3.2195e+02 - logprior: 0.0740
Epoch 7/10
39/39 - 13s - loss: 321.9551 - loglik: -3.2213e+02 - logprior: 0.1760
Fitted a model with MAP estimate = -321.0985
Time for alignment: 289.6431
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 473.1873 - loglik: -4.7139e+02 - logprior: -1.7977e+00
Epoch 2/10
39/39 - 9s - loss: 374.8791 - loglik: -3.7326e+02 - logprior: -1.6145e+00
Epoch 3/10
39/39 - 9s - loss: 367.7644 - loglik: -3.6614e+02 - logprior: -1.6229e+00
Epoch 4/10
39/39 - 10s - loss: 365.1174 - loglik: -3.6355e+02 - logprior: -1.5695e+00
Epoch 5/10
39/39 - 9s - loss: 363.8204 - loglik: -3.6228e+02 - logprior: -1.5422e+00
Epoch 6/10
39/39 - 9s - loss: 363.2297 - loglik: -3.6169e+02 - logprior: -1.5357e+00
Epoch 7/10
39/39 - 9s - loss: 362.6675 - loglik: -3.6114e+02 - logprior: -1.5289e+00
Epoch 8/10
39/39 - 9s - loss: 362.7017 - loglik: -3.6118e+02 - logprior: -1.5242e+00
Fitted a model with MAP estimate = -361.9262
expansions: [(4, 1), (5, 2), (13, 3), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (42, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (70, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 343.4634 - loglik: -3.4166e+02 - logprior: -1.8002e+00
Epoch 2/2
39/39 - 12s - loss: 330.0531 - loglik: -3.2929e+02 - logprior: -7.6475e-01
Fitted a model with MAP estimate = -328.2396
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 333.8310 - loglik: -3.3220e+02 - logprior: -1.6272e+00
Epoch 2/2
39/39 - 12s - loss: 330.0308 - loglik: -3.2948e+02 - logprior: -5.5311e-01
Fitted a model with MAP estimate = -328.2864
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 333.3999 - loglik: -3.3196e+02 - logprior: -1.4382e+00
Epoch 2/10
39/39 - 12s - loss: 329.7271 - loglik: -3.2936e+02 - logprior: -3.7184e-01
Epoch 3/10
39/39 - 12s - loss: 327.7083 - loglik: -3.2746e+02 - logprior: -2.5017e-01
Epoch 4/10
39/39 - 13s - loss: 326.2145 - loglik: -3.2606e+02 - logprior: -1.5448e-01
Epoch 5/10
39/39 - 13s - loss: 324.8904 - loglik: -3.2484e+02 - logprior: -4.5629e-02
Epoch 6/10
39/39 - 12s - loss: 323.7639 - loglik: -3.2384e+02 - logprior: 0.0767
Epoch 7/10
39/39 - 13s - loss: 322.6513 - loglik: -3.2283e+02 - logprior: 0.1827
Epoch 8/10
39/39 - 13s - loss: 322.3922 - loglik: -3.2269e+02 - logprior: 0.2947
Epoch 9/10
39/39 - 13s - loss: 323.1996 - loglik: -3.2359e+02 - logprior: 0.3935
Fitted a model with MAP estimate = -321.9825
Time for alignment: 310.0879
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 473.1773 - loglik: -4.7137e+02 - logprior: -1.8109e+00
Epoch 2/10
39/39 - 9s - loss: 374.4772 - loglik: -3.7278e+02 - logprior: -1.7003e+00
Epoch 3/10
39/39 - 9s - loss: 366.2354 - loglik: -3.6451e+02 - logprior: -1.7221e+00
Epoch 4/10
39/39 - 9s - loss: 364.2120 - loglik: -3.6256e+02 - logprior: -1.6483e+00
Epoch 5/10
39/39 - 9s - loss: 362.7803 - loglik: -3.6115e+02 - logprior: -1.6342e+00
Epoch 6/10
39/39 - 9s - loss: 361.6818 - loglik: -3.6006e+02 - logprior: -1.6262e+00
Epoch 7/10
39/39 - 9s - loss: 361.2799 - loglik: -3.5966e+02 - logprior: -1.6246e+00
Epoch 8/10
39/39 - 9s - loss: 361.1857 - loglik: -3.5956e+02 - logprior: -1.6207e+00
Epoch 9/10
39/39 - 9s - loss: 361.2930 - loglik: -3.5968e+02 - logprior: -1.6085e+00
Fitted a model with MAP estimate = -360.5574
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 2), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 343.9164 - loglik: -3.4215e+02 - logprior: -1.7668e+00
Epoch 2/2
39/39 - 13s - loss: 329.8942 - loglik: -3.2918e+02 - logprior: -7.1591e-01
Fitted a model with MAP estimate = -327.8605
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 333.4836 - loglik: -3.3195e+02 - logprior: -1.5337e+00
Epoch 2/2
39/39 - 13s - loss: 329.4482 - loglik: -3.2897e+02 - logprior: -4.8183e-01
Fitted a model with MAP estimate = -327.9276
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 333.0322 - loglik: -3.3166e+02 - logprior: -1.3721e+00
Epoch 2/10
39/39 - 12s - loss: 329.1214 - loglik: -3.2882e+02 - logprior: -3.0258e-01
Epoch 3/10
39/39 - 12s - loss: 327.4988 - loglik: -3.2731e+02 - logprior: -1.8624e-01
Epoch 4/10
39/39 - 12s - loss: 325.7621 - loglik: -3.2567e+02 - logprior: -9.3629e-02
Epoch 5/10
39/39 - 12s - loss: 324.2748 - loglik: -3.2428e+02 - logprior: 7.5807e-04
Epoch 6/10
39/39 - 12s - loss: 323.5189 - loglik: -3.2362e+02 - logprior: 0.1005
Epoch 7/10
39/39 - 12s - loss: 322.6404 - loglik: -3.2283e+02 - logprior: 0.1919
Epoch 8/10
39/39 - 12s - loss: 321.7679 - loglik: -3.2206e+02 - logprior: 0.2928
Epoch 9/10
39/39 - 12s - loss: 322.6821 - loglik: -3.2308e+02 - logprior: 0.4002
Fitted a model with MAP estimate = -321.7253
Time for alignment: 315.5527
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 471.2303 - loglik: -4.6942e+02 - logprior: -1.8089e+00
Epoch 2/10
39/39 - 9s - loss: 372.2022 - loglik: -3.7049e+02 - logprior: -1.7168e+00
Epoch 3/10
39/39 - 9s - loss: 363.8894 - loglik: -3.6215e+02 - logprior: -1.7428e+00
Epoch 4/10
39/39 - 9s - loss: 361.3512 - loglik: -3.5969e+02 - logprior: -1.6648e+00
Epoch 5/10
39/39 - 9s - loss: 359.9916 - loglik: -3.5835e+02 - logprior: -1.6430e+00
Epoch 6/10
39/39 - 9s - loss: 359.2383 - loglik: -3.5761e+02 - logprior: -1.6280e+00
Epoch 7/10
39/39 - 9s - loss: 358.3119 - loglik: -3.5667e+02 - logprior: -1.6386e+00
Epoch 8/10
39/39 - 9s - loss: 358.3598 - loglik: -3.5672e+02 - logprior: -1.6349e+00
Fitted a model with MAP estimate = -357.9128
expansions: [(2, 1), (3, 3), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (107, 1), (108, 1), (113, 1), (114, 1), (117, 1), (123, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 342.0404 - loglik: -3.4024e+02 - logprior: -1.8048e+00
Epoch 2/2
39/39 - 12s - loss: 328.3802 - loglik: -3.2761e+02 - logprior: -7.6787e-01
Fitted a model with MAP estimate = -326.4647
expansions: []
discards: [  2   5 120 171]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 333.0305 - loglik: -3.3144e+02 - logprior: -1.5940e+00
Epoch 2/2
39/39 - 12s - loss: 328.7806 - loglik: -3.2826e+02 - logprior: -5.2335e-01
Fitted a model with MAP estimate = -327.2516
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 332.5646 - loglik: -3.3115e+02 - logprior: -1.4172e+00
Epoch 2/10
39/39 - 12s - loss: 328.4322 - loglik: -3.2807e+02 - logprior: -3.6192e-01
Epoch 3/10
39/39 - 12s - loss: 326.8996 - loglik: -3.2667e+02 - logprior: -2.3026e-01
Epoch 4/10
39/39 - 12s - loss: 324.8979 - loglik: -3.2477e+02 - logprior: -1.2797e-01
Epoch 5/10
39/39 - 12s - loss: 323.7685 - loglik: -3.2375e+02 - logprior: -1.4518e-02
Epoch 6/10
39/39 - 12s - loss: 322.2439 - loglik: -3.2233e+02 - logprior: 0.0842
Epoch 7/10
39/39 - 12s - loss: 322.2664 - loglik: -3.2244e+02 - logprior: 0.1765
Fitted a model with MAP estimate = -321.5845
Time for alignment: 270.3103
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 472.4328 - loglik: -4.7064e+02 - logprior: -1.7881e+00
Epoch 2/10
39/39 - 9s - loss: 374.4176 - loglik: -3.7267e+02 - logprior: -1.7441e+00
Epoch 3/10
39/39 - 9s - loss: 365.7565 - loglik: -3.6400e+02 - logprior: -1.7577e+00
Epoch 4/10
39/39 - 9s - loss: 362.5839 - loglik: -3.6087e+02 - logprior: -1.7144e+00
Epoch 5/10
39/39 - 9s - loss: 361.1948 - loglik: -3.5949e+02 - logprior: -1.7025e+00
Epoch 6/10
39/39 - 9s - loss: 360.1546 - loglik: -3.5846e+02 - logprior: -1.6980e+00
Epoch 7/10
39/39 - 9s - loss: 359.7015 - loglik: -3.5801e+02 - logprior: -1.6952e+00
Epoch 8/10
39/39 - 9s - loss: 359.7844 - loglik: -3.5809e+02 - logprior: -1.6949e+00
Fitted a model with MAP estimate = -359.1794
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (54, 1), (58, 1), (60, 1), (63, 1), (66, 2), (72, 1), (74, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 343.4059 - loglik: -3.4162e+02 - logprior: -1.7868e+00
Epoch 2/2
39/39 - 13s - loss: 329.4460 - loglik: -3.2872e+02 - logprior: -7.2237e-01
Fitted a model with MAP estimate = -327.7182
expansions: []
discards: [ 84 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 333.8300 - loglik: -3.3229e+02 - logprior: -1.5436e+00
Epoch 2/2
39/39 - 12s - loss: 329.8741 - loglik: -3.2937e+02 - logprior: -5.0112e-01
Fitted a model with MAP estimate = -328.1930
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 333.2340 - loglik: -3.3185e+02 - logprior: -1.3889e+00
Epoch 2/10
39/39 - 12s - loss: 329.5345 - loglik: -3.2921e+02 - logprior: -3.2192e-01
Epoch 3/10
39/39 - 13s - loss: 327.6732 - loglik: -3.2747e+02 - logprior: -2.0330e-01
Epoch 4/10
39/39 - 13s - loss: 326.1612 - loglik: -3.2605e+02 - logprior: -1.1203e-01
Epoch 5/10
39/39 - 13s - loss: 324.4335 - loglik: -3.2442e+02 - logprior: -1.7489e-02
Epoch 6/10
39/39 - 13s - loss: 323.8063 - loglik: -3.2388e+02 - logprior: 0.0765
Epoch 7/10
39/39 - 13s - loss: 323.2967 - loglik: -3.2347e+02 - logprior: 0.1736
Epoch 8/10
39/39 - 13s - loss: 322.2804 - loglik: -3.2256e+02 - logprior: 0.2797
Epoch 9/10
39/39 - 13s - loss: 322.2755 - loglik: -3.2265e+02 - logprior: 0.3780
Epoch 10/10
39/39 - 13s - loss: 322.2311 - loglik: -3.2271e+02 - logprior: 0.4742
Fitted a model with MAP estimate = -321.7360
Time for alignment: 319.2829
Computed alignments with likelihoods: ['-321.0985', '-321.9825', '-321.7253', '-321.5845', '-321.7360']
Best model has likelihood: -321.0985  (prior= 0.2009 )
time for generating output: 0.1838
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.749042960442365
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 257.7416 - loglik: -2.5464e+02 - logprior: -3.0968e+00
Epoch 2/10
19/19 - 2s - loss: 226.2480 - loglik: -2.2498e+02 - logprior: -1.2640e+00
Epoch 3/10
19/19 - 2s - loss: 210.7129 - loglik: -2.0938e+02 - logprior: -1.3326e+00
Epoch 4/10
19/19 - 2s - loss: 206.6300 - loglik: -2.0542e+02 - logprior: -1.2094e+00
Epoch 5/10
19/19 - 2s - loss: 204.8327 - loglik: -2.0369e+02 - logprior: -1.1455e+00
Epoch 6/10
19/19 - 2s - loss: 204.3740 - loglik: -2.0326e+02 - logprior: -1.1164e+00
Epoch 7/10
19/19 - 2s - loss: 203.7313 - loglik: -2.0262e+02 - logprior: -1.1139e+00
Epoch 8/10
19/19 - 2s - loss: 204.0140 - loglik: -2.0291e+02 - logprior: -1.1075e+00
Fitted a model with MAP estimate = -203.2261
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (24, 1), (30, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 206.6884 - loglik: -2.0276e+02 - logprior: -3.9299e+00
Epoch 2/2
19/19 - 2s - loss: 197.0954 - loglik: -1.9530e+02 - logprior: -1.7970e+00
Fitted a model with MAP estimate = -195.0209
expansions: [(0, 2)]
discards: [ 0  7 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.0874 - loglik: -1.9426e+02 - logprior: -2.8297e+00
Epoch 2/2
19/19 - 2s - loss: 193.8796 - loglik: -1.9286e+02 - logprior: -1.0167e+00
Fitted a model with MAP estimate = -192.9310
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.1626 - loglik: -1.9478e+02 - logprior: -3.3843e+00
Epoch 2/10
19/19 - 2s - loss: 194.0917 - loglik: -1.9299e+02 - logprior: -1.1013e+00
Epoch 3/10
19/19 - 2s - loss: 193.1846 - loglik: -1.9224e+02 - logprior: -9.4200e-01
Epoch 4/10
19/19 - 2s - loss: 192.5486 - loglik: -1.9164e+02 - logprior: -9.0367e-01
Epoch 5/10
19/19 - 2s - loss: 192.1296 - loglik: -1.9125e+02 - logprior: -8.7904e-01
Epoch 6/10
19/19 - 2s - loss: 191.5568 - loglik: -1.9070e+02 - logprior: -8.5504e-01
Epoch 7/10
19/19 - 2s - loss: 191.6907 - loglik: -1.9085e+02 - logprior: -8.4099e-01
Fitted a model with MAP estimate = -191.3663
Time for alignment: 62.9164
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.3582 - loglik: -2.5526e+02 - logprior: -3.0991e+00
Epoch 2/10
19/19 - 2s - loss: 226.5378 - loglik: -2.2527e+02 - logprior: -1.2703e+00
Epoch 3/10
19/19 - 2s - loss: 212.0667 - loglik: -2.1070e+02 - logprior: -1.3701e+00
Epoch 4/10
19/19 - 2s - loss: 207.3957 - loglik: -2.0602e+02 - logprior: -1.3757e+00
Epoch 5/10
19/19 - 2s - loss: 206.0370 - loglik: -2.0458e+02 - logprior: -1.4553e+00
Epoch 6/10
19/19 - 2s - loss: 205.1658 - loglik: -2.0374e+02 - logprior: -1.4214e+00
Epoch 7/10
19/19 - 2s - loss: 205.2801 - loglik: -2.0386e+02 - logprior: -1.4249e+00
Fitted a model with MAP estimate = -204.6785
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (25, 1), (30, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (63, 1), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 206.7764 - loglik: -2.0284e+02 - logprior: -3.9343e+00
Epoch 2/2
19/19 - 2s - loss: 196.8365 - loglik: -1.9506e+02 - logprior: -1.7803e+00
Fitted a model with MAP estimate = -194.9346
expansions: [(0, 2)]
discards: [ 0  7 46]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.1122 - loglik: -1.9428e+02 - logprior: -2.8331e+00
Epoch 2/2
19/19 - 2s - loss: 193.9191 - loglik: -1.9290e+02 - logprior: -1.0172e+00
Fitted a model with MAP estimate = -192.9305
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.3728 - loglik: -1.9496e+02 - logprior: -3.4106e+00
Epoch 2/10
19/19 - 2s - loss: 194.0622 - loglik: -1.9297e+02 - logprior: -1.0968e+00
Epoch 3/10
19/19 - 2s - loss: 193.0585 - loglik: -1.9210e+02 - logprior: -9.5728e-01
Epoch 4/10
19/19 - 2s - loss: 192.7327 - loglik: -1.9183e+02 - logprior: -9.0449e-01
Epoch 5/10
19/19 - 2s - loss: 192.1559 - loglik: -1.9127e+02 - logprior: -8.8578e-01
Epoch 6/10
19/19 - 2s - loss: 191.7235 - loglik: -1.9086e+02 - logprior: -8.6008e-01
Epoch 7/10
19/19 - 2s - loss: 191.3826 - loglik: -1.9054e+02 - logprior: -8.4159e-01
Epoch 8/10
19/19 - 2s - loss: 191.5711 - loglik: -1.9075e+02 - logprior: -8.2041e-01
Fitted a model with MAP estimate = -191.2583
Time for alignment: 61.2670
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.1058 - loglik: -2.5501e+02 - logprior: -3.0925e+00
Epoch 2/10
19/19 - 2s - loss: 226.5787 - loglik: -2.2532e+02 - logprior: -1.2584e+00
Epoch 3/10
19/19 - 2s - loss: 211.9860 - loglik: -2.1066e+02 - logprior: -1.3225e+00
Epoch 4/10
19/19 - 2s - loss: 208.1660 - loglik: -2.0701e+02 - logprior: -1.1606e+00
Epoch 5/10
19/19 - 2s - loss: 206.9731 - loglik: -2.0585e+02 - logprior: -1.1196e+00
Epoch 6/10
19/19 - 2s - loss: 206.4408 - loglik: -2.0535e+02 - logprior: -1.0866e+00
Epoch 7/10
19/19 - 2s - loss: 204.6848 - loglik: -2.0359e+02 - logprior: -1.0911e+00
Epoch 8/10
19/19 - 2s - loss: 204.5730 - loglik: -2.0348e+02 - logprior: -1.0906e+00
Epoch 9/10
19/19 - 2s - loss: 203.9055 - loglik: -2.0282e+02 - logprior: -1.0873e+00
Epoch 10/10
19/19 - 2s - loss: 204.2304 - loglik: -2.0314e+02 - logprior: -1.0862e+00
Fitted a model with MAP estimate = -203.5674
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (18, 1), (30, 2), (37, 2), (38, 1), (43, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.7250 - loglik: -2.0380e+02 - logprior: -3.9275e+00
Epoch 2/2
19/19 - 2s - loss: 198.0892 - loglik: -1.9624e+02 - logprior: -1.8465e+00
Fitted a model with MAP estimate = -196.0014
expansions: [(0, 2)]
discards: [ 0  7 37 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.3031 - loglik: -1.9547e+02 - logprior: -2.8336e+00
Epoch 2/2
19/19 - 2s - loss: 194.8346 - loglik: -1.9381e+02 - logprior: -1.0224e+00
Fitted a model with MAP estimate = -193.8853
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.7309 - loglik: -1.9534e+02 - logprior: -3.3916e+00
Epoch 2/10
19/19 - 2s - loss: 194.0576 - loglik: -1.9297e+02 - logprior: -1.0897e+00
Epoch 3/10
19/19 - 2s - loss: 193.1335 - loglik: -1.9220e+02 - logprior: -9.3755e-01
Epoch 4/10
19/19 - 2s - loss: 192.3681 - loglik: -1.9147e+02 - logprior: -8.9593e-01
Epoch 5/10
19/19 - 2s - loss: 192.3084 - loglik: -1.9144e+02 - logprior: -8.7135e-01
Epoch 6/10
19/19 - 2s - loss: 191.7348 - loglik: -1.9089e+02 - logprior: -8.4633e-01
Epoch 7/10
19/19 - 2s - loss: 191.5145 - loglik: -1.9069e+02 - logprior: -8.2485e-01
Epoch 8/10
19/19 - 2s - loss: 191.4006 - loglik: -1.9059e+02 - logprior: -8.1461e-01
Epoch 9/10
19/19 - 2s - loss: 191.2758 - loglik: -1.9049e+02 - logprior: -7.8405e-01
Epoch 10/10
19/19 - 2s - loss: 191.4599 - loglik: -1.9069e+02 - logprior: -7.7407e-01
Fitted a model with MAP estimate = -191.1472
Time for alignment: 71.1006
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8243 - loglik: -2.5473e+02 - logprior: -3.0946e+00
Epoch 2/10
19/19 - 2s - loss: 225.3628 - loglik: -2.2412e+02 - logprior: -1.2457e+00
Epoch 3/10
19/19 - 2s - loss: 211.9960 - loglik: -2.1072e+02 - logprior: -1.2773e+00
Epoch 4/10
19/19 - 2s - loss: 206.8468 - loglik: -2.0564e+02 - logprior: -1.2090e+00
Epoch 5/10
19/19 - 2s - loss: 204.0913 - loglik: -2.0291e+02 - logprior: -1.1774e+00
Epoch 6/10
19/19 - 2s - loss: 203.2780 - loglik: -2.0215e+02 - logprior: -1.1308e+00
Epoch 7/10
19/19 - 2s - loss: 203.1877 - loglik: -2.0208e+02 - logprior: -1.1075e+00
Epoch 8/10
19/19 - 2s - loss: 203.0472 - loglik: -2.0196e+02 - logprior: -1.0899e+00
Epoch 9/10
19/19 - 2s - loss: 202.6587 - loglik: -2.0157e+02 - logprior: -1.0860e+00
Epoch 10/10
19/19 - 2s - loss: 202.9575 - loglik: -2.0187e+02 - logprior: -1.0851e+00
Fitted a model with MAP estimate = -202.4383
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (25, 1), (29, 1), (36, 1), (37, 2), (38, 1), (54, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 206.1808 - loglik: -2.0225e+02 - logprior: -3.9354e+00
Epoch 2/2
19/19 - 2s - loss: 197.0020 - loglik: -1.9513e+02 - logprior: -1.8690e+00
Fitted a model with MAP estimate = -194.8512
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 49]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.1130 - loglik: -1.9428e+02 - logprior: -2.8327e+00
Epoch 2/2
19/19 - 2s - loss: 193.7312 - loglik: -1.9271e+02 - logprior: -1.0180e+00
Fitted a model with MAP estimate = -192.9391
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.4184 - loglik: -1.9501e+02 - logprior: -3.4102e+00
Epoch 2/10
19/19 - 2s - loss: 193.9908 - loglik: -1.9289e+02 - logprior: -1.1013e+00
Epoch 3/10
19/19 - 2s - loss: 193.1744 - loglik: -1.9223e+02 - logprior: -9.4910e-01
Epoch 4/10
19/19 - 2s - loss: 192.4001 - loglik: -1.9150e+02 - logprior: -9.0107e-01
Epoch 5/10
19/19 - 2s - loss: 192.3124 - loglik: -1.9143e+02 - logprior: -8.7898e-01
Epoch 6/10
19/19 - 2s - loss: 191.7292 - loglik: -1.9087e+02 - logprior: -8.5515e-01
Epoch 7/10
19/19 - 2s - loss: 191.5717 - loglik: -1.9073e+02 - logprior: -8.4051e-01
Epoch 8/10
19/19 - 2s - loss: 191.3579 - loglik: -1.9054e+02 - logprior: -8.2221e-01
Epoch 9/10
19/19 - 2s - loss: 191.2938 - loglik: -1.9049e+02 - logprior: -8.0174e-01
Epoch 10/10
19/19 - 2s - loss: 191.5024 - loglik: -1.9072e+02 - logprior: -7.8279e-01
Fitted a model with MAP estimate = -191.1888
Time for alignment: 71.1652
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 257.8275 - loglik: -2.5473e+02 - logprior: -3.0957e+00
Epoch 2/10
19/19 - 2s - loss: 227.2547 - loglik: -2.2598e+02 - logprior: -1.2697e+00
Epoch 3/10
19/19 - 2s - loss: 214.4510 - loglik: -2.1311e+02 - logprior: -1.3366e+00
Epoch 4/10
19/19 - 2s - loss: 209.8991 - loglik: -2.0862e+02 - logprior: -1.2774e+00
Epoch 5/10
19/19 - 2s - loss: 207.6364 - loglik: -2.0637e+02 - logprior: -1.2685e+00
Epoch 6/10
19/19 - 2s - loss: 206.6582 - loglik: -2.0547e+02 - logprior: -1.1878e+00
Epoch 7/10
19/19 - 2s - loss: 204.9960 - loglik: -2.0384e+02 - logprior: -1.1602e+00
Epoch 8/10
19/19 - 2s - loss: 204.6843 - loglik: -2.0356e+02 - logprior: -1.1217e+00
Epoch 9/10
19/19 - 2s - loss: 204.5638 - loglik: -2.0345e+02 - logprior: -1.1093e+00
Epoch 10/10
19/19 - 2s - loss: 204.1427 - loglik: -2.0304e+02 - logprior: -1.0988e+00
Fitted a model with MAP estimate = -203.9973
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (31, 1), (37, 2), (38, 1), (43, 1), (50, 2), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.3033 - loglik: -2.0436e+02 - logprior: -3.9471e+00
Epoch 2/2
19/19 - 2s - loss: 198.2353 - loglik: -1.9637e+02 - logprior: -1.8624e+00
Fitted a model with MAP estimate = -195.9782
expansions: [(0, 2)]
discards: [ 0  7 45 62]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.1393 - loglik: -1.9530e+02 - logprior: -2.8383e+00
Epoch 2/2
19/19 - 2s - loss: 194.8051 - loglik: -1.9378e+02 - logprior: -1.0276e+00
Fitted a model with MAP estimate = -193.8728
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.0943 - loglik: -1.9569e+02 - logprior: -3.4069e+00
Epoch 2/10
19/19 - 2s - loss: 195.1375 - loglik: -1.9404e+02 - logprior: -1.1011e+00
Epoch 3/10
19/19 - 2s - loss: 194.1055 - loglik: -1.9314e+02 - logprior: -9.6082e-01
Epoch 4/10
19/19 - 2s - loss: 193.5285 - loglik: -1.9262e+02 - logprior: -9.1106e-01
Epoch 5/10
19/19 - 2s - loss: 192.9760 - loglik: -1.9209e+02 - logprior: -8.8317e-01
Epoch 6/10
19/19 - 2s - loss: 192.6830 - loglik: -1.9182e+02 - logprior: -8.6656e-01
Epoch 7/10
19/19 - 2s - loss: 192.3642 - loglik: -1.9152e+02 - logprior: -8.4903e-01
Epoch 8/10
19/19 - 2s - loss: 192.3895 - loglik: -1.9156e+02 - logprior: -8.2933e-01
Fitted a model with MAP estimate = -192.1553
Time for alignment: 66.3445
Computed alignments with likelihoods: ['-191.3663', '-191.2583', '-191.1472', '-191.1888', '-192.1553']
Best model has likelihood: -191.1472  (prior= -0.7718 )
time for generating output: 0.1381
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9703059180995788
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 566.3965 - loglik: -5.6466e+02 - logprior: -1.7391e+00
Epoch 2/10
39/39 - 10s - loss: 489.4127 - loglik: -4.8821e+02 - logprior: -1.2036e+00
Epoch 3/10
39/39 - 11s - loss: 480.4344 - loglik: -4.7924e+02 - logprior: -1.1951e+00
Epoch 4/10
39/39 - 11s - loss: 478.4520 - loglik: -4.7729e+02 - logprior: -1.1643e+00
Epoch 5/10
39/39 - 11s - loss: 477.6262 - loglik: -4.7646e+02 - logprior: -1.1664e+00
Epoch 6/10
39/39 - 11s - loss: 477.1234 - loglik: -4.7594e+02 - logprior: -1.1811e+00
Epoch 7/10
39/39 - 11s - loss: 476.9399 - loglik: -4.7574e+02 - logprior: -1.2027e+00
Epoch 8/10
39/39 - 11s - loss: 476.9887 - loglik: -4.7577e+02 - logprior: -1.2200e+00
Fitted a model with MAP estimate = -474.3098
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (69, 1), (74, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (109, 1), (113, 1), (116, 1), (124, 4), (141, 3), (149, 3), (150, 2), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 220 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 469.9339 - loglik: -4.6740e+02 - logprior: -2.5373e+00
Epoch 2/2
39/39 - 15s - loss: 458.4120 - loglik: -4.5772e+02 - logprior: -6.9531e-01
Fitted a model with MAP estimate = -454.5337
expansions: []
discards: [ 24  55 111 176 190 193]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 461.3148 - loglik: -4.5960e+02 - logprior: -1.7134e+00
Epoch 2/2
39/39 - 15s - loss: 457.9988 - loglik: -4.5752e+02 - logprior: -4.7622e-01
Fitted a model with MAP estimate = -454.2287
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 458.3842 - loglik: -4.5680e+02 - logprior: -1.5795e+00
Epoch 2/10
39/39 - 14s - loss: 455.4216 - loglik: -4.5507e+02 - logprior: -3.5193e-01
Epoch 3/10
39/39 - 14s - loss: 453.9232 - loglik: -4.5368e+02 - logprior: -2.4031e-01
Epoch 4/10
39/39 - 15s - loss: 452.2588 - loglik: -4.5209e+02 - logprior: -1.7023e-01
Epoch 5/10
39/39 - 15s - loss: 451.5062 - loglik: -4.5139e+02 - logprior: -1.1377e-01
Epoch 6/10
39/39 - 15s - loss: 451.0142 - loglik: -4.5095e+02 - logprior: -6.6666e-02
Epoch 7/10
39/39 - 15s - loss: 451.1382 - loglik: -4.5114e+02 - logprior: 9.0462e-04
Fitted a model with MAP estimate = -450.5782
Time for alignment: 331.5205
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 567.7881 - loglik: -5.6605e+02 - logprior: -1.7380e+00
Epoch 2/10
39/39 - 10s - loss: 490.6529 - loglik: -4.8944e+02 - logprior: -1.2089e+00
Epoch 3/10
39/39 - 10s - loss: 481.2927 - loglik: -4.8013e+02 - logprior: -1.1601e+00
Epoch 4/10
39/39 - 10s - loss: 479.0890 - loglik: -4.7795e+02 - logprior: -1.1348e+00
Epoch 5/10
39/39 - 10s - loss: 477.8799 - loglik: -4.7675e+02 - logprior: -1.1318e+00
Epoch 6/10
39/39 - 10s - loss: 477.2967 - loglik: -4.7616e+02 - logprior: -1.1340e+00
Epoch 7/10
39/39 - 10s - loss: 477.1813 - loglik: -4.7604e+02 - logprior: -1.1403e+00
Epoch 8/10
39/39 - 10s - loss: 477.3995 - loglik: -4.7626e+02 - logprior: -1.1396e+00
Fitted a model with MAP estimate = -474.6343
expansions: [(0, 3), (12, 1), (18, 2), (19, 2), (20, 1), (35, 2), (43, 1), (45, 1), (46, 1), (51, 1), (65, 1), (68, 1), (76, 1), (77, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (113, 1), (116, 1), (125, 3), (136, 1), (138, 1), (140, 1), (141, 3), (146, 2), (148, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 470.1156 - loglik: -4.6761e+02 - logprior: -2.5012e+00
Epoch 2/2
39/39 - 13s - loss: 458.2303 - loglik: -4.5752e+02 - logprior: -7.0733e-01
Fitted a model with MAP estimate = -454.1576
expansions: []
discards: [ 26  44  95 113 177]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 460.8512 - loglik: -4.5915e+02 - logprior: -1.7026e+00
Epoch 2/2
39/39 - 13s - loss: 457.7390 - loglik: -4.5728e+02 - logprior: -4.6333e-01
Fitted a model with MAP estimate = -453.9753
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 458.1287 - loglik: -4.5656e+02 - logprior: -1.5704e+00
Epoch 2/10
39/39 - 13s - loss: 455.2021 - loglik: -4.5487e+02 - logprior: -3.3675e-01
Epoch 3/10
39/39 - 13s - loss: 453.5838 - loglik: -4.5338e+02 - logprior: -2.0656e-01
Epoch 4/10
39/39 - 13s - loss: 452.0715 - loglik: -4.5192e+02 - logprior: -1.5600e-01
Epoch 5/10
39/39 - 13s - loss: 451.2876 - loglik: -4.5119e+02 - logprior: -9.9720e-02
Epoch 6/10
39/39 - 13s - loss: 450.6192 - loglik: -4.5057e+02 - logprior: -4.4439e-02
Epoch 7/10
39/39 - 13s - loss: 450.7293 - loglik: -4.5075e+02 - logprior: 0.0210
Fitted a model with MAP estimate = -450.1998
Time for alignment: 301.8763
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 566.7805 - loglik: -5.6507e+02 - logprior: -1.7146e+00
Epoch 2/10
39/39 - 10s - loss: 489.6158 - loglik: -4.8843e+02 - logprior: -1.1830e+00
Epoch 3/10
39/39 - 10s - loss: 480.5848 - loglik: -4.7941e+02 - logprior: -1.1743e+00
Epoch 4/10
39/39 - 10s - loss: 478.4577 - loglik: -4.7730e+02 - logprior: -1.1566e+00
Epoch 5/10
39/39 - 10s - loss: 477.4565 - loglik: -4.7630e+02 - logprior: -1.1525e+00
Epoch 6/10
39/39 - 10s - loss: 477.2964 - loglik: -4.7613e+02 - logprior: -1.1635e+00
Epoch 7/10
39/39 - 10s - loss: 477.0267 - loglik: -4.7586e+02 - logprior: -1.1665e+00
Epoch 8/10
39/39 - 10s - loss: 477.1680 - loglik: -4.7600e+02 - logprior: -1.1676e+00
Fitted a model with MAP estimate = -474.5973
expansions: [(0, 3), (12, 1), (18, 2), (19, 2), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (69, 1), (71, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 2), (136, 1), (138, 1), (141, 2), (149, 3), (150, 2), (151, 3), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 471.1071 - loglik: -4.6857e+02 - logprior: -2.5402e+00
Epoch 2/2
39/39 - 14s - loss: 459.6048 - loglik: -4.5886e+02 - logprior: -7.4393e-01
Fitted a model with MAP estimate = -455.6441
expansions: []
discards: [ 23  26  44  57  95 114 178 191 195]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 462.6573 - loglik: -4.6093e+02 - logprior: -1.7273e+00
Epoch 2/2
39/39 - 13s - loss: 459.2183 - loglik: -4.5874e+02 - logprior: -4.7420e-01
Fitted a model with MAP estimate = -455.5444
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 459.7611 - loglik: -4.5818e+02 - logprior: -1.5833e+00
Epoch 2/10
39/39 - 13s - loss: 456.8417 - loglik: -4.5649e+02 - logprior: -3.4705e-01
Epoch 3/10
39/39 - 13s - loss: 455.1241 - loglik: -4.5489e+02 - logprior: -2.3221e-01
Epoch 4/10
39/39 - 13s - loss: 453.5884 - loglik: -4.5342e+02 - logprior: -1.6888e-01
Epoch 5/10
39/39 - 13s - loss: 452.5894 - loglik: -4.5246e+02 - logprior: -1.2548e-01
Epoch 6/10
39/39 - 14s - loss: 452.8083 - loglik: -4.5274e+02 - logprior: -7.0061e-02
Fitted a model with MAP estimate = -452.0420
Time for alignment: 297.3546
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 567.4143 - loglik: -5.6568e+02 - logprior: -1.7345e+00
Epoch 2/10
39/39 - 10s - loss: 490.8870 - loglik: -4.8970e+02 - logprior: -1.1869e+00
Epoch 3/10
39/39 - 10s - loss: 481.9510 - loglik: -4.8081e+02 - logprior: -1.1410e+00
Epoch 4/10
39/39 - 10s - loss: 479.4687 - loglik: -4.7833e+02 - logprior: -1.1393e+00
Epoch 5/10
39/39 - 10s - loss: 478.4403 - loglik: -4.7731e+02 - logprior: -1.1297e+00
Epoch 6/10
39/39 - 10s - loss: 478.1450 - loglik: -4.7701e+02 - logprior: -1.1371e+00
Epoch 7/10
39/39 - 10s - loss: 478.1245 - loglik: -4.7698e+02 - logprior: -1.1419e+00
Epoch 8/10
39/39 - 10s - loss: 478.2649 - loglik: -4.7712e+02 - logprior: -1.1480e+00
Fitted a model with MAP estimate = -475.5227
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (75, 1), (76, 2), (86, 1), (87, 2), (88, 1), (89, 1), (90, 1), (102, 1), (103, 1), (109, 1), (113, 1), (116, 1), (125, 2), (136, 1), (142, 4), (148, 2), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 471.6727 - loglik: -4.6914e+02 - logprior: -2.5370e+00
Epoch 2/2
39/39 - 14s - loss: 459.4221 - loglik: -4.5867e+02 - logprior: -7.5634e-01
Fitted a model with MAP estimate = -455.2469
expansions: []
discards: [ 24  55  93 108 177 178 191]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 462.3865 - loglik: -4.6069e+02 - logprior: -1.7005e+00
Epoch 2/2
39/39 - 13s - loss: 459.1538 - loglik: -4.5869e+02 - logprior: -4.6169e-01
Fitted a model with MAP estimate = -455.4140
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 459.5070 - loglik: -4.5795e+02 - logprior: -1.5580e+00
Epoch 2/10
39/39 - 14s - loss: 456.5584 - loglik: -4.5623e+02 - logprior: -3.3240e-01
Epoch 3/10
39/39 - 14s - loss: 454.9879 - loglik: -4.5477e+02 - logprior: -2.2034e-01
Epoch 4/10
39/39 - 14s - loss: 453.5003 - loglik: -4.5335e+02 - logprior: -1.5220e-01
Epoch 5/10
39/39 - 14s - loss: 452.2988 - loglik: -4.5218e+02 - logprior: -1.1833e-01
Epoch 6/10
39/39 - 14s - loss: 452.5016 - loglik: -4.5245e+02 - logprior: -5.5391e-02
Fitted a model with MAP estimate = -451.7094
Time for alignment: 298.2817
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 567.7582 - loglik: -5.6604e+02 - logprior: -1.7196e+00
Epoch 2/10
39/39 - 10s - loss: 489.4408 - loglik: -4.8818e+02 - logprior: -1.2593e+00
Epoch 3/10
39/39 - 10s - loss: 480.1721 - loglik: -4.7896e+02 - logprior: -1.2166e+00
Epoch 4/10
39/39 - 10s - loss: 477.8005 - loglik: -4.7662e+02 - logprior: -1.1833e+00
Epoch 5/10
39/39 - 10s - loss: 476.8148 - loglik: -4.7562e+02 - logprior: -1.1907e+00
Epoch 6/10
39/39 - 10s - loss: 476.4580 - loglik: -4.7525e+02 - logprior: -1.2100e+00
Epoch 7/10
39/39 - 10s - loss: 476.4912 - loglik: -4.7528e+02 - logprior: -1.2157e+00
Fitted a model with MAP estimate = -473.8261
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (126, 1), (138, 1), (140, 2), (141, 2), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 470.6298 - loglik: -4.6812e+02 - logprior: -2.5113e+00
Epoch 2/2
39/39 - 14s - loss: 459.5883 - loglik: -4.5888e+02 - logprior: -7.0377e-01
Fitted a model with MAP estimate = -455.4771
expansions: [(186, 1)]
discards: [ 26  56 112 156 177 191]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 462.1093 - loglik: -4.6040e+02 - logprior: -1.7110e+00
Epoch 2/2
39/39 - 14s - loss: 458.7328 - loglik: -4.5826e+02 - logprior: -4.7746e-01
Fitted a model with MAP estimate = -454.9473
expansions: []
discards: [174]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 459.5665 - loglik: -4.5799e+02 - logprior: -1.5811e+00
Epoch 2/10
39/39 - 14s - loss: 456.3690 - loglik: -4.5602e+02 - logprior: -3.4899e-01
Epoch 3/10
39/39 - 14s - loss: 454.9147 - loglik: -4.5467e+02 - logprior: -2.4160e-01
Epoch 4/10
39/39 - 14s - loss: 453.2113 - loglik: -4.5303e+02 - logprior: -1.7643e-01
Epoch 5/10
39/39 - 14s - loss: 452.2765 - loglik: -4.5214e+02 - logprior: -1.3239e-01
Epoch 6/10
39/39 - 14s - loss: 452.1169 - loglik: -4.5204e+02 - logprior: -7.8834e-02
Epoch 7/10
39/39 - 14s - loss: 451.8760 - loglik: -4.5187e+02 - logprior: -8.1520e-03
Epoch 8/10
39/39 - 14s - loss: 451.6425 - loglik: -4.5169e+02 - logprior: 0.0494
Epoch 9/10
39/39 - 13s - loss: 452.0212 - loglik: -4.5215e+02 - logprior: 0.1269
Fitted a model with MAP estimate = -451.0978
Time for alignment: 330.3099
Computed alignments with likelihoods: ['-450.5782', '-450.1998', '-452.0420', '-451.7094', '-451.0978']
Best model has likelihood: -450.1998  (prior= 0.0963 )
time for generating output: 0.2890
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7211024006699545
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8867 - loglik: -1.6170e+02 - logprior: -3.1892e+00
Epoch 2/10
19/19 - 1s - loss: 137.2249 - loglik: -1.3582e+02 - logprior: -1.4008e+00
Epoch 3/10
19/19 - 1s - loss: 128.6706 - loglik: -1.2717e+02 - logprior: -1.4982e+00
Epoch 4/10
19/19 - 1s - loss: 127.1024 - loglik: -1.2577e+02 - logprior: -1.3345e+00
Epoch 5/10
19/19 - 1s - loss: 126.5410 - loglik: -1.2521e+02 - logprior: -1.3343e+00
Epoch 6/10
19/19 - 1s - loss: 126.5420 - loglik: -1.2523e+02 - logprior: -1.3083e+00
Fitted a model with MAP estimate = -126.3002
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.6623 - loglik: -1.2960e+02 - logprior: -4.0673e+00
Epoch 2/2
19/19 - 1s - loss: 126.6108 - loglik: -1.2450e+02 - logprior: -2.1060e+00
Fitted a model with MAP estimate = -124.0296
expansions: []
discards: [13 14 42 43 52]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.1492 - loglik: -1.2283e+02 - logprior: -3.3218e+00
Epoch 2/2
19/19 - 1s - loss: 122.9313 - loglik: -1.2156e+02 - logprior: -1.3680e+00
Fitted a model with MAP estimate = -122.5735
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.4310 - loglik: -1.2223e+02 - logprior: -3.1994e+00
Epoch 2/10
19/19 - 1s - loss: 122.8289 - loglik: -1.2149e+02 - logprior: -1.3358e+00
Epoch 3/10
19/19 - 1s - loss: 122.5103 - loglik: -1.2127e+02 - logprior: -1.2405e+00
Epoch 4/10
19/19 - 1s - loss: 122.3208 - loglik: -1.2114e+02 - logprior: -1.1830e+00
Epoch 5/10
19/19 - 1s - loss: 122.0331 - loglik: -1.2089e+02 - logprior: -1.1467e+00
Epoch 6/10
19/19 - 1s - loss: 121.9418 - loglik: -1.2081e+02 - logprior: -1.1309e+00
Epoch 7/10
19/19 - 1s - loss: 121.9182 - loglik: -1.2081e+02 - logprior: -1.1111e+00
Epoch 8/10
19/19 - 1s - loss: 121.9598 - loglik: -1.2086e+02 - logprior: -1.0979e+00
Fitted a model with MAP estimate = -121.8071
Time for alignment: 39.9245
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9706 - loglik: -1.6178e+02 - logprior: -3.1886e+00
Epoch 2/10
19/19 - 1s - loss: 136.4298 - loglik: -1.3503e+02 - logprior: -1.4022e+00
Epoch 3/10
19/19 - 1s - loss: 128.3031 - loglik: -1.2680e+02 - logprior: -1.5003e+00
Epoch 4/10
19/19 - 1s - loss: 126.9519 - loglik: -1.2561e+02 - logprior: -1.3416e+00
Epoch 5/10
19/19 - 1s - loss: 126.7336 - loglik: -1.2540e+02 - logprior: -1.3363e+00
Epoch 6/10
19/19 - 1s - loss: 126.2922 - loglik: -1.2498e+02 - logprior: -1.3111e+00
Epoch 7/10
19/19 - 1s - loss: 126.2902 - loglik: -1.2499e+02 - logprior: -1.2984e+00
Epoch 8/10
19/19 - 1s - loss: 126.3580 - loglik: -1.2507e+02 - logprior: -1.2904e+00
Fitted a model with MAP estimate = -126.2099
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.4035 - loglik: -1.2935e+02 - logprior: -4.0578e+00
Epoch 2/2
19/19 - 1s - loss: 126.7298 - loglik: -1.2460e+02 - logprior: -2.1258e+00
Fitted a model with MAP estimate = -124.2057
expansions: []
discards: [13 16 17 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4583 - loglik: -1.2314e+02 - logprior: -3.3191e+00
Epoch 2/2
19/19 - 1s - loss: 123.1177 - loglik: -1.2176e+02 - logprior: -1.3599e+00
Fitted a model with MAP estimate = -122.7065
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5122 - loglik: -1.2231e+02 - logprior: -3.1997e+00
Epoch 2/10
19/19 - 1s - loss: 122.9500 - loglik: -1.2162e+02 - logprior: -1.3348e+00
Epoch 3/10
19/19 - 1s - loss: 122.5528 - loglik: -1.2132e+02 - logprior: -1.2354e+00
Epoch 4/10
19/19 - 1s - loss: 122.3564 - loglik: -1.2117e+02 - logprior: -1.1848e+00
Epoch 5/10
19/19 - 1s - loss: 122.1518 - loglik: -1.2101e+02 - logprior: -1.1421e+00
Epoch 6/10
19/19 - 1s - loss: 122.1391 - loglik: -1.2101e+02 - logprior: -1.1279e+00
Epoch 7/10
19/19 - 1s - loss: 121.9002 - loglik: -1.2080e+02 - logprior: -1.1040e+00
Epoch 8/10
19/19 - 1s - loss: 122.0481 - loglik: -1.2095e+02 - logprior: -1.0975e+00
Fitted a model with MAP estimate = -121.9226
Time for alignment: 41.3576
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.6347 - loglik: -1.6145e+02 - logprior: -3.1871e+00
Epoch 2/10
19/19 - 1s - loss: 135.9408 - loglik: -1.3455e+02 - logprior: -1.3950e+00
Epoch 3/10
19/19 - 1s - loss: 128.3602 - loglik: -1.2689e+02 - logprior: -1.4702e+00
Epoch 4/10
19/19 - 1s - loss: 126.6992 - loglik: -1.2539e+02 - logprior: -1.3102e+00
Epoch 5/10
19/19 - 1s - loss: 126.1909 - loglik: -1.2487e+02 - logprior: -1.3172e+00
Epoch 6/10
19/19 - 1s - loss: 126.1362 - loglik: -1.2484e+02 - logprior: -1.2934e+00
Epoch 7/10
19/19 - 1s - loss: 126.0393 - loglik: -1.2476e+02 - logprior: -1.2826e+00
Epoch 8/10
19/19 - 1s - loss: 125.8355 - loglik: -1.2456e+02 - logprior: -1.2750e+00
Epoch 9/10
19/19 - 1s - loss: 125.9227 - loglik: -1.2465e+02 - logprior: -1.2680e+00
Fitted a model with MAP estimate = -125.8653
expansions: [(11, 5), (12, 2), (13, 1), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7138 - loglik: -1.2965e+02 - logprior: -4.0633e+00
Epoch 2/2
19/19 - 1s - loss: 126.7855 - loglik: -1.2463e+02 - logprior: -2.1518e+00
Fitted a model with MAP estimate = -124.4059
expansions: []
discards: [12 13 33 40 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5882 - loglik: -1.2326e+02 - logprior: -3.3278e+00
Epoch 2/2
19/19 - 1s - loss: 123.1682 - loglik: -1.2182e+02 - logprior: -1.3470e+00
Fitted a model with MAP estimate = -122.7677
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6419 - loglik: -1.2245e+02 - logprior: -3.1911e+00
Epoch 2/10
19/19 - 1s - loss: 123.0224 - loglik: -1.2169e+02 - logprior: -1.3286e+00
Epoch 3/10
19/19 - 1s - loss: 122.6999 - loglik: -1.2147e+02 - logprior: -1.2304e+00
Epoch 4/10
19/19 - 1s - loss: 122.2747 - loglik: -1.2110e+02 - logprior: -1.1719e+00
Epoch 5/10
19/19 - 1s - loss: 122.2615 - loglik: -1.2113e+02 - logprior: -1.1362e+00
Epoch 6/10
19/19 - 1s - loss: 122.0944 - loglik: -1.2097e+02 - logprior: -1.1253e+00
Epoch 7/10
19/19 - 1s - loss: 122.1406 - loglik: -1.2104e+02 - logprior: -1.1048e+00
Fitted a model with MAP estimate = -121.9942
Time for alignment: 42.5415
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.0196 - loglik: -1.6183e+02 - logprior: -3.1882e+00
Epoch 2/10
19/19 - 1s - loss: 136.5951 - loglik: -1.3520e+02 - logprior: -1.3990e+00
Epoch 3/10
19/19 - 1s - loss: 128.9172 - loglik: -1.2743e+02 - logprior: -1.4907e+00
Epoch 4/10
19/19 - 1s - loss: 127.0157 - loglik: -1.2569e+02 - logprior: -1.3265e+00
Epoch 5/10
19/19 - 1s - loss: 126.5842 - loglik: -1.2526e+02 - logprior: -1.3241e+00
Epoch 6/10
19/19 - 1s - loss: 126.2321 - loglik: -1.2493e+02 - logprior: -1.2993e+00
Epoch 7/10
19/19 - 1s - loss: 126.2128 - loglik: -1.2492e+02 - logprior: -1.2897e+00
Epoch 8/10
19/19 - 1s - loss: 126.0295 - loglik: -1.2475e+02 - logprior: -1.2824e+00
Epoch 9/10
19/19 - 1s - loss: 126.2256 - loglik: -1.2495e+02 - logprior: -1.2754e+00
Fitted a model with MAP estimate = -126.0460
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.7888 - loglik: -1.2971e+02 - logprior: -4.0802e+00
Epoch 2/2
19/19 - 1s - loss: 126.8657 - loglik: -1.2469e+02 - logprior: -2.1713e+00
Fitted a model with MAP estimate = -124.4245
expansions: []
discards: [12 13 16 34 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.6547 - loglik: -1.2333e+02 - logprior: -3.3269e+00
Epoch 2/2
19/19 - 1s - loss: 123.2106 - loglik: -1.2186e+02 - logprior: -1.3505e+00
Fitted a model with MAP estimate = -122.7785
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5940 - loglik: -1.2241e+02 - logprior: -3.1850e+00
Epoch 2/10
19/19 - 1s - loss: 123.1111 - loglik: -1.2178e+02 - logprior: -1.3287e+00
Epoch 3/10
19/19 - 1s - loss: 122.7041 - loglik: -1.2147e+02 - logprior: -1.2295e+00
Epoch 4/10
19/19 - 1s - loss: 122.4739 - loglik: -1.2130e+02 - logprior: -1.1731e+00
Epoch 5/10
19/19 - 1s - loss: 122.2822 - loglik: -1.2114e+02 - logprior: -1.1429e+00
Epoch 6/10
19/19 - 1s - loss: 122.1574 - loglik: -1.2104e+02 - logprior: -1.1190e+00
Epoch 7/10
19/19 - 1s - loss: 122.1856 - loglik: -1.2108e+02 - logprior: -1.1065e+00
Fitted a model with MAP estimate = -122.0581
Time for alignment: 41.4138
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.8515 - loglik: -1.6166e+02 - logprior: -3.1872e+00
Epoch 2/10
19/19 - 1s - loss: 136.0788 - loglik: -1.3468e+02 - logprior: -1.3948e+00
Epoch 3/10
19/19 - 1s - loss: 128.7605 - loglik: -1.2728e+02 - logprior: -1.4774e+00
Epoch 4/10
19/19 - 1s - loss: 127.3609 - loglik: -1.2604e+02 - logprior: -1.3191e+00
Epoch 5/10
19/19 - 1s - loss: 127.0509 - loglik: -1.2573e+02 - logprior: -1.3230e+00
Epoch 6/10
19/19 - 1s - loss: 126.6250 - loglik: -1.2532e+02 - logprior: -1.3043e+00
Epoch 7/10
19/19 - 1s - loss: 126.6076 - loglik: -1.2531e+02 - logprior: -1.2963e+00
Epoch 8/10
19/19 - 1s - loss: 126.7062 - loglik: -1.2542e+02 - logprior: -1.2816e+00
Fitted a model with MAP estimate = -126.5487
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (28, 2), (30, 2), (31, 1), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.6578 - loglik: -1.2960e+02 - logprior: -4.0573e+00
Epoch 2/2
19/19 - 1s - loss: 126.6338 - loglik: -1.2449e+02 - logprior: -2.1459e+00
Fitted a model with MAP estimate = -124.2146
expansions: []
discards: [13 14 37 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.2446 - loglik: -1.2290e+02 - logprior: -3.3402e+00
Epoch 2/2
19/19 - 1s - loss: 122.9781 - loglik: -1.2162e+02 - logprior: -1.3565e+00
Fitted a model with MAP estimate = -122.5723
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.6815 - loglik: -1.2249e+02 - logprior: -3.1946e+00
Epoch 2/10
19/19 - 1s - loss: 123.0779 - loglik: -1.2175e+02 - logprior: -1.3288e+00
Epoch 3/10
19/19 - 1s - loss: 122.6675 - loglik: -1.2144e+02 - logprior: -1.2296e+00
Epoch 4/10
19/19 - 1s - loss: 122.4840 - loglik: -1.2131e+02 - logprior: -1.1776e+00
Epoch 5/10
19/19 - 1s - loss: 122.2632 - loglik: -1.2112e+02 - logprior: -1.1400e+00
Epoch 6/10
19/19 - 1s - loss: 122.1312 - loglik: -1.2101e+02 - logprior: -1.1237e+00
Epoch 7/10
19/19 - 1s - loss: 122.1989 - loglik: -1.2109e+02 - logprior: -1.1095e+00
Fitted a model with MAP estimate = -122.0540
Time for alignment: 41.2042
Computed alignments with likelihoods: ['-121.8071', '-121.9226', '-121.9942', '-122.0581', '-122.0540']
Best model has likelihood: -121.8071  (prior= -1.1033 )
time for generating output: 0.1174
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8855421686746988
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 898.1024 - loglik: -8.9665e+02 - logprior: -1.4571e+00
Epoch 2/10
39/39 - 65s - loss: 721.5255 - loglik: -7.2037e+02 - logprior: -1.1518e+00
Epoch 3/10
39/39 - 64s - loss: 708.2105 - loglik: -7.0716e+02 - logprior: -1.0548e+00
Epoch 4/10
39/39 - 65s - loss: 705.0493 - loglik: -7.0409e+02 - logprior: -9.6408e-01
Epoch 5/10
39/39 - 65s - loss: 703.7860 - loglik: -7.0284e+02 - logprior: -9.4788e-01
Epoch 6/10
39/39 - 71s - loss: 702.6605 - loglik: -7.0169e+02 - logprior: -9.6594e-01
Epoch 7/10
39/39 - 74s - loss: 703.0008 - loglik: -7.0204e+02 - logprior: -9.6009e-01
Fitted a model with MAP estimate = -701.8444
expansions: [(0, 3), (39, 1), (43, 1), (134, 1), (164, 1), (166, 1), (178, 4), (179, 5), (181, 1), (182, 1), (193, 2), (194, 7), (195, 1), (198, 1), (199, 3), (200, 2), (201, 1), (202, 1), (204, 1), (205, 1), (207, 1), (209, 1), (212, 1), (216, 2), (217, 1), (218, 2), (223, 1), (224, 8), (226, 1), (227, 2), (228, 3), (229, 2), (230, 1), (241, 1), (245, 2), (246, 2), (247, 3), (249, 2), (250, 5), (251, 2), (252, 1), (254, 1), (267, 1), (269, 1), (270, 1), (271, 1), (286, 2), (287, 2), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (325, 1), (329, 1), (338, 1), (355, 5)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 111s - loss: 675.6025 - loglik: -6.7361e+02 - logprior: -1.9882e+00
Epoch 2/2
39/39 - 106s - loss: 656.7341 - loglik: -6.5610e+02 - logprior: -6.3108e-01
Fitted a model with MAP estimate = -653.8920
expansions: [(218, 1), (283, 1)]
discards: [  2 187 188 189 190 191 215 216 261 274 289 316 317 318 319 320 375]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 110s - loss: 664.1796 - loglik: -6.6272e+02 - logprior: -1.4616e+00
Epoch 2/2
39/39 - 104s - loss: 658.7678 - loglik: -6.5859e+02 - logprior: -1.7747e-01
Fitted a model with MAP estimate = -656.8999
expansions: [(304, 1), (305, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 106s - loss: 661.4996 - loglik: -6.6055e+02 - logprior: -9.4919e-01
Epoch 2/10
39/39 - 108s - loss: 658.2964 - loglik: -6.5855e+02 - logprior: 0.2518
Epoch 3/10
39/39 - 109s - loss: 656.7697 - loglik: -6.5707e+02 - logprior: 0.3038
Epoch 4/10
39/39 - 101s - loss: 654.8759 - loglik: -6.5541e+02 - logprior: 0.5382
Epoch 5/10
39/39 - 99s - loss: 654.7737 - loglik: -6.5559e+02 - logprior: 0.8172
Epoch 6/10
39/39 - 104s - loss: 654.1783 - loglik: -6.5508e+02 - logprior: 0.9030
Epoch 7/10
39/39 - 112s - loss: 653.6931 - loglik: -6.5457e+02 - logprior: 0.8774
Epoch 8/10
39/39 - 110s - loss: 653.3255 - loglik: -6.5451e+02 - logprior: 1.1874
Epoch 9/10
39/39 - 104s - loss: 653.6029 - loglik: -6.5501e+02 - logprior: 1.4118
Fitted a model with MAP estimate = -652.5038
Time for alignment: 2348.9000
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 903.6311 - loglik: -9.0227e+02 - logprior: -1.3606e+00
Epoch 2/10
39/39 - 65s - loss: 720.0259 - loglik: -7.1883e+02 - logprior: -1.1994e+00
Epoch 3/10
39/39 - 68s - loss: 706.1450 - loglik: -7.0496e+02 - logprior: -1.1866e+00
Epoch 4/10
39/39 - 74s - loss: 703.2328 - loglik: -7.0217e+02 - logprior: -1.0670e+00
Epoch 5/10
39/39 - 75s - loss: 701.9784 - loglik: -7.0091e+02 - logprior: -1.0649e+00
Epoch 6/10
39/39 - 63s - loss: 701.7050 - loglik: -7.0065e+02 - logprior: -1.0514e+00
Epoch 7/10
39/39 - 55s - loss: 701.4078 - loglik: -7.0032e+02 - logprior: -1.0894e+00
Epoch 8/10
39/39 - 52s - loss: 700.3367 - loglik: -6.9929e+02 - logprior: -1.0484e+00
Epoch 9/10
39/39 - 53s - loss: 700.9502 - loglik: -6.9991e+02 - logprior: -1.0431e+00
Fitted a model with MAP estimate = -700.0555
expansions: [(0, 3), (39, 1), (43, 1), (106, 1), (134, 1), (164, 1), (166, 1), (170, 1), (177, 4), (178, 6), (179, 1), (180, 1), (191, 2), (192, 7), (193, 1), (196, 1), (197, 3), (198, 1), (200, 1), (201, 2), (202, 2), (204, 1), (208, 1), (211, 1), (212, 1), (217, 1), (218, 1), (222, 2), (223, 5), (224, 2), (227, 1), (228, 2), (229, 3), (230, 2), (231, 1), (242, 1), (244, 1), (245, 1), (246, 2), (247, 3), (250, 6), (254, 1), (256, 1), (269, 1), (271, 1), (273, 1), (287, 1), (288, 2), (289, 1), (290, 2), (292, 2), (295, 1), (301, 1), (303, 1), (304, 1), (305, 1), (317, 1), (327, 1), (330, 1), (350, 1), (354, 1), (355, 6)]
discards: [  1   2   3 127 252]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 104s - loss: 677.1766 - loglik: -6.7520e+02 - logprior: -1.9755e+00
Epoch 2/2
39/39 - 102s - loss: 658.2379 - loglik: -6.5746e+02 - logprior: -7.7528e-01
Fitted a model with MAP estimate = -655.9239
expansions: [(217, 1), (283, 1), (322, 1)]
discards: [186 187 188 189 190 214 215 238 272 275 276 277 278 279 280 289 326 327
 457]
Re-initialized the encoder parameters.
Fitting a model of length 443 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 102s - loss: 668.6564 - loglik: -6.6732e+02 - logprior: -1.3369e+00
Epoch 2/2
39/39 - 96s - loss: 663.6749 - loglik: -6.6333e+02 - logprior: -3.4306e-01
Fitted a model with MAP estimate = -660.6983
expansions: [(263, 4), (267, 1), (305, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 111s - loss: 662.8530 - loglik: -6.6173e+02 - logprior: -1.1205e+00
Epoch 2/10
39/39 - 101s - loss: 658.8008 - loglik: -6.5897e+02 - logprior: 0.1658
Epoch 3/10
39/39 - 109s - loss: 657.7552 - loglik: -6.5812e+02 - logprior: 0.3694
Epoch 4/10
39/39 - 115s - loss: 655.4800 - loglik: -6.5611e+02 - logprior: 0.6278
Epoch 5/10
39/39 - 114s - loss: 655.6722 - loglik: -6.5630e+02 - logprior: 0.6284
Fitted a model with MAP estimate = -654.6779
Time for alignment: 2016.4424
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 906.7897 - loglik: -9.0532e+02 - logprior: -1.4689e+00
Epoch 2/10
39/39 - 67s - loss: 719.6324 - loglik: -7.1817e+02 - logprior: -1.4589e+00
Epoch 3/10
39/39 - 72s - loss: 706.3172 - loglik: -7.0461e+02 - logprior: -1.7079e+00
Epoch 4/10
39/39 - 78s - loss: 703.5226 - loglik: -7.0181e+02 - logprior: -1.7128e+00
Epoch 5/10
39/39 - 76s - loss: 702.3979 - loglik: -7.0078e+02 - logprior: -1.6164e+00
Epoch 6/10
39/39 - 76s - loss: 701.9536 - loglik: -7.0026e+02 - logprior: -1.6960e+00
Epoch 7/10
39/39 - 73s - loss: 701.2874 - loglik: -6.9958e+02 - logprior: -1.7113e+00
Epoch 8/10
39/39 - 68s - loss: 702.3254 - loglik: -7.0074e+02 - logprior: -1.5808e+00
Fitted a model with MAP estimate = -700.5566
expansions: [(0, 3), (37, 1), (43, 1), (88, 1), (134, 1), (144, 1), (147, 1), (163, 1), (164, 1), (168, 1), (174, 8), (175, 1), (176, 1), (177, 1), (185, 1), (187, 1), (188, 1), (189, 4), (190, 1), (191, 1), (194, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 2), (202, 1), (205, 1), (207, 1), (209, 1), (214, 1), (217, 2), (221, 2), (222, 7), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (242, 1), (243, 2), (244, 2), (245, 2), (247, 2), (248, 6), (250, 1), (252, 1), (253, 1), (266, 1), (268, 1), (269, 1), (270, 1), (285, 1), (287, 1), (288, 2), (290, 1), (301, 1), (303, 2), (304, 2), (306, 2), (326, 1), (329, 1), (341, 1), (348, 1), (350, 2), (352, 1)]
discards: [  1   2   3 127]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 120s - loss: 677.7859 - loglik: -6.7562e+02 - logprior: -2.1610e+00
Epoch 2/2
39/39 - 94s - loss: 658.7266 - loglik: -6.5821e+02 - logprior: -5.1751e-01
Fitted a model with MAP estimate = -656.1379
expansions: [(212, 1), (281, 1), (456, 1)]
discards: [  2   3 185 186 213 236 259 272 287 315 316 317 318 319 402]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 664.7597 - loglik: -6.6336e+02 - logprior: -1.4038e+00
Epoch 2/2
39/39 - 87s - loss: 659.4476 - loglik: -6.5940e+02 - logprior: -4.5811e-02
Fitted a model with MAP estimate = -657.0286
expansions: [(0, 3), (210, 1), (305, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 84s - loss: 661.2438 - loglik: -6.5978e+02 - logprior: -1.4599e+00
Epoch 2/10
39/39 - 78s - loss: 657.4603 - loglik: -6.5745e+02 - logprior: -9.6535e-03
Epoch 3/10
39/39 - 77s - loss: 655.5307 - loglik: -6.5571e+02 - logprior: 0.1803
Epoch 4/10
39/39 - 76s - loss: 654.7341 - loglik: -6.5503e+02 - logprior: 0.2947
Epoch 5/10
39/39 - 73s - loss: 653.7935 - loglik: -6.5420e+02 - logprior: 0.4107
Epoch 6/10
39/39 - 72s - loss: 653.1224 - loglik: -6.5381e+02 - logprior: 0.6877
Epoch 7/10
39/39 - 72s - loss: 652.6107 - loglik: -6.5335e+02 - logprior: 0.7378
Epoch 8/10
39/39 - 72s - loss: 653.1957 - loglik: -6.5410e+02 - logprior: 0.9016
Fitted a model with MAP estimate = -652.3634
Time for alignment: 1986.7077
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 902.1805 - loglik: -9.0080e+02 - logprior: -1.3850e+00
Epoch 2/10
39/39 - 49s - loss: 719.9106 - loglik: -7.1894e+02 - logprior: -9.7023e-01
Epoch 3/10
39/39 - 50s - loss: 705.3541 - loglik: -7.0448e+02 - logprior: -8.7389e-01
Epoch 4/10
39/39 - 50s - loss: 703.0599 - loglik: -7.0226e+02 - logprior: -7.9557e-01
Epoch 5/10
39/39 - 51s - loss: 702.3302 - loglik: -7.0156e+02 - logprior: -7.7437e-01
Epoch 6/10
39/39 - 51s - loss: 700.5311 - loglik: -6.9976e+02 - logprior: -7.7033e-01
Epoch 7/10
39/39 - 52s - loss: 700.7975 - loglik: -7.0001e+02 - logprior: -7.8487e-01
Fitted a model with MAP estimate = -700.2383
expansions: [(0, 3), (37, 1), (43, 1), (135, 1), (179, 1), (180, 8), (182, 1), (195, 2), (196, 7), (197, 1), (200, 1), (201, 3), (202, 1), (204, 1), (205, 2), (206, 2), (207, 1), (210, 1), (215, 1), (219, 1), (222, 1), (228, 1), (229, 5), (230, 2), (234, 1), (235, 1), (236, 1), (237, 1), (238, 2), (239, 1), (250, 1), (251, 1), (253, 2), (254, 11), (255, 4), (257, 2), (258, 1), (270, 1), (289, 1), (290, 2), (291, 4), (292, 1), (293, 1), (302, 1), (304, 2), (305, 2), (307, 2), (326, 2), (329, 1), (349, 1), (355, 5)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 675.9744 - loglik: -6.7390e+02 - logprior: -2.0762e+00
Epoch 2/2
39/39 - 87s - loss: 658.4362 - loglik: -6.5784e+02 - logprior: -5.9153e-01
Fitted a model with MAP estimate = -655.4658
expansions: [(216, 1)]
discards: [186 187 188 189 213 214 237 271 289 309 329 398]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 100s - loss: 664.3146 - loglik: -6.6283e+02 - logprior: -1.4859e+00
Epoch 2/2
39/39 - 93s - loss: 660.1665 - loglik: -6.5992e+02 - logprior: -2.4999e-01
Fitted a model with MAP estimate = -658.0344
expansions: [(314, 5), (362, 1)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 113s - loss: 662.5429 - loglik: -6.6149e+02 - logprior: -1.0538e+00
Epoch 2/10
39/39 - 116s - loss: 657.9312 - loglik: -6.5824e+02 - logprior: 0.3079
Epoch 3/10
39/39 - 116s - loss: 656.6070 - loglik: -6.5716e+02 - logprior: 0.5481
Epoch 4/10
39/39 - 102s - loss: 655.4402 - loglik: -6.5601e+02 - logprior: 0.5718
Epoch 5/10
39/39 - 98s - loss: 654.3163 - loglik: -6.5506e+02 - logprior: 0.7416
Epoch 6/10
39/39 - 100s - loss: 653.8046 - loglik: -6.5480e+02 - logprior: 0.9975
Epoch 7/10
39/39 - 103s - loss: 653.1640 - loglik: -6.5427e+02 - logprior: 1.1101
Epoch 8/10
39/39 - 112s - loss: 654.8816 - loglik: -6.5598e+02 - logprior: 1.1013
Fitted a model with MAP estimate = -652.7890
Time for alignment: 2020.3352
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 77s - loss: 904.8716 - loglik: -9.0343e+02 - logprior: -1.4385e+00
Epoch 2/10
39/39 - 66s - loss: 721.1302 - loglik: -7.1979e+02 - logprior: -1.3442e+00
Epoch 3/10
39/39 - 69s - loss: 706.7780 - loglik: -7.0540e+02 - logprior: -1.3788e+00
Epoch 4/10
39/39 - 62s - loss: 703.6950 - loglik: -7.0249e+02 - logprior: -1.2023e+00
Epoch 5/10
39/39 - 69s - loss: 702.4037 - loglik: -7.0123e+02 - logprior: -1.1778e+00
Epoch 6/10
39/39 - 62s - loss: 702.1144 - loglik: -7.0094e+02 - logprior: -1.1724e+00
Epoch 7/10
39/39 - 69s - loss: 701.5917 - loglik: -7.0031e+02 - logprior: -1.2802e+00
Epoch 8/10
39/39 - 76s - loss: 701.4385 - loglik: -7.0014e+02 - logprior: -1.3032e+00
Epoch 9/10
39/39 - 79s - loss: 701.7829 - loglik: -7.0048e+02 - logprior: -1.3040e+00
Fitted a model with MAP estimate = -700.6143
expansions: [(0, 3), (43, 1), (106, 1), (134, 1), (139, 1), (143, 1), (163, 1), (164, 1), (175, 8), (176, 1), (177, 1), (187, 1), (189, 2), (190, 5), (191, 1), (192, 1), (194, 2), (195, 3), (196, 2), (197, 1), (198, 1), (200, 1), (201, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (214, 1), (219, 2), (220, 5), (221, 3), (223, 1), (224, 2), (225, 3), (226, 2), (227, 1), (238, 1), (240, 1), (241, 1), (242, 1), (243, 2), (244, 2), (246, 2), (247, 6), (248, 1), (251, 1), (252, 1), (265, 1), (267, 1), (268, 1), (269, 1), (284, 1), (286, 1), (287, 2), (289, 1), (303, 1), (304, 1), (305, 1), (307, 1), (312, 1), (326, 1), (329, 1), (341, 1), (352, 1), (354, 2), (355, 3)]
discards: [  1   2 127]
Re-initialized the encoder parameters.
Fitting a model of length 460 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 121s - loss: 676.5944 - loglik: -6.7462e+02 - logprior: -1.9699e+00
Epoch 2/2
39/39 - 109s - loss: 658.2903 - loglik: -6.5772e+02 - logprior: -5.6786e-01
Fitted a model with MAP estimate = -655.2759
expansions: [(215, 1), (280, 1)]
discards: [185 186 211 212 270 272 286 313 314 315 316 317 318 319 458]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 664.3095 - loglik: -6.6285e+02 - logprior: -1.4557e+00
Epoch 2/2
39/39 - 118s - loss: 659.3132 - loglik: -6.5917e+02 - logprior: -1.4089e-01
Fitted a model with MAP estimate = -657.5366
expansions: [(307, 2)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 104s - loss: 662.4711 - loglik: -6.6150e+02 - logprior: -9.7041e-01
Epoch 2/10
39/39 - 98s - loss: 658.4346 - loglik: -6.5880e+02 - logprior: 0.3654
Epoch 3/10
39/39 - 101s - loss: 657.0844 - loglik: -6.5756e+02 - logprior: 0.4711
Epoch 4/10
39/39 - 96s - loss: 655.6681 - loglik: -6.5624e+02 - logprior: 0.5712
Epoch 5/10
39/39 - 116s - loss: 655.2530 - loglik: -6.5598e+02 - logprior: 0.7267
Epoch 6/10
39/39 - 102s - loss: 654.6432 - loglik: -6.5567e+02 - logprior: 1.0258
Epoch 7/10
39/39 - 87s - loss: 653.6284 - loglik: -6.5480e+02 - logprior: 1.1762
Epoch 8/10
39/39 - 84s - loss: 654.4944 - loglik: -6.5572e+02 - logprior: 1.2244
Fitted a model with MAP estimate = -653.3275
Time for alignment: 2379.1808
Computed alignments with likelihoods: ['-652.5038', '-654.6779', '-652.3634', '-652.7890', '-653.3275']
Best model has likelihood: -652.3634  (prior= 1.0082 )
time for generating output: 0.3850
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8742488315156911
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 409.4108 - loglik: -4.0650e+02 - logprior: -2.9060e+00
Epoch 2/10
19/19 - 4s - loss: 326.3112 - loglik: -3.2536e+02 - logprior: -9.4824e-01
Epoch 3/10
19/19 - 4s - loss: 297.2400 - loglik: -2.9617e+02 - logprior: -1.0730e+00
Epoch 4/10
19/19 - 4s - loss: 292.0531 - loglik: -2.9102e+02 - logprior: -1.0360e+00
Epoch 5/10
19/19 - 4s - loss: 289.8754 - loglik: -2.8887e+02 - logprior: -1.0051e+00
Epoch 6/10
19/19 - 4s - loss: 288.9289 - loglik: -2.8795e+02 - logprior: -9.7398e-01
Epoch 7/10
19/19 - 4s - loss: 287.9651 - loglik: -2.8701e+02 - logprior: -9.5123e-01
Epoch 8/10
19/19 - 4s - loss: 288.4839 - loglik: -2.8755e+02 - logprior: -9.3751e-01
Fitted a model with MAP estimate = -287.8445
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (59, 2), (60, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 289.6293 - loglik: -2.8575e+02 - logprior: -3.8776e+00
Epoch 2/2
19/19 - 5s - loss: 277.1865 - loglik: -2.7613e+02 - logprior: -1.0520e+00
Fitted a model with MAP estimate = -275.4268
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  73  79 101 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 282.8101 - loglik: -2.7904e+02 - logprior: -3.7664e+00
Epoch 2/2
19/19 - 5s - loss: 277.2689 - loglik: -2.7626e+02 - logprior: -1.0084e+00
Fitted a model with MAP estimate = -276.1262
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 280.4878 - loglik: -2.7751e+02 - logprior: -2.9752e+00
Epoch 2/10
19/19 - 5s - loss: 276.6238 - loglik: -2.7584e+02 - logprior: -7.8190e-01
Epoch 3/10
19/19 - 5s - loss: 275.4896 - loglik: -2.7491e+02 - logprior: -5.7746e-01
Epoch 4/10
19/19 - 5s - loss: 274.7986 - loglik: -2.7434e+02 - logprior: -4.6314e-01
Epoch 5/10
19/19 - 5s - loss: 274.7826 - loglik: -2.7438e+02 - logprior: -4.0496e-01
Epoch 6/10
19/19 - 5s - loss: 272.8293 - loglik: -2.7248e+02 - logprior: -3.4468e-01
Epoch 7/10
19/19 - 5s - loss: 274.2606 - loglik: -2.7395e+02 - logprior: -3.1030e-01
Fitted a model with MAP estimate = -273.1714
Time for alignment: 128.4303
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.0422 - loglik: -4.0614e+02 - logprior: -2.8974e+00
Epoch 2/10
19/19 - 4s - loss: 325.8578 - loglik: -3.2491e+02 - logprior: -9.4582e-01
Epoch 3/10
19/19 - 4s - loss: 296.3663 - loglik: -2.9529e+02 - logprior: -1.0763e+00
Epoch 4/10
19/19 - 4s - loss: 292.2223 - loglik: -2.9119e+02 - logprior: -1.0303e+00
Epoch 5/10
19/19 - 4s - loss: 289.8574 - loglik: -2.8887e+02 - logprior: -9.8401e-01
Epoch 6/10
19/19 - 4s - loss: 288.8700 - loglik: -2.8792e+02 - logprior: -9.4882e-01
Epoch 7/10
19/19 - 4s - loss: 288.7401 - loglik: -2.8781e+02 - logprior: -9.2641e-01
Epoch 8/10
19/19 - 4s - loss: 288.4281 - loglik: -2.8752e+02 - logprior: -9.1159e-01
Epoch 9/10
19/19 - 4s - loss: 288.1731 - loglik: -2.8727e+02 - logprior: -9.0742e-01
Epoch 10/10
19/19 - 4s - loss: 288.3873 - loglik: -2.8748e+02 - logprior: -9.0295e-01
Fitted a model with MAP estimate = -287.8231
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 289.7709 - loglik: -2.8580e+02 - logprior: -3.9736e+00
Epoch 2/2
19/19 - 5s - loss: 277.8893 - loglik: -2.7683e+02 - logprior: -1.0564e+00
Fitted a model with MAP estimate = -275.9576
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  75  76 101 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.2700 - loglik: -2.7950e+02 - logprior: -3.7671e+00
Epoch 2/2
19/19 - 5s - loss: 277.6862 - loglik: -2.7667e+02 - logprior: -1.0200e+00
Fitted a model with MAP estimate = -276.2734
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 280.7003 - loglik: -2.7771e+02 - logprior: -2.9885e+00
Epoch 2/10
19/19 - 5s - loss: 276.6305 - loglik: -2.7582e+02 - logprior: -8.1312e-01
Epoch 3/10
19/19 - 5s - loss: 275.7199 - loglik: -2.7512e+02 - logprior: -6.0198e-01
Epoch 4/10
19/19 - 5s - loss: 274.8973 - loglik: -2.7441e+02 - logprior: -4.9093e-01
Epoch 5/10
19/19 - 5s - loss: 274.3802 - loglik: -2.7398e+02 - logprior: -4.0512e-01
Epoch 6/10
19/19 - 5s - loss: 273.7747 - loglik: -2.7342e+02 - logprior: -3.5869e-01
Epoch 7/10
19/19 - 5s - loss: 273.6082 - loglik: -2.7329e+02 - logprior: -3.1550e-01
Epoch 8/10
19/19 - 5s - loss: 272.5221 - loglik: -2.7224e+02 - logprior: -2.7716e-01
Epoch 9/10
19/19 - 5s - loss: 273.5959 - loglik: -2.7334e+02 - logprior: -2.5116e-01
Fitted a model with MAP estimate = -272.9725
Time for alignment: 145.3740
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.8457 - loglik: -4.0595e+02 - logprior: -2.8969e+00
Epoch 2/10
19/19 - 4s - loss: 326.5448 - loglik: -3.2560e+02 - logprior: -9.4582e-01
Epoch 3/10
19/19 - 4s - loss: 298.4105 - loglik: -2.9731e+02 - logprior: -1.0971e+00
Epoch 4/10
19/19 - 4s - loss: 294.0228 - loglik: -2.9298e+02 - logprior: -1.0447e+00
Epoch 5/10
19/19 - 4s - loss: 293.1175 - loglik: -2.9212e+02 - logprior: -9.9658e-01
Epoch 6/10
19/19 - 4s - loss: 291.4913 - loglik: -2.9052e+02 - logprior: -9.7475e-01
Epoch 7/10
19/19 - 4s - loss: 291.1293 - loglik: -2.9018e+02 - logprior: -9.5184e-01
Epoch 8/10
19/19 - 4s - loss: 291.0692 - loglik: -2.9013e+02 - logprior: -9.4364e-01
Epoch 9/10
19/19 - 4s - loss: 290.0565 - loglik: -2.8912e+02 - logprior: -9.3858e-01
Epoch 10/10
19/19 - 4s - loss: 291.2924 - loglik: -2.9036e+02 - logprior: -9.3373e-01
Fitted a model with MAP estimate = -290.1608
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (85, 1), (86, 1), (97, 1), (98, 1), (100, 1), (105, 1), (106, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 291.1521 - loglik: -2.8725e+02 - logprior: -3.9067e+00
Epoch 2/2
19/19 - 5s - loss: 278.5886 - loglik: -2.7757e+02 - logprior: -1.0194e+00
Fitted a model with MAP estimate = -276.9936
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  74  76 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.9388 - loglik: -2.8015e+02 - logprior: -3.7925e+00
Epoch 2/2
19/19 - 5s - loss: 278.8480 - loglik: -2.7780e+02 - logprior: -1.0529e+00
Fitted a model with MAP estimate = -277.4781
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 281.8524 - loglik: -2.7883e+02 - logprior: -3.0208e+00
Epoch 2/10
19/19 - 5s - loss: 277.8617 - loglik: -2.7701e+02 - logprior: -8.4830e-01
Epoch 3/10
19/19 - 5s - loss: 276.8013 - loglik: -2.7614e+02 - logprior: -6.5785e-01
Epoch 4/10
19/19 - 5s - loss: 276.0026 - loglik: -2.7547e+02 - logprior: -5.3037e-01
Epoch 5/10
19/19 - 5s - loss: 275.9364 - loglik: -2.7549e+02 - logprior: -4.4342e-01
Epoch 6/10
19/19 - 5s - loss: 274.5641 - loglik: -2.7418e+02 - logprior: -3.8775e-01
Epoch 7/10
19/19 - 5s - loss: 274.7324 - loglik: -2.7439e+02 - logprior: -3.4243e-01
Fitted a model with MAP estimate = -274.4770
Time for alignment: 133.0583
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 408.8176 - loglik: -4.0591e+02 - logprior: -2.9034e+00
Epoch 2/10
19/19 - 4s - loss: 325.2027 - loglik: -3.2425e+02 - logprior: -9.4974e-01
Epoch 3/10
19/19 - 4s - loss: 297.1604 - loglik: -2.9607e+02 - logprior: -1.0860e+00
Epoch 4/10
19/19 - 4s - loss: 292.0966 - loglik: -2.9107e+02 - logprior: -1.0292e+00
Epoch 5/10
19/19 - 4s - loss: 290.0012 - loglik: -2.8901e+02 - logprior: -9.8846e-01
Epoch 6/10
19/19 - 4s - loss: 289.4078 - loglik: -2.8844e+02 - logprior: -9.6505e-01
Epoch 7/10
19/19 - 4s - loss: 288.8901 - loglik: -2.8795e+02 - logprior: -9.3961e-01
Epoch 8/10
19/19 - 4s - loss: 288.3580 - loglik: -2.8743e+02 - logprior: -9.3055e-01
Epoch 9/10
19/19 - 4s - loss: 288.3285 - loglik: -2.8741e+02 - logprior: -9.1817e-01
Epoch 10/10
19/19 - 4s - loss: 287.9459 - loglik: -2.8702e+02 - logprior: -9.2128e-01
Fitted a model with MAP estimate = -287.8051
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 1), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 289.9447 - loglik: -2.8598e+02 - logprior: -3.9686e+00
Epoch 2/2
19/19 - 5s - loss: 277.1969 - loglik: -2.7612e+02 - logprior: -1.0789e+00
Fitted a model with MAP estimate = -275.5242
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  74  79 101 102 148]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 282.8259 - loglik: -2.7905e+02 - logprior: -3.7761e+00
Epoch 2/2
19/19 - 5s - loss: 277.5273 - loglik: -2.7651e+02 - logprior: -1.0150e+00
Fitted a model with MAP estimate = -276.1750
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 280.6489 - loglik: -2.7765e+02 - logprior: -2.9958e+00
Epoch 2/10
19/19 - 5s - loss: 276.4391 - loglik: -2.7565e+02 - logprior: -7.8843e-01
Epoch 3/10
19/19 - 5s - loss: 275.6909 - loglik: -2.7509e+02 - logprior: -5.9591e-01
Epoch 4/10
19/19 - 5s - loss: 274.8884 - loglik: -2.7441e+02 - logprior: -4.7603e-01
Epoch 5/10
19/19 - 5s - loss: 274.2368 - loglik: -2.7384e+02 - logprior: -3.9819e-01
Epoch 6/10
19/19 - 5s - loss: 273.6692 - loglik: -2.7332e+02 - logprior: -3.4770e-01
Epoch 7/10
19/19 - 5s - loss: 273.5075 - loglik: -2.7320e+02 - logprior: -3.0742e-01
Epoch 8/10
19/19 - 5s - loss: 273.4670 - loglik: -2.7319e+02 - logprior: -2.7351e-01
Epoch 9/10
19/19 - 5s - loss: 272.7621 - loglik: -2.7252e+02 - logprior: -2.4275e-01
Epoch 10/10
19/19 - 5s - loss: 273.1584 - loglik: -2.7295e+02 - logprior: -2.0868e-01
Fitted a model with MAP estimate = -272.9159
Time for alignment: 149.2480
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.7834 - loglik: -4.0587e+02 - logprior: -2.9090e+00
Epoch 2/10
19/19 - 4s - loss: 325.0898 - loglik: -3.2413e+02 - logprior: -9.6118e-01
Epoch 3/10
19/19 - 4s - loss: 296.2271 - loglik: -2.9509e+02 - logprior: -1.1325e+00
Epoch 4/10
19/19 - 4s - loss: 292.1141 - loglik: -2.9102e+02 - logprior: -1.0950e+00
Epoch 5/10
19/19 - 4s - loss: 290.2574 - loglik: -2.8921e+02 - logprior: -1.0491e+00
Epoch 6/10
19/19 - 4s - loss: 289.0403 - loglik: -2.8802e+02 - logprior: -1.0166e+00
Epoch 7/10
19/19 - 4s - loss: 288.6935 - loglik: -2.8770e+02 - logprior: -9.9412e-01
Epoch 8/10
19/19 - 4s - loss: 288.9163 - loglik: -2.8794e+02 - logprior: -9.8042e-01
Fitted a model with MAP estimate = -288.1937
expansions: [(0, 6), (6, 1), (8, 1), (9, 5), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 289.0743 - loglik: -2.8627e+02 - logprior: -2.8006e+00
Epoch 2/2
19/19 - 5s - loss: 278.5484 - loglik: -2.7764e+02 - logprior: -9.1327e-01
Fitted a model with MAP estimate = -276.7048
expansions: [(0, 4)]
discards: [  1   2   3   4   5  20  72  74  99 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.3347 - loglik: -2.7975e+02 - logprior: -3.5896e+00
Epoch 2/2
19/19 - 5s - loss: 278.6374 - loglik: -2.7775e+02 - logprior: -8.8483e-01
Fitted a model with MAP estimate = -277.2530
expansions: [(0, 5)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 280.6436 - loglik: -2.7774e+02 - logprior: -2.9030e+00
Epoch 2/10
19/19 - 5s - loss: 276.7649 - loglik: -2.7591e+02 - logprior: -8.5913e-01
Epoch 3/10
19/19 - 5s - loss: 275.5813 - loglik: -2.7496e+02 - logprior: -6.2551e-01
Epoch 4/10
19/19 - 5s - loss: 275.2506 - loglik: -2.7471e+02 - logprior: -5.3957e-01
Epoch 5/10
19/19 - 5s - loss: 274.1386 - loglik: -2.7366e+02 - logprior: -4.8351e-01
Epoch 6/10
19/19 - 5s - loss: 274.1395 - loglik: -2.7370e+02 - logprior: -4.3835e-01
Fitted a model with MAP estimate = -273.6675
Time for alignment: 121.6896
Computed alignments with likelihoods: ['-273.1714', '-272.9725', '-274.4770', '-272.9159', '-273.6675']
Best model has likelihood: -272.9159  (prior= -0.1894 )
time for generating output: 0.1486
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6706519168615498
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.7967 - loglik: -6.5431e+01 - logprior: -3.3658e+00
Epoch 2/10
19/19 - 0s - loss: 51.5471 - loglik: -5.0168e+01 - logprior: -1.3794e+00
Epoch 3/10
19/19 - 0s - loss: 45.5530 - loglik: -4.4070e+01 - logprior: -1.4832e+00
Epoch 4/10
19/19 - 0s - loss: 44.4342 - loglik: -4.2972e+01 - logprior: -1.4622e+00
Epoch 5/10
19/19 - 0s - loss: 44.1430 - loglik: -4.2708e+01 - logprior: -1.4350e+00
Epoch 6/10
19/19 - 0s - loss: 43.9863 - loglik: -4.2563e+01 - logprior: -1.4228e+00
Epoch 7/10
19/19 - 0s - loss: 43.9520 - loglik: -4.2543e+01 - logprior: -1.4086e+00
Epoch 8/10
19/19 - 0s - loss: 43.9721 - loglik: -4.2570e+01 - logprior: -1.4017e+00
Fitted a model with MAP estimate = -43.8655
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 46.9301 - loglik: -4.2255e+01 - logprior: -4.6747e+00
Epoch 2/2
19/19 - 0s - loss: 42.2382 - loglik: -4.0840e+01 - logprior: -1.3982e+00
Fitted a model with MAP estimate = -41.4805
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.0189 - loglik: -4.0635e+01 - logprior: -3.3835e+00
Epoch 2/2
19/19 - 0s - loss: 41.4579 - loglik: -3.9995e+01 - logprior: -1.4629e+00
Fitted a model with MAP estimate = -41.2319
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.7222 - loglik: -4.0420e+01 - logprior: -3.3019e+00
Epoch 2/10
19/19 - 0s - loss: 41.3788 - loglik: -3.9931e+01 - logprior: -1.4480e+00
Epoch 3/10
19/19 - 0s - loss: 41.0630 - loglik: -3.9720e+01 - logprior: -1.3428e+00
Epoch 4/10
19/19 - 0s - loss: 40.9185 - loglik: -3.9629e+01 - logprior: -1.2895e+00
Epoch 5/10
19/19 - 0s - loss: 40.7995 - loglik: -3.9541e+01 - logprior: -1.2585e+00
Epoch 6/10
19/19 - 0s - loss: 40.7343 - loglik: -3.9486e+01 - logprior: -1.2486e+00
Epoch 7/10
19/19 - 0s - loss: 40.7108 - loglik: -3.9480e+01 - logprior: -1.2312e+00
Epoch 8/10
19/19 - 0s - loss: 40.6411 - loglik: -3.9419e+01 - logprior: -1.2226e+00
Epoch 9/10
19/19 - 0s - loss: 40.7142 - loglik: -3.9501e+01 - logprior: -1.2131e+00
Fitted a model with MAP estimate = -40.6332
Time for alignment: 28.0440
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8300 - loglik: -6.5462e+01 - logprior: -3.3680e+00
Epoch 2/10
19/19 - 0s - loss: 51.9044 - loglik: -5.0511e+01 - logprior: -1.3938e+00
Epoch 3/10
19/19 - 0s - loss: 45.1385 - loglik: -4.3631e+01 - logprior: -1.5075e+00
Epoch 4/10
19/19 - 0s - loss: 43.9367 - loglik: -4.2464e+01 - logprior: -1.4730e+00
Epoch 5/10
19/19 - 0s - loss: 43.6149 - loglik: -4.2178e+01 - logprior: -1.4368e+00
Epoch 6/10
19/19 - 0s - loss: 43.5940 - loglik: -4.2167e+01 - logprior: -1.4274e+00
Epoch 7/10
19/19 - 0s - loss: 43.4201 - loglik: -4.2006e+01 - logprior: -1.4144e+00
Epoch 8/10
19/19 - 0s - loss: 43.4374 - loglik: -4.2037e+01 - logprior: -1.4003e+00
Fitted a model with MAP estimate = -43.3757
expansions: [(0, 1), (3, 1), (4, 1), (9, 1), (11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 46.7665 - loglik: -4.2097e+01 - logprior: -4.6699e+00
Epoch 2/2
19/19 - 0s - loss: 42.2258 - loglik: -4.0816e+01 - logprior: -1.4103e+00
Fitted a model with MAP estimate = -41.5110
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.4984 - loglik: -4.0348e+01 - logprior: -3.1508e+00
Epoch 2/10
19/19 - 0s - loss: 41.3968 - loglik: -4.0040e+01 - logprior: -1.3572e+00
Epoch 3/10
19/19 - 0s - loss: 41.1417 - loglik: -3.9842e+01 - logprior: -1.2999e+00
Epoch 4/10
19/19 - 0s - loss: 41.0078 - loglik: -3.9747e+01 - logprior: -1.2604e+00
Epoch 5/10
19/19 - 0s - loss: 40.9207 - loglik: -3.9690e+01 - logprior: -1.2311e+00
Epoch 6/10
19/19 - 0s - loss: 40.8326 - loglik: -3.9612e+01 - logprior: -1.2202e+00
Epoch 7/10
19/19 - 0s - loss: 40.8330 - loglik: -3.9624e+01 - logprior: -1.2089e+00
Fitted a model with MAP estimate = -40.7625
Time for alignment: 20.6027
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.6666 - loglik: -6.5300e+01 - logprior: -3.3666e+00
Epoch 2/10
19/19 - 0s - loss: 51.2429 - loglik: -4.9901e+01 - logprior: -1.3420e+00
Epoch 3/10
19/19 - 0s - loss: 46.0900 - loglik: -4.4706e+01 - logprior: -1.3845e+00
Epoch 4/10
19/19 - 0s - loss: 44.5698 - loglik: -4.3123e+01 - logprior: -1.4471e+00
Epoch 5/10
19/19 - 0s - loss: 44.1899 - loglik: -4.2769e+01 - logprior: -1.4206e+00
Epoch 6/10
19/19 - 0s - loss: 43.9839 - loglik: -4.2567e+01 - logprior: -1.4172e+00
Epoch 7/10
19/19 - 0s - loss: 43.9971 - loglik: -4.2587e+01 - logprior: -1.4103e+00
Fitted a model with MAP estimate = -43.9088
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 46.9213 - loglik: -4.2285e+01 - logprior: -4.6359e+00
Epoch 2/2
19/19 - 0s - loss: 42.1934 - loglik: -4.0793e+01 - logprior: -1.4005e+00
Fitted a model with MAP estimate = -41.4825
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.2277 - loglik: -4.0792e+01 - logprior: -3.4358e+00
Epoch 2/2
19/19 - 0s - loss: 41.7839 - loglik: -4.0295e+01 - logprior: -1.4886e+00
Fitted a model with MAP estimate = -41.5078
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.7963 - loglik: -4.0518e+01 - logprior: -3.2780e+00
Epoch 2/10
19/19 - 0s - loss: 41.4318 - loglik: -3.9989e+01 - logprior: -1.4427e+00
Epoch 3/10
19/19 - 0s - loss: 41.0823 - loglik: -3.9738e+01 - logprior: -1.3444e+00
Epoch 4/10
19/19 - 0s - loss: 40.9449 - loglik: -3.9652e+01 - logprior: -1.2926e+00
Epoch 5/10
19/19 - 0s - loss: 40.7767 - loglik: -3.9514e+01 - logprior: -1.2626e+00
Epoch 6/10
19/19 - 0s - loss: 40.7166 - loglik: -3.9473e+01 - logprior: -1.2441e+00
Epoch 7/10
19/19 - 0s - loss: 40.7210 - loglik: -3.9485e+01 - logprior: -1.2355e+00
Fitted a model with MAP estimate = -40.6597
Time for alignment: 25.6402
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8212 - loglik: -6.5448e+01 - logprior: -3.3732e+00
Epoch 2/10
19/19 - 0s - loss: 51.7259 - loglik: -5.0320e+01 - logprior: -1.4058e+00
Epoch 3/10
19/19 - 0s - loss: 45.5943 - loglik: -4.4082e+01 - logprior: -1.5123e+00
Epoch 4/10
19/19 - 0s - loss: 44.3805 - loglik: -4.2917e+01 - logprior: -1.4636e+00
Epoch 5/10
19/19 - 0s - loss: 44.1250 - loglik: -4.2687e+01 - logprior: -1.4381e+00
Epoch 6/10
19/19 - 0s - loss: 44.0658 - loglik: -4.2646e+01 - logprior: -1.4201e+00
Epoch 7/10
19/19 - 0s - loss: 43.9028 - loglik: -4.2491e+01 - logprior: -1.4120e+00
Epoch 8/10
19/19 - 0s - loss: 43.9209 - loglik: -4.2522e+01 - logprior: -1.3985e+00
Fitted a model with MAP estimate = -43.8670
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 46.9263 - loglik: -4.2249e+01 - logprior: -4.6773e+00
Epoch 2/2
19/19 - 0s - loss: 42.2231 - loglik: -4.0821e+01 - logprior: -1.4024e+00
Fitted a model with MAP estimate = -41.5028
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.2464 - loglik: -4.0809e+01 - logprior: -3.4371e+00
Epoch 2/2
19/19 - 0s - loss: 41.7738 - loglik: -4.0291e+01 - logprior: -1.4825e+00
Fitted a model with MAP estimate = -41.4945
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7964 - loglik: -4.0521e+01 - logprior: -3.2749e+00
Epoch 2/10
19/19 - 0s - loss: 41.4208 - loglik: -3.9979e+01 - logprior: -1.4416e+00
Epoch 3/10
19/19 - 0s - loss: 41.0680 - loglik: -3.9723e+01 - logprior: -1.3447e+00
Epoch 4/10
19/19 - 0s - loss: 40.9080 - loglik: -3.9615e+01 - logprior: -1.2934e+00
Epoch 5/10
19/19 - 0s - loss: 40.8328 - loglik: -3.9574e+01 - logprior: -1.2591e+00
Epoch 6/10
19/19 - 0s - loss: 40.7713 - loglik: -3.9527e+01 - logprior: -1.2441e+00
Epoch 7/10
19/19 - 0s - loss: 40.6419 - loglik: -3.9410e+01 - logprior: -1.2317e+00
Epoch 8/10
19/19 - 0s - loss: 40.6961 - loglik: -3.9476e+01 - logprior: -1.2204e+00
Fitted a model with MAP estimate = -40.6345
Time for alignment: 26.0492
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.6579 - loglik: -6.5300e+01 - logprior: -3.3581e+00
Epoch 2/10
19/19 - 0s - loss: 51.2221 - loglik: -4.9881e+01 - logprior: -1.3409e+00
Epoch 3/10
19/19 - 0s - loss: 45.9540 - loglik: -4.4547e+01 - logprior: -1.4065e+00
Epoch 4/10
19/19 - 0s - loss: 44.4636 - loglik: -4.3015e+01 - logprior: -1.4488e+00
Epoch 5/10
19/19 - 0s - loss: 44.1396 - loglik: -4.2708e+01 - logprior: -1.4311e+00
Epoch 6/10
19/19 - 0s - loss: 44.0320 - loglik: -4.2611e+01 - logprior: -1.4212e+00
Epoch 7/10
19/19 - 0s - loss: 43.9230 - loglik: -4.2515e+01 - logprior: -1.4078e+00
Epoch 8/10
19/19 - 0s - loss: 43.9294 - loglik: -4.2530e+01 - logprior: -1.3990e+00
Fitted a model with MAP estimate = -43.8628
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 46.9144 - loglik: -4.2233e+01 - logprior: -4.6818e+00
Epoch 2/2
19/19 - 0s - loss: 42.2284 - loglik: -4.0826e+01 - logprior: -1.4021e+00
Fitted a model with MAP estimate = -41.4894
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.0328 - loglik: -4.0650e+01 - logprior: -3.3826e+00
Epoch 2/2
19/19 - 0s - loss: 41.4755 - loglik: -4.0011e+01 - logprior: -1.4642e+00
Fitted a model with MAP estimate = -41.1865
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7190 - loglik: -4.0418e+01 - logprior: -3.3010e+00
Epoch 2/10
19/19 - 0s - loss: 41.3855 - loglik: -3.9937e+01 - logprior: -1.4488e+00
Epoch 3/10
19/19 - 0s - loss: 41.0719 - loglik: -3.9732e+01 - logprior: -1.3397e+00
Epoch 4/10
19/19 - 0s - loss: 40.9238 - loglik: -3.9637e+01 - logprior: -1.2873e+00
Epoch 5/10
19/19 - 0s - loss: 40.7630 - loglik: -3.9504e+01 - logprior: -1.2588e+00
Epoch 6/10
19/19 - 0s - loss: 40.7878 - loglik: -3.9541e+01 - logprior: -1.2471e+00
Fitted a model with MAP estimate = -40.6873
Time for alignment: 24.9008
Computed alignments with likelihoods: ['-40.6332', '-40.7625', '-40.6597', '-40.6345', '-40.6873']
Best model has likelihood: -40.6332  (prior= -1.2037 )
time for generating output: 0.0784
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.9109149277688604
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 731.1305 - loglik: -7.2964e+02 - logprior: -1.4930e+00
Epoch 2/10
39/39 - 25s - loss: 528.5847 - loglik: -5.2715e+02 - logprior: -1.4314e+00
Epoch 3/10
39/39 - 26s - loss: 517.7794 - loglik: -5.1644e+02 - logprior: -1.3429e+00
Epoch 4/10
39/39 - 26s - loss: 514.7932 - loglik: -5.1352e+02 - logprior: -1.2748e+00
Epoch 5/10
39/39 - 26s - loss: 514.1368 - loglik: -5.1287e+02 - logprior: -1.2686e+00
Epoch 6/10
39/39 - 26s - loss: 513.3340 - loglik: -5.1207e+02 - logprior: -1.2676e+00
Epoch 7/10
39/39 - 25s - loss: 513.4554 - loglik: -5.1218e+02 - logprior: -1.2769e+00
Fitted a model with MAP estimate = -512.4851
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 482.8806 - loglik: -4.8096e+02 - logprior: -1.9198e+00
Epoch 2/2
39/39 - 36s - loss: 466.1862 - loglik: -4.6556e+02 - logprior: -6.2335e-01
Fitted a model with MAP estimate = -463.6684
expansions: []
discards: [  0   1  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 471.6044 - loglik: -4.7019e+02 - logprior: -1.4110e+00
Epoch 2/2
39/39 - 40s - loss: 466.6572 - loglik: -4.6686e+02 - logprior: 0.1997
Fitted a model with MAP estimate = -464.4552
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 469.5009 - loglik: -4.6833e+02 - logprior: -1.1670e+00
Epoch 2/10
39/39 - 43s - loss: 464.6522 - loglik: -4.6473e+02 - logprior: 0.0810
Epoch 3/10
39/39 - 42s - loss: 463.0387 - loglik: -4.6326e+02 - logprior: 0.2195
Epoch 4/10
39/39 - 44s - loss: 461.7231 - loglik: -4.6202e+02 - logprior: 0.2921
Epoch 5/10
39/39 - 45s - loss: 460.9738 - loglik: -4.6146e+02 - logprior: 0.4878
Epoch 6/10
39/39 - 44s - loss: 460.1235 - loglik: -4.6049e+02 - logprior: 0.3636
Epoch 7/10
39/39 - 43s - loss: 460.3840 - loglik: -4.6119e+02 - logprior: 0.8028
Fitted a model with MAP estimate = -459.4939
Time for alignment: 840.9961
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 733.2089 - loglik: -7.3172e+02 - logprior: -1.4896e+00
Epoch 2/10
39/39 - 30s - loss: 530.1235 - loglik: -5.2872e+02 - logprior: -1.4080e+00
Epoch 3/10
39/39 - 31s - loss: 517.5475 - loglik: -5.1618e+02 - logprior: -1.3691e+00
Epoch 4/10
39/39 - 30s - loss: 514.6660 - loglik: -5.1334e+02 - logprior: -1.3258e+00
Epoch 5/10
39/39 - 30s - loss: 513.3875 - loglik: -5.1205e+02 - logprior: -1.3353e+00
Epoch 6/10
39/39 - 30s - loss: 512.0850 - loglik: -5.1073e+02 - logprior: -1.3597e+00
Epoch 7/10
39/39 - 30s - loss: 511.6051 - loglik: -5.1023e+02 - logprior: -1.3719e+00
Epoch 8/10
39/39 - 30s - loss: 511.5512 - loglik: -5.1018e+02 - logprior: -1.3719e+00
Epoch 9/10
39/39 - 31s - loss: 511.5270 - loglik: -5.1015e+02 - logprior: -1.3794e+00
Epoch 10/10
39/39 - 30s - loss: 511.1066 - loglik: -5.0973e+02 - logprior: -1.3774e+00
Fitted a model with MAP estimate = -510.5305
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (40, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (78, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (132, 3), (133, 1), (145, 2), (148, 1), (149, 1), (159, 1), (167, 1), (169, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (230, 2), (231, 1), (244, 1), (257, 1), (265, 1), (267, 1), (270, 1), (271, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 484.3370 - loglik: -4.8236e+02 - logprior: -1.9802e+00
Epoch 2/2
39/39 - 43s - loss: 465.6924 - loglik: -4.6513e+02 - logprior: -5.6274e-01
Fitted a model with MAP estimate = -463.2842
expansions: [(0, 2)]
discards: [  0   1  97  98 138 142 166 182 217]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 469.9524 - loglik: -4.6853e+02 - logprior: -1.4185e+00
Epoch 2/2
39/39 - 39s - loss: 465.3562 - loglik: -4.6528e+02 - logprior: -7.1583e-02
Fitted a model with MAP estimate = -462.9861
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 470.9464 - loglik: -4.6916e+02 - logprior: -1.7901e+00
Epoch 2/10
39/39 - 35s - loss: 465.6253 - loglik: -4.6572e+02 - logprior: 0.0898
Epoch 3/10
39/39 - 35s - loss: 463.7051 - loglik: -4.6393e+02 - logprior: 0.2259
Epoch 4/10
39/39 - 35s - loss: 461.9489 - loglik: -4.6244e+02 - logprior: 0.4941
Epoch 5/10
39/39 - 35s - loss: 460.3299 - loglik: -4.6105e+02 - logprior: 0.7249
Epoch 6/10
39/39 - 35s - loss: 460.8169 - loglik: -4.6123e+02 - logprior: 0.4155
Fitted a model with MAP estimate = -460.0126
Time for alignment: 889.3876
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 735.8676 - loglik: -7.3437e+02 - logprior: -1.4993e+00
Epoch 2/10
39/39 - 24s - loss: 529.9122 - loglik: -5.2841e+02 - logprior: -1.5060e+00
Epoch 3/10
39/39 - 25s - loss: 516.6132 - loglik: -5.1514e+02 - logprior: -1.4717e+00
Epoch 4/10
39/39 - 25s - loss: 513.7368 - loglik: -5.1234e+02 - logprior: -1.3934e+00
Epoch 5/10
39/39 - 25s - loss: 512.7623 - loglik: -5.1138e+02 - logprior: -1.3818e+00
Epoch 6/10
39/39 - 25s - loss: 512.3702 - loglik: -5.1097e+02 - logprior: -1.3973e+00
Epoch 7/10
39/39 - 25s - loss: 512.0646 - loglik: -5.1066e+02 - logprior: -1.4024e+00
Epoch 8/10
39/39 - 25s - loss: 511.8016 - loglik: -5.1040e+02 - logprior: -1.4037e+00
Epoch 9/10
39/39 - 25s - loss: 511.6199 - loglik: -5.1021e+02 - logprior: -1.4095e+00
Epoch 10/10
39/39 - 25s - loss: 511.7978 - loglik: -5.1039e+02 - logprior: -1.4095e+00
Fitted a model with MAP estimate = -510.9077
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (19, 1), (37, 1), (39, 1), (45, 1), (46, 1), (47, 1), (63, 1), (64, 1), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 483.5276 - loglik: -4.8157e+02 - logprior: -1.9565e+00
Epoch 2/2
39/39 - 41s - loss: 466.2650 - loglik: -4.6583e+02 - logprior: -4.3663e-01
Fitted a model with MAP estimate = -463.7742
expansions: [(0, 2)]
discards: [  0   1  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 470.1390 - loglik: -4.6881e+02 - logprior: -1.3311e+00
Epoch 2/2
39/39 - 43s - loss: 465.6643 - loglik: -4.6556e+02 - logprior: -1.0148e-01
Fitted a model with MAP estimate = -463.6809
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 471.5045 - loglik: -4.6959e+02 - logprior: -1.9186e+00
Epoch 2/10
39/39 - 45s - loss: 466.1294 - loglik: -4.6619e+02 - logprior: 0.0585
Epoch 3/10
39/39 - 46s - loss: 463.7577 - loglik: -4.6420e+02 - logprior: 0.4460
Epoch 4/10
39/39 - 45s - loss: 462.2987 - loglik: -4.6266e+02 - logprior: 0.3626
Epoch 5/10
39/39 - 44s - loss: 460.8155 - loglik: -4.6160e+02 - logprior: 0.7868
Epoch 6/10
39/39 - 46s - loss: 461.2007 - loglik: -4.6180e+02 - logprior: 0.5955
Fitted a model with MAP estimate = -460.3953
Time for alignment: 909.6038
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 734.2865 - loglik: -7.3279e+02 - logprior: -1.4945e+00
Epoch 2/10
39/39 - 31s - loss: 530.5645 - loglik: -5.2908e+02 - logprior: -1.4879e+00
Epoch 3/10
39/39 - 31s - loss: 517.4775 - loglik: -5.1602e+02 - logprior: -1.4622e+00
Epoch 4/10
39/39 - 31s - loss: 515.0447 - loglik: -5.1368e+02 - logprior: -1.3613e+00
Epoch 5/10
39/39 - 30s - loss: 513.4346 - loglik: -5.1210e+02 - logprior: -1.3384e+00
Epoch 6/10
39/39 - 28s - loss: 513.6270 - loglik: -5.1229e+02 - logprior: -1.3408e+00
Fitted a model with MAP estimate = -512.4616
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 2), (148, 1), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (182, 3), (188, 1), (189, 1), (206, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 483.4903 - loglik: -4.8160e+02 - logprior: -1.8902e+00
Epoch 2/2
39/39 - 38s - loss: 466.3080 - loglik: -4.6564e+02 - logprior: -6.7212e-01
Fitted a model with MAP estimate = -463.5549
expansions: []
discards: [  0   1  97 212 227]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 471.6322 - loglik: -4.7014e+02 - logprior: -1.4882e+00
Epoch 2/2
39/39 - 35s - loss: 466.6659 - loglik: -4.6675e+02 - logprior: 0.0831
Fitted a model with MAP estimate = -464.3535
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 469.5990 - loglik: -4.6825e+02 - logprior: -1.3497e+00
Epoch 2/10
39/39 - 36s - loss: 464.7071 - loglik: -4.6483e+02 - logprior: 0.1276
Epoch 3/10
39/39 - 38s - loss: 463.2696 - loglik: -4.6329e+02 - logprior: 0.0202
Epoch 4/10
39/39 - 39s - loss: 461.5276 - loglik: -4.6197e+02 - logprior: 0.4449
Epoch 5/10
39/39 - 40s - loss: 460.9084 - loglik: -4.6116e+02 - logprior: 0.2482
Epoch 6/10
39/39 - 40s - loss: 460.1459 - loglik: -4.6085e+02 - logprior: 0.7038
Epoch 7/10
39/39 - 41s - loss: 460.2502 - loglik: -4.6089e+02 - logprior: 0.6363
Fitted a model with MAP estimate = -459.6007
Time for alignment: 792.6879
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 735.0473 - loglik: -7.3357e+02 - logprior: -1.4750e+00
Epoch 2/10
39/39 - 30s - loss: 532.7828 - loglik: -5.3139e+02 - logprior: -1.3921e+00
Epoch 3/10
39/39 - 30s - loss: 518.7812 - loglik: -5.1744e+02 - logprior: -1.3372e+00
Epoch 4/10
39/39 - 31s - loss: 516.0736 - loglik: -5.1482e+02 - logprior: -1.2580e+00
Epoch 5/10
39/39 - 30s - loss: 515.2829 - loglik: -5.1404e+02 - logprior: -1.2393e+00
Epoch 6/10
39/39 - 30s - loss: 514.8085 - loglik: -5.1356e+02 - logprior: -1.2441e+00
Epoch 7/10
39/39 - 29s - loss: 514.0161 - loglik: -5.1277e+02 - logprior: -1.2500e+00
Epoch 8/10
39/39 - 29s - loss: 514.4563 - loglik: -5.1320e+02 - logprior: -1.2567e+00
Fitted a model with MAP estimate = -513.4468
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (17, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (72, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (185, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 484.0338 - loglik: -4.8210e+02 - logprior: -1.9300e+00
Epoch 2/2
39/39 - 46s - loss: 466.3878 - loglik: -4.6578e+02 - logprior: -6.1256e-01
Fitted a model with MAP estimate = -463.9376
expansions: []
discards: [  0   1  57  98 164 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 471.7160 - loglik: -4.7035e+02 - logprior: -1.3616e+00
Epoch 2/2
39/39 - 43s - loss: 466.9242 - loglik: -4.6688e+02 - logprior: -4.5863e-02
Fitted a model with MAP estimate = -464.8442
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 469.3685 - loglik: -4.6806e+02 - logprior: -1.3116e+00
Epoch 2/10
39/39 - 47s - loss: 464.7126 - loglik: -4.6485e+02 - logprior: 0.1331
Epoch 3/10
39/39 - 46s - loss: 463.3240 - loglik: -4.6353e+02 - logprior: 0.2012
Epoch 4/10
39/39 - 44s - loss: 461.4655 - loglik: -4.6177e+02 - logprior: 0.3034
Epoch 5/10
39/39 - 44s - loss: 460.6021 - loglik: -4.6120e+02 - logprior: 0.5931
Epoch 6/10
39/39 - 45s - loss: 461.1535 - loglik: -4.6169e+02 - logprior: 0.5410
Fitted a model with MAP estimate = -459.7808
Time for alignment: 920.8079
Computed alignments with likelihoods: ['-459.4939', '-460.0126', '-460.3953', '-459.6007', '-459.7808']
Best model has likelihood: -459.4939  (prior= 1.0315 )
time for generating output: 0.2448
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9143389199255121
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 397.5662 - loglik: -3.9456e+02 - logprior: -3.0039e+00
Epoch 2/10
19/19 - 5s - loss: 329.4726 - loglik: -3.2823e+02 - logprior: -1.2433e+00
Epoch 3/10
19/19 - 5s - loss: 305.6031 - loglik: -3.0393e+02 - logprior: -1.6765e+00
Epoch 4/10
19/19 - 5s - loss: 299.8927 - loglik: -2.9837e+02 - logprior: -1.5239e+00
Epoch 5/10
19/19 - 5s - loss: 297.9492 - loglik: -2.9649e+02 - logprior: -1.4547e+00
Epoch 6/10
19/19 - 5s - loss: 297.7996 - loglik: -2.9638e+02 - logprior: -1.4178e+00
Epoch 7/10
19/19 - 5s - loss: 295.8201 - loglik: -2.9444e+02 - logprior: -1.3848e+00
Epoch 8/10
19/19 - 5s - loss: 294.7341 - loglik: -2.9337e+02 - logprior: -1.3654e+00
Epoch 9/10
19/19 - 5s - loss: 294.9009 - loglik: -2.9354e+02 - logprior: -1.3572e+00
Fitted a model with MAP estimate = -293.9483
expansions: [(7, 2), (22, 2), (23, 2), (24, 1), (25, 2), (28, 1), (29, 2), (34, 1), (36, 2), (46, 2), (49, 1), (50, 2), (55, 1), (60, 1), (81, 1), (84, 5), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 302.1868 - loglik: -2.9905e+02 - logprior: -3.1330e+00
Epoch 2/2
39/39 - 8s - loss: 292.2858 - loglik: -2.9081e+02 - logprior: -1.4784e+00
Fitted a model with MAP estimate = -289.0666
expansions: []
discards: [ 23  26  31  38  49  60  67 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.2817 - loglik: -2.9199e+02 - logprior: -2.2872e+00
Epoch 2/2
39/39 - 7s - loss: 290.9807 - loglik: -2.8979e+02 - logprior: -1.1918e+00
Fitted a model with MAP estimate = -288.8729
expansions: []
discards: [101]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 292.9628 - loglik: -2.9076e+02 - logprior: -2.1981e+00
Epoch 2/10
39/39 - 7s - loss: 290.0666 - loglik: -2.8896e+02 - logprior: -1.1057e+00
Epoch 3/10
39/39 - 7s - loss: 288.7116 - loglik: -2.8772e+02 - logprior: -9.9408e-01
Epoch 4/10
39/39 - 7s - loss: 286.5215 - loglik: -2.8563e+02 - logprior: -8.8953e-01
Epoch 5/10
39/39 - 7s - loss: 285.2729 - loglik: -2.8451e+02 - logprior: -7.6107e-01
Epoch 6/10
39/39 - 7s - loss: 284.2039 - loglik: -2.8354e+02 - logprior: -6.6465e-01
Epoch 7/10
39/39 - 7s - loss: 283.5826 - loglik: -2.8299e+02 - logprior: -5.9287e-01
Epoch 8/10
39/39 - 7s - loss: 283.8641 - loglik: -2.8332e+02 - logprior: -5.4091e-01
Fitted a model with MAP estimate = -283.2532
Time for alignment: 177.0442
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.8666 - loglik: -3.9485e+02 - logprior: -3.0131e+00
Epoch 2/10
19/19 - 5s - loss: 326.7338 - loglik: -3.2551e+02 - logprior: -1.2233e+00
Epoch 3/10
19/19 - 5s - loss: 305.4267 - loglik: -3.0383e+02 - logprior: -1.5917e+00
Epoch 4/10
19/19 - 5s - loss: 301.3459 - loglik: -2.9992e+02 - logprior: -1.4250e+00
Epoch 5/10
19/19 - 5s - loss: 299.7810 - loglik: -2.9840e+02 - logprior: -1.3768e+00
Epoch 6/10
19/19 - 5s - loss: 298.9480 - loglik: -2.9761e+02 - logprior: -1.3405e+00
Epoch 7/10
19/19 - 5s - loss: 296.8326 - loglik: -2.9551e+02 - logprior: -1.3196e+00
Epoch 8/10
19/19 - 5s - loss: 296.5957 - loglik: -2.9529e+02 - logprior: -1.3049e+00
Epoch 9/10
19/19 - 5s - loss: 297.3254 - loglik: -2.9604e+02 - logprior: -1.2901e+00
Fitted a model with MAP estimate = -295.3256
expansions: [(7, 2), (22, 4), (25, 2), (30, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 2), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 302.4784 - loglik: -2.9932e+02 - logprior: -3.1589e+00
Epoch 2/2
39/39 - 7s - loss: 292.3212 - loglik: -2.9078e+02 - logprior: -1.5402e+00
Fitted a model with MAP estimate = -288.9146
expansions: []
discards: [ 24  30  37  48  58  60  66  68 107 139]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.2733 - loglik: -2.9198e+02 - logprior: -2.2944e+00
Epoch 2/2
39/39 - 7s - loss: 291.2635 - loglik: -2.9006e+02 - logprior: -1.2026e+00
Fitted a model with MAP estimate = -288.9307
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 292.7929 - loglik: -2.9058e+02 - logprior: -2.2168e+00
Epoch 2/10
39/39 - 7s - loss: 289.9829 - loglik: -2.8886e+02 - logprior: -1.1258e+00
Epoch 3/10
39/39 - 7s - loss: 289.0771 - loglik: -2.8808e+02 - logprior: -9.9869e-01
Epoch 4/10
39/39 - 7s - loss: 286.7351 - loglik: -2.8586e+02 - logprior: -8.7483e-01
Epoch 5/10
39/39 - 7s - loss: 284.7397 - loglik: -2.8400e+02 - logprior: -7.3661e-01
Epoch 6/10
39/39 - 7s - loss: 284.2478 - loglik: -2.8359e+02 - logprior: -6.6176e-01
Epoch 7/10
39/39 - 7s - loss: 283.2982 - loglik: -2.8271e+02 - logprior: -5.8605e-01
Epoch 8/10
39/39 - 7s - loss: 283.5883 - loglik: -2.8305e+02 - logprior: -5.3479e-01
Fitted a model with MAP estimate = -283.1826
Time for alignment: 173.2713
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.6995 - loglik: -3.9469e+02 - logprior: -3.0070e+00
Epoch 2/10
19/19 - 5s - loss: 328.7796 - loglik: -3.2755e+02 - logprior: -1.2294e+00
Epoch 3/10
19/19 - 5s - loss: 306.0046 - loglik: -3.0437e+02 - logprior: -1.6308e+00
Epoch 4/10
19/19 - 5s - loss: 301.7802 - loglik: -3.0033e+02 - logprior: -1.4523e+00
Epoch 5/10
19/19 - 5s - loss: 299.6520 - loglik: -2.9826e+02 - logprior: -1.3918e+00
Epoch 6/10
19/19 - 5s - loss: 297.8577 - loglik: -2.9651e+02 - logprior: -1.3453e+00
Epoch 7/10
19/19 - 5s - loss: 297.3126 - loglik: -2.9599e+02 - logprior: -1.3239e+00
Epoch 8/10
19/19 - 5s - loss: 297.1818 - loglik: -2.9588e+02 - logprior: -1.3064e+00
Epoch 9/10
19/19 - 5s - loss: 297.1657 - loglik: -2.9587e+02 - logprior: -1.2976e+00
Epoch 10/10
19/19 - 5s - loss: 295.8665 - loglik: -2.9458e+02 - logprior: -1.2874e+00
Fitted a model with MAP estimate = -295.1659
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 2), (50, 2), (55, 1), (70, 1), (71, 2), (84, 5), (85, 3), (86, 2), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 302.5903 - loglik: -2.9946e+02 - logprior: -3.1256e+00
Epoch 2/2
39/39 - 7s - loss: 292.2917 - loglik: -2.9082e+02 - logprior: -1.4697e+00
Fitted a model with MAP estimate = -288.9302
expansions: []
discards: [ 23  30  36  48  58  60  66  68  93 112 141]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.0541 - loglik: -2.9180e+02 - logprior: -2.2525e+00
Epoch 2/2
39/39 - 7s - loss: 291.0876 - loglik: -2.8989e+02 - logprior: -1.2003e+00
Fitted a model with MAP estimate = -288.6731
expansions: [(107, 1)]
discards: [109]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 292.7290 - loglik: -2.9054e+02 - logprior: -2.1871e+00
Epoch 2/10
39/39 - 7s - loss: 290.3759 - loglik: -2.8929e+02 - logprior: -1.0856e+00
Epoch 3/10
39/39 - 7s - loss: 288.0004 - loglik: -2.8703e+02 - logprior: -9.6684e-01
Epoch 4/10
39/39 - 7s - loss: 286.7329 - loglik: -2.8588e+02 - logprior: -8.5473e-01
Epoch 5/10
39/39 - 7s - loss: 285.0408 - loglik: -2.8431e+02 - logprior: -7.3323e-01
Epoch 6/10
39/39 - 7s - loss: 284.2305 - loglik: -2.8357e+02 - logprior: -6.5782e-01
Epoch 7/10
39/39 - 7s - loss: 283.2733 - loglik: -2.8269e+02 - logprior: -5.7910e-01
Epoch 8/10
39/39 - 7s - loss: 282.9539 - loglik: -2.8243e+02 - logprior: -5.2007e-01
Epoch 9/10
39/39 - 7s - loss: 283.7704 - loglik: -2.8329e+02 - logprior: -4.7852e-01
Fitted a model with MAP estimate = -282.9877
Time for alignment: 184.5621
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.8329 - loglik: -3.9482e+02 - logprior: -3.0115e+00
Epoch 2/10
19/19 - 5s - loss: 328.3822 - loglik: -3.2715e+02 - logprior: -1.2334e+00
Epoch 3/10
19/19 - 5s - loss: 304.9125 - loglik: -3.0326e+02 - logprior: -1.6571e+00
Epoch 4/10
19/19 - 5s - loss: 300.5310 - loglik: -2.9906e+02 - logprior: -1.4724e+00
Epoch 5/10
19/19 - 5s - loss: 298.8136 - loglik: -2.9739e+02 - logprior: -1.4224e+00
Epoch 6/10
19/19 - 5s - loss: 297.0337 - loglik: -2.9565e+02 - logprior: -1.3829e+00
Epoch 7/10
19/19 - 5s - loss: 296.9929 - loglik: -2.9564e+02 - logprior: -1.3554e+00
Epoch 8/10
19/19 - 5s - loss: 295.3740 - loglik: -2.9403e+02 - logprior: -1.3400e+00
Epoch 9/10
19/19 - 5s - loss: 295.0309 - loglik: -2.9370e+02 - logprior: -1.3277e+00
Epoch 10/10
19/19 - 5s - loss: 295.4615 - loglik: -2.9415e+02 - logprior: -1.3153e+00
Fitted a model with MAP estimate = -294.1702
expansions: [(7, 2), (22, 2), (23, 2), (24, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 2), (50, 2), (55, 1), (56, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 302.7740 - loglik: -2.9959e+02 - logprior: -3.1806e+00
Epoch 2/2
39/39 - 8s - loss: 292.4877 - loglik: -2.9095e+02 - logprior: -1.5386e+00
Fitted a model with MAP estimate = -289.1328
expansions: []
discards: [ 23  26  31  37  49  59  61  67  69 140]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.2354 - loglik: -2.9193e+02 - logprior: -2.3012e+00
Epoch 2/2
39/39 - 7s - loss: 291.1275 - loglik: -2.8993e+02 - logprior: -1.2018e+00
Fitted a model with MAP estimate = -288.8999
expansions: [(108, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 292.6722 - loglik: -2.9046e+02 - logprior: -2.2131e+00
Epoch 2/10
39/39 - 7s - loss: 289.8912 - loglik: -2.8877e+02 - logprior: -1.1234e+00
Epoch 3/10
39/39 - 7s - loss: 288.3114 - loglik: -2.8731e+02 - logprior: -1.0010e+00
Epoch 4/10
39/39 - 7s - loss: 286.8133 - loglik: -2.8596e+02 - logprior: -8.5403e-01
Epoch 5/10
39/39 - 7s - loss: 285.0367 - loglik: -2.8430e+02 - logprior: -7.4046e-01
Epoch 6/10
39/39 - 7s - loss: 284.1604 - loglik: -2.8350e+02 - logprior: -6.5680e-01
Epoch 7/10
39/39 - 7s - loss: 282.8278 - loglik: -2.8225e+02 - logprior: -5.7975e-01
Epoch 8/10
39/39 - 7s - loss: 283.2806 - loglik: -2.8275e+02 - logprior: -5.2872e-01
Fitted a model with MAP estimate = -282.9869
Time for alignment: 180.5904
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.4640 - loglik: -3.9445e+02 - logprior: -3.0093e+00
Epoch 2/10
19/19 - 5s - loss: 327.7823 - loglik: -3.2654e+02 - logprior: -1.2377e+00
Epoch 3/10
19/19 - 5s - loss: 306.9843 - loglik: -3.0537e+02 - logprior: -1.6174e+00
Epoch 4/10
19/19 - 5s - loss: 301.8826 - loglik: -3.0041e+02 - logprior: -1.4692e+00
Epoch 5/10
19/19 - 5s - loss: 299.6547 - loglik: -2.9824e+02 - logprior: -1.4157e+00
Epoch 6/10
19/19 - 5s - loss: 297.9909 - loglik: -2.9660e+02 - logprior: -1.3872e+00
Epoch 7/10
19/19 - 5s - loss: 298.3660 - loglik: -2.9700e+02 - logprior: -1.3656e+00
Fitted a model with MAP estimate = -296.0708
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 1), (50, 1), (57, 1), (71, 1), (84, 2), (85, 4), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 301.7758 - loglik: -2.9867e+02 - logprior: -3.1070e+00
Epoch 2/2
39/39 - 7s - loss: 292.2185 - loglik: -2.9076e+02 - logprior: -1.4606e+00
Fitted a model with MAP estimate = -288.8755
expansions: []
discards: [ 23  30  36  48  59  61 105 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 294.1568 - loglik: -2.9184e+02 - logprior: -2.3128e+00
Epoch 2/2
39/39 - 7s - loss: 291.1111 - loglik: -2.8989e+02 - logprior: -1.2246e+00
Fitted a model with MAP estimate = -288.8760
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 293.1015 - loglik: -2.9086e+02 - logprior: -2.2418e+00
Epoch 2/10
39/39 - 7s - loss: 290.2494 - loglik: -2.8911e+02 - logprior: -1.1424e+00
Epoch 3/10
39/39 - 7s - loss: 288.9699 - loglik: -2.8794e+02 - logprior: -1.0252e+00
Epoch 4/10
39/39 - 7s - loss: 286.6249 - loglik: -2.8572e+02 - logprior: -9.0478e-01
Epoch 5/10
39/39 - 7s - loss: 285.3511 - loglik: -2.8458e+02 - logprior: -7.7444e-01
Epoch 6/10
39/39 - 7s - loss: 283.9849 - loglik: -2.8332e+02 - logprior: -6.6232e-01
Epoch 7/10
39/39 - 7s - loss: 283.4229 - loglik: -2.8282e+02 - logprior: -6.0552e-01
Epoch 8/10
39/39 - 7s - loss: 283.4748 - loglik: -2.8293e+02 - logprior: -5.4658e-01
Fitted a model with MAP estimate = -283.2597
Time for alignment: 162.6907
Computed alignments with likelihoods: ['-283.2532', '-283.1826', '-282.9877', '-282.9869', '-283.2597']
Best model has likelihood: -282.9869  (prior= -0.5061 )
time for generating output: 0.2131
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.22496919487192932
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 467.9933 - loglik: -4.6607e+02 - logprior: -1.9272e+00
Epoch 2/10
39/39 - 8s - loss: 426.2327 - loglik: -4.2494e+02 - logprior: -1.2879e+00
Epoch 3/10
39/39 - 8s - loss: 422.1135 - loglik: -4.2081e+02 - logprior: -1.3042e+00
Epoch 4/10
39/39 - 8s - loss: 420.3500 - loglik: -4.1906e+02 - logprior: -1.2929e+00
Epoch 5/10
39/39 - 8s - loss: 419.1543 - loglik: -4.1786e+02 - logprior: -1.2969e+00
Epoch 6/10
39/39 - 8s - loss: 418.4960 - loglik: -4.1720e+02 - logprior: -1.2931e+00
Epoch 7/10
39/39 - 8s - loss: 418.9070 - loglik: -4.1762e+02 - logprior: -1.2895e+00
Fitted a model with MAP estimate = -418.3152
expansions: [(19, 4), (29, 3), (30, 1), (32, 1), (46, 1), (47, 3), (49, 1), (58, 3), (59, 2), (60, 1), (75, 1), (77, 2), (78, 6), (80, 2), (99, 1), (103, 1), (106, 1), (107, 1), (108, 1), (117, 1), (118, 9)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 418.9057 - loglik: -4.1696e+02 - logprior: -1.9453e+00
Epoch 2/2
39/39 - 10s - loss: 410.5512 - loglik: -4.0977e+02 - logprior: -7.7919e-01
Fitted a model with MAP estimate = -409.1366
expansions: []
discards: [ 34  57  72  73  76 103 104 105 109 155 156 157 158 159 160 161]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 415.4973 - loglik: -4.1377e+02 - logprior: -1.7292e+00
Epoch 2/2
39/39 - 9s - loss: 412.2009 - loglik: -4.1166e+02 - logprior: -5.4069e-01
Fitted a model with MAP estimate = -410.7173
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 414.1505 - loglik: -4.1249e+02 - logprior: -1.6599e+00
Epoch 2/10
39/39 - 8s - loss: 411.8722 - loglik: -4.1142e+02 - logprior: -4.5632e-01
Epoch 3/10
39/39 - 8s - loss: 410.2478 - loglik: -4.0984e+02 - logprior: -4.0860e-01
Epoch 4/10
39/39 - 8s - loss: 408.1929 - loglik: -4.0782e+02 - logprior: -3.7355e-01
Epoch 5/10
39/39 - 8s - loss: 407.1127 - loglik: -4.0678e+02 - logprior: -3.3491e-01
Epoch 6/10
39/39 - 8s - loss: 406.4024 - loglik: -4.0609e+02 - logprior: -3.0844e-01
Epoch 7/10
39/39 - 8s - loss: 406.3391 - loglik: -4.0606e+02 - logprior: -2.7744e-01
Epoch 8/10
39/39 - 8s - loss: 406.2202 - loglik: -4.0598e+02 - logprior: -2.4378e-01
Epoch 9/10
39/39 - 8s - loss: 405.8127 - loglik: -4.0562e+02 - logprior: -1.9731e-01
Epoch 10/10
39/39 - 8s - loss: 406.1566 - loglik: -4.0599e+02 - logprior: -1.6866e-01
Fitted a model with MAP estimate = -405.4026
Time for alignment: 230.0604
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 469.3937 - loglik: -4.6746e+02 - logprior: -1.9288e+00
Epoch 2/10
39/39 - 7s - loss: 431.1918 - loglik: -4.3002e+02 - logprior: -1.1749e+00
Epoch 3/10
39/39 - 7s - loss: 427.4183 - loglik: -4.2629e+02 - logprior: -1.1305e+00
Epoch 4/10
39/39 - 7s - loss: 426.1161 - loglik: -4.2498e+02 - logprior: -1.1406e+00
Epoch 5/10
39/39 - 7s - loss: 424.3781 - loglik: -4.2323e+02 - logprior: -1.1522e+00
Epoch 6/10
39/39 - 7s - loss: 423.3592 - loglik: -4.2217e+02 - logprior: -1.1845e+00
Epoch 7/10
39/39 - 7s - loss: 423.0214 - loglik: -4.2183e+02 - logprior: -1.1936e+00
Epoch 8/10
39/39 - 7s - loss: 423.4300 - loglik: -4.2224e+02 - logprior: -1.1911e+00
Fitted a model with MAP estimate = -422.6611
expansions: [(19, 2), (20, 1), (31, 6), (32, 1), (46, 1), (47, 1), (48, 1), (49, 2), (73, 2), (75, 1), (78, 2), (79, 8), (93, 1), (107, 3), (109, 2)]
discards: [ 1 58]
Re-initialized the encoder parameters.
Fitting a model of length 160 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 423.1930 - loglik: -4.2128e+02 - logprior: -1.9125e+00
Epoch 2/2
39/39 - 8s - loss: 415.9619 - loglik: -4.1526e+02 - logprior: -7.0048e-01
Fitted a model with MAP estimate = -414.3120
expansions: []
discards: [ 39  58  86  99 100 101 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 419.8534 - loglik: -4.1808e+02 - logprior: -1.7697e+00
Epoch 2/2
39/39 - 8s - loss: 416.7158 - loglik: -4.1613e+02 - logprior: -5.8320e-01
Fitted a model with MAP estimate = -415.3286
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 418.7554 - loglik: -4.1704e+02 - logprior: -1.7160e+00
Epoch 2/10
39/39 - 8s - loss: 416.7383 - loglik: -4.1623e+02 - logprior: -5.0666e-01
Epoch 3/10
39/39 - 8s - loss: 414.8148 - loglik: -4.1437e+02 - logprior: -4.4882e-01
Epoch 4/10
39/39 - 8s - loss: 413.6451 - loglik: -4.1324e+02 - logprior: -4.0387e-01
Epoch 5/10
39/39 - 8s - loss: 412.1073 - loglik: -4.1172e+02 - logprior: -3.9068e-01
Epoch 6/10
39/39 - 8s - loss: 411.3385 - loglik: -4.1099e+02 - logprior: -3.4948e-01
Epoch 7/10
39/39 - 8s - loss: 411.2980 - loglik: -4.1099e+02 - logprior: -3.0832e-01
Epoch 8/10
39/39 - 8s - loss: 411.4337 - loglik: -4.1115e+02 - logprior: -2.8302e-01
Fitted a model with MAP estimate = -410.8030
Time for alignment: 205.5282
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 467.8755 - loglik: -4.6595e+02 - logprior: -1.9248e+00
Epoch 2/10
39/39 - 7s - loss: 427.4912 - loglik: -4.2632e+02 - logprior: -1.1708e+00
Epoch 3/10
39/39 - 7s - loss: 423.4780 - loglik: -4.2231e+02 - logprior: -1.1723e+00
Epoch 4/10
39/39 - 7s - loss: 421.4496 - loglik: -4.2027e+02 - logprior: -1.1796e+00
Epoch 5/10
39/39 - 7s - loss: 420.3269 - loglik: -4.1914e+02 - logprior: -1.1849e+00
Epoch 6/10
39/39 - 7s - loss: 419.9693 - loglik: -4.1878e+02 - logprior: -1.1915e+00
Epoch 7/10
39/39 - 7s - loss: 419.3419 - loglik: -4.1815e+02 - logprior: -1.1951e+00
Epoch 8/10
39/39 - 7s - loss: 419.7904 - loglik: -4.1859e+02 - logprior: -1.2038e+00
Fitted a model with MAP estimate = -418.8759
expansions: [(19, 2), (20, 1), (32, 16), (33, 2), (48, 1), (49, 1), (50, 2), (56, 1), (58, 8), (73, 1), (75, 1), (77, 5), (78, 3), (93, 1), (101, 1), (103, 2), (104, 1), (105, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 418.3376 - loglik: -4.1641e+02 - logprior: -1.9306e+00
Epoch 2/2
39/39 - 10s - loss: 406.9696 - loglik: -4.0610e+02 - logprior: -8.7295e-01
Fitted a model with MAP estimate = -404.6381
expansions: [(21, 1), (51, 1)]
discards: [ 36  37  38  39  40  41  42  43  44  45  46  47  48  69  85  86  87 115
 116 117 118 154]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 414.8346 - loglik: -4.1312e+02 - logprior: -1.7139e+00
Epoch 2/2
39/39 - 8s - loss: 411.8667 - loglik: -4.1134e+02 - logprior: -5.2279e-01
Fitted a model with MAP estimate = -410.2048
expansions: [(100, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 413.3385 - loglik: -4.1168e+02 - logprior: -1.6581e+00
Epoch 2/10
39/39 - 8s - loss: 410.7694 - loglik: -4.1030e+02 - logprior: -4.6640e-01
Epoch 3/10
39/39 - 8s - loss: 409.4504 - loglik: -4.0903e+02 - logprior: -4.2396e-01
Epoch 4/10
39/39 - 8s - loss: 406.6599 - loglik: -4.0628e+02 - logprior: -3.8361e-01
Epoch 5/10
39/39 - 8s - loss: 406.3540 - loglik: -4.0601e+02 - logprior: -3.3981e-01
Epoch 6/10
39/39 - 8s - loss: 405.0434 - loglik: -4.0473e+02 - logprior: -3.1535e-01
Epoch 7/10
39/39 - 8s - loss: 405.1824 - loglik: -4.0491e+02 - logprior: -2.7402e-01
Fitted a model with MAP estimate = -404.7258
Time for alignment: 203.0143
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 469.1775 - loglik: -4.6723e+02 - logprior: -1.9429e+00
Epoch 2/10
39/39 - 7s - loss: 427.0368 - loglik: -4.2574e+02 - logprior: -1.2970e+00
Epoch 3/10
39/39 - 7s - loss: 422.5500 - loglik: -4.2125e+02 - logprior: -1.3045e+00
Epoch 4/10
39/39 - 7s - loss: 421.0656 - loglik: -4.1975e+02 - logprior: -1.3161e+00
Epoch 5/10
39/39 - 7s - loss: 419.5675 - loglik: -4.1825e+02 - logprior: -1.3135e+00
Epoch 6/10
39/39 - 7s - loss: 419.7247 - loglik: -4.1840e+02 - logprior: -1.3220e+00
Fitted a model with MAP estimate = -419.0112
expansions: [(19, 2), (20, 1), (21, 2), (31, 1), (32, 1), (48, 3), (50, 1), (56, 1), (58, 1), (59, 1), (60, 2), (75, 1), (77, 2), (78, 4), (79, 1), (95, 1), (99, 1), (106, 1), (107, 1), (108, 2), (109, 1), (118, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 418.1688 - loglik: -4.1627e+02 - logprior: -1.8992e+00
Epoch 2/2
39/39 - 8s - loss: 411.7076 - loglik: -4.1100e+02 - logprior: -7.0889e-01
Fitted a model with MAP estimate = -410.1483
expansions: [(24, 1)]
discards: [55 74 97]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 414.1619 - loglik: -4.1239e+02 - logprior: -1.7688e+00
Epoch 2/2
39/39 - 8s - loss: 411.4743 - loglik: -4.1092e+02 - logprior: -5.5612e-01
Fitted a model with MAP estimate = -410.1588
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 413.4940 - loglik: -4.1179e+02 - logprior: -1.7032e+00
Epoch 2/10
39/39 - 8s - loss: 411.4533 - loglik: -4.1096e+02 - logprior: -4.9307e-01
Epoch 3/10
39/39 - 8s - loss: 409.4671 - loglik: -4.0903e+02 - logprior: -4.3255e-01
Epoch 4/10
39/39 - 8s - loss: 408.0580 - loglik: -4.0766e+02 - logprior: -3.9954e-01
Epoch 5/10
39/39 - 8s - loss: 407.3685 - loglik: -4.0700e+02 - logprior: -3.6580e-01
Epoch 6/10
39/39 - 8s - loss: 406.0688 - loglik: -4.0573e+02 - logprior: -3.3386e-01
Epoch 7/10
39/39 - 8s - loss: 406.0071 - loglik: -4.0571e+02 - logprior: -3.0036e-01
Epoch 8/10
39/39 - 8s - loss: 406.3753 - loglik: -4.0611e+02 - logprior: -2.6381e-01
Fitted a model with MAP estimate = -405.8137
Time for alignment: 191.6998
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 469.6874 - loglik: -4.6775e+02 - logprior: -1.9348e+00
Epoch 2/10
39/39 - 7s - loss: 428.9335 - loglik: -4.2770e+02 - logprior: -1.2371e+00
Epoch 3/10
39/39 - 7s - loss: 425.2485 - loglik: -4.2400e+02 - logprior: -1.2469e+00
Epoch 4/10
39/39 - 7s - loss: 422.9999 - loglik: -4.2176e+02 - logprior: -1.2374e+00
Epoch 5/10
39/39 - 7s - loss: 421.4649 - loglik: -4.2022e+02 - logprior: -1.2427e+00
Epoch 6/10
39/39 - 7s - loss: 421.3505 - loglik: -4.2010e+02 - logprior: -1.2472e+00
Epoch 7/10
39/39 - 7s - loss: 421.1622 - loglik: -4.1991e+02 - logprior: -1.2517e+00
Epoch 8/10
39/39 - 7s - loss: 421.0769 - loglik: -4.1982e+02 - logprior: -1.2520e+00
Epoch 9/10
39/39 - 7s - loss: 421.0834 - loglik: -4.1984e+02 - logprior: -1.2425e+00
Fitted a model with MAP estimate = -420.6881
expansions: [(19, 2), (20, 1), (31, 1), (32, 3), (34, 1), (48, 1), (49, 3), (51, 1), (55, 1), (76, 2), (77, 5), (79, 2), (94, 1), (98, 1), (104, 2), (105, 1), (106, 2), (108, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 424.2549 - loglik: -4.2233e+02 - logprior: -1.9208e+00
Epoch 2/2
39/39 - 8s - loss: 417.0215 - loglik: -4.1631e+02 - logprior: -7.0744e-01
Fitted a model with MAP estimate = -415.5294
expansions: []
discards: [ 35  36  58  94  95 100 134]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 420.1779 - loglik: -4.1844e+02 - logprior: -1.7399e+00
Epoch 2/2
39/39 - 8s - loss: 417.3992 - loglik: -4.1686e+02 - logprior: -5.4214e-01
Fitted a model with MAP estimate = -415.9049
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 419.5146 - loglik: -4.1784e+02 - logprior: -1.6747e+00
Epoch 2/10
39/39 - 8s - loss: 416.8261 - loglik: -4.1636e+02 - logprior: -4.7085e-01
Epoch 3/10
39/39 - 8s - loss: 416.0247 - loglik: -4.1561e+02 - logprior: -4.1309e-01
Epoch 4/10
39/39 - 8s - loss: 413.8085 - loglik: -4.1343e+02 - logprior: -3.7900e-01
Epoch 5/10
39/39 - 8s - loss: 412.1537 - loglik: -4.1182e+02 - logprior: -3.3610e-01
Epoch 6/10
39/39 - 8s - loss: 411.9483 - loglik: -4.1163e+02 - logprior: -3.1984e-01
Epoch 7/10
39/39 - 8s - loss: 411.5575 - loglik: -4.1128e+02 - logprior: -2.7846e-01
Epoch 8/10
39/39 - 8s - loss: 411.1473 - loglik: -4.1090e+02 - logprior: -2.4424e-01
Epoch 9/10
39/39 - 8s - loss: 411.0165 - loglik: -4.1080e+02 - logprior: -2.1312e-01
Epoch 10/10
39/39 - 8s - loss: 410.7964 - loglik: -4.1062e+02 - logprior: -1.7136e-01
Fitted a model with MAP estimate = -410.7292
Time for alignment: 224.6031
Computed alignments with likelihoods: ['-405.4026', '-410.8030', '-404.6381', '-405.8137', '-410.7292']
Best model has likelihood: -404.6381  (prior= -0.8119 )
time for generating output: 0.1754
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.862893583100837
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 793.7209 - loglik: -7.9231e+02 - logprior: -1.4075e+00
Epoch 2/10
39/39 - 19s - loss: 724.4791 - loglik: -7.2378e+02 - logprior: -7.0114e-01
Epoch 3/10
39/39 - 19s - loss: 715.5853 - loglik: -7.1477e+02 - logprior: -8.1926e-01
Epoch 4/10
39/39 - 19s - loss: 712.1490 - loglik: -7.1129e+02 - logprior: -8.6005e-01
Epoch 5/10
39/39 - 18s - loss: 710.4340 - loglik: -7.0957e+02 - logprior: -8.6866e-01
Epoch 6/10
39/39 - 18s - loss: 709.6663 - loglik: -7.0879e+02 - logprior: -8.7910e-01
Epoch 7/10
39/39 - 18s - loss: 709.8302 - loglik: -7.0894e+02 - logprior: -8.8539e-01
Fitted a model with MAP estimate = -708.6995
expansions: [(0, 3), (9, 1), (10, 1), (24, 1), (40, 1), (43, 1), (51, 2), (52, 1), (58, 1), (80, 1), (84, 8), (88, 6), (89, 1), (95, 1), (102, 1), (115, 4), (116, 2), (117, 1), (120, 1), (123, 1), (126, 1), (152, 2), (154, 3), (158, 2), (176, 1), (183, 4), (205, 6), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 711.7253 - loglik: -7.0962e+02 - logprior: -2.1093e+00
Epoch 2/2
39/39 - 26s - loss: 702.1441 - loglik: -7.0166e+02 - logprior: -4.8450e-01
Fitted a model with MAP estimate = -699.4984
expansions: [(0, 3)]
discards: [  1   2   3  59  94 112 113 147 148 149 193 199 201 202 203 258]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 707.3556 - loglik: -7.0504e+02 - logprior: -2.3176e+00
Epoch 2/2
39/39 - 24s - loss: 702.9501 - loglik: -7.0260e+02 - logprior: -3.5185e-01
Fitted a model with MAP estimate = -700.5917
expansions: [(0, 3), (189, 1)]
discards: [  1   2   3   4 100 190]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 706.2264 - loglik: -7.0381e+02 - logprior: -2.4168e+00
Epoch 2/10
39/39 - 24s - loss: 702.0960 - loglik: -7.0188e+02 - logprior: -2.1730e-01
Epoch 3/10
39/39 - 24s - loss: 700.3799 - loglik: -7.0041e+02 - logprior: 0.0313
Epoch 4/10
39/39 - 24s - loss: 698.4657 - loglik: -6.9857e+02 - logprior: 0.1083
Epoch 5/10
39/39 - 24s - loss: 696.7032 - loglik: -6.9688e+02 - logprior: 0.1801
Epoch 6/10
39/39 - 24s - loss: 696.9474 - loglik: -6.9719e+02 - logprior: 0.2435
Fitted a model with MAP estimate = -695.9517
Time for alignment: 505.6746
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 793.3640 - loglik: -7.9197e+02 - logprior: -1.3921e+00
Epoch 2/10
39/39 - 18s - loss: 724.3876 - loglik: -7.2375e+02 - logprior: -6.4218e-01
Epoch 3/10
39/39 - 19s - loss: 716.1558 - loglik: -7.1535e+02 - logprior: -8.0170e-01
Epoch 4/10
39/39 - 18s - loss: 713.0940 - loglik: -7.1229e+02 - logprior: -7.9952e-01
Epoch 5/10
39/39 - 19s - loss: 711.8038 - loglik: -7.1101e+02 - logprior: -7.9280e-01
Epoch 6/10
39/39 - 19s - loss: 710.9047 - loglik: -7.1008e+02 - logprior: -8.2076e-01
Epoch 7/10
39/39 - 19s - loss: 710.8540 - loglik: -7.1002e+02 - logprior: -8.2908e-01
Epoch 8/10
39/39 - 19s - loss: 710.7703 - loglik: -7.0995e+02 - logprior: -8.2232e-01
Epoch 9/10
39/39 - 18s - loss: 710.5577 - loglik: -7.0973e+02 - logprior: -8.2300e-01
Epoch 10/10
39/39 - 19s - loss: 711.0059 - loglik: -7.1018e+02 - logprior: -8.3002e-01
Fitted a model with MAP estimate = -709.6165
expansions: [(0, 3), (9, 1), (10, 1), (19, 1), (23, 1), (43, 1), (51, 2), (53, 1), (81, 1), (85, 3), (86, 1), (90, 13), (97, 2), (103, 1), (114, 3), (117, 1), (120, 1), (125, 1), (152, 2), (158, 2), (179, 6), (183, 2), (184, 1), (206, 2), (207, 5), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 712.8743 - loglik: -7.1079e+02 - logprior: -2.0823e+00
Epoch 2/2
39/39 - 26s - loss: 701.4534 - loglik: -7.0097e+02 - logprior: -4.8682e-01
Fitted a model with MAP estimate = -698.9404
expansions: [(0, 3), (194, 3)]
discards: [  1   2   3  59  93  94 111 112 113 127 198 199 226 260]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 707.0862 - loglik: -7.0476e+02 - logprior: -2.3289e+00
Epoch 2/2
39/39 - 25s - loss: 702.1571 - loglik: -7.0179e+02 - logprior: -3.6936e-01
Fitted a model with MAP estimate = -700.0310
expansions: [(0, 3), (92, 2)]
discards: [  1   2   3 193 204]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 705.4835 - loglik: -7.0310e+02 - logprior: -2.3872e+00
Epoch 2/10
39/39 - 25s - loss: 700.5468 - loglik: -7.0027e+02 - logprior: -2.7723e-01
Epoch 3/10
39/39 - 25s - loss: 698.5586 - loglik: -6.9847e+02 - logprior: -8.7919e-02
Epoch 4/10
39/39 - 25s - loss: 696.5551 - loglik: -6.9653e+02 - logprior: -2.2820e-02
Epoch 5/10
39/39 - 25s - loss: 695.5424 - loglik: -6.9560e+02 - logprior: 0.0555
Epoch 6/10
39/39 - 25s - loss: 695.0548 - loglik: -6.9517e+02 - logprior: 0.1189
Epoch 7/10
39/39 - 25s - loss: 695.0251 - loglik: -6.9522e+02 - logprior: 0.1944
Epoch 8/10
39/39 - 25s - loss: 694.5432 - loglik: -6.9482e+02 - logprior: 0.2745
Epoch 9/10
39/39 - 25s - loss: 694.4899 - loglik: -6.9483e+02 - logprior: 0.3449
Epoch 10/10
39/39 - 25s - loss: 694.7460 - loglik: -6.9518e+02 - logprior: 0.4360
Fitted a model with MAP estimate = -693.7767
Time for alignment: 664.0454
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 791.7383 - loglik: -7.9034e+02 - logprior: -1.3949e+00
Epoch 2/10
39/39 - 18s - loss: 723.3438 - loglik: -7.2274e+02 - logprior: -6.0015e-01
Epoch 3/10
39/39 - 18s - loss: 714.8872 - loglik: -7.1420e+02 - logprior: -6.9005e-01
Epoch 4/10
39/39 - 18s - loss: 712.1876 - loglik: -7.1153e+02 - logprior: -6.5712e-01
Epoch 5/10
39/39 - 19s - loss: 710.4293 - loglik: -7.0976e+02 - logprior: -6.7085e-01
Epoch 6/10
39/39 - 18s - loss: 709.9699 - loglik: -7.0928e+02 - logprior: -6.8669e-01
Epoch 7/10
39/39 - 19s - loss: 709.9785 - loglik: -7.0929e+02 - logprior: -6.8480e-01
Fitted a model with MAP estimate = -708.7473
expansions: [(0, 4), (23, 1), (39, 1), (42, 1), (50, 2), (61, 1), (86, 1), (87, 8), (115, 3), (118, 2), (119, 1), (121, 1), (125, 1), (152, 1), (154, 3), (158, 2), (178, 6), (180, 1), (181, 1), (182, 1), (206, 4), (208, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 712.4212 - loglik: -7.1037e+02 - logprior: -2.0556e+00
Epoch 2/2
39/39 - 24s - loss: 704.0467 - loglik: -7.0362e+02 - logprior: -4.2204e-01
Fitted a model with MAP estimate = -701.4784
expansions: [(0, 3), (95, 3)]
discards: [  0   1   2  57 100 141 188 189 213 248 249]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 707.0548 - loglik: -7.0552e+02 - logprior: -1.5339e+00
Epoch 2/2
39/39 - 24s - loss: 703.3358 - loglik: -7.0301e+02 - logprior: -3.2246e-01
Fitted a model with MAP estimate = -701.3021
expansions: [(0, 3), (180, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 705.6469 - loglik: -7.0386e+02 - logprior: -1.7889e+00
Epoch 2/10
39/39 - 24s - loss: 702.3813 - loglik: -7.0215e+02 - logprior: -2.3383e-01
Epoch 3/10
39/39 - 24s - loss: 700.5501 - loglik: -7.0042e+02 - logprior: -1.2795e-01
Epoch 4/10
39/39 - 24s - loss: 698.4887 - loglik: -6.9844e+02 - logprior: -4.7351e-02
Epoch 5/10
39/39 - 24s - loss: 697.6063 - loglik: -6.9764e+02 - logprior: 0.0314
Epoch 6/10
39/39 - 24s - loss: 697.1634 - loglik: -6.9726e+02 - logprior: 0.1000
Epoch 7/10
39/39 - 24s - loss: 696.4944 - loglik: -6.9666e+02 - logprior: 0.1679
Epoch 8/10
39/39 - 24s - loss: 696.7548 - loglik: -6.9700e+02 - logprior: 0.2426
Fitted a model with MAP estimate = -696.0751
Time for alignment: 545.9248
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 793.1225 - loglik: -7.9172e+02 - logprior: -1.4016e+00
Epoch 2/10
39/39 - 19s - loss: 724.7613 - loglik: -7.2407e+02 - logprior: -6.8740e-01
Epoch 3/10
39/39 - 19s - loss: 717.0369 - loglik: -7.1624e+02 - logprior: -7.9578e-01
Epoch 4/10
39/39 - 19s - loss: 713.9236 - loglik: -7.1311e+02 - logprior: -8.1013e-01
Epoch 5/10
39/39 - 19s - loss: 712.3459 - loglik: -7.1149e+02 - logprior: -8.5997e-01
Epoch 6/10
39/39 - 18s - loss: 711.8333 - loglik: -7.1095e+02 - logprior: -8.7959e-01
Epoch 7/10
39/39 - 19s - loss: 711.3307 - loglik: -7.1045e+02 - logprior: -8.7999e-01
Epoch 8/10
39/39 - 19s - loss: 711.5916 - loglik: -7.1070e+02 - logprior: -8.8966e-01
Fitted a model with MAP estimate = -710.4881
expansions: [(0, 3), (9, 1), (10, 1), (23, 3), (43, 1), (51, 2), (53, 1), (74, 1), (86, 2), (87, 1), (92, 9), (96, 1), (98, 2), (116, 4), (117, 2), (118, 1), (122, 1), (126, 1), (153, 2), (155, 3), (156, 1), (158, 2), (176, 1), (183, 3), (184, 1), (211, 1), (212, 1), (213, 2), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 284 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 712.1837 - loglik: -7.1006e+02 - logprior: -2.1227e+00
Epoch 2/2
39/39 - 25s - loss: 701.5765 - loglik: -7.0103e+02 - logprior: -5.5125e-01
Fitted a model with MAP estimate = -699.2710
expansions: []
discards: [  1   2   3  29  60  91  92  93  94  95 125 146 147 148 196 199 200 201
 230]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 707.3028 - loglik: -7.0592e+02 - logprior: -1.3850e+00
Epoch 2/2
39/39 - 23s - loss: 703.6362 - loglik: -7.0338e+02 - logprior: -2.5801e-01
Fitted a model with MAP estimate = -701.4509
expansions: [(0, 3), (88, 2), (91, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 705.0449 - loglik: -7.0301e+02 - logprior: -2.0362e+00
Epoch 2/10
39/39 - 24s - loss: 700.9125 - loglik: -7.0071e+02 - logprior: -2.0430e-01
Epoch 3/10
39/39 - 24s - loss: 699.3396 - loglik: -6.9924e+02 - logprior: -1.0024e-01
Epoch 4/10
39/39 - 24s - loss: 697.5804 - loglik: -6.9754e+02 - logprior: -4.1550e-02
Epoch 5/10
39/39 - 23s - loss: 696.0155 - loglik: -6.9603e+02 - logprior: 0.0172
Epoch 6/10
39/39 - 24s - loss: 695.4265 - loglik: -6.9551e+02 - logprior: 0.0790
Epoch 7/10
39/39 - 24s - loss: 695.5118 - loglik: -6.9567e+02 - logprior: 0.1566
Fitted a model with MAP estimate = -694.9246
Time for alignment: 536.2772
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 792.7768 - loglik: -7.9140e+02 - logprior: -1.3803e+00
Epoch 2/10
39/39 - 18s - loss: 724.5032 - loglik: -7.2399e+02 - logprior: -5.1300e-01
Epoch 3/10
39/39 - 18s - loss: 715.8596 - loglik: -7.1525e+02 - logprior: -6.1266e-01
Epoch 4/10
39/39 - 19s - loss: 713.1033 - loglik: -7.1245e+02 - logprior: -6.4939e-01
Epoch 5/10
39/39 - 18s - loss: 710.9669 - loglik: -7.1030e+02 - logprior: -6.6869e-01
Epoch 6/10
39/39 - 19s - loss: 710.3620 - loglik: -7.0968e+02 - logprior: -6.8619e-01
Epoch 7/10
39/39 - 19s - loss: 709.9599 - loglik: -7.0929e+02 - logprior: -6.7101e-01
Epoch 8/10
39/39 - 18s - loss: 710.2648 - loglik: -7.0959e+02 - logprior: -6.7354e-01
Fitted a model with MAP estimate = -709.1169
expansions: [(0, 4), (40, 1), (47, 1), (49, 1), (50, 1), (74, 1), (85, 2), (89, 10), (96, 2), (114, 4), (115, 2), (116, 1), (120, 1), (152, 2), (153, 3), (157, 2), (163, 1), (175, 1), (177, 2), (183, 1), (205, 6), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 279 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 712.3408 - loglik: -7.1016e+02 - logprior: -2.1768e+00
Epoch 2/2
39/39 - 24s - loss: 703.5438 - loglik: -7.0307e+02 - logprior: -4.7543e-01
Fitted a model with MAP estimate = -701.2094
expansions: [(0, 3), (184, 1), (186, 2)]
discards: [  1   2   3   4 118 139 140 141 190 191 192 193 194 205 206 217 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 708.9675 - loglik: -7.0661e+02 - logprior: -2.3560e+00
Epoch 2/2
39/39 - 23s - loss: 704.5872 - loglik: -7.0426e+02 - logprior: -3.2688e-01
Fitted a model with MAP estimate = -702.1720
expansions: [(0, 3)]
discards: [  1   2   3 186 187]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 708.0468 - loglik: -7.0570e+02 - logprior: -2.3428e+00
Epoch 2/10
39/39 - 23s - loss: 703.7686 - loglik: -7.0353e+02 - logprior: -2.4094e-01
Epoch 3/10
39/39 - 23s - loss: 701.9368 - loglik: -7.0188e+02 - logprior: -6.0976e-02
Epoch 4/10
39/39 - 23s - loss: 700.2955 - loglik: -7.0034e+02 - logprior: 0.0479
Epoch 5/10
39/39 - 23s - loss: 698.6371 - loglik: -6.9877e+02 - logprior: 0.1322
Epoch 6/10
39/39 - 23s - loss: 698.5080 - loglik: -6.9871e+02 - logprior: 0.2038
Epoch 7/10
39/39 - 23s - loss: 698.5497 - loglik: -6.9882e+02 - logprior: 0.2718
Fitted a model with MAP estimate = -697.7135
Time for alignment: 529.7439
Computed alignments with likelihoods: ['-695.9517', '-693.7767', '-696.0751', '-694.9246', '-697.7135']
Best model has likelihood: -693.7767  (prior= 0.4647 )
time for generating output: 0.3566
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.6079702048417133
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 572.9698 - loglik: -5.7116e+02 - logprior: -1.8123e+00
Epoch 2/10
39/39 - 10s - loss: 480.9544 - loglik: -4.7933e+02 - logprior: -1.6224e+00
Epoch 3/10
39/39 - 10s - loss: 473.6487 - loglik: -4.7204e+02 - logprior: -1.6101e+00
Epoch 4/10
39/39 - 10s - loss: 471.7282 - loglik: -4.7012e+02 - logprior: -1.6063e+00
Epoch 5/10
39/39 - 10s - loss: 470.9835 - loglik: -4.6936e+02 - logprior: -1.6191e+00
Epoch 6/10
39/39 - 10s - loss: 470.8997 - loglik: -4.6928e+02 - logprior: -1.6211e+00
Epoch 7/10
39/39 - 10s - loss: 470.7448 - loglik: -4.6912e+02 - logprior: -1.6213e+00
Epoch 8/10
39/39 - 10s - loss: 470.9091 - loglik: -4.6928e+02 - logprior: -1.6245e+00
Fitted a model with MAP estimate = -467.9309
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (50, 1), (66, 1), (67, 2), (71, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (150, 4), (151, 2), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 463.9145 - loglik: -4.6135e+02 - logprior: -2.5657e+00
Epoch 2/2
39/39 - 13s - loss: 453.3350 - loglik: -4.5219e+02 - logprior: -1.1448e+00
Fitted a model with MAP estimate = -448.8108
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 191 199 202]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 457.4388 - loglik: -4.5508e+02 - logprior: -2.3565e+00
Epoch 2/2
39/39 - 13s - loss: 452.7529 - loglik: -4.5210e+02 - logprior: -6.5372e-01
Fitted a model with MAP estimate = -448.6608
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 454.6381 - loglik: -4.5236e+02 - logprior: -2.2778e+00
Epoch 2/10
39/39 - 13s - loss: 450.5089 - loglik: -4.4949e+02 - logprior: -1.0158e+00
Epoch 3/10
39/39 - 13s - loss: 448.3717 - loglik: -4.4821e+02 - logprior: -1.5747e-01
Epoch 4/10
39/39 - 13s - loss: 447.5564 - loglik: -4.4747e+02 - logprior: -8.4403e-02
Epoch 5/10
39/39 - 13s - loss: 446.3985 - loglik: -4.4639e+02 - logprior: -1.2395e-02
Epoch 6/10
39/39 - 13s - loss: 446.8812 - loglik: -4.4694e+02 - logprior: 0.0548
Fitted a model with MAP estimate = -446.2389
Time for alignment: 293.9862
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 574.5738 - loglik: -5.7277e+02 - logprior: -1.8009e+00
Epoch 2/10
39/39 - 10s - loss: 479.7155 - loglik: -4.7810e+02 - logprior: -1.6152e+00
Epoch 3/10
39/39 - 10s - loss: 472.2314 - loglik: -4.7058e+02 - logprior: -1.6516e+00
Epoch 4/10
39/39 - 10s - loss: 470.7667 - loglik: -4.6917e+02 - logprior: -1.5950e+00
Epoch 5/10
39/39 - 10s - loss: 470.0939 - loglik: -4.6850e+02 - logprior: -1.5892e+00
Epoch 6/10
39/39 - 10s - loss: 469.9465 - loglik: -4.6836e+02 - logprior: -1.5910e+00
Epoch 7/10
39/39 - 10s - loss: 469.9110 - loglik: -4.6832e+02 - logprior: -1.5958e+00
Epoch 8/10
39/39 - 10s - loss: 469.9395 - loglik: -4.6834e+02 - logprior: -1.5983e+00
Fitted a model with MAP estimate = -467.0445
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (46, 1), (47, 1), (49, 1), (52, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 463.2306 - loglik: -4.6067e+02 - logprior: -2.5592e+00
Epoch 2/2
39/39 - 13s - loss: 453.3503 - loglik: -4.5221e+02 - logprior: -1.1356e+00
Fitted a model with MAP estimate = -448.8020
expansions: [(3, 1)]
discards: [  0  26  85 139 142 185 194]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 457.4163 - loglik: -4.5506e+02 - logprior: -2.3573e+00
Epoch 2/2
39/39 - 13s - loss: 452.7199 - loglik: -4.5206e+02 - logprior: -6.5871e-01
Fitted a model with MAP estimate = -448.6338
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 454.4953 - loglik: -4.5221e+02 - logprior: -2.2881e+00
Epoch 2/10
39/39 - 13s - loss: 450.6520 - loglik: -4.4963e+02 - logprior: -1.0244e+00
Epoch 3/10
39/39 - 13s - loss: 448.4084 - loglik: -4.4825e+02 - logprior: -1.5655e-01
Epoch 4/10
39/39 - 13s - loss: 447.4564 - loglik: -4.4736e+02 - logprior: -9.3755e-02
Epoch 5/10
39/39 - 13s - loss: 446.2094 - loglik: -4.4618e+02 - logprior: -2.7203e-02
Epoch 6/10
39/39 - 13s - loss: 446.8394 - loglik: -4.4689e+02 - logprior: 0.0485
Fitted a model with MAP estimate = -446.2115
Time for alignment: 292.9912
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 574.7426 - loglik: -5.7293e+02 - logprior: -1.8152e+00
Epoch 2/10
39/39 - 10s - loss: 480.3434 - loglik: -4.7871e+02 - logprior: -1.6304e+00
Epoch 3/10
39/39 - 10s - loss: 472.9041 - loglik: -4.7122e+02 - logprior: -1.6828e+00
Epoch 4/10
39/39 - 10s - loss: 471.2325 - loglik: -4.6960e+02 - logprior: -1.6350e+00
Epoch 5/10
39/39 - 10s - loss: 470.7988 - loglik: -4.6916e+02 - logprior: -1.6362e+00
Epoch 6/10
39/39 - 10s - loss: 470.4482 - loglik: -4.6881e+02 - logprior: -1.6380e+00
Epoch 7/10
39/39 - 10s - loss: 470.4364 - loglik: -4.6880e+02 - logprior: -1.6397e+00
Epoch 8/10
39/39 - 10s - loss: 470.4615 - loglik: -4.6882e+02 - logprior: -1.6416e+00
Fitted a model with MAP estimate = -467.6588
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (83, 1), (86, 1), (87, 1), (92, 1), (95, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 463.3310 - loglik: -4.6073e+02 - logprior: -2.6059e+00
Epoch 2/2
39/39 - 13s - loss: 453.4592 - loglik: -4.5232e+02 - logprior: -1.1399e+00
Fitted a model with MAP estimate = -448.6734
expansions: [(3, 1)]
discards: [  0  26  84 139 142 190]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 457.3080 - loglik: -4.5495e+02 - logprior: -2.3558e+00
Epoch 2/2
39/39 - 13s - loss: 452.6390 - loglik: -4.5200e+02 - logprior: -6.3724e-01
Fitted a model with MAP estimate = -448.5667
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 454.3393 - loglik: -4.5204e+02 - logprior: -2.2950e+00
Epoch 2/10
39/39 - 13s - loss: 450.5124 - loglik: -4.4946e+02 - logprior: -1.0513e+00
Epoch 3/10
39/39 - 13s - loss: 448.2170 - loglik: -4.4804e+02 - logprior: -1.7823e-01
Epoch 4/10
39/39 - 13s - loss: 447.5152 - loglik: -4.4741e+02 - logprior: -1.0308e-01
Epoch 5/10
39/39 - 13s - loss: 446.0103 - loglik: -4.4596e+02 - logprior: -4.6808e-02
Epoch 6/10
39/39 - 13s - loss: 447.0170 - loglik: -4.4704e+02 - logprior: 0.0277
Fitted a model with MAP estimate = -446.1633
Time for alignment: 293.6444
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 574.2645 - loglik: -5.7245e+02 - logprior: -1.8111e+00
Epoch 2/10
39/39 - 10s - loss: 481.1973 - loglik: -4.7957e+02 - logprior: -1.6256e+00
Epoch 3/10
39/39 - 10s - loss: 473.7957 - loglik: -4.7215e+02 - logprior: -1.6407e+00
Epoch 4/10
39/39 - 10s - loss: 471.5750 - loglik: -4.6997e+02 - logprior: -1.6017e+00
Epoch 5/10
39/39 - 10s - loss: 470.9603 - loglik: -4.6936e+02 - logprior: -1.6000e+00
Epoch 6/10
39/39 - 10s - loss: 470.6896 - loglik: -4.6909e+02 - logprior: -1.6041e+00
Epoch 7/10
39/39 - 10s - loss: 470.7287 - loglik: -4.6912e+02 - logprior: -1.6087e+00
Fitted a model with MAP estimate = -467.8305
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 2), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (90, 1), (93, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 3), (151, 2), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 463.9995 - loglik: -4.6137e+02 - logprior: -2.6269e+00
Epoch 2/2
39/39 - 14s - loss: 453.1630 - loglik: -4.5196e+02 - logprior: -1.2020e+00
Fitted a model with MAP estimate = -448.5119
expansions: [(3, 1)]
discards: [  0  26  42  50  87 141 144 192 193 194 202 205]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 457.3482 - loglik: -4.5498e+02 - logprior: -2.3635e+00
Epoch 2/2
39/39 - 13s - loss: 452.7200 - loglik: -4.5210e+02 - logprior: -6.1989e-01
Fitted a model with MAP estimate = -448.6381
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 454.5295 - loglik: -4.5224e+02 - logprior: -2.2898e+00
Epoch 2/10
39/39 - 13s - loss: 450.4129 - loglik: -4.4938e+02 - logprior: -1.0346e+00
Epoch 3/10
39/39 - 13s - loss: 448.3275 - loglik: -4.4815e+02 - logprior: -1.7302e-01
Epoch 4/10
39/39 - 13s - loss: 447.5669 - loglik: -4.4746e+02 - logprior: -1.0436e-01
Epoch 5/10
39/39 - 13s - loss: 446.6998 - loglik: -4.4667e+02 - logprior: -3.2416e-02
Epoch 6/10
39/39 - 13s - loss: 446.6302 - loglik: -4.4666e+02 - logprior: 0.0306
Epoch 7/10
39/39 - 13s - loss: 446.5588 - loglik: -4.4667e+02 - logprior: 0.1114
Epoch 8/10
39/39 - 13s - loss: 446.8659 - loglik: -4.4704e+02 - logprior: 0.1786
Fitted a model with MAP estimate = -445.9570
Time for alignment: 309.1449
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 573.9640 - loglik: -5.7216e+02 - logprior: -1.8069e+00
Epoch 2/10
39/39 - 10s - loss: 480.0883 - loglik: -4.7844e+02 - logprior: -1.6489e+00
Epoch 3/10
39/39 - 10s - loss: 473.4837 - loglik: -4.7183e+02 - logprior: -1.6567e+00
Epoch 4/10
39/39 - 10s - loss: 471.9698 - loglik: -4.7038e+02 - logprior: -1.5905e+00
Epoch 5/10
39/39 - 10s - loss: 471.5722 - loglik: -4.6999e+02 - logprior: -1.5851e+00
Epoch 6/10
39/39 - 10s - loss: 471.3274 - loglik: -4.6974e+02 - logprior: -1.5889e+00
Epoch 7/10
39/39 - 10s - loss: 471.3014 - loglik: -4.6971e+02 - logprior: -1.5947e+00
Epoch 8/10
39/39 - 10s - loss: 471.3779 - loglik: -4.6977e+02 - logprior: -1.6049e+00
Fitted a model with MAP estimate = -468.4111
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (94, 1), (97, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 464.1268 - loglik: -4.6155e+02 - logprior: -2.5767e+00
Epoch 2/2
39/39 - 14s - loss: 453.2780 - loglik: -4.5209e+02 - logprior: -1.1879e+00
Fitted a model with MAP estimate = -448.4654
expansions: [(3, 1)]
discards: [  0  26  84 139 142 189 190 191 192 200 203]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 457.5485 - loglik: -4.5519e+02 - logprior: -2.3552e+00
Epoch 2/2
39/39 - 13s - loss: 452.9150 - loglik: -4.5228e+02 - logprior: -6.3037e-01
Fitted a model with MAP estimate = -448.7804
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 454.5674 - loglik: -4.5228e+02 - logprior: -2.2847e+00
Epoch 2/10
39/39 - 13s - loss: 450.6931 - loglik: -4.4966e+02 - logprior: -1.0305e+00
Epoch 3/10
39/39 - 13s - loss: 448.6176 - loglik: -4.4845e+02 - logprior: -1.6919e-01
Epoch 4/10
39/39 - 13s - loss: 447.3447 - loglik: -4.4725e+02 - logprior: -9.1145e-02
Epoch 5/10
39/39 - 13s - loss: 446.9776 - loglik: -4.4695e+02 - logprior: -3.0930e-02
Epoch 6/10
39/39 - 13s - loss: 447.0781 - loglik: -4.4712e+02 - logprior: 0.0441
Fitted a model with MAP estimate = -446.4174
Time for alignment: 292.7610
Computed alignments with likelihoods: ['-446.2389', '-446.2115', '-446.1633', '-445.9570', '-446.4174']
Best model has likelihood: -445.9570  (prior= 0.2117 )
time for generating output: 0.3436
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.6735218106105842
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fef88229220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fef882293a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882298b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882881c0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882884f0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882886a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fef882888e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288490>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288430>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288e50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288070>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882887f0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288340>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef882882b0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288610>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288820>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fef88288760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef882886d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288640> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fef882a3e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef88288a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7ff0a7257700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7fef88283160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef88288370>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.8335 - loglik: -3.1776e+02 - logprior: -3.0698e+00
Epoch 2/10
19/19 - 2s - loss: 277.5284 - loglik: -2.7617e+02 - logprior: -1.3561e+00
Epoch 3/10
19/19 - 2s - loss: 254.8738 - loglik: -2.5320e+02 - logprior: -1.6736e+00
Epoch 4/10
19/19 - 2s - loss: 249.0117 - loglik: -2.4743e+02 - logprior: -1.5836e+00
Epoch 5/10
19/19 - 2s - loss: 246.3215 - loglik: -2.4476e+02 - logprior: -1.5651e+00
Epoch 6/10
19/19 - 2s - loss: 244.3609 - loglik: -2.4282e+02 - logprior: -1.5414e+00
Epoch 7/10
19/19 - 2s - loss: 243.9594 - loglik: -2.4243e+02 - logprior: -1.5307e+00
Epoch 8/10
19/19 - 2s - loss: 243.2154 - loglik: -2.4169e+02 - logprior: -1.5206e+00
Epoch 9/10
19/19 - 2s - loss: 243.7878 - loglik: -2.4227e+02 - logprior: -1.5203e+00
Fitted a model with MAP estimate = -243.2240
expansions: [(17, 1), (18, 3), (19, 4), (21, 2), (24, 1), (28, 2), (32, 2), (41, 1), (44, 1), (48, 1), (49, 1), (56, 1), (63, 2), (64, 1), (65, 2), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 249.6198 - loglik: -2.4568e+02 - logprior: -3.9371e+00
Epoch 2/2
19/19 - 3s - loss: 237.8549 - loglik: -2.3587e+02 - logprior: -1.9830e+00
Fitted a model with MAP estimate = -235.6970
expansions: [(0, 2)]
discards: [ 0 15 16 23 28 38 44 83 88 93]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 238.8521 - loglik: -2.3598e+02 - logprior: -2.8679e+00
Epoch 2/2
19/19 - 3s - loss: 234.9449 - loglik: -2.3392e+02 - logprior: -1.0288e+00
Fitted a model with MAP estimate = -233.8779
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.2969 - loglik: -2.3665e+02 - logprior: -3.6479e+00
Epoch 2/10
19/19 - 3s - loss: 235.7079 - loglik: -2.3456e+02 - logprior: -1.1447e+00
Epoch 3/10
19/19 - 3s - loss: 233.7660 - loglik: -2.3280e+02 - logprior: -9.7064e-01
Epoch 4/10
19/19 - 3s - loss: 232.5354 - loglik: -2.3160e+02 - logprior: -9.3158e-01
Epoch 5/10
19/19 - 3s - loss: 232.2466 - loglik: -2.3135e+02 - logprior: -8.9914e-01
Epoch 6/10
19/19 - 3s - loss: 231.5376 - loglik: -2.3066e+02 - logprior: -8.8128e-01
Epoch 7/10
19/19 - 3s - loss: 231.2767 - loglik: -2.3041e+02 - logprior: -8.6322e-01
Epoch 8/10
19/19 - 3s - loss: 231.4834 - loglik: -2.3064e+02 - logprior: -8.4690e-01
Fitted a model with MAP estimate = -231.0138
Time for alignment: 86.2492
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.0333 - loglik: -3.1796e+02 - logprior: -3.0718e+00
Epoch 2/10
19/19 - 2s - loss: 278.6827 - loglik: -2.7733e+02 - logprior: -1.3554e+00
Epoch 3/10
19/19 - 2s - loss: 256.4977 - loglik: -2.5484e+02 - logprior: -1.6527e+00
Epoch 4/10
19/19 - 2s - loss: 249.5166 - loglik: -2.4795e+02 - logprior: -1.5700e+00
Epoch 5/10
19/19 - 2s - loss: 246.8256 - loglik: -2.4526e+02 - logprior: -1.5686e+00
Epoch 6/10
19/19 - 2s - loss: 245.6109 - loglik: -2.4407e+02 - logprior: -1.5423e+00
Epoch 7/10
19/19 - 2s - loss: 245.7383 - loglik: -2.4421e+02 - logprior: -1.5288e+00
Fitted a model with MAP estimate = -245.2398
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 2), (23, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (44, 1), (48, 1), (56, 1), (63, 1), (64, 2), (65, 2), (69, 1), (71, 1), (76, 2), (82, 1), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 249.5214 - loglik: -2.4559e+02 - logprior: -3.9359e+00
Epoch 2/2
19/19 - 3s - loss: 238.3604 - loglik: -2.3647e+02 - logprior: -1.8869e+00
Fitted a model with MAP estimate = -236.5773
expansions: [(0, 2)]
discards: [ 0 15 16 29 38 83]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.8540 - loglik: -2.3594e+02 - logprior: -2.9184e+00
Epoch 2/2
19/19 - 3s - loss: 234.9240 - loglik: -2.3386e+02 - logprior: -1.0614e+00
Fitted a model with MAP estimate = -234.0170
expansions: [(22, 1)]
discards: [ 0 19 83]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.9229 - loglik: -2.3729e+02 - logprior: -3.6323e+00
Epoch 2/10
19/19 - 3s - loss: 236.2418 - loglik: -2.3516e+02 - logprior: -1.0848e+00
Epoch 3/10
19/19 - 3s - loss: 233.7454 - loglik: -2.3287e+02 - logprior: -8.7087e-01
Epoch 4/10
19/19 - 3s - loss: 233.2736 - loglik: -2.3245e+02 - logprior: -8.2365e-01
Epoch 5/10
19/19 - 3s - loss: 232.7555 - loglik: -2.3195e+02 - logprior: -8.0498e-01
Epoch 6/10
19/19 - 3s - loss: 232.0444 - loglik: -2.3126e+02 - logprior: -7.8451e-01
Epoch 7/10
19/19 - 3s - loss: 231.2938 - loglik: -2.3053e+02 - logprior: -7.6629e-01
Epoch 8/10
19/19 - 3s - loss: 231.9602 - loglik: -2.3119e+02 - logprior: -7.7026e-01
Fitted a model with MAP estimate = -231.2700
Time for alignment: 81.5251
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 321.1321 - loglik: -3.1806e+02 - logprior: -3.0732e+00
Epoch 2/10
19/19 - 2s - loss: 277.2203 - loglik: -2.7586e+02 - logprior: -1.3584e+00
Epoch 3/10
19/19 - 2s - loss: 253.5260 - loglik: -2.5182e+02 - logprior: -1.7012e+00
Epoch 4/10
19/19 - 2s - loss: 246.7322 - loglik: -2.4509e+02 - logprior: -1.6427e+00
Epoch 5/10
19/19 - 2s - loss: 244.2657 - loglik: -2.4265e+02 - logprior: -1.6127e+00
Epoch 6/10
19/19 - 2s - loss: 244.0983 - loglik: -2.4253e+02 - logprior: -1.5699e+00
Epoch 7/10
19/19 - 2s - loss: 243.4272 - loglik: -2.4188e+02 - logprior: -1.5462e+00
Epoch 8/10
19/19 - 2s - loss: 243.5159 - loglik: -2.4198e+02 - logprior: -1.5406e+00
Fitted a model with MAP estimate = -243.0597
expansions: [(17, 1), (18, 3), (19, 4), (22, 1), (24, 1), (28, 2), (32, 2), (40, 1), (43, 1), (44, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 248.3806 - loglik: -2.4449e+02 - logprior: -3.8931e+00
Epoch 2/2
19/19 - 3s - loss: 237.5223 - loglik: -2.3564e+02 - logprior: -1.8865e+00
Fitted a model with MAP estimate = -235.6881
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 37 43]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.5977 - loglik: -2.3574e+02 - logprior: -2.8600e+00
Epoch 2/2
19/19 - 3s - loss: 234.8533 - loglik: -2.3382e+02 - logprior: -1.0293e+00
Fitted a model with MAP estimate = -233.9641
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.5167 - loglik: -2.3687e+02 - logprior: -3.6514e+00
Epoch 2/10
19/19 - 3s - loss: 235.7527 - loglik: -2.3460e+02 - logprior: -1.1557e+00
Epoch 3/10
19/19 - 3s - loss: 233.8134 - loglik: -2.3283e+02 - logprior: -9.7889e-01
Epoch 4/10
19/19 - 3s - loss: 232.8006 - loglik: -2.3186e+02 - logprior: -9.4466e-01
Epoch 5/10
19/19 - 3s - loss: 232.1451 - loglik: -2.3123e+02 - logprior: -9.1664e-01
Epoch 6/10
19/19 - 3s - loss: 231.6972 - loglik: -2.3080e+02 - logprior: -8.9770e-01
Epoch 7/10
19/19 - 3s - loss: 231.5144 - loglik: -2.3063e+02 - logprior: -8.7966e-01
Epoch 8/10
19/19 - 3s - loss: 231.1277 - loglik: -2.3027e+02 - logprior: -8.5355e-01
Epoch 9/10
19/19 - 3s - loss: 231.3844 - loglik: -2.3055e+02 - logprior: -8.3645e-01
Fitted a model with MAP estimate = -231.0478
Time for alignment: 85.1404
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.7975 - loglik: -3.1772e+02 - logprior: -3.0739e+00
Epoch 2/10
19/19 - 2s - loss: 275.7768 - loglik: -2.7442e+02 - logprior: -1.3524e+00
Epoch 3/10
19/19 - 2s - loss: 252.7131 - loglik: -2.5103e+02 - logprior: -1.6797e+00
Epoch 4/10
19/19 - 2s - loss: 246.4707 - loglik: -2.4484e+02 - logprior: -1.6301e+00
Epoch 5/10
19/19 - 2s - loss: 244.8717 - loglik: -2.4328e+02 - logprior: -1.5885e+00
Epoch 6/10
19/19 - 2s - loss: 244.0566 - loglik: -2.4251e+02 - logprior: -1.5443e+00
Epoch 7/10
19/19 - 2s - loss: 243.5596 - loglik: -2.4203e+02 - logprior: -1.5293e+00
Epoch 8/10
19/19 - 2s - loss: 243.5381 - loglik: -2.4203e+02 - logprior: -1.5126e+00
Epoch 9/10
19/19 - 2s - loss: 243.5388 - loglik: -2.4203e+02 - logprior: -1.5130e+00
Fitted a model with MAP estimate = -243.2342
expansions: [(17, 1), (18, 3), (19, 4), (22, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (47, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 248.7914 - loglik: -2.4495e+02 - logprior: -3.8427e+00
Epoch 2/2
19/19 - 3s - loss: 238.1430 - loglik: -2.3638e+02 - logprior: -1.7669e+00
Fitted a model with MAP estimate = -236.3412
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 43 90]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 239.0396 - loglik: -2.3615e+02 - logprior: -2.8901e+00
Epoch 2/2
19/19 - 3s - loss: 235.4210 - loglik: -2.3441e+02 - logprior: -1.0092e+00
Fitted a model with MAP estimate = -234.5612
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 241.4614 - loglik: -2.3790e+02 - logprior: -3.5606e+00
Epoch 2/10
19/19 - 3s - loss: 236.5696 - loglik: -2.3562e+02 - logprior: -9.4810e-01
Epoch 3/10
19/19 - 3s - loss: 234.5042 - loglik: -2.3383e+02 - logprior: -6.7138e-01
Epoch 4/10
19/19 - 3s - loss: 233.7547 - loglik: -2.3309e+02 - logprior: -6.6557e-01
Epoch 5/10
19/19 - 3s - loss: 232.7739 - loglik: -2.3205e+02 - logprior: -7.2798e-01
Epoch 6/10
19/19 - 3s - loss: 232.0087 - loglik: -2.3117e+02 - logprior: -8.3852e-01
Epoch 7/10
19/19 - 3s - loss: 231.3463 - loglik: -2.3050e+02 - logprior: -8.4857e-01
Epoch 8/10
19/19 - 3s - loss: 231.6244 - loglik: -2.3081e+02 - logprior: -8.1545e-01
Fitted a model with MAP estimate = -231.2204
Time for alignment: 84.7164
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.8706 - loglik: -3.1779e+02 - logprior: -3.0767e+00
Epoch 2/10
19/19 - 2s - loss: 276.2491 - loglik: -2.7489e+02 - logprior: -1.3636e+00
Epoch 3/10
19/19 - 2s - loss: 252.1890 - loglik: -2.5048e+02 - logprior: -1.7118e+00
Epoch 4/10
19/19 - 2s - loss: 246.8717 - loglik: -2.4523e+02 - logprior: -1.6450e+00
Epoch 5/10
19/19 - 2s - loss: 244.6918 - loglik: -2.4307e+02 - logprior: -1.6210e+00
Epoch 6/10
19/19 - 2s - loss: 244.0530 - loglik: -2.4247e+02 - logprior: -1.5837e+00
Epoch 7/10
19/19 - 2s - loss: 243.8640 - loglik: -2.4230e+02 - logprior: -1.5609e+00
Epoch 8/10
19/19 - 2s - loss: 243.8351 - loglik: -2.4229e+02 - logprior: -1.5477e+00
Epoch 9/10
19/19 - 2s - loss: 243.2728 - loglik: -2.4173e+02 - logprior: -1.5474e+00
Epoch 10/10
19/19 - 2s - loss: 243.5769 - loglik: -2.4203e+02 - logprior: -1.5469e+00
Fitted a model with MAP estimate = -243.2466
expansions: [(17, 1), (19, 2), (20, 4), (22, 2), (23, 2), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (56, 1), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 248.9072 - loglik: -2.4500e+02 - logprior: -3.9114e+00
Epoch 2/2
19/19 - 3s - loss: 237.6977 - loglik: -2.3575e+02 - logprior: -1.9462e+00
Fitted a model with MAP estimate = -235.8110
expansions: [(0, 2)]
discards: [ 0 15 16 23 30 31 39 45]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 238.4511 - loglik: -2.3559e+02 - logprior: -2.8571e+00
Epoch 2/2
19/19 - 3s - loss: 234.7344 - loglik: -2.3371e+02 - logprior: -1.0290e+00
Fitted a model with MAP estimate = -233.8405
expansions: [(16, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.2360 - loglik: -2.3652e+02 - logprior: -3.7142e+00
Epoch 2/10
19/19 - 3s - loss: 235.2699 - loglik: -2.3407e+02 - logprior: -1.1981e+00
Epoch 3/10
19/19 - 3s - loss: 233.0797 - loglik: -2.3211e+02 - logprior: -9.7280e-01
Epoch 4/10
19/19 - 3s - loss: 232.1770 - loglik: -2.3124e+02 - logprior: -9.3313e-01
Epoch 5/10
19/19 - 3s - loss: 231.2500 - loglik: -2.3035e+02 - logprior: -9.0325e-01
Epoch 6/10
19/19 - 3s - loss: 231.0090 - loglik: -2.3013e+02 - logprior: -8.8191e-01
Epoch 7/10
19/19 - 3s - loss: 230.5944 - loglik: -2.2973e+02 - logprior: -8.6392e-01
Epoch 8/10
19/19 - 3s - loss: 230.4756 - loglik: -2.2964e+02 - logprior: -8.4024e-01
Epoch 9/10
19/19 - 3s - loss: 230.2617 - loglik: -2.2945e+02 - logprior: -8.1611e-01
Epoch 10/10
19/19 - 3s - loss: 230.4674 - loglik: -2.2968e+02 - logprior: -7.8813e-01
Fitted a model with MAP estimate = -230.2535
Time for alignment: 94.6850
Computed alignments with likelihoods: ['-231.0138', '-231.2700', '-231.0478', '-231.2204', '-230.2535']
Best model has likelihood: -230.2535  (prior= -0.7759 )
time for generating output: 0.1481
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8540583136327817
