Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9177ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9690735460>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 770.8486 - loglik: -7.6931e+02 - logprior: -1.5391e+00
Epoch 2/10
39/39 - 30s - loss: 646.0724 - loglik: -6.4414e+02 - logprior: -1.9327e+00
Epoch 3/10
39/39 - 31s - loss: 636.1779 - loglik: -6.3434e+02 - logprior: -1.8428e+00
Epoch 4/10
39/39 - 31s - loss: 633.9541 - loglik: -6.3218e+02 - logprior: -1.7766e+00
Epoch 5/10
39/39 - 31s - loss: 632.5034 - loglik: -6.3071e+02 - logprior: -1.7943e+00
Epoch 6/10
39/39 - 31s - loss: 631.4572 - loglik: -6.2965e+02 - logprior: -1.8111e+00
Epoch 7/10
39/39 - 31s - loss: 631.7125 - loglik: -6.2989e+02 - logprior: -1.8238e+00
Fitted a model with MAP estimate = -631.1426
expansions: [(12, 2), (13, 1), (14, 1), (52, 4), (54, 1), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (85, 1), (87, 1), (90, 1), (94, 1), (97, 1), (99, 1), (102, 1), (108, 1), (117, 1), (119, 1), (121, 1), (134, 1), (139, 1), (141, 1), (142, 1), (145, 1), (156, 1), (157, 1), (158, 1), (161, 2), (162, 1), (163, 1), (164, 1), (171, 1), (182, 1), (183, 1), (187, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (200, 1), (213, 1), (216, 1), (217, 1), (219, 1), (227, 1), (228, 1), (234, 1), (235, 2), (237, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 612.3241 - loglik: -6.1069e+02 - logprior: -1.6376e+00
Epoch 2/2
39/39 - 45s - loss: 602.4499 - loglik: -6.0192e+02 - logprior: -5.3227e-01
Fitted a model with MAP estimate = -600.8981
expansions: [(19, 1)]
discards: [ 55  56 250 287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 603.1823 - loglik: -6.0194e+02 - logprior: -1.2411e+00
Epoch 2/2
39/39 - 45s - loss: 601.7953 - loglik: -6.0165e+02 - logprior: -1.4346e-01
Fitted a model with MAP estimate = -600.6526
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 602.4525 - loglik: -6.0143e+02 - logprior: -1.0259e+00
Epoch 2/10
39/39 - 44s - loss: 601.3341 - loglik: -6.0145e+02 - logprior: 0.1184
Epoch 3/10
39/39 - 44s - loss: 600.8251 - loglik: -6.0114e+02 - logprior: 0.3192
Epoch 4/10
39/39 - 45s - loss: 600.6719 - loglik: -6.0112e+02 - logprior: 0.4509
Epoch 5/10
39/39 - 45s - loss: 600.1335 - loglik: -6.0077e+02 - logprior: 0.6370
Epoch 6/10
39/39 - 44s - loss: 599.9505 - loglik: -6.0071e+02 - logprior: 0.7596
Epoch 7/10
39/39 - 45s - loss: 599.8123 - loglik: -6.0074e+02 - logprior: 0.9284
Epoch 8/10
39/39 - 45s - loss: 600.5177 - loglik: -6.0160e+02 - logprior: 1.0832
Fitted a model with MAP estimate = -599.5099
Time for alignment: 952.4369
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 769.8102 - loglik: -7.6826e+02 - logprior: -1.5500e+00
Epoch 2/10
39/39 - 31s - loss: 645.2134 - loglik: -6.4329e+02 - logprior: -1.9216e+00
Epoch 3/10
39/39 - 31s - loss: 636.1586 - loglik: -6.3433e+02 - logprior: -1.8252e+00
Epoch 4/10
39/39 - 31s - loss: 633.6010 - loglik: -6.3184e+02 - logprior: -1.7597e+00
Epoch 5/10
39/39 - 31s - loss: 632.3798 - loglik: -6.3061e+02 - logprior: -1.7693e+00
Epoch 6/10
39/39 - 31s - loss: 631.8446 - loglik: -6.3006e+02 - logprior: -1.7894e+00
Epoch 7/10
39/39 - 31s - loss: 631.1174 - loglik: -6.2931e+02 - logprior: -1.8105e+00
Epoch 8/10
39/39 - 31s - loss: 631.5484 - loglik: -6.2974e+02 - logprior: -1.8076e+00
Fitted a model with MAP estimate = -630.9782
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (52, 3), (56, 1), (62, 2), (63, 2), (64, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 2), (85, 1), (90, 1), (95, 1), (98, 1), (100, 1), (103, 1), (106, 1), (112, 1), (117, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (157, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (174, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 2), (197, 1), (200, 1), (201, 1), (213, 1), (214, 1), (216, 1), (217, 1), (219, 1), (228, 1), (234, 2), (235, 2), (237, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 612.3093 - loglik: -6.1069e+02 - logprior: -1.6177e+00
Epoch 2/2
39/39 - 45s - loss: 603.1642 - loglik: -6.0261e+02 - logprior: -5.5553e-01
Fitted a model with MAP estimate = -601.7480
expansions: []
discards: [ 76 295]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 603.2554 - loglik: -6.0199e+02 - logprior: -1.2615e+00
Epoch 2/2
39/39 - 45s - loss: 602.3276 - loglik: -6.0222e+02 - logprior: -1.0638e-01
Fitted a model with MAP estimate = -600.9627
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 602.6002 - loglik: -6.0159e+02 - logprior: -1.0070e+00
Epoch 2/10
39/39 - 44s - loss: 601.9797 - loglik: -6.0210e+02 - logprior: 0.1172
Epoch 3/10
39/39 - 45s - loss: 601.1677 - loglik: -6.0151e+02 - logprior: 0.3408
Epoch 4/10
39/39 - 45s - loss: 600.6409 - loglik: -6.0112e+02 - logprior: 0.4743
Epoch 5/10
39/39 - 44s - loss: 600.7094 - loglik: -6.0133e+02 - logprior: 0.6242
Fitted a model with MAP estimate = -600.1319
Time for alignment: 847.8723
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 771.0950 - loglik: -7.6955e+02 - logprior: -1.5436e+00
Epoch 2/10
39/39 - 31s - loss: 645.9129 - loglik: -6.4404e+02 - logprior: -1.8776e+00
Epoch 3/10
39/39 - 31s - loss: 635.8096 - loglik: -6.3404e+02 - logprior: -1.7702e+00
Epoch 4/10
39/39 - 31s - loss: 633.3162 - loglik: -6.3162e+02 - logprior: -1.6977e+00
Epoch 5/10
39/39 - 31s - loss: 631.0699 - loglik: -6.2935e+02 - logprior: -1.7199e+00
Epoch 6/10
39/39 - 31s - loss: 631.3040 - loglik: -6.2956e+02 - logprior: -1.7438e+00
Fitted a model with MAP estimate = -630.3776
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 5), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (94, 1), (97, 1), (99, 1), (102, 1), (105, 1), (117, 1), (119, 1), (121, 1), (134, 1), (140, 1), (142, 1), (143, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 2), (163, 3), (164, 1), (172, 1), (183, 1), (187, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (200, 1), (213, 1), (217, 1), (219, 1), (227, 1), (228, 2), (234, 1), (235, 2), (263, 1), (264, 2), (265, 1), (266, 3), (267, 2), (268, 1), (279, 2), (280, 3)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 611.8787 - loglik: -6.1025e+02 - logprior: -1.6332e+00
Epoch 2/2
39/39 - 46s - loss: 602.2291 - loglik: -6.0169e+02 - logprior: -5.4114e-01
Fitted a model with MAP estimate = -600.7021
expansions: [(16, 1)]
discards: [ 56 250 287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 602.8178 - loglik: -6.0158e+02 - logprior: -1.2335e+00
Epoch 2/2
39/39 - 45s - loss: 601.4173 - loglik: -6.0128e+02 - logprior: -1.3462e-01
Fitted a model with MAP estimate = -600.2669
expansions: []
discards: [56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 602.2866 - loglik: -6.0128e+02 - logprior: -1.0088e+00
Epoch 2/10
39/39 - 44s - loss: 601.2360 - loglik: -6.0136e+02 - logprior: 0.1281
Epoch 3/10
39/39 - 45s - loss: 600.2827 - loglik: -6.0057e+02 - logprior: 0.2845
Epoch 4/10
39/39 - 45s - loss: 600.6057 - loglik: -6.0105e+02 - logprior: 0.4410
Fitted a model with MAP estimate = -599.6935
Time for alignment: 746.6444
Computed alignments with likelihoods: ['-599.5099', '-600.1319', '-599.6935']
Best model has likelihood: -599.5099
SP score = 0.9586
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9177ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9690735070>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0087 - loglik: -1.4681e+02 - logprior: -3.1977e+00
Epoch 2/10
19/19 - 1s - loss: 128.0039 - loglik: -1.2663e+02 - logprior: -1.3773e+00
Epoch 3/10
19/19 - 1s - loss: 119.9396 - loglik: -1.1838e+02 - logprior: -1.5632e+00
Epoch 4/10
19/19 - 1s - loss: 118.4783 - loglik: -1.1704e+02 - logprior: -1.4384e+00
Epoch 5/10
19/19 - 1s - loss: 118.0124 - loglik: -1.1659e+02 - logprior: -1.4229e+00
Epoch 6/10
19/19 - 1s - loss: 117.8667 - loglik: -1.1647e+02 - logprior: -1.3942e+00
Epoch 7/10
19/19 - 1s - loss: 117.6697 - loglik: -1.1629e+02 - logprior: -1.3834e+00
Epoch 8/10
19/19 - 1s - loss: 117.5816 - loglik: -1.1621e+02 - logprior: -1.3747e+00
Epoch 9/10
19/19 - 1s - loss: 117.5990 - loglik: -1.1623e+02 - logprior: -1.3701e+00
Fitted a model with MAP estimate = -117.3852
expansions: [(6, 1), (7, 1), (8, 1), (16, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 121.3847 - loglik: -1.1723e+02 - logprior: -4.1539e+00
Epoch 2/2
19/19 - 1s - loss: 113.0249 - loglik: -1.1088e+02 - logprior: -2.1438e+00
Fitted a model with MAP estimate = -111.6457
expansions: [(0, 2)]
discards: [ 0 22 34 36 40 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 112.3111 - loglik: -1.0929e+02 - logprior: -3.0240e+00
Epoch 2/2
19/19 - 1s - loss: 109.3609 - loglik: -1.0815e+02 - logprior: -1.2153e+00
Fitted a model with MAP estimate = -108.9915
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 113.8542 - loglik: -1.1008e+02 - logprior: -3.7769e+00
Epoch 2/10
19/19 - 1s - loss: 110.0103 - loglik: -1.0859e+02 - logprior: -1.4158e+00
Epoch 3/10
19/19 - 1s - loss: 109.5120 - loglik: -1.0827e+02 - logprior: -1.2444e+00
Epoch 4/10
19/19 - 1s - loss: 109.4295 - loglik: -1.0822e+02 - logprior: -1.2053e+00
Epoch 5/10
19/19 - 1s - loss: 109.3101 - loglik: -1.0813e+02 - logprior: -1.1797e+00
Epoch 6/10
19/19 - 1s - loss: 109.3734 - loglik: -1.0821e+02 - logprior: -1.1649e+00
Fitted a model with MAP estimate = -109.2251
Time for alignment: 37.5561
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1665 - loglik: -1.4697e+02 - logprior: -3.1949e+00
Epoch 2/10
19/19 - 1s - loss: 128.7413 - loglik: -1.2734e+02 - logprior: -1.3992e+00
Epoch 3/10
19/19 - 1s - loss: 119.0336 - loglik: -1.1742e+02 - logprior: -1.6157e+00
Epoch 4/10
19/19 - 1s - loss: 117.5191 - loglik: -1.1603e+02 - logprior: -1.4895e+00
Epoch 5/10
19/19 - 1s - loss: 116.9007 - loglik: -1.1543e+02 - logprior: -1.4689e+00
Epoch 6/10
19/19 - 1s - loss: 116.7061 - loglik: -1.1526e+02 - logprior: -1.4413e+00
Epoch 7/10
19/19 - 1s - loss: 116.4939 - loglik: -1.1507e+02 - logprior: -1.4275e+00
Epoch 8/10
19/19 - 1s - loss: 116.5549 - loglik: -1.1514e+02 - logprior: -1.4123e+00
Fitted a model with MAP estimate = -116.3503
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (23, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 120.5672 - loglik: -1.1643e+02 - logprior: -4.1375e+00
Epoch 2/2
19/19 - 1s - loss: 112.8344 - loglik: -1.1072e+02 - logprior: -2.1103e+00
Fitted a model with MAP estimate = -111.5256
expansions: [(0, 2)]
discards: [ 0 22 34 36 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 112.1746 - loglik: -1.0916e+02 - logprior: -3.0179e+00
Epoch 2/2
19/19 - 1s - loss: 109.3807 - loglik: -1.0817e+02 - logprior: -1.2156e+00
Fitted a model with MAP estimate = -108.9771
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 113.8211 - loglik: -1.1006e+02 - logprior: -3.7654e+00
Epoch 2/10
19/19 - 1s - loss: 110.0177 - loglik: -1.0860e+02 - logprior: -1.4152e+00
Epoch 3/10
19/19 - 1s - loss: 109.6160 - loglik: -1.0837e+02 - logprior: -1.2470e+00
Epoch 4/10
19/19 - 1s - loss: 109.3302 - loglik: -1.0812e+02 - logprior: -1.2053e+00
Epoch 5/10
19/19 - 1s - loss: 109.2974 - loglik: -1.0812e+02 - logprior: -1.1804e+00
Epoch 6/10
19/19 - 1s - loss: 109.3099 - loglik: -1.0815e+02 - logprior: -1.1609e+00
Fitted a model with MAP estimate = -109.2354
Time for alignment: 36.1406
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1965 - loglik: -1.4700e+02 - logprior: -3.1963e+00
Epoch 2/10
19/19 - 1s - loss: 129.9780 - loglik: -1.2860e+02 - logprior: -1.3814e+00
Epoch 3/10
19/19 - 1s - loss: 119.7944 - loglik: -1.1819e+02 - logprior: -1.6046e+00
Epoch 4/10
19/19 - 1s - loss: 117.8001 - loglik: -1.1632e+02 - logprior: -1.4815e+00
Epoch 5/10
19/19 - 1s - loss: 117.3104 - loglik: -1.1585e+02 - logprior: -1.4596e+00
Epoch 6/10
19/19 - 1s - loss: 116.9185 - loglik: -1.1548e+02 - logprior: -1.4349e+00
Epoch 7/10
19/19 - 1s - loss: 116.9423 - loglik: -1.1552e+02 - logprior: -1.4207e+00
Fitted a model with MAP estimate = -116.7040
expansions: [(6, 1), (7, 1), (8, 1), (10, 1), (15, 1), (23, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 120.0119 - loglik: -1.1589e+02 - logprior: -4.1200e+00
Epoch 2/2
19/19 - 1s - loss: 112.6925 - loglik: -1.1065e+02 - logprior: -2.0395e+00
Fitted a model with MAP estimate = -111.3501
expansions: [(0, 2)]
discards: [ 0 33 35 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 112.1114 - loglik: -1.0910e+02 - logprior: -3.0109e+00
Epoch 2/2
19/19 - 1s - loss: 109.3914 - loglik: -1.0818e+02 - logprior: -1.2152e+00
Fitted a model with MAP estimate = -108.9835
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 113.6102 - loglik: -1.0985e+02 - logprior: -3.7609e+00
Epoch 2/10
19/19 - 1s - loss: 109.8990 - loglik: -1.0848e+02 - logprior: -1.4239e+00
Epoch 3/10
19/19 - 1s - loss: 109.5145 - loglik: -1.0827e+02 - logprior: -1.2451e+00
Epoch 4/10
19/19 - 1s - loss: 109.4942 - loglik: -1.0829e+02 - logprior: -1.2060e+00
Epoch 5/10
19/19 - 1s - loss: 109.3131 - loglik: -1.0813e+02 - logprior: -1.1832e+00
Epoch 6/10
19/19 - 1s - loss: 109.3320 - loglik: -1.0816e+02 - logprior: -1.1718e+00
Fitted a model with MAP estimate = -109.2268
Time for alignment: 35.7865
Computed alignments with likelihoods: ['-108.9915', '-108.9771', '-108.9835']
Best model has likelihood: -108.9771
SP score = 0.6576
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9691164d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95467e5d30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 360.2233 - loglik: -3.5722e+02 - logprior: -3.0044e+00
Epoch 2/10
19/19 - 3s - loss: 302.1848 - loglik: -3.0098e+02 - logprior: -1.2015e+00
Epoch 3/10
19/19 - 3s - loss: 276.6215 - loglik: -2.7528e+02 - logprior: -1.3444e+00
Epoch 4/10
19/19 - 3s - loss: 271.0439 - loglik: -2.6971e+02 - logprior: -1.3337e+00
Epoch 5/10
19/19 - 3s - loss: 269.6352 - loglik: -2.6837e+02 - logprior: -1.2605e+00
Epoch 6/10
19/19 - 3s - loss: 268.8625 - loglik: -2.6766e+02 - logprior: -1.2073e+00
Epoch 7/10
19/19 - 3s - loss: 268.1865 - loglik: -2.6700e+02 - logprior: -1.1856e+00
Epoch 8/10
19/19 - 3s - loss: 267.7195 - loglik: -2.6654e+02 - logprior: -1.1835e+00
Epoch 9/10
19/19 - 3s - loss: 267.6683 - loglik: -2.6648e+02 - logprior: -1.1883e+00
Epoch 10/10
19/19 - 3s - loss: 268.3445 - loglik: -2.6717e+02 - logprior: -1.1793e+00
Fitted a model with MAP estimate = -267.4466
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 266.5052 - loglik: -2.6262e+02 - logprior: -3.8895e+00
Epoch 2/2
19/19 - 3s - loss: 258.1215 - loglik: -2.5623e+02 - logprior: -1.8944e+00
Fitted a model with MAP estimate = -256.6478
expansions: [(0, 2)]
discards: [ 0 37 75]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 257.6305 - loglik: -2.5492e+02 - logprior: -2.7093e+00
Epoch 2/2
19/19 - 3s - loss: 255.5136 - loglik: -2.5452e+02 - logprior: -9.9130e-01
Fitted a model with MAP estimate = -254.8502
expansions: []
discards: [ 16 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 257.6379 - loglik: -2.5494e+02 - logprior: -2.7018e+00
Epoch 2/10
19/19 - 3s - loss: 255.8416 - loglik: -2.5491e+02 - logprior: -9.3111e-01
Epoch 3/10
19/19 - 3s - loss: 255.1446 - loglik: -2.5438e+02 - logprior: -7.6070e-01
Epoch 4/10
19/19 - 3s - loss: 255.1532 - loglik: -2.5444e+02 - logprior: -7.0905e-01
Fitted a model with MAP estimate = -254.9061
Time for alignment: 87.5555
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 360.1457 - loglik: -3.5714e+02 - logprior: -3.0066e+00
Epoch 2/10
19/19 - 3s - loss: 304.7838 - loglik: -3.0361e+02 - logprior: -1.1695e+00
Epoch 3/10
19/19 - 3s - loss: 277.9792 - loglik: -2.7667e+02 - logprior: -1.3115e+00
Epoch 4/10
19/19 - 3s - loss: 271.9872 - loglik: -2.7068e+02 - logprior: -1.3086e+00
Epoch 5/10
19/19 - 3s - loss: 270.3336 - loglik: -2.6907e+02 - logprior: -1.2606e+00
Epoch 6/10
19/19 - 3s - loss: 268.4146 - loglik: -2.6718e+02 - logprior: -1.2317e+00
Epoch 7/10
19/19 - 3s - loss: 267.7545 - loglik: -2.6654e+02 - logprior: -1.2144e+00
Epoch 8/10
19/19 - 3s - loss: 268.2391 - loglik: -2.6703e+02 - logprior: -1.2076e+00
Fitted a model with MAP estimate = -267.3730
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 2), (93, 1), (96, 1), (101, 1), (107, 1), (109, 1), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 267.3459 - loglik: -2.6343e+02 - logprior: -3.9125e+00
Epoch 2/2
19/19 - 3s - loss: 258.5366 - loglik: -2.5663e+02 - logprior: -1.9085e+00
Fitted a model with MAP estimate = -256.9972
expansions: [(0, 3), (140, 1)]
discards: [ 0 37 75 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 258.4673 - loglik: -2.5574e+02 - logprior: -2.7250e+00
Epoch 2/2
19/19 - 3s - loss: 255.3074 - loglik: -2.5429e+02 - logprior: -1.0127e+00
Fitted a model with MAP estimate = -254.8141
expansions: []
discards: [  0   2  18 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 260.4900 - loglik: -2.5676e+02 - logprior: -3.7318e+00
Epoch 2/10
19/19 - 3s - loss: 257.0531 - loglik: -2.5564e+02 - logprior: -1.4159e+00
Epoch 3/10
19/19 - 3s - loss: 256.2069 - loglik: -2.5552e+02 - logprior: -6.8659e-01
Epoch 4/10
19/19 - 3s - loss: 255.8720 - loglik: -2.5527e+02 - logprior: -6.0212e-01
Epoch 5/10
19/19 - 3s - loss: 254.7329 - loglik: -2.5416e+02 - logprior: -5.6975e-01
Epoch 6/10
19/19 - 3s - loss: 255.2879 - loglik: -2.5480e+02 - logprior: -4.8876e-01
Fitted a model with MAP estimate = -254.9711
Time for alignment: 87.6192
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 359.8611 - loglik: -3.5685e+02 - logprior: -3.0120e+00
Epoch 2/10
19/19 - 3s - loss: 302.3149 - loglik: -3.0114e+02 - logprior: -1.1780e+00
Epoch 3/10
19/19 - 3s - loss: 277.6702 - loglik: -2.7635e+02 - logprior: -1.3204e+00
Epoch 4/10
19/19 - 3s - loss: 272.4787 - loglik: -2.7118e+02 - logprior: -1.2977e+00
Epoch 5/10
19/19 - 3s - loss: 270.3802 - loglik: -2.6913e+02 - logprior: -1.2531e+00
Epoch 6/10
19/19 - 3s - loss: 269.3746 - loglik: -2.6815e+02 - logprior: -1.2254e+00
Epoch 7/10
19/19 - 3s - loss: 268.9146 - loglik: -2.6770e+02 - logprior: -1.2104e+00
Epoch 8/10
19/19 - 3s - loss: 268.6274 - loglik: -2.6743e+02 - logprior: -1.1930e+00
Epoch 9/10
19/19 - 3s - loss: 268.8776 - loglik: -2.6768e+02 - logprior: -1.1950e+00
Fitted a model with MAP estimate = -268.3348
expansions: [(12, 2), (14, 4), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (94, 1), (97, 1), (101, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 267.5962 - loglik: -2.6366e+02 - logprior: -3.9343e+00
Epoch 2/2
19/19 - 3s - loss: 258.2275 - loglik: -2.5628e+02 - logprior: -1.9482e+00
Fitted a model with MAP estimate = -257.2137
expansions: [(0, 2)]
discards: [ 0 12 17 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 258.6591 - loglik: -2.5596e+02 - logprior: -2.7010e+00
Epoch 2/2
19/19 - 3s - loss: 256.2390 - loglik: -2.5524e+02 - logprior: -9.9413e-01
Fitted a model with MAP estimate = -255.7661
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 260.3124 - loglik: -2.5658e+02 - logprior: -3.7312e+00
Epoch 2/10
19/19 - 3s - loss: 257.5111 - loglik: -2.5612e+02 - logprior: -1.3863e+00
Epoch 3/10
19/19 - 3s - loss: 256.1205 - loglik: -2.5541e+02 - logprior: -7.1273e-01
Epoch 4/10
19/19 - 3s - loss: 255.8441 - loglik: -2.5520e+02 - logprior: -6.4220e-01
Epoch 5/10
19/19 - 3s - loss: 255.7165 - loglik: -2.5511e+02 - logprior: -6.0868e-01
Epoch 6/10
19/19 - 3s - loss: 255.8793 - loglik: -2.5531e+02 - logprior: -5.6534e-01
Fitted a model with MAP estimate = -255.5043
Time for alignment: 90.4059
Computed alignments with likelihoods: ['-254.8502', '-254.8141', '-255.5043']
Best model has likelihood: -254.8141
SP score = 0.8437
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f962faa7910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649e7c820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.5822 - loglik: -2.7151e+02 - logprior: -3.0713e+00
Epoch 2/10
19/19 - 2s - loss: 236.7175 - loglik: -2.3547e+02 - logprior: -1.2467e+00
Epoch 3/10
19/19 - 2s - loss: 222.1440 - loglik: -2.2064e+02 - logprior: -1.5075e+00
Epoch 4/10
19/19 - 2s - loss: 219.1568 - loglik: -2.1761e+02 - logprior: -1.5490e+00
Epoch 5/10
19/19 - 2s - loss: 218.0804 - loglik: -2.1662e+02 - logprior: -1.4620e+00
Epoch 6/10
19/19 - 2s - loss: 217.4921 - loglik: -2.1604e+02 - logprior: -1.4479e+00
Epoch 7/10
19/19 - 2s - loss: 217.5200 - loglik: -2.1609e+02 - logprior: -1.4344e+00
Fitted a model with MAP estimate = -216.8662
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (19, 1), (28, 1), (29, 1), (32, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 216.4455 - loglik: -2.1250e+02 - logprior: -3.9458e+00
Epoch 2/2
19/19 - 2s - loss: 207.4794 - loglik: -2.0539e+02 - logprior: -2.0849e+00
Fitted a model with MAP estimate = -206.0474
expansions: [(0, 2)]
discards: [ 0 22 73 77 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5496 - loglik: -2.0464e+02 - logprior: -2.9120e+00
Epoch 2/2
19/19 - 2s - loss: 205.0641 - loglik: -2.0394e+02 - logprior: -1.1241e+00
Fitted a model with MAP estimate = -204.4184
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.5463 - loglik: -2.0572e+02 - logprior: -3.8246e+00
Epoch 2/10
19/19 - 2s - loss: 207.1509 - loglik: -2.0532e+02 - logprior: -1.8358e+00
Epoch 3/10
19/19 - 2s - loss: 205.8176 - loglik: -2.0473e+02 - logprior: -1.0911e+00
Epoch 4/10
19/19 - 2s - loss: 205.1581 - loglik: -2.0433e+02 - logprior: -8.2546e-01
Epoch 5/10
19/19 - 2s - loss: 205.0366 - loglik: -2.0421e+02 - logprior: -8.2418e-01
Epoch 6/10
19/19 - 2s - loss: 204.9854 - loglik: -2.0420e+02 - logprior: -7.8782e-01
Epoch 7/10
19/19 - 2s - loss: 204.8161 - loglik: -2.0404e+02 - logprior: -7.8071e-01
Epoch 8/10
19/19 - 2s - loss: 204.9605 - loglik: -2.0421e+02 - logprior: -7.5024e-01
Fitted a model with MAP estimate = -204.7988
Time for alignment: 69.1861
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.6600 - loglik: -2.7159e+02 - logprior: -3.0741e+00
Epoch 2/10
19/19 - 2s - loss: 236.2281 - loglik: -2.3494e+02 - logprior: -1.2859e+00
Epoch 3/10
19/19 - 2s - loss: 221.3230 - loglik: -2.1990e+02 - logprior: -1.4251e+00
Epoch 4/10
19/19 - 2s - loss: 217.7390 - loglik: -2.1631e+02 - logprior: -1.4287e+00
Epoch 5/10
19/19 - 2s - loss: 216.5027 - loglik: -2.1513e+02 - logprior: -1.3757e+00
Epoch 6/10
19/19 - 2s - loss: 215.9975 - loglik: -2.1464e+02 - logprior: -1.3558e+00
Epoch 7/10
19/19 - 2s - loss: 216.0497 - loglik: -2.1472e+02 - logprior: -1.3306e+00
Fitted a model with MAP estimate = -215.4205
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 214.8775 - loglik: -2.1107e+02 - logprior: -3.8052e+00
Epoch 2/2
19/19 - 2s - loss: 205.9909 - loglik: -2.0468e+02 - logprior: -1.3063e+00
Fitted a model with MAP estimate = -204.7175
expansions: []
discards: [ 0 74 78 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.3048 - loglik: -2.0532e+02 - logprior: -3.9884e+00
Epoch 2/2
19/19 - 2s - loss: 205.8963 - loglik: -2.0444e+02 - logprior: -1.4594e+00
Fitted a model with MAP estimate = -204.8068
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 206.4771 - loglik: -2.0362e+02 - logprior: -2.8541e+00
Epoch 2/10
19/19 - 2s - loss: 204.6172 - loglik: -2.0356e+02 - logprior: -1.0592e+00
Epoch 3/10
19/19 - 2s - loss: 204.3643 - loglik: -2.0335e+02 - logprior: -1.0133e+00
Epoch 4/10
19/19 - 2s - loss: 203.9080 - loglik: -2.0295e+02 - logprior: -9.5817e-01
Epoch 5/10
19/19 - 2s - loss: 204.2083 - loglik: -2.0329e+02 - logprior: -9.1629e-01
Fitted a model with MAP estimate = -203.9929
Time for alignment: 61.7208
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.7432 - loglik: -2.7168e+02 - logprior: -3.0680e+00
Epoch 2/10
19/19 - 2s - loss: 237.4102 - loglik: -2.3615e+02 - logprior: -1.2553e+00
Epoch 3/10
19/19 - 2s - loss: 222.2806 - loglik: -2.2084e+02 - logprior: -1.4386e+00
Epoch 4/10
19/19 - 2s - loss: 217.8098 - loglik: -2.1630e+02 - logprior: -1.5078e+00
Epoch 5/10
19/19 - 2s - loss: 216.7046 - loglik: -2.1534e+02 - logprior: -1.3655e+00
Epoch 6/10
19/19 - 2s - loss: 216.0917 - loglik: -2.1474e+02 - logprior: -1.3564e+00
Epoch 7/10
19/19 - 2s - loss: 216.1512 - loglik: -2.1481e+02 - logprior: -1.3423e+00
Fitted a model with MAP estimate = -215.4967
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 215.7357 - loglik: -2.1194e+02 - logprior: -3.7987e+00
Epoch 2/2
19/19 - 2s - loss: 205.9887 - loglik: -2.0468e+02 - logprior: -1.3077e+00
Fitted a model with MAP estimate = -204.7335
expansions: []
discards: [ 0 74 79 82 84 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.9765 - loglik: -2.0601e+02 - logprior: -3.9651e+00
Epoch 2/2
19/19 - 2s - loss: 206.2120 - loglik: -2.0479e+02 - logprior: -1.4237e+00
Fitted a model with MAP estimate = -205.1197
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 208.0737 - loglik: -2.0500e+02 - logprior: -3.0743e+00
Epoch 2/10
19/19 - 2s - loss: 205.5225 - loglik: -2.0453e+02 - logprior: -9.8797e-01
Epoch 3/10
19/19 - 2s - loss: 205.2904 - loglik: -2.0442e+02 - logprior: -8.6928e-01
Epoch 4/10
19/19 - 2s - loss: 205.0170 - loglik: -2.0417e+02 - logprior: -8.4941e-01
Epoch 5/10
19/19 - 2s - loss: 205.2696 - loglik: -2.0447e+02 - logprior: -7.9655e-01
Fitted a model with MAP estimate = -204.9443
Time for alignment: 60.8309
Computed alignments with likelihoods: ['-204.4184', '-203.9929', '-204.7335']
Best model has likelihood: -203.9929
SP score = 0.6674
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961f7a6f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649cefca0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.5203 - loglik: -1.7330e+02 - logprior: -3.2179e+00
Epoch 2/10
19/19 - 1s - loss: 134.0947 - loglik: -1.3266e+02 - logprior: -1.4390e+00
Epoch 3/10
19/19 - 1s - loss: 117.7749 - loglik: -1.1637e+02 - logprior: -1.4019e+00
Epoch 4/10
19/19 - 1s - loss: 115.1243 - loglik: -1.1369e+02 - logprior: -1.4361e+00
Epoch 5/10
19/19 - 1s - loss: 114.0737 - loglik: -1.1269e+02 - logprior: -1.3830e+00
Epoch 6/10
19/19 - 1s - loss: 113.8905 - loglik: -1.1253e+02 - logprior: -1.3568e+00
Epoch 7/10
19/19 - 1s - loss: 113.7460 - loglik: -1.1240e+02 - logprior: -1.3483e+00
Epoch 8/10
19/19 - 1s - loss: 113.3758 - loglik: -1.1204e+02 - logprior: -1.3368e+00
Epoch 9/10
19/19 - 1s - loss: 113.3268 - loglik: -1.1199e+02 - logprior: -1.3364e+00
Epoch 10/10
19/19 - 1s - loss: 113.7957 - loglik: -1.1247e+02 - logprior: -1.3276e+00
Fitted a model with MAP estimate = -113.2996
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 111.7601 - loglik: -1.0755e+02 - logprior: -4.2099e+00
Epoch 2/2
19/19 - 1s - loss: 103.1407 - loglik: -1.0187e+02 - logprior: -1.2672e+00
Fitted a model with MAP estimate = -102.3325
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.0290 - loglik: -1.0294e+02 - logprior: -4.0887e+00
Epoch 2/2
19/19 - 1s - loss: 103.2166 - loglik: -1.0170e+02 - logprior: -1.5154e+00
Fitted a model with MAP estimate = -102.8172
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 105.0317 - loglik: -1.0181e+02 - logprior: -3.2251e+00
Epoch 2/10
19/19 - 1s - loss: 102.9964 - loglik: -1.0159e+02 - logprior: -1.4022e+00
Epoch 3/10
19/19 - 1s - loss: 102.5912 - loglik: -1.0130e+02 - logprior: -1.2940e+00
Epoch 4/10
19/19 - 1s - loss: 102.6081 - loglik: -1.0138e+02 - logprior: -1.2294e+00
Fitted a model with MAP estimate = -102.4627
Time for alignment: 42.4164
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.5072 - loglik: -1.7329e+02 - logprior: -3.2159e+00
Epoch 2/10
19/19 - 1s - loss: 132.9927 - loglik: -1.3154e+02 - logprior: -1.4538e+00
Epoch 3/10
19/19 - 1s - loss: 116.6343 - loglik: -1.1521e+02 - logprior: -1.4275e+00
Epoch 4/10
19/19 - 1s - loss: 114.4119 - loglik: -1.1299e+02 - logprior: -1.4247e+00
Epoch 5/10
19/19 - 1s - loss: 113.5276 - loglik: -1.1214e+02 - logprior: -1.3872e+00
Epoch 6/10
19/19 - 1s - loss: 113.2851 - loglik: -1.1192e+02 - logprior: -1.3649e+00
Epoch 7/10
19/19 - 1s - loss: 113.0128 - loglik: -1.1166e+02 - logprior: -1.3533e+00
Epoch 8/10
19/19 - 1s - loss: 112.8081 - loglik: -1.1147e+02 - logprior: -1.3425e+00
Epoch 9/10
19/19 - 1s - loss: 113.2469 - loglik: -1.1191e+02 - logprior: -1.3390e+00
Fitted a model with MAP estimate = -112.7964
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (34, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 111.4636 - loglik: -1.0728e+02 - logprior: -4.1873e+00
Epoch 2/2
19/19 - 1s - loss: 103.1921 - loglik: -1.0192e+02 - logprior: -1.2729e+00
Fitted a model with MAP estimate = -102.3103
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.9436 - loglik: -1.0284e+02 - logprior: -4.1001e+00
Epoch 2/2
19/19 - 1s - loss: 103.3788 - loglik: -1.0186e+02 - logprior: -1.5196e+00
Fitted a model with MAP estimate = -102.8415
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 105.1124 - loglik: -1.0189e+02 - logprior: -3.2197e+00
Epoch 2/10
19/19 - 1s - loss: 102.9300 - loglik: -1.0153e+02 - logprior: -1.4016e+00
Epoch 3/10
19/19 - 1s - loss: 102.7938 - loglik: -1.0150e+02 - logprior: -1.2972e+00
Epoch 4/10
19/19 - 1s - loss: 102.3753 - loglik: -1.0114e+02 - logprior: -1.2401e+00
Epoch 5/10
19/19 - 1s - loss: 102.3948 - loglik: -1.0120e+02 - logprior: -1.1945e+00
Fitted a model with MAP estimate = -102.4185
Time for alignment: 41.8850
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.3022 - loglik: -1.7309e+02 - logprior: -3.2162e+00
Epoch 2/10
19/19 - 1s - loss: 133.0149 - loglik: -1.3157e+02 - logprior: -1.4495e+00
Epoch 3/10
19/19 - 1s - loss: 117.2913 - loglik: -1.1587e+02 - logprior: -1.4234e+00
Epoch 4/10
19/19 - 1s - loss: 114.8452 - loglik: -1.1342e+02 - logprior: -1.4252e+00
Epoch 5/10
19/19 - 1s - loss: 114.0373 - loglik: -1.1265e+02 - logprior: -1.3868e+00
Epoch 6/10
19/19 - 1s - loss: 113.6460 - loglik: -1.1229e+02 - logprior: -1.3595e+00
Epoch 7/10
19/19 - 1s - loss: 113.8346 - loglik: -1.1248e+02 - logprior: -1.3509e+00
Fitted a model with MAP estimate = -113.3247
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 111.6409 - loglik: -1.0754e+02 - logprior: -4.1051e+00
Epoch 2/2
19/19 - 1s - loss: 103.1203 - loglik: -1.0183e+02 - logprior: -1.2869e+00
Fitted a model with MAP estimate = -102.3359
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.8964 - loglik: -1.0280e+02 - logprior: -4.0997e+00
Epoch 2/2
19/19 - 1s - loss: 103.3388 - loglik: -1.0181e+02 - logprior: -1.5308e+00
Fitted a model with MAP estimate = -102.8737
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 105.1447 - loglik: -1.0192e+02 - logprior: -3.2217e+00
Epoch 2/10
19/19 - 1s - loss: 102.8231 - loglik: -1.0142e+02 - logprior: -1.4027e+00
Epoch 3/10
19/19 - 1s - loss: 102.8605 - loglik: -1.0156e+02 - logprior: -1.2984e+00
Fitted a model with MAP estimate = -102.4733
Time for alignment: 37.0241
Computed alignments with likelihoods: ['-102.3325', '-102.3103', '-102.3359']
Best model has likelihood: -102.3103
SP score = 0.7833
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9649c60550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f960dfcd190>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 617.9141 - loglik: -6.1534e+02 - logprior: -2.5768e+00
Epoch 2/10
19/19 - 10s - loss: 522.3565 - loglik: -5.2116e+02 - logprior: -1.1972e+00
Epoch 3/10
19/19 - 10s - loss: 484.9969 - loglik: -4.8370e+02 - logprior: -1.2927e+00
Epoch 4/10
19/19 - 10s - loss: 475.8208 - loglik: -4.7464e+02 - logprior: -1.1777e+00
Epoch 5/10
19/19 - 10s - loss: 474.5014 - loglik: -4.7337e+02 - logprior: -1.1268e+00
Epoch 6/10
19/19 - 10s - loss: 470.9152 - loglik: -4.6983e+02 - logprior: -1.0901e+00
Epoch 7/10
19/19 - 10s - loss: 473.4745 - loglik: -4.7240e+02 - logprior: -1.0709e+00
Fitted a model with MAP estimate = -470.5539
expansions: [(20, 1), (34, 1), (57, 1), (58, 1), (61, 1), (75, 2), (81, 1), (82, 1), (101, 1), (102, 1), (105, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 471.9500 - loglik: -4.6930e+02 - logprior: -2.6495e+00
Epoch 2/2
19/19 - 11s - loss: 465.6155 - loglik: -4.6451e+02 - logprior: -1.1066e+00
Fitted a model with MAP estimate = -463.2489
expansions: []
discards: [79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 467.8431 - loglik: -4.6498e+02 - logprior: -2.8616e+00
Epoch 2/2
19/19 - 11s - loss: 465.1418 - loglik: -4.6416e+02 - logprior: -9.8156e-01
Fitted a model with MAP estimate = -463.2228
expansions: [(2, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 465.2763 - loglik: -4.6251e+02 - logprior: -2.7690e+00
Epoch 2/10
19/19 - 11s - loss: 462.8924 - loglik: -4.6198e+02 - logprior: -9.1226e-01
Epoch 3/10
19/19 - 11s - loss: 462.4621 - loglik: -4.6169e+02 - logprior: -7.7496e-01
Epoch 4/10
19/19 - 11s - loss: 461.8481 - loglik: -4.6114e+02 - logprior: -7.0516e-01
Epoch 5/10
19/19 - 11s - loss: 461.5004 - loglik: -4.6085e+02 - logprior: -6.5115e-01
Epoch 6/10
19/19 - 11s - loss: 463.1129 - loglik: -4.6251e+02 - logprior: -6.0427e-01
Fitted a model with MAP estimate = -461.9336
Time for alignment: 247.5961
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 617.7085 - loglik: -6.1514e+02 - logprior: -2.5731e+00
Epoch 2/10
19/19 - 10s - loss: 524.0508 - loglik: -5.2296e+02 - logprior: -1.0952e+00
Epoch 3/10
19/19 - 10s - loss: 489.7903 - loglik: -4.8860e+02 - logprior: -1.1870e+00
Epoch 4/10
19/19 - 10s - loss: 483.2061 - loglik: -4.8210e+02 - logprior: -1.1063e+00
Epoch 5/10
19/19 - 10s - loss: 478.3749 - loglik: -4.7730e+02 - logprior: -1.0769e+00
Epoch 6/10
19/19 - 10s - loss: 477.4184 - loglik: -4.7639e+02 - logprior: -1.0322e+00
Epoch 7/10
19/19 - 10s - loss: 476.5978 - loglik: -4.7559e+02 - logprior: -1.0036e+00
Epoch 8/10
19/19 - 10s - loss: 475.9324 - loglik: -4.7494e+02 - logprior: -9.9422e-01
Epoch 9/10
19/19 - 10s - loss: 476.0868 - loglik: -4.7509e+02 - logprior: -9.9939e-01
Fitted a model with MAP estimate = -473.6749
expansions: [(25, 1), (30, 3), (56, 1), (57, 1), (80, 5), (101, 1), (105, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 472.7137 - loglik: -4.6989e+02 - logprior: -2.8280e+00
Epoch 2/2
19/19 - 11s - loss: 466.3372 - loglik: -4.6529e+02 - logprior: -1.0461e+00
Fitted a model with MAP estimate = -463.9903
expansions: [(92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 470.2628 - loglik: -4.6648e+02 - logprior: -3.7820e+00
Epoch 2/2
19/19 - 11s - loss: 467.4228 - loglik: -4.6554e+02 - logprior: -1.8782e+00
Fitted a model with MAP estimate = -465.2389
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 466.2442 - loglik: -4.6368e+02 - logprior: -2.5620e+00
Epoch 2/10
19/19 - 11s - loss: 463.6181 - loglik: -4.6276e+02 - logprior: -8.6252e-01
Epoch 3/10
19/19 - 11s - loss: 462.9868 - loglik: -4.6214e+02 - logprior: -8.4763e-01
Epoch 4/10
19/19 - 11s - loss: 462.6291 - loglik: -4.6194e+02 - logprior: -6.8907e-01
Epoch 5/10
19/19 - 11s - loss: 462.8031 - loglik: -4.6214e+02 - logprior: -6.6320e-01
Fitted a model with MAP estimate = -462.4991
Time for alignment: 259.7191
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 616.9958 - loglik: -6.1442e+02 - logprior: -2.5762e+00
Epoch 2/10
19/19 - 10s - loss: 522.0906 - loglik: -5.2088e+02 - logprior: -1.2092e+00
Epoch 3/10
19/19 - 10s - loss: 483.3650 - loglik: -4.8203e+02 - logprior: -1.3340e+00
Epoch 4/10
19/19 - 10s - loss: 474.9954 - loglik: -4.7378e+02 - logprior: -1.2151e+00
Epoch 5/10
19/19 - 10s - loss: 471.6991 - loglik: -4.7052e+02 - logprior: -1.1783e+00
Epoch 6/10
19/19 - 10s - loss: 471.5005 - loglik: -4.7037e+02 - logprior: -1.1287e+00
Epoch 7/10
19/19 - 10s - loss: 469.7944 - loglik: -4.6869e+02 - logprior: -1.1065e+00
Epoch 8/10
19/19 - 10s - loss: 470.6679 - loglik: -4.6958e+02 - logprior: -1.0866e+00
Fitted a model with MAP estimate = -468.4871
expansions: [(25, 1), (30, 4), (56, 1), (57, 1), (75, 1), (83, 1), (101, 1), (102, 1), (105, 1), (106, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 470.1614 - loglik: -4.6753e+02 - logprior: -2.6321e+00
Epoch 2/2
19/19 - 11s - loss: 463.6882 - loglik: -4.6267e+02 - logprior: -1.0146e+00
Fitted a model with MAP estimate = -461.5920
expansions: []
discards: [32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 465.4461 - loglik: -4.6264e+02 - logprior: -2.8050e+00
Epoch 2/2
19/19 - 11s - loss: 462.2728 - loglik: -4.6133e+02 - logprior: -9.4600e-01
Fitted a model with MAP estimate = -461.0973
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 465.8449 - loglik: -4.6222e+02 - logprior: -3.6239e+00
Epoch 2/10
19/19 - 11s - loss: 463.3420 - loglik: -4.6143e+02 - logprior: -1.9103e+00
Epoch 3/10
19/19 - 11s - loss: 463.8370 - loglik: -4.6218e+02 - logprior: -1.6610e+00
Fitted a model with MAP estimate = -462.3125
Time for alignment: 226.5604
Computed alignments with likelihoods: ['-461.9336', '-462.4991', '-461.0973']
Best model has likelihood: -461.0973
SP score = 0.7454
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961eae5880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95f4725a00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.8059 - loglik: -1.3959e+02 - logprior: -3.2141e+00
Epoch 2/10
19/19 - 1s - loss: 113.5391 - loglik: -1.1211e+02 - logprior: -1.4284e+00
Epoch 3/10
19/19 - 1s - loss: 104.0596 - loglik: -1.0247e+02 - logprior: -1.5893e+00
Epoch 4/10
19/19 - 1s - loss: 102.4422 - loglik: -1.0100e+02 - logprior: -1.4410e+00
Epoch 5/10
19/19 - 1s - loss: 101.9820 - loglik: -1.0055e+02 - logprior: -1.4303e+00
Epoch 6/10
19/19 - 1s - loss: 101.6655 - loglik: -1.0025e+02 - logprior: -1.4159e+00
Epoch 7/10
19/19 - 1s - loss: 101.6644 - loglik: -1.0026e+02 - logprior: -1.4025e+00
Epoch 8/10
19/19 - 1s - loss: 101.3819 - loglik: -9.9989e+01 - logprior: -1.3932e+00
Epoch 9/10
19/19 - 1s - loss: 101.3013 - loglik: -9.9914e+01 - logprior: -1.3873e+00
Epoch 10/10
19/19 - 1s - loss: 101.3688 - loglik: -9.9981e+01 - logprior: -1.3875e+00
Fitted a model with MAP estimate = -101.1719
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 2), (29, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.1488 - loglik: -1.0297e+02 - logprior: -4.1770e+00
Epoch 2/2
19/19 - 1s - loss: 99.4458 - loglik: -9.7267e+01 - logprior: -2.1785e+00
Fitted a model with MAP estimate = -97.9527
expansions: [(0, 1)]
discards: [ 0  8 16 26 38 40 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.3146 - loglik: -9.7123e+01 - logprior: -3.1914e+00
Epoch 2/2
19/19 - 1s - loss: 96.7393 - loglik: -9.5280e+01 - logprior: -1.4589e+00
Fitted a model with MAP estimate = -96.4477
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 98.7329 - loglik: -9.5481e+01 - logprior: -3.2523e+00
Epoch 2/10
19/19 - 1s - loss: 96.6050 - loglik: -9.5164e+01 - logprior: -1.4409e+00
Epoch 3/10
19/19 - 1s - loss: 96.2978 - loglik: -9.4950e+01 - logprior: -1.3477e+00
Epoch 4/10
19/19 - 1s - loss: 96.1600 - loglik: -9.4863e+01 - logprior: -1.2968e+00
Epoch 5/10
19/19 - 1s - loss: 96.2972 - loglik: -9.5038e+01 - logprior: -1.2589e+00
Fitted a model with MAP estimate = -96.1556
Time for alignment: 39.2223
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7539 - loglik: -1.3954e+02 - logprior: -3.2154e+00
Epoch 2/10
19/19 - 1s - loss: 114.1975 - loglik: -1.1277e+02 - logprior: -1.4265e+00
Epoch 3/10
19/19 - 1s - loss: 104.0928 - loglik: -1.0251e+02 - logprior: -1.5866e+00
Epoch 4/10
19/19 - 1s - loss: 102.0551 - loglik: -1.0062e+02 - logprior: -1.4399e+00
Epoch 5/10
19/19 - 1s - loss: 101.3518 - loglik: -9.9918e+01 - logprior: -1.4337e+00
Epoch 6/10
19/19 - 1s - loss: 101.3430 - loglik: -9.9924e+01 - logprior: -1.4191e+00
Epoch 7/10
19/19 - 1s - loss: 101.0414 - loglik: -9.9638e+01 - logprior: -1.4034e+00
Epoch 8/10
19/19 - 1s - loss: 101.1948 - loglik: -9.9800e+01 - logprior: -1.3951e+00
Fitted a model with MAP estimate = -100.9663
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 3), (26, 1), (28, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.8722 - loglik: -1.0270e+02 - logprior: -4.1723e+00
Epoch 2/2
19/19 - 1s - loss: 99.3293 - loglik: -9.7172e+01 - logprior: -2.1572e+00
Fitted a model with MAP estimate = -97.8869
expansions: [(0, 1)]
discards: [ 0  7 16 26 29 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.7698 - loglik: -9.6552e+01 - logprior: -3.2177e+00
Epoch 2/2
19/19 - 1s - loss: 96.3333 - loglik: -9.4867e+01 - logprior: -1.4661e+00
Fitted a model with MAP estimate = -96.0560
expansions: []
discards: [35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 98.8754 - loglik: -9.5622e+01 - logprior: -3.2530e+00
Epoch 2/10
19/19 - 1s - loss: 96.5443 - loglik: -9.5106e+01 - logprior: -1.4382e+00
Epoch 3/10
19/19 - 1s - loss: 96.3290 - loglik: -9.4984e+01 - logprior: -1.3454e+00
Epoch 4/10
19/19 - 1s - loss: 96.3664 - loglik: -9.5074e+01 - logprior: -1.2920e+00
Fitted a model with MAP estimate = -96.1943
Time for alignment: 36.1718
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8596 - loglik: -1.3965e+02 - logprior: -3.2129e+00
Epoch 2/10
19/19 - 1s - loss: 114.3692 - loglik: -1.1295e+02 - logprior: -1.4184e+00
Epoch 3/10
19/19 - 1s - loss: 104.4043 - loglik: -1.0282e+02 - logprior: -1.5868e+00
Epoch 4/10
19/19 - 1s - loss: 101.9852 - loglik: -1.0054e+02 - logprior: -1.4463e+00
Epoch 5/10
19/19 - 1s - loss: 101.3786 - loglik: -9.9945e+01 - logprior: -1.4332e+00
Epoch 6/10
19/19 - 1s - loss: 100.9919 - loglik: -9.9572e+01 - logprior: -1.4195e+00
Epoch 7/10
19/19 - 1s - loss: 100.8420 - loglik: -9.9434e+01 - logprior: -1.4081e+00
Epoch 8/10
19/19 - 1s - loss: 100.8193 - loglik: -9.9418e+01 - logprior: -1.4009e+00
Epoch 9/10
19/19 - 1s - loss: 100.6928 - loglik: -9.9297e+01 - logprior: -1.3954e+00
Epoch 10/10
19/19 - 1s - loss: 100.7880 - loglik: -9.9398e+01 - logprior: -1.3904e+00
Fitted a model with MAP estimate = -100.6651
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.8221 - loglik: -1.0265e+02 - logprior: -4.1701e+00
Epoch 2/2
19/19 - 1s - loss: 99.4608 - loglik: -9.7284e+01 - logprior: -2.1766e+00
Fitted a model with MAP estimate = -98.0254
expansions: [(0, 1)]
discards: [ 0  7 16 26 27 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.7005 - loglik: -9.6490e+01 - logprior: -3.2110e+00
Epoch 2/2
19/19 - 1s - loss: 96.4244 - loglik: -9.4956e+01 - logprior: -1.4688e+00
Fitted a model with MAP estimate = -96.0190
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 98.8075 - loglik: -9.5558e+01 - logprior: -3.2491e+00
Epoch 2/10
19/19 - 1s - loss: 96.5534 - loglik: -9.5112e+01 - logprior: -1.4419e+00
Epoch 3/10
19/19 - 1s - loss: 96.3475 - loglik: -9.4995e+01 - logprior: -1.3524e+00
Epoch 4/10
19/19 - 1s - loss: 96.1908 - loglik: -9.4893e+01 - logprior: -1.2976e+00
Epoch 5/10
19/19 - 1s - loss: 96.1634 - loglik: -9.4901e+01 - logprior: -1.2621e+00
Epoch 6/10
19/19 - 1s - loss: 96.3012 - loglik: -9.5058e+01 - logprior: -1.2430e+00
Fitted a model with MAP estimate = -96.1066
Time for alignment: 38.7636
Computed alignments with likelihoods: ['-96.1556', '-96.0560', '-96.0190']
Best model has likelihood: -96.0190
SP score = 0.7708
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961e951eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f962fba42b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 457.9225 - loglik: -4.5506e+02 - logprior: -2.8606e+00
Epoch 2/10
19/19 - 4s - loss: 378.3460 - loglik: -3.7697e+02 - logprior: -1.3769e+00
Epoch 3/10
19/19 - 4s - loss: 346.0824 - loglik: -3.4438e+02 - logprior: -1.7025e+00
Epoch 4/10
19/19 - 4s - loss: 340.4601 - loglik: -3.3869e+02 - logprior: -1.7735e+00
Epoch 5/10
19/19 - 4s - loss: 338.0641 - loglik: -3.3638e+02 - logprior: -1.6887e+00
Epoch 6/10
19/19 - 4s - loss: 337.2141 - loglik: -3.3556e+02 - logprior: -1.6514e+00
Epoch 7/10
19/19 - 4s - loss: 336.9023 - loglik: -3.3529e+02 - logprior: -1.6099e+00
Epoch 8/10
19/19 - 4s - loss: 336.5021 - loglik: -3.3489e+02 - logprior: -1.6107e+00
Epoch 9/10
19/19 - 4s - loss: 335.9113 - loglik: -3.3430e+02 - logprior: -1.6120e+00
Epoch 10/10
19/19 - 4s - loss: 335.6664 - loglik: -3.3404e+02 - logprior: -1.6311e+00
Fitted a model with MAP estimate = -332.6783
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (66, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 327.0347 - loglik: -3.2327e+02 - logprior: -3.7635e+00
Epoch 2/2
19/19 - 6s - loss: 311.5713 - loglik: -3.1043e+02 - logprior: -1.1404e+00
Fitted a model with MAP estimate = -307.1733
expansions: []
discards: [  0  54 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 314.8609 - loglik: -3.1091e+02 - logprior: -3.9489e+00
Epoch 2/2
19/19 - 6s - loss: 311.0444 - loglik: -3.0939e+02 - logprior: -1.6583e+00
Fitted a model with MAP estimate = -307.0976
expansions: [(0, 2)]
discards: [  0 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 309.7027 - loglik: -3.0697e+02 - logprior: -2.7345e+00
Epoch 2/10
19/19 - 6s - loss: 306.6492 - loglik: -3.0572e+02 - logprior: -9.3234e-01
Epoch 3/10
19/19 - 6s - loss: 306.6103 - loglik: -3.0578e+02 - logprior: -8.3160e-01
Epoch 4/10
19/19 - 6s - loss: 306.7020 - loglik: -3.0595e+02 - logprior: -7.5612e-01
Fitted a model with MAP estimate = -306.4520
Time for alignment: 142.8118
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.8863 - loglik: -4.5503e+02 - logprior: -2.8583e+00
Epoch 2/10
19/19 - 4s - loss: 380.0510 - loglik: -3.7870e+02 - logprior: -1.3549e+00
Epoch 3/10
19/19 - 4s - loss: 347.0337 - loglik: -3.4531e+02 - logprior: -1.7202e+00
Epoch 4/10
19/19 - 4s - loss: 341.8944 - loglik: -3.4006e+02 - logprior: -1.8376e+00
Epoch 5/10
19/19 - 4s - loss: 340.4711 - loglik: -3.3872e+02 - logprior: -1.7464e+00
Epoch 6/10
19/19 - 4s - loss: 338.9365 - loglik: -3.3723e+02 - logprior: -1.7021e+00
Epoch 7/10
19/19 - 4s - loss: 338.2363 - loglik: -3.3656e+02 - logprior: -1.6748e+00
Epoch 8/10
19/19 - 4s - loss: 338.2083 - loglik: -3.3654e+02 - logprior: -1.6646e+00
Epoch 9/10
19/19 - 4s - loss: 338.1495 - loglik: -3.3649e+02 - logprior: -1.6586e+00
Epoch 10/10
19/19 - 4s - loss: 337.2853 - loglik: -3.3563e+02 - logprior: -1.6511e+00
Fitted a model with MAP estimate = -334.6493
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (20, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 2), (80, 1), (87, 2), (88, 2), (89, 1), (99, 1), (100, 1), (105, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 329.1596 - loglik: -3.2541e+02 - logprior: -3.7518e+00
Epoch 2/2
19/19 - 6s - loss: 312.7675 - loglik: -3.1165e+02 - logprior: -1.1223e+00
Fitted a model with MAP estimate = -308.2952
expansions: []
discards: [  0  54  93 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 316.0626 - loglik: -3.1215e+02 - logprior: -3.9171e+00
Epoch 2/2
19/19 - 6s - loss: 312.1274 - loglik: -3.1048e+02 - logprior: -1.6524e+00
Fitted a model with MAP estimate = -308.2431
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 309.7422 - loglik: -3.0699e+02 - logprior: -2.7558e+00
Epoch 2/10
19/19 - 6s - loss: 307.3979 - loglik: -3.0648e+02 - logprior: -9.1830e-01
Epoch 3/10
19/19 - 6s - loss: 307.2735 - loglik: -3.0643e+02 - logprior: -8.4063e-01
Epoch 4/10
19/19 - 6s - loss: 307.3898 - loglik: -3.0663e+02 - logprior: -7.5488e-01
Fitted a model with MAP estimate = -306.5817
Time for alignment: 142.6321
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 458.0957 - loglik: -4.5523e+02 - logprior: -2.8616e+00
Epoch 2/10
19/19 - 4s - loss: 381.8227 - loglik: -3.8045e+02 - logprior: -1.3719e+00
Epoch 3/10
19/19 - 4s - loss: 351.4531 - loglik: -3.4972e+02 - logprior: -1.7349e+00
Epoch 4/10
19/19 - 4s - loss: 343.5869 - loglik: -3.4176e+02 - logprior: -1.8264e+00
Epoch 5/10
19/19 - 4s - loss: 341.9130 - loglik: -3.4013e+02 - logprior: -1.7834e+00
Epoch 6/10
19/19 - 4s - loss: 340.1254 - loglik: -3.3839e+02 - logprior: -1.7354e+00
Epoch 7/10
19/19 - 4s - loss: 339.3544 - loglik: -3.3764e+02 - logprior: -1.7105e+00
Epoch 8/10
19/19 - 4s - loss: 339.1298 - loglik: -3.3743e+02 - logprior: -1.7044e+00
Epoch 9/10
19/19 - 4s - loss: 339.8106 - loglik: -3.3812e+02 - logprior: -1.6921e+00
Fitted a model with MAP estimate = -335.8099
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (22, 1), (33, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (66, 1), (69, 1), (70, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (103, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 330.5864 - loglik: -3.2680e+02 - logprior: -3.7912e+00
Epoch 2/2
19/19 - 6s - loss: 314.6183 - loglik: -3.1349e+02 - logprior: -1.1298e+00
Fitted a model with MAP estimate = -309.8278
expansions: []
discards: [  0  54 144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 317.4044 - loglik: -3.1346e+02 - logprior: -3.9450e+00
Epoch 2/2
19/19 - 6s - loss: 313.5708 - loglik: -3.1192e+02 - logprior: -1.6495e+00
Fitted a model with MAP estimate = -309.6506
expansions: [(0, 2)]
discards: [  0 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 311.8167 - loglik: -3.0907e+02 - logprior: -2.7429e+00
Epoch 2/10
19/19 - 6s - loss: 309.0162 - loglik: -3.0810e+02 - logprior: -9.1165e-01
Epoch 3/10
19/19 - 6s - loss: 309.0660 - loglik: -3.0825e+02 - logprior: -8.1742e-01
Fitted a model with MAP estimate = -308.7394
Time for alignment: 131.6517
Computed alignments with likelihoods: ['-306.4520', '-306.5817', '-308.7394']
Best model has likelihood: -306.4520
SP score = 0.2953
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ecbebb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9630379e20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 902.6462 - loglik: -9.0121e+02 - logprior: -1.4344e+00
Epoch 2/10
39/39 - 32s - loss: 723.3853 - loglik: -7.2134e+02 - logprior: -2.0452e+00
Epoch 3/10
39/39 - 32s - loss: 708.1302 - loglik: -7.0613e+02 - logprior: -2.0048e+00
Epoch 4/10
39/39 - 32s - loss: 703.7142 - loglik: -7.0175e+02 - logprior: -1.9620e+00
Epoch 5/10
39/39 - 32s - loss: 700.7385 - loglik: -6.9874e+02 - logprior: -2.0000e+00
Epoch 6/10
39/39 - 32s - loss: 699.5417 - loglik: -6.9752e+02 - logprior: -2.0211e+00
Epoch 7/10
39/39 - 32s - loss: 699.2834 - loglik: -6.9719e+02 - logprior: -2.0889e+00
Epoch 8/10
39/39 - 32s - loss: 698.3754 - loglik: -6.9634e+02 - logprior: -2.0360e+00
Epoch 9/10
39/39 - 32s - loss: 699.4865 - loglik: -6.9744e+02 - logprior: -2.0488e+00
Fitted a model with MAP estimate = -698.0897
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (42, 1), (45, 1), (55, 1), (59, 1), (61, 2), (65, 1), (71, 1), (72, 1), (78, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (102, 1), (111, 1), (112, 1), (117, 1), (121, 2), (124, 2), (140, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (160, 2), (161, 2), (162, 1), (164, 1), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (193, 1), (196, 1), (205, 1), (207, 1), (208, 2), (209, 2), (227, 1), (228, 1), (231, 4), (232, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 665.0356 - loglik: -6.6336e+02 - logprior: -1.6802e+00
Epoch 2/2
39/39 - 47s - loss: 650.8906 - loglik: -6.5024e+02 - logprior: -6.4853e-01
Fitted a model with MAP estimate = -648.7967
expansions: []
discards: [146 151 198 256 288 323]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 651.1198 - loglik: -6.4978e+02 - logprior: -1.3433e+00
Epoch 2/2
39/39 - 46s - loss: 649.8401 - loglik: -6.4965e+02 - logprior: -1.8730e-01
Fitted a model with MAP estimate = -648.1921
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 649.9199 - loglik: -6.4883e+02 - logprior: -1.0856e+00
Epoch 2/10
39/39 - 46s - loss: 648.8259 - loglik: -6.4882e+02 - logprior: -1.3216e-03
Epoch 3/10
39/39 - 46s - loss: 648.2630 - loglik: -6.4856e+02 - logprior: 0.2929
Epoch 4/10
39/39 - 46s - loss: 647.9586 - loglik: -6.4851e+02 - logprior: 0.5466
Epoch 5/10
39/39 - 46s - loss: 648.0230 - loglik: -6.4872e+02 - logprior: 0.6975
Fitted a model with MAP estimate = -647.1375
Time for alignment: 943.0671
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 902.6592 - loglik: -9.0128e+02 - logprior: -1.3785e+00
Epoch 2/10
39/39 - 32s - loss: 722.1221 - loglik: -7.2043e+02 - logprior: -1.6890e+00
Epoch 3/10
39/39 - 32s - loss: 708.6793 - loglik: -7.0699e+02 - logprior: -1.6867e+00
Epoch 4/10
39/39 - 32s - loss: 703.7036 - loglik: -7.0203e+02 - logprior: -1.6724e+00
Epoch 5/10
39/39 - 32s - loss: 700.8936 - loglik: -6.9917e+02 - logprior: -1.7275e+00
Epoch 6/10
39/39 - 32s - loss: 699.7324 - loglik: -6.9797e+02 - logprior: -1.7615e+00
Epoch 7/10
39/39 - 32s - loss: 699.6856 - loglik: -6.9783e+02 - logprior: -1.8566e+00
Epoch 8/10
39/39 - 32s - loss: 699.2144 - loglik: -6.9741e+02 - logprior: -1.8018e+00
Epoch 9/10
39/39 - 32s - loss: 698.6865 - loglik: -6.9694e+02 - logprior: -1.7508e+00
Epoch 10/10
39/39 - 32s - loss: 699.0712 - loglik: -6.9733e+02 - logprior: -1.7431e+00
Fitted a model with MAP estimate = -698.0451
expansions: [(0, 5), (25, 1), (45, 2), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 3), (84, 2), (90, 1), (91, 1), (92, 1), (112, 3), (117, 1), (124, 3), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 2), (168, 1), (186, 1), (188, 1), (189, 1), (190, 1), (193, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (232, 1), (236, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 383 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 667.0419 - loglik: -6.6475e+02 - logprior: -2.2917e+00
Epoch 2/2
39/39 - 47s - loss: 650.6454 - loglik: -6.4976e+02 - logprior: -8.8459e-01
Fitted a model with MAP estimate = -648.2216
expansions: []
discards: [  1   2   3 102 152 347 362 363]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 652.5916 - loglik: -6.5122e+02 - logprior: -1.3685e+00
Epoch 2/2
39/39 - 46s - loss: 650.4914 - loglik: -6.5040e+02 - logprior: -8.8116e-02
Fitted a model with MAP estimate = -649.0296
expansions: [(0, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 379 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 651.8204 - loglik: -6.4990e+02 - logprior: -1.9168e+00
Epoch 2/10
39/39 - 46s - loss: 649.2169 - loglik: -6.4929e+02 - logprior: 0.0704
Epoch 3/10
39/39 - 46s - loss: 648.7504 - loglik: -6.4890e+02 - logprior: 0.1534
Epoch 4/10
39/39 - 46s - loss: 648.5268 - loglik: -6.4886e+02 - logprior: 0.3325
Epoch 5/10
39/39 - 46s - loss: 647.3188 - loglik: -6.4789e+02 - logprior: 0.5703
Epoch 6/10
39/39 - 46s - loss: 647.7214 - loglik: -6.4848e+02 - logprior: 0.7576
Fitted a model with MAP estimate = -647.3871
Time for alignment: 1022.8080
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 902.7403 - loglik: -9.0133e+02 - logprior: -1.4106e+00
Epoch 2/10
39/39 - 32s - loss: 725.1177 - loglik: -7.2338e+02 - logprior: -1.7344e+00
Epoch 3/10
39/39 - 32s - loss: 709.1550 - loglik: -7.0750e+02 - logprior: -1.6546e+00
Epoch 4/10
39/39 - 32s - loss: 704.2197 - loglik: -7.0259e+02 - logprior: -1.6322e+00
Epoch 5/10
39/39 - 32s - loss: 701.5760 - loglik: -6.9993e+02 - logprior: -1.6430e+00
Epoch 6/10
39/39 - 32s - loss: 700.1132 - loglik: -6.9840e+02 - logprior: -1.7146e+00
Epoch 7/10
39/39 - 32s - loss: 699.2968 - loglik: -6.9760e+02 - logprior: -1.6963e+00
Epoch 8/10
39/39 - 32s - loss: 698.8756 - loglik: -6.9715e+02 - logprior: -1.7256e+00
Epoch 9/10
39/39 - 32s - loss: 698.9794 - loglik: -6.9728e+02 - logprior: -1.6959e+00
Fitted a model with MAP estimate = -698.1054
expansions: [(0, 15), (25, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 3), (118, 1), (122, 2), (125, 2), (141, 1), (145, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 667.3531 - loglik: -6.6507e+02 - logprior: -2.2791e+00
Epoch 2/2
39/39 - 49s - loss: 650.6714 - loglik: -6.4992e+02 - logprior: -7.5074e-01
Fitted a model with MAP estimate = -648.5201
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12 158 163 242 298 334]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 650.6963 - loglik: -6.4937e+02 - logprior: -1.3223e+00
Epoch 2/2
39/39 - 46s - loss: 649.6328 - loglik: -6.4929e+02 - logprior: -3.4616e-01
Fitted a model with MAP estimate = -647.9053
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 649.8339 - loglik: -6.4871e+02 - logprior: -1.1277e+00
Epoch 2/10
39/39 - 46s - loss: 648.5589 - loglik: -6.4854e+02 - logprior: -1.5886e-02
Epoch 3/10
39/39 - 46s - loss: 647.3799 - loglik: -6.4762e+02 - logprior: 0.2382
Epoch 4/10
39/39 - 46s - loss: 648.2240 - loglik: -6.4853e+02 - logprior: 0.3046
Fitted a model with MAP estimate = -647.2385
Time for alignment: 904.7178
Computed alignments with likelihoods: ['-647.1375', '-647.3871', '-647.2385']
Best model has likelihood: -647.1375
SP score = 0.9486
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e25482b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e31f4fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 690.4600 - loglik: -6.8786e+02 - logprior: -2.5997e+00
Epoch 2/10
19/19 - 12s - loss: 600.7524 - loglik: -5.9960e+02 - logprior: -1.1535e+00
Epoch 3/10
19/19 - 12s - loss: 557.8641 - loglik: -5.5639e+02 - logprior: -1.4730e+00
Epoch 4/10
19/19 - 12s - loss: 548.4243 - loglik: -5.4677e+02 - logprior: -1.6561e+00
Epoch 5/10
19/19 - 12s - loss: 545.9846 - loglik: -5.4429e+02 - logprior: -1.6986e+00
Epoch 6/10
19/19 - 12s - loss: 545.0532 - loglik: -5.4336e+02 - logprior: -1.6914e+00
Epoch 7/10
19/19 - 12s - loss: 544.4684 - loglik: -5.4278e+02 - logprior: -1.6921e+00
Epoch 8/10
19/19 - 12s - loss: 542.1702 - loglik: -5.4047e+02 - logprior: -1.7011e+00
Epoch 9/10
19/19 - 12s - loss: 543.4946 - loglik: -5.4178e+02 - logprior: -1.7140e+00
Fitted a model with MAP estimate = -542.3720
expansions: [(4, 1), (6, 1), (33, 1), (44, 1), (82, 3), (88, 1), (90, 1), (91, 1), (93, 1), (117, 1), (118, 8), (119, 2), (120, 1), (121, 1), (129, 1), (130, 1), (132, 3), (133, 2), (136, 1), (137, 1), (139, 1), (142, 3), (143, 9), (144, 1), (146, 1), (147, 2), (156, 1), (159, 2), (160, 2)]
discards: [  0 149 150 151 152 161 162 163 164 165 166 167 168 169 175 176 177 178
 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 570.5251 - loglik: -5.6693e+02 - logprior: -3.5902e+00
Epoch 2/2
19/19 - 12s - loss: 557.3495 - loglik: -5.5563e+02 - logprior: -1.7226e+00
Fitted a model with MAP estimate = -555.1366
expansions: [(0, 2), (87, 1), (214, 1), (215, 15), (216, 6)]
discards: [  0 131 132 133 134 160 181 182 183 184 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 555.4766 - loglik: -5.5261e+02 - logprior: -2.8712e+00
Epoch 2/2
19/19 - 13s - loss: 547.4692 - loglik: -5.4645e+02 - logprior: -1.0160e+00
Fitted a model with MAP estimate = -544.1056
expansions: [(193, 17), (197, 1)]
discards: [  0   1   2 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224
 225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 551.7020 - loglik: -5.4831e+02 - logprior: -3.3916e+00
Epoch 2/10
19/19 - 13s - loss: 543.1577 - loglik: -5.4225e+02 - logprior: -9.1014e-01
Epoch 3/10
19/19 - 13s - loss: 540.1268 - loglik: -5.3987e+02 - logprior: -2.5802e-01
Epoch 4/10
19/19 - 13s - loss: 540.6374 - loglik: -5.4053e+02 - logprior: -1.0526e-01
Fitted a model with MAP estimate = -539.4804
Time for alignment: 293.9367
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 690.4837 - loglik: -6.8789e+02 - logprior: -2.5925e+00
Epoch 2/10
19/19 - 12s - loss: 599.3658 - loglik: -5.9824e+02 - logprior: -1.1242e+00
Epoch 3/10
19/19 - 12s - loss: 555.6609 - loglik: -5.5421e+02 - logprior: -1.4493e+00
Epoch 4/10
19/19 - 12s - loss: 547.4962 - loglik: -5.4592e+02 - logprior: -1.5761e+00
Epoch 5/10
19/19 - 12s - loss: 544.0524 - loglik: -5.4245e+02 - logprior: -1.6046e+00
Epoch 6/10
19/19 - 12s - loss: 543.4514 - loglik: -5.4187e+02 - logprior: -1.5858e+00
Epoch 7/10
19/19 - 12s - loss: 541.6511 - loglik: -5.4005e+02 - logprior: -1.5976e+00
Epoch 8/10
19/19 - 12s - loss: 540.9180 - loglik: -5.3929e+02 - logprior: -1.6273e+00
Epoch 9/10
19/19 - 12s - loss: 542.6387 - loglik: -5.4101e+02 - logprior: -1.6253e+00
Fitted a model with MAP estimate = -541.2643
expansions: [(4, 1), (6, 1), (31, 1), (40, 1), (85, 3), (91, 3), (93, 2), (117, 1), (118, 8), (119, 2), (120, 1), (121, 1), (128, 1), (129, 2), (130, 3), (131, 7), (136, 1), (138, 1), (139, 1), (140, 3), (141, 8), (142, 1), (146, 1), (153, 1), (155, 1), (156, 1), (159, 1), (160, 2), (161, 1), (171, 5), (173, 1), (184, 1)]
discards: [  0 162 163 164 165 166 167 168 169 175 176 177 178 179 180 181 186 187
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 562.2101 - loglik: -5.5859e+02 - logprior: -3.6197e+00
Epoch 2/2
19/19 - 14s - loss: 546.0256 - loglik: -5.4436e+02 - logprior: -1.6683e+00
Fitted a model with MAP estimate = -543.9252
expansions: [(0, 1), (214, 1), (238, 13)]
discards: [  0   1 132 133 134 135 157 158 159 163 164 186 187 217 228 229 230]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 548.3112 - loglik: -5.4555e+02 - logprior: -2.7661e+00
Epoch 2/2
19/19 - 13s - loss: 540.5978 - loglik: -5.3961e+02 - logprior: -9.8398e-01
Fitted a model with MAP estimate = -538.5683
expansions: [(0, 2), (85, 3), (206, 2), (216, 1), (219, 15)]
discards: [223 224 225 226 227 228 229 230 231 232 233 234 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 246 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 541.5999 - loglik: -5.3839e+02 - logprior: -3.2075e+00
Epoch 2/10
19/19 - 14s - loss: 533.0768 - loglik: -5.3222e+02 - logprior: -8.5408e-01
Epoch 3/10
19/19 - 14s - loss: 529.8013 - loglik: -5.2931e+02 - logprior: -4.9394e-01
Epoch 4/10
19/19 - 14s - loss: 529.4431 - loglik: -5.2903e+02 - logprior: -4.1279e-01
Epoch 5/10
19/19 - 14s - loss: 529.9805 - loglik: -5.2962e+02 - logprior: -3.5843e-01
Fitted a model with MAP estimate = -528.9626
Time for alignment: 324.3015
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 689.7866 - loglik: -6.8719e+02 - logprior: -2.5981e+00
Epoch 2/10
19/19 - 12s - loss: 601.5350 - loglik: -6.0040e+02 - logprior: -1.1311e+00
Epoch 3/10
19/19 - 11s - loss: 559.2891 - loglik: -5.5786e+02 - logprior: -1.4314e+00
Epoch 4/10
19/19 - 12s - loss: 550.2910 - loglik: -5.4872e+02 - logprior: -1.5682e+00
Epoch 5/10
19/19 - 11s - loss: 548.0540 - loglik: -5.4645e+02 - logprior: -1.6058e+00
Epoch 6/10
19/19 - 12s - loss: 546.3387 - loglik: -5.4474e+02 - logprior: -1.6002e+00
Epoch 7/10
19/19 - 12s - loss: 544.8073 - loglik: -5.4319e+02 - logprior: -1.6146e+00
Epoch 8/10
19/19 - 11s - loss: 544.8026 - loglik: -5.4316e+02 - logprior: -1.6416e+00
Epoch 9/10
19/19 - 12s - loss: 545.3812 - loglik: -5.4372e+02 - logprior: -1.6616e+00
Fitted a model with MAP estimate = -544.0339
expansions: [(4, 1), (6, 1), (33, 1), (40, 1), (83, 6), (86, 1), (90, 1), (91, 1), (93, 1), (117, 1), (118, 7), (119, 2), (120, 1), (121, 2), (128, 1), (129, 1), (131, 3), (132, 2), (135, 1), (136, 1), (138, 1), (141, 3), (142, 9), (143, 2), (145, 2), (147, 1), (157, 1), (158, 4), (159, 1), (172, 1), (185, 1)]
discards: [  0 151 152 153 154 155 161 162 163 164 165 166 167 168 169 170 174 175
 176 177 178 179 180 186 187 188 189 190 191 192 193 194 195 196 197 198
 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 570.4333 - loglik: -5.6680e+02 - logprior: -3.6288e+00
Epoch 2/2
19/19 - 13s - loss: 555.3853 - loglik: -5.5362e+02 - logprior: -1.7610e+00
Fitted a model with MAP estimate = -551.8399
expansions: [(0, 1), (204, 5), (217, 1), (220, 13), (224, 7), (225, 8)]
discards: [  0   1  80  81  82  87 134 135 136 161 183 184 185 186 195]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 552.3724 - loglik: -5.4964e+02 - logprior: -2.7331e+00
Epoch 2/2
19/19 - 14s - loss: 539.4623 - loglik: -5.3847e+02 - logprior: -9.9478e-01
Fitted a model with MAP estimate = -536.2729
expansions: [(0, 2), (82, 2), (200, 3), (211, 7), (215, 2), (216, 1)]
discards: [ 79 178 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235
 236 237 238 239 240 241 242 243 244]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 546.8328 - loglik: -5.4363e+02 - logprior: -3.2011e+00
Epoch 2/10
19/19 - 13s - loss: 537.5322 - loglik: -5.3669e+02 - logprior: -8.3772e-01
Epoch 3/10
19/19 - 13s - loss: 537.2365 - loglik: -5.3672e+02 - logprior: -5.1806e-01
Epoch 4/10
19/19 - 13s - loss: 536.5657 - loglik: -5.3617e+02 - logprior: -3.9837e-01
Epoch 5/10
19/19 - 13s - loss: 534.8960 - loglik: -5.3456e+02 - logprior: -3.3304e-01
Epoch 6/10
19/19 - 13s - loss: 536.3806 - loglik: -5.3610e+02 - logprior: -2.7935e-01
Fitted a model with MAP estimate = -535.3696
Time for alignment: 331.9640
Computed alignments with likelihoods: ['-539.4804', '-528.9626', '-535.3696']
Best model has likelihood: -528.9626
SP score = 0.8721
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e34b2cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f969016d160>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.8700 - loglik: -2.6675e+02 - logprior: -3.1248e+00
Epoch 2/10
19/19 - 2s - loss: 199.4921 - loglik: -1.9814e+02 - logprior: -1.3487e+00
Epoch 3/10
19/19 - 2s - loss: 176.5985 - loglik: -1.7488e+02 - logprior: -1.7177e+00
Epoch 4/10
19/19 - 2s - loss: 173.4738 - loglik: -1.7184e+02 - logprior: -1.6331e+00
Epoch 5/10
19/19 - 2s - loss: 171.5434 - loglik: -1.6996e+02 - logprior: -1.5807e+00
Epoch 6/10
19/19 - 2s - loss: 171.0997 - loglik: -1.6953e+02 - logprior: -1.5689e+00
Epoch 7/10
19/19 - 2s - loss: 170.8269 - loglik: -1.6928e+02 - logprior: -1.5490e+00
Epoch 8/10
19/19 - 2s - loss: 170.7669 - loglik: -1.6923e+02 - logprior: -1.5380e+00
Epoch 9/10
19/19 - 2s - loss: 170.4909 - loglik: -1.6895e+02 - logprior: -1.5404e+00
Epoch 10/10
19/19 - 2s - loss: 170.1735 - loglik: -1.6863e+02 - logprior: -1.5405e+00
Fitted a model with MAP estimate = -170.1991
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 166.8744 - loglik: -1.6287e+02 - logprior: -4.0045e+00
Epoch 2/2
19/19 - 2s - loss: 155.2950 - loglik: -1.5319e+02 - logprior: -2.1006e+00
Fitted a model with MAP estimate = -153.7618
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 154.7652 - loglik: -1.5182e+02 - logprior: -2.9403e+00
Epoch 2/2
19/19 - 2s - loss: 151.0952 - loglik: -1.4995e+02 - logprior: -1.1478e+00
Fitted a model with MAP estimate = -150.4896
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 156.4703 - loglik: -1.5258e+02 - logprior: -3.8891e+00
Epoch 2/10
19/19 - 2s - loss: 152.1492 - loglik: -1.5065e+02 - logprior: -1.4991e+00
Epoch 3/10
19/19 - 2s - loss: 150.9988 - loglik: -1.4991e+02 - logprior: -1.0861e+00
Epoch 4/10
19/19 - 2s - loss: 151.0228 - loglik: -1.4998e+02 - logprior: -1.0460e+00
Fitted a model with MAP estimate = -150.7246
Time for alignment: 67.2339
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.8211 - loglik: -2.6670e+02 - logprior: -3.1242e+00
Epoch 2/10
19/19 - 2s - loss: 200.3871 - loglik: -1.9905e+02 - logprior: -1.3381e+00
Epoch 3/10
19/19 - 2s - loss: 177.2788 - loglik: -1.7558e+02 - logprior: -1.6992e+00
Epoch 4/10
19/19 - 2s - loss: 172.3581 - loglik: -1.7070e+02 - logprior: -1.6583e+00
Epoch 5/10
19/19 - 2s - loss: 171.0640 - loglik: -1.6947e+02 - logprior: -1.5958e+00
Epoch 6/10
19/19 - 2s - loss: 170.4740 - loglik: -1.6889e+02 - logprior: -1.5832e+00
Epoch 7/10
19/19 - 2s - loss: 170.6369 - loglik: -1.6908e+02 - logprior: -1.5574e+00
Fitted a model with MAP estimate = -170.1015
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 162.7917 - loglik: -1.5980e+02 - logprior: -2.9877e+00
Epoch 2/2
19/19 - 2s - loss: 152.2692 - loglik: -1.5105e+02 - logprior: -1.2231e+00
Fitted a model with MAP estimate = -151.1302
expansions: []
discards: [42 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 153.4812 - loglik: -1.5053e+02 - logprior: -2.9465e+00
Epoch 2/2
19/19 - 2s - loss: 151.3639 - loglik: -1.5021e+02 - logprior: -1.1506e+00
Fitted a model with MAP estimate = -150.8755
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 157.8609 - loglik: -1.5402e+02 - logprior: -3.8405e+00
Epoch 2/10
19/19 - 2s - loss: 155.0757 - loglik: -1.5309e+02 - logprior: -1.9840e+00
Epoch 3/10
19/19 - 2s - loss: 154.6752 - loglik: -1.5283e+02 - logprior: -1.8423e+00
Epoch 4/10
19/19 - 2s - loss: 154.0574 - loglik: -1.5262e+02 - logprior: -1.4334e+00
Epoch 5/10
19/19 - 2s - loss: 153.0909 - loglik: -1.5226e+02 - logprior: -8.3428e-01
Epoch 6/10
19/19 - 2s - loss: 152.8744 - loglik: -1.5202e+02 - logprior: -8.5694e-01
Epoch 7/10
19/19 - 2s - loss: 151.2807 - loglik: -1.5033e+02 - logprior: -9.4945e-01
Epoch 8/10
19/19 - 2s - loss: 151.1386 - loglik: -1.5019e+02 - logprior: -9.4419e-01
Epoch 9/10
19/19 - 2s - loss: 151.0383 - loglik: -1.5012e+02 - logprior: -9.2084e-01
Epoch 10/10
19/19 - 2s - loss: 150.9124 - loglik: -1.5001e+02 - logprior: -9.0613e-01
Fitted a model with MAP estimate = -151.0077
Time for alignment: 74.0913
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.7563 - loglik: -2.6663e+02 - logprior: -3.1273e+00
Epoch 2/10
19/19 - 2s - loss: 199.6420 - loglik: -1.9830e+02 - logprior: -1.3448e+00
Epoch 3/10
19/19 - 2s - loss: 176.5375 - loglik: -1.7482e+02 - logprior: -1.7147e+00
Epoch 4/10
19/19 - 2s - loss: 172.7200 - loglik: -1.7108e+02 - logprior: -1.6398e+00
Epoch 5/10
19/19 - 2s - loss: 172.0723 - loglik: -1.7048e+02 - logprior: -1.5924e+00
Epoch 6/10
19/19 - 2s - loss: 171.0673 - loglik: -1.6949e+02 - logprior: -1.5756e+00
Epoch 7/10
19/19 - 2s - loss: 171.0279 - loglik: -1.6948e+02 - logprior: -1.5471e+00
Epoch 8/10
19/19 - 2s - loss: 170.3812 - loglik: -1.6884e+02 - logprior: -1.5426e+00
Epoch 9/10
19/19 - 2s - loss: 170.8641 - loglik: -1.6933e+02 - logprior: -1.5340e+00
Fitted a model with MAP estimate = -170.3573
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 166.9311 - loglik: -1.6293e+02 - logprior: -4.0059e+00
Epoch 2/2
19/19 - 2s - loss: 155.1892 - loglik: -1.5310e+02 - logprior: -2.0926e+00
Fitted a model with MAP estimate = -153.7496
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 154.7793 - loglik: -1.5185e+02 - logprior: -2.9317e+00
Epoch 2/2
19/19 - 2s - loss: 151.1631 - loglik: -1.5002e+02 - logprior: -1.1480e+00
Fitted a model with MAP estimate = -150.4964
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 156.4507 - loglik: -1.5256e+02 - logprior: -3.8906e+00
Epoch 2/10
19/19 - 2s - loss: 152.3751 - loglik: -1.5087e+02 - logprior: -1.5016e+00
Epoch 3/10
19/19 - 2s - loss: 151.0456 - loglik: -1.4996e+02 - logprior: -1.0889e+00
Epoch 4/10
19/19 - 2s - loss: 150.9609 - loglik: -1.4992e+02 - logprior: -1.0378e+00
Epoch 5/10
19/19 - 2s - loss: 150.7144 - loglik: -1.4969e+02 - logprior: -1.0199e+00
Epoch 6/10
19/19 - 2s - loss: 150.5817 - loglik: -1.4960e+02 - logprior: -9.8475e-01
Epoch 7/10
19/19 - 2s - loss: 150.7352 - loglik: -1.4978e+02 - logprior: -9.5331e-01
Fitted a model with MAP estimate = -150.4751
Time for alignment: 71.8067
Computed alignments with likelihoods: ['-150.4896', '-150.8755', '-150.4751']
Best model has likelihood: -150.4751
SP score = 0.9155
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d8c5f310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966354e970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.6397 - loglik: -2.2851e+02 - logprior: -3.1267e+00
Epoch 2/10
19/19 - 1s - loss: 195.7923 - loglik: -1.9450e+02 - logprior: -1.2953e+00
Epoch 3/10
19/19 - 1s - loss: 184.6269 - loglik: -1.8326e+02 - logprior: -1.3666e+00
Epoch 4/10
19/19 - 1s - loss: 181.8813 - loglik: -1.8055e+02 - logprior: -1.3296e+00
Epoch 5/10
19/19 - 1s - loss: 180.2884 - loglik: -1.7898e+02 - logprior: -1.3044e+00
Epoch 6/10
19/19 - 1s - loss: 180.2163 - loglik: -1.7893e+02 - logprior: -1.2830e+00
Epoch 7/10
19/19 - 1s - loss: 179.7908 - loglik: -1.7852e+02 - logprior: -1.2727e+00
Epoch 8/10
19/19 - 1s - loss: 179.3069 - loglik: -1.7804e+02 - logprior: -1.2694e+00
Epoch 9/10
19/19 - 1s - loss: 179.5498 - loglik: -1.7829e+02 - logprior: -1.2642e+00
Fitted a model with MAP estimate = -179.4030
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (32, 1), (33, 1), (34, 2), (35, 1), (47, 1), (49, 1), (50, 1), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 180.6526 - loglik: -1.7673e+02 - logprior: -3.9241e+00
Epoch 2/2
19/19 - 1s - loss: 174.0292 - loglik: -1.7191e+02 - logprior: -2.1161e+00
Fitted a model with MAP estimate = -172.8307
expansions: [(0, 2), (10, 1)]
discards: [ 0 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 173.4789 - loglik: -1.7055e+02 - logprior: -2.9301e+00
Epoch 2/2
19/19 - 1s - loss: 170.7358 - loglik: -1.6958e+02 - logprior: -1.1565e+00
Fitted a model with MAP estimate = -170.2674
expansions: []
discards: [ 0 69 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 174.9187 - loglik: -1.7126e+02 - logprior: -3.6581e+00
Epoch 2/10
19/19 - 1s - loss: 171.6062 - loglik: -1.7035e+02 - logprior: -1.2568e+00
Epoch 3/10
19/19 - 1s - loss: 170.6414 - loglik: -1.6957e+02 - logprior: -1.0664e+00
Epoch 4/10
19/19 - 1s - loss: 170.2417 - loglik: -1.6922e+02 - logprior: -1.0197e+00
Epoch 5/10
19/19 - 1s - loss: 170.6328 - loglik: -1.6965e+02 - logprior: -9.8168e-01
Fitted a model with MAP estimate = -170.2879
Time for alignment: 48.2870
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 232.0118 - loglik: -2.2888e+02 - logprior: -3.1303e+00
Epoch 2/10
19/19 - 1s - loss: 197.8994 - loglik: -1.9660e+02 - logprior: -1.2969e+00
Epoch 3/10
19/19 - 1s - loss: 185.7466 - loglik: -1.8438e+02 - logprior: -1.3671e+00
Epoch 4/10
19/19 - 1s - loss: 181.9554 - loglik: -1.8064e+02 - logprior: -1.3160e+00
Epoch 5/10
19/19 - 1s - loss: 180.4910 - loglik: -1.7919e+02 - logprior: -1.3044e+00
Epoch 6/10
19/19 - 1s - loss: 180.3158 - loglik: -1.7903e+02 - logprior: -1.2872e+00
Epoch 7/10
19/19 - 1s - loss: 179.7698 - loglik: -1.7849e+02 - logprior: -1.2777e+00
Epoch 8/10
19/19 - 1s - loss: 179.6165 - loglik: -1.7834e+02 - logprior: -1.2781e+00
Epoch 9/10
19/19 - 1s - loss: 179.6122 - loglik: -1.7834e+02 - logprior: -1.2729e+00
Epoch 10/10
19/19 - 1s - loss: 179.7090 - loglik: -1.7844e+02 - logprior: -1.2696e+00
Fitted a model with MAP estimate = -179.4527
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (26, 2), (31, 2), (34, 1), (35, 2), (47, 1), (49, 1), (50, 1), (53, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 181.7922 - loglik: -1.7785e+02 - logprior: -3.9424e+00
Epoch 2/2
19/19 - 1s - loss: 174.2693 - loglik: -1.7210e+02 - logprior: -2.1725e+00
Fitted a model with MAP estimate = -173.0121
expansions: [(0, 2)]
discards: [ 0 34 41 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 173.8296 - loglik: -1.7089e+02 - logprior: -2.9380e+00
Epoch 2/2
19/19 - 1s - loss: 170.9928 - loglik: -1.6981e+02 - logprior: -1.1782e+00
Fitted a model with MAP estimate = -170.5312
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 174.4000 - loglik: -1.7071e+02 - logprior: -3.6889e+00
Epoch 2/10
19/19 - 1s - loss: 171.5223 - loglik: -1.7022e+02 - logprior: -1.3002e+00
Epoch 3/10
19/19 - 1s - loss: 170.7051 - loglik: -1.6959e+02 - logprior: -1.1116e+00
Epoch 4/10
19/19 - 1s - loss: 170.3983 - loglik: -1.6934e+02 - logprior: -1.0602e+00
Epoch 5/10
19/19 - 1s - loss: 170.3528 - loglik: -1.6933e+02 - logprior: -1.0263e+00
Epoch 6/10
19/19 - 1s - loss: 170.1354 - loglik: -1.6912e+02 - logprior: -1.0110e+00
Epoch 7/10
19/19 - 1s - loss: 169.7854 - loglik: -1.6879e+02 - logprior: -9.9243e-01
Epoch 8/10
19/19 - 1s - loss: 170.2997 - loglik: -1.6934e+02 - logprior: -9.5934e-01
Fitted a model with MAP estimate = -169.9429
Time for alignment: 53.3766
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.0174 - loglik: -2.2889e+02 - logprior: -3.1302e+00
Epoch 2/10
19/19 - 1s - loss: 198.2175 - loglik: -1.9691e+02 - logprior: -1.3116e+00
Epoch 3/10
19/19 - 1s - loss: 185.6048 - loglik: -1.8426e+02 - logprior: -1.3401e+00
Epoch 4/10
19/19 - 1s - loss: 182.2260 - loglik: -1.8089e+02 - logprior: -1.3363e+00
Epoch 5/10
19/19 - 1s - loss: 180.8066 - loglik: -1.7948e+02 - logprior: -1.3227e+00
Epoch 6/10
19/19 - 1s - loss: 180.3038 - loglik: -1.7899e+02 - logprior: -1.3103e+00
Epoch 7/10
19/19 - 1s - loss: 179.7337 - loglik: -1.7843e+02 - logprior: -1.3048e+00
Epoch 8/10
19/19 - 1s - loss: 179.8859 - loglik: -1.7858e+02 - logprior: -1.3033e+00
Fitted a model with MAP estimate = -179.7225
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 2), (34, 1), (35, 1), (47, 1), (49, 1), (50, 2), (53, 2), (55, 2), (57, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 181.4400 - loglik: -1.7750e+02 - logprior: -3.9360e+00
Epoch 2/2
19/19 - 1s - loss: 174.1210 - loglik: -1.7199e+02 - logprior: -2.1267e+00
Fitted a model with MAP estimate = -173.0168
expansions: [(0, 2), (10, 1)]
discards: [ 0 40 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 173.6254 - loglik: -1.7070e+02 - logprior: -2.9288e+00
Epoch 2/2
19/19 - 1s - loss: 170.9811 - loglik: -1.6982e+02 - logprior: -1.1608e+00
Fitted a model with MAP estimate = -170.2894
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 174.0246 - loglik: -1.7034e+02 - logprior: -3.6859e+00
Epoch 2/10
19/19 - 1s - loss: 171.1718 - loglik: -1.6989e+02 - logprior: -1.2863e+00
Epoch 3/10
19/19 - 1s - loss: 170.1986 - loglik: -1.6910e+02 - logprior: -1.0959e+00
Epoch 4/10
19/19 - 1s - loss: 169.7742 - loglik: -1.6872e+02 - logprior: -1.0517e+00
Epoch 5/10
19/19 - 1s - loss: 170.3798 - loglik: -1.6937e+02 - logprior: -1.0081e+00
Fitted a model with MAP estimate = -169.8980
Time for alignment: 47.1379
Computed alignments with likelihoods: ['-170.2674', '-169.9429', '-169.8980']
Best model has likelihood: -169.8980
SP score = 0.7857
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e3371520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95460e9a00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 667.0037 - loglik: -6.6437e+02 - logprior: -2.6306e+00
Epoch 2/10
19/19 - 11s - loss: 620.5312 - loglik: -6.1960e+02 - logprior: -9.2788e-01
Epoch 3/10
19/19 - 11s - loss: 597.0513 - loglik: -5.9597e+02 - logprior: -1.0822e+00
Epoch 4/10
19/19 - 11s - loss: 589.6545 - loglik: -5.8849e+02 - logprior: -1.1671e+00
Epoch 5/10
19/19 - 11s - loss: 586.5912 - loglik: -5.8542e+02 - logprior: -1.1677e+00
Epoch 6/10
19/19 - 11s - loss: 582.0853 - loglik: -5.8094e+02 - logprior: -1.1437e+00
Epoch 7/10
19/19 - 11s - loss: 581.0389 - loglik: -5.7989e+02 - logprior: -1.1472e+00
Epoch 8/10
19/19 - 11s - loss: 577.9432 - loglik: -5.7677e+02 - logprior: -1.1757e+00
Epoch 9/10
19/19 - 11s - loss: 577.0133 - loglik: -5.7582e+02 - logprior: -1.1932e+00
Epoch 10/10
19/19 - 11s - loss: 576.1276 - loglik: -5.7492e+02 - logprior: -1.2117e+00
Fitted a model with MAP estimate = -575.9978
expansions: [(23, 1), (24, 6), (28, 1), (47, 5), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (82, 1), (83, 1), (96, 3), (104, 2), (105, 2), (109, 1), (123, 1), (124, 1), (126, 1), (135, 1), (155, 1), (156, 3), (160, 1), (161, 1), (162, 1), (164, 1), (167, 1), (169, 1), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 575.7981 - loglik: -5.7213e+02 - logprior: -3.6700e+00
Epoch 2/2
19/19 - 15s - loss: 566.4091 - loglik: -5.6466e+02 - logprior: -1.7452e+00
Fitted a model with MAP estimate = -564.5846
expansions: [(0, 2), (60, 3)]
discards: [  0  26  27  28  77  78 119 130]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 565.5605 - loglik: -5.6278e+02 - logprior: -2.7819e+00
Epoch 2/2
19/19 - 15s - loss: 563.7410 - loglik: -5.6282e+02 - logprior: -9.2358e-01
Fitted a model with MAP estimate = -561.8395
expansions: []
discards: [ 0 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 566.7372 - loglik: -5.6314e+02 - logprior: -3.6013e+00
Epoch 2/10
19/19 - 15s - loss: 564.2634 - loglik: -5.6290e+02 - logprior: -1.3662e+00
Epoch 3/10
19/19 - 15s - loss: 562.7018 - loglik: -5.6214e+02 - logprior: -5.5933e-01
Epoch 4/10
19/19 - 15s - loss: 561.6003 - loglik: -5.6122e+02 - logprior: -3.7654e-01
Epoch 5/10
19/19 - 15s - loss: 559.8909 - loglik: -5.5949e+02 - logprior: -4.0172e-01
Epoch 6/10
19/19 - 15s - loss: 562.8192 - loglik: -5.6250e+02 - logprior: -3.1821e-01
Fitted a model with MAP estimate = -561.1493
Time for alignment: 355.0878
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 666.6825 - loglik: -6.6405e+02 - logprior: -2.6325e+00
Epoch 2/10
19/19 - 11s - loss: 620.3711 - loglik: -6.1944e+02 - logprior: -9.2880e-01
Epoch 3/10
19/19 - 11s - loss: 596.8306 - loglik: -5.9574e+02 - logprior: -1.0909e+00
Epoch 4/10
19/19 - 11s - loss: 589.8448 - loglik: -5.8869e+02 - logprior: -1.1515e+00
Epoch 5/10
19/19 - 11s - loss: 585.2241 - loglik: -5.8406e+02 - logprior: -1.1655e+00
Epoch 6/10
19/19 - 11s - loss: 581.6046 - loglik: -5.8045e+02 - logprior: -1.1594e+00
Epoch 7/10
19/19 - 11s - loss: 580.2036 - loglik: -5.7905e+02 - logprior: -1.1513e+00
Epoch 8/10
19/19 - 11s - loss: 576.8970 - loglik: -5.7570e+02 - logprior: -1.1948e+00
Epoch 9/10
19/19 - 11s - loss: 576.6027 - loglik: -5.7540e+02 - logprior: -1.2021e+00
Epoch 10/10
19/19 - 11s - loss: 577.4878 - loglik: -5.7627e+02 - logprior: -1.2222e+00
Fitted a model with MAP estimate = -575.5954
expansions: [(23, 1), (24, 6), (28, 1), (47, 5), (49, 1), (57, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (104, 1), (105, 4), (106, 2), (110, 1), (121, 1), (124, 1), (126, 1), (127, 1), (135, 1), (151, 2), (156, 2), (161, 1), (163, 1), (167, 1), (168, 1), (169, 1), (170, 1), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 576.1281 - loglik: -5.7249e+02 - logprior: -3.6401e+00
Epoch 2/2
19/19 - 16s - loss: 565.8339 - loglik: -5.6409e+02 - logprior: -1.7471e+00
Fitted a model with MAP estimate = -564.1437
expansions: [(0, 2)]
discards: [  0  26  27  28  79  80 107 132 133 134]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 566.1259 - loglik: -5.6335e+02 - logprior: -2.7768e+00
Epoch 2/2
19/19 - 15s - loss: 562.8599 - loglik: -5.6195e+02 - logprior: -9.1137e-01
Fitted a model with MAP estimate = -561.8565
expansions: [(216, 3)]
discards: [ 0 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 566.4146 - loglik: -5.6285e+02 - logprior: -3.5664e+00
Epoch 2/10
19/19 - 15s - loss: 563.9283 - loglik: -5.6259e+02 - logprior: -1.3341e+00
Epoch 3/10
19/19 - 15s - loss: 561.4487 - loglik: -5.6091e+02 - logprior: -5.4324e-01
Epoch 4/10
19/19 - 15s - loss: 561.4330 - loglik: -5.6105e+02 - logprior: -3.8528e-01
Epoch 5/10
19/19 - 15s - loss: 560.1827 - loglik: -5.5982e+02 - logprior: -3.5914e-01
Epoch 6/10
19/19 - 15s - loss: 561.3975 - loglik: -5.6108e+02 - logprior: -3.2069e-01
Fitted a model with MAP estimate = -560.5510
Time for alignment: 358.4961
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 667.0870 - loglik: -6.6446e+02 - logprior: -2.6312e+00
Epoch 2/10
19/19 - 11s - loss: 621.8088 - loglik: -6.2089e+02 - logprior: -9.1512e-01
Epoch 3/10
19/19 - 11s - loss: 596.3831 - loglik: -5.9529e+02 - logprior: -1.0957e+00
Epoch 4/10
19/19 - 11s - loss: 589.6689 - loglik: -5.8852e+02 - logprior: -1.1488e+00
Epoch 5/10
19/19 - 11s - loss: 585.0701 - loglik: -5.8394e+02 - logprior: -1.1334e+00
Epoch 6/10
19/19 - 11s - loss: 581.7430 - loglik: -5.8060e+02 - logprior: -1.1395e+00
Epoch 7/10
19/19 - 11s - loss: 579.7787 - loglik: -5.7861e+02 - logprior: -1.1680e+00
Epoch 8/10
19/19 - 11s - loss: 577.7440 - loglik: -5.7657e+02 - logprior: -1.1782e+00
Epoch 9/10
19/19 - 11s - loss: 576.5380 - loglik: -5.7533e+02 - logprior: -1.2096e+00
Epoch 10/10
19/19 - 11s - loss: 575.7260 - loglik: -5.7449e+02 - logprior: -1.2313e+00
Fitted a model with MAP estimate = -575.1144
expansions: [(21, 1), (22, 1), (24, 5), (28, 1), (47, 5), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (96, 1), (103, 1), (104, 3), (105, 1), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (135, 1), (155, 2), (156, 3), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (176, 3), (179, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 575.4465 - loglik: -5.7181e+02 - logprior: -3.6406e+00
Epoch 2/2
19/19 - 16s - loss: 565.8628 - loglik: -5.6411e+02 - logprior: -1.7492e+00
Fitted a model with MAP estimate = -563.8780
expansions: [(0, 2), (30, 1), (60, 3)]
discards: [  0  26  27  28  77  78 105 130 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 565.2795 - loglik: -5.6252e+02 - logprior: -2.7621e+00
Epoch 2/2
19/19 - 15s - loss: 562.6890 - loglik: -5.6178e+02 - logprior: -9.0782e-01
Fitted a model with MAP estimate = -560.9933
expansions: []
discards: [ 0 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 565.8813 - loglik: -5.6229e+02 - logprior: -3.5889e+00
Epoch 2/10
19/19 - 15s - loss: 563.7170 - loglik: -5.6237e+02 - logprior: -1.3442e+00
Epoch 3/10
19/19 - 15s - loss: 561.1741 - loglik: -5.6063e+02 - logprior: -5.3999e-01
Epoch 4/10
19/19 - 15s - loss: 560.5192 - loglik: -5.6014e+02 - logprior: -3.8407e-01
Epoch 5/10
19/19 - 15s - loss: 561.1757 - loglik: -5.6080e+02 - logprior: -3.7776e-01
Fitted a model with MAP estimate = -560.3408
Time for alignment: 347.5219
Computed alignments with likelihoods: ['-561.1493', '-560.5510', '-560.3408']
Best model has likelihood: -560.3408
SP score = 0.6310
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e3073670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9638ebdd60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1043 - loglik: -1.5088e+02 - logprior: -3.2254e+00
Epoch 2/10
19/19 - 1s - loss: 124.2983 - loglik: -1.2281e+02 - logprior: -1.4886e+00
Epoch 3/10
19/19 - 1s - loss: 110.4615 - loglik: -1.0890e+02 - logprior: -1.5661e+00
Epoch 4/10
19/19 - 1s - loss: 107.3161 - loglik: -1.0571e+02 - logprior: -1.6100e+00
Epoch 5/10
19/19 - 1s - loss: 106.1913 - loglik: -1.0464e+02 - logprior: -1.5547e+00
Epoch 6/10
19/19 - 1s - loss: 105.8975 - loglik: -1.0435e+02 - logprior: -1.5472e+00
Epoch 7/10
19/19 - 1s - loss: 105.6153 - loglik: -1.0409e+02 - logprior: -1.5224e+00
Epoch 8/10
19/19 - 1s - loss: 105.7690 - loglik: -1.0426e+02 - logprior: -1.5087e+00
Fitted a model with MAP estimate = -105.4207
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.1332 - loglik: -1.0204e+02 - logprior: -3.0917e+00
Epoch 2/2
19/19 - 1s - loss: 98.4868 - loglik: -9.7150e+01 - logprior: -1.3364e+00
Fitted a model with MAP estimate = -97.9122
expansions: []
discards: [ 0 21 37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.6507 - loglik: -9.9582e+01 - logprior: -4.0686e+00
Epoch 2/2
19/19 - 1s - loss: 100.3225 - loglik: -9.8078e+01 - logprior: -2.2441e+00
Fitted a model with MAP estimate = -99.3797
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.6993 - loglik: -9.8628e+01 - logprior: -4.0709e+00
Epoch 2/10
19/19 - 1s - loss: 99.0706 - loglik: -9.7295e+01 - logprior: -1.7757e+00
Epoch 3/10
19/19 - 1s - loss: 98.3291 - loglik: -9.7039e+01 - logprior: -1.2902e+00
Epoch 4/10
19/19 - 1s - loss: 98.1265 - loglik: -9.6904e+01 - logprior: -1.2220e+00
Epoch 5/10
19/19 - 1s - loss: 98.1210 - loglik: -9.6937e+01 - logprior: -1.1839e+00
Epoch 6/10
19/19 - 1s - loss: 98.0619 - loglik: -9.6902e+01 - logprior: -1.1603e+00
Epoch 7/10
19/19 - 1s - loss: 97.9743 - loglik: -9.6836e+01 - logprior: -1.1380e+00
Epoch 8/10
19/19 - 1s - loss: 98.1860 - loglik: -9.7069e+01 - logprior: -1.1166e+00
Fitted a model with MAP estimate = -97.9425
Time for alignment: 42.3807
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0987 - loglik: -1.5087e+02 - logprior: -3.2260e+00
Epoch 2/10
19/19 - 1s - loss: 124.0898 - loglik: -1.2259e+02 - logprior: -1.4988e+00
Epoch 3/10
19/19 - 1s - loss: 111.1689 - loglik: -1.0961e+02 - logprior: -1.5626e+00
Epoch 4/10
19/19 - 1s - loss: 107.8708 - loglik: -1.0627e+02 - logprior: -1.6044e+00
Epoch 5/10
19/19 - 1s - loss: 106.1748 - loglik: -1.0463e+02 - logprior: -1.5463e+00
Epoch 6/10
19/19 - 1s - loss: 106.0164 - loglik: -1.0446e+02 - logprior: -1.5554e+00
Epoch 7/10
19/19 - 1s - loss: 105.5204 - loglik: -1.0400e+02 - logprior: -1.5238e+00
Epoch 8/10
19/19 - 1s - loss: 105.6209 - loglik: -1.0411e+02 - logprior: -1.5128e+00
Fitted a model with MAP estimate = -105.4233
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.8465 - loglik: -1.0467e+02 - logprior: -4.1727e+00
Epoch 2/2
19/19 - 1s - loss: 100.2679 - loglik: -9.7943e+01 - logprior: -2.3249e+00
Fitted a model with MAP estimate = -99.2620
expansions: [(3, 1)]
discards: [ 0 21 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.9606 - loglik: -9.8998e+01 - logprior: -3.9624e+00
Epoch 2/2
19/19 - 1s - loss: 99.0718 - loglik: -9.7543e+01 - logprior: -1.5293e+00
Fitted a model with MAP estimate = -98.4930
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.1650 - loglik: -9.8689e+01 - logprior: -4.4756e+00
Epoch 2/10
19/19 - 1s - loss: 99.3423 - loglik: -9.7285e+01 - logprior: -2.0571e+00
Epoch 3/10
19/19 - 1s - loss: 98.5228 - loglik: -9.7115e+01 - logprior: -1.4078e+00
Epoch 4/10
19/19 - 1s - loss: 98.1554 - loglik: -9.6913e+01 - logprior: -1.2425e+00
Epoch 5/10
19/19 - 1s - loss: 98.0516 - loglik: -9.6862e+01 - logprior: -1.1893e+00
Epoch 6/10
19/19 - 1s - loss: 98.1125 - loglik: -9.6954e+01 - logprior: -1.1587e+00
Fitted a model with MAP estimate = -98.0463
Time for alignment: 39.0833
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0993 - loglik: -1.5087e+02 - logprior: -3.2255e+00
Epoch 2/10
19/19 - 1s - loss: 124.7258 - loglik: -1.2321e+02 - logprior: -1.5154e+00
Epoch 3/10
19/19 - 1s - loss: 111.6920 - loglik: -1.1011e+02 - logprior: -1.5854e+00
Epoch 4/10
19/19 - 1s - loss: 107.8171 - loglik: -1.0620e+02 - logprior: -1.6203e+00
Epoch 5/10
19/19 - 1s - loss: 106.6789 - loglik: -1.0506e+02 - logprior: -1.6158e+00
Epoch 6/10
19/19 - 1s - loss: 106.0468 - loglik: -1.0447e+02 - logprior: -1.5794e+00
Epoch 7/10
19/19 - 1s - loss: 106.3036 - loglik: -1.0474e+02 - logprior: -1.5613e+00
Fitted a model with MAP estimate = -105.8681
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.3612 - loglik: -1.0229e+02 - logprior: -3.0713e+00
Epoch 2/2
19/19 - 1s - loss: 98.4923 - loglik: -9.7178e+01 - logprior: -1.3146e+00
Fitted a model with MAP estimate = -97.8807
expansions: []
discards: [ 0 21 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.5657 - loglik: -9.9508e+01 - logprior: -4.0572e+00
Epoch 2/2
19/19 - 1s - loss: 100.3421 - loglik: -9.8127e+01 - logprior: -2.2155e+00
Fitted a model with MAP estimate = -99.2169
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.6876 - loglik: -9.8605e+01 - logprior: -4.0822e+00
Epoch 2/10
19/19 - 1s - loss: 99.1536 - loglik: -9.7298e+01 - logprior: -1.8553e+00
Epoch 3/10
19/19 - 1s - loss: 98.2770 - loglik: -9.6979e+01 - logprior: -1.2982e+00
Epoch 4/10
19/19 - 1s - loss: 98.2290 - loglik: -9.7005e+01 - logprior: -1.2245e+00
Epoch 5/10
19/19 - 1s - loss: 98.2122 - loglik: -9.7027e+01 - logprior: -1.1851e+00
Epoch 6/10
19/19 - 1s - loss: 97.9855 - loglik: -9.6827e+01 - logprior: -1.1580e+00
Epoch 7/10
19/19 - 1s - loss: 98.1054 - loglik: -9.6966e+01 - logprior: -1.1399e+00
Fitted a model with MAP estimate = -97.9617
Time for alignment: 40.1727
Computed alignments with likelihoods: ['-97.9122', '-98.0463', '-97.8807']
Best model has likelihood: -97.8807
SP score = 0.9647
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96633d7b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9545bf7a30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.6646 - loglik: -3.6267e+02 - logprior: -2.9929e+00
Epoch 2/10
19/19 - 3s - loss: 338.1120 - loglik: -3.3707e+02 - logprior: -1.0452e+00
Epoch 3/10
19/19 - 3s - loss: 324.8855 - loglik: -3.2361e+02 - logprior: -1.2792e+00
Epoch 4/10
19/19 - 3s - loss: 321.1025 - loglik: -3.1992e+02 - logprior: -1.1781e+00
Epoch 5/10
19/19 - 3s - loss: 318.7065 - loglik: -3.1754e+02 - logprior: -1.1694e+00
Epoch 6/10
19/19 - 3s - loss: 316.9907 - loglik: -3.1584e+02 - logprior: -1.1457e+00
Epoch 7/10
19/19 - 3s - loss: 316.0544 - loglik: -3.1492e+02 - logprior: -1.1376e+00
Epoch 8/10
19/19 - 3s - loss: 314.9032 - loglik: -3.1377e+02 - logprior: -1.1335e+00
Epoch 9/10
19/19 - 3s - loss: 314.4686 - loglik: -3.1332e+02 - logprior: -1.1486e+00
Epoch 10/10
19/19 - 3s - loss: 314.6848 - loglik: -3.1353e+02 - logprior: -1.1576e+00
Fitted a model with MAP estimate = -313.9766
expansions: [(18, 1), (20, 2), (21, 4), (24, 3), (26, 1), (29, 1), (32, 1), (51, 1), (53, 2), (73, 1), (74, 1), (75, 1), (76, 4), (82, 1), (84, 3), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 315.2919 - loglik: -3.1146e+02 - logprior: -3.8337e+00
Epoch 2/2
19/19 - 3s - loss: 308.0655 - loglik: -3.0607e+02 - logprior: -1.9921e+00
Fitted a model with MAP estimate = -306.6980
expansions: [(0, 1), (24, 1), (30, 1)]
discards: [  0  94 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 307.9164 - loglik: -3.0521e+02 - logprior: -2.7065e+00
Epoch 2/2
19/19 - 3s - loss: 305.0405 - loglik: -3.0394e+02 - logprior: -1.1000e+00
Fitted a model with MAP estimate = -304.2230
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 306.2226 - loglik: -3.0338e+02 - logprior: -2.8406e+00
Epoch 2/10
19/19 - 3s - loss: 304.5025 - loglik: -3.0347e+02 - logprior: -1.0356e+00
Epoch 3/10
19/19 - 3s - loss: 303.9209 - loglik: -3.0298e+02 - logprior: -9.4564e-01
Epoch 4/10
19/19 - 3s - loss: 303.4528 - loglik: -3.0256e+02 - logprior: -8.9428e-01
Epoch 5/10
19/19 - 3s - loss: 303.6179 - loglik: -3.0277e+02 - logprior: -8.4550e-01
Fitted a model with MAP estimate = -303.2363
Time for alignment: 89.5260
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.8136 - loglik: -3.6282e+02 - logprior: -2.9944e+00
Epoch 2/10
19/19 - 3s - loss: 338.0524 - loglik: -3.3701e+02 - logprior: -1.0405e+00
Epoch 3/10
19/19 - 3s - loss: 326.7015 - loglik: -3.2547e+02 - logprior: -1.2334e+00
Epoch 4/10
19/19 - 3s - loss: 322.6662 - loglik: -3.2154e+02 - logprior: -1.1303e+00
Epoch 5/10
19/19 - 3s - loss: 319.5196 - loglik: -3.1835e+02 - logprior: -1.1712e+00
Epoch 6/10
19/19 - 3s - loss: 317.3119 - loglik: -3.1614e+02 - logprior: -1.1707e+00
Epoch 7/10
19/19 - 3s - loss: 316.2445 - loglik: -3.1508e+02 - logprior: -1.1682e+00
Epoch 8/10
19/19 - 3s - loss: 315.2180 - loglik: -3.1404e+02 - logprior: -1.1742e+00
Epoch 9/10
19/19 - 3s - loss: 314.5924 - loglik: -3.1341e+02 - logprior: -1.1791e+00
Epoch 10/10
19/19 - 3s - loss: 314.3266 - loglik: -3.1314e+02 - logprior: -1.1900e+00
Fitted a model with MAP estimate = -313.9814
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (24, 2), (25, 1), (28, 1), (31, 1), (54, 3), (73, 1), (74, 1), (75, 1), (76, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 315.3010 - loglik: -3.1146e+02 - logprior: -3.8367e+00
Epoch 2/2
19/19 - 3s - loss: 307.8089 - loglik: -3.0581e+02 - logprior: -1.9949e+00
Fitted a model with MAP estimate = -306.4073
expansions: [(0, 1), (35, 1)]
discards: [  0 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 307.3759 - loglik: -3.0467e+02 - logprior: -2.7063e+00
Epoch 2/2
19/19 - 3s - loss: 304.6723 - loglik: -3.0359e+02 - logprior: -1.0819e+00
Fitted a model with MAP estimate = -303.9616
expansions: []
discards: [ 98  99 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 306.6675 - loglik: -3.0384e+02 - logprior: -2.8234e+00
Epoch 2/10
19/19 - 3s - loss: 304.8048 - loglik: -3.0380e+02 - logprior: -1.0057e+00
Epoch 3/10
19/19 - 3s - loss: 304.5431 - loglik: -3.0361e+02 - logprior: -9.3221e-01
Epoch 4/10
19/19 - 3s - loss: 303.7924 - loglik: -3.0291e+02 - logprior: -8.7780e-01
Epoch 5/10
19/19 - 3s - loss: 303.6949 - loglik: -3.0286e+02 - logprior: -8.3049e-01
Epoch 6/10
19/19 - 3s - loss: 303.7653 - loglik: -3.0297e+02 - logprior: -7.9161e-01
Fitted a model with MAP estimate = -303.5188
Time for alignment: 93.0500
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.7601 - loglik: -3.6277e+02 - logprior: -2.9910e+00
Epoch 2/10
19/19 - 3s - loss: 337.1161 - loglik: -3.3610e+02 - logprior: -1.0188e+00
Epoch 3/10
19/19 - 3s - loss: 325.3804 - loglik: -3.2413e+02 - logprior: -1.2463e+00
Epoch 4/10
19/19 - 3s - loss: 321.3748 - loglik: -3.2024e+02 - logprior: -1.1317e+00
Epoch 5/10
19/19 - 3s - loss: 319.9075 - loglik: -3.1878e+02 - logprior: -1.1273e+00
Epoch 6/10
19/19 - 3s - loss: 317.6529 - loglik: -3.1654e+02 - logprior: -1.1109e+00
Epoch 7/10
19/19 - 3s - loss: 316.3219 - loglik: -3.1522e+02 - logprior: -1.1056e+00
Epoch 8/10
19/19 - 3s - loss: 315.7190 - loglik: -3.1462e+02 - logprior: -1.1034e+00
Epoch 9/10
19/19 - 3s - loss: 315.2818 - loglik: -3.1418e+02 - logprior: -1.1053e+00
Epoch 10/10
19/19 - 3s - loss: 314.9456 - loglik: -3.1383e+02 - logprior: -1.1125e+00
Fitted a model with MAP estimate = -314.5234
expansions: [(18, 1), (20, 2), (21, 4), (24, 2), (25, 2), (26, 3), (28, 1), (31, 1), (54, 4), (74, 1), (75, 1), (76, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 315.5774 - loglik: -3.1174e+02 - logprior: -3.8325e+00
Epoch 2/2
19/19 - 3s - loss: 308.5479 - loglik: -3.0654e+02 - logprior: -2.0045e+00
Fitted a model with MAP estimate = -306.6427
expansions: [(0, 1), (24, 1)]
discards: [  0 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 307.5995 - loglik: -3.0490e+02 - logprior: -2.7030e+00
Epoch 2/2
19/19 - 3s - loss: 304.7437 - loglik: -3.0366e+02 - logprior: -1.0880e+00
Fitted a model with MAP estimate = -304.0184
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 305.8353 - loglik: -3.0299e+02 - logprior: -2.8433e+00
Epoch 2/10
19/19 - 3s - loss: 304.4178 - loglik: -3.0338e+02 - logprior: -1.0406e+00
Epoch 3/10
19/19 - 3s - loss: 303.5579 - loglik: -3.0260e+02 - logprior: -9.5563e-01
Epoch 4/10
19/19 - 3s - loss: 303.2650 - loglik: -3.0236e+02 - logprior: -9.0194e-01
Epoch 5/10
19/19 - 3s - loss: 303.1398 - loglik: -3.0229e+02 - logprior: -8.5454e-01
Epoch 6/10
19/19 - 3s - loss: 302.7107 - loglik: -3.0190e+02 - logprior: -8.1044e-01
Epoch 7/10
19/19 - 3s - loss: 302.9444 - loglik: -3.0218e+02 - logprior: -7.6781e-01
Fitted a model with MAP estimate = -302.6437
Time for alignment: 96.0872
Computed alignments with likelihoods: ['-303.2363', '-303.5188', '-302.6437']
Best model has likelihood: -302.6437
SP score = 0.7943
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f951375fbe0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ef0c0d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 919.7856 - loglik: -9.1833e+02 - logprior: -1.4511e+00
Epoch 2/10
39/39 - 25s - loss: 826.3958 - loglik: -8.2500e+02 - logprior: -1.3957e+00
Epoch 3/10
39/39 - 25s - loss: 814.9507 - loglik: -8.1356e+02 - logprior: -1.3938e+00
Epoch 4/10
39/39 - 25s - loss: 811.1744 - loglik: -8.0979e+02 - logprior: -1.3837e+00
Epoch 5/10
39/39 - 25s - loss: 808.5065 - loglik: -8.0711e+02 - logprior: -1.3934e+00
Epoch 6/10
39/39 - 25s - loss: 807.4620 - loglik: -8.0603e+02 - logprior: -1.4296e+00
Epoch 7/10
39/39 - 25s - loss: 807.3454 - loglik: -8.0590e+02 - logprior: -1.4502e+00
Epoch 8/10
39/39 - 25s - loss: 807.6459 - loglik: -8.0617e+02 - logprior: -1.4746e+00
Fitted a model with MAP estimate = -798.9072
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (37, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 3), (103, 1), (105, 2), (121, 1), (134, 1), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (162, 1), (182, 1), (184, 1), (186, 2), (187, 2), (188, 2), (189, 1), (197, 1), (205, 3), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (228, 2), (240, 2), (241, 2), (242, 1), (251, 1), (258, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 792.6357 - loglik: -7.9047e+02 - logprior: -2.1704e+00
Epoch 2/2
39/39 - 36s - loss: 779.5797 - loglik: -7.7880e+02 - logprior: -7.8195e-01
Fitted a model with MAP estimate = -770.2180
expansions: []
discards: [  0  31 104 121 135 191 232 233 258 303 318 344]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 345 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 783.1530 - loglik: -7.8055e+02 - logprior: -2.6038e+00
Epoch 2/2
39/39 - 34s - loss: 780.6924 - loglik: -7.8009e+02 - logprior: -6.0492e-01
Fitted a model with MAP estimate = -771.2828
expansions: [(0, 2), (152, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 770.8481 - loglik: -7.6965e+02 - logprior: -1.1951e+00
Epoch 2/10
39/39 - 35s - loss: 770.6688 - loglik: -7.7058e+02 - logprior: -9.0762e-02
Epoch 3/10
39/39 - 35s - loss: 769.0980 - loglik: -7.6918e+02 - logprior: 0.0838
Epoch 4/10
39/39 - 35s - loss: 767.6391 - loglik: -7.6785e+02 - logprior: 0.2106
Epoch 5/10
39/39 - 35s - loss: 768.2212 - loglik: -7.6856e+02 - logprior: 0.3392
Fitted a model with MAP estimate = -767.2644
Time for alignment: 714.3208
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 920.1362 - loglik: -9.1869e+02 - logprior: -1.4477e+00
Epoch 2/10
39/39 - 25s - loss: 825.4059 - loglik: -8.2403e+02 - logprior: -1.3749e+00
Epoch 3/10
39/39 - 25s - loss: 815.0581 - loglik: -8.1368e+02 - logprior: -1.3750e+00
Epoch 4/10
39/39 - 25s - loss: 811.5414 - loglik: -8.1019e+02 - logprior: -1.3480e+00
Epoch 5/10
39/39 - 25s - loss: 808.8253 - loglik: -8.0743e+02 - logprior: -1.3943e+00
Epoch 6/10
39/39 - 25s - loss: 807.7256 - loglik: -8.0631e+02 - logprior: -1.4199e+00
Epoch 7/10
39/39 - 25s - loss: 807.8121 - loglik: -8.0638e+02 - logprior: -1.4311e+00
Fitted a model with MAP estimate = -799.6445
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (34, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (62, 1), (69, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 3), (103, 1), (105, 2), (121, 1), (124, 2), (130, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (182, 1), (184, 2), (186, 1), (187, 1), (188, 1), (201, 1), (206, 4), (207, 1), (208, 1), (209, 2), (211, 1), (220, 1), (222, 1), (228, 2), (240, 2), (241, 2), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 359 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 791.8828 - loglik: -7.8967e+02 - logprior: -2.2150e+00
Epoch 2/2
39/39 - 37s - loss: 778.0250 - loglik: -7.7717e+02 - logprior: -8.5105e-01
Fitted a model with MAP estimate = -768.1334
expansions: [(159, 1)]
discards: [  0  30 104 121 135 193 266 305 320]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 779.7734 - loglik: -7.7727e+02 - logprior: -2.4990e+00
Epoch 2/2
39/39 - 35s - loss: 777.0302 - loglik: -7.7650e+02 - logprior: -5.3057e-01
Fitted a model with MAP estimate = -768.3795
expansions: [(0, 2)]
discards: [  0 338]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 769.9700 - loglik: -7.6870e+02 - logprior: -1.2680e+00
Epoch 2/10
39/39 - 35s - loss: 769.0567 - loglik: -7.6894e+02 - logprior: -1.1836e-01
Epoch 3/10
39/39 - 35s - loss: 767.7620 - loglik: -7.6788e+02 - logprior: 0.1195
Epoch 4/10
39/39 - 36s - loss: 767.5063 - loglik: -7.6776e+02 - logprior: 0.2546
Epoch 5/10
39/39 - 35s - loss: 766.7593 - loglik: -7.6717e+02 - logprior: 0.4074
Epoch 6/10
39/39 - 36s - loss: 767.9027 - loglik: -7.6848e+02 - logprior: 0.5752
Fitted a model with MAP estimate = -766.2750
Time for alignment: 732.4033
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 918.7465 - loglik: -9.1730e+02 - logprior: -1.4497e+00
Epoch 2/10
39/39 - 25s - loss: 824.0674 - loglik: -8.2265e+02 - logprior: -1.4171e+00
Epoch 3/10
39/39 - 25s - loss: 814.5472 - loglik: -8.1314e+02 - logprior: -1.4027e+00
Epoch 4/10
39/39 - 25s - loss: 810.4721 - loglik: -8.0908e+02 - logprior: -1.3910e+00
Epoch 5/10
39/39 - 25s - loss: 807.7334 - loglik: -8.0634e+02 - logprior: -1.3956e+00
Epoch 6/10
39/39 - 25s - loss: 807.0540 - loglik: -8.0562e+02 - logprior: -1.4377e+00
Epoch 7/10
39/39 - 25s - loss: 806.8735 - loglik: -8.0537e+02 - logprior: -1.5006e+00
Epoch 8/10
39/39 - 25s - loss: 806.7867 - loglik: -8.0526e+02 - logprior: -1.5314e+00
Epoch 9/10
39/39 - 25s - loss: 806.6639 - loglik: -8.0513e+02 - logprior: -1.5374e+00
Epoch 10/10
39/39 - 25s - loss: 806.3109 - loglik: -8.0478e+02 - logprior: -1.5282e+00
Fitted a model with MAP estimate = -797.7861
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 3), (103, 1), (120, 1), (121, 1), (124, 2), (132, 2), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (170, 1), (182, 1), (185, 1), (186, 1), (187, 1), (189, 1), (190, 1), (201, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (228, 2), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 791.0370 - loglik: -7.8884e+02 - logprior: -2.1956e+00
Epoch 2/2
39/39 - 36s - loss: 777.4512 - loglik: -7.7669e+02 - logprior: -7.6517e-01
Fitted a model with MAP estimate = -768.0613
expansions: [(160, 1)]
discards: [  0  30  54 106 123 168 187 195]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 779.0336 - loglik: -7.7654e+02 - logprior: -2.4934e+00
Epoch 2/2
39/39 - 35s - loss: 776.8289 - loglik: -7.7635e+02 - logprior: -4.7856e-01
Fitted a model with MAP estimate = -767.6011
expansions: [(0, 2)]
discards: [  0 313 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 769.4886 - loglik: -7.6825e+02 - logprior: -1.2433e+00
Epoch 2/10
39/39 - 35s - loss: 769.0729 - loglik: -7.6902e+02 - logprior: -5.7384e-02
Epoch 3/10
39/39 - 35s - loss: 768.2467 - loglik: -7.6840e+02 - logprior: 0.1515
Epoch 4/10
39/39 - 35s - loss: 767.1793 - loglik: -7.6749e+02 - logprior: 0.3071
Epoch 5/10
39/39 - 35s - loss: 766.9662 - loglik: -7.6741e+02 - logprior: 0.4398
Epoch 6/10
39/39 - 35s - loss: 767.5135 - loglik: -7.6812e+02 - logprior: 0.6082
Fitted a model with MAP estimate = -766.5074
Time for alignment: 804.6506
Computed alignments with likelihoods: ['-767.2644', '-766.2750', '-766.5074']
Best model has likelihood: -766.2750
SP score = 0.4312
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96498e95b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f960dec05b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 893.5325 - loglik: -8.9207e+02 - logprior: -1.4613e+00
Epoch 2/10
39/39 - 38s - loss: 758.4852 - loglik: -7.5717e+02 - logprior: -1.3181e+00
Epoch 3/10
39/39 - 38s - loss: 747.7592 - loglik: -7.4637e+02 - logprior: -1.3867e+00
Epoch 4/10
39/39 - 38s - loss: 744.7651 - loglik: -7.4337e+02 - logprior: -1.3919e+00
Epoch 5/10
39/39 - 38s - loss: 743.7487 - loglik: -7.4233e+02 - logprior: -1.4204e+00
Epoch 6/10
39/39 - 38s - loss: 743.5710 - loglik: -7.4212e+02 - logprior: -1.4507e+00
Epoch 7/10
39/39 - 38s - loss: 742.9062 - loglik: -7.4142e+02 - logprior: -1.4842e+00
Epoch 8/10
39/39 - 38s - loss: 742.5955 - loglik: -7.4113e+02 - logprior: -1.4683e+00
Epoch 9/10
39/39 - 38s - loss: 743.7579 - loglik: -7.4228e+02 - logprior: -1.4772e+00
Fitted a model with MAP estimate = -741.7861
expansions: [(6, 1), (20, 2), (21, 1), (31, 1), (67, 1), (69, 1), (101, 2), (104, 1), (106, 1), (123, 1), (142, 2), (143, 1), (144, 1), (145, 5), (146, 1), (160, 1), (161, 1), (162, 2), (163, 1), (164, 1), (168, 1), (176, 3), (177, 4), (180, 4), (181, 1), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 2), (210, 2), (211, 2), (218, 2), (219, 1), (220, 1), (228, 1), (233, 1), (234, 3), (236, 1), (239, 1), (241, 1), (245, 1), (251, 1), (253, 1), (256, 1), (283, 1), (284, 2), (297, 1), (298, 2), (299, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 192 193 194 195 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 403 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 727.6865 - loglik: -7.2616e+02 - logprior: -1.5261e+00
Epoch 2/2
39/39 - 54s - loss: 717.7471 - loglik: -7.1737e+02 - logprior: -3.7294e-01
Fitted a model with MAP estimate = -715.3512
expansions: [(235, 4), (301, 1)]
discards: [  5 209 210 219 246 258 260 261 267 289 290]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 719.0063 - loglik: -7.1776e+02 - logprior: -1.2499e+00
Epoch 2/2
39/39 - 52s - loss: 716.2208 - loglik: -7.1613e+02 - logprior: -8.7772e-02
Fitted a model with MAP estimate = -714.3867
expansions: [(233, 1), (234, 1), (284, 2)]
discards: [216 235 236 237 238 239 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 719.1102 - loglik: -7.1807e+02 - logprior: -1.0394e+00
Epoch 2/10
39/39 - 52s - loss: 716.9868 - loglik: -7.1725e+02 - logprior: 0.2650
Epoch 3/10
39/39 - 52s - loss: 716.2134 - loglik: -7.1662e+02 - logprior: 0.4039
Epoch 4/10
39/39 - 52s - loss: 715.4646 - loglik: -7.1607e+02 - logprior: 0.6080
Epoch 5/10
39/39 - 52s - loss: 716.3271 - loglik: -7.1709e+02 - logprior: 0.7660
Fitted a model with MAP estimate = -715.0005
Time for alignment: 1059.0711
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 890.8128 - loglik: -8.8934e+02 - logprior: -1.4685e+00
Epoch 2/10
39/39 - 38s - loss: 754.5989 - loglik: -7.5314e+02 - logprior: -1.4616e+00
Epoch 3/10
39/39 - 38s - loss: 742.9630 - loglik: -7.4135e+02 - logprior: -1.6105e+00
Epoch 4/10
39/39 - 38s - loss: 740.7475 - loglik: -7.3921e+02 - logprior: -1.5419e+00
Epoch 5/10
39/39 - 38s - loss: 739.2300 - loglik: -7.3765e+02 - logprior: -1.5793e+00
Epoch 6/10
39/39 - 38s - loss: 739.0494 - loglik: -7.3746e+02 - logprior: -1.5864e+00
Epoch 7/10
39/39 - 38s - loss: 738.9196 - loglik: -7.3727e+02 - logprior: -1.6542e+00
Epoch 8/10
39/39 - 38s - loss: 738.6810 - loglik: -7.3709e+02 - logprior: -1.5960e+00
Epoch 9/10
39/39 - 38s - loss: 738.7318 - loglik: -7.3712e+02 - logprior: -1.6082e+00
Fitted a model with MAP estimate = -737.5614
expansions: [(9, 1), (19, 1), (21, 1), (34, 1), (67, 1), (69, 1), (101, 2), (104, 1), (106, 1), (116, 1), (122, 1), (141, 1), (143, 1), (144, 1), (145, 3), (146, 3), (161, 2), (162, 2), (163, 1), (168, 1), (175, 1), (177, 1), (178, 5), (181, 1), (182, 5), (184, 1), (186, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (202, 1), (203, 1), (211, 3), (219, 2), (220, 1), (221, 1), (229, 1), (234, 1), (235, 3), (237, 1), (240, 1), (242, 1), (246, 1), (249, 2), (250, 1), (256, 1), (277, 1), (279, 2), (283, 4), (284, 1), (288, 1), (289, 2), (297, 1), (307, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 195 196 197 198 199]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 406 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 724.6777 - loglik: -7.2298e+02 - logprior: -1.6980e+00
Epoch 2/2
39/39 - 54s - loss: 711.2038 - loglik: -7.1064e+02 - logprior: -5.6840e-01
Fitted a model with MAP estimate = -708.2553
expansions: [(257, 2), (299, 1)]
discards: [160 208 209 241 242 243 244 245 246 259 265 287 350 351 360]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 714.5043 - loglik: -7.1326e+02 - logprior: -1.2467e+00
Epoch 2/2
39/39 - 51s - loss: 711.8937 - loglik: -7.1186e+02 - logprior: -3.1807e-02
Fitted a model with MAP estimate = -710.3522
expansions: [(5, 1), (252, 1)]
discards: [215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 712.1556 - loglik: -7.1113e+02 - logprior: -1.0283e+00
Epoch 2/10
39/39 - 52s - loss: 710.1044 - loglik: -7.1035e+02 - logprior: 0.2476
Epoch 3/10
39/39 - 52s - loss: 710.6644 - loglik: -7.1100e+02 - logprior: 0.3393
Fitted a model with MAP estimate = -709.4825
Time for alignment: 957.6109
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 894.0841 - loglik: -8.9259e+02 - logprior: -1.4923e+00
Epoch 2/10
39/39 - 38s - loss: 756.6375 - loglik: -7.5515e+02 - logprior: -1.4833e+00
Epoch 3/10
39/39 - 38s - loss: 744.6230 - loglik: -7.4299e+02 - logprior: -1.6292e+00
Epoch 4/10
39/39 - 38s - loss: 742.2001 - loglik: -7.4060e+02 - logprior: -1.6018e+00
Epoch 5/10
39/39 - 38s - loss: 740.4648 - loglik: -7.3882e+02 - logprior: -1.6448e+00
Epoch 6/10
39/39 - 38s - loss: 740.5486 - loglik: -7.3888e+02 - logprior: -1.6658e+00
Fitted a model with MAP estimate = -739.3297
expansions: [(9, 1), (19, 1), (31, 1), (61, 1), (65, 1), (96, 1), (99, 1), (103, 1), (105, 1), (106, 1), (107, 1), (119, 1), (120, 1), (138, 1), (140, 1), (141, 1), (143, 3), (144, 2), (160, 2), (161, 1), (162, 1), (163, 1), (167, 1), (176, 3), (177, 4), (180, 1), (181, 5), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 2), (202, 1), (211, 1), (218, 1), (219, 1), (220, 1), (221, 1), (229, 1), (234, 1), (238, 1), (241, 1), (243, 1), (245, 2), (247, 1), (248, 1), (254, 1), (257, 1), (259, 1), (278, 1), (279, 3), (280, 1), (281, 3), (282, 3), (296, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 193 194 195 196 197 207 208 209]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 727.6540 - loglik: -7.2598e+02 - logprior: -1.6704e+00
Epoch 2/2
39/39 - 53s - loss: 716.6421 - loglik: -7.1611e+02 - logprior: -5.3466e-01
Fitted a model with MAP estimate = -714.0421
expansions: [(205, 1), (236, 3), (246, 2), (247, 1)]
discards: [159 206 207 208 217 258 259 296 339 340]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 719.4804 - loglik: -7.1806e+02 - logprior: -1.4224e+00
Epoch 2/2
39/39 - 52s - loss: 715.3845 - loglik: -7.1518e+02 - logprior: -2.0287e-01
Fitted a model with MAP estimate = -713.0510
expansions: [(5, 1), (23, 1), (232, 1), (233, 1), (257, 1), (262, 1)]
discards: [214 235 236 237 238 239 245 296]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 716.8957 - loglik: -7.1578e+02 - logprior: -1.1171e+00
Epoch 2/10
39/39 - 52s - loss: 714.6883 - loglik: -7.1485e+02 - logprior: 0.1640
Epoch 3/10
39/39 - 52s - loss: 713.4755 - loglik: -7.1375e+02 - logprior: 0.2753
Epoch 4/10
39/39 - 52s - loss: 713.1383 - loglik: -7.1360e+02 - logprior: 0.4659
Epoch 5/10
39/39 - 52s - loss: 713.1011 - loglik: -7.1366e+02 - logprior: 0.5622
Epoch 6/10
39/39 - 52s - loss: 711.9626 - loglik: -7.1271e+02 - logprior: 0.7460
Epoch 7/10
39/39 - 52s - loss: 712.8621 - loglik: -7.1373e+02 - logprior: 0.8658
Fitted a model with MAP estimate = -712.0248
Time for alignment: 1052.8453
Computed alignments with likelihoods: ['-714.3867', '-708.2553', '-712.0248']
Best model has likelihood: -708.2553
SP score = 0.8115
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d90c56a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96906ad610>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 252.5238 - loglik: -2.4945e+02 - logprior: -3.0690e+00
Epoch 2/10
19/19 - 1s - loss: 221.5615 - loglik: -2.2029e+02 - logprior: -1.2760e+00
Epoch 3/10
19/19 - 2s - loss: 207.3903 - loglik: -2.0588e+02 - logprior: -1.5088e+00
Epoch 4/10
19/19 - 1s - loss: 204.8627 - loglik: -2.0345e+02 - logprior: -1.4083e+00
Epoch 5/10
19/19 - 2s - loss: 204.1807 - loglik: -2.0277e+02 - logprior: -1.4102e+00
Epoch 6/10
19/19 - 2s - loss: 203.9000 - loglik: -2.0251e+02 - logprior: -1.3944e+00
Epoch 7/10
19/19 - 2s - loss: 203.6594 - loglik: -2.0228e+02 - logprior: -1.3753e+00
Epoch 8/10
19/19 - 1s - loss: 203.5135 - loglik: -2.0214e+02 - logprior: -1.3709e+00
Epoch 9/10
19/19 - 2s - loss: 203.8663 - loglik: -2.0250e+02 - logprior: -1.3675e+00
Fitted a model with MAP estimate = -203.4192
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (26, 1), (40, 2), (41, 3), (43, 2), (48, 1), (49, 2), (50, 2), (59, 1), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 206.0528 - loglik: -2.0212e+02 - logprior: -3.9356e+00
Epoch 2/2
19/19 - 2s - loss: 199.1640 - loglik: -1.9703e+02 - logprior: -2.1378e+00
Fitted a model with MAP estimate = -197.8976
expansions: [(0, 2)]
discards: [ 0 10 49 52 66 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 198.6798 - loglik: -1.9579e+02 - logprior: -2.8856e+00
Epoch 2/2
19/19 - 2s - loss: 195.9989 - loglik: -1.9491e+02 - logprior: -1.0846e+00
Fitted a model with MAP estimate = -195.4396
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.9291 - loglik: -1.9621e+02 - logprior: -3.7154e+00
Epoch 2/10
19/19 - 2s - loss: 196.2347 - loglik: -1.9496e+02 - logprior: -1.2740e+00
Epoch 3/10
19/19 - 2s - loss: 195.5113 - loglik: -1.9449e+02 - logprior: -1.0187e+00
Epoch 4/10
19/19 - 2s - loss: 195.4525 - loglik: -1.9448e+02 - logprior: -9.7552e-01
Epoch 5/10
19/19 - 2s - loss: 195.1124 - loglik: -1.9418e+02 - logprior: -9.3632e-01
Epoch 6/10
19/19 - 2s - loss: 195.2318 - loglik: -1.9433e+02 - logprior: -9.0594e-01
Fitted a model with MAP estimate = -195.0864
Time for alignment: 56.6775
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.7632 - loglik: -2.4970e+02 - logprior: -3.0678e+00
Epoch 2/10
19/19 - 2s - loss: 221.7035 - loglik: -2.2046e+02 - logprior: -1.2420e+00
Epoch 3/10
19/19 - 2s - loss: 208.9623 - loglik: -2.0753e+02 - logprior: -1.4361e+00
Epoch 4/10
19/19 - 2s - loss: 206.4929 - loglik: -2.0515e+02 - logprior: -1.3392e+00
Epoch 5/10
19/19 - 2s - loss: 205.7383 - loglik: -2.0440e+02 - logprior: -1.3354e+00
Epoch 6/10
19/19 - 2s - loss: 205.3142 - loglik: -2.0400e+02 - logprior: -1.3176e+00
Epoch 7/10
19/19 - 2s - loss: 205.2353 - loglik: -2.0392e+02 - logprior: -1.3152e+00
Epoch 8/10
19/19 - 1s - loss: 205.3013 - loglik: -2.0400e+02 - logprior: -1.3037e+00
Fitted a model with MAP estimate = -204.9412
expansions: [(8, 1), (10, 2), (11, 2), (13, 1), (14, 1), (19, 1), (20, 1), (37, 2), (39, 1), (40, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 205.6922 - loglik: -2.0175e+02 - logprior: -3.9435e+00
Epoch 2/2
19/19 - 2s - loss: 198.9999 - loglik: -1.9689e+02 - logprior: -2.1078e+00
Fitted a model with MAP estimate = -197.7781
expansions: [(0, 2)]
discards: [ 0 10 45 52 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 198.6093 - loglik: -1.9572e+02 - logprior: -2.8872e+00
Epoch 2/2
19/19 - 2s - loss: 195.9823 - loglik: -1.9489e+02 - logprior: -1.0934e+00
Fitted a model with MAP estimate = -195.4397
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.9329 - loglik: -1.9622e+02 - logprior: -3.7122e+00
Epoch 2/10
19/19 - 2s - loss: 196.3000 - loglik: -1.9503e+02 - logprior: -1.2681e+00
Epoch 3/10
19/19 - 2s - loss: 195.4115 - loglik: -1.9440e+02 - logprior: -1.0161e+00
Epoch 4/10
19/19 - 2s - loss: 195.3727 - loglik: -1.9439e+02 - logprior: -9.8191e-01
Epoch 5/10
19/19 - 2s - loss: 195.1807 - loglik: -1.9425e+02 - logprior: -9.3198e-01
Epoch 6/10
19/19 - 2s - loss: 195.3206 - loglik: -1.9440e+02 - logprior: -9.1758e-01
Fitted a model with MAP estimate = -195.0942
Time for alignment: 53.7841
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.8317 - loglik: -2.4977e+02 - logprior: -3.0648e+00
Epoch 2/10
19/19 - 2s - loss: 220.7631 - loglik: -2.1949e+02 - logprior: -1.2689e+00
Epoch 3/10
19/19 - 2s - loss: 208.3154 - loglik: -2.0681e+02 - logprior: -1.5072e+00
Epoch 4/10
19/19 - 2s - loss: 205.1098 - loglik: -2.0369e+02 - logprior: -1.4194e+00
Epoch 5/10
19/19 - 1s - loss: 204.3886 - loglik: -2.0295e+02 - logprior: -1.4340e+00
Epoch 6/10
19/19 - 1s - loss: 203.8739 - loglik: -2.0246e+02 - logprior: -1.4119e+00
Epoch 7/10
19/19 - 1s - loss: 203.6649 - loglik: -2.0226e+02 - logprior: -1.4093e+00
Epoch 8/10
19/19 - 1s - loss: 203.9823 - loglik: -2.0258e+02 - logprior: -1.3989e+00
Fitted a model with MAP estimate = -203.5415
expansions: [(9, 1), (10, 1), (11, 1), (13, 2), (14, 1), (19, 1), (20, 1), (40, 1), (41, 2), (44, 2), (48, 1), (49, 2), (50, 1), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 205.7185 - loglik: -2.0177e+02 - logprior: -3.9455e+00
Epoch 2/2
19/19 - 2s - loss: 199.0429 - loglik: -1.9695e+02 - logprior: -2.0948e+00
Fitted a model with MAP estimate = -197.8370
expansions: [(0, 2)]
discards: [ 0 50 62 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.6272 - loglik: -1.9574e+02 - logprior: -2.8858e+00
Epoch 2/2
19/19 - 2s - loss: 195.9324 - loglik: -1.9485e+02 - logprior: -1.0849e+00
Fitted a model with MAP estimate = -195.4180
expansions: [(54, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 199.9507 - loglik: -1.9624e+02 - logprior: -3.7119e+00
Epoch 2/10
19/19 - 2s - loss: 196.1608 - loglik: -1.9489e+02 - logprior: -1.2741e+00
Epoch 3/10
19/19 - 2s - loss: 195.5730 - loglik: -1.9456e+02 - logprior: -1.0161e+00
Epoch 4/10
19/19 - 2s - loss: 195.2188 - loglik: -1.9424e+02 - logprior: -9.7644e-01
Epoch 5/10
19/19 - 2s - loss: 195.3204 - loglik: -1.9439e+02 - logprior: -9.3067e-01
Fitted a model with MAP estimate = -195.1075
Time for alignment: 51.1453
Computed alignments with likelihoods: ['-195.0864', '-195.0942', '-195.1075']
Best model has likelihood: -195.0864
SP score = 0.7489
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f953558a8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d8c2c970>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.6876 - loglik: -2.9865e+02 - logprior: -3.0333e+00
Epoch 2/10
19/19 - 2s - loss: 271.2673 - loglik: -2.7004e+02 - logprior: -1.2272e+00
Epoch 3/10
19/19 - 2s - loss: 258.8118 - loglik: -2.5761e+02 - logprior: -1.2012e+00
Epoch 4/10
19/19 - 2s - loss: 256.0797 - loglik: -2.5499e+02 - logprior: -1.0865e+00
Epoch 5/10
19/19 - 2s - loss: 255.2298 - loglik: -2.5418e+02 - logprior: -1.0546e+00
Epoch 6/10
19/19 - 2s - loss: 255.0843 - loglik: -2.5405e+02 - logprior: -1.0392e+00
Epoch 7/10
19/19 - 2s - loss: 254.2604 - loglik: -2.5322e+02 - logprior: -1.0436e+00
Epoch 8/10
19/19 - 2s - loss: 254.3433 - loglik: -2.5330e+02 - logprior: -1.0392e+00
Fitted a model with MAP estimate = -253.4941
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (52, 2), (53, 3), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.4821 - loglik: -2.5142e+02 - logprior: -4.0591e+00
Epoch 2/2
19/19 - 2s - loss: 250.4883 - loglik: -2.4920e+02 - logprior: -1.2867e+00
Fitted a model with MAP estimate = -249.0769
expansions: []
discards: [ 1 27 67 96 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.8019 - loglik: -2.4890e+02 - logprior: -2.9055e+00
Epoch 2/2
19/19 - 2s - loss: 249.5141 - loglik: -2.4838e+02 - logprior: -1.1351e+00
Fitted a model with MAP estimate = -248.5823
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 250.7264 - loglik: -2.4781e+02 - logprior: -2.9192e+00
Epoch 2/10
19/19 - 2s - loss: 248.9979 - loglik: -2.4792e+02 - logprior: -1.0776e+00
Epoch 3/10
19/19 - 2s - loss: 248.3693 - loglik: -2.4746e+02 - logprior: -9.0649e-01
Epoch 4/10
19/19 - 2s - loss: 248.3288 - loglik: -2.4750e+02 - logprior: -8.3337e-01
Epoch 5/10
19/19 - 2s - loss: 248.5178 - loglik: -2.4771e+02 - logprior: -8.0404e-01
Fitted a model with MAP estimate = -248.1438
Time for alignment: 62.4108
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.7943 - loglik: -2.9875e+02 - logprior: -3.0425e+00
Epoch 2/10
19/19 - 2s - loss: 273.0370 - loglik: -2.7181e+02 - logprior: -1.2233e+00
Epoch 3/10
19/19 - 2s - loss: 261.1557 - loglik: -2.5995e+02 - logprior: -1.2106e+00
Epoch 4/10
19/19 - 2s - loss: 258.3507 - loglik: -2.5726e+02 - logprior: -1.0886e+00
Epoch 5/10
19/19 - 2s - loss: 256.8617 - loglik: -2.5580e+02 - logprior: -1.0664e+00
Epoch 6/10
19/19 - 2s - loss: 255.2909 - loglik: -2.5422e+02 - logprior: -1.0682e+00
Epoch 7/10
19/19 - 2s - loss: 255.0673 - loglik: -2.5400e+02 - logprior: -1.0720e+00
Epoch 8/10
19/19 - 2s - loss: 254.7747 - loglik: -2.5370e+02 - logprior: -1.0744e+00
Epoch 9/10
19/19 - 2s - loss: 254.5657 - loglik: -2.5350e+02 - logprior: -1.0701e+00
Epoch 10/10
19/19 - 2s - loss: 254.4227 - loglik: -2.5336e+02 - logprior: -1.0653e+00
Fitted a model with MAP estimate = -253.9140
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (28, 1), (39, 2), (42, 3), (43, 2), (44, 1), (45, 1), (53, 3), (75, 3), (76, 2), (77, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.9283 - loglik: -2.5180e+02 - logprior: -4.1233e+00
Epoch 2/2
19/19 - 2s - loss: 250.5172 - loglik: -2.4923e+02 - logprior: -1.2915e+00
Fitted a model with MAP estimate = -249.2553
expansions: []
discards: [ 1 24 46 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.1938 - loglik: -2.4929e+02 - logprior: -2.9083e+00
Epoch 2/2
19/19 - 2s - loss: 250.0683 - loglik: -2.4894e+02 - logprior: -1.1234e+00
Fitted a model with MAP estimate = -248.9736
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 251.0659 - loglik: -2.4815e+02 - logprior: -2.9111e+00
Epoch 2/10
19/19 - 2s - loss: 249.3148 - loglik: -2.4826e+02 - logprior: -1.0578e+00
Epoch 3/10
19/19 - 2s - loss: 248.7627 - loglik: -2.4788e+02 - logprior: -8.7917e-01
Epoch 4/10
19/19 - 2s - loss: 248.5032 - loglik: -2.4768e+02 - logprior: -8.2005e-01
Epoch 5/10
19/19 - 2s - loss: 248.7098 - loglik: -2.4792e+02 - logprior: -7.8667e-01
Fitted a model with MAP estimate = -248.2764
Time for alignment: 63.6686
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.7444 - loglik: -2.9870e+02 - logprior: -3.0446e+00
Epoch 2/10
19/19 - 2s - loss: 271.5431 - loglik: -2.7032e+02 - logprior: -1.2262e+00
Epoch 3/10
19/19 - 2s - loss: 259.3584 - loglik: -2.5815e+02 - logprior: -1.2114e+00
Epoch 4/10
19/19 - 2s - loss: 256.5519 - loglik: -2.5548e+02 - logprior: -1.0756e+00
Epoch 5/10
19/19 - 2s - loss: 255.5515 - loglik: -2.5448e+02 - logprior: -1.0713e+00
Epoch 6/10
19/19 - 2s - loss: 255.0969 - loglik: -2.5403e+02 - logprior: -1.0640e+00
Epoch 7/10
19/19 - 2s - loss: 254.4326 - loglik: -2.5338e+02 - logprior: -1.0535e+00
Epoch 8/10
19/19 - 2s - loss: 254.6863 - loglik: -2.5363e+02 - logprior: -1.0530e+00
Fitted a model with MAP estimate = -253.7712
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (41, 3), (42, 2), (52, 1), (53, 2), (54, 1), (74, 1), (75, 1), (76, 2), (77, 4), (78, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.3923 - loglik: -2.5136e+02 - logprior: -4.0345e+00
Epoch 2/2
19/19 - 2s - loss: 250.3270 - loglik: -2.4910e+02 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -248.8500
expansions: []
discards: [ 1 27 70 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.5650 - loglik: -2.4868e+02 - logprior: -2.8854e+00
Epoch 2/2
19/19 - 2s - loss: 249.4763 - loglik: -2.4837e+02 - logprior: -1.1058e+00
Fitted a model with MAP estimate = -248.4574
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 250.6938 - loglik: -2.4780e+02 - logprior: -2.8908e+00
Epoch 2/10
19/19 - 2s - loss: 248.7321 - loglik: -2.4769e+02 - logprior: -1.0436e+00
Epoch 3/10
19/19 - 2s - loss: 248.0860 - loglik: -2.4723e+02 - logprior: -8.6052e-01
Epoch 4/10
19/19 - 2s - loss: 248.2024 - loglik: -2.4741e+02 - logprior: -7.9406e-01
Fitted a model with MAP estimate = -248.0595
Time for alignment: 59.1780
Computed alignments with likelihoods: ['-248.1438', '-248.2764', '-248.0595']
Best model has likelihood: -248.0595
SP score = 0.8792
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96304f51f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95462aa610>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3873 - loglik: -1.8319e+02 - logprior: -3.1952e+00
Epoch 2/10
19/19 - 1s - loss: 154.3106 - loglik: -1.5286e+02 - logprior: -1.4464e+00
Epoch 3/10
19/19 - 1s - loss: 142.4165 - loglik: -1.4079e+02 - logprior: -1.6274e+00
Epoch 4/10
19/19 - 1s - loss: 139.8947 - loglik: -1.3838e+02 - logprior: -1.5114e+00
Epoch 5/10
19/19 - 1s - loss: 138.7080 - loglik: -1.3719e+02 - logprior: -1.5187e+00
Epoch 6/10
19/19 - 1s - loss: 138.1818 - loglik: -1.3668e+02 - logprior: -1.4994e+00
Epoch 7/10
19/19 - 1s - loss: 137.9585 - loglik: -1.3646e+02 - logprior: -1.4953e+00
Epoch 8/10
19/19 - 1s - loss: 137.9397 - loglik: -1.3645e+02 - logprior: -1.4860e+00
Epoch 9/10
19/19 - 1s - loss: 137.7698 - loglik: -1.3629e+02 - logprior: -1.4831e+00
Epoch 10/10
19/19 - 1s - loss: 137.8974 - loglik: -1.3642e+02 - logprior: -1.4809e+00
Fitted a model with MAP estimate = -137.6061
expansions: [(12, 1), (16, 2), (17, 2), (18, 1), (21, 1), (22, 2), (23, 1), (24, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 138.3059 - loglik: -1.3418e+02 - logprior: -4.1290e+00
Epoch 2/2
19/19 - 1s - loss: 129.2039 - loglik: -1.2711e+02 - logprior: -2.0957e+00
Fitted a model with MAP estimate = -127.7726
expansions: [(0, 2)]
discards: [ 0 22 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 128.1514 - loglik: -1.2514e+02 - logprior: -3.0118e+00
Epoch 2/2
19/19 - 1s - loss: 125.1625 - loglik: -1.2397e+02 - logprior: -1.1931e+00
Fitted a model with MAP estimate = -124.8050
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.0629 - loglik: -1.2626e+02 - logprior: -3.8003e+00
Epoch 2/10
19/19 - 1s - loss: 125.9642 - loglik: -1.2458e+02 - logprior: -1.3847e+00
Epoch 3/10
19/19 - 1s - loss: 125.4330 - loglik: -1.2423e+02 - logprior: -1.2028e+00
Epoch 4/10
19/19 - 1s - loss: 125.0874 - loglik: -1.2393e+02 - logprior: -1.1568e+00
Epoch 5/10
19/19 - 1s - loss: 125.4417 - loglik: -1.2431e+02 - logprior: -1.1300e+00
Fitted a model with MAP estimate = -125.0979
Time for alignment: 43.7297
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.1097 - loglik: -1.8291e+02 - logprior: -3.1948e+00
Epoch 2/10
19/19 - 1s - loss: 153.1833 - loglik: -1.5177e+02 - logprior: -1.4119e+00
Epoch 3/10
19/19 - 1s - loss: 142.6856 - loglik: -1.4113e+02 - logprior: -1.5539e+00
Epoch 4/10
19/19 - 1s - loss: 139.8563 - loglik: -1.3839e+02 - logprior: -1.4653e+00
Epoch 5/10
19/19 - 1s - loss: 139.1873 - loglik: -1.3769e+02 - logprior: -1.4955e+00
Epoch 6/10
19/19 - 1s - loss: 138.5099 - loglik: -1.3703e+02 - logprior: -1.4812e+00
Epoch 7/10
19/19 - 1s - loss: 138.2722 - loglik: -1.3680e+02 - logprior: -1.4688e+00
Epoch 8/10
19/19 - 1s - loss: 138.4333 - loglik: -1.3697e+02 - logprior: -1.4654e+00
Fitted a model with MAP estimate = -138.0261
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 5), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 137.4359 - loglik: -1.3332e+02 - logprior: -4.1157e+00
Epoch 2/2
19/19 - 1s - loss: 128.6257 - loglik: -1.2657e+02 - logprior: -2.0556e+00
Fitted a model with MAP estimate = -127.2928
expansions: [(0, 2)]
discards: [ 0 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 127.5882 - loglik: -1.2458e+02 - logprior: -3.0090e+00
Epoch 2/2
19/19 - 1s - loss: 125.0609 - loglik: -1.2387e+02 - logprior: -1.1958e+00
Fitted a model with MAP estimate = -124.5563
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 129.6626 - loglik: -1.2587e+02 - logprior: -3.7950e+00
Epoch 2/10
19/19 - 1s - loss: 125.7281 - loglik: -1.2433e+02 - logprior: -1.3986e+00
Epoch 3/10
19/19 - 1s - loss: 125.0196 - loglik: -1.2381e+02 - logprior: -1.2069e+00
Epoch 4/10
19/19 - 1s - loss: 125.0043 - loglik: -1.2384e+02 - logprior: -1.1626e+00
Epoch 5/10
19/19 - 1s - loss: 124.8018 - loglik: -1.2367e+02 - logprior: -1.1345e+00
Epoch 6/10
19/19 - 1s - loss: 125.0472 - loglik: -1.2394e+02 - logprior: -1.1089e+00
Fitted a model with MAP estimate = -124.7910
Time for alignment: 43.5888
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.4318 - loglik: -1.8324e+02 - logprior: -3.1961e+00
Epoch 2/10
19/19 - 1s - loss: 155.0469 - loglik: -1.5361e+02 - logprior: -1.4325e+00
Epoch 3/10
19/19 - 1s - loss: 141.6703 - loglik: -1.4004e+02 - logprior: -1.6258e+00
Epoch 4/10
19/19 - 1s - loss: 138.8401 - loglik: -1.3732e+02 - logprior: -1.5205e+00
Epoch 5/10
19/19 - 1s - loss: 137.7015 - loglik: -1.3615e+02 - logprior: -1.5541e+00
Epoch 6/10
19/19 - 1s - loss: 137.3475 - loglik: -1.3581e+02 - logprior: -1.5419e+00
Epoch 7/10
19/19 - 1s - loss: 137.0401 - loglik: -1.3550e+02 - logprior: -1.5422e+00
Epoch 8/10
19/19 - 1s - loss: 136.9847 - loglik: -1.3545e+02 - logprior: -1.5305e+00
Epoch 9/10
19/19 - 1s - loss: 136.9572 - loglik: -1.3543e+02 - logprior: -1.5296e+00
Epoch 10/10
19/19 - 1s - loss: 136.4359 - loglik: -1.3491e+02 - logprior: -1.5260e+00
Fitted a model with MAP estimate = -136.6364
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 137.2100 - loglik: -1.3309e+02 - logprior: -4.1237e+00
Epoch 2/2
19/19 - 1s - loss: 128.5677 - loglik: -1.2649e+02 - logprior: -2.0800e+00
Fitted a model with MAP estimate = -127.4445
expansions: [(0, 2)]
discards: [ 0 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 127.6746 - loglik: -1.2466e+02 - logprior: -3.0131e+00
Epoch 2/2
19/19 - 1s - loss: 124.7892 - loglik: -1.2360e+02 - logprior: -1.1925e+00
Fitted a model with MAP estimate = -124.5812
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 129.6416 - loglik: -1.2584e+02 - logprior: -3.8020e+00
Epoch 2/10
19/19 - 1s - loss: 125.7382 - loglik: -1.2435e+02 - logprior: -1.3927e+00
Epoch 3/10
19/19 - 1s - loss: 125.1119 - loglik: -1.2391e+02 - logprior: -1.2040e+00
Epoch 4/10
19/19 - 1s - loss: 124.9844 - loglik: -1.2382e+02 - logprior: -1.1608e+00
Epoch 5/10
19/19 - 1s - loss: 125.0068 - loglik: -1.2388e+02 - logprior: -1.1281e+00
Fitted a model with MAP estimate = -124.8279
Time for alignment: 43.3229
Computed alignments with likelihoods: ['-124.8050', '-124.5563', '-124.5812']
Best model has likelihood: -124.5563
SP score = 0.9702
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9616566fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f962fbb97f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 788.7703 - loglik: -7.8637e+02 - logprior: -2.4043e+00
Epoch 2/10
19/19 - 16s - loss: 676.7838 - loglik: -6.7585e+02 - logprior: -9.3677e-01
Epoch 3/10
19/19 - 16s - loss: 624.9256 - loglik: -6.2342e+02 - logprior: -1.5041e+00
Epoch 4/10
19/19 - 16s - loss: 607.7169 - loglik: -6.0611e+02 - logprior: -1.6103e+00
Epoch 5/10
19/19 - 16s - loss: 603.3072 - loglik: -6.0158e+02 - logprior: -1.7308e+00
Epoch 6/10
19/19 - 16s - loss: 600.3423 - loglik: -5.9862e+02 - logprior: -1.7240e+00
Epoch 7/10
19/19 - 16s - loss: 598.6408 - loglik: -5.9690e+02 - logprior: -1.7377e+00
Epoch 8/10
19/19 - 16s - loss: 598.0746 - loglik: -5.9634e+02 - logprior: -1.7358e+00
Epoch 9/10
19/19 - 16s - loss: 598.1875 - loglik: -5.9644e+02 - logprior: -1.7447e+00
Fitted a model with MAP estimate = -597.0410
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (33, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (72, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 2), (135, 1), (148, 1), (149, 1), (153, 1), (154, 1), (155, 1), (163, 1), (164, 6), (173, 2), (176, 1), (177, 1), (179, 1), (185, 1), (186, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 579.9014 - loglik: -5.7726e+02 - logprior: -2.6426e+00
Epoch 2/2
39/39 - 25s - loss: 566.4443 - loglik: -5.6485e+02 - logprior: -1.5915e+00
Fitted a model with MAP estimate = -562.2903
expansions: [(0, 3), (149, 1), (206, 1), (207, 1)]
discards: [  0  86 163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 563.5155 - loglik: -5.6189e+02 - logprior: -1.6269e+00
Epoch 2/2
39/39 - 26s - loss: 560.8310 - loglik: -5.6030e+02 - logprior: -5.3211e-01
Fitted a model with MAP estimate = -559.1760
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 564.5331 - loglik: -5.6226e+02 - logprior: -2.2758e+00
Epoch 2/10
39/39 - 26s - loss: 560.9414 - loglik: -5.6051e+02 - logprior: -4.3522e-01
Epoch 3/10
39/39 - 26s - loss: 559.8549 - loglik: -5.5982e+02 - logprior: -3.7768e-02
Epoch 4/10
39/39 - 26s - loss: 558.9247 - loglik: -5.5905e+02 - logprior: 0.1222
Epoch 5/10
39/39 - 26s - loss: 558.9966 - loglik: -5.5928e+02 - logprior: 0.2789
Fitted a model with MAP estimate = -558.7762
Time for alignment: 518.6161
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 788.9524 - loglik: -7.8655e+02 - logprior: -2.3992e+00
Epoch 2/10
19/19 - 16s - loss: 676.3704 - loglik: -6.7545e+02 - logprior: -9.2323e-01
Epoch 3/10
19/19 - 16s - loss: 624.5939 - loglik: -6.2304e+02 - logprior: -1.5492e+00
Epoch 4/10
19/19 - 16s - loss: 609.0706 - loglik: -6.0739e+02 - logprior: -1.6785e+00
Epoch 5/10
19/19 - 16s - loss: 603.6725 - loglik: -6.0193e+02 - logprior: -1.7443e+00
Epoch 6/10
19/19 - 16s - loss: 599.7229 - loglik: -5.9802e+02 - logprior: -1.6986e+00
Epoch 7/10
19/19 - 16s - loss: 599.7581 - loglik: -5.9808e+02 - logprior: -1.6823e+00
Fitted a model with MAP estimate = -597.4254
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 2), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 1), (72, 1), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (135, 2), (136, 2), (137, 2), (140, 1), (146, 1), (149, 1), (154, 1), (163, 1), (164, 2), (173, 2), (176, 1), (177, 1), (179, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 581.8971 - loglik: -5.7932e+02 - logprior: -2.5736e+00
Epoch 2/2
39/39 - 25s - loss: 568.4792 - loglik: -5.6702e+02 - logprior: -1.4615e+00
Fitted a model with MAP estimate = -564.2345
expansions: [(147, 1), (179, 1), (190, 1), (192, 1), (196, 4)]
discards: [171 272]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 563.9664 - loglik: -5.6226e+02 - logprior: -1.7087e+00
Epoch 2/2
39/39 - 26s - loss: 561.0420 - loglik: -5.6051e+02 - logprior: -5.3301e-01
Fitted a model with MAP estimate = -559.3876
expansions: [(202, 2)]
discards: [191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 561.1975 - loglik: -5.5962e+02 - logprior: -1.5793e+00
Epoch 2/10
39/39 - 26s - loss: 558.9899 - loglik: -5.5869e+02 - logprior: -3.0211e-01
Epoch 3/10
39/39 - 26s - loss: 557.5264 - loglik: -5.5741e+02 - logprior: -1.1825e-01
Epoch 4/10
39/39 - 26s - loss: 558.1396 - loglik: -5.5818e+02 - logprior: 0.0419
Fitted a model with MAP estimate = -557.1205
Time for alignment: 463.3285
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 789.3582 - loglik: -7.8695e+02 - logprior: -2.4041e+00
Epoch 2/10
19/19 - 16s - loss: 677.1342 - loglik: -6.7621e+02 - logprior: -9.2700e-01
Epoch 3/10
19/19 - 16s - loss: 622.5601 - loglik: -6.2098e+02 - logprior: -1.5814e+00
Epoch 4/10
19/19 - 16s - loss: 605.5435 - loglik: -6.0377e+02 - logprior: -1.7727e+00
Epoch 5/10
19/19 - 16s - loss: 601.4073 - loglik: -5.9960e+02 - logprior: -1.8120e+00
Epoch 6/10
19/19 - 16s - loss: 598.8895 - loglik: -5.9709e+02 - logprior: -1.7972e+00
Epoch 7/10
19/19 - 16s - loss: 597.2957 - loglik: -5.9547e+02 - logprior: -1.8207e+00
Epoch 8/10
19/19 - 16s - loss: 595.7278 - loglik: -5.9389e+02 - logprior: -1.8337e+00
Epoch 9/10
19/19 - 16s - loss: 595.8225 - loglik: -5.9399e+02 - logprior: -1.8343e+00
Fitted a model with MAP estimate = -595.0411
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (21, 1), (32, 1), (33, 1), (35, 1), (41, 1), (45, 1), (46, 1), (51, 1), (52, 1), (66, 1), (67, 1), (68, 2), (70, 1), (76, 1), (86, 1), (112, 1), (114, 1), (115, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (149, 1), (153, 2), (162, 4), (164, 2), (173, 2), (176, 1), (177, 1), (179, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [ 0 37 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 579.8404 - loglik: -5.7725e+02 - logprior: -2.5906e+00
Epoch 2/2
39/39 - 25s - loss: 567.2521 - loglik: -5.6575e+02 - logprior: -1.5011e+00
Fitted a model with MAP estimate = -563.9191
expansions: [(0, 3), (149, 1), (195, 2), (205, 2)]
discards: [ 0 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 563.0790 - loglik: -5.6152e+02 - logprior: -1.5572e+00
Epoch 2/2
39/39 - 26s - loss: 559.5388 - loglik: -5.5900e+02 - logprior: -5.4092e-01
Fitted a model with MAP estimate = -557.8730
expansions: [(209, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 563.2542 - loglik: -5.6098e+02 - logprior: -2.2731e+00
Epoch 2/10
39/39 - 26s - loss: 559.3073 - loglik: -5.5882e+02 - logprior: -4.8528e-01
Epoch 3/10
39/39 - 26s - loss: 558.0497 - loglik: -5.5802e+02 - logprior: -3.0571e-02
Epoch 4/10
39/39 - 26s - loss: 558.0668 - loglik: -5.5820e+02 - logprior: 0.1329
Fitted a model with MAP estimate = -557.3540
Time for alignment: 495.4526
Computed alignments with likelihoods: ['-558.7762', '-557.1205', '-557.3540']
Best model has likelihood: -557.1205
SP score = 0.3068
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9546364190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95240e7220>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 971.2784 - loglik: -9.6992e+02 - logprior: -1.3578e+00
Epoch 2/10
39/39 - 33s - loss: 811.0572 - loglik: -8.0994e+02 - logprior: -1.1198e+00
Epoch 3/10
39/39 - 33s - loss: 798.6109 - loglik: -7.9739e+02 - logprior: -1.2229e+00
Epoch 4/10
39/39 - 33s - loss: 796.3030 - loglik: -7.9507e+02 - logprior: -1.2283e+00
Epoch 5/10
39/39 - 33s - loss: 794.8068 - loglik: -7.9355e+02 - logprior: -1.2599e+00
Epoch 6/10
39/39 - 33s - loss: 794.5545 - loglik: -7.9328e+02 - logprior: -1.2791e+00
Epoch 7/10
39/39 - 33s - loss: 794.4583 - loglik: -7.9319e+02 - logprior: -1.2703e+00
Epoch 8/10
39/39 - 33s - loss: 793.9842 - loglik: -7.9269e+02 - logprior: -1.2964e+00
Epoch 9/10
39/39 - 33s - loss: 793.9058 - loglik: -7.9262e+02 - logprior: -1.2864e+00
Epoch 10/10
39/39 - 33s - loss: 794.5076 - loglik: -7.9321e+02 - logprior: -1.3001e+00
Fitted a model with MAP estimate = -785.5179
expansions: [(0, 3), (15, 1), (48, 1), (52, 1), (53, 1), (57, 1), (70, 1), (71, 2), (75, 2), (99, 1), (120, 2), (121, 1), (123, 1), (128, 1), (146, 3), (148, 1), (167, 1), (171, 1), (176, 1), (177, 1), (186, 1), (193, 2), (196, 1), (197, 3), (218, 1), (219, 1), (220, 1), (223, 2), (224, 3), (247, 1), (249, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (264, 1), (285, 4), (286, 3), (287, 3), (288, 1), (289, 1), (296, 3), (297, 2), (298, 2), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 771.6689 - loglik: -7.6942e+02 - logprior: -2.2536e+00
Epoch 2/2
39/39 - 47s - loss: 755.2822 - loglik: -7.5465e+02 - logprior: -6.3325e-01
Fitted a model with MAP estimate = -746.1679
expansions: [(0, 2)]
discards: [  1   2  86 135 221 229 338 339 340 341 367 368 392 393]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 759.9647 - loglik: -7.5771e+02 - logprior: -2.2529e+00
Epoch 2/2
39/39 - 45s - loss: 756.7181 - loglik: -7.5664e+02 - logprior: -7.6074e-02
Fitted a model with MAP estimate = -748.0317
expansions: [(0, 2), (335, 2), (336, 1), (382, 1)]
discards: [  1   2 261]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 750.1525 - loglik: -7.4813e+02 - logprior: -2.0275e+00
Epoch 2/10
39/39 - 46s - loss: 746.8719 - loglik: -7.4701e+02 - logprior: 0.1420
Epoch 3/10
39/39 - 46s - loss: 746.3856 - loglik: -7.4684e+02 - logprior: 0.4516
Epoch 4/10
39/39 - 45s - loss: 746.4220 - loglik: -7.4705e+02 - logprior: 0.6326
Fitted a model with MAP estimate = -745.1106
Time for alignment: 956.9011
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 971.6691 - loglik: -9.7032e+02 - logprior: -1.3517e+00
Epoch 2/10
39/39 - 33s - loss: 809.9109 - loglik: -8.0876e+02 - logprior: -1.1475e+00
Epoch 3/10
39/39 - 33s - loss: 798.4487 - loglik: -7.9724e+02 - logprior: -1.2110e+00
Epoch 4/10
39/39 - 33s - loss: 795.7184 - loglik: -7.9452e+02 - logprior: -1.1939e+00
Epoch 5/10
39/39 - 33s - loss: 794.8221 - loglik: -7.9361e+02 - logprior: -1.2146e+00
Epoch 6/10
39/39 - 33s - loss: 794.4355 - loglik: -7.9321e+02 - logprior: -1.2218e+00
Epoch 7/10
39/39 - 33s - loss: 794.0107 - loglik: -7.9277e+02 - logprior: -1.2381e+00
Epoch 8/10
39/39 - 33s - loss: 794.1232 - loglik: -7.9292e+02 - logprior: -1.2041e+00
Fitted a model with MAP estimate = -785.5626
expansions: [(0, 2), (6, 1), (20, 1), (44, 1), (51, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (100, 1), (105, 1), (120, 2), (121, 2), (122, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (176, 1), (177, 1), (178, 1), (193, 2), (196, 1), (197, 3), (219, 1), (220, 1), (221, 1), (223, 2), (224, 3), (244, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (263, 1), (266, 2), (285, 1), (286, 2), (287, 8), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 772.1951 - loglik: -7.7008e+02 - logprior: -2.1107e+00
Epoch 2/2
39/39 - 47s - loss: 757.6047 - loglik: -7.5703e+02 - logprior: -5.7948e-01
Fitted a model with MAP estimate = -747.9670
expansions: [(0, 2)]
discards: [  1   2   3 134 168 221 229 264 319 388]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 759.7672 - loglik: -7.5751e+02 - logprior: -2.2585e+00
Epoch 2/2
39/39 - 45s - loss: 756.7803 - loglik: -7.5664e+02 - logprior: -1.4155e-01
Fitted a model with MAP estimate = -747.9092
expansions: [(0, 2), (258, 1), (338, 2)]
discards: [1 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 750.3449 - loglik: -7.4848e+02 - logprior: -1.8603e+00
Epoch 2/10
39/39 - 46s - loss: 748.1750 - loglik: -7.4827e+02 - logprior: 0.0905
Epoch 3/10
39/39 - 46s - loss: 747.3263 - loglik: -7.4768e+02 - logprior: 0.3504
Epoch 4/10
39/39 - 46s - loss: 746.4357 - loglik: -7.4693e+02 - logprior: 0.4956
Epoch 5/10
39/39 - 46s - loss: 747.2814 - loglik: -7.4796e+02 - logprior: 0.6770
Fitted a model with MAP estimate = -745.8880
Time for alignment: 936.0358
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 972.9853 - loglik: -9.7167e+02 - logprior: -1.3170e+00
Epoch 2/10
39/39 - 33s - loss: 810.9619 - loglik: -8.0985e+02 - logprior: -1.1145e+00
Epoch 3/10
39/39 - 33s - loss: 796.2709 - loglik: -7.9500e+02 - logprior: -1.2700e+00
Epoch 4/10
39/39 - 33s - loss: 793.1339 - loglik: -7.9189e+02 - logprior: -1.2462e+00
Epoch 5/10
39/39 - 33s - loss: 792.0474 - loglik: -7.9082e+02 - logprior: -1.2322e+00
Epoch 6/10
39/39 - 33s - loss: 791.8547 - loglik: -7.9057e+02 - logprior: -1.2824e+00
Epoch 7/10
39/39 - 33s - loss: 791.4866 - loglik: -7.9021e+02 - logprior: -1.2720e+00
Epoch 8/10
39/39 - 33s - loss: 791.2817 - loglik: -7.9000e+02 - logprior: -1.2828e+00
Epoch 9/10
39/39 - 33s - loss: 791.4370 - loglik: -7.9019e+02 - logprior: -1.2517e+00
Fitted a model with MAP estimate = -782.9319
expansions: [(0, 3), (48, 1), (52, 1), (53, 1), (57, 1), (68, 1), (70, 1), (71, 1), (75, 2), (99, 1), (104, 1), (119, 1), (120, 2), (122, 1), (131, 1), (146, 2), (147, 1), (148, 1), (167, 1), (170, 1), (176, 1), (177, 1), (189, 1), (193, 2), (196, 1), (197, 1), (200, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (227, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (262, 1), (266, 2), (285, 1), (286, 2), (287, 8), (295, 3), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 771.7822 - loglik: -7.6950e+02 - logprior: -2.2830e+00
Epoch 2/2
39/39 - 47s - loss: 757.0491 - loglik: -7.5635e+02 - logprior: -6.9801e-01
Fitted a model with MAP estimate = -747.6553
expansions: []
discards: [  0  85 135 168 221 266 319 388]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 758.9457 - loglik: -7.5677e+02 - logprior: -2.1762e+00
Epoch 2/2
39/39 - 45s - loss: 756.2172 - loglik: -7.5612e+02 - logprior: -9.3306e-02
Fitted a model with MAP estimate = -747.1298
expansions: [(0, 2), (338, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 748.9717 - loglik: -7.4758e+02 - logprior: -1.3885e+00
Epoch 2/10
39/39 - 46s - loss: 747.4173 - loglik: -7.4761e+02 - logprior: 0.1900
Epoch 3/10
39/39 - 46s - loss: 745.6116 - loglik: -7.4604e+02 - logprior: 0.4305
Epoch 4/10
39/39 - 46s - loss: 746.4982 - loglik: -7.4713e+02 - logprior: 0.6360
Fitted a model with MAP estimate = -745.5031
Time for alignment: 926.7856
Computed alignments with likelihoods: ['-745.1106', '-745.8880', '-745.5031']
Best model has likelihood: -745.1106
SP score = 0.4554
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966bb1fbe0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966b904790>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 133.6889 - loglik: -1.3047e+02 - logprior: -3.2207e+00
Epoch 2/10
19/19 - 1s - loss: 109.4366 - loglik: -1.0804e+02 - logprior: -1.4007e+00
Epoch 3/10
19/19 - 1s - loss: 101.2668 - loglik: -9.9714e+01 - logprior: -1.5525e+00
Epoch 4/10
19/19 - 1s - loss: 99.0490 - loglik: -9.7604e+01 - logprior: -1.4448e+00
Epoch 5/10
19/19 - 1s - loss: 98.4283 - loglik: -9.7009e+01 - logprior: -1.4197e+00
Epoch 6/10
19/19 - 1s - loss: 98.2633 - loglik: -9.6861e+01 - logprior: -1.4026e+00
Epoch 7/10
19/19 - 1s - loss: 98.1791 - loglik: -9.6794e+01 - logprior: -1.3851e+00
Epoch 8/10
19/19 - 1s - loss: 98.1347 - loglik: -9.6759e+01 - logprior: -1.3756e+00
Epoch 9/10
19/19 - 1s - loss: 98.0089 - loglik: -9.6640e+01 - logprior: -1.3685e+00
Epoch 10/10
19/19 - 1s - loss: 98.1630 - loglik: -9.6798e+01 - logprior: -1.3647e+00
Fitted a model with MAP estimate = -97.9423
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.5800 - loglik: -9.7387e+01 - logprior: -4.1927e+00
Epoch 2/2
19/19 - 1s - loss: 94.7943 - loglik: -9.2656e+01 - logprior: -2.1381e+00
Fitted a model with MAP estimate = -93.6212
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 94.8040 - loglik: -9.1721e+01 - logprior: -3.0829e+00
Epoch 2/2
19/19 - 1s - loss: 92.1444 - loglik: -9.0870e+01 - logprior: -1.2743e+00
Fitted a model with MAP estimate = -91.8644
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 96.0583 - loglik: -9.2525e+01 - logprior: -3.5334e+00
Epoch 2/10
19/19 - 1s - loss: 92.8557 - loglik: -9.1397e+01 - logprior: -1.4585e+00
Epoch 3/10
19/19 - 1s - loss: 92.5409 - loglik: -9.1176e+01 - logprior: -1.3651e+00
Epoch 4/10
19/19 - 1s - loss: 92.5083 - loglik: -9.1197e+01 - logprior: -1.3112e+00
Epoch 5/10
19/19 - 1s - loss: 92.3682 - loglik: -9.1086e+01 - logprior: -1.2817e+00
Epoch 6/10
19/19 - 1s - loss: 92.3590 - loglik: -9.1097e+01 - logprior: -1.2623e+00
Epoch 7/10
19/19 - 1s - loss: 92.3572 - loglik: -9.1108e+01 - logprior: -1.2489e+00
Epoch 8/10
19/19 - 1s - loss: 92.3858 - loglik: -9.1157e+01 - logprior: -1.2286e+00
Fitted a model with MAP estimate = -92.2746
Time for alignment: 40.2107
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 133.6556 - loglik: -1.3043e+02 - logprior: -3.2245e+00
Epoch 2/10
19/19 - 1s - loss: 109.3152 - loglik: -1.0791e+02 - logprior: -1.4033e+00
Epoch 3/10
19/19 - 1s - loss: 101.5546 - loglik: -1.0000e+02 - logprior: -1.5532e+00
Epoch 4/10
19/19 - 1s - loss: 99.1542 - loglik: -9.7722e+01 - logprior: -1.4322e+00
Epoch 5/10
19/19 - 1s - loss: 98.4816 - loglik: -9.7065e+01 - logprior: -1.4164e+00
Epoch 6/10
19/19 - 1s - loss: 98.1869 - loglik: -9.6786e+01 - logprior: -1.4010e+00
Epoch 7/10
19/19 - 1s - loss: 98.1454 - loglik: -9.6761e+01 - logprior: -1.3840e+00
Epoch 8/10
19/19 - 1s - loss: 98.1820 - loglik: -9.6806e+01 - logprior: -1.3756e+00
Fitted a model with MAP estimate = -97.9938
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.5342 - loglik: -9.7346e+01 - logprior: -4.1882e+00
Epoch 2/2
19/19 - 1s - loss: 94.7356 - loglik: -9.2629e+01 - logprior: -2.1071e+00
Fitted a model with MAP estimate = -93.5481
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 94.8085 - loglik: -9.1722e+01 - logprior: -3.0862e+00
Epoch 2/2
19/19 - 1s - loss: 92.1886 - loglik: -9.0913e+01 - logprior: -1.2760e+00
Fitted a model with MAP estimate = -91.8742
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 95.9949 - loglik: -9.2472e+01 - logprior: -3.5227e+00
Epoch 2/10
19/19 - 1s - loss: 92.8419 - loglik: -9.1385e+01 - logprior: -1.4572e+00
Epoch 3/10
19/19 - 1s - loss: 92.5731 - loglik: -9.1207e+01 - logprior: -1.3665e+00
Epoch 4/10
19/19 - 1s - loss: 92.5379 - loglik: -9.1226e+01 - logprior: -1.3122e+00
Epoch 5/10
19/19 - 1s - loss: 92.3237 - loglik: -9.1042e+01 - logprior: -1.2813e+00
Epoch 6/10
19/19 - 1s - loss: 92.4737 - loglik: -9.1208e+01 - logprior: -1.2658e+00
Fitted a model with MAP estimate = -92.3261
Time for alignment: 34.5991
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.6854 - loglik: -1.3046e+02 - logprior: -3.2245e+00
Epoch 2/10
19/19 - 1s - loss: 108.8481 - loglik: -1.0744e+02 - logprior: -1.4039e+00
Epoch 3/10
19/19 - 1s - loss: 100.8110 - loglik: -9.9267e+01 - logprior: -1.5444e+00
Epoch 4/10
19/19 - 1s - loss: 98.9487 - loglik: -9.7506e+01 - logprior: -1.4425e+00
Epoch 5/10
19/19 - 1s - loss: 98.3532 - loglik: -9.6942e+01 - logprior: -1.4107e+00
Epoch 6/10
19/19 - 1s - loss: 98.2449 - loglik: -9.6847e+01 - logprior: -1.3978e+00
Epoch 7/10
19/19 - 1s - loss: 98.2167 - loglik: -9.6834e+01 - logprior: -1.3831e+00
Epoch 8/10
19/19 - 1s - loss: 98.0391 - loglik: -9.6666e+01 - logprior: -1.3736e+00
Epoch 9/10
19/19 - 1s - loss: 98.1888 - loglik: -9.6819e+01 - logprior: -1.3697e+00
Fitted a model with MAP estimate = -97.9710
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.5445 - loglik: -9.7357e+01 - logprior: -4.1871e+00
Epoch 2/2
19/19 - 1s - loss: 94.7745 - loglik: -9.2641e+01 - logprior: -2.1339e+00
Fitted a model with MAP estimate = -93.5883
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 94.7520 - loglik: -9.1666e+01 - logprior: -3.0861e+00
Epoch 2/2
19/19 - 1s - loss: 92.2049 - loglik: -9.0929e+01 - logprior: -1.2758e+00
Fitted a model with MAP estimate = -91.8688
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.9773 - loglik: -9.2445e+01 - logprior: -3.5326e+00
Epoch 2/10
19/19 - 1s - loss: 92.8117 - loglik: -9.1353e+01 - logprior: -1.4588e+00
Epoch 3/10
19/19 - 1s - loss: 92.5845 - loglik: -9.1219e+01 - logprior: -1.3651e+00
Epoch 4/10
19/19 - 1s - loss: 92.5183 - loglik: -9.1205e+01 - logprior: -1.3131e+00
Epoch 5/10
19/19 - 1s - loss: 92.4104 - loglik: -9.1128e+01 - logprior: -1.2828e+00
Epoch 6/10
19/19 - 1s - loss: 92.3156 - loglik: -9.1052e+01 - logprior: -1.2632e+00
Epoch 7/10
19/19 - 1s - loss: 92.4493 - loglik: -9.1200e+01 - logprior: -1.2488e+00
Fitted a model with MAP estimate = -92.3057
Time for alignment: 37.0302
Computed alignments with likelihoods: ['-91.8644', '-91.8742', '-91.8688']
Best model has likelihood: -91.8644
SP score = 0.8682
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d8817130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e2657820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.0879 - loglik: -5.2235e+02 - logprior: -2.7400e+00
Epoch 2/10
19/19 - 6s - loss: 424.3475 - loglik: -4.2341e+02 - logprior: -9.3703e-01
Epoch 3/10
19/19 - 6s - loss: 380.6388 - loglik: -3.7904e+02 - logprior: -1.5960e+00
Epoch 4/10
19/19 - 6s - loss: 370.7587 - loglik: -3.6890e+02 - logprior: -1.8629e+00
Epoch 5/10
19/19 - 6s - loss: 366.9708 - loglik: -3.6519e+02 - logprior: -1.7780e+00
Epoch 6/10
19/19 - 6s - loss: 364.6633 - loglik: -3.6292e+02 - logprior: -1.7411e+00
Epoch 7/10
19/19 - 6s - loss: 362.3186 - loglik: -3.6063e+02 - logprior: -1.6910e+00
Epoch 8/10
19/19 - 6s - loss: 362.5066 - loglik: -3.6083e+02 - logprior: -1.6754e+00
Fitted a model with MAP estimate = -362.0272
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (22, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 1), (80, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (123, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 344.6929 - loglik: -3.4205e+02 - logprior: -2.6383e+00
Epoch 2/2
19/19 - 9s - loss: 325.6471 - loglik: -3.2473e+02 - logprior: -9.2126e-01
Fitted a model with MAP estimate = -322.8594
expansions: []
discards: [118 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 325.4796 - loglik: -3.2297e+02 - logprior: -2.5128e+00
Epoch 2/2
19/19 - 9s - loss: 322.0599 - loglik: -3.2125e+02 - logprior: -8.0670e-01
Fitted a model with MAP estimate = -321.3603
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 323.2734 - loglik: -3.2077e+02 - logprior: -2.5016e+00
Epoch 2/10
19/19 - 9s - loss: 322.3228 - loglik: -3.2162e+02 - logprior: -7.0673e-01
Epoch 3/10
19/19 - 8s - loss: 320.2228 - loglik: -3.1962e+02 - logprior: -6.0117e-01
Epoch 4/10
19/19 - 9s - loss: 322.1143 - loglik: -3.2157e+02 - logprior: -5.4333e-01
Fitted a model with MAP estimate = -320.9751
Time for alignment: 181.7481
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 525.7635 - loglik: -5.2302e+02 - logprior: -2.7402e+00
Epoch 2/10
19/19 - 6s - loss: 422.9757 - loglik: -4.2204e+02 - logprior: -9.4066e-01
Epoch 3/10
19/19 - 6s - loss: 378.3371 - loglik: -3.7673e+02 - logprior: -1.6060e+00
Epoch 4/10
19/19 - 6s - loss: 368.0491 - loglik: -3.6617e+02 - logprior: -1.8812e+00
Epoch 5/10
19/19 - 6s - loss: 365.9749 - loglik: -3.6417e+02 - logprior: -1.8040e+00
Epoch 6/10
19/19 - 6s - loss: 363.2486 - loglik: -3.6148e+02 - logprior: -1.7663e+00
Epoch 7/10
19/19 - 6s - loss: 362.1965 - loglik: -3.6047e+02 - logprior: -1.7290e+00
Epoch 8/10
19/19 - 6s - loss: 361.0517 - loglik: -3.5935e+02 - logprior: -1.6989e+00
Epoch 9/10
19/19 - 6s - loss: 362.9377 - loglik: -3.6124e+02 - logprior: -1.6964e+00
Fitted a model with MAP estimate = -361.0424
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (22, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (54, 1), (58, 1), (60, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 345.3440 - loglik: -3.4268e+02 - logprior: -2.6649e+00
Epoch 2/2
19/19 - 9s - loss: 326.5118 - loglik: -3.2556e+02 - logprior: -9.5120e-01
Fitted a model with MAP estimate = -324.6396
expansions: []
discards: [118 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 326.7773 - loglik: -3.2424e+02 - logprior: -2.5351e+00
Epoch 2/2
19/19 - 9s - loss: 324.2228 - loglik: -3.2347e+02 - logprior: -7.5356e-01
Fitted a model with MAP estimate = -323.2661
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 325.6800 - loglik: -3.2318e+02 - logprior: -2.4959e+00
Epoch 2/10
19/19 - 9s - loss: 322.5905 - loglik: -3.2188e+02 - logprior: -7.1215e-01
Epoch 3/10
19/19 - 8s - loss: 323.5099 - loglik: -3.2290e+02 - logprior: -6.1047e-01
Fitted a model with MAP estimate = -322.5597
Time for alignment: 180.7643
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.1445 - loglik: -5.2240e+02 - logprior: -2.7406e+00
Epoch 2/10
19/19 - 6s - loss: 422.2085 - loglik: -4.2126e+02 - logprior: -9.4670e-01
Epoch 3/10
19/19 - 6s - loss: 376.5654 - loglik: -3.7493e+02 - logprior: -1.6309e+00
Epoch 4/10
19/19 - 6s - loss: 369.2399 - loglik: -3.6734e+02 - logprior: -1.8999e+00
Epoch 5/10
19/19 - 6s - loss: 364.6054 - loglik: -3.6280e+02 - logprior: -1.8050e+00
Epoch 6/10
19/19 - 6s - loss: 362.5122 - loglik: -3.6075e+02 - logprior: -1.7594e+00
Epoch 7/10
19/19 - 6s - loss: 361.3987 - loglik: -3.5969e+02 - logprior: -1.7106e+00
Epoch 8/10
19/19 - 6s - loss: 360.9695 - loglik: -3.5928e+02 - logprior: -1.6908e+00
Epoch 9/10
19/19 - 6s - loss: 359.8668 - loglik: -3.5819e+02 - logprior: -1.6792e+00
Epoch 10/10
19/19 - 6s - loss: 360.6512 - loglik: -3.5898e+02 - logprior: -1.6738e+00
Fitted a model with MAP estimate = -360.2153
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (42, 1), (58, 1), (59, 2), (60, 1), (63, 1), (66, 2), (72, 1), (74, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 344.6447 - loglik: -3.4199e+02 - logprior: -2.6561e+00
Epoch 2/2
19/19 - 9s - loss: 325.5558 - loglik: -3.2462e+02 - logprior: -9.3124e-01
Fitted a model with MAP estimate = -323.4070
expansions: []
discards: [ 74  85 120 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 326.5174 - loglik: -3.2402e+02 - logprior: -2.4966e+00
Epoch 2/2
19/19 - 9s - loss: 323.1794 - loglik: -3.2245e+02 - logprior: -7.3229e-01
Fitted a model with MAP estimate = -323.1232
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 325.7307 - loglik: -3.2334e+02 - logprior: -2.3936e+00
Epoch 2/10
19/19 - 9s - loss: 322.2245 - loglik: -3.2154e+02 - logprior: -6.8307e-01
Epoch 3/10
19/19 - 8s - loss: 323.3672 - loglik: -3.2280e+02 - logprior: -5.6687e-01
Fitted a model with MAP estimate = -322.7034
Time for alignment: 185.2304
Computed alignments with likelihoods: ['-320.9751', '-322.5597', '-322.7034']
Best model has likelihood: -320.9751
SP score = 0.7303
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966ba76100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96304ae460>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.0351 - loglik: -2.5493e+02 - logprior: -3.1086e+00
Epoch 2/10
19/19 - 2s - loss: 226.9688 - loglik: -2.2570e+02 - logprior: -1.2700e+00
Epoch 3/10
19/19 - 2s - loss: 214.0436 - loglik: -2.1273e+02 - logprior: -1.3117e+00
Epoch 4/10
19/19 - 2s - loss: 209.7334 - loglik: -2.0849e+02 - logprior: -1.2445e+00
Epoch 5/10
19/19 - 2s - loss: 207.5087 - loglik: -2.0625e+02 - logprior: -1.2552e+00
Epoch 6/10
19/19 - 1s - loss: 205.7895 - loglik: -2.0455e+02 - logprior: -1.2409e+00
Epoch 7/10
19/19 - 2s - loss: 204.3461 - loglik: -2.0316e+02 - logprior: -1.1882e+00
Epoch 8/10
19/19 - 2s - loss: 204.7135 - loglik: -2.0355e+02 - logprior: -1.1673e+00
Fitted a model with MAP estimate = -203.9128
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (18, 1), (30, 2), (37, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 204.0263 - loglik: -2.0004e+02 - logprior: -3.9895e+00
Epoch 2/2
19/19 - 2s - loss: 195.2458 - loglik: -1.9332e+02 - logprior: -1.9296e+00
Fitted a model with MAP estimate = -193.4711
expansions: [(0, 2)]
discards: [ 0  7 37 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 194.6473 - loglik: -1.9179e+02 - logprior: -2.8554e+00
Epoch 2/2
19/19 - 2s - loss: 191.9645 - loglik: -1.9093e+02 - logprior: -1.0366e+00
Fitted a model with MAP estimate = -191.2532
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.7970 - loglik: -1.9243e+02 - logprior: -3.3690e+00
Epoch 2/10
19/19 - 2s - loss: 192.4692 - loglik: -1.9126e+02 - logprior: -1.2059e+00
Epoch 3/10
19/19 - 2s - loss: 191.7873 - loglik: -1.9072e+02 - logprior: -1.0713e+00
Epoch 4/10
19/19 - 2s - loss: 191.6577 - loglik: -1.9065e+02 - logprior: -1.0070e+00
Epoch 5/10
19/19 - 2s - loss: 191.5056 - loglik: -1.9053e+02 - logprior: -9.7613e-01
Epoch 6/10
19/19 - 2s - loss: 191.6809 - loglik: -1.9074e+02 - logprior: -9.4250e-01
Fitted a model with MAP estimate = -191.4373
Time for alignment: 58.5184
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.1168 - loglik: -2.5501e+02 - logprior: -3.1070e+00
Epoch 2/10
19/19 - 2s - loss: 227.4893 - loglik: -2.2620e+02 - logprior: -1.2877e+00
Epoch 3/10
19/19 - 2s - loss: 212.3098 - loglik: -2.1093e+02 - logprior: -1.3824e+00
Epoch 4/10
19/19 - 2s - loss: 206.9383 - loglik: -2.0564e+02 - logprior: -1.2964e+00
Epoch 5/10
19/19 - 2s - loss: 204.4539 - loglik: -2.0318e+02 - logprior: -1.2765e+00
Epoch 6/10
19/19 - 2s - loss: 203.7810 - loglik: -2.0254e+02 - logprior: -1.2375e+00
Epoch 7/10
19/19 - 2s - loss: 203.7361 - loglik: -2.0251e+02 - logprior: -1.2244e+00
Epoch 8/10
19/19 - 2s - loss: 203.6624 - loglik: -2.0245e+02 - logprior: -1.2134e+00
Epoch 9/10
19/19 - 2s - loss: 203.2231 - loglik: -2.0201e+02 - logprior: -1.2131e+00
Epoch 10/10
19/19 - 2s - loss: 203.3506 - loglik: -2.0214e+02 - logprior: -1.2098e+00
Fitted a model with MAP estimate = -203.0049
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 204.0041 - loglik: -2.0000e+02 - logprior: -4.0004e+00
Epoch 2/2
19/19 - 2s - loss: 195.2311 - loglik: -1.9328e+02 - logprior: -1.9554e+00
Fitted a model with MAP estimate = -193.6347
expansions: [(0, 2)]
discards: [ 0  7 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 194.7012 - loglik: -1.9184e+02 - logprior: -2.8597e+00
Epoch 2/2
19/19 - 2s - loss: 191.9611 - loglik: -1.9092e+02 - logprior: -1.0437e+00
Fitted a model with MAP estimate = -191.2810
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.7798 - loglik: -1.9240e+02 - logprior: -3.3759e+00
Epoch 2/10
19/19 - 2s - loss: 192.3239 - loglik: -1.9112e+02 - logprior: -1.2073e+00
Epoch 3/10
19/19 - 2s - loss: 191.7949 - loglik: -1.9073e+02 - logprior: -1.0639e+00
Epoch 4/10
19/19 - 2s - loss: 191.7916 - loglik: -1.9078e+02 - logprior: -1.0082e+00
Epoch 5/10
19/19 - 2s - loss: 191.3588 - loglik: -1.9039e+02 - logprior: -9.7031e-01
Epoch 6/10
19/19 - 2s - loss: 191.5522 - loglik: -1.9062e+02 - logprior: -9.3613e-01
Fitted a model with MAP estimate = -191.4168
Time for alignment: 60.1491
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.1203 - loglik: -2.5501e+02 - logprior: -3.1122e+00
Epoch 2/10
19/19 - 1s - loss: 226.0885 - loglik: -2.2478e+02 - logprior: -1.3086e+00
Epoch 3/10
19/19 - 1s - loss: 210.4876 - loglik: -2.0906e+02 - logprior: -1.4266e+00
Epoch 4/10
19/19 - 1s - loss: 206.7592 - loglik: -2.0538e+02 - logprior: -1.3795e+00
Epoch 5/10
19/19 - 2s - loss: 205.8888 - loglik: -2.0445e+02 - logprior: -1.4432e+00
Epoch 6/10
19/19 - 2s - loss: 205.3142 - loglik: -2.0391e+02 - logprior: -1.4077e+00
Epoch 7/10
19/19 - 2s - loss: 204.9596 - loglik: -2.0356e+02 - logprior: -1.3986e+00
Epoch 8/10
19/19 - 2s - loss: 205.0988 - loglik: -2.0371e+02 - logprior: -1.3918e+00
Fitted a model with MAP estimate = -204.4954
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (22, 1), (29, 1), (32, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 205.5175 - loglik: -2.0152e+02 - logprior: -3.9976e+00
Epoch 2/2
19/19 - 2s - loss: 195.2396 - loglik: -1.9325e+02 - logprior: -1.9885e+00
Fitted a model with MAP estimate = -193.4183
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 50 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 194.8895 - loglik: -1.9202e+02 - logprior: -2.8704e+00
Epoch 2/2
19/19 - 2s - loss: 191.9202 - loglik: -1.9088e+02 - logprior: -1.0445e+00
Fitted a model with MAP estimate = -191.2735
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.7179 - loglik: -1.9232e+02 - logprior: -3.3951e+00
Epoch 2/10
19/19 - 2s - loss: 192.5409 - loglik: -1.9133e+02 - logprior: -1.2126e+00
Epoch 3/10
19/19 - 2s - loss: 191.9261 - loglik: -1.9085e+02 - logprior: -1.0751e+00
Epoch 4/10
19/19 - 2s - loss: 191.5834 - loglik: -1.9055e+02 - logprior: -1.0296e+00
Epoch 5/10
19/19 - 2s - loss: 191.4451 - loglik: -1.9046e+02 - logprior: -9.8257e-01
Epoch 6/10
19/19 - 2s - loss: 191.8371 - loglik: -1.9088e+02 - logprior: -9.5621e-01
Fitted a model with MAP estimate = -191.4579
Time for alignment: 55.9227
Computed alignments with likelihoods: ['-191.2532', '-191.2810', '-191.2735']
Best model has likelihood: -191.2532
SP score = 0.9646
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f960dfcd730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9e8feb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 604.3851 - loglik: -6.0173e+02 - logprior: -2.6507e+00
Epoch 2/10
19/19 - 7s - loss: 533.2822 - loglik: -5.3242e+02 - logprior: -8.6632e-01
Epoch 3/10
19/19 - 7s - loss: 496.0957 - loglik: -4.9480e+02 - logprior: -1.2982e+00
Epoch 4/10
19/19 - 7s - loss: 483.8046 - loglik: -4.8248e+02 - logprior: -1.3222e+00
Epoch 5/10
19/19 - 7s - loss: 480.0777 - loglik: -4.7878e+02 - logprior: -1.2969e+00
Epoch 6/10
19/19 - 7s - loss: 478.8844 - loglik: -4.7760e+02 - logprior: -1.2828e+00
Epoch 7/10
19/19 - 7s - loss: 478.0269 - loglik: -4.7676e+02 - logprior: -1.2665e+00
Epoch 8/10
19/19 - 7s - loss: 477.8621 - loglik: -4.7660e+02 - logprior: -1.2633e+00
Epoch 9/10
19/19 - 7s - loss: 477.0321 - loglik: -4.7576e+02 - logprior: -1.2728e+00
Epoch 10/10
19/19 - 7s - loss: 477.4188 - loglik: -4.7614e+02 - logprior: -1.2826e+00
Fitted a model with MAP estimate = -474.6720
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (103, 1), (109, 1), (113, 1), (116, 1), (124, 4), (140, 2), (141, 3), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 223 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 472.1410 - loglik: -4.6820e+02 - logprior: -3.9434e+00
Epoch 2/2
19/19 - 10s - loss: 456.5164 - loglik: -4.5522e+02 - logprior: -1.2967e+00
Fitted a model with MAP estimate = -452.3032
expansions: []
discards: [ 44  57  95 114 177 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 456.9914 - loglik: -4.5412e+02 - logprior: -2.8719e+00
Epoch 2/2
19/19 - 9s - loss: 453.6583 - loglik: -4.5268e+02 - logprior: -9.8312e-01
Fitted a model with MAP estimate = -450.8218
expansions: []
discards: [ 25 184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 453.8307 - loglik: -4.5104e+02 - logprior: -2.7888e+00
Epoch 2/10
19/19 - 9s - loss: 452.1109 - loglik: -4.5121e+02 - logprior: -9.0231e-01
Epoch 3/10
19/19 - 9s - loss: 450.6938 - loglik: -4.5001e+02 - logprior: -6.8825e-01
Epoch 4/10
19/19 - 9s - loss: 451.0961 - loglik: -4.5048e+02 - logprior: -6.1327e-01
Fitted a model with MAP estimate = -450.5217
Time for alignment: 218.8899
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 604.3299 - loglik: -6.0167e+02 - logprior: -2.6645e+00
Epoch 2/10
19/19 - 7s - loss: 532.7365 - loglik: -5.3185e+02 - logprior: -8.8644e-01
Epoch 3/10
19/19 - 7s - loss: 494.3300 - loglik: -4.9301e+02 - logprior: -1.3184e+00
Epoch 4/10
19/19 - 7s - loss: 484.9029 - loglik: -4.8356e+02 - logprior: -1.3418e+00
Epoch 5/10
19/19 - 7s - loss: 480.3329 - loglik: -4.7901e+02 - logprior: -1.3247e+00
Epoch 6/10
19/19 - 7s - loss: 478.5385 - loglik: -4.7721e+02 - logprior: -1.3310e+00
Epoch 7/10
19/19 - 7s - loss: 477.4712 - loglik: -4.7615e+02 - logprior: -1.3239e+00
Epoch 8/10
19/19 - 7s - loss: 477.4982 - loglik: -4.7617e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -474.7725
expansions: [(0, 3), (11, 1), (18, 2), (19, 1), (20, 1), (35, 2), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (102, 1), (103, 1), (109, 1), (113, 1), (116, 1), (126, 1), (127, 2), (132, 1), (141, 3), (142, 2), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 222 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 471.9118 - loglik: -4.6800e+02 - logprior: -3.9097e+00
Epoch 2/2
19/19 - 10s - loss: 456.6911 - loglik: -4.5540e+02 - logprior: -1.2882e+00
Fitted a model with MAP estimate = -452.4631
expansions: []
discards: [ 56  94 113 178 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 456.9768 - loglik: -4.5410e+02 - logprior: -2.8805e+00
Epoch 2/2
19/19 - 9s - loss: 454.1555 - loglik: -4.5317e+02 - logprior: -9.8802e-01
Fitted a model with MAP estimate = -451.1660
expansions: []
discards: [ 43 184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 453.9422 - loglik: -4.5115e+02 - logprior: -2.7897e+00
Epoch 2/10
19/19 - 9s - loss: 452.0156 - loglik: -4.5110e+02 - logprior: -9.2035e-01
Epoch 3/10
19/19 - 9s - loss: 451.2090 - loglik: -4.5052e+02 - logprior: -6.9329e-01
Epoch 4/10
19/19 - 9s - loss: 450.7025 - loglik: -4.5009e+02 - logprior: -6.1322e-01
Epoch 5/10
19/19 - 9s - loss: 451.0654 - loglik: -4.5052e+02 - logprior: -5.4196e-01
Fitted a model with MAP estimate = -450.6394
Time for alignment: 211.8604
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 603.6947 - loglik: -6.0104e+02 - logprior: -2.6557e+00
Epoch 2/10
19/19 - 7s - loss: 529.9658 - loglik: -5.2906e+02 - logprior: -9.0835e-01
Epoch 3/10
19/19 - 7s - loss: 492.9749 - loglik: -4.9163e+02 - logprior: -1.3408e+00
Epoch 4/10
19/19 - 7s - loss: 482.7297 - loglik: -4.8138e+02 - logprior: -1.3540e+00
Epoch 5/10
19/19 - 7s - loss: 479.1133 - loglik: -4.7782e+02 - logprior: -1.2975e+00
Epoch 6/10
19/19 - 7s - loss: 478.1633 - loglik: -4.7687e+02 - logprior: -1.2948e+00
Epoch 7/10
19/19 - 7s - loss: 476.8771 - loglik: -4.7559e+02 - logprior: -1.2912e+00
Epoch 8/10
19/19 - 7s - loss: 477.4417 - loglik: -4.7615e+02 - logprior: -1.2960e+00
Fitted a model with MAP estimate = -474.3260
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (112, 1), (116, 1), (124, 2), (135, 1), (137, 1), (140, 2), (147, 1), (148, 2), (149, 2), (150, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 472.2170 - loglik: -4.6832e+02 - logprior: -3.8921e+00
Epoch 2/2
19/19 - 10s - loss: 457.8333 - loglik: -4.5654e+02 - logprior: -1.2885e+00
Fitted a model with MAP estimate = -453.6501
expansions: []
discards: [ 25  56  94 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 457.9114 - loglik: -4.5501e+02 - logprior: -2.8979e+00
Epoch 2/2
19/19 - 9s - loss: 454.9749 - loglik: -4.5398e+02 - logprior: -9.9236e-01
Fitted a model with MAP estimate = -452.0869
expansions: []
discards: [173 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 454.9988 - loglik: -4.5220e+02 - logprior: -2.7954e+00
Epoch 2/10
19/19 - 9s - loss: 452.9572 - loglik: -4.5206e+02 - logprior: -8.9414e-01
Epoch 3/10
19/19 - 9s - loss: 452.2115 - loglik: -4.5152e+02 - logprior: -6.9601e-01
Epoch 4/10
19/19 - 9s - loss: 452.1638 - loglik: -4.5156e+02 - logprior: -6.0071e-01
Epoch 5/10
19/19 - 9s - loss: 451.6317 - loglik: -4.5108e+02 - logprior: -5.5363e-01
Epoch 6/10
19/19 - 9s - loss: 451.2345 - loglik: -4.5073e+02 - logprior: -5.0160e-01
Epoch 7/10
19/19 - 9s - loss: 452.2915 - loglik: -4.5184e+02 - logprior: -4.5234e-01
Fitted a model with MAP estimate = -451.4972
Time for alignment: 231.8088
Computed alignments with likelihoods: ['-450.5217', '-450.6394', '-451.4972']
Best model has likelihood: -450.5217
SP score = 0.7091
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96303792e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f964991b4c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.7885 - loglik: -1.6160e+02 - logprior: -3.1915e+00
Epoch 2/10
19/19 - 1s - loss: 136.8513 - loglik: -1.3545e+02 - logprior: -1.3965e+00
Epoch 3/10
19/19 - 1s - loss: 128.5286 - loglik: -1.2704e+02 - logprior: -1.4881e+00
Epoch 4/10
19/19 - 1s - loss: 126.9542 - loglik: -1.2562e+02 - logprior: -1.3341e+00
Epoch 5/10
19/19 - 1s - loss: 126.6129 - loglik: -1.2528e+02 - logprior: -1.3344e+00
Epoch 6/10
19/19 - 1s - loss: 126.4295 - loglik: -1.2512e+02 - logprior: -1.3096e+00
Epoch 7/10
19/19 - 1s - loss: 126.3733 - loglik: -1.2508e+02 - logprior: -1.2973e+00
Epoch 8/10
19/19 - 1s - loss: 126.3334 - loglik: -1.2504e+02 - logprior: -1.2906e+00
Epoch 9/10
19/19 - 1s - loss: 126.2306 - loglik: -1.2495e+02 - logprior: -1.2820e+00
Epoch 10/10
19/19 - 1s - loss: 126.1708 - loglik: -1.2489e+02 - logprior: -1.2813e+00
Fitted a model with MAP estimate = -126.1728
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 132.8360 - loglik: -1.2875e+02 - logprior: -4.0907e+00
Epoch 2/2
19/19 - 1s - loss: 126.6824 - loglik: -1.2447e+02 - logprior: -2.2155e+00
Fitted a model with MAP estimate = -124.5778
expansions: []
discards: [13 14 16 17 41 42 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4537 - loglik: -1.2310e+02 - logprior: -3.3534e+00
Epoch 2/2
19/19 - 1s - loss: 122.8591 - loglik: -1.2145e+02 - logprior: -1.4102e+00
Fitted a model with MAP estimate = -122.6014
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.0084 - loglik: -1.2182e+02 - logprior: -3.1906e+00
Epoch 2/10
19/19 - 1s - loss: 122.8552 - loglik: -1.2147e+02 - logprior: -1.3849e+00
Epoch 3/10
19/19 - 1s - loss: 122.5125 - loglik: -1.2123e+02 - logprior: -1.2828e+00
Epoch 4/10
19/19 - 1s - loss: 122.5658 - loglik: -1.2134e+02 - logprior: -1.2301e+00
Fitted a model with MAP estimate = -122.3759
Time for alignment: 41.9159
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8503 - loglik: -1.6166e+02 - logprior: -3.1910e+00
Epoch 2/10
19/19 - 1s - loss: 136.7592 - loglik: -1.3537e+02 - logprior: -1.3907e+00
Epoch 3/10
19/19 - 1s - loss: 128.6889 - loglik: -1.2721e+02 - logprior: -1.4799e+00
Epoch 4/10
19/19 - 1s - loss: 127.5333 - loglik: -1.2620e+02 - logprior: -1.3291e+00
Epoch 5/10
19/19 - 1s - loss: 127.0608 - loglik: -1.2574e+02 - logprior: -1.3256e+00
Epoch 6/10
19/19 - 1s - loss: 126.8993 - loglik: -1.2560e+02 - logprior: -1.3035e+00
Epoch 7/10
19/19 - 1s - loss: 126.9146 - loglik: -1.2563e+02 - logprior: -1.2876e+00
Fitted a model with MAP estimate = -126.7195
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.1301 - loglik: -1.2903e+02 - logprior: -4.1020e+00
Epoch 2/2
19/19 - 1s - loss: 126.5009 - loglik: -1.2430e+02 - logprior: -2.1979e+00
Fitted a model with MAP estimate = -124.2914
expansions: []
discards: [11 13 14 42 48 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 125.8745 - loglik: -1.2254e+02 - logprior: -3.3332e+00
Epoch 2/2
19/19 - 1s - loss: 122.7993 - loglik: -1.2138e+02 - logprior: -1.4169e+00
Fitted a model with MAP estimate = -122.4378
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 124.8444 - loglik: -1.2165e+02 - logprior: -3.1991e+00
Epoch 2/10
19/19 - 1s - loss: 122.6750 - loglik: -1.2127e+02 - logprior: -1.4009e+00
Epoch 3/10
19/19 - 1s - loss: 122.2229 - loglik: -1.2093e+02 - logprior: -1.2928e+00
Epoch 4/10
19/19 - 1s - loss: 122.2297 - loglik: -1.2099e+02 - logprior: -1.2407e+00
Fitted a model with MAP estimate = -122.1351
Time for alignment: 36.6260
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8965 - loglik: -1.6170e+02 - logprior: -3.1918e+00
Epoch 2/10
19/19 - 1s - loss: 135.7336 - loglik: -1.3434e+02 - logprior: -1.3900e+00
Epoch 3/10
19/19 - 1s - loss: 129.0166 - loglik: -1.2755e+02 - logprior: -1.4621e+00
Epoch 4/10
19/19 - 1s - loss: 127.3146 - loglik: -1.2601e+02 - logprior: -1.3026e+00
Epoch 5/10
19/19 - 1s - loss: 126.3955 - loglik: -1.2508e+02 - logprior: -1.3163e+00
Epoch 6/10
19/19 - 1s - loss: 126.1783 - loglik: -1.2488e+02 - logprior: -1.2951e+00
Epoch 7/10
19/19 - 1s - loss: 126.1567 - loglik: -1.2487e+02 - logprior: -1.2831e+00
Epoch 8/10
19/19 - 1s - loss: 125.8958 - loglik: -1.2462e+02 - logprior: -1.2769e+00
Epoch 9/10
19/19 - 1s - loss: 126.0996 - loglik: -1.2483e+02 - logprior: -1.2648e+00
Fitted a model with MAP estimate = -125.8990
expansions: [(11, 5), (12, 3), (13, 1), (28, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 132.9217 - loglik: -1.2882e+02 - logprior: -4.1001e+00
Epoch 2/2
19/19 - 1s - loss: 126.6566 - loglik: -1.2442e+02 - logprior: -2.2394e+00
Fitted a model with MAP estimate = -124.4531
expansions: []
discards: [12 13 16 36 41 42 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.2561 - loglik: -1.2291e+02 - logprior: -3.3425e+00
Epoch 2/2
19/19 - 1s - loss: 122.9770 - loglik: -1.2157e+02 - logprior: -1.4041e+00
Fitted a model with MAP estimate = -122.6118
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 124.9704 - loglik: -1.2178e+02 - logprior: -3.1914e+00
Epoch 2/10
19/19 - 1s - loss: 122.8761 - loglik: -1.2149e+02 - logprior: -1.3882e+00
Epoch 3/10
19/19 - 1s - loss: 122.5683 - loglik: -1.2128e+02 - logprior: -1.2896e+00
Epoch 4/10
19/19 - 1s - loss: 122.3223 - loglik: -1.2109e+02 - logprior: -1.2315e+00
Epoch 5/10
19/19 - 1s - loss: 122.4732 - loglik: -1.2128e+02 - logprior: -1.1971e+00
Fitted a model with MAP estimate = -122.3283
Time for alignment: 38.7302
Computed alignments with likelihoods: ['-122.3759', '-122.1351', '-122.3283']
Best model has likelihood: -122.1351
SP score = 0.9096
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e31f26d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966b89bf10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 901.3362 - loglik: -8.9990e+02 - logprior: -1.4392e+00
Epoch 2/10
39/39 - 42s - loss: 720.6218 - loglik: -7.1946e+02 - logprior: -1.1633e+00
Epoch 3/10
39/39 - 42s - loss: 708.6768 - loglik: -7.0753e+02 - logprior: -1.1438e+00
Epoch 4/10
39/39 - 42s - loss: 704.8174 - loglik: -7.0371e+02 - logprior: -1.1027e+00
Epoch 5/10
39/39 - 42s - loss: 704.4893 - loglik: -7.0340e+02 - logprior: -1.0861e+00
Epoch 6/10
39/39 - 42s - loss: 702.8434 - loglik: -7.0176e+02 - logprior: -1.0796e+00
Epoch 7/10
39/39 - 42s - loss: 701.1999 - loglik: -7.0012e+02 - logprior: -1.0790e+00
Epoch 8/10
39/39 - 42s - loss: 701.9340 - loglik: -7.0084e+02 - logprior: -1.0896e+00
Fitted a model with MAP estimate = -700.7871
expansions: [(0, 3), (43, 1), (45, 1), (133, 1), (143, 1), (161, 1), (162, 1), (163, 1), (174, 5), (175, 2), (176, 1), (177, 1), (178, 1), (189, 2), (190, 5), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 2), (202, 2), (203, 1), (208, 1), (211, 2), (217, 1), (222, 2), (223, 5), (224, 2), (227, 1), (228, 2), (229, 3), (230, 2), (231, 1), (240, 1), (241, 1), (243, 1), (244, 2), (245, 2), (246, 3), (248, 2), (249, 5), (250, 2), (251, 1), (253, 1), (269, 1), (288, 2), (289, 1), (290, 2), (292, 1), (293, 1), (305, 2), (306, 2), (308, 2), (313, 1), (327, 1), (330, 1), (339, 1), (349, 1), (355, 6)]
discards: [2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 672.8141 - loglik: -6.7053e+02 - logprior: -2.2798e+00
Epoch 2/2
39/39 - 64s - loss: 655.2731 - loglik: -6.5442e+02 - logprior: -8.5693e-01
Fitted a model with MAP estimate = -652.8435
expansions: [(216, 1), (250, 1), (281, 1)]
discards: [  2   3 185 186 212 213 237 270 273 274 275 276 277 288 315 316 317 402
 459]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 663.0457 - loglik: -6.6166e+02 - logprior: -1.3868e+00
Epoch 2/2
39/39 - 61s - loss: 659.1548 - loglik: -6.5905e+02 - logprior: -1.0720e-01
Fitted a model with MAP estimate = -657.0264
expansions: [(267, 4), (268, 1), (301, 1)]
discards: [302 303 304]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 657.3802 - loglik: -6.5624e+02 - logprior: -1.1420e+00
Epoch 2/10
39/39 - 61s - loss: 654.7272 - loglik: -6.5498e+02 - logprior: 0.2547
Epoch 3/10
39/39 - 62s - loss: 654.8084 - loglik: -6.5524e+02 - logprior: 0.4269
Fitted a model with MAP estimate = -654.1588
Time for alignment: 1076.8813
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 901.1941 - loglik: -8.9972e+02 - logprior: -1.4693e+00
Epoch 2/10
39/39 - 42s - loss: 723.2297 - loglik: -7.2193e+02 - logprior: -1.3015e+00
Epoch 3/10
39/39 - 42s - loss: 709.0696 - loglik: -7.0764e+02 - logprior: -1.4307e+00
Epoch 4/10
39/39 - 42s - loss: 705.5807 - loglik: -7.0424e+02 - logprior: -1.3397e+00
Epoch 5/10
39/39 - 42s - loss: 704.7533 - loglik: -7.0347e+02 - logprior: -1.2801e+00
Epoch 6/10
39/39 - 42s - loss: 703.9359 - loglik: -7.0263e+02 - logprior: -1.3065e+00
Epoch 7/10
39/39 - 42s - loss: 704.5673 - loglik: -7.0317e+02 - logprior: -1.3988e+00
Fitted a model with MAP estimate = -702.8965
expansions: [(0, 3), (37, 1), (38, 1), (42, 1), (134, 1), (143, 1), (163, 1), (164, 1), (170, 1), (175, 6), (176, 4), (177, 1), (178, 1), (188, 1), (189, 1), (190, 6), (191, 2), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 2), (221, 7), (224, 1), (226, 2), (227, 3), (228, 2), (229, 1), (244, 1), (245, 3), (246, 2), (247, 2), (248, 3), (249, 3), (250, 1), (253, 1), (254, 1), (267, 2), (268, 1), (269, 1), (270, 1), (285, 1), (287, 2), (288, 2), (290, 1), (300, 2), (302, 1), (303, 1), (304, 1), (316, 1), (326, 1), (329, 1), (349, 1), (352, 1), (354, 2), (355, 3)]
discards: [2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 462 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 671.4670 - loglik: -6.6932e+02 - logprior: -2.1468e+00
Epoch 2/2
39/39 - 65s - loss: 654.3609 - loglik: -6.5365e+02 - logprior: -7.1108e-01
Fitted a model with MAP estimate = -651.8520
expansions: [(321, 1), (322, 1)]
discards: [  2   3 187 188 189 190 191 220 273 315 316 324 325 326 374 460]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 659.1366 - loglik: -6.5791e+02 - logprior: -1.2301e+00
Epoch 2/2
39/39 - 61s - loss: 656.2072 - loglik: -6.5608e+02 - logprior: -1.2775e-01
Fitted a model with MAP estimate = -654.9121
expansions: [(304, 1), (314, 2)]
discards: [305 306 307 308 309]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 446 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 657.6492 - loglik: -6.5658e+02 - logprior: -1.0733e+00
Epoch 2/10
39/39 - 61s - loss: 655.6157 - loglik: -6.5579e+02 - logprior: 0.1699
Epoch 3/10
39/39 - 61s - loss: 655.2236 - loglik: -6.5556e+02 - logprior: 0.3381
Epoch 4/10
39/39 - 61s - loss: 654.2494 - loglik: -6.5478e+02 - logprior: 0.5343
Epoch 5/10
39/39 - 61s - loss: 654.8592 - loglik: -6.5548e+02 - logprior: 0.6174
Fitted a model with MAP estimate = -653.6429
Time for alignment: 1157.8491
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 901.5532 - loglik: -9.0011e+02 - logprior: -1.4455e+00
Epoch 2/10
39/39 - 42s - loss: 721.6586 - loglik: -7.2036e+02 - logprior: -1.2935e+00
Epoch 3/10
39/39 - 42s - loss: 707.6925 - loglik: -7.0608e+02 - logprior: -1.6130e+00
Epoch 4/10
39/39 - 42s - loss: 704.5604 - loglik: -7.0295e+02 - logprior: -1.6088e+00
Epoch 5/10
39/39 - 42s - loss: 702.9909 - loglik: -7.0141e+02 - logprior: -1.5804e+00
Epoch 6/10
39/39 - 42s - loss: 703.0088 - loglik: -7.0130e+02 - logprior: -1.7105e+00
Fitted a model with MAP estimate = -702.1826
expansions: [(37, 1), (43, 1), (45, 1), (131, 1), (134, 1), (163, 1), (172, 1), (177, 1), (178, 7), (180, 1), (181, 1), (192, 2), (193, 6), (194, 1), (197, 1), (198, 1), (199, 1), (200, 2), (201, 1), (202, 2), (203, 2), (205, 1), (210, 1), (212, 1), (216, 1), (219, 1), (224, 2), (225, 9), (228, 1), (229, 5), (242, 1), (244, 1), (245, 1), (246, 3), (247, 2), (248, 2), (249, 6), (250, 2), (253, 1), (266, 1), (269, 1), (270, 1), (286, 1), (288, 2), (289, 2), (290, 3), (299, 1), (301, 1), (304, 1), (306, 1), (315, 1), (328, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [  2 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 673.2442 - loglik: -6.7170e+02 - logprior: -1.5452e+00
Epoch 2/2
39/39 - 62s - loss: 658.9019 - loglik: -6.5847e+02 - logprior: -4.3303e-01
Fitted a model with MAP estimate = -657.4407
expansions: [(308, 1), (316, 1), (317, 1), (326, 1), (449, 1)]
discards: [185 186 187 188 235 269 270 271 281 282 283 310 318 319 320 370]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 441 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 665.2879 - loglik: -6.6386e+02 - logprior: -1.4277e+00
Epoch 2/2
39/39 - 60s - loss: 662.5811 - loglik: -6.6250e+02 - logprior: -8.2976e-02
Fitted a model with MAP estimate = -661.1856
expansions: []
discards: [299 300 301]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 438 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 663.6824 - loglik: -6.6251e+02 - logprior: -1.1711e+00
Epoch 2/10
39/39 - 59s - loss: 662.2745 - loglik: -6.6250e+02 - logprior: 0.2275
Epoch 3/10
39/39 - 60s - loss: 661.2236 - loglik: -6.6164e+02 - logprior: 0.4202
Epoch 4/10
39/39 - 60s - loss: 661.5077 - loglik: -6.6190e+02 - logprior: 0.3900
Fitted a model with MAP estimate = -660.2127
Time for alignment: 1032.9857
Computed alignments with likelihoods: ['-652.8435', '-651.8520', '-657.4407']
Best model has likelihood: -651.8520
SP score = 0.8657
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95f4239520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95e2a6e850>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.7104 - loglik: -4.0580e+02 - logprior: -2.9102e+00
Epoch 2/10
19/19 - 3s - loss: 330.7354 - loglik: -3.2975e+02 - logprior: -9.8194e-01
Epoch 3/10
19/19 - 3s - loss: 300.2915 - loglik: -2.9918e+02 - logprior: -1.1145e+00
Epoch 4/10
19/19 - 3s - loss: 295.3551 - loglik: -2.9429e+02 - logprior: -1.0687e+00
Epoch 5/10
19/19 - 3s - loss: 293.3914 - loglik: -2.9235e+02 - logprior: -1.0414e+00
Epoch 6/10
19/19 - 3s - loss: 292.5172 - loglik: -2.9151e+02 - logprior: -1.0059e+00
Epoch 7/10
19/19 - 3s - loss: 292.3550 - loglik: -2.9138e+02 - logprior: -9.7955e-01
Epoch 8/10
19/19 - 3s - loss: 291.8698 - loglik: -2.9090e+02 - logprior: -9.6829e-01
Epoch 9/10
19/19 - 3s - loss: 292.3932 - loglik: -2.9143e+02 - logprior: -9.5914e-01
Fitted a model with MAP estimate = -291.2617
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (27, 1), (35, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 287.2885 - loglik: -2.8338e+02 - logprior: -3.9086e+00
Epoch 2/2
19/19 - 4s - loss: 276.3058 - loglik: -2.7519e+02 - logprior: -1.1195e+00
Fitted a model with MAP estimate = -274.8659
expansions: [(0, 3)]
discards: [  1   2   3   4   6  22  99 100 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 281.1424 - loglik: -2.7729e+02 - logprior: -3.8480e+00
Epoch 2/2
19/19 - 4s - loss: 276.4586 - loglik: -2.7531e+02 - logprior: -1.1479e+00
Fitted a model with MAP estimate = -275.7021
expansions: [(0, 4)]
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 278.5060 - loglik: -2.7547e+02 - logprior: -3.0319e+00
Epoch 2/10
19/19 - 4s - loss: 275.4140 - loglik: -2.7455e+02 - logprior: -8.6232e-01
Epoch 3/10
19/19 - 4s - loss: 274.7622 - loglik: -2.7408e+02 - logprior: -6.7742e-01
Epoch 4/10
19/19 - 4s - loss: 274.8151 - loglik: -2.7420e+02 - logprior: -6.2008e-01
Fitted a model with MAP estimate = -274.4745
Time for alignment: 103.1610
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.1653 - loglik: -4.0625e+02 - logprior: -2.9110e+00
Epoch 2/10
19/19 - 3s - loss: 327.0692 - loglik: -3.2609e+02 - logprior: -9.8043e-01
Epoch 3/10
19/19 - 3s - loss: 297.6862 - loglik: -2.9658e+02 - logprior: -1.1108e+00
Epoch 4/10
19/19 - 3s - loss: 293.4301 - loglik: -2.9239e+02 - logprior: -1.0408e+00
Epoch 5/10
19/19 - 3s - loss: 291.4062 - loglik: -2.9042e+02 - logprior: -9.8396e-01
Epoch 6/10
19/19 - 3s - loss: 291.1754 - loglik: -2.9022e+02 - logprior: -9.5343e-01
Epoch 7/10
19/19 - 3s - loss: 290.1220 - loglik: -2.8919e+02 - logprior: -9.3341e-01
Epoch 8/10
19/19 - 3s - loss: 290.2032 - loglik: -2.8929e+02 - logprior: -9.1685e-01
Fitted a model with MAP estimate = -289.8434
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 287.2720 - loglik: -2.8339e+02 - logprior: -3.8797e+00
Epoch 2/2
19/19 - 4s - loss: 275.9377 - loglik: -2.7480e+02 - logprior: -1.1418e+00
Fitted a model with MAP estimate = -274.5786
expansions: [(0, 3)]
discards: [  1   2   3   4  22  75  76 101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 280.0569 - loglik: -2.7617e+02 - logprior: -3.8875e+00
Epoch 2/2
19/19 - 4s - loss: 275.6640 - loglik: -2.7447e+02 - logprior: -1.1979e+00
Fitted a model with MAP estimate = -274.5664
expansions: [(0, 4)]
discards: [  0   1   2   3 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 277.6532 - loglik: -2.7461e+02 - logprior: -3.0425e+00
Epoch 2/10
19/19 - 4s - loss: 274.7477 - loglik: -2.7383e+02 - logprior: -9.1321e-01
Epoch 3/10
19/19 - 4s - loss: 274.0728 - loglik: -2.7340e+02 - logprior: -6.7422e-01
Epoch 4/10
19/19 - 4s - loss: 273.6595 - loglik: -2.7306e+02 - logprior: -6.0400e-01
Epoch 5/10
19/19 - 4s - loss: 273.7743 - loglik: -2.7325e+02 - logprior: -5.2296e-01
Fitted a model with MAP estimate = -273.5073
Time for alignment: 105.2752
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.5203 - loglik: -4.0561e+02 - logprior: -2.9141e+00
Epoch 2/10
19/19 - 3s - loss: 325.7850 - loglik: -3.2481e+02 - logprior: -9.7546e-01
Epoch 3/10
19/19 - 3s - loss: 297.6382 - loglik: -2.9651e+02 - logprior: -1.1313e+00
Epoch 4/10
19/19 - 3s - loss: 293.1675 - loglik: -2.9205e+02 - logprior: -1.1146e+00
Epoch 5/10
19/19 - 3s - loss: 291.4966 - loglik: -2.9041e+02 - logprior: -1.0870e+00
Epoch 6/10
19/19 - 3s - loss: 290.2374 - loglik: -2.8918e+02 - logprior: -1.0550e+00
Epoch 7/10
19/19 - 3s - loss: 290.0821 - loglik: -2.8905e+02 - logprior: -1.0273e+00
Epoch 8/10
19/19 - 3s - loss: 289.8520 - loglik: -2.8883e+02 - logprior: -1.0222e+00
Epoch 9/10
19/19 - 3s - loss: 289.7117 - loglik: -2.8871e+02 - logprior: -1.0066e+00
Epoch 10/10
19/19 - 3s - loss: 289.0106 - loglik: -2.8801e+02 - logprior: -1.0009e+00
Fitted a model with MAP estimate = -289.1414
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (28, 1), (36, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 286.8178 - loglik: -2.8289e+02 - logprior: -3.9327e+00
Epoch 2/2
19/19 - 4s - loss: 276.0666 - loglik: -2.7494e+02 - logprior: -1.1245e+00
Fitted a model with MAP estimate = -274.3357
expansions: [(0, 3)]
discards: [  1   2   3   4   6  22  99 100 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 280.4257 - loglik: -2.7658e+02 - logprior: -3.8421e+00
Epoch 2/2
19/19 - 4s - loss: 276.3835 - loglik: -2.7525e+02 - logprior: -1.1370e+00
Fitted a model with MAP estimate = -275.4673
expansions: [(0, 4)]
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 278.2608 - loglik: -2.7524e+02 - logprior: -3.0253e+00
Epoch 2/10
19/19 - 4s - loss: 274.9778 - loglik: -2.7411e+02 - logprior: -8.6912e-01
Epoch 3/10
19/19 - 4s - loss: 274.4905 - loglik: -2.7382e+02 - logprior: -6.7298e-01
Epoch 4/10
19/19 - 4s - loss: 274.7883 - loglik: -2.7418e+02 - logprior: -6.1293e-01
Fitted a model with MAP estimate = -274.1625
Time for alignment: 104.7969
Computed alignments with likelihoods: ['-274.4745', '-273.5073', '-274.1625']
Best model has likelihood: -273.5073
SP score = 0.6836
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9652783a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9652681730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 69.0239 - loglik: -6.5662e+01 - logprior: -3.3621e+00
Epoch 2/10
19/19 - 1s - loss: 52.4289 - loglik: -5.0960e+01 - logprior: -1.4685e+00
Epoch 3/10
19/19 - 0s - loss: 46.0823 - loglik: -4.4527e+01 - logprior: -1.5554e+00
Epoch 4/10
19/19 - 0s - loss: 44.7460 - loglik: -4.3218e+01 - logprior: -1.5283e+00
Epoch 5/10
19/19 - 0s - loss: 44.3705 - loglik: -4.2871e+01 - logprior: -1.4998e+00
Epoch 6/10
19/19 - 0s - loss: 44.3227 - loglik: -4.2832e+01 - logprior: -1.4904e+00
Epoch 7/10
19/19 - 0s - loss: 44.2025 - loglik: -4.2725e+01 - logprior: -1.4776e+00
Epoch 8/10
19/19 - 0s - loss: 44.2413 - loglik: -4.2775e+01 - logprior: -1.4663e+00
Fitted a model with MAP estimate = -44.1528
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 46.5806 - loglik: -4.2007e+01 - logprior: -4.5736e+00
Epoch 2/2
19/19 - 0s - loss: 41.6983 - loglik: -4.0359e+01 - logprior: -1.3390e+00
Fitted a model with MAP estimate = -41.1047
expansions: [(2, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 43.7423 - loglik: -4.0412e+01 - logprior: -3.3303e+00
Epoch 2/2
19/19 - 0s - loss: 41.4119 - loglik: -3.9884e+01 - logprior: -1.5280e+00
Fitted a model with MAP estimate = -41.1923
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.3791 - loglik: -4.0100e+01 - logprior: -3.2791e+00
Epoch 2/10
19/19 - 1s - loss: 41.3178 - loglik: -3.9813e+01 - logprior: -1.5052e+00
Epoch 3/10
19/19 - 0s - loss: 41.1301 - loglik: -3.9717e+01 - logprior: -1.4131e+00
Epoch 4/10
19/19 - 0s - loss: 40.9598 - loglik: -3.9599e+01 - logprior: -1.3607e+00
Epoch 5/10
19/19 - 0s - loss: 41.0347 - loglik: -3.9709e+01 - logprior: -1.3256e+00
Fitted a model with MAP estimate = -40.9379
Time for alignment: 26.2085
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8430 - loglik: -6.5483e+01 - logprior: -3.3596e+00
Epoch 2/10
19/19 - 0s - loss: 51.6800 - loglik: -5.0249e+01 - logprior: -1.4314e+00
Epoch 3/10
19/19 - 0s - loss: 46.4568 - loglik: -4.5073e+01 - logprior: -1.3836e+00
Epoch 4/10
19/19 - 0s - loss: 44.9935 - loglik: -4.3731e+01 - logprior: -1.2629e+00
Epoch 5/10
19/19 - 0s - loss: 44.5623 - loglik: -4.3358e+01 - logprior: -1.2038e+00
Epoch 6/10
19/19 - 0s - loss: 44.3980 - loglik: -4.3213e+01 - logprior: -1.1849e+00
Epoch 7/10
19/19 - 0s - loss: 44.3779 - loglik: -4.3206e+01 - logprior: -1.1716e+00
Epoch 8/10
19/19 - 0s - loss: 44.2411 - loglik: -4.3079e+01 - logprior: -1.1625e+00
Epoch 9/10
19/19 - 0s - loss: 44.2794 - loglik: -4.3123e+01 - logprior: -1.1562e+00
Fitted a model with MAP estimate = -44.2217
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 46.7560 - loglik: -4.2137e+01 - logprior: -4.6194e+00
Epoch 2/2
19/19 - 1s - loss: 41.9129 - loglik: -4.0238e+01 - logprior: -1.6746e+00
Fitted a model with MAP estimate = -41.4100
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.4557 - loglik: -4.0233e+01 - logprior: -3.2227e+00
Epoch 2/10
19/19 - 0s - loss: 41.4701 - loglik: -3.9990e+01 - logprior: -1.4803e+00
Epoch 3/10
19/19 - 0s - loss: 41.1280 - loglik: -3.9700e+01 - logprior: -1.4281e+00
Epoch 4/10
19/19 - 0s - loss: 41.0609 - loglik: -3.9688e+01 - logprior: -1.3727e+00
Epoch 5/10
19/19 - 1s - loss: 41.0508 - loglik: -3.9707e+01 - logprior: -1.3433e+00
Epoch 6/10
19/19 - 0s - loss: 40.9168 - loglik: -3.9590e+01 - logprior: -1.3267e+00
Epoch 7/10
19/19 - 0s - loss: 40.8891 - loglik: -3.9575e+01 - logprior: -1.3140e+00
Epoch 8/10
19/19 - 1s - loss: 40.9513 - loglik: -3.9647e+01 - logprior: -1.3039e+00
Fitted a model with MAP estimate = -40.8986
Time for alignment: 21.6059
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8077 - loglik: -6.5444e+01 - logprior: -3.3638e+00
Epoch 2/10
19/19 - 0s - loss: 51.4824 - loglik: -5.0055e+01 - logprior: -1.4271e+00
Epoch 3/10
19/19 - 0s - loss: 46.4475 - loglik: -4.5049e+01 - logprior: -1.3986e+00
Epoch 4/10
19/19 - 0s - loss: 45.4634 - loglik: -4.4120e+01 - logprior: -1.3436e+00
Epoch 5/10
19/19 - 0s - loss: 44.9436 - loglik: -4.3694e+01 - logprior: -1.2494e+00
Epoch 6/10
19/19 - 0s - loss: 44.4017 - loglik: -4.3213e+01 - logprior: -1.1885e+00
Epoch 7/10
19/19 - 0s - loss: 44.4189 - loglik: -4.3247e+01 - logprior: -1.1720e+00
Fitted a model with MAP estimate = -44.2892
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 46.7294 - loglik: -4.2202e+01 - logprior: -4.5277e+00
Epoch 2/2
19/19 - 0s - loss: 41.8731 - loglik: -4.0203e+01 - logprior: -1.6698e+00
Fitted a model with MAP estimate = -41.3895
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.4386 - loglik: -4.0180e+01 - logprior: -3.2589e+00
Epoch 2/10
19/19 - 0s - loss: 41.4137 - loglik: -3.9916e+01 - logprior: -1.4974e+00
Epoch 3/10
19/19 - 0s - loss: 41.0654 - loglik: -3.9642e+01 - logprior: -1.4233e+00
Epoch 4/10
19/19 - 0s - loss: 41.0874 - loglik: -3.9715e+01 - logprior: -1.3724e+00
Fitted a model with MAP estimate = -40.9813
Time for alignment: 18.8674
Computed alignments with likelihoods: ['-40.9379', '-40.8986', '-40.9813']
Best model has likelihood: -40.8986
SP score = 0.8475
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ec2fa00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9523aca580>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 732.6924 - loglik: -7.3116e+02 - logprior: -1.5313e+00
Epoch 2/10
39/39 - 21s - loss: 529.8509 - loglik: -5.2835e+02 - logprior: -1.4970e+00
Epoch 3/10
39/39 - 21s - loss: 516.4582 - loglik: -5.1499e+02 - logprior: -1.4687e+00
Epoch 4/10
39/39 - 21s - loss: 514.0651 - loglik: -5.1265e+02 - logprior: -1.4146e+00
Epoch 5/10
39/39 - 21s - loss: 513.0021 - loglik: -5.1160e+02 - logprior: -1.4060e+00
Epoch 6/10
39/39 - 21s - loss: 512.3870 - loglik: -5.1097e+02 - logprior: -1.4207e+00
Epoch 7/10
39/39 - 21s - loss: 512.2955 - loglik: -5.1087e+02 - logprior: -1.4211e+00
Epoch 8/10
39/39 - 21s - loss: 511.8708 - loglik: -5.1044e+02 - logprior: -1.4358e+00
Epoch 9/10
39/39 - 21s - loss: 512.1662 - loglik: -5.1073e+02 - logprior: -1.4353e+00
Fitted a model with MAP estimate = -511.3024
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 477.0770 - loglik: -4.7502e+02 - logprior: -2.0616e+00
Epoch 2/2
39/39 - 30s - loss: 462.4543 - loglik: -4.6187e+02 - logprior: -5.8583e-01
Fitted a model with MAP estimate = -460.9524
expansions: [(0, 2)]
discards: [  0   1  97 163 179 214]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 463.5496 - loglik: -4.6216e+02 - logprior: -1.3864e+00
Epoch 2/2
39/39 - 30s - loss: 461.6994 - loglik: -4.6136e+02 - logprior: -3.3756e-01
Fitted a model with MAP estimate = -460.7584
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 464.7673 - loglik: -4.6272e+02 - logprior: -2.0520e+00
Epoch 2/10
39/39 - 30s - loss: 461.8960 - loglik: -4.6177e+02 - logprior: -1.2918e-01
Epoch 3/10
39/39 - 30s - loss: 461.1598 - loglik: -4.6149e+02 - logprior: 0.3336
Epoch 4/10
39/39 - 30s - loss: 460.8319 - loglik: -4.6111e+02 - logprior: 0.2749
Epoch 5/10
39/39 - 30s - loss: 460.6519 - loglik: -4.6136e+02 - logprior: 0.7051
Epoch 6/10
39/39 - 30s - loss: 460.8340 - loglik: -4.6129e+02 - logprior: 0.4587
Fitted a model with MAP estimate = -460.4068
Time for alignment: 666.9609
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 736.8964 - loglik: -7.3538e+02 - logprior: -1.5209e+00
Epoch 2/10
39/39 - 21s - loss: 532.0115 - loglik: -5.3053e+02 - logprior: -1.4787e+00
Epoch 3/10
39/39 - 21s - loss: 518.6069 - loglik: -5.1712e+02 - logprior: -1.4825e+00
Epoch 4/10
39/39 - 21s - loss: 516.0905 - loglik: -5.1466e+02 - logprior: -1.4333e+00
Epoch 5/10
39/39 - 21s - loss: 514.8724 - loglik: -5.1345e+02 - logprior: -1.4249e+00
Epoch 6/10
39/39 - 21s - loss: 514.8319 - loglik: -5.1340e+02 - logprior: -1.4349e+00
Epoch 7/10
39/39 - 21s - loss: 513.9114 - loglik: -5.1246e+02 - logprior: -1.4507e+00
Epoch 8/10
39/39 - 21s - loss: 514.4016 - loglik: -5.1294e+02 - logprior: -1.4569e+00
Fitted a model with MAP estimate = -513.3454
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (40, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 2), (80, 1), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (109, 1), (110, 1), (111, 1), (112, 2), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (217, 3), (228, 1), (230, 2), (231, 1), (237, 1), (243, 1), (265, 1), (267, 1), (270, 1), (271, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 477.2741 - loglik: -4.7523e+02 - logprior: -2.0435e+00
Epoch 2/2
39/39 - 30s - loss: 462.3503 - loglik: -4.6173e+02 - logprior: -6.1882e-01
Fitted a model with MAP estimate = -460.9017
expansions: []
discards: [  0   1  97 140 164 180 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 465.1488 - loglik: -4.6372e+02 - logprior: -1.4327e+00
Epoch 2/2
39/39 - 30s - loss: 462.9137 - loglik: -4.6276e+02 - logprior: -1.5402e-01
Fitted a model with MAP estimate = -461.6443
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 463.0358 - loglik: -4.6176e+02 - logprior: -1.2776e+00
Epoch 2/10
39/39 - 30s - loss: 460.7307 - loglik: -4.6075e+02 - logprior: 0.0194
Epoch 3/10
39/39 - 30s - loss: 460.5775 - loglik: -4.6076e+02 - logprior: 0.1821
Epoch 4/10
39/39 - 30s - loss: 459.8452 - loglik: -4.6005e+02 - logprior: 0.2062
Epoch 5/10
39/39 - 30s - loss: 460.1668 - loglik: -4.6072e+02 - logprior: 0.5556
Fitted a model with MAP estimate = -459.2328
Time for alignment: 618.9237
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 735.0411 - loglik: -7.3351e+02 - logprior: -1.5286e+00
Epoch 2/10
39/39 - 21s - loss: 530.9467 - loglik: -5.2943e+02 - logprior: -1.5187e+00
Epoch 3/10
39/39 - 21s - loss: 518.7617 - loglik: -5.1726e+02 - logprior: -1.4976e+00
Epoch 4/10
39/39 - 21s - loss: 516.1533 - loglik: -5.1472e+02 - logprior: -1.4333e+00
Epoch 5/10
39/39 - 21s - loss: 515.1534 - loglik: -5.1372e+02 - logprior: -1.4315e+00
Epoch 6/10
39/39 - 21s - loss: 514.7981 - loglik: -5.1336e+02 - logprior: -1.4424e+00
Epoch 7/10
39/39 - 21s - loss: 514.8184 - loglik: -5.1336e+02 - logprior: -1.4542e+00
Fitted a model with MAP estimate = -513.5710
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (133, 1), (134, 3), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (170, 1), (172, 2), (173, 1), (180, 1), (183, 1), (184, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 2), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 477.7288 - loglik: -4.7567e+02 - logprior: -2.0619e+00
Epoch 2/2
39/39 - 31s - loss: 462.2058 - loglik: -4.6142e+02 - logprior: -7.8193e-01
Fitted a model with MAP estimate = -460.2857
expansions: []
discards: [  0   1  97 137 141 168 181 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 464.8740 - loglik: -4.6337e+02 - logprior: -1.5053e+00
Epoch 2/2
39/39 - 29s - loss: 462.7030 - loglik: -4.6251e+02 - logprior: -1.9135e-01
Fitted a model with MAP estimate = -461.6245
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 462.7721 - loglik: -4.6154e+02 - logprior: -1.2281e+00
Epoch 2/10
39/39 - 30s - loss: 460.6447 - loglik: -4.6064e+02 - logprior: -2.2961e-04
Epoch 3/10
39/39 - 30s - loss: 460.3103 - loglik: -4.6048e+02 - logprior: 0.1732
Epoch 4/10
39/39 - 30s - loss: 459.9464 - loglik: -4.6021e+02 - logprior: 0.2628
Epoch 5/10
39/39 - 30s - loss: 460.5983 - loglik: -4.6083e+02 - logprior: 0.2287
Fitted a model with MAP estimate = -459.3670
Time for alignment: 598.1572
Computed alignments with likelihoods: ['-460.4068', '-459.2328', '-459.3670']
Best model has likelihood: -459.2328
SP score = 0.9198
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f96909e5370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9546079fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.2692 - loglik: -3.9424e+02 - logprior: -3.0316e+00
Epoch 2/10
19/19 - 4s - loss: 328.9139 - loglik: -3.2767e+02 - logprior: -1.2444e+00
Epoch 3/10
19/19 - 4s - loss: 306.3109 - loglik: -3.0470e+02 - logprior: -1.6095e+00
Epoch 4/10
19/19 - 4s - loss: 302.2686 - loglik: -3.0080e+02 - logprior: -1.4647e+00
Epoch 5/10
19/19 - 4s - loss: 300.0807 - loglik: -2.9868e+02 - logprior: -1.4032e+00
Epoch 6/10
19/19 - 4s - loss: 298.1467 - loglik: -2.9677e+02 - logprior: -1.3808e+00
Epoch 7/10
19/19 - 4s - loss: 297.5258 - loglik: -2.9616e+02 - logprior: -1.3630e+00
Epoch 8/10
19/19 - 4s - loss: 297.7281 - loglik: -2.9638e+02 - logprior: -1.3443e+00
Fitted a model with MAP estimate = -295.6421
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (29, 2), (35, 1), (36, 1), (37, 2), (46, 2), (49, 1), (50, 2), (57, 1), (60, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 298.6229 - loglik: -2.9468e+02 - logprior: -3.9453e+00
Epoch 2/2
19/19 - 5s - loss: 289.7441 - loglik: -2.8761e+02 - logprior: -2.1373e+00
Fitted a model with MAP estimate = -287.2341
expansions: [(0, 2)]
discards: [  0  23  35  47  65 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 287.8096 - loglik: -2.8496e+02 - logprior: -2.8473e+00
Epoch 2/2
19/19 - 5s - loss: 284.7553 - loglik: -2.8374e+02 - logprior: -1.0149e+00
Fitted a model with MAP estimate = -282.9840
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 288.8288 - loglik: -2.8522e+02 - logprior: -3.6126e+00
Epoch 2/10
19/19 - 5s - loss: 284.9569 - loglik: -2.8372e+02 - logprior: -1.2331e+00
Epoch 3/10
19/19 - 5s - loss: 283.4853 - loglik: -2.8245e+02 - logprior: -1.0320e+00
Epoch 4/10
19/19 - 5s - loss: 282.8706 - loglik: -2.8191e+02 - logprior: -9.5843e-01
Epoch 5/10
19/19 - 5s - loss: 283.9838 - loglik: -2.8309e+02 - logprior: -8.9815e-01
Fitted a model with MAP estimate = -283.2454
Time for alignment: 117.2479
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.4877 - loglik: -3.9447e+02 - logprior: -3.0222e+00
Epoch 2/10
19/19 - 4s - loss: 326.7889 - loglik: -3.2555e+02 - logprior: -1.2416e+00
Epoch 3/10
19/19 - 4s - loss: 305.8056 - loglik: -3.0419e+02 - logprior: -1.6200e+00
Epoch 4/10
19/19 - 4s - loss: 301.2311 - loglik: -2.9978e+02 - logprior: -1.4480e+00
Epoch 5/10
19/19 - 4s - loss: 299.0023 - loglik: -2.9760e+02 - logprior: -1.4047e+00
Epoch 6/10
19/19 - 4s - loss: 297.3829 - loglik: -2.9601e+02 - logprior: -1.3693e+00
Epoch 7/10
19/19 - 4s - loss: 297.0277 - loglik: -2.9568e+02 - logprior: -1.3461e+00
Epoch 8/10
19/19 - 4s - loss: 296.7033 - loglik: -2.9537e+02 - logprior: -1.3312e+00
Epoch 9/10
19/19 - 4s - loss: 294.1895 - loglik: -2.9287e+02 - logprior: -1.3173e+00
Epoch 10/10
19/19 - 4s - loss: 297.8443 - loglik: -2.9654e+02 - logprior: -1.3060e+00
Fitted a model with MAP estimate = -294.7240
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (30, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 2), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 299.2974 - loglik: -2.9534e+02 - logprior: -3.9556e+00
Epoch 2/2
19/19 - 5s - loss: 289.4705 - loglik: -2.8729e+02 - logprior: -2.1800e+00
Fitted a model with MAP estimate = -287.4159
expansions: [(0, 2)]
discards: [  0  23  30  37  47  58  60  66  68 107 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 288.3582 - loglik: -2.8553e+02 - logprior: -2.8281e+00
Epoch 2/2
19/19 - 5s - loss: 284.3477 - loglik: -2.8334e+02 - logprior: -1.0079e+00
Fitted a model with MAP estimate = -283.2297
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 288.9923 - loglik: -2.8540e+02 - logprior: -3.5938e+00
Epoch 2/10
19/19 - 5s - loss: 285.3332 - loglik: -2.8410e+02 - logprior: -1.2340e+00
Epoch 3/10
19/19 - 5s - loss: 284.2260 - loglik: -2.8320e+02 - logprior: -1.0234e+00
Epoch 4/10
19/19 - 5s - loss: 282.9522 - loglik: -2.8201e+02 - logprior: -9.4583e-01
Epoch 5/10
19/19 - 5s - loss: 283.3600 - loglik: -2.8247e+02 - logprior: -8.9102e-01
Fitted a model with MAP estimate = -283.4905
Time for alignment: 124.9278
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.5334 - loglik: -3.9452e+02 - logprior: -3.0155e+00
Epoch 2/10
19/19 - 4s - loss: 325.9315 - loglik: -3.2468e+02 - logprior: -1.2478e+00
Epoch 3/10
19/19 - 4s - loss: 304.7989 - loglik: -3.0317e+02 - logprior: -1.6320e+00
Epoch 4/10
19/19 - 4s - loss: 300.6791 - loglik: -2.9922e+02 - logprior: -1.4635e+00
Epoch 5/10
19/19 - 4s - loss: 298.5579 - loglik: -2.9715e+02 - logprior: -1.4106e+00
Epoch 6/10
19/19 - 4s - loss: 297.6532 - loglik: -2.9627e+02 - logprior: -1.3829e+00
Epoch 7/10
19/19 - 4s - loss: 296.2551 - loglik: -2.9489e+02 - logprior: -1.3639e+00
Epoch 8/10
19/19 - 4s - loss: 295.5334 - loglik: -2.9419e+02 - logprior: -1.3467e+00
Epoch 9/10
19/19 - 4s - loss: 296.2470 - loglik: -2.9491e+02 - logprior: -1.3381e+00
Fitted a model with MAP estimate = -294.5021
expansions: [(7, 2), (22, 2), (23, 2), (24, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 299.1265 - loglik: -2.9517e+02 - logprior: -3.9571e+00
Epoch 2/2
19/19 - 5s - loss: 289.6382 - loglik: -2.8744e+02 - logprior: -2.1953e+00
Fitted a model with MAP estimate = -287.4651
expansions: [(0, 2)]
discards: [  0  24  26  31  37  49  61  68 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 288.3817 - loglik: -2.8553e+02 - logprior: -2.8476e+00
Epoch 2/2
19/19 - 5s - loss: 284.5202 - loglik: -2.8349e+02 - logprior: -1.0293e+00
Fitted a model with MAP estimate = -283.0378
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 289.2440 - loglik: -2.8563e+02 - logprior: -3.6111e+00
Epoch 2/10
19/19 - 5s - loss: 283.9504 - loglik: -2.8270e+02 - logprior: -1.2467e+00
Epoch 3/10
19/19 - 5s - loss: 283.6008 - loglik: -2.8256e+02 - logprior: -1.0410e+00
Epoch 4/10
19/19 - 5s - loss: 284.1199 - loglik: -2.8316e+02 - logprior: -9.6466e-01
Fitted a model with MAP estimate = -283.3657
Time for alignment: 116.2610
Computed alignments with likelihoods: ['-282.9840', '-283.2297', '-283.0378']
Best model has likelihood: -282.9840
SP score = 0.2004
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9545c4dfa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d969e070>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 489.0429 - loglik: -4.8621e+02 - logprior: -2.8360e+00
Epoch 2/10
19/19 - 5s - loss: 450.4605 - loglik: -4.4936e+02 - logprior: -1.1051e+00
Epoch 3/10
19/19 - 5s - loss: 431.4174 - loglik: -4.3011e+02 - logprior: -1.3025e+00
Epoch 4/10
19/19 - 5s - loss: 426.7685 - loglik: -4.2550e+02 - logprior: -1.2731e+00
Epoch 5/10
19/19 - 5s - loss: 423.6321 - loglik: -4.2231e+02 - logprior: -1.3263e+00
Epoch 6/10
19/19 - 5s - loss: 423.0320 - loglik: -4.2173e+02 - logprior: -1.3068e+00
Epoch 7/10
19/19 - 5s - loss: 422.3021 - loglik: -4.2100e+02 - logprior: -1.3044e+00
Epoch 8/10
19/19 - 5s - loss: 422.4684 - loglik: -4.2117e+02 - logprior: -1.2944e+00
Fitted a model with MAP estimate = -421.6119
expansions: [(20, 1), (31, 1), (32, 1), (46, 1), (47, 1), (48, 1), (49, 2), (58, 1), (59, 4), (74, 1), (76, 2), (77, 4), (78, 1), (94, 1), (97, 1), (101, 1), (106, 1), (107, 1), (109, 2)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 418.8969 - loglik: -4.1602e+02 - logprior: -2.8743e+00
Epoch 2/2
19/19 - 6s - loss: 411.7498 - loglik: -4.1069e+02 - logprior: -1.0565e+00
Fitted a model with MAP estimate = -411.1069
expansions: []
discards: [69 70 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 413.6622 - loglik: -4.1085e+02 - logprior: -2.8082e+00
Epoch 2/2
19/19 - 6s - loss: 411.2123 - loglik: -4.1028e+02 - logprior: -9.3548e-01
Fitted a model with MAP estimate = -410.6565
expansions: []
discards: [51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 413.4472 - loglik: -4.1066e+02 - logprior: -2.7901e+00
Epoch 2/10
19/19 - 6s - loss: 411.3229 - loglik: -4.1044e+02 - logprior: -8.8249e-01
Epoch 3/10
19/19 - 6s - loss: 410.8170 - loglik: -4.1013e+02 - logprior: -6.8725e-01
Epoch 4/10
19/19 - 6s - loss: 411.2844 - loglik: -4.1064e+02 - logprior: -6.4677e-01
Fitted a model with MAP estimate = -410.5233
Time for alignment: 141.9200
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 489.5063 - loglik: -4.8667e+02 - logprior: -2.8370e+00
Epoch 2/10
19/19 - 5s - loss: 449.8156 - loglik: -4.4872e+02 - logprior: -1.0944e+00
Epoch 3/10
19/19 - 5s - loss: 430.5166 - loglik: -4.2917e+02 - logprior: -1.3499e+00
Epoch 4/10
19/19 - 5s - loss: 426.6998 - loglik: -4.2536e+02 - logprior: -1.3352e+00
Epoch 5/10
19/19 - 5s - loss: 423.8532 - loglik: -4.2248e+02 - logprior: -1.3726e+00
Epoch 6/10
19/19 - 5s - loss: 422.3692 - loglik: -4.2100e+02 - logprior: -1.3659e+00
Epoch 7/10
19/19 - 5s - loss: 421.2782 - loglik: -4.1991e+02 - logprior: -1.3649e+00
Epoch 8/10
19/19 - 5s - loss: 423.0102 - loglik: -4.2164e+02 - logprior: -1.3653e+00
Fitted a model with MAP estimate = -421.1266
expansions: [(19, 2), (20, 1), (21, 1), (32, 4), (33, 3), (46, 1), (47, 1), (48, 1), (49, 2), (74, 1), (76, 2), (79, 3), (80, 5), (94, 1), (106, 1), (107, 1), (108, 1), (110, 1), (112, 2)]
discards: [ 1 58 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 419.3348 - loglik: -4.1645e+02 - logprior: -2.8819e+00
Epoch 2/2
19/19 - 7s - loss: 412.5077 - loglik: -4.1145e+02 - logprior: -1.0555e+00
Fitted a model with MAP estimate = -410.9208
expansions: []
discards: [ 18  37  91  98 101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 414.5682 - loglik: -4.1176e+02 - logprior: -2.8079e+00
Epoch 2/2
19/19 - 6s - loss: 411.6656 - loglik: -4.1071e+02 - logprior: -9.5147e-01
Fitted a model with MAP estimate = -411.0512
expansions: []
discards: [57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 413.1691 - loglik: -4.1037e+02 - logprior: -2.8040e+00
Epoch 2/10
19/19 - 6s - loss: 412.2496 - loglik: -4.1135e+02 - logprior: -8.9827e-01
Epoch 3/10
19/19 - 6s - loss: 411.2184 - loglik: -4.1050e+02 - logprior: -7.1529e-01
Epoch 4/10
19/19 - 6s - loss: 411.5906 - loglik: -4.1094e+02 - logprior: -6.4921e-01
Fitted a model with MAP estimate = -410.8463
Time for alignment: 141.9927
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 488.7865 - loglik: -4.8595e+02 - logprior: -2.8342e+00
Epoch 2/10
19/19 - 5s - loss: 449.0934 - loglik: -4.4803e+02 - logprior: -1.0635e+00
Epoch 3/10
19/19 - 5s - loss: 431.4293 - loglik: -4.3021e+02 - logprior: -1.2228e+00
Epoch 4/10
19/19 - 5s - loss: 427.4915 - loglik: -4.2634e+02 - logprior: -1.1548e+00
Epoch 5/10
19/19 - 5s - loss: 424.9305 - loglik: -4.2374e+02 - logprior: -1.1950e+00
Epoch 6/10
19/19 - 5s - loss: 425.7949 - loglik: -4.2459e+02 - logprior: -1.2071e+00
Fitted a model with MAP estimate = -423.6366
expansions: [(20, 1), (31, 1), (32, 1), (33, 3), (46, 1), (47, 2), (48, 1), (49, 2), (58, 3), (59, 3), (74, 1), (76, 2), (77, 4), (78, 3), (93, 1), (107, 3), (109, 2)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 420.2483 - loglik: -4.1663e+02 - logprior: -3.6146e+00
Epoch 2/2
19/19 - 7s - loss: 412.4277 - loglik: -4.1064e+02 - logprior: -1.7879e+00
Fitted a model with MAP estimate = -410.3582
expansions: [(0, 4)]
discards: [  0  55  73  74  75  98 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 412.8656 - loglik: -4.0999e+02 - logprior: -2.8737e+00
Epoch 2/2
19/19 - 7s - loss: 409.2302 - loglik: -4.0811e+02 - logprior: -1.1168e+00
Fitted a model with MAP estimate = -407.8280
expansions: []
discards: [ 0  2 70 71 72 73 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 415.4194 - loglik: -4.1186e+02 - logprior: -3.5573e+00
Epoch 2/10
19/19 - 6s - loss: 412.4049 - loglik: -4.1084e+02 - logprior: -1.5696e+00
Epoch 3/10
19/19 - 6s - loss: 410.4333 - loglik: -4.0960e+02 - logprior: -8.2975e-01
Epoch 4/10
19/19 - 6s - loss: 410.9818 - loglik: -4.1037e+02 - logprior: -6.1329e-01
Fitted a model with MAP estimate = -410.4039
Time for alignment: 131.5598
Computed alignments with likelihoods: ['-410.5233', '-410.8463', '-407.8280']
Best model has likelihood: -407.8280
SP score = 0.9095
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f961ea0bdf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95d9414ee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 793.4988 - loglik: -7.9207e+02 - logprior: -1.4306e+00
Epoch 2/10
39/39 - 18s - loss: 725.8611 - loglik: -7.2510e+02 - logprior: -7.6240e-01
Epoch 3/10
39/39 - 18s - loss: 717.2220 - loglik: -7.1632e+02 - logprior: -8.9872e-01
Epoch 4/10
39/39 - 18s - loss: 714.3282 - loglik: -7.1342e+02 - logprior: -9.0362e-01
Epoch 5/10
39/39 - 18s - loss: 712.4813 - loglik: -7.1157e+02 - logprior: -9.1424e-01
Epoch 6/10
39/39 - 18s - loss: 711.8605 - loglik: -7.1094e+02 - logprior: -9.2350e-01
Epoch 7/10
39/39 - 18s - loss: 711.5239 - loglik: -7.1060e+02 - logprior: -9.2076e-01
Epoch 8/10
39/39 - 18s - loss: 711.9173 - loglik: -7.1100e+02 - logprior: -9.1431e-01
Fitted a model with MAP estimate = -710.4976
expansions: [(0, 3), (9, 2), (24, 1), (44, 1), (52, 1), (53, 3), (61, 1), (86, 1), (95, 1), (98, 2), (104, 1), (115, 3), (116, 2), (121, 1), (125, 1), (127, 1), (131, 1), (151, 2), (153, 3), (178, 5), (206, 6), (208, 1), (209, 1), (214, 1)]
discards: [156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 706.7947 - loglik: -7.0457e+02 - logprior: -2.2276e+00
Epoch 2/2
39/39 - 23s - loss: 701.4297 - loglik: -7.0077e+02 - logprior: -6.5824e-01
Fitted a model with MAP estimate = -699.6745
expansions: [(209, 2), (210, 1)]
discards: [  1   2   3   4  13  62 113 136 186 244]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 702.2526 - loglik: -7.0079e+02 - logprior: -1.4669e+00
Epoch 2/2
39/39 - 22s - loss: 700.4128 - loglik: -7.0011e+02 - logprior: -3.0161e-01
Fitted a model with MAP estimate = -698.6574
expansions: [(0, 3), (95, 7)]
discards: [200]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 699.2649 - loglik: -6.9723e+02 - logprior: -2.0389e+00
Epoch 2/10
39/39 - 23s - loss: 696.5374 - loglik: -6.9631e+02 - logprior: -2.3174e-01
Epoch 3/10
39/39 - 23s - loss: 695.4929 - loglik: -6.9540e+02 - logprior: -9.1865e-02
Epoch 4/10
39/39 - 23s - loss: 695.4677 - loglik: -6.9548e+02 - logprior: 0.0078
Epoch 5/10
39/39 - 23s - loss: 695.4330 - loglik: -6.9554e+02 - logprior: 0.1067
Epoch 6/10
39/39 - 23s - loss: 694.9284 - loglik: -6.9514e+02 - logprior: 0.2150
Epoch 7/10
39/39 - 23s - loss: 695.1270 - loglik: -6.9546e+02 - logprior: 0.3289
Fitted a model with MAP estimate = -694.2743
Time for alignment: 523.4352
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 792.6631 - loglik: -7.9126e+02 - logprior: -1.4047e+00
Epoch 2/10
39/39 - 18s - loss: 726.8997 - loglik: -7.2632e+02 - logprior: -5.8118e-01
Epoch 3/10
39/39 - 18s - loss: 715.8145 - loglik: -7.1506e+02 - logprior: -7.5081e-01
Epoch 4/10
39/39 - 18s - loss: 713.1260 - loglik: -7.1236e+02 - logprior: -7.6705e-01
Epoch 5/10
39/39 - 18s - loss: 711.9776 - loglik: -7.1119e+02 - logprior: -7.8606e-01
Epoch 6/10
39/39 - 18s - loss: 710.9641 - loglik: -7.1018e+02 - logprior: -7.8116e-01
Epoch 7/10
39/39 - 18s - loss: 711.1403 - loglik: -7.1036e+02 - logprior: -7.8212e-01
Fitted a model with MAP estimate = -710.2770
expansions: [(0, 4), (25, 2), (26, 1), (42, 2), (50, 2), (51, 3), (88, 6), (95, 1), (116, 2), (119, 1), (120, 1), (122, 1), (125, 1), (153, 1), (155, 3), (179, 1), (180, 5), (206, 7), (207, 1), (214, 1)]
discards: [157 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 706.7108 - loglik: -7.0446e+02 - logprior: -2.2545e+00
Epoch 2/2
39/39 - 23s - loss: 700.7192 - loglik: -7.0007e+02 - logprior: -6.5403e-01
Fitted a model with MAP estimate = -698.8621
expansions: [(98, 4), (211, 2), (212, 2)]
discards: [  1   2   3   4  30  59  62 103 185 218 242]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 700.0204 - loglik: -6.9861e+02 - logprior: -1.4134e+00
Epoch 2/2
39/39 - 23s - loss: 698.1028 - loglik: -6.9778e+02 - logprior: -3.2146e-01
Fitted a model with MAP estimate = -696.2384
expansions: [(0, 4), (92, 1), (207, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 278 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 697.8463 - loglik: -6.9578e+02 - logprior: -2.0645e+00
Epoch 2/10
39/39 - 23s - loss: 696.4577 - loglik: -6.9622e+02 - logprior: -2.4009e-01
Epoch 3/10
39/39 - 23s - loss: 695.0237 - loglik: -6.9495e+02 - logprior: -7.2884e-02
Epoch 4/10
39/39 - 23s - loss: 694.9460 - loglik: -6.9498e+02 - logprior: 0.0328
Epoch 5/10
39/39 - 23s - loss: 694.7374 - loglik: -6.9490e+02 - logprior: 0.1662
Epoch 6/10
39/39 - 24s - loss: 694.9142 - loglik: -6.9518e+02 - logprior: 0.2700
Fitted a model with MAP estimate = -694.2102
Time for alignment: 485.9352
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 794.0120 - loglik: -7.9258e+02 - logprior: -1.4300e+00
Epoch 2/10
39/39 - 18s - loss: 725.0766 - loglik: -7.2436e+02 - logprior: -7.1778e-01
Epoch 3/10
39/39 - 18s - loss: 715.3177 - loglik: -7.1449e+02 - logprior: -8.2443e-01
Epoch 4/10
39/39 - 18s - loss: 712.9496 - loglik: -7.1211e+02 - logprior: -8.4175e-01
Epoch 5/10
39/39 - 18s - loss: 711.2400 - loglik: -7.1039e+02 - logprior: -8.4801e-01
Epoch 6/10
39/39 - 18s - loss: 710.9933 - loglik: -7.1013e+02 - logprior: -8.5882e-01
Epoch 7/10
39/39 - 18s - loss: 710.4033 - loglik: -7.0954e+02 - logprior: -8.5854e-01
Epoch 8/10
39/39 - 18s - loss: 710.2250 - loglik: -7.0936e+02 - logprior: -8.6122e-01
Epoch 9/10
39/39 - 18s - loss: 710.4626 - loglik: -7.0958e+02 - logprior: -8.8334e-01
Fitted a model with MAP estimate = -709.3494
expansions: [(0, 3), (9, 1), (10, 1), (24, 1), (26, 2), (42, 1), (50, 1), (56, 1), (62, 1), (80, 1), (85, 1), (90, 10), (102, 1), (115, 4), (116, 2), (117, 1), (120, 1), (153, 1), (154, 1), (156, 1), (158, 2), (176, 1), (179, 7), (205, 7), (214, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 703.6013 - loglik: -7.0141e+02 - logprior: -2.1943e+00
Epoch 2/2
39/39 - 24s - loss: 697.3146 - loglik: -6.9672e+02 - logprior: -5.9400e-01
Fitted a model with MAP estimate = -695.3375
expansions: []
discards: [  1   2   3  33 142 143 144 195 252]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 698.0391 - loglik: -6.9654e+02 - logprior: -1.5007e+00
Epoch 2/2
39/39 - 23s - loss: 696.3923 - loglik: -6.9603e+02 - logprior: -3.6479e-01
Fitted a model with MAP estimate = -694.9592
expansions: [(0, 3), (183, 3)]
discards: [273]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 697.2970 - loglik: -6.9518e+02 - logprior: -2.1139e+00
Epoch 2/10
39/39 - 24s - loss: 695.3424 - loglik: -6.9523e+02 - logprior: -1.1408e-01
Epoch 3/10
39/39 - 24s - loss: 694.6171 - loglik: -6.9468e+02 - logprior: 0.0665
Epoch 4/10
39/39 - 24s - loss: 694.3868 - loglik: -6.9456e+02 - logprior: 0.1782
Epoch 5/10
39/39 - 24s - loss: 694.0704 - loglik: -6.9437e+02 - logprior: 0.2985
Epoch 6/10
39/39 - 24s - loss: 693.9348 - loglik: -6.9435e+02 - logprior: 0.4139
Epoch 7/10
39/39 - 24s - loss: 694.3746 - loglik: -6.9492e+02 - logprior: 0.5463
Fitted a model with MAP estimate = -693.5987
Time for alignment: 550.5523
Computed alignments with likelihoods: ['-694.2743', '-694.2102', '-693.5987']
Best model has likelihood: -693.5987
SP score = 0.6217
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f966be3bd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f9663350610>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 623.4042 - loglik: -6.2079e+02 - logprior: -2.6191e+00
Epoch 2/10
19/19 - 8s - loss: 527.6879 - loglik: -5.2662e+02 - logprior: -1.0638e+00
Epoch 3/10
19/19 - 8s - loss: 484.6249 - loglik: -4.8305e+02 - logprior: -1.5751e+00
Epoch 4/10
19/19 - 8s - loss: 475.6557 - loglik: -4.7398e+02 - logprior: -1.6788e+00
Epoch 5/10
19/19 - 8s - loss: 473.2595 - loglik: -4.7156e+02 - logprior: -1.7026e+00
Epoch 6/10
19/19 - 8s - loss: 471.5673 - loglik: -4.6986e+02 - logprior: -1.7104e+00
Epoch 7/10
19/19 - 8s - loss: 471.2265 - loglik: -4.6953e+02 - logprior: -1.7006e+00
Epoch 8/10
19/19 - 8s - loss: 470.4727 - loglik: -4.6880e+02 - logprior: -1.6728e+00
Epoch 9/10
19/19 - 8s - loss: 470.0746 - loglik: -4.6841e+02 - logprior: -1.6635e+00
Epoch 10/10
19/19 - 8s - loss: 470.8505 - loglik: -4.6919e+02 - logprior: -1.6604e+00
Fitted a model with MAP estimate = -467.5121
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (52, 1), (66, 1), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (183, 2), (184, 1), (188, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 467.3803 - loglik: -4.6385e+02 - logprior: -3.5306e+00
Epoch 2/2
19/19 - 11s - loss: 452.8167 - loglik: -4.5097e+02 - logprior: -1.8513e+00
Fitted a model with MAP estimate = -448.6392
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 190 191 195 201 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 456.0615 - loglik: -4.5262e+02 - logprior: -3.4408e+00
Epoch 2/2
19/19 - 11s - loss: 450.8570 - loglik: -4.4973e+02 - logprior: -1.1221e+00
Fitted a model with MAP estimate = -447.4737
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 452.1668 - loglik: -4.4868e+02 - logprior: -3.4835e+00
Epoch 2/10
19/19 - 11s - loss: 449.0213 - loglik: -4.4741e+02 - logprior: -1.6141e+00
Epoch 3/10
19/19 - 11s - loss: 447.9222 - loglik: -4.4661e+02 - logprior: -1.3164e+00
Epoch 4/10
19/19 - 11s - loss: 446.3534 - loglik: -4.4579e+02 - logprior: -5.6772e-01
Epoch 5/10
19/19 - 11s - loss: 447.1093 - loglik: -4.4671e+02 - logprior: -3.9933e-01
Fitted a model with MAP estimate = -446.3390
Time for alignment: 264.4153
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.6019 - loglik: -6.2098e+02 - logprior: -2.6208e+00
Epoch 2/10
19/19 - 8s - loss: 527.3232 - loglik: -5.2625e+02 - logprior: -1.0706e+00
Epoch 3/10
19/19 - 8s - loss: 484.8827 - loglik: -4.8326e+02 - logprior: -1.6247e+00
Epoch 4/10
19/19 - 8s - loss: 474.0313 - loglik: -4.7230e+02 - logprior: -1.7328e+00
Epoch 5/10
19/19 - 8s - loss: 472.5881 - loglik: -4.7080e+02 - logprior: -1.7911e+00
Epoch 6/10
19/19 - 8s - loss: 470.7580 - loglik: -4.6903e+02 - logprior: -1.7311e+00
Epoch 7/10
19/19 - 8s - loss: 470.1526 - loglik: -4.6845e+02 - logprior: -1.7008e+00
Epoch 8/10
19/19 - 8s - loss: 470.1556 - loglik: -4.6848e+02 - logprior: -1.6740e+00
Fitted a model with MAP estimate = -467.2654
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (52, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 2), (154, 1), (155, 4), (156, 2), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 466.7278 - loglik: -4.6315e+02 - logprior: -3.5739e+00
Epoch 2/2
19/19 - 11s - loss: 453.3588 - loglik: -4.5144e+02 - logprior: -1.9165e+00
Fitted a model with MAP estimate = -448.9305
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 200 202]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 455.4240 - loglik: -4.5199e+02 - logprior: -3.4374e+00
Epoch 2/2
19/19 - 11s - loss: 450.8512 - loglik: -4.4977e+02 - logprior: -1.0798e+00
Fitted a model with MAP estimate = -447.1132
expansions: [(3, 1)]
discards: [  0 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 452.1901 - loglik: -4.4872e+02 - logprior: -3.4678e+00
Epoch 2/10
19/19 - 11s - loss: 448.4627 - loglik: -4.4687e+02 - logprior: -1.5963e+00
Epoch 3/10
19/19 - 11s - loss: 447.8865 - loglik: -4.4653e+02 - logprior: -1.3515e+00
Epoch 4/10
19/19 - 11s - loss: 446.3387 - loglik: -4.4573e+02 - logprior: -6.1292e-01
Epoch 5/10
19/19 - 11s - loss: 446.6196 - loglik: -4.4619e+02 - logprior: -4.2549e-01
Fitted a model with MAP estimate = -446.1406
Time for alignment: 249.5252
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.7224 - loglik: -6.2110e+02 - logprior: -2.6218e+00
Epoch 2/10
19/19 - 8s - loss: 527.3669 - loglik: -5.2630e+02 - logprior: -1.0692e+00
Epoch 3/10
19/19 - 8s - loss: 484.7221 - loglik: -4.8312e+02 - logprior: -1.6067e+00
Epoch 4/10
19/19 - 8s - loss: 475.1662 - loglik: -4.7345e+02 - logprior: -1.7124e+00
Epoch 5/10
19/19 - 8s - loss: 473.6785 - loglik: -4.7194e+02 - logprior: -1.7339e+00
Epoch 6/10
19/19 - 8s - loss: 471.7855 - loglik: -4.7008e+02 - logprior: -1.7019e+00
Epoch 7/10
19/19 - 8s - loss: 471.9919 - loglik: -4.7030e+02 - logprior: -1.6924e+00
Fitted a model with MAP estimate = -468.7916
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 3), (151, 2), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (180, 1), (182, 2), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 467.4877 - loglik: -4.6391e+02 - logprior: -3.5740e+00
Epoch 2/2
19/19 - 11s - loss: 452.6792 - loglik: -4.5078e+02 - logprior: -1.8944e+00
Fitted a model with MAP estimate = -448.5534
expansions: [(3, 1)]
discards: [  0  26  84 139 142 190 191 201 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 455.5442 - loglik: -4.5211e+02 - logprior: -3.4368e+00
Epoch 2/2
19/19 - 11s - loss: 450.9088 - loglik: -4.4984e+02 - logprior: -1.0658e+00
Fitted a model with MAP estimate = -447.1448
expansions: [(3, 1)]
discards: [  0 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 451.9137 - loglik: -4.4845e+02 - logprior: -3.4631e+00
Epoch 2/10
19/19 - 11s - loss: 448.7414 - loglik: -4.4713e+02 - logprior: -1.6076e+00
Epoch 3/10
19/19 - 11s - loss: 447.8346 - loglik: -4.4650e+02 - logprior: -1.3357e+00
Epoch 4/10
19/19 - 11s - loss: 446.0980 - loglik: -4.4550e+02 - logprior: -5.9759e-01
Epoch 5/10
19/19 - 11s - loss: 446.6510 - loglik: -4.4623e+02 - logprior: -4.1811e-01
Fitted a model with MAP estimate = -446.2147
Time for alignment: 239.7070
Computed alignments with likelihoods: ['-446.3390', '-446.1406', '-446.2147']
Best model has likelihood: -446.1406
SP score = 0.6464
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f93d03f5430>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f93d03f51f0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001b8e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001ba60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bb50>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bbb0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f943001beb0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f943001bdf0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422460>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422d30>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422040>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422cd0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422100>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422b50>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422af0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f93d0422d90> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f93d0422310>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f93d0422190> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f93d0399670> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95cfed1eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f95cfebd070>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f955579e670> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f954a1b9d30>
 , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f93d0422220> , kernel_dim : alphabet_size , trainable_exchangeabilities : False
 , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ] , encoder_weight_extractor : <function extract_encoder_weights at 0x7f95e3e6c160>
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.8701 - loglik: -3.1779e+02 - logprior: -3.0829e+00
Epoch 2/10
19/19 - 2s - loss: 276.9340 - loglik: -2.7558e+02 - logprior: -1.3586e+00
Epoch 3/10
19/19 - 2s - loss: 252.3122 - loglik: -2.5061e+02 - logprior: -1.7047e+00
Epoch 4/10
19/19 - 2s - loss: 246.5619 - loglik: -2.4490e+02 - logprior: -1.6569e+00
Epoch 5/10
19/19 - 2s - loss: 244.2415 - loglik: -2.4265e+02 - logprior: -1.5958e+00
Epoch 6/10
19/19 - 2s - loss: 243.3311 - loglik: -2.4178e+02 - logprior: -1.5470e+00
Epoch 7/10
19/19 - 2s - loss: 243.1374 - loglik: -2.4160e+02 - logprior: -1.5402e+00
Epoch 8/10
19/19 - 2s - loss: 242.7053 - loglik: -2.4117e+02 - logprior: -1.5328e+00
Epoch 9/10
19/19 - 2s - loss: 242.5761 - loglik: -2.4105e+02 - logprior: -1.5280e+00
Epoch 10/10
19/19 - 2s - loss: 242.8945 - loglik: -2.4136e+02 - logprior: -1.5310e+00
Fitted a model with MAP estimate = -242.4002
expansions: [(17, 1), (19, 2), (20, 4), (22, 2), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (48, 1), (50, 1), (56, 1), (59, 2), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 246.1057 - loglik: -2.4215e+02 - logprior: -3.9555e+00
Epoch 2/2
19/19 - 3s - loss: 235.7512 - loglik: -2.3377e+02 - logprior: -1.9830e+00
Fitted a model with MAP estimate = -234.4321
expansions: [(0, 2)]
discards: [ 0 15 23 29 38 44 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 235.2031 - loglik: -2.3229e+02 - logprior: -2.9085e+00
Epoch 2/2
19/19 - 3s - loss: 231.4650 - loglik: -2.3038e+02 - logprior: -1.0810e+00
Fitted a model with MAP estimate = -230.9598
expansions: []
discards: [ 0 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 236.9055 - loglik: -2.3323e+02 - logprior: -3.6759e+00
Epoch 2/10
19/19 - 3s - loss: 232.8162 - loglik: -2.3155e+02 - logprior: -1.2670e+00
Epoch 3/10
19/19 - 3s - loss: 231.7807 - loglik: -2.3068e+02 - logprior: -1.0970e+00
Epoch 4/10
19/19 - 3s - loss: 231.3253 - loglik: -2.3028e+02 - logprior: -1.0444e+00
Epoch 5/10
19/19 - 3s - loss: 231.2257 - loglik: -2.3023e+02 - logprior: -1.0006e+00
Epoch 6/10
19/19 - 3s - loss: 231.2229 - loglik: -2.3027e+02 - logprior: -9.5563e-01
Epoch 7/10
19/19 - 3s - loss: 231.4110 - loglik: -2.3048e+02 - logprior: -9.2823e-01
Fitted a model with MAP estimate = -231.0850
Time for alignment: 85.9002
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.8374 - loglik: -3.1775e+02 - logprior: -3.0837e+00
Epoch 2/10
19/19 - 2s - loss: 276.9339 - loglik: -2.7557e+02 - logprior: -1.3615e+00
Epoch 3/10
19/19 - 2s - loss: 253.7601 - loglik: -2.5207e+02 - logprior: -1.6929e+00
Epoch 4/10
19/19 - 2s - loss: 247.6209 - loglik: -2.4598e+02 - logprior: -1.6403e+00
Epoch 5/10
19/19 - 2s - loss: 245.0409 - loglik: -2.4340e+02 - logprior: -1.6432e+00
Epoch 6/10
19/19 - 2s - loss: 244.5488 - loglik: -2.4293e+02 - logprior: -1.6167e+00
Epoch 7/10
19/19 - 2s - loss: 244.0682 - loglik: -2.4247e+02 - logprior: -1.6002e+00
Epoch 8/10
19/19 - 2s - loss: 244.0235 - loglik: -2.4244e+02 - logprior: -1.5852e+00
Epoch 9/10
19/19 - 2s - loss: 243.6224 - loglik: -2.4205e+02 - logprior: -1.5773e+00
Epoch 10/10
19/19 - 2s - loss: 243.6222 - loglik: -2.4205e+02 - logprior: -1.5732e+00
Fitted a model with MAP estimate = -243.4700
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 1), (23, 1), (27, 1), (28, 2), (32, 2), (40, 1), (43, 1), (44, 1), (50, 1), (56, 1), (63, 1), (64, 1), (66, 2), (68, 1), (71, 1), (76, 1), (77, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.8464 - loglik: -2.4186e+02 - logprior: -3.9876e+00
Epoch 2/2
19/19 - 3s - loss: 235.5157 - loglik: -2.3343e+02 - logprior: -2.0883e+00
Fitted a model with MAP estimate = -234.0355
expansions: [(0, 2)]
discards: [ 0 15 16 22 37 43 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 235.3125 - loglik: -2.3241e+02 - logprior: -2.9071e+00
Epoch 2/2
19/19 - 3s - loss: 232.1139 - loglik: -2.3107e+02 - logprior: -1.0444e+00
Fitted a model with MAP estimate = -231.4412
expansions: [(19, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 237.1959 - loglik: -2.3344e+02 - logprior: -3.7514e+00
Epoch 2/10
19/19 - 3s - loss: 233.0793 - loglik: -2.3178e+02 - logprior: -1.3034e+00
Epoch 3/10
19/19 - 3s - loss: 231.3771 - loglik: -2.3029e+02 - logprior: -1.0918e+00
Epoch 4/10
19/19 - 3s - loss: 230.8667 - loglik: -2.2983e+02 - logprior: -1.0392e+00
Epoch 5/10
19/19 - 3s - loss: 230.9356 - loglik: -2.2994e+02 - logprior: -9.9116e-01
Fitted a model with MAP estimate = -230.8625
Time for alignment: 78.8734
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.9299 - loglik: -3.1784e+02 - logprior: -3.0861e+00
Epoch 2/10
19/19 - 2s - loss: 276.9243 - loglik: -2.7555e+02 - logprior: -1.3719e+00
Epoch 3/10
19/19 - 2s - loss: 255.3903 - loglik: -2.5369e+02 - logprior: -1.6957e+00
Epoch 4/10
19/19 - 2s - loss: 248.5963 - loglik: -2.4697e+02 - logprior: -1.6239e+00
Epoch 5/10
19/19 - 2s - loss: 245.8964 - loglik: -2.4428e+02 - logprior: -1.6123e+00
Epoch 6/10
19/19 - 2s - loss: 245.1828 - loglik: -2.4361e+02 - logprior: -1.5720e+00
Epoch 7/10
19/19 - 2s - loss: 244.3291 - loglik: -2.4276e+02 - logprior: -1.5679e+00
Epoch 8/10
19/19 - 2s - loss: 244.2612 - loglik: -2.4270e+02 - logprior: -1.5579e+00
Epoch 9/10
19/19 - 2s - loss: 243.8671 - loglik: -2.4231e+02 - logprior: -1.5588e+00
Epoch 10/10
19/19 - 2s - loss: 244.8378 - loglik: -2.4328e+02 - logprior: -1.5581e+00
Fitted a model with MAP estimate = -243.8531
expansions: [(17, 1), (18, 2), (19, 2), (20, 3), (22, 2), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (56, 1), (59, 2), (64, 2), (65, 2), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 247.3298 - loglik: -2.4331e+02 - logprior: -4.0212e+00
Epoch 2/2
19/19 - 3s - loss: 235.6907 - loglik: -2.3354e+02 - logprior: -2.1524e+00
Fitted a model with MAP estimate = -233.9548
expansions: [(0, 2)]
discards: [ 0 15 16 19 24 25 30 39 45 79 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 235.5431 - loglik: -2.3263e+02 - logprior: -2.9100e+00
Epoch 2/2
19/19 - 3s - loss: 232.3837 - loglik: -2.3134e+02 - logprior: -1.0481e+00
Fitted a model with MAP estimate = -231.6774
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 237.2612 - loglik: -2.3359e+02 - logprior: -3.6672e+00
Epoch 2/10
19/19 - 3s - loss: 233.1063 - loglik: -2.3185e+02 - logprior: -1.2606e+00
Epoch 3/10
19/19 - 3s - loss: 231.8078 - loglik: -2.3072e+02 - logprior: -1.0895e+00
Epoch 4/10
19/19 - 3s - loss: 231.5941 - loglik: -2.3057e+02 - logprior: -1.0271e+00
Epoch 5/10
19/19 - 3s - loss: 231.5263 - loglik: -2.3053e+02 - logprior: -9.9286e-01
Epoch 6/10
19/19 - 3s - loss: 231.5742 - loglik: -2.3062e+02 - logprior: -9.5642e-01
Fitted a model with MAP estimate = -231.2143
Time for alignment: 81.4184
Computed alignments with likelihoods: ['-230.9598', '-230.8625', '-231.2143']
Best model has likelihood: -230.8625
SP score = 0.8493
