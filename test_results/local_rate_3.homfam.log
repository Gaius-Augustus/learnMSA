Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6e1d581f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6e1d585b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6e1d58130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c563d5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c563d430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c563d610>, <__main__.SimpleDirichletPrior object at 0x7fb6d8cc6790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.5904 - loglik: -5.3343e+02 - logprior: -5.1243e+00
Epoch 2/10
25/25 - 8s - loss: 430.5643 - loglik: -4.2821e+02 - logprior: -2.0891e+00
Epoch 3/10
25/25 - 8s - loss: 406.3172 - loglik: -4.0294e+02 - logprior: -2.4666e+00
Epoch 4/10
25/25 - 8s - loss: 404.2262 - loglik: -4.0097e+02 - logprior: -2.4019e+00
Epoch 5/10
25/25 - 8s - loss: 400.4310 - loglik: -3.9725e+02 - logprior: -2.4253e+00
Epoch 6/10
25/25 - 8s - loss: 401.3778 - loglik: -3.9812e+02 - logprior: -2.4720e+00
Fitted a model with MAP estimate = -399.1035
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 1), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (163, 1), (167, 1), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 405.9270 - loglik: -3.9850e+02 - logprior: -7.2855e+00
Epoch 2/2
25/25 - 9s - loss: 384.6231 - loglik: -3.8112e+02 - logprior: -3.0140e+00
Fitted a model with MAP estimate = -381.7595
expansions: [(0, 3)]
discards: [  0   9  44  61  62  81 106 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 392.8081 - loglik: -3.8791e+02 - logprior: -4.7581e+00
Epoch 2/2
25/25 - 9s - loss: 384.6555 - loglik: -3.8314e+02 - logprior: -9.5929e-01
Fitted a model with MAP estimate = -380.4849
expansions: [(60, 2)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 395.2760 - loglik: -3.8844e+02 - logprior: -6.7035e+00
Epoch 2/10
25/25 - 9s - loss: 384.6050 - loglik: -3.8217e+02 - logprior: -1.8717e+00
Epoch 3/10
25/25 - 9s - loss: 380.9984 - loglik: -3.7975e+02 - logprior: -2.7349e-01
Epoch 4/10
25/25 - 9s - loss: 377.4857 - loglik: -3.7626e+02 - logprior: -2.0921e-01
Epoch 5/10
25/25 - 9s - loss: 376.4714 - loglik: -3.7544e+02 - logprior: -1.2163e-01
Epoch 6/10
25/25 - 9s - loss: 377.2097 - loglik: -3.7638e+02 - logprior: -7.1804e-04
Fitted a model with MAP estimate = -375.1034
Time for alignment: 186.1448
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.6368 - loglik: -5.3346e+02 - logprior: -5.1346e+00
Epoch 2/10
25/25 - 8s - loss: 428.5251 - loglik: -4.2605e+02 - logprior: -2.1742e+00
Epoch 3/10
25/25 - 8s - loss: 406.4380 - loglik: -4.0303e+02 - logprior: -2.4355e+00
Epoch 4/10
25/25 - 8s - loss: 403.3083 - loglik: -4.0008e+02 - logprior: -2.3247e+00
Epoch 5/10
25/25 - 8s - loss: 400.3106 - loglik: -3.9707e+02 - logprior: -2.3623e+00
Epoch 6/10
25/25 - 8s - loss: 400.5542 - loglik: -3.9729e+02 - logprior: -2.4046e+00
Fitted a model with MAP estimate = -398.6849
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (63, 1), (75, 1), (80, 1), (82, 1), (83, 2), (85, 1), (90, 1), (92, 1), (95, 1), (98, 1), (102, 1), (112, 2), (122, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (163, 1), (167, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 402.7486 - loglik: -3.9532e+02 - logprior: -7.2798e+00
Epoch 2/2
25/25 - 9s - loss: 387.5060 - loglik: -3.8401e+02 - logprior: -3.0047e+00
Fitted a model with MAP estimate = -381.8363
expansions: [(0, 3)]
discards: [  0   9  42  46  62  63 144 206]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 391.8197 - loglik: -3.8692e+02 - logprior: -4.7706e+00
Epoch 2/2
25/25 - 10s - loss: 384.0200 - loglik: -3.8251e+02 - logprior: -9.4842e-01
Fitted a model with MAP estimate = -379.8894
expansions: [(60, 2)]
discards: [  0   2 105]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 393.8536 - loglik: -3.8702e+02 - logprior: -6.6975e+00
Epoch 2/10
25/25 - 9s - loss: 385.2719 - loglik: -3.8285e+02 - logprior: -1.8619e+00
Epoch 3/10
25/25 - 9s - loss: 380.1473 - loglik: -3.7891e+02 - logprior: -2.6729e-01
Epoch 4/10
25/25 - 9s - loss: 377.6155 - loglik: -3.7640e+02 - logprior: -2.0935e-01
Epoch 5/10
25/25 - 9s - loss: 376.1566 - loglik: -3.7515e+02 - logprior: -1.1274e-01
Epoch 6/10
25/25 - 9s - loss: 376.1600 - loglik: -3.7535e+02 - logprior: 0.0116
Fitted a model with MAP estimate = -374.5798
Time for alignment: 187.3530
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 539.1204 - loglik: -5.3396e+02 - logprior: -5.1202e+00
Epoch 2/10
25/25 - 8s - loss: 427.3837 - loglik: -4.2509e+02 - logprior: -2.1122e+00
Epoch 3/10
25/25 - 8s - loss: 408.5851 - loglik: -4.0555e+02 - logprior: -2.3577e+00
Epoch 4/10
25/25 - 8s - loss: 403.1143 - loglik: -4.0011e+02 - logprior: -2.2224e+00
Epoch 5/10
25/25 - 8s - loss: 402.7805 - loglik: -3.9981e+02 - logprior: -2.1823e+00
Epoch 6/10
25/25 - 8s - loss: 403.2603 - loglik: -4.0025e+02 - logprior: -2.2143e+00
Fitted a model with MAP estimate = -400.7069
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (91, 1), (92, 2), (96, 1), (102, 1), (112, 1), (113, 2), (122, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 404.6238 - loglik: -3.9714e+02 - logprior: -7.3408e+00
Epoch 2/2
25/25 - 9s - loss: 386.6886 - loglik: -3.8307e+02 - logprior: -3.1269e+00
Fitted a model with MAP estimate = -382.1863
expansions: [(0, 3), (214, 1)]
discards: [  0   9  44  61  81 102 107 144 206]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 392.0209 - loglik: -3.8711e+02 - logprior: -4.7767e+00
Epoch 2/2
25/25 - 9s - loss: 384.0911 - loglik: -3.8255e+02 - logprior: -9.7230e-01
Fitted a model with MAP estimate = -380.0215
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 13s - loss: 394.4801 - loglik: -3.8766e+02 - logprior: -6.6832e+00
Epoch 2/10
25/25 - 9s - loss: 385.3707 - loglik: -3.8296e+02 - logprior: -1.8260e+00
Epoch 3/10
25/25 - 9s - loss: 379.9113 - loglik: -3.7866e+02 - logprior: -2.6977e-01
Epoch 4/10
25/25 - 9s - loss: 379.4260 - loglik: -3.7821e+02 - logprior: -2.0334e-01
Epoch 5/10
25/25 - 9s - loss: 377.6080 - loglik: -3.7659e+02 - logprior: -1.1696e-01
Epoch 6/10
25/25 - 9s - loss: 376.0248 - loglik: -3.7520e+02 - logprior: 0.0018
Epoch 7/10
25/25 - 9s - loss: 376.8684 - loglik: -3.7621e+02 - logprior: 0.1266
Fitted a model with MAP estimate = -375.1399
Time for alignment: 194.6807
Computed alignments with likelihoods: ['-375.1034', '-374.5798', '-375.1399']
Best model has likelihood: -374.5798
time for generating output: 0.2483
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9211364192995346
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6e1d58130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6e1d584f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6e1d585b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb674d23d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb675798d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69fc97760>, <__main__.SimpleDirichletPrior object at 0x7fb686492400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.7108 - loglik: -3.8624e+02 - logprior: -2.0463e+01
Epoch 2/10
10/10 - 2s - loss: 342.3121 - loglik: -3.3735e+02 - logprior: -4.9538e+00
Epoch 3/10
10/10 - 2s - loss: 290.8746 - loglik: -2.8810e+02 - logprior: -2.7609e+00
Epoch 4/10
10/10 - 2s - loss: 257.8318 - loglik: -2.5520e+02 - logprior: -2.4834e+00
Epoch 5/10
10/10 - 2s - loss: 245.8705 - loglik: -2.4305e+02 - logprior: -2.3830e+00
Epoch 6/10
10/10 - 2s - loss: 240.7972 - loglik: -2.3796e+02 - logprior: -2.2265e+00
Epoch 7/10
10/10 - 2s - loss: 238.0941 - loglik: -2.3540e+02 - logprior: -2.1859e+00
Epoch 8/10
10/10 - 2s - loss: 236.7401 - loglik: -2.3412e+02 - logprior: -2.2072e+00
Epoch 9/10
10/10 - 2s - loss: 236.7591 - loglik: -2.3418e+02 - logprior: -2.1728e+00
Fitted a model with MAP estimate = -235.4960
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (18, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.4604 - loglik: -2.3253e+02 - logprior: -2.6908e+01
Epoch 2/2
10/10 - 2s - loss: 219.6323 - loglik: -2.1153e+02 - logprior: -7.9454e+00
Fitted a model with MAP estimate = -212.1543
expansions: []
discards: [ 59  62 136]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.5229 - loglik: -2.0948e+02 - logprior: -1.9018e+01
Epoch 2/2
10/10 - 2s - loss: 208.8584 - loglik: -2.0423e+02 - logprior: -4.4700e+00
Fitted a model with MAP estimate = -206.7730
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.9427 - loglik: -2.0601e+02 - logprior: -1.7911e+01
Epoch 2/10
10/10 - 2s - loss: 209.6783 - loglik: -2.0541e+02 - logprior: -4.1129e+00
Epoch 3/10
10/10 - 2s - loss: 205.5153 - loglik: -2.0362e+02 - logprior: -1.5504e+00
Epoch 4/10
10/10 - 2s - loss: 202.6109 - loglik: -2.0142e+02 - logprior: -7.0763e-01
Epoch 5/10
10/10 - 2s - loss: 202.4539 - loglik: -2.0163e+02 - logprior: -3.1539e-01
Epoch 6/10
10/10 - 2s - loss: 201.4555 - loglik: -2.0094e+02 - logprior: -1.4296e-02
Epoch 7/10
10/10 - 2s - loss: 200.3874 - loglik: -2.0014e+02 - logprior: 0.2444
Epoch 8/10
10/10 - 2s - loss: 199.9265 - loglik: -1.9985e+02 - logprior: 0.4116
Epoch 9/10
10/10 - 2s - loss: 200.3284 - loglik: -2.0035e+02 - logprior: 0.5026
Fitted a model with MAP estimate = -199.2529
Time for alignment: 67.3534
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.8677 - loglik: -3.8640e+02 - logprior: -2.0463e+01
Epoch 2/10
10/10 - 2s - loss: 341.6928 - loglik: -3.3674e+02 - logprior: -4.9477e+00
Epoch 3/10
10/10 - 2s - loss: 289.0952 - loglik: -2.8635e+02 - logprior: -2.7264e+00
Epoch 4/10
10/10 - 2s - loss: 257.8624 - loglik: -2.5532e+02 - logprior: -2.4087e+00
Epoch 5/10
10/10 - 2s - loss: 245.5762 - loglik: -2.4302e+02 - logprior: -2.2863e+00
Epoch 6/10
10/10 - 2s - loss: 242.6143 - loglik: -2.4011e+02 - logprior: -2.0171e+00
Epoch 7/10
10/10 - 2s - loss: 239.1630 - loglik: -2.3689e+02 - logprior: -1.7464e+00
Epoch 8/10
10/10 - 2s - loss: 239.1010 - loglik: -2.3697e+02 - logprior: -1.6643e+00
Epoch 9/10
10/10 - 2s - loss: 237.8068 - loglik: -2.3567e+02 - logprior: -1.6844e+00
Epoch 10/10
10/10 - 2s - loss: 237.5691 - loglik: -2.3545e+02 - logprior: -1.6656e+00
Fitted a model with MAP estimate = -236.8668
expansions: [(0, 4), (13, 3), (20, 1), (26, 1), (30, 1), (31, 2), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.0133 - loglik: -2.3194e+02 - logprior: -2.7053e+01
Epoch 2/2
10/10 - 2s - loss: 220.4309 - loglik: -2.1225e+02 - logprior: -8.0214e+00
Fitted a model with MAP estimate = -212.3399
expansions: []
discards: [ 59  62 136]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.0140 - loglik: -2.0893e+02 - logprior: -1.9056e+01
Epoch 2/2
10/10 - 2s - loss: 209.2532 - loglik: -2.0457e+02 - logprior: -4.5154e+00
Fitted a model with MAP estimate = -206.7386
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 224.9006 - loglik: -2.0692e+02 - logprior: -1.7952e+01
Epoch 2/10
10/10 - 2s - loss: 208.2366 - loglik: -2.0395e+02 - logprior: -4.1374e+00
Epoch 3/10
10/10 - 2s - loss: 205.9481 - loglik: -2.0402e+02 - logprior: -1.5901e+00
Epoch 4/10
10/10 - 2s - loss: 202.7315 - loglik: -2.0152e+02 - logprior: -7.4822e-01
Epoch 5/10
10/10 - 2s - loss: 201.8194 - loglik: -2.0096e+02 - logprior: -3.5028e-01
Epoch 6/10
10/10 - 2s - loss: 201.1695 - loglik: -2.0062e+02 - logprior: -3.9697e-02
Epoch 7/10
10/10 - 2s - loss: 201.0996 - loglik: -2.0081e+02 - logprior: 0.2154
Epoch 8/10
10/10 - 2s - loss: 200.0361 - loglik: -1.9993e+02 - logprior: 0.3834
Epoch 9/10
10/10 - 2s - loss: 200.1043 - loglik: -2.0011e+02 - logprior: 0.4871
Fitted a model with MAP estimate = -199.2816
Time for alignment: 68.7638
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.8719 - loglik: -3.8640e+02 - logprior: -2.0462e+01
Epoch 2/10
10/10 - 2s - loss: 341.7929 - loglik: -3.3684e+02 - logprior: -4.9503e+00
Epoch 3/10
10/10 - 2s - loss: 288.6649 - loglik: -2.8592e+02 - logprior: -2.7356e+00
Epoch 4/10
10/10 - 2s - loss: 256.5482 - loglik: -2.5400e+02 - logprior: -2.4052e+00
Epoch 5/10
10/10 - 2s - loss: 245.9700 - loglik: -2.4322e+02 - logprior: -2.3007e+00
Epoch 6/10
10/10 - 2s - loss: 242.6066 - loglik: -2.3998e+02 - logprior: -2.0261e+00
Epoch 7/10
10/10 - 2s - loss: 240.9041 - loglik: -2.3860e+02 - logprior: -1.7985e+00
Epoch 8/10
10/10 - 2s - loss: 240.0021 - loglik: -2.3786e+02 - logprior: -1.7005e+00
Epoch 9/10
10/10 - 2s - loss: 238.6784 - loglik: -2.3658e+02 - logprior: -1.6723e+00
Epoch 10/10
10/10 - 2s - loss: 238.1743 - loglik: -2.3606e+02 - logprior: -1.6777e+00
Fitted a model with MAP estimate = -237.5448
expansions: [(0, 4), (13, 3), (15, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 5), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 261.3353 - loglik: -2.3425e+02 - logprior: -2.7062e+01
Epoch 2/2
10/10 - 2s - loss: 222.0164 - loglik: -2.1389e+02 - logprior: -7.9651e+00
Fitted a model with MAP estimate = -213.7786
expansions: [(132, 1)]
discards: [ 62 134]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.6741 - loglik: -2.0960e+02 - logprior: -1.9053e+01
Epoch 2/2
10/10 - 2s - loss: 209.7289 - loglik: -2.0509e+02 - logprior: -4.4769e+00
Fitted a model with MAP estimate = -206.8057
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 225.0547 - loglik: -2.0711e+02 - logprior: -1.7916e+01
Epoch 2/10
10/10 - 2s - loss: 208.2301 - loglik: -2.0397e+02 - logprior: -4.1036e+00
Epoch 3/10
10/10 - 2s - loss: 205.7033 - loglik: -2.0382e+02 - logprior: -1.5469e+00
Epoch 4/10
10/10 - 2s - loss: 202.8439 - loglik: -2.0166e+02 - logprior: -7.1414e-01
Epoch 5/10
10/10 - 2s - loss: 202.1102 - loglik: -2.0129e+02 - logprior: -3.1286e-01
Epoch 6/10
10/10 - 2s - loss: 201.2500 - loglik: -2.0074e+02 - logprior: -7.5200e-03
Epoch 7/10
10/10 - 2s - loss: 201.0881 - loglik: -2.0084e+02 - logprior: 0.2485
Epoch 8/10
10/10 - 2s - loss: 200.3454 - loglik: -2.0027e+02 - logprior: 0.4185
Epoch 9/10
10/10 - 2s - loss: 199.6493 - loglik: -1.9967e+02 - logprior: 0.5094
Epoch 10/10
10/10 - 2s - loss: 199.7049 - loglik: -1.9981e+02 - logprior: 0.5872
Fitted a model with MAP estimate = -199.0403
Time for alignment: 70.9430
Computed alignments with likelihoods: ['-199.2529', '-199.2816', '-199.0403']
Best model has likelihood: -199.0403
time for generating output: 0.2052
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9174061433447099
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb686f56130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686109bb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb67dc10730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6864674f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb697306760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69706d3a0>, <__main__.SimpleDirichletPrior object at 0x7fb674c2b5b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.7196 - loglik: -2.4850e+02 - logprior: -6.9211e+01
Epoch 2/10
10/10 - 2s - loss: 236.9196 - loglik: -2.1895e+02 - logprior: -1.7948e+01
Epoch 3/10
10/10 - 2s - loss: 204.0152 - loglik: -1.9581e+02 - logprior: -8.1172e+00
Epoch 4/10
10/10 - 2s - loss: 189.8203 - loglik: -1.8518e+02 - logprior: -4.5971e+00
Epoch 5/10
10/10 - 2s - loss: 183.9914 - loglik: -1.8139e+02 - logprior: -2.6006e+00
Epoch 6/10
10/10 - 2s - loss: 180.9812 - loglik: -1.7944e+02 - logprior: -1.5310e+00
Epoch 7/10
10/10 - 2s - loss: 179.1507 - loglik: -1.7820e+02 - logprior: -9.3742e-01
Epoch 8/10
10/10 - 2s - loss: 177.8613 - loglik: -1.7713e+02 - logprior: -6.5265e-01
Epoch 9/10
10/10 - 2s - loss: 177.2135 - loglik: -1.7653e+02 - logprior: -4.5159e-01
Epoch 10/10
10/10 - 2s - loss: 176.7049 - loglik: -1.7615e+02 - logprior: -2.9039e-01
Fitted a model with MAP estimate = -176.0482
expansions: [(9, 3), (14, 2), (15, 1), (25, 2), (26, 2), (37, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 254.2509 - loglik: -1.7646e+02 - logprior: -7.7770e+01
Epoch 2/2
10/10 - 2s - loss: 200.6805 - loglik: -1.6872e+02 - logprior: -3.1890e+01
Fitted a model with MAP estimate = -191.4777
expansions: []
discards: [16]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 240.5345 - loglik: -1.6634e+02 - logprior: -7.4170e+01
Epoch 2/2
10/10 - 2s - loss: 186.3265 - loglik: -1.6430e+02 - logprior: -2.1917e+01
Fitted a model with MAP estimate = -175.3602
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 225.7632 - loglik: -1.6345e+02 - logprior: -6.2288e+01
Epoch 2/10
10/10 - 2s - loss: 178.9517 - loglik: -1.6296e+02 - logprior: -1.5876e+01
Epoch 3/10
10/10 - 2s - loss: 169.6915 - loglik: -1.6335e+02 - logprior: -6.2224e+00
Epoch 4/10
10/10 - 2s - loss: 165.8699 - loglik: -1.6358e+02 - logprior: -2.2252e+00
Epoch 5/10
10/10 - 2s - loss: 163.7443 - loglik: -1.6366e+02 - logprior: -4.9581e-02
Epoch 6/10
10/10 - 2s - loss: 162.0981 - loglik: -1.6321e+02 - logprior: 1.1594
Epoch 7/10
10/10 - 2s - loss: 160.5808 - loglik: -1.6233e+02 - logprior: 1.8849
Epoch 8/10
10/10 - 2s - loss: 159.6892 - loglik: -1.6180e+02 - logprior: 2.3473
Epoch 9/10
10/10 - 2s - loss: 159.1857 - loglik: -1.6164e+02 - logprior: 2.7305
Epoch 10/10
10/10 - 2s - loss: 158.8229 - loglik: -1.6165e+02 - logprior: 3.0903
Fitted a model with MAP estimate = -158.3769
Time for alignment: 68.2651
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.6879 - loglik: -2.4847e+02 - logprior: -6.9210e+01
Epoch 2/10
10/10 - 2s - loss: 236.7286 - loglik: -2.1876e+02 - logprior: -1.7948e+01
Epoch 3/10
10/10 - 2s - loss: 204.0311 - loglik: -1.9585e+02 - logprior: -8.0907e+00
Epoch 4/10
10/10 - 2s - loss: 190.8583 - loglik: -1.8635e+02 - logprior: -4.4624e+00
Epoch 5/10
10/10 - 2s - loss: 186.0251 - loglik: -1.8360e+02 - logprior: -2.4273e+00
Epoch 6/10
10/10 - 2s - loss: 183.7602 - loglik: -1.8243e+02 - logprior: -1.3222e+00
Epoch 7/10
10/10 - 2s - loss: 182.2673 - loglik: -1.8140e+02 - logprior: -7.8289e-01
Epoch 8/10
10/10 - 2s - loss: 180.6326 - loglik: -1.7992e+02 - logprior: -4.9544e-01
Epoch 9/10
10/10 - 2s - loss: 178.5785 - loglik: -1.7793e+02 - logprior: -3.9386e-01
Epoch 10/10
10/10 - 2s - loss: 177.0349 - loglik: -1.7651e+02 - logprior: -2.7759e-01
Fitted a model with MAP estimate = -176.2144
expansions: [(10, 2), (14, 2), (24, 1), (26, 2), (37, 1), (52, 2), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.0603 - loglik: -1.7711e+02 - logprior: -7.7937e+01
Epoch 2/2
10/10 - 2s - loss: 200.5690 - loglik: -1.6849e+02 - logprior: -3.2004e+01
Fitted a model with MAP estimate = -191.2173
expansions: []
discards: [ 0  9 30 59]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 245.2541 - loglik: -1.6890e+02 - logprior: -7.6328e+01
Epoch 2/2
10/10 - 2s - loss: 193.8024 - loglik: -1.6696e+02 - logprior: -2.6728e+01
Fitted a model with MAP estimate = -182.5126
expansions: [(5, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.9806 - loglik: -1.6688e+02 - logprior: -7.0081e+01
Epoch 2/10
10/10 - 2s - loss: 183.9101 - loglik: -1.6568e+02 - logprior: -1.8119e+01
Epoch 3/10
10/10 - 2s - loss: 172.8536 - loglik: -1.6599e+02 - logprior: -6.7451e+00
Epoch 4/10
10/10 - 2s - loss: 168.8045 - loglik: -1.6635e+02 - logprior: -2.3912e+00
Epoch 5/10
10/10 - 2s - loss: 166.5122 - loglik: -1.6641e+02 - logprior: -7.1728e-02
Epoch 6/10
10/10 - 2s - loss: 164.9023 - loglik: -1.6612e+02 - logprior: 1.2361
Epoch 7/10
10/10 - 2s - loss: 163.6990 - loglik: -1.6556e+02 - logprior: 1.9482
Epoch 8/10
10/10 - 2s - loss: 163.0075 - loglik: -1.6518e+02 - logprior: 2.4064
Epoch 9/10
10/10 - 2s - loss: 162.4781 - loglik: -1.6500e+02 - logprior: 2.7835
Epoch 10/10
10/10 - 2s - loss: 162.0547 - loglik: -1.6493e+02 - logprior: 3.1018
Fitted a model with MAP estimate = -161.6269
Time for alignment: 68.0687
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.6804 - loglik: -2.4846e+02 - logprior: -6.9211e+01
Epoch 2/10
10/10 - 2s - loss: 236.8283 - loglik: -2.1886e+02 - logprior: -1.7947e+01
Epoch 3/10
10/10 - 2s - loss: 204.4735 - loglik: -1.9629e+02 - logprior: -8.0884e+00
Epoch 4/10
10/10 - 2s - loss: 191.0088 - loglik: -1.8648e+02 - logprior: -4.4841e+00
Epoch 5/10
10/10 - 2s - loss: 184.7135 - loglik: -1.8210e+02 - logprior: -2.6134e+00
Epoch 6/10
10/10 - 2s - loss: 181.2678 - loglik: -1.7966e+02 - logprior: -1.6011e+00
Epoch 7/10
10/10 - 2s - loss: 178.9317 - loglik: -1.7793e+02 - logprior: -9.8151e-01
Epoch 8/10
10/10 - 2s - loss: 177.1642 - loglik: -1.7630e+02 - logprior: -7.0349e-01
Epoch 9/10
10/10 - 2s - loss: 176.3101 - loglik: -1.7555e+02 - logprior: -4.8117e-01
Epoch 10/10
10/10 - 2s - loss: 175.4754 - loglik: -1.7492e+02 - logprior: -3.2640e-01
Fitted a model with MAP estimate = -174.9059
expansions: [(14, 2), (15, 1), (25, 1), (26, 2), (37, 2), (46, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.1123 - loglik: -1.7623e+02 - logprior: -7.7861e+01
Epoch 2/2
10/10 - 2s - loss: 200.7772 - loglik: -1.6874e+02 - logprior: -3.1966e+01
Fitted a model with MAP estimate = -191.7852
expansions: []
discards: [ 0 13 42]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 244.8367 - loglik: -1.6856e+02 - logprior: -7.6255e+01
Epoch 2/2
10/10 - 2s - loss: 193.3453 - loglik: -1.6665e+02 - logprior: -2.6597e+01
Fitted a model with MAP estimate = -181.9166
expansions: [(7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 246.4921 - loglik: -1.6804e+02 - logprior: -7.8431e+01
Epoch 2/10
10/10 - 2s - loss: 198.5668 - loglik: -1.6683e+02 - logprior: -3.1623e+01
Epoch 3/10
10/10 - 2s - loss: 189.0755 - loglik: -1.6704e+02 - logprior: -2.1933e+01
Epoch 4/10
10/10 - 2s - loss: 184.8618 - loglik: -1.6729e+02 - logprior: -1.7531e+01
Epoch 5/10
10/10 - 2s - loss: 180.5154 - loglik: -1.6742e+02 - logprior: -1.3079e+01
Epoch 6/10
10/10 - 2s - loss: 170.7995 - loglik: -1.6718e+02 - logprior: -3.5942e+00
Epoch 7/10
10/10 - 2s - loss: 165.0702 - loglik: -1.6628e+02 - logprior: 1.3139
Epoch 8/10
10/10 - 2s - loss: 163.9812 - loglik: -1.6593e+02 - logprior: 2.1976
Epoch 9/10
10/10 - 2s - loss: 163.5145 - loglik: -1.6595e+02 - logprior: 2.7214
Epoch 10/10
10/10 - 2s - loss: 163.1883 - loglik: -1.6609e+02 - logprior: 3.1536
Fitted a model with MAP estimate = -162.7652
Time for alignment: 67.3272
Computed alignments with likelihoods: ['-158.3769', '-161.6269', '-162.7652']
Best model has likelihood: -158.3769
time for generating output: 0.2711
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3595765795567317
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6c4ab9c10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686ca6b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c45f04f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c45f0310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c45f08b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6867c2dc0>, <__main__.SimpleDirichletPrior object at 0x7fb69fe2ceb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 757.5889 - loglik: -7.4905e+02 - logprior: -8.4251e+00
Epoch 2/10
20/20 - 11s - loss: 688.1281 - loglik: -6.8712e+02 - logprior: -2.0940e-01
Epoch 3/10
20/20 - 11s - loss: 654.6858 - loglik: -6.5257e+02 - logprior: -5.9880e-01
Epoch 4/10
20/20 - 11s - loss: 642.9849 - loglik: -6.4001e+02 - logprior: -9.2712e-01
Epoch 5/10
20/20 - 11s - loss: 638.5681 - loglik: -6.3574e+02 - logprior: -1.0204e+00
Epoch 6/10
20/20 - 11s - loss: 635.4865 - loglik: -6.3294e+02 - logprior: -1.1216e+00
Epoch 7/10
20/20 - 11s - loss: 635.4933 - loglik: -6.3310e+02 - logprior: -1.1656e+00
Fitted a model with MAP estimate = -632.2610
expansions: [(0, 4), (15, 2), (45, 1), (52, 1), (54, 2), (82, 12), (92, 1), (94, 3), (112, 2), (115, 2), (117, 1), (154, 1), (155, 2), (171, 12), (174, 1), (175, 1), (204, 1), (205, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 691.4344 - loglik: -6.7848e+02 - logprior: -1.2844e+01
Epoch 2/2
20/20 - 14s - loss: 646.8766 - loglik: -6.4469e+02 - logprior: -1.7072e+00
Fitted a model with MAP estimate = -637.5486
expansions: [(268, 1)]
discards: [  1   2   3   4   5   6 118 143 187 205 206 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 652.9392 - loglik: -6.4489e+02 - logprior: -7.9548e+00
Epoch 2/2
20/20 - 13s - loss: 637.7398 - loglik: -6.3707e+02 - logprior: -2.6634e-01
Fitted a model with MAP estimate = -632.1367
expansions: [(0, 4), (18, 5), (94, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 653.9327 - loglik: -6.4033e+02 - logprior: -1.3524e+01
Epoch 2/10
20/20 - 14s - loss: 638.2410 - loglik: -6.3642e+02 - logprior: -1.4366e+00
Epoch 3/10
20/20 - 13s - loss: 624.4297 - loglik: -6.2374e+02 - logprior: 0.3212
Epoch 4/10
20/20 - 13s - loss: 624.0668 - loglik: -6.2340e+02 - logprior: 0.7976
Epoch 5/10
20/20 - 13s - loss: 619.6382 - loglik: -6.1909e+02 - logprior: 0.9791
Epoch 6/10
20/20 - 13s - loss: 617.9271 - loglik: -6.1758e+02 - logprior: 1.0845
Epoch 7/10
20/20 - 13s - loss: 615.2594 - loglik: -6.1508e+02 - logprior: 1.1767
Epoch 8/10
20/20 - 13s - loss: 614.0311 - loglik: -6.1419e+02 - logprior: 1.4224
Epoch 9/10
20/20 - 14s - loss: 613.5482 - loglik: -6.1389e+02 - logprior: 1.5002
Epoch 10/10
20/20 - 13s - loss: 612.1669 - loglik: -6.1280e+02 - logprior: 1.7717
Fitted a model with MAP estimate = -610.3039
Time for alignment: 314.4426
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 756.7283 - loglik: -7.4819e+02 - logprior: -8.4179e+00
Epoch 2/10
20/20 - 11s - loss: 683.8437 - loglik: -6.8280e+02 - logprior: -2.8196e-01
Epoch 3/10
20/20 - 11s - loss: 652.4208 - loglik: -6.5041e+02 - logprior: -8.7418e-01
Epoch 4/10
20/20 - 11s - loss: 643.2389 - loglik: -6.4062e+02 - logprior: -1.0490e+00
Epoch 5/10
20/20 - 11s - loss: 637.7823 - loglik: -6.3526e+02 - logprior: -1.0076e+00
Epoch 6/10
20/20 - 11s - loss: 635.9271 - loglik: -6.3365e+02 - logprior: -1.0184e+00
Epoch 7/10
20/20 - 11s - loss: 633.0732 - loglik: -6.3086e+02 - logprior: -1.0621e+00
Epoch 8/10
20/20 - 11s - loss: 631.2564 - loglik: -6.2911e+02 - logprior: -1.0666e+00
Epoch 9/10
20/20 - 11s - loss: 632.4241 - loglik: -6.3031e+02 - logprior: -1.0938e+00
Fitted a model with MAP estimate = -630.1432
expansions: [(0, 4), (21, 5), (43, 1), (52, 1), (54, 2), (56, 1), (81, 5), (83, 6), (86, 1), (93, 1), (110, 3), (115, 2), (148, 1), (170, 12), (171, 1), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 697.8243 - loglik: -6.8400e+02 - logprior: -1.3718e+01
Epoch 2/2
20/20 - 14s - loss: 644.0654 - loglik: -6.4152e+02 - logprior: -2.0753e+00
Fitted a model with MAP estimate = -636.0763
expansions: [(247, 1)]
discards: [  0   3   4  52  99 145 204 205 206]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 656.9272 - loglik: -6.4549e+02 - logprior: -1.1342e+01
Epoch 2/2
20/20 - 13s - loss: 636.9528 - loglik: -6.3457e+02 - logprior: -1.9613e+00
Fitted a model with MAP estimate = -630.8034
expansions: [(0, 4), (28, 1), (100, 4), (182, 1)]
discards: [  0  22 198]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 649.4406 - loglik: -6.4177e+02 - logprior: -7.5982e+00
Epoch 2/10
20/20 - 14s - loss: 633.8373 - loglik: -6.3356e+02 - logprior: 0.1499
Epoch 3/10
20/20 - 14s - loss: 626.7305 - loglik: -6.2624e+02 - logprior: 0.5795
Epoch 4/10
20/20 - 14s - loss: 623.8077 - loglik: -6.2317e+02 - logprior: 0.8483
Epoch 5/10
20/20 - 14s - loss: 619.9330 - loglik: -6.1946e+02 - logprior: 1.1151
Epoch 6/10
20/20 - 14s - loss: 616.4503 - loglik: -6.1621e+02 - logprior: 1.2771
Epoch 7/10
20/20 - 14s - loss: 617.1733 - loglik: -6.1716e+02 - logprior: 1.3797
Fitted a model with MAP estimate = -613.4524
Time for alignment: 298.0337
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 755.3516 - loglik: -7.4682e+02 - logprior: -8.4107e+00
Epoch 2/10
20/20 - 11s - loss: 683.3267 - loglik: -6.8224e+02 - logprior: -3.1354e-01
Epoch 3/10
20/20 - 11s - loss: 652.9901 - loglik: -6.5072e+02 - logprior: -6.9698e-01
Epoch 4/10
20/20 - 11s - loss: 642.1745 - loglik: -6.3942e+02 - logprior: -6.8806e-01
Epoch 5/10
20/20 - 11s - loss: 637.7078 - loglik: -6.3545e+02 - logprior: -6.4915e-01
Epoch 6/10
20/20 - 11s - loss: 634.7144 - loglik: -6.3262e+02 - logprior: -8.1499e-01
Epoch 7/10
20/20 - 11s - loss: 634.3649 - loglik: -6.3229e+02 - logprior: -8.8076e-01
Epoch 8/10
20/20 - 11s - loss: 633.0331 - loglik: -6.3106e+02 - logprior: -8.8895e-01
Epoch 9/10
20/20 - 11s - loss: 632.0660 - loglik: -6.3010e+02 - logprior: -9.0936e-01
Epoch 10/10
20/20 - 11s - loss: 630.5673 - loglik: -6.2867e+02 - logprior: -8.8329e-01
Fitted a model with MAP estimate = -629.9462
expansions: [(0, 3), (44, 5), (50, 1), (51, 1), (82, 11), (83, 2), (91, 3), (92, 1), (117, 1), (146, 3), (152, 2), (170, 11), (196, 1), (198, 3), (207, 1), (208, 1)]
discards: [  1 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 699.7635 - loglik: -6.8607e+02 - logprior: -1.3585e+01
Epoch 2/2
20/20 - 14s - loss: 649.3949 - loglik: -6.4700e+02 - logprior: -1.8412e+00
Fitted a model with MAP estimate = -640.4807
expansions: [(23, 5), (103, 1), (266, 1)]
discards: [  2   3   4  46  47  96  97  98  99 114 175 202 203 204 205 212 245]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 654.4630 - loglik: -6.4658e+02 - logprior: -7.7904e+00
Epoch 2/2
20/20 - 13s - loss: 639.5465 - loglik: -6.3902e+02 - logprior: -9.7113e-02
Fitted a model with MAP estimate = -633.1179
expansions: [(0, 3), (2, 1)]
discards: [ 21 197]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 656.0660 - loglik: -6.4228e+02 - logprior: -1.3709e+01
Epoch 2/10
20/20 - 13s - loss: 637.1630 - loglik: -6.3532e+02 - logprior: -1.4347e+00
Epoch 3/10
20/20 - 13s - loss: 631.8526 - loglik: -6.3128e+02 - logprior: 0.4134
Epoch 4/10
20/20 - 13s - loss: 624.0295 - loglik: -6.2360e+02 - logprior: 0.9477
Epoch 5/10
20/20 - 13s - loss: 626.3844 - loglik: -6.2606e+02 - logprior: 1.1379
Fitted a model with MAP estimate = -619.6713
Time for alignment: 277.2949
Computed alignments with likelihoods: ['-610.3039', '-613.4524', '-619.6713']
Best model has likelihood: -610.3039
time for generating output: 0.4365
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6509688496443463
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6749657c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb66423fe50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6641ef3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c5112fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c5112670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c5112a30>, <__main__.SimpleDirichletPrior object at 0x7fb6862efac0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7427 - loglik: -1.3147e+02 - logprior: -3.2624e+00
Epoch 2/10
19/19 - 2s - loss: 113.9079 - loglik: -1.1248e+02 - logprior: -1.3730e+00
Epoch 3/10
19/19 - 2s - loss: 107.0565 - loglik: -1.0538e+02 - logprior: -1.5078e+00
Epoch 4/10
19/19 - 2s - loss: 105.3582 - loglik: -1.0370e+02 - logprior: -1.4603e+00
Epoch 5/10
19/19 - 1s - loss: 104.8029 - loglik: -1.0318e+02 - logprior: -1.4284e+00
Epoch 6/10
19/19 - 2s - loss: 104.4324 - loglik: -1.0283e+02 - logprior: -1.4141e+00
Epoch 7/10
19/19 - 2s - loss: 104.1667 - loglik: -1.0259e+02 - logprior: -1.4039e+00
Epoch 8/10
19/19 - 2s - loss: 104.0198 - loglik: -1.0245e+02 - logprior: -1.4043e+00
Epoch 9/10
19/19 - 2s - loss: 103.9136 - loglik: -1.0235e+02 - logprior: -1.3947e+00
Epoch 10/10
19/19 - 2s - loss: 103.7690 - loglik: -1.0222e+02 - logprior: -1.3886e+00
Fitted a model with MAP estimate = -102.2184
expansions: [(10, 2), (12, 3), (13, 3), (15, 1), (22, 2), (23, 1), (28, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.8127 - loglik: -1.0151e+02 - logprior: -4.2509e+00
Epoch 2/2
19/19 - 2s - loss: 96.0700 - loglik: -9.3671e+01 - logprior: -2.2022e+00
Fitted a model with MAP estimate = -92.7352
expansions: [(0, 1)]
discards: [ 0 10 14 32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.8593 - loglik: -9.3680e+01 - logprior: -3.1336e+00
Epoch 2/2
19/19 - 2s - loss: 93.1144 - loglik: -9.1476e+01 - logprior: -1.4910e+00
Fitted a model with MAP estimate = -91.0926
expansions: []
discards: [20]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.5236 - loglik: -9.1297e+01 - logprior: -3.1855e+00
Epoch 2/10
19/19 - 2s - loss: 91.7353 - loglik: -9.0117e+01 - logprior: -1.4782e+00
Epoch 3/10
19/19 - 1s - loss: 90.9614 - loglik: -8.9330e+01 - logprior: -1.3828e+00
Epoch 4/10
19/19 - 2s - loss: 90.4007 - loglik: -8.8741e+01 - logprior: -1.3601e+00
Epoch 5/10
19/19 - 2s - loss: 90.0955 - loglik: -8.8475e+01 - logprior: -1.3435e+00
Epoch 6/10
19/19 - 2s - loss: 89.9687 - loglik: -8.8397e+01 - logprior: -1.3204e+00
Epoch 7/10
19/19 - 2s - loss: 89.9054 - loglik: -8.8373e+01 - logprior: -1.3112e+00
Epoch 8/10
19/19 - 1s - loss: 89.4766 - loglik: -8.7966e+01 - logprior: -1.3004e+00
Epoch 9/10
19/19 - 2s - loss: 89.7860 - loglik: -8.8295e+01 - logprior: -1.2875e+00
Fitted a model with MAP estimate = -89.3748
Time for alignment: 59.6155
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7078 - loglik: -1.3144e+02 - logprior: -3.2590e+00
Epoch 2/10
19/19 - 1s - loss: 113.2332 - loglik: -1.1180e+02 - logprior: -1.3997e+00
Epoch 3/10
19/19 - 2s - loss: 105.9369 - loglik: -1.0410e+02 - logprior: -1.5745e+00
Epoch 4/10
19/19 - 2s - loss: 104.2583 - loglik: -1.0247e+02 - logprior: -1.4953e+00
Epoch 5/10
19/19 - 1s - loss: 103.3337 - loglik: -1.0164e+02 - logprior: -1.4618e+00
Epoch 6/10
19/19 - 2s - loss: 103.0097 - loglik: -1.0135e+02 - logprior: -1.4574e+00
Epoch 7/10
19/19 - 2s - loss: 102.5511 - loglik: -1.0092e+02 - logprior: -1.4563e+00
Epoch 8/10
19/19 - 2s - loss: 102.2746 - loglik: -1.0066e+02 - logprior: -1.4582e+00
Epoch 9/10
19/19 - 2s - loss: 102.0905 - loglik: -1.0048e+02 - logprior: -1.4532e+00
Epoch 10/10
19/19 - 2s - loss: 102.2810 - loglik: -1.0068e+02 - logprior: -1.4503e+00
Fitted a model with MAP estimate = -100.5218
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 3), (29, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.0835 - loglik: -1.0078e+02 - logprior: -4.2506e+00
Epoch 2/2
19/19 - 2s - loss: 95.8118 - loglik: -9.3457e+01 - logprior: -2.1647e+00
Fitted a model with MAP estimate = -92.5429
expansions: [(0, 1)]
discards: [ 0  9 30]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.7394 - loglik: -9.3568e+01 - logprior: -3.1351e+00
Epoch 2/2
19/19 - 1s - loss: 93.4232 - loglik: -9.1786e+01 - logprior: -1.4939e+00
Fitted a model with MAP estimate = -91.3127
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.4726 - loglik: -9.1249e+01 - logprior: -3.1829e+00
Epoch 2/10
19/19 - 2s - loss: 91.9181 - loglik: -9.0292e+01 - logprior: -1.4876e+00
Epoch 3/10
19/19 - 2s - loss: 90.9408 - loglik: -8.9310e+01 - logprior: -1.3887e+00
Epoch 4/10
19/19 - 2s - loss: 90.5753 - loglik: -8.8916e+01 - logprior: -1.3650e+00
Epoch 5/10
19/19 - 2s - loss: 90.2533 - loglik: -8.8641e+01 - logprior: -1.3417e+00
Epoch 6/10
19/19 - 2s - loss: 90.2082 - loglik: -8.8638e+01 - logprior: -1.3242e+00
Epoch 7/10
19/19 - 2s - loss: 89.8999 - loglik: -8.8369e+01 - logprior: -1.3154e+00
Epoch 8/10
19/19 - 2s - loss: 89.7647 - loglik: -8.8254e+01 - logprior: -1.3072e+00
Epoch 9/10
19/19 - 2s - loss: 89.9853 - loglik: -8.8499e+01 - logprior: -1.2926e+00
Fitted a model with MAP estimate = -89.5304
Time for alignment: 59.2268
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8100 - loglik: -1.3154e+02 - logprior: -3.2603e+00
Epoch 2/10
19/19 - 2s - loss: 114.5946 - loglik: -1.1315e+02 - logprior: -1.3966e+00
Epoch 3/10
19/19 - 2s - loss: 106.5930 - loglik: -1.0476e+02 - logprior: -1.5570e+00
Epoch 4/10
19/19 - 2s - loss: 103.5610 - loglik: -1.0172e+02 - logprior: -1.5445e+00
Epoch 5/10
19/19 - 2s - loss: 102.2048 - loglik: -1.0046e+02 - logprior: -1.5305e+00
Epoch 6/10
19/19 - 2s - loss: 101.1409 - loglik: -9.9434e+01 - logprior: -1.5361e+00
Epoch 7/10
19/19 - 2s - loss: 100.8015 - loglik: -9.9115e+01 - logprior: -1.5221e+00
Epoch 8/10
19/19 - 2s - loss: 101.1590 - loglik: -9.9483e+01 - logprior: -1.5160e+00
Fitted a model with MAP estimate = -99.2673
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 3), (28, 2), (29, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.4991 - loglik: -9.9226e+01 - logprior: -4.2182e+00
Epoch 2/2
19/19 - 2s - loss: 95.3422 - loglik: -9.3126e+01 - logprior: -2.0534e+00
Fitted a model with MAP estimate = -92.4137
expansions: [(0, 1)]
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.7392 - loglik: -9.3590e+01 - logprior: -3.1071e+00
Epoch 2/2
19/19 - 2s - loss: 93.4709 - loglik: -9.1843e+01 - logprior: -1.4842e+00
Fitted a model with MAP estimate = -91.3598
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.3976 - loglik: -9.1188e+01 - logprior: -3.1683e+00
Epoch 2/10
19/19 - 2s - loss: 91.9555 - loglik: -9.0338e+01 - logprior: -1.4852e+00
Epoch 3/10
19/19 - 2s - loss: 91.0575 - loglik: -8.9432e+01 - logprior: -1.3866e+00
Epoch 4/10
19/19 - 1s - loss: 90.5590 - loglik: -8.8901e+01 - logprior: -1.3643e+00
Epoch 5/10
19/19 - 2s - loss: 90.1378 - loglik: -8.8520e+01 - logprior: -1.3414e+00
Epoch 6/10
19/19 - 2s - loss: 90.3376 - loglik: -8.8765e+01 - logprior: -1.3318e+00
Fitted a model with MAP estimate = -89.6872
Time for alignment: 51.5609
Computed alignments with likelihoods: ['-89.3748', '-89.5304', '-89.6872']
Best model has likelihood: -89.3748
time for generating output: 0.1094
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9438877755511023
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6c5119eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb64ada6400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4765fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6640091c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4964c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb653308310>, <__main__.SimpleDirichletPrior object at 0x7fb68650ecd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 649.1827 - loglik: -6.2962e+02 - logprior: -1.9504e+01
Epoch 2/10
15/15 - 8s - loss: 574.8329 - loglik: -5.7314e+02 - logprior: -1.4513e+00
Epoch 3/10
15/15 - 8s - loss: 507.3117 - loglik: -5.0593e+02 - logprior: -1.1386e+00
Epoch 4/10
15/15 - 8s - loss: 480.7674 - loglik: -4.7791e+02 - logprior: -2.1917e+00
Epoch 5/10
15/15 - 8s - loss: 468.1840 - loglik: -4.6456e+02 - logprior: -2.8498e+00
Epoch 6/10
15/15 - 8s - loss: 466.1971 - loglik: -4.6279e+02 - logprior: -2.7288e+00
Epoch 7/10
15/15 - 8s - loss: 462.7720 - loglik: -4.5956e+02 - logprior: -2.5774e+00
Epoch 8/10
15/15 - 8s - loss: 465.9774 - loglik: -4.6284e+02 - logprior: -2.5076e+00
Fitted a model with MAP estimate = -462.4423
expansions: [(23, 1), (24, 2), (25, 2), (27, 1), (37, 1), (40, 1), (50, 1), (52, 1), (54, 1), (55, 1), (57, 1), (70, 1), (71, 5), (72, 2), (73, 1), (75, 1), (76, 1), (77, 1), (79, 2), (81, 1), (87, 1), (100, 1), (102, 1), (103, 1), (104, 3), (106, 2), (133, 2), (134, 2), (135, 4), (136, 2), (137, 1), (138, 1), (140, 1), (157, 1), (158, 5), (159, 2), (167, 1), (173, 8), (179, 3)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 474.4253 - loglik: -4.5647e+02 - logprior: -1.7909e+01
Epoch 2/2
15/15 - 11s - loss: 436.0452 - loglik: -4.3478e+02 - logprior: -1.0090e+00
Fitted a model with MAP estimate = -429.4520
expansions: [(26, 1)]
discards: [ 23  89 139 171 179 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 451.8149 - loglik: -4.3513e+02 - logprior: -1.6627e+01
Epoch 2/2
15/15 - 10s - loss: 431.6744 - loglik: -4.3134e+02 - logprior: -2.2689e-02
Fitted a model with MAP estimate = -427.7573
expansions: [(0, 3), (204, 1), (239, 1)]
discards: [88]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 15s - loss: 458.8259 - loglik: -4.3328e+02 - logprior: -2.5491e+01
Epoch 2/10
15/15 - 10s - loss: 429.5465 - loglik: -4.2645e+02 - logprior: -2.7848e+00
Epoch 3/10
15/15 - 10s - loss: 425.6147 - loglik: -4.2745e+02 - logprior: 2.3743
Epoch 4/10
15/15 - 10s - loss: 420.5849 - loglik: -4.2406e+02 - logprior: 4.0901
Epoch 5/10
15/15 - 10s - loss: 420.1075 - loglik: -4.2424e+02 - logprior: 4.7119
Epoch 6/10
15/15 - 10s - loss: 417.0462 - loglik: -4.2167e+02 - logprior: 5.1530
Epoch 7/10
15/15 - 10s - loss: 419.2003 - loglik: -4.2417e+02 - logprior: 5.4670
Fitted a model with MAP estimate = -417.7339
Time for alignment: 210.6459
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 650.1241 - loglik: -6.3055e+02 - logprior: -1.9511e+01
Epoch 2/10
15/15 - 8s - loss: 575.2195 - loglik: -5.7357e+02 - logprior: -1.3917e+00
Epoch 3/10
15/15 - 8s - loss: 505.1200 - loglik: -5.0396e+02 - logprior: -8.9531e-01
Epoch 4/10
15/15 - 8s - loss: 479.2690 - loglik: -4.7677e+02 - logprior: -1.8788e+00
Epoch 5/10
15/15 - 8s - loss: 468.1644 - loglik: -4.6506e+02 - logprior: -2.3582e+00
Epoch 6/10
15/15 - 8s - loss: 465.4510 - loglik: -4.6261e+02 - logprior: -2.2163e+00
Epoch 7/10
15/15 - 8s - loss: 467.2290 - loglik: -4.6462e+02 - logprior: -2.0297e+00
Fitted a model with MAP estimate = -463.9246
expansions: [(19, 1), (23, 1), (24, 2), (25, 3), (37, 3), (38, 1), (40, 1), (41, 1), (42, 2), (44, 2), (45, 1), (47, 2), (48, 1), (49, 1), (52, 1), (60, 1), (62, 1), (64, 1), (65, 3), (68, 1), (70, 1), (72, 1), (73, 1), (75, 1), (95, 1), (99, 5), (101, 2), (128, 2), (129, 2), (130, 1), (131, 1), (133, 1), (136, 1), (156, 6), (173, 10)]
discards: [  1   2 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 477.2516 - loglik: -4.5931e+02 - logprior: -1.7890e+01
Epoch 2/2
15/15 - 10s - loss: 435.6025 - loglik: -4.3446e+02 - logprior: -9.8097e-01
Fitted a model with MAP estimate = -429.1829
expansions: [(0, 3), (43, 1), (68, 1), (204, 2)]
discards: [  4  89  90 139 171]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 457.2983 - loglik: -4.3206e+02 - logprior: -2.5181e+01
Epoch 2/2
15/15 - 10s - loss: 430.7118 - loglik: -4.2758e+02 - logprior: -2.8148e+00
Fitted a model with MAP estimate = -424.5519
expansions: [(207, 1)]
discards: [  0   1   2  25 169]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 448.3559 - loglik: -4.3011e+02 - logprior: -1.8185e+01
Epoch 2/10
15/15 - 10s - loss: 429.5775 - loglik: -4.3007e+02 - logprior: 0.7985
Epoch 3/10
15/15 - 10s - loss: 421.1440 - loglik: -4.2407e+02 - logprior: 3.4471
Epoch 4/10
15/15 - 10s - loss: 423.7526 - loglik: -4.2746e+02 - logprior: 4.3072
Fitted a model with MAP estimate = -419.6470
Time for alignment: 170.7289
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 652.5979 - loglik: -6.3302e+02 - logprior: -1.9515e+01
Epoch 2/10
15/15 - 8s - loss: 575.6077 - loglik: -5.7409e+02 - logprior: -1.2132e+00
Epoch 3/10
15/15 - 8s - loss: 510.0808 - loglik: -5.0936e+02 - logprior: -4.9629e-01
Epoch 4/10
15/15 - 8s - loss: 481.7198 - loglik: -4.7981e+02 - logprior: -1.3987e+00
Epoch 5/10
15/15 - 8s - loss: 471.7446 - loglik: -4.6936e+02 - logprior: -1.7244e+00
Epoch 6/10
15/15 - 8s - loss: 466.0354 - loglik: -4.6404e+02 - logprior: -1.4657e+00
Epoch 7/10
15/15 - 8s - loss: 464.2165 - loglik: -4.6245e+02 - logprior: -1.2861e+00
Epoch 8/10
15/15 - 8s - loss: 461.0575 - loglik: -4.5935e+02 - logprior: -1.2287e+00
Epoch 9/10
15/15 - 8s - loss: 469.7311 - loglik: -4.6811e+02 - logprior: -1.1523e+00
Fitted a model with MAP estimate = -463.1350
expansions: [(19, 1), (25, 4), (54, 2), (55, 1), (57, 2), (58, 3), (61, 1), (67, 1), (68, 1), (69, 3), (70, 6), (72, 2), (73, 2), (96, 1), (98, 1), (99, 2), (100, 4), (106, 1), (128, 2), (129, 2), (130, 1), (131, 1), (133, 1), (136, 1), (156, 1), (157, 6), (173, 7), (183, 3), (198, 1)]
discards: [  1   2   6 203 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 476.8321 - loglik: -4.5884e+02 - logprior: -1.7953e+01
Epoch 2/2
15/15 - 10s - loss: 440.5727 - loglik: -4.3940e+02 - logprior: -9.1701e-01
Fitted a model with MAP estimate = -432.7035
expansions: [(0, 3), (62, 1), (86, 2), (202, 2), (203, 1), (240, 1), (241, 1), (264, 3)]
discards: [ 23 128 166 230]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 458.0976 - loglik: -4.3268e+02 - logprior: -2.5361e+01
Epoch 2/2
15/15 - 11s - loss: 432.5274 - loglik: -4.2935e+02 - logprior: -2.8666e+00
Fitted a model with MAP estimate = -424.1008
expansions: [(28, 1)]
discards: [  0   1   2 167 205 206 234 271 272]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 449.6439 - loglik: -4.3141e+02 - logprior: -1.8174e+01
Epoch 2/10
15/15 - 10s - loss: 428.6009 - loglik: -4.2913e+02 - logprior: 0.8203
Epoch 3/10
15/15 - 10s - loss: 421.9074 - loglik: -4.2497e+02 - logprior: 3.5775
Epoch 4/10
15/15 - 10s - loss: 420.3593 - loglik: -4.2420e+02 - logprior: 4.4313
Epoch 5/10
15/15 - 10s - loss: 420.0344 - loglik: -4.2433e+02 - logprior: 4.8696
Epoch 6/10
15/15 - 10s - loss: 418.4225 - loglik: -4.2322e+02 - logprior: 5.3312
Epoch 7/10
15/15 - 10s - loss: 420.4886 - loglik: -4.2571e+02 - logprior: 5.7302
Fitted a model with MAP estimate = -417.7178
Time for alignment: 215.5738
Computed alignments with likelihoods: ['-417.7339', '-419.6470', '-417.7178']
Best model has likelihood: -417.7178
time for generating output: 0.3434
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9204583754634311
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb67d8dd730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57a15baf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a050cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb605603fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a210490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6418638b0>, <__main__.SimpleDirichletPrior object at 0x7fb6757b0610>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 227.1971 - loglik: -1.8519e+02 - logprior: -4.1988e+01
Epoch 2/10
10/10 - 1s - loss: 180.4991 - loglik: -1.6897e+02 - logprior: -1.1506e+01
Epoch 3/10
10/10 - 1s - loss: 161.8808 - loglik: -1.5607e+02 - logprior: -5.8004e+00
Epoch 4/10
10/10 - 1s - loss: 152.4034 - loglik: -1.4857e+02 - logprior: -3.7813e+00
Epoch 5/10
10/10 - 1s - loss: 148.1723 - loglik: -1.4512e+02 - logprior: -2.8587e+00
Epoch 6/10
10/10 - 1s - loss: 146.5489 - loglik: -1.4371e+02 - logprior: -2.4772e+00
Epoch 7/10
10/10 - 1s - loss: 145.3380 - loglik: -1.4276e+02 - logprior: -2.2463e+00
Epoch 8/10
10/10 - 1s - loss: 145.0521 - loglik: -1.4271e+02 - logprior: -2.0588e+00
Epoch 9/10
10/10 - 1s - loss: 144.4030 - loglik: -1.4225e+02 - logprior: -1.8743e+00
Epoch 10/10
10/10 - 1s - loss: 144.1906 - loglik: -1.4218e+02 - logprior: -1.7195e+00
Fitted a model with MAP estimate = -143.7189
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (19, 1), (20, 2), (21, 2), (24, 1), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.2794 - loglik: -1.4199e+02 - logprior: -5.5262e+01
Epoch 2/2
10/10 - 1s - loss: 152.6457 - loglik: -1.3539e+02 - logprior: -1.7135e+01
Fitted a model with MAP estimate = -144.5386
expansions: []
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.3297 - loglik: -1.3570e+02 - logprior: -4.7604e+01
Epoch 2/2
10/10 - 1s - loss: 153.9371 - loglik: -1.3498e+02 - logprior: -1.8825e+01
Fitted a model with MAP estimate = -148.6832
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.1203 - loglik: -1.3504e+02 - logprior: -4.3053e+01
Epoch 2/10
10/10 - 1s - loss: 146.0260 - loglik: -1.3388e+02 - logprior: -1.2019e+01
Epoch 3/10
10/10 - 1s - loss: 138.5953 - loglik: -1.3335e+02 - logprior: -4.9839e+00
Epoch 4/10
10/10 - 1s - loss: 135.8892 - loglik: -1.3303e+02 - logprior: -2.5159e+00
Epoch 5/10
10/10 - 1s - loss: 134.2042 - loglik: -1.3254e+02 - logprior: -1.3368e+00
Epoch 6/10
10/10 - 1s - loss: 133.5457 - loglik: -1.3245e+02 - logprior: -7.9320e-01
Epoch 7/10
10/10 - 1s - loss: 132.7653 - loglik: -1.3193e+02 - logprior: -5.3743e-01
Epoch 8/10
10/10 - 1s - loss: 132.7837 - loglik: -1.3222e+02 - logprior: -2.5746e-01
Fitted a model with MAP estimate = -132.2139
Time for alignment: 35.0444
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 227.0176 - loglik: -1.8501e+02 - logprior: -4.1988e+01
Epoch 2/10
10/10 - 1s - loss: 180.4054 - loglik: -1.6887e+02 - logprior: -1.1510e+01
Epoch 3/10
10/10 - 1s - loss: 161.5426 - loglik: -1.5571e+02 - logprior: -5.8251e+00
Epoch 4/10
10/10 - 1s - loss: 151.8901 - loglik: -1.4805e+02 - logprior: -3.8039e+00
Epoch 5/10
10/10 - 1s - loss: 147.4792 - loglik: -1.4444e+02 - logprior: -2.9004e+00
Epoch 6/10
10/10 - 1s - loss: 145.7798 - loglik: -1.4299e+02 - logprior: -2.4796e+00
Epoch 7/10
10/10 - 1s - loss: 144.6382 - loglik: -1.4209e+02 - logprior: -2.2398e+00
Epoch 8/10
10/10 - 1s - loss: 144.2013 - loglik: -1.4192e+02 - logprior: -2.0191e+00
Epoch 9/10
10/10 - 1s - loss: 143.6366 - loglik: -1.4156e+02 - logprior: -1.8069e+00
Epoch 10/10
10/10 - 1s - loss: 143.4376 - loglik: -1.4148e+02 - logprior: -1.6625e+00
Fitted a model with MAP estimate = -142.9890
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.0311 - loglik: -1.4066e+02 - logprior: -5.5350e+01
Epoch 2/2
10/10 - 1s - loss: 152.1250 - loglik: -1.3494e+02 - logprior: -1.7072e+01
Fitted a model with MAP estimate = -144.0372
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.0334 - loglik: -1.3531e+02 - logprior: -4.7701e+01
Epoch 2/2
10/10 - 1s - loss: 153.4559 - loglik: -1.3453e+02 - logprior: -1.8794e+01
Fitted a model with MAP estimate = -148.1116
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.4222 - loglik: -1.3440e+02 - logprior: -4.3000e+01
Epoch 2/10
10/10 - 1s - loss: 145.6182 - loglik: -1.3353e+02 - logprior: -1.1959e+01
Epoch 3/10
10/10 - 1s - loss: 138.0884 - loglik: -1.3289e+02 - logprior: -4.9449e+00
Epoch 4/10
10/10 - 1s - loss: 135.4716 - loglik: -1.3270e+02 - logprior: -2.4783e+00
Epoch 5/10
10/10 - 1s - loss: 134.1349 - loglik: -1.3255e+02 - logprior: -1.3076e+00
Epoch 6/10
10/10 - 1s - loss: 133.2348 - loglik: -1.3221e+02 - logprior: -7.6457e-01
Epoch 7/10
10/10 - 1s - loss: 132.6325 - loglik: -1.3188e+02 - logprior: -4.8904e-01
Epoch 8/10
10/10 - 1s - loss: 132.4514 - loglik: -1.3199e+02 - logprior: -1.9132e-01
Epoch 9/10
10/10 - 1s - loss: 131.9884 - loglik: -1.3182e+02 - logprior: 0.1089
Epoch 10/10
10/10 - 1s - loss: 131.9362 - loglik: -1.3197e+02 - logprior: 0.3102
Fitted a model with MAP estimate = -131.5949
Time for alignment: 35.8689
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 227.0204 - loglik: -1.8501e+02 - logprior: -4.1988e+01
Epoch 2/10
10/10 - 1s - loss: 180.6143 - loglik: -1.6909e+02 - logprior: -1.1497e+01
Epoch 3/10
10/10 - 1s - loss: 161.4681 - loglik: -1.5568e+02 - logprior: -5.7829e+00
Epoch 4/10
10/10 - 1s - loss: 151.3933 - loglik: -1.4750e+02 - logprior: -3.8345e+00
Epoch 5/10
10/10 - 1s - loss: 147.3291 - loglik: -1.4423e+02 - logprior: -2.8745e+00
Epoch 6/10
10/10 - 1s - loss: 145.1451 - loglik: -1.4240e+02 - logprior: -2.4528e+00
Epoch 7/10
10/10 - 1s - loss: 144.2233 - loglik: -1.4170e+02 - logprior: -2.2551e+00
Epoch 8/10
10/10 - 1s - loss: 143.5425 - loglik: -1.4124e+02 - logprior: -2.0517e+00
Epoch 9/10
10/10 - 1s - loss: 143.2989 - loglik: -1.4121e+02 - logprior: -1.8249e+00
Epoch 10/10
10/10 - 1s - loss: 143.0876 - loglik: -1.4114e+02 - logprior: -1.6681e+00
Fitted a model with MAP estimate = -142.6953
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.6927 - loglik: -1.4032e+02 - logprior: -5.5352e+01
Epoch 2/2
10/10 - 1s - loss: 152.1232 - loglik: -1.3494e+02 - logprior: -1.7068e+01
Fitted a model with MAP estimate = -144.0054
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.9974 - loglik: -1.3527e+02 - logprior: -4.7703e+01
Epoch 2/2
10/10 - 1s - loss: 153.1480 - loglik: -1.3424e+02 - logprior: -1.8767e+01
Fitted a model with MAP estimate = -147.8404
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.2860 - loglik: -1.3430e+02 - logprior: -4.2961e+01
Epoch 2/10
10/10 - 1s - loss: 145.2190 - loglik: -1.3313e+02 - logprior: -1.1952e+01
Epoch 3/10
10/10 - 1s - loss: 137.6676 - loglik: -1.3248e+02 - logprior: -4.9207e+00
Epoch 4/10
10/10 - 1s - loss: 134.7536 - loglik: -1.3200e+02 - logprior: -2.4438e+00
Epoch 5/10
10/10 - 1s - loss: 133.1474 - loglik: -1.3161e+02 - logprior: -1.2357e+00
Epoch 6/10
10/10 - 1s - loss: 132.3571 - loglik: -1.3139e+02 - logprior: -6.8472e-01
Epoch 7/10
10/10 - 1s - loss: 131.8070 - loglik: -1.3109e+02 - logprior: -4.2361e-01
Epoch 8/10
10/10 - 1s - loss: 131.2895 - loglik: -1.3083e+02 - logprior: -1.6232e-01
Epoch 9/10
10/10 - 1s - loss: 131.1319 - loglik: -1.3096e+02 - logprior: 0.1289
Epoch 10/10
10/10 - 1s - loss: 131.0083 - loglik: -1.3105e+02 - logprior: 0.3419
Fitted a model with MAP estimate = -130.5705
Time for alignment: 36.0355
Computed alignments with likelihoods: ['-132.2139', '-131.5949', '-130.5705']
Best model has likelihood: -130.5705
time for generating output: 0.1278
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.912404634077423
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb630fabc10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb64af04250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb69f92a580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb606668f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64adf9940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69fffef70>, <__main__.SimpleDirichletPrior object at 0x7fb686900e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.9553 - loglik: -2.3761e+02 - logprior: -1.3321e+01
Epoch 2/10
11/11 - 2s - loss: 221.2497 - loglik: -2.1763e+02 - logprior: -3.5489e+00
Epoch 3/10
11/11 - 2s - loss: 200.0266 - loglik: -1.9751e+02 - logprior: -2.2879e+00
Epoch 4/10
11/11 - 2s - loss: 190.1548 - loglik: -1.8736e+02 - logprior: -2.2365e+00
Epoch 5/10
11/11 - 2s - loss: 186.6195 - loglik: -1.8384e+02 - logprior: -2.2491e+00
Epoch 6/10
11/11 - 2s - loss: 185.7689 - loglik: -1.8325e+02 - logprior: -2.1100e+00
Epoch 7/10
11/11 - 2s - loss: 183.8614 - loglik: -1.8152e+02 - logprior: -1.9652e+00
Epoch 8/10
11/11 - 2s - loss: 183.9795 - loglik: -1.8169e+02 - logprior: -1.9446e+00
Fitted a model with MAP estimate = -183.0486
expansions: [(8, 1), (10, 2), (11, 2), (12, 2), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 201.7799 - loglik: -1.8649e+02 - logprior: -1.5266e+01
Epoch 2/2
11/11 - 2s - loss: 185.2815 - loglik: -1.7862e+02 - logprior: -6.5214e+00
Fitted a model with MAP estimate = -181.7194
expansions: [(0, 2)]
discards: [ 0 16]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.6444 - loglik: -1.7660e+02 - logprior: -1.2018e+01
Epoch 2/2
11/11 - 2s - loss: 178.2443 - loglik: -1.7477e+02 - logprior: -3.3203e+00
Fitted a model with MAP estimate = -176.1678
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.4965 - loglik: -1.7607e+02 - logprior: -1.4396e+01
Epoch 2/10
11/11 - 2s - loss: 181.4444 - loglik: -1.7692e+02 - logprior: -4.3725e+00
Epoch 3/10
11/11 - 2s - loss: 175.9041 - loglik: -1.7317e+02 - logprior: -2.3999e+00
Epoch 4/10
11/11 - 2s - loss: 175.2050 - loglik: -1.7296e+02 - logprior: -1.7452e+00
Epoch 5/10
11/11 - 2s - loss: 174.2040 - loglik: -1.7230e+02 - logprior: -1.3436e+00
Epoch 6/10
11/11 - 2s - loss: 173.3931 - loglik: -1.7160e+02 - logprior: -1.2750e+00
Epoch 7/10
11/11 - 2s - loss: 173.4902 - loglik: -1.7186e+02 - logprior: -1.1561e+00
Fitted a model with MAP estimate = -172.4782
Time for alignment: 55.4303
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 250.9329 - loglik: -2.3759e+02 - logprior: -1.3319e+01
Epoch 2/10
11/11 - 2s - loss: 221.1998 - loglik: -2.1757e+02 - logprior: -3.5581e+00
Epoch 3/10
11/11 - 2s - loss: 199.5940 - loglik: -1.9705e+02 - logprior: -2.3022e+00
Epoch 4/10
11/11 - 2s - loss: 188.8975 - loglik: -1.8605e+02 - logprior: -2.2524e+00
Epoch 5/10
11/11 - 2s - loss: 186.0476 - loglik: -1.8321e+02 - logprior: -2.2316e+00
Epoch 6/10
11/11 - 2s - loss: 185.2255 - loglik: -1.8274e+02 - logprior: -2.0480e+00
Epoch 7/10
11/11 - 2s - loss: 184.8277 - loglik: -1.8254e+02 - logprior: -1.8948e+00
Epoch 8/10
11/11 - 2s - loss: 182.8807 - loglik: -1.8064e+02 - logprior: -1.8750e+00
Epoch 9/10
11/11 - 2s - loss: 183.4577 - loglik: -1.8116e+02 - logprior: -1.9211e+00
Fitted a model with MAP estimate = -182.5585
expansions: [(11, 4), (17, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 201.0417 - loglik: -1.8579e+02 - logprior: -1.5219e+01
Epoch 2/2
11/11 - 2s - loss: 184.3749 - loglik: -1.7783e+02 - logprior: -6.3994e+00
Fitted a model with MAP estimate = -181.1650
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 187.8409 - loglik: -1.7586e+02 - logprior: -1.1960e+01
Epoch 2/2
11/11 - 2s - loss: 178.3571 - loglik: -1.7492e+02 - logprior: -3.2834e+00
Fitted a model with MAP estimate = -176.0569
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.8362 - loglik: -1.7638e+02 - logprior: -1.4426e+01
Epoch 2/10
11/11 - 2s - loss: 180.6241 - loglik: -1.7605e+02 - logprior: -4.4200e+00
Epoch 3/10
11/11 - 2s - loss: 176.7517 - loglik: -1.7402e+02 - logprior: -2.4014e+00
Epoch 4/10
11/11 - 2s - loss: 175.1054 - loglik: -1.7288e+02 - logprior: -1.7246e+00
Epoch 5/10
11/11 - 2s - loss: 173.9317 - loglik: -1.7204e+02 - logprior: -1.3354e+00
Epoch 6/10
11/11 - 2s - loss: 173.2660 - loglik: -1.7148e+02 - logprior: -1.2547e+00
Epoch 7/10
11/11 - 2s - loss: 173.3580 - loglik: -1.7173e+02 - logprior: -1.1462e+00
Fitted a model with MAP estimate = -172.4611
Time for alignment: 56.9973
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.6610 - loglik: -2.3732e+02 - logprior: -1.3321e+01
Epoch 2/10
11/11 - 2s - loss: 221.2416 - loglik: -2.1762e+02 - logprior: -3.5551e+00
Epoch 3/10
11/11 - 2s - loss: 198.9189 - loglik: -1.9644e+02 - logprior: -2.3147e+00
Epoch 4/10
11/11 - 2s - loss: 189.0709 - loglik: -1.8641e+02 - logprior: -2.3126e+00
Epoch 5/10
11/11 - 2s - loss: 186.1578 - loglik: -1.8332e+02 - logprior: -2.3348e+00
Epoch 6/10
11/11 - 2s - loss: 184.3113 - loglik: -1.8167e+02 - logprior: -2.1760e+00
Epoch 7/10
11/11 - 2s - loss: 184.3138 - loglik: -1.8186e+02 - logprior: -2.0162e+00
Fitted a model with MAP estimate = -183.2359
expansions: [(9, 1), (10, 3), (11, 1), (17, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 200.6087 - loglik: -1.8535e+02 - logprior: -1.5224e+01
Epoch 2/2
11/11 - 2s - loss: 184.5477 - loglik: -1.7799e+02 - logprior: -6.4459e+00
Fitted a model with MAP estimate = -181.6884
expansions: [(0, 2)]
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.1474 - loglik: -1.7614e+02 - logprior: -1.1978e+01
Epoch 2/2
11/11 - 2s - loss: 178.6483 - loglik: -1.7529e+02 - logprior: -3.2557e+00
Fitted a model with MAP estimate = -176.3631
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 191.0425 - loglik: -1.7672e+02 - logprior: -1.4299e+01
Epoch 2/10
11/11 - 2s - loss: 180.5892 - loglik: -1.7615e+02 - logprior: -4.2934e+00
Epoch 3/10
11/11 - 2s - loss: 175.7706 - loglik: -1.7302e+02 - logprior: -2.3865e+00
Epoch 4/10
11/11 - 2s - loss: 175.4335 - loglik: -1.7316e+02 - logprior: -1.7430e+00
Epoch 5/10
11/11 - 2s - loss: 174.0627 - loglik: -1.7213e+02 - logprior: -1.3687e+00
Epoch 6/10
11/11 - 2s - loss: 174.2329 - loglik: -1.7245e+02 - logprior: -1.2709e+00
Fitted a model with MAP estimate = -172.8760
Time for alignment: 51.9740
Computed alignments with likelihoods: ['-172.4782', '-172.4611', '-172.8760']
Best model has likelihood: -172.4611
time for generating output: 0.1544
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8193172356369692
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb604e92370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6974a4a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6866e9850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb64aa03bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64aa03d90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb64aa03e50>, <__main__.SimpleDirichletPrior object at 0x7fb60eb15130>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.7295 - loglik: -2.2369e+02 - logprior: -4.1023e+01
Epoch 2/10
10/10 - 1s - loss: 209.0049 - loglik: -1.9786e+02 - logprior: -1.1137e+01
Epoch 3/10
10/10 - 1s - loss: 178.4604 - loglik: -1.7260e+02 - logprior: -5.8452e+00
Epoch 4/10
10/10 - 1s - loss: 156.6456 - loglik: -1.5248e+02 - logprior: -4.1609e+00
Epoch 5/10
10/10 - 1s - loss: 148.2593 - loglik: -1.4469e+02 - logprior: -3.5222e+00
Epoch 6/10
10/10 - 1s - loss: 144.8503 - loglik: -1.4140e+02 - logprior: -3.1942e+00
Epoch 7/10
10/10 - 1s - loss: 143.5926 - loglik: -1.4026e+02 - logprior: -2.8965e+00
Epoch 8/10
10/10 - 1s - loss: 142.4835 - loglik: -1.3936e+02 - logprior: -2.7078e+00
Epoch 9/10
10/10 - 1s - loss: 142.3216 - loglik: -1.3938e+02 - logprior: -2.5796e+00
Epoch 10/10
10/10 - 1s - loss: 141.9048 - loglik: -1.3904e+02 - logprior: -2.5093e+00
Fitted a model with MAP estimate = -141.4425
expansions: [(2, 1), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 172.3713 - loglik: -1.3485e+02 - logprior: -3.7495e+01
Epoch 2/2
10/10 - 1s - loss: 138.2419 - loglik: -1.2778e+02 - logprior: -1.0338e+01
Fitted a model with MAP estimate = -133.2266
expansions: []
discards: [45 58 69]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.4781 - loglik: -1.2704e+02 - logprior: -3.6424e+01
Epoch 2/2
10/10 - 1s - loss: 136.3174 - loglik: -1.2639e+02 - logprior: -9.8286e+00
Fitted a model with MAP estimate = -131.9759
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.3052 - loglik: -1.3081e+02 - logprior: -4.5474e+01
Epoch 2/10
10/10 - 1s - loss: 148.7086 - loglik: -1.2977e+02 - logprior: -1.8828e+01
Epoch 3/10
10/10 - 1s - loss: 142.1396 - loglik: -1.2853e+02 - logprior: -1.3407e+01
Epoch 4/10
10/10 - 1s - loss: 139.3146 - loglik: -1.2800e+02 - logprior: -1.1061e+01
Epoch 5/10
10/10 - 1s - loss: 136.0894 - loglik: -1.2780e+02 - logprior: -8.0156e+00
Epoch 6/10
10/10 - 1s - loss: 130.4792 - loglik: -1.2807e+02 - logprior: -2.1029e+00
Epoch 7/10
10/10 - 1s - loss: 128.4342 - loglik: -1.2803e+02 - logprior: -8.8695e-02
Epoch 8/10
10/10 - 1s - loss: 128.1240 - loglik: -1.2805e+02 - logprior: 0.2596
Epoch 9/10
10/10 - 1s - loss: 127.9575 - loglik: -1.2806e+02 - logprior: 0.4455
Epoch 10/10
10/10 - 1s - loss: 127.7041 - loglik: -1.2794e+02 - logprior: 0.5983
Fitted a model with MAP estimate = -127.2429
Time for alignment: 40.3303
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.7706 - loglik: -2.2373e+02 - logprior: -4.1025e+01
Epoch 2/10
10/10 - 1s - loss: 209.0388 - loglik: -1.9789e+02 - logprior: -1.1136e+01
Epoch 3/10
10/10 - 1s - loss: 177.8346 - loglik: -1.7200e+02 - logprior: -5.8245e+00
Epoch 4/10
10/10 - 1s - loss: 156.0311 - loglik: -1.5184e+02 - logprior: -4.1652e+00
Epoch 5/10
10/10 - 1s - loss: 147.6919 - loglik: -1.4401e+02 - logprior: -3.5388e+00
Epoch 6/10
10/10 - 1s - loss: 144.6671 - loglik: -1.4115e+02 - logprior: -3.1339e+00
Epoch 7/10
10/10 - 1s - loss: 143.8684 - loglik: -1.4056e+02 - logprior: -2.8294e+00
Epoch 8/10
10/10 - 1s - loss: 142.8915 - loglik: -1.3977e+02 - logprior: -2.6626e+00
Epoch 9/10
10/10 - 1s - loss: 142.4749 - loglik: -1.3951e+02 - logprior: -2.5333e+00
Epoch 10/10
10/10 - 1s - loss: 142.3410 - loglik: -1.3948e+02 - logprior: -2.4602e+00
Fitted a model with MAP estimate = -141.6504
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.2922 - loglik: -1.3383e+02 - logprior: -3.7438e+01
Epoch 2/2
10/10 - 1s - loss: 137.3174 - loglik: -1.2706e+02 - logprior: -1.0141e+01
Fitted a model with MAP estimate = -132.2154
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 177.2852 - loglik: -1.3152e+02 - logprior: -4.5749e+01
Epoch 2/2
10/10 - 1s - loss: 149.6358 - loglik: -1.3051e+02 - logprior: -1.9003e+01
Fitted a model with MAP estimate = -145.1019
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 165.2914 - loglik: -1.2837e+02 - logprior: -3.6899e+01
Epoch 2/10
10/10 - 1s - loss: 136.4600 - loglik: -1.2678e+02 - logprior: -9.5855e+00
Epoch 3/10
10/10 - 1s - loss: 129.8733 - loglik: -1.2559e+02 - logprior: -4.0965e+00
Epoch 4/10
10/10 - 1s - loss: 127.7381 - loglik: -1.2559e+02 - logprior: -1.9181e+00
Epoch 5/10
10/10 - 1s - loss: 126.7455 - loglik: -1.2570e+02 - logprior: -7.8743e-01
Epoch 6/10
10/10 - 1s - loss: 126.0184 - loglik: -1.2555e+02 - logprior: -1.7842e-01
Epoch 7/10
10/10 - 1s - loss: 125.8227 - loglik: -1.2569e+02 - logprior: 0.1748
Epoch 8/10
10/10 - 1s - loss: 125.4338 - loglik: -1.2552e+02 - logprior: 0.4081
Epoch 9/10
10/10 - 1s - loss: 125.4585 - loglik: -1.2574e+02 - logprior: 0.6111
Fitted a model with MAP estimate = -124.8350
Time for alignment: 38.9138
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.7819 - loglik: -2.2374e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 209.1082 - loglik: -1.9796e+02 - logprior: -1.1138e+01
Epoch 3/10
10/10 - 1s - loss: 179.4690 - loglik: -1.7359e+02 - logprior: -5.8628e+00
Epoch 4/10
10/10 - 1s - loss: 156.9375 - loglik: -1.5260e+02 - logprior: -4.3214e+00
Epoch 5/10
10/10 - 1s - loss: 147.3909 - loglik: -1.4355e+02 - logprior: -3.8046e+00
Epoch 6/10
10/10 - 1s - loss: 143.5184 - loglik: -1.3978e+02 - logprior: -3.5187e+00
Epoch 7/10
10/10 - 1s - loss: 142.1968 - loglik: -1.3860e+02 - logprior: -3.2003e+00
Epoch 8/10
10/10 - 1s - loss: 141.3258 - loglik: -1.3798e+02 - logprior: -2.9404e+00
Epoch 9/10
10/10 - 1s - loss: 141.1697 - loglik: -1.3802e+02 - logprior: -2.7861e+00
Epoch 10/10
10/10 - 1s - loss: 140.9513 - loglik: -1.3787e+02 - logprior: -2.7170e+00
Fitted a model with MAP estimate = -140.3712
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.2315 - loglik: -1.3471e+02 - logprior: -3.7502e+01
Epoch 2/2
10/10 - 1s - loss: 138.4919 - loglik: -1.2804e+02 - logprior: -1.0332e+01
Fitted a model with MAP estimate = -133.2297
expansions: []
discards: [45 58 69]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.7000 - loglik: -1.2727e+02 - logprior: -3.6419e+01
Epoch 2/2
10/10 - 1s - loss: 136.0396 - loglik: -1.2613e+02 - logprior: -9.8156e+00
Fitted a model with MAP estimate = -131.9659
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.2878 - loglik: -1.3079e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 1s - loss: 148.6879 - loglik: -1.2976e+02 - logprior: -1.8818e+01
Epoch 3/10
10/10 - 1s - loss: 142.2030 - loglik: -1.2861e+02 - logprior: -1.3393e+01
Epoch 4/10
10/10 - 1s - loss: 139.3817 - loglik: -1.2804e+02 - logprior: -1.1085e+01
Epoch 5/10
10/10 - 1s - loss: 136.5461 - loglik: -1.2800e+02 - logprior: -8.2673e+00
Epoch 6/10
10/10 - 1s - loss: 130.3226 - loglik: -1.2766e+02 - logprior: -2.3593e+00
Epoch 7/10
10/10 - 1s - loss: 128.5579 - loglik: -1.2814e+02 - logprior: -9.8296e-02
Epoch 8/10
10/10 - 1s - loss: 128.3090 - loglik: -1.2824e+02 - logprior: 0.2676
Epoch 9/10
10/10 - 1s - loss: 127.7072 - loglik: -1.2781e+02 - logprior: 0.4504
Epoch 10/10
10/10 - 1s - loss: 127.8960 - loglik: -1.2815e+02 - logprior: 0.6086
Fitted a model with MAP estimate = -127.2441
Time for alignment: 39.3148
Computed alignments with likelihoods: ['-127.2429', '-124.8350', '-127.2441']
Best model has likelihood: -124.8350
time for generating output: 0.1431
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.9008880994671403
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6055c3fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb69fb2d160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4697400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c46975e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb69fa6ae80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6055b0790>, <__main__.SimpleDirichletPrior object at 0x7fb6970b8580>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4836 - loglik: -1.6678e+02 - logprior: -5.6642e+00
Epoch 2/10
15/15 - 1s - loss: 144.9773 - loglik: -1.4314e+02 - logprior: -1.8111e+00
Epoch 3/10
15/15 - 1s - loss: 131.0632 - loglik: -1.2919e+02 - logprior: -1.8458e+00
Epoch 4/10
15/15 - 1s - loss: 126.6122 - loglik: -1.2457e+02 - logprior: -1.8421e+00
Epoch 5/10
15/15 - 1s - loss: 125.2223 - loglik: -1.2291e+02 - logprior: -1.8013e+00
Epoch 6/10
15/15 - 1s - loss: 124.6182 - loglik: -1.2234e+02 - logprior: -1.8154e+00
Epoch 7/10
15/15 - 1s - loss: 124.2531 - loglik: -1.2202e+02 - logprior: -1.7933e+00
Epoch 8/10
15/15 - 1s - loss: 124.0253 - loglik: -1.2182e+02 - logprior: -1.7727e+00
Epoch 9/10
15/15 - 1s - loss: 123.9132 - loglik: -1.2172e+02 - logprior: -1.7639e+00
Epoch 10/10
15/15 - 1s - loss: 123.6036 - loglik: -1.2140e+02 - logprior: -1.7536e+00
Fitted a model with MAP estimate = -123.1071
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 137.7038 - loglik: -1.3064e+02 - logprior: -7.0031e+00
Epoch 2/2
15/15 - 1s - loss: 126.8025 - loglik: -1.2293e+02 - logprior: -3.5776e+00
Fitted a model with MAP estimate = -124.4935
expansions: [(0, 2)]
discards: [ 0 13 15 31 46]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.7078 - loglik: -1.2245e+02 - logprior: -5.2067e+00
Epoch 2/2
15/15 - 1s - loss: 121.2045 - loglik: -1.1918e+02 - logprior: -1.7689e+00
Fitted a model with MAP estimate = -119.7686
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.2598 - loglik: -1.2259e+02 - logprior: -6.6080e+00
Epoch 2/10
15/15 - 1s - loss: 122.1366 - loglik: -1.1957e+02 - logprior: -2.2800e+00
Epoch 3/10
15/15 - 1s - loss: 120.2304 - loglik: -1.1818e+02 - logprior: -1.6034e+00
Epoch 4/10
15/15 - 1s - loss: 119.6234 - loglik: -1.1770e+02 - logprior: -1.4568e+00
Epoch 5/10
15/15 - 1s - loss: 119.3133 - loglik: -1.1743e+02 - logprior: -1.4090e+00
Epoch 6/10
15/15 - 1s - loss: 118.8350 - loglik: -1.1697e+02 - logprior: -1.3947e+00
Epoch 7/10
15/15 - 1s - loss: 118.7837 - loglik: -1.1694e+02 - logprior: -1.3817e+00
Epoch 8/10
15/15 - 1s - loss: 118.4369 - loglik: -1.1661e+02 - logprior: -1.3701e+00
Epoch 9/10
15/15 - 1s - loss: 118.3446 - loglik: -1.1653e+02 - logprior: -1.3518e+00
Epoch 10/10
15/15 - 1s - loss: 118.1861 - loglik: -1.1638e+02 - logprior: -1.3347e+00
Fitted a model with MAP estimate = -117.5725
Time for alignment: 47.3905
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 172.3256 - loglik: -1.6662e+02 - logprior: -5.6654e+00
Epoch 2/10
15/15 - 1s - loss: 145.2790 - loglik: -1.4345e+02 - logprior: -1.8084e+00
Epoch 3/10
15/15 - 1s - loss: 130.5197 - loglik: -1.2853e+02 - logprior: -1.8729e+00
Epoch 4/10
15/15 - 1s - loss: 126.4768 - loglik: -1.2418e+02 - logprior: -1.8407e+00
Epoch 5/10
15/15 - 1s - loss: 125.5072 - loglik: -1.2331e+02 - logprior: -1.7613e+00
Epoch 6/10
15/15 - 1s - loss: 124.9459 - loglik: -1.2273e+02 - logprior: -1.7866e+00
Epoch 7/10
15/15 - 1s - loss: 124.3000 - loglik: -1.2209e+02 - logprior: -1.7719e+00
Epoch 8/10
15/15 - 1s - loss: 124.0284 - loglik: -1.2181e+02 - logprior: -1.7665e+00
Epoch 9/10
15/15 - 1s - loss: 123.6628 - loglik: -1.2143e+02 - logprior: -1.7593e+00
Epoch 10/10
15/15 - 1s - loss: 123.6012 - loglik: -1.2137e+02 - logprior: -1.7529e+00
Fitted a model with MAP estimate = -122.9494
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 137.4727 - loglik: -1.3044e+02 - logprior: -6.9686e+00
Epoch 2/2
15/15 - 1s - loss: 126.8715 - loglik: -1.2302e+02 - logprior: -3.5503e+00
Fitted a model with MAP estimate = -124.4421
expansions: [(0, 1)]
discards: [ 0 13 15 38 45]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 129.2556 - loglik: -1.2388e+02 - logprior: -5.3201e+00
Epoch 2/2
15/15 - 1s - loss: 122.3464 - loglik: -1.2004e+02 - logprior: -2.0405e+00
Fitted a model with MAP estimate = -120.6394
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 125.9248 - loglik: -1.2044e+02 - logprior: -5.4306e+00
Epoch 2/10
15/15 - 1s - loss: 121.3577 - loglik: -1.1912e+02 - logprior: -1.9948e+00
Epoch 3/10
15/15 - 1s - loss: 120.5221 - loglik: -1.1864e+02 - logprior: -1.5745e+00
Epoch 4/10
15/15 - 1s - loss: 119.9414 - loglik: -1.1809e+02 - logprior: -1.4416e+00
Epoch 5/10
15/15 - 1s - loss: 119.4341 - loglik: -1.1757e+02 - logprior: -1.4051e+00
Epoch 6/10
15/15 - 1s - loss: 119.0545 - loglik: -1.1721e+02 - logprior: -1.3832e+00
Epoch 7/10
15/15 - 1s - loss: 118.8067 - loglik: -1.1698e+02 - logprior: -1.3698e+00
Epoch 8/10
15/15 - 1s - loss: 118.7346 - loglik: -1.1693e+02 - logprior: -1.3483e+00
Epoch 9/10
15/15 - 1s - loss: 118.6398 - loglik: -1.1684e+02 - logprior: -1.3382e+00
Epoch 10/10
15/15 - 1s - loss: 118.3555 - loglik: -1.1657e+02 - logprior: -1.3219e+00
Fitted a model with MAP estimate = -117.8014
Time for alignment: 48.5089
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4377 - loglik: -1.6673e+02 - logprior: -5.6663e+00
Epoch 2/10
15/15 - 1s - loss: 146.1730 - loglik: -1.4434e+02 - logprior: -1.8116e+00
Epoch 3/10
15/15 - 1s - loss: 132.0764 - loglik: -1.3015e+02 - logprior: -1.8751e+00
Epoch 4/10
15/15 - 1s - loss: 126.5127 - loglik: -1.2425e+02 - logprior: -1.8788e+00
Epoch 5/10
15/15 - 1s - loss: 125.3084 - loglik: -1.2298e+02 - logprior: -1.8121e+00
Epoch 6/10
15/15 - 1s - loss: 124.6818 - loglik: -1.2238e+02 - logprior: -1.8298e+00
Epoch 7/10
15/15 - 1s - loss: 124.4139 - loglik: -1.2213e+02 - logprior: -1.7943e+00
Epoch 8/10
15/15 - 1s - loss: 123.9471 - loglik: -1.2169e+02 - logprior: -1.7777e+00
Epoch 9/10
15/15 - 1s - loss: 124.0642 - loglik: -1.2185e+02 - logprior: -1.7696e+00
Fitted a model with MAP estimate = -123.2219
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 138.3418 - loglik: -1.3125e+02 - logprior: -7.0359e+00
Epoch 2/2
15/15 - 1s - loss: 127.3735 - loglik: -1.2346e+02 - logprior: -3.6078e+00
Fitted a model with MAP estimate = -124.5537
expansions: [(0, 1)]
discards: [ 0 13 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.9384 - loglik: -1.2355e+02 - logprior: -5.3306e+00
Epoch 2/2
15/15 - 1s - loss: 122.3050 - loglik: -1.1999e+02 - logprior: -2.0570e+00
Fitted a model with MAP estimate = -120.5208
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 125.7294 - loglik: -1.2023e+02 - logprior: -5.4436e+00
Epoch 2/10
15/15 - 1s - loss: 121.3044 - loglik: -1.1901e+02 - logprior: -2.0127e+00
Epoch 3/10
15/15 - 1s - loss: 120.2398 - loglik: -1.1819e+02 - logprior: -1.5870e+00
Epoch 4/10
15/15 - 1s - loss: 119.7849 - loglik: -1.1785e+02 - logprior: -1.4621e+00
Epoch 5/10
15/15 - 1s - loss: 119.1062 - loglik: -1.1722e+02 - logprior: -1.4096e+00
Epoch 6/10
15/15 - 1s - loss: 118.9856 - loglik: -1.1712e+02 - logprior: -1.4047e+00
Epoch 7/10
15/15 - 1s - loss: 118.5782 - loglik: -1.1674e+02 - logprior: -1.3755e+00
Epoch 8/10
15/15 - 1s - loss: 118.3870 - loglik: -1.1656e+02 - logprior: -1.3687e+00
Epoch 9/10
15/15 - 1s - loss: 118.4241 - loglik: -1.1660e+02 - logprior: -1.3529e+00
Fitted a model with MAP estimate = -117.7076
Time for alignment: 45.1117
Computed alignments with likelihoods: ['-117.5725', '-117.8014', '-117.7076']
Best model has likelihood: -117.5725
time for generating output: 0.1395
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64ab1afa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c47c3cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c47c3fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb68699eb20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb68699ed00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb630e7ffd0>, <__main__.SimpleDirichletPrior object at 0x7fb664008be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.8955 - loglik: -1.8345e+02 - logprior: -1.4440e+01
Epoch 2/10
10/10 - 2s - loss: 170.5826 - loglik: -1.6636e+02 - logprior: -4.1884e+00
Epoch 3/10
10/10 - 2s - loss: 153.9225 - loglik: -1.5117e+02 - logprior: -2.5992e+00
Epoch 4/10
10/10 - 2s - loss: 141.3251 - loglik: -1.3862e+02 - logprior: -2.4442e+00
Epoch 5/10
10/10 - 2s - loss: 135.9290 - loglik: -1.3301e+02 - logprior: -2.5729e+00
Epoch 6/10
10/10 - 2s - loss: 133.5664 - loglik: -1.3064e+02 - logprior: -2.6908e+00
Epoch 7/10
10/10 - 2s - loss: 132.9621 - loglik: -1.3017e+02 - logprior: -2.6128e+00
Epoch 8/10
10/10 - 1s - loss: 131.7887 - loglik: -1.2919e+02 - logprior: -2.4455e+00
Epoch 9/10
10/10 - 2s - loss: 132.1385 - loglik: -1.2966e+02 - logprior: -2.3482e+00
Fitted a model with MAP estimate = -131.6250
expansions: [(4, 2), (5, 2), (7, 1), (8, 2), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 147.4250 - loglik: -1.3120e+02 - logprior: -1.6202e+01
Epoch 2/2
10/10 - 2s - loss: 128.6295 - loglik: -1.2160e+02 - logprior: -6.9268e+00
Fitted a model with MAP estimate = -125.4872
expansions: [(0, 2)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.3756 - loglik: -1.1955e+02 - logprior: -1.2801e+01
Epoch 2/2
10/10 - 2s - loss: 121.4673 - loglik: -1.1776e+02 - logprior: -3.6113e+00
Fitted a model with MAP estimate = -120.0118
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.1520 - loglik: -1.1996e+02 - logprior: -1.5178e+01
Epoch 2/10
10/10 - 2s - loss: 124.0918 - loglik: -1.1916e+02 - logprior: -4.8423e+00
Epoch 3/10
10/10 - 2s - loss: 121.0955 - loglik: -1.1823e+02 - logprior: -2.6637e+00
Epoch 4/10
10/10 - 2s - loss: 119.4387 - loglik: -1.1721e+02 - logprior: -1.9546e+00
Epoch 5/10
10/10 - 2s - loss: 118.4652 - loglik: -1.1673e+02 - logprior: -1.4743e+00
Epoch 6/10
10/10 - 2s - loss: 118.4339 - loglik: -1.1687e+02 - logprior: -1.3474e+00
Epoch 7/10
10/10 - 2s - loss: 118.3496 - loglik: -1.1693e+02 - logprior: -1.2307e+00
Epoch 8/10
10/10 - 2s - loss: 117.8413 - loglik: -1.1651e+02 - logprior: -1.1577e+00
Epoch 9/10
10/10 - 2s - loss: 118.0012 - loglik: -1.1669e+02 - logprior: -1.1418e+00
Fitted a model with MAP estimate = -117.6637
Time for alignment: 68.5600
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.8454 - loglik: -1.8340e+02 - logprior: -1.4440e+01
Epoch 2/10
10/10 - 2s - loss: 170.6081 - loglik: -1.6640e+02 - logprior: -4.1867e+00
Epoch 3/10
10/10 - 2s - loss: 153.6996 - loglik: -1.5098e+02 - logprior: -2.5642e+00
Epoch 4/10
10/10 - 2s - loss: 141.1029 - loglik: -1.3838e+02 - logprior: -2.3520e+00
Epoch 5/10
10/10 - 2s - loss: 137.0811 - loglik: -1.3428e+02 - logprior: -2.4331e+00
Epoch 6/10
10/10 - 2s - loss: 134.9645 - loglik: -1.3230e+02 - logprior: -2.4772e+00
Epoch 7/10
10/10 - 2s - loss: 133.6924 - loglik: -1.3115e+02 - logprior: -2.3877e+00
Epoch 8/10
10/10 - 2s - loss: 133.4584 - loglik: -1.3109e+02 - logprior: -2.2472e+00
Epoch 9/10
10/10 - 2s - loss: 132.9976 - loglik: -1.3071e+02 - logprior: -2.1739e+00
Epoch 10/10
10/10 - 2s - loss: 132.7293 - loglik: -1.3044e+02 - logprior: -2.1758e+00
Fitted a model with MAP estimate = -132.3473
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 1), (39, 1), (40, 2), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.5619 - loglik: -1.3135e+02 - logprior: -1.6185e+01
Epoch 2/2
10/10 - 2s - loss: 128.5874 - loglik: -1.2149e+02 - logprior: -6.9856e+00
Fitted a model with MAP estimate = -125.4020
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 132.1927 - loglik: -1.1936e+02 - logprior: -1.2814e+01
Epoch 2/2
10/10 - 2s - loss: 121.3882 - loglik: -1.1767e+02 - logprior: -3.6202e+00
Fitted a model with MAP estimate = -119.9305
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.3190 - loglik: -1.2008e+02 - logprior: -1.5220e+01
Epoch 2/10
10/10 - 2s - loss: 124.2794 - loglik: -1.1928e+02 - logprior: -4.8976e+00
Epoch 3/10
10/10 - 2s - loss: 120.5397 - loglik: -1.1763e+02 - logprior: -2.6816e+00
Epoch 4/10
10/10 - 2s - loss: 119.5745 - loglik: -1.1732e+02 - logprior: -1.9593e+00
Epoch 5/10
10/10 - 2s - loss: 118.6410 - loglik: -1.1690e+02 - logprior: -1.4664e+00
Epoch 6/10
10/10 - 2s - loss: 118.2558 - loglik: -1.1669e+02 - logprior: -1.3413e+00
Epoch 7/10
10/10 - 2s - loss: 118.0244 - loglik: -1.1661e+02 - logprior: -1.2253e+00
Epoch 8/10
10/10 - 2s - loss: 118.0878 - loglik: -1.1676e+02 - logprior: -1.1532e+00
Fitted a model with MAP estimate = -117.7104
Time for alignment: 66.2855
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 197.5439 - loglik: -1.8310e+02 - logprior: -1.4440e+01
Epoch 2/10
10/10 - 2s - loss: 170.7535 - loglik: -1.6654e+02 - logprior: -4.1829e+00
Epoch 3/10
10/10 - 2s - loss: 152.8353 - loglik: -1.5012e+02 - logprior: -2.5464e+00
Epoch 4/10
10/10 - 2s - loss: 141.4451 - loglik: -1.3871e+02 - logprior: -2.3375e+00
Epoch 5/10
10/10 - 2s - loss: 137.6356 - loglik: -1.3482e+02 - logprior: -2.4222e+00
Epoch 6/10
10/10 - 2s - loss: 136.4438 - loglik: -1.3369e+02 - logprior: -2.5206e+00
Epoch 7/10
10/10 - 2s - loss: 134.8975 - loglik: -1.3218e+02 - logprior: -2.5000e+00
Epoch 8/10
10/10 - 2s - loss: 133.6392 - loglik: -1.3105e+02 - logprior: -2.4262e+00
Epoch 9/10
10/10 - 2s - loss: 133.8972 - loglik: -1.3137e+02 - logprior: -2.3832e+00
Fitted a model with MAP estimate = -133.4805
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (33, 1), (34, 1), (35, 1), (36, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 150.4181 - loglik: -1.3418e+02 - logprior: -1.6215e+01
Epoch 2/2
10/10 - 2s - loss: 130.0107 - loglik: -1.2291e+02 - logprior: -6.9877e+00
Fitted a model with MAP estimate = -125.8905
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.0474 - loglik: -1.1922e+02 - logprior: -1.2811e+01
Epoch 2/2
10/10 - 2s - loss: 121.9561 - loglik: -1.1824e+02 - logprior: -3.6208e+00
Fitted a model with MAP estimate = -119.9419
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.3254 - loglik: -1.2006e+02 - logprior: -1.5243e+01
Epoch 2/10
10/10 - 2s - loss: 124.1866 - loglik: -1.1917e+02 - logprior: -4.9110e+00
Epoch 3/10
10/10 - 2s - loss: 120.8093 - loglik: -1.1791e+02 - logprior: -2.6794e+00
Epoch 4/10
10/10 - 2s - loss: 119.0990 - loglik: -1.1684e+02 - logprior: -1.9727e+00
Epoch 5/10
10/10 - 2s - loss: 119.0250 - loglik: -1.1728e+02 - logprior: -1.4703e+00
Epoch 6/10
10/10 - 2s - loss: 118.0653 - loglik: -1.1649e+02 - logprior: -1.3470e+00
Epoch 7/10
10/10 - 3s - loss: 118.4763 - loglik: -1.1705e+02 - logprior: -1.2305e+00
Fitted a model with MAP estimate = -117.8254
Time for alignment: 64.9152
Computed alignments with likelihoods: ['-117.6637', '-117.7104', '-117.8254']
Best model has likelihood: -117.6637
time for generating output: 0.3289
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb603cdd610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb69fb6a970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60eaaa3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb67df366a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb69fa6e130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69fa6e5e0>, <__main__.SimpleDirichletPrior object at 0x7fb605f2b760>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 528.7505 - loglik: -5.2207e+02 - logprior: -6.5135e+00
Epoch 2/10
23/23 - 6s - loss: 454.4612 - loglik: -4.5223e+02 - logprior: -1.1181e+00
Epoch 3/10
23/23 - 6s - loss: 433.5693 - loglik: -4.3091e+02 - logprior: -1.0754e+00
Epoch 4/10
23/23 - 6s - loss: 428.3651 - loglik: -4.2560e+02 - logprior: -1.0215e+00
Epoch 5/10
23/23 - 6s - loss: 425.5442 - loglik: -4.2315e+02 - logprior: -1.0241e+00
Epoch 6/10
23/23 - 6s - loss: 423.1073 - loglik: -4.2099e+02 - logprior: -1.0591e+00
Epoch 7/10
23/23 - 6s - loss: 422.6716 - loglik: -4.2075e+02 - logprior: -1.0722e+00
Epoch 8/10
23/23 - 6s - loss: 422.9108 - loglik: -4.2112e+02 - logprior: -1.0612e+00
Fitted a model with MAP estimate = -421.2957
expansions: [(0, 8), (8, 5), (43, 1), (57, 2), (58, 2), (59, 1), (66, 1), (71, 1), (72, 1), (78, 3), (83, 1), (92, 1), (113, 1), (120, 1), (121, 1), (122, 1), (127, 1), (128, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 12s - loss: 447.7703 - loglik: -4.3879e+02 - logprior: -8.8357e+00
Epoch 2/2
23/23 - 8s - loss: 420.5310 - loglik: -4.1904e+02 - logprior: -7.9342e-01
Fitted a model with MAP estimate = -412.2491
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6   7  72 196 197 198 199 200]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 434.0560 - loglik: -4.2490e+02 - logprior: -9.0282e+00
Epoch 2/2
23/23 - 7s - loss: 419.2131 - loglik: -4.1782e+02 - logprior: -8.0031e-01
Fitted a model with MAP estimate = -411.7321
expansions: [(0, 6), (195, 6)]
discards: [1 2 3 4 7]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 432.4028 - loglik: -4.2323e+02 - logprior: -9.0671e+00
Epoch 2/10
23/23 - 8s - loss: 417.0845 - loglik: -4.1540e+02 - logprior: -1.1139e+00
Epoch 3/10
23/23 - 8s - loss: 408.9976 - loglik: -4.0789e+02 - logprior: 0.1552
Epoch 4/10
23/23 - 8s - loss: 405.6432 - loglik: -4.0462e+02 - logprior: 0.4879
Epoch 5/10
23/23 - 8s - loss: 404.8497 - loglik: -4.0419e+02 - logprior: 0.6411
Epoch 6/10
23/23 - 8s - loss: 402.5465 - loglik: -4.0222e+02 - logprior: 0.7572
Epoch 7/10
23/23 - 8s - loss: 401.9578 - loglik: -4.0192e+02 - logprior: 0.8879
Epoch 8/10
23/23 - 8s - loss: 403.2804 - loglik: -4.0346e+02 - logprior: 1.0113
Fitted a model with MAP estimate = -400.6172
Time for alignment: 180.3010
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 527.8177 - loglik: -5.2113e+02 - logprior: -6.5191e+00
Epoch 2/10
23/23 - 6s - loss: 454.3035 - loglik: -4.5194e+02 - logprior: -1.2113e+00
Epoch 3/10
23/23 - 6s - loss: 429.4403 - loglik: -4.2596e+02 - logprior: -1.3780e+00
Epoch 4/10
23/23 - 6s - loss: 425.0013 - loglik: -4.2192e+02 - logprior: -1.1818e+00
Epoch 5/10
23/23 - 6s - loss: 423.8779 - loglik: -4.2134e+02 - logprior: -1.1628e+00
Epoch 6/10
23/23 - 6s - loss: 419.5420 - loglik: -4.1723e+02 - logprior: -1.2196e+00
Epoch 7/10
23/23 - 6s - loss: 419.8086 - loglik: -4.1765e+02 - logprior: -1.2741e+00
Fitted a model with MAP estimate = -418.9566
expansions: [(0, 8), (9, 4), (15, 1), (18, 1), (42, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 5), (83, 1), (84, 1), (86, 1), (112, 2), (113, 1), (115, 1), (121, 1), (124, 1), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 449.0129 - loglik: -4.3999e+02 - logprior: -8.8839e+00
Epoch 2/2
23/23 - 8s - loss: 420.8487 - loglik: -4.1932e+02 - logprior: -9.0792e-01
Fitted a model with MAP estimate = -412.6247
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6   7   8   9  72  93  94 142 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 434.7887 - loglik: -4.2548e+02 - logprior: -9.1846e+00
Epoch 2/2
23/23 - 7s - loss: 418.3787 - loglik: -4.1700e+02 - logprior: -8.2648e-01
Fitted a model with MAP estimate = -411.9001
expansions: [(0, 7), (193, 5)]
discards: [1 2 7]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 431.9785 - loglik: -4.2278e+02 - logprior: -9.0931e+00
Epoch 2/10
23/23 - 8s - loss: 417.8184 - loglik: -4.1627e+02 - logprior: -9.9206e-01
Epoch 3/10
23/23 - 8s - loss: 409.4518 - loglik: -4.0828e+02 - logprior: 0.0992
Epoch 4/10
23/23 - 8s - loss: 406.6304 - loglik: -4.0565e+02 - logprior: 0.5253
Epoch 5/10
23/23 - 8s - loss: 403.0624 - loglik: -4.0242e+02 - logprior: 0.6559
Epoch 6/10
23/23 - 8s - loss: 404.0834 - loglik: -4.0379e+02 - logprior: 0.7838
Fitted a model with MAP estimate = -401.7076
Time for alignment: 158.4964
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 527.5342 - loglik: -5.2087e+02 - logprior: -6.4917e+00
Epoch 2/10
23/23 - 6s - loss: 453.3012 - loglik: -4.5106e+02 - logprior: -1.1320e+00
Epoch 3/10
23/23 - 6s - loss: 432.4568 - loglik: -4.2960e+02 - logprior: -1.1967e+00
Epoch 4/10
23/23 - 6s - loss: 428.5884 - loglik: -4.2574e+02 - logprior: -1.1340e+00
Epoch 5/10
23/23 - 6s - loss: 423.5340 - loglik: -4.2111e+02 - logprior: -1.0893e+00
Epoch 6/10
23/23 - 6s - loss: 423.1589 - loglik: -4.2103e+02 - logprior: -1.1002e+00
Epoch 7/10
23/23 - 6s - loss: 422.7215 - loglik: -4.2076e+02 - logprior: -1.0973e+00
Epoch 8/10
23/23 - 6s - loss: 423.4169 - loglik: -4.2157e+02 - logprior: -1.0918e+00
Fitted a model with MAP estimate = -421.2847
expansions: [(0, 8), (9, 4), (10, 1), (18, 1), (51, 1), (56, 2), (57, 1), (59, 1), (66, 1), (70, 1), (72, 2), (80, 3), (83, 2), (86, 1), (107, 2), (112, 1), (119, 1), (120, 2), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 450.3298 - loglik: -4.4114e+02 - logprior: -9.0551e+00
Epoch 2/2
23/23 - 8s - loss: 419.5202 - loglik: -4.1779e+02 - logprior: -1.0231e+00
Fitted a model with MAP estimate = -411.8921
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6   7   8  72  93 109 137 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 435.2364 - loglik: -4.2589e+02 - logprior: -9.2208e+00
Epoch 2/2
23/23 - 7s - loss: 418.1900 - loglik: -4.1693e+02 - logprior: -6.7758e-01
Fitted a model with MAP estimate = -411.5823
expansions: [(0, 6), (194, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 12s - loss: 429.2449 - loglik: -4.2240e+02 - logprior: -6.7381e+00
Epoch 2/10
23/23 - 8s - loss: 416.1859 - loglik: -4.1557e+02 - logprior: -5.0034e-02
Epoch 3/10
23/23 - 8s - loss: 409.6671 - loglik: -4.0857e+02 - logprior: 0.1835
Epoch 4/10
23/23 - 8s - loss: 406.4911 - loglik: -4.0554e+02 - logprior: 0.5374
Epoch 5/10
23/23 - 8s - loss: 403.2960 - loglik: -4.0270e+02 - logprior: 0.6855
Epoch 6/10
23/23 - 8s - loss: 403.3306 - loglik: -4.0310e+02 - logprior: 0.8375
Fitted a model with MAP estimate = -401.5443
Time for alignment: 164.2732
Computed alignments with likelihoods: ['-400.6172', '-401.7076', '-401.5443']
Best model has likelihood: -400.6172
time for generating output: 0.2725
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.7153951576062129
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb60ec00fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6748f5040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4827970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4c941c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57af7f9a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb604ab6dc0>, <__main__.SimpleDirichletPrior object at 0x7fb697641340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.7545 - loglik: -2.4831e+02 - logprior: -1.1805e+00
Epoch 2/10
29/29 - 4s - loss: 223.9461 - loglik: -2.2215e+02 - logprior: -8.8687e-01
Epoch 3/10
29/29 - 4s - loss: 218.7138 - loglik: -2.1682e+02 - logprior: -8.8299e-01
Epoch 4/10
29/29 - 4s - loss: 217.2249 - loglik: -2.1544e+02 - logprior: -8.8948e-01
Epoch 5/10
29/29 - 4s - loss: 216.4379 - loglik: -2.1468e+02 - logprior: -8.8158e-01
Epoch 6/10
29/29 - 4s - loss: 216.0593 - loglik: -2.1436e+02 - logprior: -8.7179e-01
Epoch 7/10
29/29 - 4s - loss: 214.9511 - loglik: -2.1323e+02 - logprior: -8.7405e-01
Epoch 8/10
29/29 - 4s - loss: 214.8670 - loglik: -2.1310e+02 - logprior: -8.6969e-01
Epoch 9/10
29/29 - 4s - loss: 214.1145 - loglik: -2.1244e+02 - logprior: -8.7393e-01
Epoch 10/10
29/29 - 4s - loss: 213.6603 - loglik: -2.1199e+02 - logprior: -8.7113e-01
Fitted a model with MAP estimate = -204.9845
expansions: [(2, 1), (3, 1), (15, 2), (22, 1), (27, 2), (38, 2), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 231.3339 - loglik: -2.2988e+02 - logprior: -1.2346e+00
Epoch 2/2
29/29 - 4s - loss: 215.8019 - loglik: -2.1399e+02 - logprior: -8.7993e-01
Fitted a model with MAP estimate = -195.1827
expansions: [(18, 2), (59, 1)]
discards: [ 0 32 46 60]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 219.2220 - loglik: -2.1761e+02 - logprior: -1.5165e+00
Epoch 2/2
29/29 - 4s - loss: 216.1006 - loglik: -2.1488e+02 - logprior: -9.1080e-01
Fitted a model with MAP estimate = -196.1220
expansions: [(0, 5), (3, 1), (15, 1)]
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 197.3683 - loglik: -1.9650e+02 - logprior: -7.4098e-01
Epoch 2/10
42/42 - 5s - loss: 194.6514 - loglik: -1.9355e+02 - logprior: -5.5695e-01
Epoch 3/10
42/42 - 6s - loss: 193.1217 - loglik: -1.9174e+02 - logprior: -5.4845e-01
Epoch 4/10
42/42 - 6s - loss: 192.6412 - loglik: -1.9107e+02 - logprior: -5.3571e-01
Epoch 5/10
42/42 - 6s - loss: 191.3997 - loglik: -1.8980e+02 - logprior: -5.3031e-01
Epoch 6/10
42/42 - 6s - loss: 191.3645 - loglik: -1.8977e+02 - logprior: -5.2278e-01
Epoch 7/10
42/42 - 6s - loss: 190.1272 - loglik: -1.8851e+02 - logprior: -5.1759e-01
Epoch 8/10
42/42 - 6s - loss: 189.5037 - loglik: -1.8766e+02 - logprior: -5.5115e-01
Epoch 9/10
42/42 - 6s - loss: 188.9839 - loglik: -1.8697e+02 - logprior: -5.5275e-01
Epoch 10/10
42/42 - 6s - loss: 187.4975 - loglik: -1.8573e+02 - logprior: -5.5290e-01
Fitted a model with MAP estimate = -185.9890
Time for alignment: 178.4004
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 249.8365 - loglik: -2.4839e+02 - logprior: -1.1754e+00
Epoch 2/10
29/29 - 4s - loss: 223.1080 - loglik: -2.2133e+02 - logprior: -8.6471e-01
Epoch 3/10
29/29 - 4s - loss: 218.9227 - loglik: -2.1710e+02 - logprior: -8.6908e-01
Epoch 4/10
29/29 - 4s - loss: 217.4466 - loglik: -2.1568e+02 - logprior: -8.8945e-01
Epoch 5/10
29/29 - 4s - loss: 216.1847 - loglik: -2.1447e+02 - logprior: -8.7389e-01
Epoch 6/10
29/29 - 4s - loss: 215.8429 - loglik: -2.1419e+02 - logprior: -8.6947e-01
Epoch 7/10
29/29 - 4s - loss: 215.5836 - loglik: -2.1391e+02 - logprior: -8.6564e-01
Epoch 8/10
29/29 - 4s - loss: 214.4130 - loglik: -2.1267e+02 - logprior: -8.7132e-01
Epoch 9/10
29/29 - 4s - loss: 213.7392 - loglik: -2.1210e+02 - logprior: -8.7078e-01
Epoch 10/10
29/29 - 4s - loss: 214.1896 - loglik: -2.1252e+02 - logprior: -8.7136e-01
Fitted a model with MAP estimate = -204.5815
expansions: [(1, 1), (2, 1), (12, 3), (14, 3), (27, 2), (38, 2), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 230.6320 - loglik: -2.2919e+02 - logprior: -1.2305e+00
Epoch 2/2
29/29 - 4s - loss: 215.9315 - loglik: -2.1413e+02 - logprior: -8.6722e-01
Fitted a model with MAP estimate = -195.0843
expansions: [(16, 1), (62, 1)]
discards: [ 2 17 22 36 49 58 63]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 218.1490 - loglik: -2.1696e+02 - logprior: -1.0934e+00
Epoch 2/2
29/29 - 4s - loss: 214.6010 - loglik: -2.1352e+02 - logprior: -7.8563e-01
Fitted a model with MAP estimate = -195.2855
expansions: [(2, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 196.2208 - loglik: -1.9540e+02 - logprior: -7.0465e-01
Epoch 2/10
42/42 - 5s - loss: 194.8388 - loglik: -1.9379e+02 - logprior: -5.2851e-01
Epoch 3/10
42/42 - 6s - loss: 192.9820 - loglik: -1.9165e+02 - logprior: -5.2085e-01
Epoch 4/10
42/42 - 6s - loss: 192.6745 - loglik: -1.9115e+02 - logprior: -5.1302e-01
Epoch 5/10
42/42 - 6s - loss: 191.6967 - loglik: -1.9015e+02 - logprior: -5.0737e-01
Epoch 6/10
42/42 - 6s - loss: 191.2405 - loglik: -1.8969e+02 - logprior: -5.0033e-01
Epoch 7/10
42/42 - 6s - loss: 190.3488 - loglik: -1.8879e+02 - logprior: -4.9572e-01
Epoch 8/10
42/42 - 6s - loss: 189.4820 - loglik: -1.8772e+02 - logprior: -4.9581e-01
Epoch 9/10
42/42 - 6s - loss: 189.3766 - loglik: -1.8747e+02 - logprior: -4.9027e-01
Epoch 10/10
42/42 - 6s - loss: 188.0368 - loglik: -1.8637e+02 - logprior: -4.8780e-01
Fitted a model with MAP estimate = -186.2823
Time for alignment: 179.4464
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.3695 - loglik: -2.4792e+02 - logprior: -1.1810e+00
Epoch 2/10
29/29 - 4s - loss: 224.1170 - loglik: -2.2227e+02 - logprior: -8.7477e-01
Epoch 3/10
29/29 - 4s - loss: 219.5226 - loglik: -2.1757e+02 - logprior: -8.5907e-01
Epoch 4/10
29/29 - 4s - loss: 217.2550 - loglik: -2.1549e+02 - logprior: -8.8248e-01
Epoch 5/10
29/29 - 4s - loss: 216.3204 - loglik: -2.1455e+02 - logprior: -8.7375e-01
Epoch 6/10
29/29 - 4s - loss: 215.5688 - loglik: -2.1384e+02 - logprior: -8.6690e-01
Epoch 7/10
29/29 - 4s - loss: 215.0101 - loglik: -2.1327e+02 - logprior: -8.6330e-01
Epoch 8/10
29/29 - 4s - loss: 214.0880 - loglik: -2.1230e+02 - logprior: -8.6622e-01
Epoch 9/10
29/29 - 4s - loss: 213.8770 - loglik: -2.1223e+02 - logprior: -8.6901e-01
Epoch 10/10
29/29 - 4s - loss: 213.3901 - loglik: -2.1174e+02 - logprior: -8.6748e-01
Fitted a model with MAP estimate = -205.1516
expansions: [(5, 1), (14, 2), (15, 1), (17, 2), (39, 2), (41, 1), (42, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 230.8231 - loglik: -2.2940e+02 - logprior: -1.2509e+00
Epoch 2/2
29/29 - 4s - loss: 216.4174 - loglik: -2.1493e+02 - logprior: -8.8200e-01
Fitted a model with MAP estimate = -195.7177
expansions: [(13, 2), (59, 1)]
discards: [46 60]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 217.7395 - loglik: -2.1648e+02 - logprior: -1.1604e+00
Epoch 2/2
29/29 - 4s - loss: 214.4785 - loglik: -2.1338e+02 - logprior: -7.8174e-01
Fitted a model with MAP estimate = -195.1223
expansions: [(3, 1), (21, 1)]
discards: [17 18 19 57]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 196.8943 - loglik: -1.9608e+02 - logprior: -6.9996e-01
Epoch 2/10
42/42 - 6s - loss: 194.4768 - loglik: -1.9343e+02 - logprior: -5.3422e-01
Epoch 3/10
42/42 - 6s - loss: 193.3231 - loglik: -1.9199e+02 - logprior: -5.2635e-01
Epoch 4/10
42/42 - 6s - loss: 192.6472 - loglik: -1.9112e+02 - logprior: -5.1556e-01
Epoch 5/10
42/42 - 6s - loss: 191.6870 - loglik: -1.9013e+02 - logprior: -5.1267e-01
Epoch 6/10
42/42 - 5s - loss: 191.4119 - loglik: -1.8986e+02 - logprior: -5.0325e-01
Epoch 7/10
42/42 - 6s - loss: 190.5043 - loglik: -1.8892e+02 - logprior: -5.0017e-01
Epoch 8/10
42/42 - 6s - loss: 189.7253 - loglik: -1.8796e+02 - logprior: -4.9691e-01
Epoch 9/10
42/42 - 6s - loss: 189.0648 - loglik: -1.8715e+02 - logprior: -4.9706e-01
Epoch 10/10
42/42 - 6s - loss: 188.1027 - loglik: -1.8645e+02 - logprior: -4.9526e-01
Fitted a model with MAP estimate = -186.4539
Time for alignment: 177.1119
Computed alignments with likelihoods: ['-185.9890', '-186.2823', '-186.4539']
Best model has likelihood: -185.9890
time for generating output: 0.2153
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.5823035096391498
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6972addc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb69fc9c190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb69fc9c790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60ef0dca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57b08d820>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c4653430>, <__main__.SimpleDirichletPrior object at 0x7fb697063d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.4239 - loglik: -3.3047e+02 - logprior: -4.5934e+01
Epoch 2/10
10/10 - 1s - loss: 298.9841 - loglik: -2.8748e+02 - logprior: -1.1492e+01
Epoch 3/10
10/10 - 1s - loss: 251.2833 - loglik: -2.4567e+02 - logprior: -5.6001e+00
Epoch 4/10
10/10 - 1s - loss: 221.8586 - loglik: -2.1793e+02 - logprior: -3.9139e+00
Epoch 5/10
10/10 - 1s - loss: 210.3130 - loglik: -2.0691e+02 - logprior: -3.3163e+00
Epoch 6/10
10/10 - 1s - loss: 205.8803 - loglik: -2.0272e+02 - logprior: -2.8172e+00
Epoch 7/10
10/10 - 1s - loss: 203.7068 - loglik: -2.0073e+02 - logprior: -2.4515e+00
Epoch 8/10
10/10 - 1s - loss: 203.3647 - loglik: -2.0060e+02 - logprior: -2.2405e+00
Epoch 9/10
10/10 - 1s - loss: 202.5699 - loglik: -1.9999e+02 - logprior: -2.1217e+00
Epoch 10/10
10/10 - 1s - loss: 202.0121 - loglik: -1.9959e+02 - logprior: -2.0013e+00
Fitted a model with MAP estimate = -201.6162
expansions: [(10, 4), (12, 1), (13, 1), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 251.3258 - loglik: -1.9910e+02 - logprior: -5.2211e+01
Epoch 2/2
10/10 - 2s - loss: 208.0170 - loglik: -1.8686e+02 - logprior: -2.1033e+01
Fitted a model with MAP estimate = -199.7663
expansions: [(0, 3)]
discards: [  0 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.4476 - loglik: -1.8608e+02 - logprior: -4.1347e+01
Epoch 2/2
10/10 - 2s - loss: 192.6058 - loglik: -1.8221e+02 - logprior: -1.0275e+01
Fitted a model with MAP estimate = -186.8517
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.4304 - loglik: -1.8506e+02 - logprior: -5.0350e+01
Epoch 2/10
10/10 - 2s - loss: 199.4388 - loglik: -1.8296e+02 - logprior: -1.6333e+01
Epoch 3/10
10/10 - 2s - loss: 187.2060 - loglik: -1.8140e+02 - logprior: -5.4961e+00
Epoch 4/10
10/10 - 2s - loss: 181.5221 - loglik: -1.7949e+02 - logprior: -1.6086e+00
Epoch 5/10
10/10 - 2s - loss: 179.2424 - loglik: -1.7864e+02 - logprior: -1.8221e-01
Epoch 6/10
10/10 - 2s - loss: 178.3038 - loglik: -1.7845e+02 - logprior: 0.5755
Epoch 7/10
10/10 - 2s - loss: 176.9814 - loglik: -1.7778e+02 - logprior: 1.2481
Epoch 8/10
10/10 - 2s - loss: 176.8363 - loglik: -1.7816e+02 - logprior: 1.7638
Epoch 9/10
10/10 - 2s - loss: 176.3655 - loglik: -1.7803e+02 - logprior: 2.1111
Epoch 10/10
10/10 - 2s - loss: 175.8761 - loglik: -1.7780e+02 - logprior: 2.3698
Fitted a model with MAP estimate = -175.3147
Time for alignment: 54.2117
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.1691 - loglik: -3.3022e+02 - logprior: -4.5934e+01
Epoch 2/10
10/10 - 1s - loss: 299.2413 - loglik: -2.8774e+02 - logprior: -1.1486e+01
Epoch 3/10
10/10 - 1s - loss: 252.1463 - loglik: -2.4651e+02 - logprior: -5.6240e+00
Epoch 4/10
10/10 - 1s - loss: 224.8123 - loglik: -2.2099e+02 - logprior: -3.8175e+00
Epoch 5/10
10/10 - 1s - loss: 212.1211 - loglik: -2.0922e+02 - logprior: -2.8883e+00
Epoch 6/10
10/10 - 1s - loss: 206.4049 - loglik: -2.0374e+02 - logprior: -2.5199e+00
Epoch 7/10
10/10 - 1s - loss: 203.7292 - loglik: -2.0098e+02 - logprior: -2.3632e+00
Epoch 8/10
10/10 - 1s - loss: 202.9443 - loglik: -2.0022e+02 - logprior: -2.2539e+00
Epoch 9/10
10/10 - 1s - loss: 202.2307 - loglik: -1.9965e+02 - logprior: -2.1555e+00
Epoch 10/10
10/10 - 1s - loss: 202.0530 - loglik: -1.9961e+02 - logprior: -2.0750e+00
Fitted a model with MAP estimate = -201.2941
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.2235 - loglik: -1.9993e+02 - logprior: -5.2266e+01
Epoch 2/2
10/10 - 2s - loss: 208.5152 - loglik: -1.8704e+02 - logprior: -2.1295e+01
Fitted a model with MAP estimate = -199.8983
expansions: [(0, 3)]
discards: [  0  10  20 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.4120 - loglik: -1.8600e+02 - logprior: -4.1391e+01
Epoch 2/2
10/10 - 2s - loss: 192.4784 - loglik: -1.8197e+02 - logprior: -1.0334e+01
Fitted a model with MAP estimate = -186.7310
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.1425 - loglik: -1.8492e+02 - logprior: -5.0200e+01
Epoch 2/10
10/10 - 2s - loss: 198.6662 - loglik: -1.8281e+02 - logprior: -1.5706e+01
Epoch 3/10
10/10 - 2s - loss: 186.3481 - loglik: -1.8079e+02 - logprior: -5.2111e+00
Epoch 4/10
10/10 - 2s - loss: 181.5079 - loglik: -1.7945e+02 - logprior: -1.6186e+00
Epoch 5/10
10/10 - 2s - loss: 179.1159 - loglik: -1.7851e+02 - logprior: -1.9878e-01
Epoch 6/10
10/10 - 2s - loss: 178.0830 - loglik: -1.7823e+02 - logprior: 0.5521
Epoch 7/10
10/10 - 2s - loss: 177.3701 - loglik: -1.7814e+02 - logprior: 1.1881
Epoch 8/10
10/10 - 2s - loss: 176.7001 - loglik: -1.7796e+02 - logprior: 1.7000
Epoch 9/10
10/10 - 2s - loss: 176.5384 - loglik: -1.7814e+02 - logprior: 2.0487
Epoch 10/10
10/10 - 2s - loss: 175.9589 - loglik: -1.7782e+02 - logprior: 2.3066
Fitted a model with MAP estimate = -175.3767
Time for alignment: 53.2024
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 376.2348 - loglik: -3.3028e+02 - logprior: -4.5932e+01
Epoch 2/10
10/10 - 1s - loss: 299.0722 - loglik: -2.8757e+02 - logprior: -1.1485e+01
Epoch 3/10
10/10 - 1s - loss: 251.3103 - loglik: -2.4566e+02 - logprior: -5.6322e+00
Epoch 4/10
10/10 - 1s - loss: 221.7786 - loglik: -2.1780e+02 - logprior: -3.9649e+00
Epoch 5/10
10/10 - 1s - loss: 209.9649 - loglik: -2.0664e+02 - logprior: -3.2872e+00
Epoch 6/10
10/10 - 1s - loss: 205.8775 - loglik: -2.0278e+02 - logprior: -2.8337e+00
Epoch 7/10
10/10 - 1s - loss: 203.6994 - loglik: -2.0066e+02 - logprior: -2.5447e+00
Epoch 8/10
10/10 - 1s - loss: 202.9987 - loglik: -2.0010e+02 - logprior: -2.3667e+00
Epoch 9/10
10/10 - 1s - loss: 202.4254 - loglik: -1.9976e+02 - logprior: -2.1937e+00
Epoch 10/10
10/10 - 1s - loss: 201.9076 - loglik: -1.9939e+02 - logprior: -2.1022e+00
Fitted a model with MAP estimate = -201.3612
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.5030 - loglik: -1.9923e+02 - logprior: -5.2250e+01
Epoch 2/2
10/10 - 2s - loss: 207.9095 - loglik: -1.8658e+02 - logprior: -2.1170e+01
Fitted a model with MAP estimate = -200.0110
expansions: [(0, 3)]
discards: [  0  10 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.4175 - loglik: -1.8601e+02 - logprior: -4.1381e+01
Epoch 2/2
10/10 - 2s - loss: 193.0383 - loglik: -1.8258e+02 - logprior: -1.0301e+01
Fitted a model with MAP estimate = -186.9294
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.4148 - loglik: -1.8516e+02 - logprior: -5.0228e+01
Epoch 2/10
10/10 - 2s - loss: 199.1270 - loglik: -1.8308e+02 - logprior: -1.5890e+01
Epoch 3/10
10/10 - 2s - loss: 186.7053 - loglik: -1.8108e+02 - logprior: -5.2889e+00
Epoch 4/10
10/10 - 2s - loss: 181.8474 - loglik: -1.7979e+02 - logprior: -1.6078e+00
Epoch 5/10
10/10 - 2s - loss: 178.9927 - loglik: -1.7837e+02 - logprior: -1.6270e-01
Epoch 6/10
10/10 - 2s - loss: 178.1133 - loglik: -1.7828e+02 - logprior: 0.6098
Epoch 7/10
10/10 - 2s - loss: 177.6095 - loglik: -1.7841e+02 - logprior: 1.2427
Epoch 8/10
10/10 - 2s - loss: 177.0393 - loglik: -1.7836e+02 - logprior: 1.7685
Epoch 9/10
10/10 - 2s - loss: 176.5569 - loglik: -1.7823e+02 - logprior: 2.1206
Epoch 10/10
10/10 - 2s - loss: 176.2146 - loglik: -1.7813e+02 - logprior: 2.3701
Fitted a model with MAP estimate = -175.5006
Time for alignment: 53.9820
Computed alignments with likelihoods: ['-175.3147', '-175.3767', '-175.5006']
Best model has likelihood: -175.3147
time for generating output: 0.1855
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9280026727546077
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6973b8fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60625bb80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60625b7f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4170c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4170550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57a05ea90>, <__main__.SimpleDirichletPrior object at 0x7fb579e523d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 630.4067 - loglik: -6.0703e+02 - logprior: -2.3330e+01
Epoch 2/10
14/14 - 4s - loss: 541.1039 - loglik: -5.3814e+02 - logprior: -2.8498e+00
Epoch 3/10
14/14 - 4s - loss: 479.9777 - loglik: -4.7777e+02 - logprior: -1.9922e+00
Epoch 4/10
14/14 - 4s - loss: 460.6719 - loglik: -4.5801e+02 - logprior: -1.9900e+00
Epoch 5/10
14/14 - 4s - loss: 455.9086 - loglik: -4.5326e+02 - logprior: -1.7786e+00
Epoch 6/10
14/14 - 4s - loss: 452.9180 - loglik: -4.5051e+02 - logprior: -1.5869e+00
Epoch 7/10
14/14 - 4s - loss: 454.4607 - loglik: -4.5220e+02 - logprior: -1.4764e+00
Fitted a model with MAP estimate = -451.9221
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 1), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 4), (62, 1), (80, 1), (90, 1), (106, 2), (112, 1), (113, 3), (114, 2), (115, 2), (118, 1), (119, 1), (120, 3), (130, 2), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 483.6974 - loglik: -4.5580e+02 - logprior: -2.7872e+01
Epoch 2/2
14/14 - 5s - loss: 448.8717 - loglik: -4.3986e+02 - logprior: -8.8923e+00
Fitted a model with MAP estimate = -442.9322
expansions: [(36, 1), (70, 2)]
discards: [  0  18  48  50  73 129 154 168 208]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 470.5011 - loglik: -4.4362e+02 - logprior: -2.6844e+01
Epoch 2/2
14/14 - 5s - loss: 445.0523 - loglik: -4.3753e+02 - logprior: -7.2715e+00
Fitted a model with MAP estimate = -438.7043
expansions: [(0, 4)]
discards: [ 0 72 73 74]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 460.6236 - loglik: -4.4109e+02 - logprior: -1.9483e+01
Epoch 2/10
14/14 - 5s - loss: 437.2220 - loglik: -4.3616e+02 - logprior: -7.2771e-01
Epoch 3/10
14/14 - 5s - loss: 432.8574 - loglik: -4.3437e+02 - logprior: 2.1966
Epoch 4/10
14/14 - 5s - loss: 429.6960 - loglik: -4.3221e+02 - logprior: 3.3323
Epoch 5/10
14/14 - 5s - loss: 428.7486 - loglik: -4.3184e+02 - logprior: 3.8611
Epoch 6/10
14/14 - 5s - loss: 427.3780 - loglik: -4.3084e+02 - logprior: 4.1814
Epoch 7/10
14/14 - 5s - loss: 427.3883 - loglik: -4.3121e+02 - logprior: 4.4922
Fitted a model with MAP estimate = -425.9903
Time for alignment: 117.4268
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 631.3632 - loglik: -6.0799e+02 - logprior: -2.3322e+01
Epoch 2/10
14/14 - 4s - loss: 542.7169 - loglik: -5.3980e+02 - logprior: -2.8067e+00
Epoch 3/10
14/14 - 4s - loss: 482.3344 - loglik: -4.8050e+02 - logprior: -1.6862e+00
Epoch 4/10
14/14 - 4s - loss: 461.1391 - loglik: -4.5892e+02 - logprior: -1.6609e+00
Epoch 5/10
14/14 - 4s - loss: 455.9566 - loglik: -4.5362e+02 - logprior: -1.5137e+00
Epoch 6/10
14/14 - 4s - loss: 454.0017 - loglik: -4.5183e+02 - logprior: -1.3862e+00
Epoch 7/10
14/14 - 4s - loss: 451.9854 - loglik: -4.4991e+02 - logprior: -1.3209e+00
Epoch 8/10
14/14 - 4s - loss: 450.6920 - loglik: -4.4868e+02 - logprior: -1.2583e+00
Epoch 9/10
14/14 - 4s - loss: 449.9760 - loglik: -4.4804e+02 - logprior: -1.2019e+00
Epoch 10/10
14/14 - 4s - loss: 450.9986 - loglik: -4.4914e+02 - logprior: -1.1694e+00
Fitted a model with MAP estimate = -449.4688
expansions: [(14, 1), (15, 1), (28, 2), (29, 2), (30, 2), (31, 1), (39, 2), (40, 2), (41, 1), (50, 1), (51, 1), (52, 1), (53, 3), (90, 1), (110, 1), (112, 1), (114, 2), (115, 4), (118, 1), (119, 1), (120, 3), (130, 2), (154, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 485.0085 - loglik: -4.5695e+02 - logprior: -2.8009e+01
Epoch 2/2
14/14 - 5s - loss: 448.1773 - loglik: -4.3895e+02 - logprior: -8.8838e+00
Fitted a model with MAP estimate = -442.8701
expansions: [(0, 3), (142, 1)]
discards: [  0  50 150 164]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 459.7931 - loglik: -4.3968e+02 - logprior: -2.0059e+01
Epoch 2/2
14/14 - 5s - loss: 435.2566 - loglik: -4.3376e+02 - logprior: -1.1893e+00
Fitted a model with MAP estimate = -431.8090
expansions: [(71, 2)]
discards: [  0   1  73  74 204]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 466.3435 - loglik: -4.3998e+02 - logprior: -2.6307e+01
Epoch 2/10
14/14 - 5s - loss: 443.1291 - loglik: -4.3630e+02 - logprior: -6.5161e+00
Epoch 3/10
14/14 - 5s - loss: 432.0103 - loglik: -4.3207e+02 - logprior: 0.6695
Epoch 4/10
14/14 - 5s - loss: 429.1718 - loglik: -4.3153e+02 - logprior: 3.1114
Epoch 5/10
14/14 - 5s - loss: 427.8924 - loglik: -4.3095e+02 - logprior: 3.8164
Epoch 6/10
14/14 - 5s - loss: 426.1855 - loglik: -4.2970e+02 - logprior: 4.2392
Epoch 7/10
14/14 - 5s - loss: 425.3891 - loglik: -4.2917e+02 - logprior: 4.4612
Epoch 8/10
14/14 - 5s - loss: 425.1643 - loglik: -4.2927e+02 - logprior: 4.7565
Epoch 9/10
14/14 - 5s - loss: 423.8289 - loglik: -4.2827e+02 - logprior: 5.0824
Epoch 10/10
14/14 - 5s - loss: 424.5817 - loglik: -4.2930e+02 - logprior: 5.3626
Fitted a model with MAP estimate = -422.9689
Time for alignment: 145.3607
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 630.4902 - loglik: -6.0712e+02 - logprior: -2.3320e+01
Epoch 2/10
14/14 - 4s - loss: 541.9406 - loglik: -5.3903e+02 - logprior: -2.7964e+00
Epoch 3/10
14/14 - 4s - loss: 480.1313 - loglik: -4.7819e+02 - logprior: -1.7736e+00
Epoch 4/10
14/14 - 4s - loss: 460.4707 - loglik: -4.5792e+02 - logprior: -1.9453e+00
Epoch 5/10
14/14 - 4s - loss: 455.6404 - loglik: -4.5296e+02 - logprior: -1.8139e+00
Epoch 6/10
14/14 - 4s - loss: 453.0082 - loglik: -4.5050e+02 - logprior: -1.6764e+00
Epoch 7/10
14/14 - 4s - loss: 452.4067 - loglik: -4.5002e+02 - logprior: -1.5951e+00
Epoch 8/10
14/14 - 4s - loss: 453.0758 - loglik: -4.5080e+02 - logprior: -1.4943e+00
Fitted a model with MAP estimate = -451.0398
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 1), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (55, 4), (63, 1), (80, 1), (90, 6), (92, 3), (112, 1), (114, 2), (115, 2), (116, 2), (118, 2), (119, 1), (120, 1), (121, 1), (125, 1), (161, 1), (164, 1), (166, 2), (167, 8)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 484.8976 - loglik: -4.5687e+02 - logprior: -2.8001e+01
Epoch 2/2
14/14 - 5s - loss: 445.5171 - loglik: -4.3627e+02 - logprior: -9.1113e+00
Fitted a model with MAP estimate = -439.7952
expansions: [(36, 1), (69, 2), (118, 1)]
discards: [  0  18  50  73  74  75 111 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 468.6028 - loglik: -4.4157e+02 - logprior: -2.6985e+01
Epoch 2/2
14/14 - 5s - loss: 445.5688 - loglik: -4.3763e+02 - logprior: -7.6121e+00
Fitted a model with MAP estimate = -437.3688
expansions: [(0, 4), (76, 1), (77, 2)]
discards: [  0  73 106 107 153]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 459.4544 - loglik: -4.3976e+02 - logprior: -1.9638e+01
Epoch 2/10
14/14 - 5s - loss: 433.9568 - loglik: -4.3265e+02 - logprior: -9.9913e-01
Epoch 3/10
14/14 - 5s - loss: 429.2962 - loglik: -4.3060e+02 - logprior: 1.9235
Epoch 4/10
14/14 - 5s - loss: 427.0639 - loglik: -4.2938e+02 - logprior: 3.0953
Epoch 5/10
14/14 - 5s - loss: 425.7031 - loglik: -4.2854e+02 - logprior: 3.6168
Epoch 6/10
14/14 - 5s - loss: 424.4012 - loglik: -4.2756e+02 - logprior: 3.8897
Epoch 7/10
14/14 - 5s - loss: 423.2224 - loglik: -4.2670e+02 - logprior: 4.1657
Epoch 8/10
14/14 - 5s - loss: 422.4121 - loglik: -4.2618e+02 - logprior: 4.4412
Epoch 9/10
14/14 - 5s - loss: 422.4988 - loglik: -4.2657e+02 - logprior: 4.7305
Fitted a model with MAP estimate = -421.1880
Time for alignment: 132.9292
Computed alignments with likelihoods: ['-425.9903', '-422.9689', '-421.1880']
Best model has likelihood: -421.1880
time for generating output: 0.3010
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.9006908462867013
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb604a6d340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb63010fe20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb63010f250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6976c91c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb67d8a14c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb67490e520>, <__main__.SimpleDirichletPrior object at 0x7fb6307feaf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 739.8375 - loglik: -6.9731e+02 - logprior: -4.2490e+01
Epoch 2/10
11/11 - 10s - loss: 633.9611 - loglik: -6.2909e+02 - logprior: -4.7518e+00
Epoch 3/10
11/11 - 9s - loss: 542.5909 - loglik: -5.4194e+02 - logprior: -5.9601e-01
Epoch 4/10
11/11 - 10s - loss: 492.8800 - loglik: -4.9234e+02 - logprior: -4.5121e-01
Epoch 5/10
11/11 - 9s - loss: 475.8936 - loglik: -4.7492e+02 - logprior: -6.6559e-01
Epoch 6/10
11/11 - 10s - loss: 469.5464 - loglik: -4.6865e+02 - logprior: -3.5354e-01
Epoch 7/10
11/11 - 9s - loss: 466.4412 - loglik: -4.6599e+02 - logprior: 0.0675
Epoch 8/10
11/11 - 10s - loss: 465.4632 - loglik: -4.6523e+02 - logprior: 0.1817
Epoch 9/10
11/11 - 9s - loss: 464.3733 - loglik: -4.6427e+02 - logprior: 0.2721
Epoch 10/10
11/11 - 9s - loss: 461.8925 - loglik: -4.6198e+02 - logprior: 0.4884
Fitted a model with MAP estimate = -461.5229
expansions: [(22, 4), (29, 1), (48, 1), (50, 1), (52, 2), (64, 4), (65, 1), (78, 1), (79, 1), (81, 1), (91, 1), (93, 1), (103, 5), (104, 1), (105, 1), (106, 2), (129, 1), (134, 1), (137, 1), (150, 1), (151, 1), (160, 1), (162, 5), (179, 1), (180, 1), (181, 1), (182, 2), (198, 6), (200, 2), (201, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0   1 208]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 513.6181 - loglik: -4.6402e+02 - logprior: -4.9568e+01
Epoch 2/2
11/11 - 13s - loss: 457.1140 - loglik: -4.4192e+02 - logprior: -1.4979e+01
Fitted a model with MAP estimate = -443.6846
expansions: [(0, 3), (243, 1)]
discards: [  0  58  74 123 124 125 159]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 474.8217 - loglik: -4.3769e+02 - logprior: -3.7109e+01
Epoch 2/2
11/11 - 12s - loss: 432.5330 - loglik: -4.3004e+02 - logprior: -2.3444e+00
Fitted a model with MAP estimate = -426.2616
expansions: [(22, 2), (245, 1)]
discards: [  0  75 128]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 481.2618 - loglik: -4.3424e+02 - logprior: -4.7008e+01
Epoch 2/10
11/11 - 12s - loss: 443.3123 - loglik: -4.3203e+02 - logprior: -1.1190e+01
Epoch 3/10
11/11 - 13s - loss: 427.6718 - loglik: -4.2858e+02 - logprior: 1.1875
Epoch 4/10
11/11 - 12s - loss: 418.3190 - loglik: -4.2558e+02 - logprior: 7.7583
Epoch 5/10
11/11 - 12s - loss: 415.3591 - loglik: -4.2463e+02 - logprior: 9.7846
Epoch 6/10
11/11 - 13s - loss: 410.3152 - loglik: -4.2069e+02 - logprior: 10.7985
Epoch 7/10
11/11 - 12s - loss: 410.9872 - loglik: -4.2218e+02 - logprior: 11.5971
Fitted a model with MAP estimate = -410.2802
Time for alignment: 270.8989
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 740.3909 - loglik: -6.9787e+02 - logprior: -4.2489e+01
Epoch 2/10
11/11 - 10s - loss: 630.5772 - loglik: -6.2573e+02 - logprior: -4.7277e+00
Epoch 3/10
11/11 - 9s - loss: 545.3340 - loglik: -5.4474e+02 - logprior: -5.3148e-01
Epoch 4/10
11/11 - 10s - loss: 493.3575 - loglik: -4.9270e+02 - logprior: -5.3921e-01
Epoch 5/10
11/11 - 11s - loss: 480.9551 - loglik: -4.7969e+02 - logprior: -8.7325e-01
Epoch 6/10
11/11 - 9s - loss: 470.7908 - loglik: -4.6981e+02 - logprior: -4.2276e-01
Epoch 7/10
11/11 - 9s - loss: 470.7803 - loglik: -4.7017e+02 - logprior: -1.3140e-01
Epoch 8/10
11/11 - 9s - loss: 467.8121 - loglik: -4.6752e+02 - logprior: 0.1076
Epoch 9/10
11/11 - 9s - loss: 467.6104 - loglik: -4.6759e+02 - logprior: 0.3684
Epoch 10/10
11/11 - 10s - loss: 466.0349 - loglik: -4.6623e+02 - logprior: 0.5889
Fitted a model with MAP estimate = -465.7403
expansions: [(19, 5), (21, 1), (22, 1), (24, 1), (36, 1), (47, 1), (49, 1), (51, 2), (52, 1), (62, 1), (64, 1), (77, 1), (78, 1), (79, 1), (90, 1), (92, 1), (102, 3), (103, 1), (104, 1), (106, 1), (129, 1), (159, 1), (160, 2), (168, 2), (181, 1), (182, 3), (198, 4), (201, 2), (202, 6), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0   1 211]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 17s - loss: 520.8803 - loglik: -4.7113e+02 - logprior: -4.9716e+01
Epoch 2/2
11/11 - 12s - loss: 462.7708 - loglik: -4.4726e+02 - logprior: -1.5302e+01
Fitted a model with MAP estimate = -452.6055
expansions: [(0, 3), (214, 1), (238, 3)]
discards: [  0  20  21 122 123]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 481.0053 - loglik: -4.4393e+02 - logprior: -3.7053e+01
Epoch 2/2
11/11 - 13s - loss: 441.4497 - loglik: -4.3910e+02 - logprior: -2.1975e+00
Fitted a model with MAP estimate = -433.0089
expansions: [(250, 1)]
discards: [ 0 61]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 18s - loss: 489.5004 - loglik: -4.4271e+02 - logprior: -4.6771e+01
Epoch 2/10
11/11 - 13s - loss: 448.5612 - loglik: -4.3702e+02 - logprior: -1.1447e+01
Epoch 3/10
11/11 - 13s - loss: 433.1059 - loglik: -4.3365e+02 - logprior: 0.8304
Epoch 4/10
11/11 - 13s - loss: 425.2134 - loglik: -4.3229e+02 - logprior: 7.5929
Epoch 5/10
11/11 - 12s - loss: 423.8065 - loglik: -4.3295e+02 - logprior: 9.6996
Epoch 6/10
11/11 - 13s - loss: 419.8967 - loglik: -4.3010e+02 - logprior: 10.6997
Epoch 7/10
11/11 - 12s - loss: 419.1993 - loglik: -4.3007e+02 - logprior: 11.3568
Epoch 8/10
11/11 - 12s - loss: 416.2296 - loglik: -4.2799e+02 - logprior: 12.2580
Epoch 9/10
11/11 - 13s - loss: 417.8862 - loglik: -4.3020e+02 - logprior: 12.8046
Fitted a model with MAP estimate = -415.9324
Time for alignment: 296.3216
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 741.1957 - loglik: -6.9866e+02 - logprior: -4.2499e+01
Epoch 2/10
11/11 - 10s - loss: 634.4072 - loglik: -6.2958e+02 - logprior: -4.6937e+00
Epoch 3/10
11/11 - 10s - loss: 546.5967 - loglik: -5.4624e+02 - logprior: -2.8892e-01
Epoch 4/10
11/11 - 10s - loss: 491.1497 - loglik: -4.9088e+02 - logprior: -1.5097e-01
Epoch 5/10
11/11 - 10s - loss: 476.2205 - loglik: -4.7545e+02 - logprior: -3.7878e-01
Epoch 6/10
11/11 - 10s - loss: 469.1196 - loglik: -4.6863e+02 - logprior: 0.0718
Epoch 7/10
11/11 - 10s - loss: 466.0587 - loglik: -4.6610e+02 - logprior: 0.5357
Epoch 8/10
11/11 - 10s - loss: 465.4711 - loglik: -4.6590e+02 - logprior: 0.8453
Epoch 9/10
11/11 - 10s - loss: 464.4052 - loglik: -4.6513e+02 - logprior: 1.1271
Epoch 10/10
11/11 - 10s - loss: 461.1125 - loglik: -4.6208e+02 - logprior: 1.4000
Fitted a model with MAP estimate = -462.0324
expansions: [(22, 4), (33, 1), (36, 1), (47, 1), (49, 1), (51, 2), (60, 1), (62, 4), (63, 2), (75, 2), (76, 1), (77, 1), (88, 1), (101, 3), (102, 1), (103, 1), (107, 1), (159, 1), (161, 1), (162, 5), (168, 2), (182, 4), (198, 6), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0   1 209]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 516.4029 - loglik: -4.6665e+02 - logprior: -4.9719e+01
Epoch 2/2
11/11 - 12s - loss: 458.6467 - loglik: -4.4311e+02 - logprior: -1.5324e+01
Fitted a model with MAP estimate = -447.5183
expansions: [(0, 3), (218, 1), (239, 1), (245, 1)]
discards: [  0  58  74  90 122 123 193 194]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 17s - loss: 477.8020 - loglik: -4.4049e+02 - logprior: -3.7289e+01
Epoch 2/2
11/11 - 12s - loss: 437.6667 - loglik: -4.3525e+02 - logprior: -2.2702e+00
Fitted a model with MAP estimate = -428.9192
expansions: [(22, 2)]
discards: [ 0 75]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 483.4090 - loglik: -4.3667e+02 - logprior: -4.6727e+01
Epoch 2/10
11/11 - 14s - loss: 445.3818 - loglik: -4.3393e+02 - logprior: -1.1366e+01
Epoch 3/10
11/11 - 12s - loss: 430.6938 - loglik: -4.3131e+02 - logprior: 0.8804
Epoch 4/10
11/11 - 12s - loss: 419.6467 - loglik: -4.2677e+02 - logprior: 7.6183
Epoch 5/10
11/11 - 12s - loss: 416.7041 - loglik: -4.2585e+02 - logprior: 9.6728
Epoch 6/10
11/11 - 13s - loss: 415.6566 - loglik: -4.2589e+02 - logprior: 10.6849
Epoch 7/10
11/11 - 12s - loss: 415.4312 - loglik: -4.2652e+02 - logprior: 11.5194
Epoch 8/10
11/11 - 13s - loss: 413.3443 - loglik: -4.2513e+02 - logprior: 12.2277
Epoch 9/10
11/11 - 12s - loss: 410.3164 - loglik: -4.2274e+02 - logprior: 12.8764
Epoch 10/10
11/11 - 13s - loss: 414.8402 - loglik: -4.2780e+02 - logprior: 13.4047
Fitted a model with MAP estimate = -410.8860
Time for alignment: 309.3021
Computed alignments with likelihoods: ['-410.2802', '-415.9324', '-410.8860']
Best model has likelihood: -410.2802
time for generating output: 0.5586
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9162612925642808
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb674a03610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60300f040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb605588490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb641e35550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb686fabfa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb630f36dc0>, <__main__.SimpleDirichletPrior object at 0x7fb69fc9cc40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 23s - loss: 815.7562 - loglik: -8.0910e+02 - logprior: -6.5196e+00
Epoch 2/10
21/21 - 17s - loss: 691.8226 - loglik: -6.8989e+02 - logprior: -1.3293e+00
Epoch 3/10
21/21 - 17s - loss: 640.9520 - loglik: -6.3650e+02 - logprior: -3.5055e+00
Epoch 4/10
21/21 - 18s - loss: 634.4106 - loglik: -6.2989e+02 - logprior: -3.4235e+00
Epoch 5/10
21/21 - 17s - loss: 633.2698 - loglik: -6.2904e+02 - logprior: -3.2585e+00
Epoch 6/10
21/21 - 17s - loss: 628.8727 - loglik: -6.2470e+02 - logprior: -3.3104e+00
Epoch 7/10
21/21 - 17s - loss: 630.9628 - loglik: -6.2679e+02 - logprior: -3.3669e+00
Fitted a model with MAP estimate = -628.7512
expansions: [(13, 1), (14, 2), (15, 1), (53, 2), (55, 3), (56, 2), (61, 2), (62, 2), (63, 1), (64, 2), (76, 1), (77, 1), (78, 3), (79, 2), (80, 2), (81, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (112, 1), (114, 1), (116, 1), (134, 1), (135, 1), (138, 1), (144, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (212, 1), (214, 1), (219, 1), (228, 1), (229, 2), (230, 3), (231, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 1), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 644.9730 - loglik: -6.3540e+02 - logprior: -9.4557e+00
Epoch 2/2
21/21 - 25s - loss: 608.6281 - loglik: -6.0553e+02 - logprior: -2.7295e+00
Fitted a model with MAP estimate = -603.0654
expansions: [(0, 3)]
discards: [  0 102 297]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 614.5454 - loglik: -6.0913e+02 - logprior: -5.3336e+00
Epoch 2/2
21/21 - 26s - loss: 597.5101 - loglik: -5.9828e+02 - logprior: 1.1225
Fitted a model with MAP estimate = -596.1065
expansions: []
discards: [ 1 58]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 30s - loss: 612.5208 - loglik: -6.0754e+02 - logprior: -4.8787e+00
Epoch 2/10
21/21 - 25s - loss: 597.8533 - loglik: -5.9867e+02 - logprior: 1.3273
Epoch 3/10
21/21 - 25s - loss: 595.0743 - loglik: -5.9623e+02 - logprior: 2.0613
Epoch 4/10
21/21 - 25s - loss: 593.1624 - loglik: -5.9452e+02 - logprior: 2.4447
Epoch 5/10
21/21 - 25s - loss: 593.1683 - loglik: -5.9474e+02 - logprior: 2.6144
Fitted a model with MAP estimate = -589.5018
Time for alignment: 431.9215
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 818.0981 - loglik: -8.1145e+02 - logprior: -6.5155e+00
Epoch 2/10
21/21 - 18s - loss: 692.8609 - loglik: -6.9086e+02 - logprior: -1.3787e+00
Epoch 3/10
21/21 - 17s - loss: 641.1900 - loglik: -6.3635e+02 - logprior: -3.5536e+00
Epoch 4/10
21/21 - 17s - loss: 636.1721 - loglik: -6.3132e+02 - logprior: -3.5233e+00
Epoch 5/10
21/21 - 17s - loss: 629.4017 - loglik: -6.2492e+02 - logprior: -3.4539e+00
Epoch 6/10
21/21 - 17s - loss: 629.7536 - loglik: -6.2526e+02 - logprior: -3.6037e+00
Fitted a model with MAP estimate = -627.7857
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (55, 1), (56, 1), (61, 2), (62, 2), (64, 2), (74, 1), (75, 1), (76, 1), (78, 1), (80, 1), (81, 2), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (107, 1), (113, 1), (115, 1), (117, 1), (135, 1), (136, 1), (139, 1), (145, 1), (155, 1), (156, 1), (159, 1), (160, 1), (163, 1), (164, 1), (173, 1), (183, 2), (186, 1), (192, 3), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (209, 1), (210, 1), (212, 1), (213, 1), (219, 1), (224, 1), (228, 1), (229, 1), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 641.2579 - loglik: -6.3168e+02 - logprior: -9.4766e+00
Epoch 2/2
21/21 - 25s - loss: 606.1216 - loglik: -6.0302e+02 - logprior: -2.7426e+00
Fitted a model with MAP estimate = -602.0737
expansions: [(0, 3), (74, 1)]
discards: [  0 333]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 30s - loss: 612.5518 - loglik: -6.0700e+02 - logprior: -5.4754e+00
Epoch 2/2
21/21 - 25s - loss: 603.0376 - loglik: -6.0353e+02 - logprior: 0.9598
Fitted a model with MAP estimate = -595.8247
expansions: [(57, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 28s - loss: 612.8214 - loglik: -6.0773e+02 - logprior: -4.9831e+00
Epoch 2/10
21/21 - 25s - loss: 598.1960 - loglik: -5.9916e+02 - logprior: 1.4923
Epoch 3/10
21/21 - 25s - loss: 595.6127 - loglik: -5.9666e+02 - logprior: 2.0051
Epoch 4/10
21/21 - 25s - loss: 592.5468 - loglik: -5.9381e+02 - logprior: 2.3966
Epoch 5/10
21/21 - 25s - loss: 591.2340 - loglik: -5.9277e+02 - logprior: 2.5969
Epoch 6/10
21/21 - 25s - loss: 591.5106 - loglik: -5.9342e+02 - logprior: 2.8468
Fitted a model with MAP estimate = -588.8792
Time for alignment: 435.6159
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 22s - loss: 814.1232 - loglik: -8.0745e+02 - logprior: -6.5344e+00
Epoch 2/10
21/21 - 17s - loss: 698.4159 - loglik: -6.9647e+02 - logprior: -1.3320e+00
Epoch 3/10
21/21 - 17s - loss: 643.3228 - loglik: -6.3852e+02 - logprior: -3.8393e+00
Epoch 4/10
21/21 - 17s - loss: 631.8070 - loglik: -6.2682e+02 - logprior: -3.9073e+00
Epoch 5/10
21/21 - 18s - loss: 628.5193 - loglik: -6.2381e+02 - logprior: -3.7538e+00
Epoch 6/10
21/21 - 17s - loss: 629.5223 - loglik: -6.2493e+02 - logprior: -3.7374e+00
Fitted a model with MAP estimate = -626.7322
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (55, 1), (56, 1), (58, 1), (60, 2), (61, 1), (62, 1), (64, 3), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (111, 1), (113, 1), (115, 1), (117, 1), (128, 1), (136, 1), (138, 1), (139, 1), (144, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 3), (158, 1), (159, 1), (160, 1), (169, 1), (178, 2), (181, 1), (188, 1), (189, 1), (190, 2), (191, 1), (192, 1), (193, 1), (194, 1), (196, 1), (207, 1), (210, 1), (217, 1), (222, 1), (226, 1), (228, 1), (229, 1), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 639.6745 - loglik: -6.3019e+02 - logprior: -9.3666e+00
Epoch 2/2
21/21 - 25s - loss: 611.7858 - loglik: -6.0896e+02 - logprior: -2.4579e+00
Fitted a model with MAP estimate = -602.3232
expansions: [(0, 3)]
discards: [  0 205 334]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 612.8890 - loglik: -6.0743e+02 - logprior: -5.3825e+00
Epoch 2/2
21/21 - 25s - loss: 600.5507 - loglik: -6.0099e+02 - logprior: 0.8599
Fitted a model with MAP estimate = -595.5764
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 612.6548 - loglik: -6.0738e+02 - logprior: -5.1753e+00
Epoch 2/10
21/21 - 25s - loss: 598.0536 - loglik: -5.9904e+02 - logprior: 1.5204
Epoch 3/10
21/21 - 25s - loss: 595.9453 - loglik: -5.9698e+02 - logprior: 2.0209
Epoch 4/10
21/21 - 25s - loss: 591.3971 - loglik: -5.9265e+02 - logprior: 2.4086
Epoch 5/10
21/21 - 25s - loss: 592.0319 - loglik: -5.9356e+02 - logprior: 2.5920
Fitted a model with MAP estimate = -589.6018
Time for alignment: 410.8752
Computed alignments with likelihoods: ['-589.5018', '-588.8792', '-589.6018']
Best model has likelihood: -588.8792
time for generating output: 0.4196
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9640522875816994
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb630e8b040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4a7cfd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6057a7550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60e8c9af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60e8c98b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69fc9cc10>, <__main__.SimpleDirichletPrior object at 0x7fb65ba63d00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 837.9378 - loglik: -8.3572e+02 - logprior: -1.8562e+00
Epoch 2/10
39/39 - 24s - loss: 737.6185 - loglik: -7.3502e+02 - logprior: -1.6623e+00
Epoch 3/10
39/39 - 24s - loss: 721.3029 - loglik: -7.1772e+02 - logprior: -1.9111e+00
Epoch 4/10
39/39 - 24s - loss: 716.4124 - loglik: -7.1260e+02 - logprior: -1.9536e+00
Epoch 5/10
39/39 - 24s - loss: 714.1763 - loglik: -7.1043e+02 - logprior: -1.9580e+00
Epoch 6/10
39/39 - 24s - loss: 712.6924 - loglik: -7.0912e+02 - logprior: -1.9642e+00
Epoch 7/10
39/39 - 24s - loss: 711.5883 - loglik: -7.0811e+02 - logprior: -1.9803e+00
Epoch 8/10
39/39 - 24s - loss: 710.6949 - loglik: -7.0734e+02 - logprior: -1.9879e+00
Epoch 9/10
39/39 - 24s - loss: 709.8270 - loglik: -7.0655e+02 - logprior: -2.0204e+00
Epoch 10/10
39/39 - 24s - loss: 709.4108 - loglik: -7.0622e+02 - logprior: -2.0348e+00
Fitted a model with MAP estimate = -613.0118
expansions: [(14, 1), (30, 1), (41, 1), (56, 1), (76, 1), (80, 2), (81, 4), (82, 2), (93, 4), (94, 2), (99, 2), (102, 1), (118, 1), (119, 1), (120, 1), (121, 3), (122, 1), (147, 1), (159, 6), (167, 6), (168, 2), (169, 3), (170, 2), (172, 1), (173, 1), (179, 3), (180, 1), (185, 1), (186, 1), (187, 2), (189, 3), (190, 1), (206, 1), (212, 4), (213, 1), (244, 3)]
discards: [  0 103 104 105 106 107 108 148 149 150 151 152]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 737.7706 - loglik: -7.3420e+02 - logprior: -3.3329e+00
Epoch 2/2
39/39 - 32s - loss: 705.0121 - loglik: -7.0206e+02 - logprior: -2.1771e+00
Fitted a model with MAP estimate = -602.0324
expansions: [(0, 2), (124, 4), (125, 7), (181, 1), (219, 1), (221, 1), (304, 2)]
discards: [  0  86  87 107 117 142 269 270 271 301 302 303]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 706.4148 - loglik: -7.0390e+02 - logprior: -2.2374e+00
Epoch 2/2
39/39 - 32s - loss: 692.5679 - loglik: -6.9012e+02 - logprior: -1.2881e+00
Fitted a model with MAP estimate = -591.9175
expansions: [(115, 1), (176, 3), (177, 1), (225, 2)]
discards: [  0 104 105 106 121 122 123 124 125 126 127 128 233 234 235 308 309]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 609.2418 - loglik: -6.0666e+02 - logprior: -2.3143e+00
Epoch 2/10
43/43 - 34s - loss: 598.3863 - loglik: -5.9641e+02 - logprior: -8.8489e-01
Epoch 3/10
43/43 - 34s - loss: 591.1347 - loglik: -5.8853e+02 - logprior: -7.8374e-01
Epoch 4/10
43/43 - 34s - loss: 591.6696 - loglik: -5.8918e+02 - logprior: -7.0050e-01
Fitted a model with MAP estimate = -588.4303
Time for alignment: 675.9706
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 839.7017 - loglik: -8.3749e+02 - logprior: -1.8549e+00
Epoch 2/10
39/39 - 24s - loss: 737.2986 - loglik: -7.3442e+02 - logprior: -1.7438e+00
Epoch 3/10
39/39 - 24s - loss: 720.1810 - loglik: -7.1627e+02 - logprior: -1.9636e+00
Epoch 4/10
39/39 - 24s - loss: 716.1020 - loglik: -7.1237e+02 - logprior: -1.9468e+00
Epoch 5/10
39/39 - 24s - loss: 714.3478 - loglik: -7.1074e+02 - logprior: -1.9642e+00
Epoch 6/10
39/39 - 24s - loss: 712.5237 - loglik: -7.0904e+02 - logprior: -1.9739e+00
Epoch 7/10
39/39 - 24s - loss: 711.2928 - loglik: -7.0790e+02 - logprior: -1.9629e+00
Epoch 8/10
39/39 - 24s - loss: 710.3201 - loglik: -7.0695e+02 - logprior: -1.9720e+00
Epoch 9/10
39/39 - 24s - loss: 709.3303 - loglik: -7.0598e+02 - logprior: -1.9826e+00
Epoch 10/10
39/39 - 24s - loss: 708.8947 - loglik: -7.0562e+02 - logprior: -2.0034e+00
Fitted a model with MAP estimate = -612.4867
expansions: [(14, 1), (19, 1), (40, 1), (48, 1), (55, 1), (75, 1), (79, 3), (80, 2), (81, 2), (91, 3), (93, 2), (101, 1), (117, 1), (118, 1), (119, 1), (120, 3), (121, 1), (148, 8), (161, 1), (162, 1), (167, 1), (168, 2), (170, 1), (178, 6), (179, 1), (181, 4), (186, 1), (187, 1), (188, 2), (190, 1), (192, 2), (193, 1), (208, 1), (209, 1), (213, 4), (214, 2), (218, 1), (244, 3)]
discards: [  0  33 102 103 104 105 107]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 734.9195 - loglik: -7.3137e+02 - logprior: -3.2892e+00
Epoch 2/2
39/39 - 32s - loss: 702.4222 - loglik: -6.9925e+02 - logprior: -2.1976e+00
Fitted a model with MAP estimate = -599.7185
expansions: [(0, 2), (91, 1), (120, 6), (121, 5), (223, 1), (308, 2)]
discards: [  0  85 115 116 139 199 217 218 219 220 221 241 269 305 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 706.1428 - loglik: -7.0370e+02 - logprior: -2.1762e+00
Epoch 2/2
39/39 - 32s - loss: 692.8622 - loglik: -6.9042e+02 - logprior: -1.3734e+00
Fitted a model with MAP estimate = -592.6508
expansions: [(32, 1), (122, 1), (128, 1), (129, 1), (274, 1)]
discards: [  0 114 115 116 117 118 119 307 308]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 605.9214 - loglik: -6.0339e+02 - logprior: -2.2500e+00
Epoch 2/10
43/43 - 35s - loss: 593.9200 - loglik: -5.9196e+02 - logprior: -8.5272e-01
Epoch 3/10
43/43 - 35s - loss: 593.1423 - loglik: -5.9056e+02 - logprior: -7.2619e-01
Epoch 4/10
43/43 - 35s - loss: 590.6976 - loglik: -5.8818e+02 - logprior: -6.2511e-01
Epoch 5/10
43/43 - 34s - loss: 584.6979 - loglik: -5.8244e+02 - logprior: -5.3585e-01
Epoch 6/10
43/43 - 35s - loss: 585.5102 - loglik: -5.8354e+02 - logprior: -4.4514e-01
Fitted a model with MAP estimate = -583.5052
Time for alignment: 749.2886
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 837.4755 - loglik: -8.3525e+02 - logprior: -1.8580e+00
Epoch 2/10
39/39 - 24s - loss: 735.4365 - loglik: -7.3205e+02 - logprior: -1.6898e+00
Epoch 3/10
39/39 - 24s - loss: 721.7657 - loglik: -7.1776e+02 - logprior: -1.8503e+00
Epoch 4/10
39/39 - 24s - loss: 717.6362 - loglik: -7.1376e+02 - logprior: -1.9109e+00
Epoch 5/10
39/39 - 24s - loss: 715.0200 - loglik: -7.1138e+02 - logprior: -1.9485e+00
Epoch 6/10
39/39 - 24s - loss: 713.9191 - loglik: -7.1046e+02 - logprior: -1.9615e+00
Epoch 7/10
39/39 - 24s - loss: 712.5332 - loglik: -7.0918e+02 - logprior: -1.9707e+00
Epoch 8/10
39/39 - 24s - loss: 712.0618 - loglik: -7.0876e+02 - logprior: -1.9852e+00
Epoch 9/10
39/39 - 24s - loss: 711.2850 - loglik: -7.0797e+02 - logprior: -1.9828e+00
Epoch 10/10
39/39 - 24s - loss: 710.8615 - loglik: -7.0757e+02 - logprior: -1.9913e+00
Fitted a model with MAP estimate = -613.6674
expansions: [(14, 1), (30, 1), (41, 1), (44, 1), (81, 3), (82, 2), (83, 1), (93, 3), (95, 2), (103, 1), (119, 1), (120, 4), (121, 3), (122, 1), (126, 2), (147, 6), (158, 2), (159, 2), (163, 1), (164, 1), (172, 1), (173, 1), (175, 1), (176, 2), (177, 1), (181, 7), (187, 1), (188, 1), (189, 1), (192, 2), (193, 1), (208, 2), (209, 1), (213, 4), (214, 2), (239, 1), (244, 3)]
discards: [  0 104 106 107 108 109 165 166 167 168 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 738.2355 - loglik: -7.3462e+02 - logprior: -3.3665e+00
Epoch 2/2
39/39 - 31s - loss: 704.3978 - loglik: -7.0129e+02 - logprior: -2.2027e+00
Fitted a model with MAP estimate = -601.8412
expansions: [(0, 2), (120, 10), (174, 1), (218, 2), (236, 1)]
discards: [  0 115 116 135 145 188 198 205 264 300 301 302]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 706.5610 - loglik: -7.0411e+02 - logprior: -2.1823e+00
Epoch 2/2
39/39 - 32s - loss: 693.7745 - loglik: -6.9150e+02 - logprior: -1.1700e+00
Fitted a model with MAP estimate = -593.7883
expansions: [(114, 1), (115, 2), (122, 2), (129, 2), (205, 1), (222, 1), (228, 1), (229, 1), (242, 1), (274, 1), (307, 3)]
discards: [  0 224 225 226 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 41s - loss: 603.9163 - loglik: -6.0126e+02 - logprior: -2.3761e+00
Epoch 2/10
43/43 - 37s - loss: 594.1068 - loglik: -5.9194e+02 - logprior: -1.0047e+00
Epoch 3/10
43/43 - 36s - loss: 588.5450 - loglik: -5.8583e+02 - logprior: -8.3642e-01
Epoch 4/10
43/43 - 37s - loss: 586.5861 - loglik: -5.8398e+02 - logprior: -7.2738e-01
Epoch 5/10
43/43 - 36s - loss: 582.7601 - loglik: -5.8040e+02 - logprior: -6.4462e-01
Epoch 6/10
43/43 - 36s - loss: 585.4300 - loglik: -5.8310e+02 - logprior: -8.0997e-01
Fitted a model with MAP estimate = -581.5258
Time for alignment: 757.9767
Computed alignments with likelihoods: ['-588.4303', '-583.5052', '-581.5258']
Best model has likelihood: -581.5258
time for generating output: 0.4523
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5294021472559812
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64aa71ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4747f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb65bcdbca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb65305a1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb697271070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6749f0cd0>, <__main__.SimpleDirichletPrior object at 0x7fb605b522e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.9678 - loglik: -2.3644e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 224.0416 - loglik: -2.1359e+02 - logprior: -1.0431e+01
Epoch 3/10
10/10 - 2s - loss: 197.2437 - loglik: -1.9178e+02 - logprior: -5.4564e+00
Epoch 4/10
10/10 - 2s - loss: 181.3428 - loglik: -1.7740e+02 - logprior: -3.8726e+00
Epoch 5/10
10/10 - 2s - loss: 173.8927 - loglik: -1.7047e+02 - logprior: -3.1640e+00
Epoch 6/10
10/10 - 2s - loss: 170.9505 - loglik: -1.6776e+02 - logprior: -2.8022e+00
Epoch 7/10
10/10 - 2s - loss: 169.7061 - loglik: -1.6675e+02 - logprior: -2.6165e+00
Epoch 8/10
10/10 - 2s - loss: 168.5323 - loglik: -1.6568e+02 - logprior: -2.5303e+00
Epoch 9/10
10/10 - 2s - loss: 168.1047 - loglik: -1.6534e+02 - logprior: -2.4442e+00
Epoch 10/10
10/10 - 2s - loss: 167.4424 - loglik: -1.6473e+02 - logprior: -2.3948e+00
Fitted a model with MAP estimate = -166.8968
expansions: [(7, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.6027 - loglik: -1.6612e+02 - logprior: -4.3459e+01
Epoch 2/2
10/10 - 2s - loss: 174.4916 - loglik: -1.5630e+02 - logprior: -1.8050e+01
Fitted a model with MAP estimate = -167.9134
expansions: [(0, 2)]
discards: [ 0 18 25 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 186.8343 - loglik: -1.5249e+02 - logprior: -3.4325e+01
Epoch 2/2
10/10 - 2s - loss: 159.7104 - loglik: -1.5055e+02 - logprior: -9.0276e+00
Fitted a model with MAP estimate = -155.3137
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 193.1184 - loglik: -1.5322e+02 - logprior: -3.9875e+01
Epoch 2/10
10/10 - 2s - loss: 163.2164 - loglik: -1.5197e+02 - logprior: -1.1134e+01
Epoch 3/10
10/10 - 2s - loss: 155.7341 - loglik: -1.5102e+02 - logprior: -4.4748e+00
Epoch 4/10
10/10 - 2s - loss: 152.5997 - loglik: -1.5000e+02 - logprior: -2.3276e+00
Epoch 5/10
10/10 - 2s - loss: 151.3256 - loglik: -1.4965e+02 - logprior: -1.4084e+00
Epoch 6/10
10/10 - 2s - loss: 150.2468 - loglik: -1.4917e+02 - logprior: -8.2805e-01
Epoch 7/10
10/10 - 2s - loss: 150.1479 - loglik: -1.4960e+02 - logprior: -3.0294e-01
Epoch 8/10
10/10 - 2s - loss: 149.7143 - loglik: -1.4952e+02 - logprior: 0.0542
Epoch 9/10
10/10 - 2s - loss: 149.4811 - loglik: -1.4947e+02 - logprior: 0.2412
Epoch 10/10
10/10 - 2s - loss: 149.5863 - loglik: -1.4970e+02 - logprior: 0.3683
Fitted a model with MAP estimate = -149.0513
Time for alignment: 58.0676
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.0031 - loglik: -2.3647e+02 - logprior: -3.8512e+01
Epoch 2/10
10/10 - 2s - loss: 223.8182 - loglik: -2.1337e+02 - logprior: -1.0426e+01
Epoch 3/10
10/10 - 2s - loss: 197.9591 - loglik: -1.9252e+02 - logprior: -5.4283e+00
Epoch 4/10
10/10 - 2s - loss: 181.3657 - loglik: -1.7745e+02 - logprior: -3.8556e+00
Epoch 5/10
10/10 - 2s - loss: 174.0902 - loglik: -1.7069e+02 - logprior: -3.1801e+00
Epoch 6/10
10/10 - 2s - loss: 171.0702 - loglik: -1.6793e+02 - logprior: -2.7583e+00
Epoch 7/10
10/10 - 2s - loss: 169.9913 - loglik: -1.6710e+02 - logprior: -2.5409e+00
Epoch 8/10
10/10 - 2s - loss: 169.0008 - loglik: -1.6624e+02 - logprior: -2.4604e+00
Epoch 9/10
10/10 - 2s - loss: 168.5412 - loglik: -1.6584e+02 - logprior: -2.3944e+00
Epoch 10/10
10/10 - 2s - loss: 167.7028 - loglik: -1.6504e+02 - logprior: -2.3421e+00
Fitted a model with MAP estimate = -167.2175
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 209.8935 - loglik: -1.6644e+02 - logprior: -4.3436e+01
Epoch 2/2
10/10 - 2s - loss: 174.4056 - loglik: -1.5623e+02 - logprior: -1.8041e+01
Fitted a model with MAP estimate = -167.9807
expansions: [(0, 2)]
discards: [ 0 18 25 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.1512 - loglik: -1.5280e+02 - logprior: -3.4325e+01
Epoch 2/2
10/10 - 2s - loss: 159.4521 - loglik: -1.5029e+02 - logprior: -9.0307e+00
Fitted a model with MAP estimate = -155.3161
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 193.0314 - loglik: -1.5313e+02 - logprior: -3.9877e+01
Epoch 2/10
10/10 - 2s - loss: 163.2484 - loglik: -1.5200e+02 - logprior: -1.1128e+01
Epoch 3/10
10/10 - 2s - loss: 155.7913 - loglik: -1.5108e+02 - logprior: -4.4780e+00
Epoch 4/10
10/10 - 2s - loss: 152.6525 - loglik: -1.5004e+02 - logprior: -2.3333e+00
Epoch 5/10
10/10 - 2s - loss: 151.2230 - loglik: -1.4954e+02 - logprior: -1.4172e+00
Epoch 6/10
10/10 - 2s - loss: 150.2983 - loglik: -1.4922e+02 - logprior: -8.2953e-01
Epoch 7/10
10/10 - 2s - loss: 150.0668 - loglik: -1.4952e+02 - logprior: -2.9735e-01
Epoch 8/10
10/10 - 2s - loss: 149.7377 - loglik: -1.4954e+02 - logprior: 0.0559
Epoch 9/10
10/10 - 2s - loss: 149.6546 - loglik: -1.4965e+02 - logprior: 0.2465
Epoch 10/10
10/10 - 2s - loss: 149.2661 - loglik: -1.4938e+02 - logprior: 0.3712
Fitted a model with MAP estimate = -149.0426
Time for alignment: 58.0504
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.6927 - loglik: -2.3616e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 224.0584 - loglik: -2.1360e+02 - logprior: -1.0430e+01
Epoch 3/10
10/10 - 2s - loss: 196.9062 - loglik: -1.9145e+02 - logprior: -5.4443e+00
Epoch 4/10
10/10 - 2s - loss: 180.2390 - loglik: -1.7626e+02 - logprior: -3.8580e+00
Epoch 5/10
10/10 - 2s - loss: 173.0282 - loglik: -1.6953e+02 - logprior: -3.1234e+00
Epoch 6/10
10/10 - 2s - loss: 170.7802 - loglik: -1.6767e+02 - logprior: -2.7026e+00
Epoch 7/10
10/10 - 2s - loss: 169.0782 - loglik: -1.6626e+02 - logprior: -2.5168e+00
Epoch 8/10
10/10 - 2s - loss: 168.1725 - loglik: -1.6545e+02 - logprior: -2.4376e+00
Epoch 9/10
10/10 - 2s - loss: 167.6790 - loglik: -1.6502e+02 - logprior: -2.3820e+00
Epoch 10/10
10/10 - 2s - loss: 167.2982 - loglik: -1.6470e+02 - logprior: -2.3345e+00
Fitted a model with MAP estimate = -166.6835
expansions: [(7, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 3), (30, 1), (32, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.4191 - loglik: -1.6593e+02 - logprior: -4.3468e+01
Epoch 2/2
10/10 - 2s - loss: 174.7832 - loglik: -1.5662e+02 - logprior: -1.8051e+01
Fitted a model with MAP estimate = -168.0505
expansions: [(0, 2)]
discards: [ 0 18 26 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.3115 - loglik: -1.5296e+02 - logprior: -3.4323e+01
Epoch 2/2
10/10 - 2s - loss: 159.2991 - loglik: -1.5013e+02 - logprior: -9.0404e+00
Fitted a model with MAP estimate = -155.2456
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.8391 - loglik: -1.5298e+02 - logprior: -3.9837e+01
Epoch 2/10
10/10 - 2s - loss: 163.3156 - loglik: -1.5212e+02 - logprior: -1.1081e+01
Epoch 3/10
10/10 - 2s - loss: 156.0287 - loglik: -1.5135e+02 - logprior: -4.4424e+00
Epoch 4/10
10/10 - 2s - loss: 152.2436 - loglik: -1.4964e+02 - logprior: -2.3094e+00
Epoch 5/10
10/10 - 2s - loss: 151.0652 - loglik: -1.4940e+02 - logprior: -1.3850e+00
Epoch 6/10
10/10 - 2s - loss: 150.0425 - loglik: -1.4897e+02 - logprior: -8.1456e-01
Epoch 7/10
10/10 - 2s - loss: 150.2631 - loglik: -1.4973e+02 - logprior: -2.9087e-01
Fitted a model with MAP estimate = -149.4587
Time for alignment: 51.4731
Computed alignments with likelihoods: ['-149.0513', '-149.0426', '-149.4587']
Best model has likelihood: -149.0426
time for generating output: 0.2200
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9242723933314496
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb639438ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb579bb4880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb579bb41f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb63950fd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb63950fdc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb579d4a550>, <__main__.SimpleDirichletPrior object at 0x7fb603e4a7c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 302.4890 - loglik: -2.9130e+02 - logprior: -1.1187e+01
Epoch 2/10
19/19 - 8s - loss: 241.4899 - loglik: -2.3894e+02 - logprior: -2.5102e+00
Epoch 3/10
19/19 - 9s - loss: 220.2933 - loglik: -2.1762e+02 - logprior: -2.4025e+00
Epoch 4/10
19/19 - 7s - loss: 215.6326 - loglik: -2.1303e+02 - logprior: -2.2236e+00
Epoch 5/10
19/19 - 7s - loss: 216.0713 - loglik: -2.1375e+02 - logprior: -2.0207e+00
Fitted a model with MAP estimate = -214.2651
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (33, 1), (42, 1), (47, 2), (48, 4), (49, 1), (50, 4), (56, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 224.0825 - loglik: -2.1006e+02 - logprior: -1.3966e+01
Epoch 2/2
19/19 - 5s - loss: 204.0768 - loglik: -1.9931e+02 - logprior: -4.5435e+00
Fitted a model with MAP estimate = -200.6653
expansions: [(0, 2)]
discards: [ 0 13 61 62 65 68]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 208.8467 - loglik: -1.9901e+02 - logprior: -9.7554e+00
Epoch 2/2
19/19 - 8s - loss: 200.0187 - loglik: -1.9817e+02 - logprior: -1.6194e+00
Fitted a model with MAP estimate = -197.5278
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 212.2915 - loglik: -2.0068e+02 - logprior: -1.1527e+01
Epoch 2/10
19/19 - 8s - loss: 200.3478 - loglik: -1.9834e+02 - logprior: -1.7705e+00
Epoch 3/10
19/19 - 7s - loss: 197.6337 - loglik: -1.9670e+02 - logprior: -6.2788e-01
Epoch 4/10
19/19 - 7s - loss: 196.9430 - loglik: -1.9635e+02 - logprior: -2.7268e-01
Epoch 5/10
19/19 - 8s - loss: 196.8779 - loglik: -1.9649e+02 - logprior: -5.7247e-02
Epoch 6/10
19/19 - 7s - loss: 197.2850 - loglik: -1.9694e+02 - logprior: 0.0066
Fitted a model with MAP estimate = -195.9234
Time for alignment: 144.3445
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 302.9742 - loglik: -2.9178e+02 - logprior: -1.1184e+01
Epoch 2/10
19/19 - 7s - loss: 241.0149 - loglik: -2.3854e+02 - logprior: -2.4251e+00
Epoch 3/10
19/19 - 7s - loss: 220.5413 - loglik: -2.1797e+02 - logprior: -2.2304e+00
Epoch 4/10
19/19 - 7s - loss: 215.8662 - loglik: -2.1356e+02 - logprior: -2.0087e+00
Epoch 5/10
19/19 - 7s - loss: 215.2413 - loglik: -2.1310e+02 - logprior: -1.8792e+00
Epoch 6/10
19/19 - 7s - loss: 214.2064 - loglik: -2.1205e+02 - logprior: -1.8871e+00
Epoch 7/10
19/19 - 7s - loss: 214.8513 - loglik: -2.1269e+02 - logprior: -1.8713e+00
Fitted a model with MAP estimate = -213.8713
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (44, 1), (48, 6), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 223.0500 - loglik: -2.0909e+02 - logprior: -1.3900e+01
Epoch 2/2
19/19 - 8s - loss: 205.3297 - loglik: -2.0056e+02 - logprior: -4.5258e+00
Fitted a model with MAP estimate = -200.8594
expansions: [(0, 2)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 208.9521 - loglik: -1.9916e+02 - logprior: -9.7137e+00
Epoch 2/2
19/19 - 7s - loss: 199.1726 - loglik: -1.9735e+02 - logprior: -1.5831e+00
Fitted a model with MAP estimate = -197.3593
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 212.1160 - loglik: -2.0050e+02 - logprior: -1.1538e+01
Epoch 2/10
19/19 - 8s - loss: 200.7645 - loglik: -1.9878e+02 - logprior: -1.7488e+00
Epoch 3/10
19/19 - 7s - loss: 197.5795 - loglik: -1.9669e+02 - logprior: -6.0184e-01
Epoch 4/10
19/19 - 7s - loss: 198.5395 - loglik: -1.9802e+02 - logprior: -2.2460e-01
Fitted a model with MAP estimate = -197.0069
Time for alignment: 140.0494
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 303.6252 - loglik: -2.9243e+02 - logprior: -1.1186e+01
Epoch 2/10
19/19 - 7s - loss: 240.1431 - loglik: -2.3767e+02 - logprior: -2.4310e+00
Epoch 3/10
19/19 - 6s - loss: 218.8900 - loglik: -2.1638e+02 - logprior: -2.1666e+00
Epoch 4/10
19/19 - 8s - loss: 217.1825 - loglik: -2.1500e+02 - logprior: -1.8897e+00
Epoch 5/10
19/19 - 6s - loss: 214.5726 - loglik: -2.1255e+02 - logprior: -1.7600e+00
Epoch 6/10
19/19 - 8s - loss: 215.5223 - loglik: -2.1345e+02 - logprior: -1.7884e+00
Fitted a model with MAP estimate = -214.0902
expansions: [(7, 1), (9, 3), (10, 2), (11, 1), (26, 1), (29, 1), (48, 5), (49, 4), (50, 3), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 223.2221 - loglik: -2.0924e+02 - logprior: -1.3917e+01
Epoch 2/2
19/19 - 7s - loss: 204.2753 - loglik: -1.9943e+02 - logprior: -4.6096e+00
Fitted a model with MAP estimate = -200.4380
expansions: [(0, 2)]
discards: [ 0 13 62 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 209.3971 - loglik: -1.9958e+02 - logprior: -9.7422e+00
Epoch 2/2
19/19 - 7s - loss: 198.7933 - loglik: -1.9695e+02 - logprior: -1.6076e+00
Fitted a model with MAP estimate = -197.6560
expansions: []
discards: [ 0 13 64]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 213.4686 - loglik: -2.0178e+02 - logprior: -1.1608e+01
Epoch 2/10
19/19 - 7s - loss: 200.1112 - loglik: -1.9797e+02 - logprior: -1.8994e+00
Epoch 3/10
19/19 - 7s - loss: 199.0353 - loglik: -1.9794e+02 - logprior: -7.8869e-01
Epoch 4/10
19/19 - 7s - loss: 198.7235 - loglik: -1.9801e+02 - logprior: -4.1565e-01
Epoch 5/10
19/19 - 6s - loss: 196.7641 - loglik: -1.9622e+02 - logprior: -2.4351e-01
Epoch 6/10
19/19 - 8s - loss: 198.1833 - loglik: -1.9771e+02 - logprior: -1.5056e-01
Fitted a model with MAP estimate = -196.7153
Time for alignment: 144.9900
Computed alignments with likelihoods: ['-195.9234', '-197.0069', '-196.7153']
Best model has likelihood: -195.9234
time for generating output: 0.6666
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21432594817874578
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb57b2c3ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60eef6ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6535dd0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60e9b3c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb697309310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb674c6cee0>, <__main__.SimpleDirichletPrior object at 0x7fb60e9b6a00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 54s - loss: 1103.9769 - loglik: -1.1016e+03 - logprior: -1.9330e+00
Epoch 2/10
49/49 - 49s - loss: 946.3513 - loglik: -9.4382e+02 - logprior: -1.1481e+00
Epoch 3/10
49/49 - 49s - loss: 944.0227 - loglik: -9.4041e+02 - logprior: -1.2543e+00
Epoch 4/10
49/49 - 49s - loss: 938.4096 - loglik: -9.3441e+02 - logprior: -1.5604e+00
Epoch 5/10
49/49 - 49s - loss: 930.1202 - loglik: -9.2636e+02 - logprior: -1.4945e+00
Epoch 6/10
49/49 - 49s - loss: 933.4549 - loglik: -9.2968e+02 - logprior: -1.7142e+00
Fitted a model with MAP estimate = -926.8717
expansions: [(0, 3), (140, 1), (162, 2), (184, 1), (212, 1), (213, 1), (231, 3), (232, 1), (233, 1), (234, 1), (235, 1), (248, 1), (258, 1), (262, 1), (263, 2), (264, 6), (282, 10), (288, 1), (298, 1), (300, 3), (301, 5), (303, 2), (304, 5), (305, 4), (306, 1), (315, 1), (316, 1), (317, 4), (318, 1), (326, 2), (327, 2), (328, 1), (329, 2), (330, 1), (333, 1), (347, 6), (348, 1), (349, 1), (366, 1), (378, 2), (379, 2), (391, 2), (392, 4), (396, 11), (404, 4)]
discards: [81]
Re-initialized the encoder parameters.
Fitting a model of length 512 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 73s - loss: 963.0822 - loglik: -9.5815e+02 - logprior: -4.5714e+00
Epoch 2/2
49/49 - 68s - loss: 911.3641 - loglik: -9.1017e+02 - logprior: 0.0735
Fitted a model with MAP estimate = -904.0947
expansions: [(0, 4), (38, 3), (313, 1), (361, 1), (494, 1), (496, 1), (498, 4)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13 167 242 379 380 381 424
 481 503 504 505 506 507 508 509 510 511]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 70s - loss: 931.5818 - loglik: -9.2853e+02 - logprior: -2.6657e+00
Epoch 2/2
49/49 - 66s - loss: 911.0467 - loglik: -9.1153e+02 - logprior: 2.0167
Fitted a model with MAP estimate = -900.8294
expansions: [(0, 5), (332, 1), (338, 2), (493, 1), (494, 1), (495, 1), (499, 5)]
discards: [ 1  2  3  4  5  6  7  8 30 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 72s - loss: 928.7378 - loglik: -9.2625e+02 - logprior: -2.1001e+00
Epoch 2/10
49/49 - 67s - loss: 908.4922 - loglik: -9.0984e+02 - logprior: 2.8637
Epoch 3/10
49/49 - 67s - loss: 893.1736 - loglik: -8.9394e+02 - logprior: 3.2586
Epoch 4/10
49/49 - 67s - loss: 895.4274 - loglik: -8.9611e+02 - logprior: 3.4768
Fitted a model with MAP estimate = -886.2751
Time for alignment: 1028.0582
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 53s - loss: 1103.4449 - loglik: -1.1008e+03 - logprior: -2.1438e+00
Epoch 2/10
49/49 - 49s - loss: 955.6221 - loglik: -9.5194e+02 - logprior: -1.4069e+00
Epoch 3/10
49/49 - 49s - loss: 940.9943 - loglik: -9.3666e+02 - logprior: -1.5420e+00
Epoch 4/10
49/49 - 49s - loss: 937.5164 - loglik: -9.3305e+02 - logprior: -1.6736e+00
Epoch 5/10
49/49 - 49s - loss: 933.9268 - loglik: -9.2967e+02 - logprior: -1.8044e+00
Epoch 6/10
49/49 - 49s - loss: 929.5759 - loglik: -9.2546e+02 - logprior: -1.8946e+00
Epoch 7/10
49/49 - 49s - loss: 929.7161 - loglik: -9.2556e+02 - logprior: -2.0684e+00
Fitted a model with MAP estimate = -925.6796
expansions: [(0, 3), (133, 1), (141, 1), (183, 1), (207, 1), (213, 1), (214, 1), (228, 1), (231, 7), (232, 3), (234, 2), (238, 2), (239, 2), (240, 1), (245, 1), (248, 1), (251, 1), (252, 2), (253, 6), (254, 1), (270, 2), (271, 8), (278, 1), (286, 1), (287, 7), (288, 2), (291, 1), (294, 2), (295, 4), (297, 1), (298, 1), (307, 2), (308, 1), (309, 1), (310, 4), (311, 1), (320, 1), (321, 1), (322, 1), (324, 1), (340, 1), (341, 4), (366, 3), (368, 1), (370, 1), (371, 3), (372, 2), (389, 1), (394, 8), (404, 4)]
discards: [35 36 37]
Re-initialized the encoder parameters.
Fitting a model of length 511 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 72s - loss: 967.7513 - loglik: -9.6313e+02 - logprior: -4.2884e+00
Epoch 2/2
49/49 - 68s - loss: 918.6263 - loglik: -9.1736e+02 - logprior: -6.9532e-02
Fitted a model with MAP estimate = -905.0857
expansions: [(0, 3), (38, 3), (244, 1), (281, 1), (308, 1), (339, 2), (346, 1), (358, 2), (422, 1), (490, 1), (492, 1), (494, 1), (496, 4)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13 246 349 380 381 425 426
 427 450 497 498 499 500 501 502 503 504 505 506 507 508 509 510]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 70s - loss: 932.4124 - loglik: -9.2954e+02 - logprior: -2.4795e+00
Epoch 2/2
49/49 - 66s - loss: 909.3804 - loglik: -9.1000e+02 - logprior: 2.2162
Fitted a model with MAP estimate = -900.4231
expansions: [(0, 5), (423, 1), (424, 1), (498, 1), (499, 7)]
discards: [  1   2   3   5   6   7  29  30 336 356]
Re-initialized the encoder parameters.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 71s - loss: 931.8050 - loglik: -9.2917e+02 - logprior: -2.2435e+00
Epoch 2/10
49/49 - 67s - loss: 906.2173 - loglik: -9.0767e+02 - logprior: 3.0108
Epoch 3/10
49/49 - 67s - loss: 894.4408 - loglik: -8.9516e+02 - logprior: 3.2497
Epoch 4/10
49/49 - 67s - loss: 894.6510 - loglik: -8.9543e+02 - logprior: 3.6250
Fitted a model with MAP estimate = -886.4726
Time for alignment: 1077.4338
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1098.0194 - loglik: -1.0955e+03 - logprior: -2.0844e+00
Epoch 2/10
49/49 - 49s - loss: 951.2856 - loglik: -9.4835e+02 - logprior: -1.0330e+00
Epoch 3/10
49/49 - 49s - loss: 939.9477 - loglik: -9.3599e+02 - logprior: -1.3261e+00
Epoch 4/10
49/49 - 49s - loss: 930.0881 - loglik: -9.2588e+02 - logprior: -1.6236e+00
Epoch 5/10
49/49 - 49s - loss: 929.0175 - loglik: -9.2501e+02 - logprior: -1.7183e+00
Epoch 6/10
49/49 - 49s - loss: 923.9916 - loglik: -9.2020e+02 - logprior: -1.7731e+00
Epoch 7/10
49/49 - 49s - loss: 929.6388 - loglik: -9.2599e+02 - logprior: -1.7368e+00
Fitted a model with MAP estimate = -922.4972
expansions: [(0, 3), (83, 1), (131, 1), (139, 1), (181, 1), (212, 1), (213, 1), (224, 1), (226, 2), (228, 5), (229, 3), (230, 1), (233, 2), (236, 3), (237, 2), (238, 1), (243, 1), (250, 1), (251, 3), (252, 5), (273, 1), (274, 1), (275, 1), (276, 1), (294, 2), (297, 9), (298, 2), (299, 2), (300, 5), (301, 2), (311, 1), (312, 1), (313, 1), (315, 1), (316, 1), (322, 1), (324, 1), (325, 1), (326, 2), (327, 1), (330, 1), (344, 5), (345, 1), (346, 1), (363, 1), (392, 6), (395, 6), (404, 4)]
discards: [34 35]
Re-initialized the encoder parameters.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 962.2762 - loglik: -9.5784e+02 - logprior: -4.1038e+00
Epoch 2/2
49/49 - 67s - loss: 921.9130 - loglik: -9.2063e+02 - logprior: -6.2470e-02
Fitted a model with MAP estimate = -906.4887
expansions: [(0, 4), (360, 2), (361, 1), (483, 6), (487, 1), (488, 2)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  37 235 336 489 490 491
 492 493 494 495 496 497 498 499 500 501 502]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 68s - loss: 936.9120 - loglik: -9.3409e+02 - logprior: -2.4164e+00
Epoch 2/2
49/49 - 64s - loss: 907.0795 - loglik: -9.0776e+02 - logprior: 2.2679
Fitted a model with MAP estimate = -902.5022
expansions: [(0, 5), (29, 3), (478, 1), (479, 1), (480, 1), (489, 1), (490, 7)]
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 500 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 70s - loss: 930.4420 - loglik: -9.2796e+02 - logprior: -2.0805e+00
Epoch 2/10
49/49 - 66s - loss: 907.5890 - loglik: -9.0890e+02 - logprior: 2.8598
Epoch 3/10
49/49 - 66s - loss: 897.6301 - loglik: -8.9848e+02 - logprior: 3.3317
Epoch 4/10
49/49 - 66s - loss: 894.4502 - loglik: -8.9536e+02 - logprior: 3.7205
Epoch 5/10
49/49 - 66s - loss: 890.3143 - loglik: -8.9168e+02 - logprior: 4.1564
Epoch 6/10
49/49 - 66s - loss: 885.1446 - loglik: -8.8702e+02 - logprior: 4.5477
Epoch 7/10
49/49 - 66s - loss: 883.5588 - loglik: -8.8608e+02 - logprior: 5.0698
Epoch 8/10
49/49 - 66s - loss: 882.1898 - loglik: -8.8537e+02 - logprior: 5.6959
Epoch 9/10
49/49 - 66s - loss: 875.0794 - loglik: -8.7871e+02 - logprior: 6.2649
Epoch 10/10
49/49 - 66s - loss: 876.4692 - loglik: -8.8060e+02 - logprior: 6.7955
Fitted a model with MAP estimate = -870.9225
Time for alignment: 1459.4401
Computed alignments with likelihoods: ['-886.2751', '-886.4726', '-870.9225']
Best model has likelihood: -870.9225
time for generating output: 0.4771
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8230194718591625
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6644750d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb67da59df0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb67da59340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb68616c610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb68616cbe0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb686a56ac0>, <__main__.SimpleDirichletPrior object at 0x7fb67579e0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 476.8556 - loglik: -4.7457e+02 - logprior: -2.0345e+00
Epoch 2/10
39/39 - 12s - loss: 408.9184 - loglik: -4.0622e+02 - logprior: -1.7463e+00
Epoch 3/10
39/39 - 11s - loss: 402.1844 - loglik: -3.9956e+02 - logprior: -1.7418e+00
Epoch 4/10
39/39 - 12s - loss: 399.7684 - loglik: -3.9721e+02 - logprior: -1.7591e+00
Epoch 5/10
39/39 - 12s - loss: 398.9818 - loglik: -3.9650e+02 - logprior: -1.7500e+00
Epoch 6/10
39/39 - 12s - loss: 398.4672 - loglik: -3.9605e+02 - logprior: -1.7462e+00
Epoch 7/10
39/39 - 12s - loss: 398.0248 - loglik: -3.9566e+02 - logprior: -1.7441e+00
Epoch 8/10
39/39 - 12s - loss: 398.0464 - loglik: -3.9572e+02 - logprior: -1.7465e+00
Fitted a model with MAP estimate = -397.6709
expansions: [(9, 2), (10, 2), (11, 2), (12, 2), (22, 1), (23, 1), (26, 1), (39, 3), (40, 2), (45, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (102, 2), (103, 2), (104, 3), (106, 2), (118, 1), (119, 1), (122, 1), (126, 1), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 398.2484 - loglik: -3.9488e+02 - logprior: -3.1255e+00
Epoch 2/2
39/39 - 14s - loss: 382.3211 - loglik: -3.8045e+02 - logprior: -1.3497e+00
Fitted a model with MAP estimate = -379.5416
expansions: []
discards: [  8  12  14  53  54  92 113 132 135 142]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 386.9544 - loglik: -3.8454e+02 - logprior: -2.1923e+00
Epoch 2/2
39/39 - 14s - loss: 381.2338 - loglik: -3.7975e+02 - logprior: -1.0172e+00
Fitted a model with MAP estimate = -379.8917
expansions: [(50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 383.0675 - loglik: -3.8106e+02 - logprior: -1.8105e+00
Epoch 2/10
41/41 - 14s - loss: 377.2506 - loglik: -3.7598e+02 - logprior: -7.9857e-01
Epoch 3/10
41/41 - 14s - loss: 375.9326 - loglik: -3.7446e+02 - logprior: -7.3997e-01
Epoch 4/10
41/41 - 14s - loss: 373.5079 - loglik: -3.7196e+02 - logprior: -6.9163e-01
Epoch 5/10
41/41 - 14s - loss: 373.3985 - loglik: -3.7190e+02 - logprior: -6.3074e-01
Epoch 6/10
41/41 - 15s - loss: 373.8134 - loglik: -3.7244e+02 - logprior: -5.5087e-01
Fitted a model with MAP estimate = -371.5231
Time for alignment: 318.1597
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 478.3336 - loglik: -4.7605e+02 - logprior: -2.0401e+00
Epoch 2/10
39/39 - 12s - loss: 411.0793 - loglik: -4.0870e+02 - logprior: -1.7994e+00
Epoch 3/10
39/39 - 12s - loss: 399.8435 - loglik: -3.9706e+02 - logprior: -1.9890e+00
Epoch 4/10
39/39 - 12s - loss: 397.5157 - loglik: -3.9476e+02 - logprior: -1.9802e+00
Epoch 5/10
39/39 - 12s - loss: 396.2534 - loglik: -3.9354e+02 - logprior: -1.9799e+00
Epoch 6/10
39/39 - 12s - loss: 395.6834 - loglik: -3.9303e+02 - logprior: -1.9869e+00
Epoch 7/10
39/39 - 12s - loss: 395.3573 - loglik: -3.9275e+02 - logprior: -1.9938e+00
Epoch 8/10
39/39 - 12s - loss: 394.8464 - loglik: -3.9229e+02 - logprior: -1.9897e+00
Epoch 9/10
39/39 - 12s - loss: 395.0254 - loglik: -3.9249e+02 - logprior: -1.9921e+00
Fitted a model with MAP estimate = -395.1651
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 2), (40, 3), (41, 1), (44, 1), (45, 1), (55, 2), (59, 1), (60, 1), (69, 1), (70, 2), (71, 2), (90, 1), (91, 2), (92, 1), (93, 1), (95, 2), (99, 2), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 2), (131, 1)]
discards: [ 0 81]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 398.0919 - loglik: -3.9467e+02 - logprior: -3.1668e+00
Epoch 2/2
39/39 - 14s - loss: 380.9205 - loglik: -3.7902e+02 - logprior: -1.3436e+00
Fitted a model with MAP estimate = -377.8385
expansions: []
discards: [ 13  34  50  71  91  93 115 123 130 136 167]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 385.4122 - loglik: -3.8303e+02 - logprior: -2.1498e+00
Epoch 2/2
39/39 - 14s - loss: 380.5794 - loglik: -3.7918e+02 - logprior: -8.9810e-01
Fitted a model with MAP estimate = -379.1533
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 383.0124 - loglik: -3.8100e+02 - logprior: -1.7802e+00
Epoch 2/10
41/41 - 14s - loss: 378.5450 - loglik: -3.7730e+02 - logprior: -7.5382e-01
Epoch 3/10
41/41 - 14s - loss: 376.0604 - loglik: -3.7465e+02 - logprior: -6.9222e-01
Epoch 4/10
41/41 - 14s - loss: 375.9579 - loglik: -3.7451e+02 - logprior: -6.3563e-01
Epoch 5/10
41/41 - 14s - loss: 373.6778 - loglik: -3.7224e+02 - logprior: -5.9279e-01
Epoch 6/10
41/41 - 14s - loss: 374.1002 - loglik: -3.7276e+02 - logprior: -5.1989e-01
Fitted a model with MAP estimate = -372.5689
Time for alignment: 332.0335
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 479.1638 - loglik: -4.7685e+02 - logprior: -2.0676e+00
Epoch 2/10
39/39 - 12s - loss: 409.6798 - loglik: -4.0701e+02 - logprior: -1.8131e+00
Epoch 3/10
39/39 - 12s - loss: 400.7937 - loglik: -3.9807e+02 - logprior: -1.8755e+00
Epoch 4/10
39/39 - 12s - loss: 397.7402 - loglik: -3.9502e+02 - logprior: -1.9083e+00
Epoch 5/10
39/39 - 12s - loss: 397.2676 - loglik: -3.9466e+02 - logprior: -1.8918e+00
Epoch 6/10
39/39 - 11s - loss: 396.2755 - loglik: -3.9373e+02 - logprior: -1.8947e+00
Epoch 7/10
39/39 - 11s - loss: 396.7054 - loglik: -3.9419e+02 - logprior: -1.8894e+00
Fitted a model with MAP estimate = -396.0358
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 1), (26, 1), (31, 1), (39, 2), (40, 1), (43, 1), (55, 2), (57, 2), (58, 1), (59, 2), (66, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (102, 1), (103, 2), (104, 2), (105, 1), (107, 1), (109, 1), (118, 1), (119, 1), (121, 1), (126, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 396.9502 - loglik: -3.9356e+02 - logprior: -3.1440e+00
Epoch 2/2
39/39 - 14s - loss: 379.9030 - loglik: -3.7804e+02 - logprior: -1.3321e+00
Fitted a model with MAP estimate = -376.7420
expansions: []
discards: [ 13  28  70  73  78  93 114 121 136 139]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 383.2628 - loglik: -3.8093e+02 - logprior: -2.1362e+00
Epoch 2/2
39/39 - 14s - loss: 378.2186 - loglik: -3.7687e+02 - logprior: -9.1441e-01
Fitted a model with MAP estimate = -376.9575
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 381.4835 - loglik: -3.7952e+02 - logprior: -1.7682e+00
Epoch 2/10
41/41 - 14s - loss: 377.2157 - loglik: -3.7601e+02 - logprior: -7.4857e-01
Epoch 3/10
41/41 - 14s - loss: 374.5918 - loglik: -3.7318e+02 - logprior: -6.9673e-01
Epoch 4/10
41/41 - 14s - loss: 373.9185 - loglik: -3.7246e+02 - logprior: -6.2827e-01
Epoch 5/10
41/41 - 15s - loss: 372.6527 - loglik: -3.7123e+02 - logprior: -5.7174e-01
Epoch 6/10
41/41 - 14s - loss: 371.7821 - loglik: -3.7046e+02 - logprior: -4.9899e-01
Epoch 7/10
41/41 - 14s - loss: 371.2458 - loglik: -3.7004e+02 - logprior: -4.1992e-01
Epoch 8/10
41/41 - 14s - loss: 372.0974 - loglik: -3.7101e+02 - logprior: -3.6147e-01
Fitted a model with MAP estimate = -370.0375
Time for alignment: 334.0325
Computed alignments with likelihoods: ['-371.5231', '-372.5689', '-370.0375']
Best model has likelihood: -370.0375
time for generating output: 0.3225
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.4877149877149877
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb60e9acaf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686d79700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a9c3dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6041ceee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6041cefa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57a3c44c0>, <__main__.SimpleDirichletPrior object at 0x7fb69758c1c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.8340 - loglik: -1.1164e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 106.0536 - loglik: -8.9196e+01 - logprior: -1.6850e+01
Epoch 3/10
10/10 - 1s - loss: 80.9681 - loglik: -7.2410e+01 - logprior: -8.5229e+00
Epoch 4/10
10/10 - 1s - loss: 70.0901 - loglik: -6.4612e+01 - logprior: -5.4290e+00
Epoch 5/10
10/10 - 1s - loss: 64.7786 - loglik: -6.0908e+01 - logprior: -3.8517e+00
Epoch 6/10
10/10 - 1s - loss: 62.6946 - loglik: -5.9485e+01 - logprior: -3.1247e+00
Epoch 7/10
10/10 - 1s - loss: 61.7336 - loglik: -5.8776e+01 - logprior: -2.6913e+00
Epoch 8/10
10/10 - 1s - loss: 61.3101 - loglik: -5.8565e+01 - logprior: -2.4142e+00
Epoch 9/10
10/10 - 1s - loss: 61.1692 - loglik: -5.8676e+01 - logprior: -2.2036e+00
Epoch 10/10
10/10 - 1s - loss: 60.8914 - loglik: -5.8568e+01 - logprior: -2.0497e+00
Fitted a model with MAP estimate = -60.5589
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.4366 - loglik: -5.7731e+01 - logprior: -7.9686e+01
Epoch 2/2
10/10 - 1s - loss: 79.6413 - loglik: -5.3491e+01 - logprior: -2.6049e+01
Fitted a model with MAP estimate = -68.4201
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.4509 - loglik: -5.1211e+01 - logprior: -6.6221e+01
Epoch 2/2
10/10 - 1s - loss: 74.0442 - loglik: -5.0498e+01 - logprior: -2.3451e+01
Fitted a model with MAP estimate = -64.8221
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 105.1470 - loglik: -4.9937e+01 - logprior: -5.5193e+01
Epoch 2/10
10/10 - 1s - loss: 66.6161 - loglik: -5.0764e+01 - logprior: -1.5772e+01
Epoch 3/10
10/10 - 1s - loss: 58.8542 - loglik: -5.0900e+01 - logprior: -7.8091e+00
Epoch 4/10
10/10 - 1s - loss: 56.0614 - loglik: -5.1126e+01 - logprior: -4.7181e+00
Epoch 5/10
10/10 - 1s - loss: 54.4368 - loglik: -5.0983e+01 - logprior: -3.1915e+00
Epoch 6/10
10/10 - 1s - loss: 53.3935 - loglik: -5.0867e+01 - logprior: -2.2427e+00
Epoch 7/10
10/10 - 1s - loss: 52.7569 - loglik: -5.0835e+01 - logprior: -1.6146e+00
Epoch 8/10
10/10 - 1s - loss: 52.5157 - loglik: -5.0947e+01 - logprior: -1.2447e+00
Epoch 9/10
10/10 - 1s - loss: 52.2747 - loglik: -5.0938e+01 - logprior: -1.0049e+00
Epoch 10/10
10/10 - 1s - loss: 52.1987 - loglik: -5.1030e+01 - logprior: -8.2944e-01
Fitted a model with MAP estimate = -51.7222
Time for alignment: 31.0408
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.8782 - loglik: -1.1168e+02 - logprior: -5.9180e+01
Epoch 2/10
10/10 - 1s - loss: 105.9692 - loglik: -8.9113e+01 - logprior: -1.6848e+01
Epoch 3/10
10/10 - 1s - loss: 81.1229 - loglik: -7.2556e+01 - logprior: -8.5323e+00
Epoch 4/10
10/10 - 1s - loss: 70.1102 - loglik: -6.4578e+01 - logprior: -5.4824e+00
Epoch 5/10
10/10 - 1s - loss: 65.7088 - loglik: -6.1724e+01 - logprior: -3.9537e+00
Epoch 6/10
10/10 - 1s - loss: 63.9561 - loglik: -6.0601e+01 - logprior: -3.2066e+00
Epoch 7/10
10/10 - 1s - loss: 63.3424 - loglik: -6.0279e+01 - logprior: -2.7592e+00
Epoch 8/10
10/10 - 1s - loss: 62.7454 - loglik: -5.9968e+01 - logprior: -2.4694e+00
Epoch 9/10
10/10 - 1s - loss: 62.5077 - loglik: -5.9958e+01 - logprior: -2.2882e+00
Epoch 10/10
10/10 - 1s - loss: 62.4293 - loglik: -5.9971e+01 - logprior: -2.1992e+00
Fitted a model with MAP estimate = -61.8797
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.4302 - loglik: -5.8891e+01 - logprior: -7.9522e+01
Epoch 2/2
10/10 - 1s - loss: 79.8651 - loglik: -5.3765e+01 - logprior: -2.6017e+01
Fitted a model with MAP estimate = -68.7138
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 117.6050 - loglik: -5.1509e+01 - logprior: -6.6080e+01
Epoch 2/2
10/10 - 1s - loss: 74.3230 - loglik: -5.0836e+01 - logprior: -2.3417e+01
Fitted a model with MAP estimate = -65.1115
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 105.3878 - loglik: -5.0217e+01 - logprior: -5.5153e+01
Epoch 2/10
10/10 - 1s - loss: 66.7273 - loglik: -5.0959e+01 - logprior: -1.5696e+01
Epoch 3/10
10/10 - 1s - loss: 59.0881 - loglik: -5.1225e+01 - logprior: -7.7250e+00
Epoch 4/10
10/10 - 1s - loss: 56.1787 - loglik: -5.1312e+01 - logprior: -4.6488e+00
Epoch 5/10
10/10 - 1s - loss: 54.3964 - loglik: -5.0969e+01 - logprior: -3.1485e+00
Epoch 6/10
10/10 - 1s - loss: 53.5300 - loglik: -5.1084e+01 - logprior: -2.1604e+00
Epoch 7/10
10/10 - 1s - loss: 52.7395 - loglik: -5.0905e+01 - logprior: -1.5322e+00
Epoch 8/10
10/10 - 1s - loss: 52.5438 - loglik: -5.1052e+01 - logprior: -1.1678e+00
Epoch 9/10
10/10 - 1s - loss: 52.3029 - loglik: -5.1050e+01 - logprior: -9.1926e-01
Epoch 10/10
10/10 - 1s - loss: 52.2800 - loglik: -5.1193e+01 - logprior: -7.4719e-01
Fitted a model with MAP estimate = -51.7751
Time for alignment: 29.0202
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.7941 - loglik: -1.1160e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 106.0748 - loglik: -8.9218e+01 - logprior: -1.6849e+01
Epoch 3/10
10/10 - 1s - loss: 81.0428 - loglik: -7.2481e+01 - logprior: -8.5275e+00
Epoch 4/10
10/10 - 1s - loss: 70.3726 - loglik: -6.4869e+01 - logprior: -5.4542e+00
Epoch 5/10
10/10 - 1s - loss: 66.1962 - loglik: -6.2276e+01 - logprior: -3.9017e+00
Epoch 6/10
10/10 - 1s - loss: 63.6613 - loglik: -6.0398e+01 - logprior: -3.1905e+00
Epoch 7/10
10/10 - 1s - loss: 62.3856 - loglik: -5.9371e+01 - logprior: -2.7600e+00
Epoch 8/10
10/10 - 1s - loss: 61.7489 - loglik: -5.8931e+01 - logprior: -2.4862e+00
Epoch 9/10
10/10 - 1s - loss: 61.4982 - loglik: -5.8935e+01 - logprior: -2.2800e+00
Epoch 10/10
10/10 - 1s - loss: 61.0920 - loglik: -5.8715e+01 - logprior: -2.1191e+00
Fitted a model with MAP estimate = -60.8278
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.3774 - loglik: -5.7613e+01 - logprior: -7.9747e+01
Epoch 2/2
10/10 - 1s - loss: 79.4698 - loglik: -5.3512e+01 - logprior: -2.5872e+01
Fitted a model with MAP estimate = -68.1900
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.4808 - loglik: -5.1272e+01 - logprior: -6.6193e+01
Epoch 2/2
10/10 - 1s - loss: 73.9638 - loglik: -5.0453e+01 - logprior: -2.3440e+01
Fitted a model with MAP estimate = -64.8601
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.1347 - loglik: -4.9937e+01 - logprior: -5.5181e+01
Epoch 2/10
10/10 - 1s - loss: 66.5052 - loglik: -5.0674e+01 - logprior: -1.5762e+01
Epoch 3/10
10/10 - 1s - loss: 59.0911 - loglik: -5.1175e+01 - logprior: -7.7914e+00
Epoch 4/10
10/10 - 1s - loss: 55.9993 - loglik: -5.1105e+01 - logprior: -4.6965e+00
Epoch 5/10
10/10 - 1s - loss: 54.4875 - loglik: -5.1061e+01 - logprior: -3.1693e+00
Epoch 6/10
10/10 - 1s - loss: 53.4057 - loglik: -5.0891e+01 - logprior: -2.2401e+00
Epoch 7/10
10/10 - 1s - loss: 52.7938 - loglik: -5.0889e+01 - logprior: -1.6041e+00
Epoch 8/10
10/10 - 1s - loss: 52.5438 - loglik: -5.0994e+01 - logprior: -1.2297e+00
Epoch 9/10
10/10 - 1s - loss: 52.2494 - loglik: -5.0929e+01 - logprior: -9.9204e-01
Epoch 10/10
10/10 - 1s - loss: 52.1051 - loglik: -5.0953e+01 - logprior: -8.1437e-01
Fitted a model with MAP estimate = -51.7133
Time for alignment: 29.6649
Computed alignments with likelihoods: ['-51.7222', '-51.7751', '-51.7133']
Best model has likelihood: -51.7133
time for generating output: 0.1022
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9734299516908212
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb61f44bc10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb61f7e76d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6307d8310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6046b6370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6057d4e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb641da7880>, <__main__.SimpleDirichletPrior object at 0x7fb6032330d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.7191 - loglik: -3.6972e+02 - logprior: -2.8746e+00
Epoch 2/10
20/20 - 4s - loss: 329.0528 - loglik: -3.2695e+02 - logprior: -1.3448e+00
Epoch 3/10
20/20 - 4s - loss: 309.2250 - loglik: -3.0625e+02 - logprior: -1.6250e+00
Epoch 4/10
20/20 - 4s - loss: 303.9009 - loglik: -3.0117e+02 - logprior: -1.5694e+00
Epoch 5/10
20/20 - 5s - loss: 300.9303 - loglik: -2.9829e+02 - logprior: -1.5986e+00
Epoch 6/10
20/20 - 4s - loss: 299.8070 - loglik: -2.9727e+02 - logprior: -1.5839e+00
Epoch 7/10
20/20 - 5s - loss: 298.9892 - loglik: -2.9657e+02 - logprior: -1.5689e+00
Epoch 8/10
20/20 - 4s - loss: 297.9715 - loglik: -2.9562e+02 - logprior: -1.5733e+00
Epoch 9/10
20/20 - 5s - loss: 298.0288 - loglik: -2.9578e+02 - logprior: -1.5558e+00
Fitted a model with MAP estimate = -290.4571
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (39, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 12s - loss: 309.0488 - loglik: -3.0590e+02 - logprior: -2.8686e+00
Epoch 2/2
40/40 - 7s - loss: 289.9662 - loglik: -2.8792e+02 - logprior: -1.1685e+00
Fitted a model with MAP estimate = -274.2360
expansions: [(99, 1), (100, 2), (126, 1)]
discards: [  8  13  46  76  83  85 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 295.0875 - loglik: -2.9290e+02 - logprior: -2.0069e+00
Epoch 2/2
40/40 - 7s - loss: 288.3250 - loglik: -2.8696e+02 - logprior: -9.1945e-01
Fitted a model with MAP estimate = -274.3567
expansions: []
discards: [93]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 274.3680 - loglik: -2.7298e+02 - logprior: -1.1376e+00
Epoch 2/10
57/57 - 9s - loss: 269.7447 - loglik: -2.6826e+02 - logprior: -8.2343e-01
Epoch 3/10
57/57 - 9s - loss: 267.9478 - loglik: -2.6604e+02 - logprior: -8.0756e-01
Epoch 4/10
57/57 - 9s - loss: 266.4651 - loglik: -2.6449e+02 - logprior: -7.8908e-01
Epoch 5/10
57/57 - 9s - loss: 265.0260 - loglik: -2.6311e+02 - logprior: -7.5828e-01
Epoch 6/10
57/57 - 9s - loss: 265.1703 - loglik: -2.6324e+02 - logprior: -7.3381e-01
Fitted a model with MAP estimate = -262.9369
Time for alignment: 193.0711
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 372.6167 - loglik: -3.6961e+02 - logprior: -2.8798e+00
Epoch 2/10
20/20 - 4s - loss: 326.3716 - loglik: -3.2429e+02 - logprior: -1.3494e+00
Epoch 3/10
20/20 - 4s - loss: 306.7518 - loglik: -3.0384e+02 - logprior: -1.6566e+00
Epoch 4/10
20/20 - 4s - loss: 302.5258 - loglik: -2.9981e+02 - logprior: -1.6093e+00
Epoch 5/10
20/20 - 4s - loss: 300.7041 - loglik: -2.9806e+02 - logprior: -1.6100e+00
Epoch 6/10
20/20 - 4s - loss: 298.9439 - loglik: -2.9640e+02 - logprior: -1.6033e+00
Epoch 7/10
20/20 - 4s - loss: 298.4378 - loglik: -2.9601e+02 - logprior: -1.5804e+00
Epoch 8/10
20/20 - 4s - loss: 297.8918 - loglik: -2.9555e+02 - logprior: -1.5713e+00
Epoch 9/10
20/20 - 4s - loss: 297.3994 - loglik: -2.9514e+02 - logprior: -1.5715e+00
Epoch 10/10
20/20 - 4s - loss: 296.9170 - loglik: -2.9470e+02 - logprior: -1.5666e+00
Fitted a model with MAP estimate = -290.4928
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (37, 2), (39, 1), (44, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (64, 1), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 308.6442 - loglik: -3.0552e+02 - logprior: -2.8462e+00
Epoch 2/2
40/40 - 7s - loss: 289.7519 - loglik: -2.8779e+02 - logprior: -1.1023e+00
Fitted a model with MAP estimate = -273.9326
expansions: []
discards: [  8  13  46  76 103]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 294.6530 - loglik: -2.9247e+02 - logprior: -1.9937e+00
Epoch 2/2
40/40 - 7s - loss: 288.6071 - loglik: -2.8726e+02 - logprior: -8.8907e-01
Fitted a model with MAP estimate = -274.4870
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 274.4580 - loglik: -2.7307e+02 - logprior: -1.1555e+00
Epoch 2/10
57/57 - 10s - loss: 269.9392 - loglik: -2.6841e+02 - logprior: -8.7891e-01
Epoch 3/10
57/57 - 9s - loss: 267.5204 - loglik: -2.6556e+02 - logprior: -8.6908e-01
Epoch 4/10
57/57 - 9s - loss: 266.1777 - loglik: -2.6414e+02 - logprior: -8.5338e-01
Epoch 5/10
57/57 - 9s - loss: 265.5867 - loglik: -2.6359e+02 - logprior: -8.2483e-01
Epoch 6/10
57/57 - 9s - loss: 264.4695 - loglik: -2.6246e+02 - logprior: -8.0424e-01
Epoch 7/10
57/57 - 9s - loss: 264.0137 - loglik: -2.6215e+02 - logprior: -7.8217e-01
Epoch 8/10
57/57 - 9s - loss: 263.8392 - loglik: -2.6205e+02 - logprior: -7.5242e-01
Epoch 9/10
57/57 - 9s - loss: 262.7801 - loglik: -2.6106e+02 - logprior: -7.2712e-01
Epoch 10/10
57/57 - 10s - loss: 262.4821 - loglik: -2.6091e+02 - logprior: -7.0159e-01
Fitted a model with MAP estimate = -261.1324
Time for alignment: 234.8599
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 372.5718 - loglik: -3.6958e+02 - logprior: -2.8728e+00
Epoch 2/10
20/20 - 5s - loss: 327.6889 - loglik: -3.2566e+02 - logprior: -1.3303e+00
Epoch 3/10
20/20 - 4s - loss: 308.4394 - loglik: -3.0583e+02 - logprior: -1.6371e+00
Epoch 4/10
20/20 - 4s - loss: 302.5752 - loglik: -2.9983e+02 - logprior: -1.6320e+00
Epoch 5/10
20/20 - 5s - loss: 301.2099 - loglik: -2.9856e+02 - logprior: -1.6547e+00
Epoch 6/10
20/20 - 4s - loss: 299.7593 - loglik: -2.9725e+02 - logprior: -1.6168e+00
Epoch 7/10
20/20 - 4s - loss: 299.1039 - loglik: -2.9671e+02 - logprior: -1.5952e+00
Epoch 8/10
20/20 - 4s - loss: 299.1069 - loglik: -2.9678e+02 - logprior: -1.5900e+00
Fitted a model with MAP estimate = -289.3422
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (18, 1), (19, 1), (23, 2), (35, 1), (36, 2), (37, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 307.4399 - loglik: -3.0429e+02 - logprior: -2.8455e+00
Epoch 2/2
40/40 - 7s - loss: 289.8460 - loglik: -2.8776e+02 - logprior: -1.1281e+00
Fitted a model with MAP estimate = -273.7190
expansions: [(98, 1)]
discards: [  8  13  31  47  82 102]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 294.9589 - loglik: -2.9275e+02 - logprior: -2.0176e+00
Epoch 2/2
40/40 - 7s - loss: 288.6455 - loglik: -2.8677e+02 - logprior: -9.4903e-01
Fitted a model with MAP estimate = -273.9323
expansions: [(95, 1)]
discards: [98 99]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 274.8729 - loglik: -2.7331e+02 - logprior: -1.1644e+00
Epoch 2/10
57/57 - 9s - loss: 269.4677 - loglik: -2.6729e+02 - logprior: -8.8714e-01
Epoch 3/10
57/57 - 9s - loss: 268.0605 - loglik: -2.6569e+02 - logprior: -8.7359e-01
Epoch 4/10
57/57 - 9s - loss: 266.4658 - loglik: -2.6436e+02 - logprior: -8.6015e-01
Epoch 5/10
57/57 - 9s - loss: 265.6689 - loglik: -2.6364e+02 - logprior: -8.2850e-01
Epoch 6/10
57/57 - 9s - loss: 264.7335 - loglik: -2.6268e+02 - logprior: -8.1020e-01
Epoch 7/10
57/57 - 9s - loss: 264.5122 - loglik: -2.6268e+02 - logprior: -7.8634e-01
Epoch 8/10
57/57 - 9s - loss: 263.4729 - loglik: -2.6170e+02 - logprior: -7.5087e-01
Epoch 9/10
57/57 - 8s - loss: 262.7130 - loglik: -2.6098e+02 - logprior: -7.3398e-01
Epoch 10/10
57/57 - 9s - loss: 262.8387 - loglik: -2.6129e+02 - logprior: -6.9615e-01
Fitted a model with MAP estimate = -261.2317
Time for alignment: 223.3913
Computed alignments with likelihoods: ['-262.9369', '-261.1324', '-261.2317']
Best model has likelihood: -261.1324
time for generating output: 0.3635
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6555704697986577
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64ac56d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb641edf850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb641edf3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6757cfb20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6757cf790>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c4997910>, <__main__.SimpleDirichletPrior object at 0x7fb55869a190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 687.5135 - loglik: -6.7734e+02 - logprior: -1.0056e+01
Epoch 2/10
19/19 - 8s - loss: 621.2038 - loglik: -6.1938e+02 - logprior: -9.9775e-01
Epoch 3/10
19/19 - 8s - loss: 590.6602 - loglik: -5.8832e+02 - logprior: -1.0535e+00
Epoch 4/10
19/19 - 8s - loss: 582.5901 - loglik: -5.8005e+02 - logprior: -1.3233e+00
Epoch 5/10
19/19 - 8s - loss: 573.3257 - loglik: -5.7044e+02 - logprior: -1.2628e+00
Epoch 6/10
19/19 - 8s - loss: 573.9149 - loglik: -5.7081e+02 - logprior: -1.3455e+00
Fitted a model with MAP estimate = -569.1355
expansions: [(25, 1), (29, 1), (30, 2), (33, 17), (43, 5), (44, 2), (64, 3), (90, 1), (102, 7), (118, 1), (133, 1), (134, 1), (146, 2), (148, 1), (149, 3), (151, 1), (183, 1), (184, 2)]
discards: [  1 169 170 171 172 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 628.6926 - loglik: -6.1779e+02 - logprior: -1.0800e+01
Epoch 2/2
19/19 - 10s - loss: 586.0693 - loglik: -5.8377e+02 - logprior: -1.7272e+00
Fitted a model with MAP estimate = -578.4154
expansions: [(214, 7), (217, 1), (227, 1), (228, 1)]
discards: [ 32  33  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57 188 193 196 202 203 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 599.5161 - loglik: -5.9000e+02 - logprior: -9.4228e+00
Epoch 2/2
19/19 - 9s - loss: 580.4813 - loglik: -5.7937e+02 - logprior: -6.6375e-01
Fitted a model with MAP estimate = -574.5589
expansions: [(41, 1), (42, 1), (100, 2), (175, 1), (184, 1)]
discards: [ 50 180 181 192 193 194 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 594.9315 - loglik: -5.8587e+02 - logprior: -8.9783e+00
Epoch 2/10
19/19 - 9s - loss: 579.3682 - loglik: -5.7877e+02 - logprior: -1.6031e-01
Epoch 3/10
19/19 - 9s - loss: 570.3591 - loglik: -5.6962e+02 - logprior: 0.2636
Epoch 4/10
19/19 - 9s - loss: 565.2255 - loglik: -5.6422e+02 - logprior: 0.6229
Epoch 5/10
19/19 - 9s - loss: 562.2881 - loglik: -5.6109e+02 - logprior: 0.7097
Epoch 6/10
19/19 - 9s - loss: 557.9729 - loglik: -5.5673e+02 - logprior: 0.7052
Epoch 7/10
19/19 - 9s - loss: 554.2242 - loglik: -5.5311e+02 - logprior: 0.7487
Epoch 8/10
19/19 - 9s - loss: 554.3354 - loglik: -5.5353e+02 - logprior: 0.8694
Fitted a model with MAP estimate = -551.5096
Time for alignment: 197.0170
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 688.1915 - loglik: -6.7802e+02 - logprior: -1.0058e+01
Epoch 2/10
19/19 - 8s - loss: 621.1685 - loglik: -6.1938e+02 - logprior: -9.8181e-01
Epoch 3/10
19/19 - 8s - loss: 586.6670 - loglik: -5.8385e+02 - logprior: -1.5353e+00
Epoch 4/10
19/19 - 8s - loss: 579.7252 - loglik: -5.7634e+02 - logprior: -1.8143e+00
Epoch 5/10
19/19 - 8s - loss: 573.1476 - loglik: -5.6964e+02 - logprior: -1.6889e+00
Epoch 6/10
19/19 - 8s - loss: 571.0418 - loglik: -5.6748e+02 - logprior: -1.8025e+00
Epoch 7/10
19/19 - 8s - loss: 569.1725 - loglik: -5.6567e+02 - logprior: -1.8466e+00
Epoch 8/10
19/19 - 8s - loss: 566.9401 - loglik: -5.6349e+02 - logprior: -1.8931e+00
Epoch 9/10
19/19 - 8s - loss: 565.0015 - loglik: -5.6168e+02 - logprior: -1.8964e+00
Epoch 10/10
19/19 - 8s - loss: 563.7049 - loglik: -5.6050e+02 - logprior: -1.8804e+00
Fitted a model with MAP estimate = -563.4999
expansions: [(0, 2), (25, 1), (27, 1), (28, 1), (43, 6), (64, 2), (65, 2), (91, 1), (92, 1), (93, 2), (101, 1), (102, 2), (103, 1), (104, 1), (118, 1), (119, 1), (131, 1), (133, 3), (134, 1), (147, 1), (148, 3), (162, 1), (183, 1), (184, 2), (185, 1)]
discards: [  1 169 170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 653.4152 - loglik: -6.3822e+02 - logprior: -1.5086e+01
Epoch 2/2
19/19 - 9s - loss: 592.9152 - loglik: -5.8928e+02 - logprior: -3.0415e+00
Fitted a model with MAP estimate = -582.1928
expansions: [(45, 4), (210, 1)]
discards: [  0  30  52  53  75  76 106 108 181 182 183 184 185 186 187 188 189 190
 191]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 607.8564 - loglik: -5.9410e+02 - logprior: -1.3665e+01
Epoch 2/2
19/19 - 8s - loss: 586.1486 - loglik: -5.8229e+02 - logprior: -3.3539e+00
Fitted a model with MAP estimate = -578.1569
expansions: [(0, 2), (29, 1), (104, 2), (176, 6), (177, 3), (185, 9), (188, 1), (189, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 592.3860 - loglik: -5.8339e+02 - logprior: -8.9163e+00
Epoch 2/10
19/19 - 9s - loss: 572.1763 - loglik: -5.7178e+02 - logprior: 0.0242
Epoch 3/10
19/19 - 9s - loss: 563.7410 - loglik: -5.6329e+02 - logprior: 0.5887
Epoch 4/10
19/19 - 9s - loss: 559.2277 - loglik: -5.5864e+02 - logprior: 0.9618
Epoch 5/10
19/19 - 9s - loss: 555.1240 - loglik: -5.5448e+02 - logprior: 1.1298
Epoch 6/10
19/19 - 9s - loss: 551.2356 - loglik: -5.5054e+02 - logprior: 1.1593
Epoch 7/10
19/19 - 9s - loss: 550.1920 - loglik: -5.4956e+02 - logprior: 1.1960
Epoch 8/10
19/19 - 10s - loss: 547.3338 - loglik: -5.4677e+02 - logprior: 1.2246
Epoch 9/10
19/19 - 10s - loss: 546.7662 - loglik: -5.4631e+02 - logprior: 1.2827
Epoch 10/10
19/19 - 9s - loss: 544.5928 - loglik: -5.4430e+02 - logprior: 1.3772
Fitted a model with MAP estimate = -542.4593
Time for alignment: 249.8313
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 687.5161 - loglik: -6.7735e+02 - logprior: -1.0049e+01
Epoch 2/10
19/19 - 8s - loss: 619.9345 - loglik: -6.1825e+02 - logprior: -8.7529e-01
Epoch 3/10
19/19 - 8s - loss: 589.0164 - loglik: -5.8681e+02 - logprior: -9.5931e-01
Epoch 4/10
19/19 - 8s - loss: 580.9820 - loglik: -5.7839e+02 - logprior: -1.2887e+00
Epoch 5/10
19/19 - 8s - loss: 575.3478 - loglik: -5.7232e+02 - logprior: -1.2687e+00
Epoch 6/10
19/19 - 8s - loss: 571.0392 - loglik: -5.6779e+02 - logprior: -1.3132e+00
Epoch 7/10
19/19 - 8s - loss: 566.8166 - loglik: -5.6358e+02 - logprior: -1.4096e+00
Epoch 8/10
19/19 - 8s - loss: 567.4949 - loglik: -5.6438e+02 - logprior: -1.4940e+00
Fitted a model with MAP estimate = -564.3208
expansions: [(25, 1), (27, 1), (28, 2), (44, 6), (45, 1), (65, 3), (91, 6), (103, 1), (118, 1), (136, 2), (137, 2), (148, 2), (150, 4), (165, 6), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 643.3425 - loglik: -6.3228e+02 - logprior: -1.0953e+01
Epoch 2/2
19/19 - 9s - loss: 589.8541 - loglik: -5.8685e+02 - logprior: -2.3684e+00
Fitted a model with MAP estimate = -579.9626
expansions: [(202, 2), (215, 1)]
discards: [ 29 111 112 160 176 182 183 184 185 186 187 188 189 203 204 205 206 207]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 599.6662 - loglik: -5.8994e+02 - logprior: -9.6357e+00
Epoch 2/2
19/19 - 9s - loss: 580.9954 - loglik: -5.8003e+02 - logprior: -4.7328e-01
Fitted a model with MAP estimate = -575.2036
expansions: [(103, 2), (175, 5), (177, 1), (186, 8), (188, 1)]
discards: [53]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 590.8711 - loglik: -5.8196e+02 - logprior: -8.8343e+00
Epoch 2/10
19/19 - 9s - loss: 574.2491 - loglik: -5.7369e+02 - logprior: -1.6783e-01
Epoch 3/10
19/19 - 9s - loss: 566.9548 - loglik: -5.6637e+02 - logprior: 0.3696
Epoch 4/10
19/19 - 9s - loss: 559.8268 - loglik: -5.5907e+02 - logprior: 0.7057
Epoch 5/10
19/19 - 9s - loss: 554.8581 - loglik: -5.5393e+02 - logprior: 0.8272
Epoch 6/10
19/19 - 9s - loss: 552.9124 - loglik: -5.5180e+02 - logprior: 0.8030
Epoch 7/10
19/19 - 9s - loss: 549.5706 - loglik: -5.4849e+02 - logprior: 0.8321
Epoch 8/10
19/19 - 9s - loss: 547.3110 - loglik: -5.4646e+02 - logprior: 0.9522
Epoch 9/10
19/19 - 9s - loss: 547.5309 - loglik: -5.4694e+02 - logprior: 1.0662
Fitted a model with MAP estimate = -544.3915
Time for alignment: 224.0552
Computed alignments with likelihoods: ['-551.5096', '-542.4593', '-544.3915']
Best model has likelihood: -542.4593
time for generating output: 0.3391
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7252609603340292
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb674b14790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb641a5a520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb642765820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb697061d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6646519d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb65306cdc0>, <__main__.SimpleDirichletPrior object at 0x7fb61f1119d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.2969 - loglik: -1.5103e+02 - logprior: -3.2601e+00
Epoch 2/10
19/19 - 1s - loss: 120.8531 - loglik: -1.1930e+02 - logprior: -1.5219e+00
Epoch 3/10
19/19 - 1s - loss: 107.5149 - loglik: -1.0563e+02 - logprior: -1.5869e+00
Epoch 4/10
19/19 - 1s - loss: 104.5517 - loglik: -1.0252e+02 - logprior: -1.6999e+00
Epoch 5/10
19/19 - 1s - loss: 103.3943 - loglik: -1.0157e+02 - logprior: -1.5842e+00
Epoch 6/10
19/19 - 1s - loss: 102.9278 - loglik: -1.0113e+02 - logprior: -1.5788e+00
Epoch 7/10
19/19 - 1s - loss: 102.8253 - loglik: -1.0106e+02 - logprior: -1.5607e+00
Epoch 8/10
19/19 - 1s - loss: 102.3344 - loglik: -1.0057e+02 - logprior: -1.5528e+00
Epoch 9/10
19/19 - 1s - loss: 102.4329 - loglik: -1.0066e+02 - logprior: -1.5565e+00
Fitted a model with MAP estimate = -97.9576
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.2718 - loglik: -1.0301e+02 - logprior: -4.2038e+00
Epoch 2/2
19/19 - 1s - loss: 96.8541 - loglik: -9.4368e+01 - logprior: -2.3532e+00
Fitted a model with MAP estimate = -90.9297
expansions: [(3, 1)]
discards: [ 0 12 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 100.0003 - loglik: -9.5997e+01 - logprior: -3.9536e+00
Epoch 2/2
19/19 - 1s - loss: 95.5120 - loglik: -9.3887e+01 - logprior: -1.4846e+00
Fitted a model with MAP estimate = -90.4377
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 94.6211 - loglik: -9.0884e+01 - logprior: -3.6821e+00
Epoch 2/10
21/21 - 2s - loss: 90.4743 - loglik: -8.8489e+01 - logprior: -1.8516e+00
Epoch 3/10
21/21 - 2s - loss: 89.2207 - loglik: -8.7608e+01 - logprior: -1.4036e+00
Epoch 4/10
21/21 - 2s - loss: 88.6365 - loglik: -8.7057e+01 - logprior: -1.3156e+00
Epoch 5/10
21/21 - 2s - loss: 87.8095 - loglik: -8.6240e+01 - logprior: -1.2987e+00
Epoch 6/10
21/21 - 2s - loss: 87.7651 - loglik: -8.6201e+01 - logprior: -1.2945e+00
Epoch 7/10
21/21 - 2s - loss: 87.2936 - loglik: -8.5756e+01 - logprior: -1.2737e+00
Epoch 8/10
21/21 - 2s - loss: 87.4000 - loglik: -8.5865e+01 - logprior: -1.2642e+00
Fitted a model with MAP estimate = -86.8435
Time for alignment: 59.2718
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.3641 - loglik: -1.5110e+02 - logprior: -3.2566e+00
Epoch 2/10
19/19 - 1s - loss: 120.5691 - loglik: -1.1902e+02 - logprior: -1.5259e+00
Epoch 3/10
19/19 - 1s - loss: 106.0789 - loglik: -1.0423e+02 - logprior: -1.6161e+00
Epoch 4/10
19/19 - 1s - loss: 103.1213 - loglik: -1.0112e+02 - logprior: -1.7284e+00
Epoch 5/10
19/19 - 1s - loss: 102.4548 - loglik: -1.0064e+02 - logprior: -1.5956e+00
Epoch 6/10
19/19 - 1s - loss: 101.6686 - loglik: -9.9861e+01 - logprior: -1.6007e+00
Epoch 7/10
19/19 - 1s - loss: 101.6986 - loglik: -9.9916e+01 - logprior: -1.5804e+00
Fitted a model with MAP estimate = -97.2358
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.6952 - loglik: -1.0244e+02 - logprior: -4.2043e+00
Epoch 2/2
19/19 - 1s - loss: 96.7259 - loglik: -9.4284e+01 - logprior: -2.3218e+00
Fitted a model with MAP estimate = -90.6983
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.9832 - loglik: -9.5963e+01 - logprior: -3.9710e+00
Epoch 2/2
19/19 - 1s - loss: 95.3030 - loglik: -9.3713e+01 - logprior: -1.4600e+00
Fitted a model with MAP estimate = -90.3164
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.5612 - loglik: -9.0822e+01 - logprior: -3.6856e+00
Epoch 2/10
21/21 - 2s - loss: 90.3368 - loglik: -8.8360e+01 - logprior: -1.8466e+00
Epoch 3/10
21/21 - 2s - loss: 89.5349 - loglik: -8.7920e+01 - logprior: -1.4064e+00
Epoch 4/10
21/21 - 2s - loss: 88.1456 - loglik: -8.6561e+01 - logprior: -1.3241e+00
Epoch 5/10
21/21 - 2s - loss: 87.9508 - loglik: -8.6366e+01 - logprior: -1.3094e+00
Epoch 6/10
21/21 - 2s - loss: 87.8151 - loglik: -8.6258e+01 - logprior: -1.2889e+00
Epoch 7/10
21/21 - 2s - loss: 87.1168 - loglik: -8.5581e+01 - logprior: -1.2708e+00
Epoch 8/10
21/21 - 2s - loss: 87.6028 - loglik: -8.6072e+01 - logprior: -1.2493e+00
Fitted a model with MAP estimate = -86.8550
Time for alignment: 53.3527
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.4495 - loglik: -1.5118e+02 - logprior: -3.2596e+00
Epoch 2/10
19/19 - 2s - loss: 121.6354 - loglik: -1.2010e+02 - logprior: -1.5121e+00
Epoch 3/10
19/19 - 1s - loss: 108.2506 - loglik: -1.0643e+02 - logprior: -1.5825e+00
Epoch 4/10
19/19 - 1s - loss: 104.5809 - loglik: -1.0254e+02 - logprior: -1.6923e+00
Epoch 5/10
19/19 - 1s - loss: 103.4753 - loglik: -1.0164e+02 - logprior: -1.5713e+00
Epoch 6/10
19/19 - 1s - loss: 102.9110 - loglik: -1.0109e+02 - logprior: -1.5782e+00
Epoch 7/10
19/19 - 2s - loss: 102.7734 - loglik: -1.0098e+02 - logprior: -1.5646e+00
Epoch 8/10
19/19 - 1s - loss: 102.7384 - loglik: -1.0096e+02 - logprior: -1.5586e+00
Epoch 9/10
19/19 - 2s - loss: 102.1900 - loglik: -1.0041e+02 - logprior: -1.5582e+00
Epoch 10/10
19/19 - 1s - loss: 102.3112 - loglik: -1.0053e+02 - logprior: -1.5601e+00
Fitted a model with MAP estimate = -97.9054
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.9218 - loglik: -1.0267e+02 - logprior: -4.1911e+00
Epoch 2/2
19/19 - 1s - loss: 96.6781 - loglik: -9.4208e+01 - logprior: -2.3326e+00
Fitted a model with MAP estimate = -90.8383
expansions: [(3, 1)]
discards: [ 0 35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 99.8370 - loglik: -9.5819e+01 - logprior: -3.9679e+00
Epoch 2/2
19/19 - 1s - loss: 95.3745 - loglik: -9.3743e+01 - logprior: -1.4973e+00
Fitted a model with MAP estimate = -90.5299
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.6492 - loglik: -9.0919e+01 - logprior: -3.6786e+00
Epoch 2/10
21/21 - 2s - loss: 90.4411 - loglik: -8.8444e+01 - logprior: -1.8682e+00
Epoch 3/10
21/21 - 2s - loss: 89.2440 - loglik: -8.7625e+01 - logprior: -1.4097e+00
Epoch 4/10
21/21 - 2s - loss: 88.4190 - loglik: -8.6850e+01 - logprior: -1.3136e+00
Epoch 5/10
21/21 - 2s - loss: 88.2329 - loglik: -8.6669e+01 - logprior: -1.2934e+00
Epoch 6/10
21/21 - 2s - loss: 87.3018 - loglik: -8.5739e+01 - logprior: -1.2930e+00
Epoch 7/10
21/21 - 2s - loss: 87.5330 - loglik: -8.6007e+01 - logprior: -1.2645e+00
Fitted a model with MAP estimate = -86.9913
Time for alignment: 58.2657
Computed alignments with likelihoods: ['-86.8435', '-86.8550', '-86.9913']
Best model has likelihood: -86.8435
time for generating output: 0.1316
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9162910495814552
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6058b7c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb55888fa90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6977a34c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686ed5070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb67dcab400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb65b904f70>, <__main__.SimpleDirichletPrior object at 0x7fb65be515e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 417.9101 - loglik: -4.1300e+02 - logprior: -4.8504e+00
Epoch 2/10
27/27 - 5s - loss: 330.5719 - loglik: -3.2817e+02 - logprior: -2.1981e+00
Epoch 3/10
27/27 - 5s - loss: 315.6080 - loglik: -3.1277e+02 - logprior: -2.3607e+00
Epoch 4/10
27/27 - 5s - loss: 312.9607 - loglik: -3.1027e+02 - logprior: -2.2661e+00
Epoch 5/10
27/27 - 5s - loss: 312.2605 - loglik: -3.0961e+02 - logprior: -2.2157e+00
Epoch 6/10
27/27 - 5s - loss: 311.9185 - loglik: -3.0929e+02 - logprior: -2.1975e+00
Epoch 7/10
27/27 - 5s - loss: 311.4295 - loglik: -3.0881e+02 - logprior: -2.1854e+00
Epoch 8/10
27/27 - 5s - loss: 311.5257 - loglik: -3.0890e+02 - logprior: -2.1829e+00
Fitted a model with MAP estimate = -310.5169
expansions: [(0, 2), (19, 1), (22, 1), (25, 2), (34, 1), (37, 1), (38, 4), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (68, 1), (69, 3), (70, 1), (71, 2), (73, 1), (78, 1), (81, 1), (82, 2), (100, 1), (102, 2), (103, 1), (104, 2), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 308.3760 - loglik: -3.0162e+02 - logprior: -6.5920e+00
Epoch 2/2
27/27 - 6s - loss: 288.8026 - loglik: -2.8664e+02 - logprior: -1.7103e+00
Fitted a model with MAP estimate = -286.1189
expansions: [(42, 1), (130, 1)]
discards: [  0  29  47  88  91 109 132 137]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 298.6145 - loglik: -2.9202e+02 - logprior: -6.4416e+00
Epoch 2/2
27/27 - 5s - loss: 288.9847 - loglik: -2.8713e+02 - logprior: -1.3818e+00
Fitted a model with MAP estimate = -286.4175
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 293.2175 - loglik: -2.8882e+02 - logprior: -4.2513e+00
Epoch 2/10
27/27 - 5s - loss: 287.5058 - loglik: -2.8607e+02 - logprior: -9.5385e-01
Epoch 3/10
27/27 - 6s - loss: 285.9233 - loglik: -2.8463e+02 - logprior: -7.1233e-01
Epoch 4/10
27/27 - 6s - loss: 284.7831 - loglik: -2.8368e+02 - logprior: -5.8193e-01
Epoch 5/10
27/27 - 6s - loss: 285.2733 - loglik: -2.8429e+02 - logprior: -4.8215e-01
Fitted a model with MAP estimate = -283.8888
Time for alignment: 136.4634
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.5172 - loglik: -4.1261e+02 - logprior: -4.8513e+00
Epoch 2/10
27/27 - 5s - loss: 333.2954 - loglik: -3.3071e+02 - logprior: -2.3150e+00
Epoch 3/10
27/27 - 5s - loss: 317.1573 - loglik: -3.1420e+02 - logprior: -2.4440e+00
Epoch 4/10
27/27 - 5s - loss: 315.3465 - loglik: -3.1258e+02 - logprior: -2.2942e+00
Epoch 5/10
27/27 - 5s - loss: 313.5346 - loglik: -3.1083e+02 - logprior: -2.2701e+00
Epoch 6/10
27/27 - 5s - loss: 312.5208 - loglik: -3.0981e+02 - logprior: -2.2599e+00
Epoch 7/10
27/27 - 5s - loss: 311.5711 - loglik: -3.0886e+02 - logprior: -2.2562e+00
Epoch 8/10
27/27 - 5s - loss: 311.9511 - loglik: -3.0924e+02 - logprior: -2.2432e+00
Fitted a model with MAP estimate = -310.7248
expansions: [(0, 2), (9, 1), (21, 1), (24, 2), (26, 1), (33, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (68, 1), (69, 2), (71, 2), (73, 1), (78, 1), (82, 2), (94, 2), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 308.4919 - loglik: -3.0162e+02 - logprior: -6.6991e+00
Epoch 2/2
27/27 - 6s - loss: 288.8254 - loglik: -2.8667e+02 - logprior: -1.6593e+00
Fitted a model with MAP estimate = -286.1174
expansions: [(88, 1)]
discards: [  0  28  48  49 108 135 137]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 299.4093 - loglik: -2.9275e+02 - logprior: -6.5125e+00
Epoch 2/2
27/27 - 6s - loss: 290.0618 - loglik: -2.8814e+02 - logprior: -1.4453e+00
Fitted a model with MAP estimate = -287.8997
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 295.2091 - loglik: -2.9077e+02 - logprior: -4.2951e+00
Epoch 2/10
27/27 - 6s - loss: 289.2595 - loglik: -2.8781e+02 - logprior: -9.5911e-01
Epoch 3/10
27/27 - 6s - loss: 287.6642 - loglik: -2.8630e+02 - logprior: -7.5462e-01
Epoch 4/10
27/27 - 6s - loss: 286.6155 - loglik: -2.8542e+02 - logprior: -6.2697e-01
Epoch 5/10
27/27 - 6s - loss: 286.1080 - loglik: -2.8505e+02 - logprior: -5.2238e-01
Epoch 6/10
27/27 - 6s - loss: 285.7697 - loglik: -2.8483e+02 - logprior: -4.3128e-01
Epoch 7/10
27/27 - 6s - loss: 285.3915 - loglik: -2.8458e+02 - logprior: -3.1283e-01
Epoch 8/10
27/27 - 6s - loss: 285.6335 - loglik: -2.8494e+02 - logprior: -2.0727e-01
Fitted a model with MAP estimate = -284.5449
Time for alignment: 153.6703
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 416.9142 - loglik: -4.1199e+02 - logprior: -4.8628e+00
Epoch 2/10
27/27 - 5s - loss: 328.7841 - loglik: -3.2634e+02 - logprior: -2.2392e+00
Epoch 3/10
27/27 - 5s - loss: 316.1305 - loglik: -3.1336e+02 - logprior: -2.3365e+00
Epoch 4/10
27/27 - 5s - loss: 313.7405 - loglik: -3.1108e+02 - logprior: -2.2508e+00
Epoch 5/10
27/27 - 5s - loss: 312.8516 - loglik: -3.1025e+02 - logprior: -2.1989e+00
Epoch 6/10
27/27 - 5s - loss: 311.7917 - loglik: -3.0917e+02 - logprior: -2.2040e+00
Epoch 7/10
27/27 - 5s - loss: 310.8172 - loglik: -3.0814e+02 - logprior: -2.2483e+00
Epoch 8/10
27/27 - 6s - loss: 310.7517 - loglik: -3.0807e+02 - logprior: -2.2508e+00
Epoch 9/10
27/27 - 5s - loss: 310.4844 - loglik: -3.0780e+02 - logprior: -2.2600e+00
Epoch 10/10
27/27 - 5s - loss: 310.6462 - loglik: -3.0797e+02 - logprior: -2.2472e+00
Fitted a model with MAP estimate = -309.8531
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 3), (99, 1), (100, 1), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 307.8217 - loglik: -3.0092e+02 - logprior: -6.7368e+00
Epoch 2/2
27/27 - 6s - loss: 288.3830 - loglik: -2.8633e+02 - logprior: -1.5609e+00
Fitted a model with MAP estimate = -285.9597
expansions: []
discards: [ 12  30  48  49  50  67 111 139]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 294.6157 - loglik: -2.9022e+02 - logprior: -4.2457e+00
Epoch 2/2
27/27 - 6s - loss: 289.0664 - loglik: -2.8758e+02 - logprior: -1.0079e+00
Fitted a model with MAP estimate = -287.0294
expansions: []
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 295.7119 - loglik: -2.9158e+02 - logprior: -3.9840e+00
Epoch 2/10
27/27 - 6s - loss: 289.6203 - loglik: -2.8822e+02 - logprior: -9.2707e-01
Epoch 3/10
27/27 - 6s - loss: 288.4239 - loglik: -2.8716e+02 - logprior: -6.6711e-01
Epoch 4/10
27/27 - 6s - loss: 287.4430 - loglik: -2.8631e+02 - logprior: -5.7996e-01
Epoch 5/10
27/27 - 6s - loss: 286.4905 - loglik: -2.8550e+02 - logprior: -4.7766e-01
Epoch 6/10
27/27 - 6s - loss: 286.8282 - loglik: -2.8596e+02 - logprior: -3.8118e-01
Fitted a model with MAP estimate = -285.8412
Time for alignment: 153.0651
Computed alignments with likelihoods: ['-283.8888', '-284.5449', '-285.8412']
Best model has likelihood: -283.8888
time for generating output: 0.3214
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5681063122923588
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64aab8f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60eab61f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60eab6580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60306b2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4678340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb686300640>, <__main__.SimpleDirichletPrior object at 0x7fb67502aee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 767.0577 - loglik: -7.6448e+02 - logprior: -2.3211e+00
Epoch 2/10
34/34 - 17s - loss: 639.5901 - loglik: -6.3670e+02 - logprior: -2.1229e+00
Epoch 3/10
34/34 - 17s - loss: 621.5248 - loglik: -6.1808e+02 - logprior: -2.3582e+00
Epoch 4/10
34/34 - 16s - loss: 616.6745 - loglik: -6.1354e+02 - logprior: -2.2586e+00
Epoch 5/10
34/34 - 16s - loss: 616.3374 - loglik: -6.1333e+02 - logprior: -2.2455e+00
Epoch 6/10
34/34 - 17s - loss: 614.8550 - loglik: -6.1190e+02 - logprior: -2.2503e+00
Epoch 7/10
34/34 - 17s - loss: 614.3959 - loglik: -6.1146e+02 - logprior: -2.2594e+00
Epoch 8/10
34/34 - 17s - loss: 614.5081 - loglik: -6.1158e+02 - logprior: -2.2674e+00
Fitted a model with MAP estimate = -613.2256
expansions: [(9, 1), (13, 1), (14, 1), (15, 3), (16, 1), (17, 5), (22, 1), (29, 3), (42, 1), (51, 1), (56, 3), (57, 1), (61, 1), (65, 1), (68, 1), (92, 1), (94, 1), (95, 2), (96, 2), (99, 1), (100, 2), (102, 1), (104, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 1), (145, 2), (148, 1), (165, 4), (168, 1), (172, 1), (173, 1), (174, 1), (178, 1), (185, 3), (186, 2), (189, 1), (190, 1), (199, 2), (203, 1), (208, 1), (226, 1), (228, 1), (229, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 602.4829 - loglik: -5.9853e+02 - logprior: -3.7070e+00
Epoch 2/2
34/34 - 23s - loss: 576.1829 - loglik: -5.7362e+02 - logprior: -1.9172e+00
Fitted a model with MAP estimate = -569.7294
expansions: [(25, 1), (43, 1), (238, 1)]
discards: [ 17  23 122 124 211 260]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 580.5942 - loglik: -5.7803e+02 - logprior: -2.3358e+00
Epoch 2/2
34/34 - 23s - loss: 568.5399 - loglik: -5.6693e+02 - logprior: -7.7952e-01
Fitted a model with MAP estimate = -563.4775
expansions: [(209, 2), (230, 1)]
discards: [259 260 261]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 28s - loss: 576.4996 - loglik: -5.7413e+02 - logprior: -2.1410e+00
Epoch 2/10
34/34 - 23s - loss: 567.2632 - loglik: -5.6604e+02 - logprior: -4.5700e-01
Epoch 3/10
34/34 - 23s - loss: 565.4965 - loglik: -5.6418e+02 - logprior: -2.8860e-01
Epoch 4/10
34/34 - 23s - loss: 564.3958 - loglik: -5.6326e+02 - logprior: -1.7451e-01
Epoch 5/10
34/34 - 23s - loss: 562.9701 - loglik: -5.6212e+02 - logprior: 0.0014
Epoch 6/10
34/34 - 23s - loss: 561.9618 - loglik: -5.6127e+02 - logprior: 0.1105
Epoch 7/10
34/34 - 23s - loss: 562.2056 - loglik: -5.6169e+02 - logprior: 0.2799
Fitted a model with MAP estimate = -560.7192
Time for alignment: 510.7592
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 766.7298 - loglik: -7.6414e+02 - logprior: -2.3305e+00
Epoch 2/10
34/34 - 17s - loss: 641.3636 - loglik: -6.3881e+02 - logprior: -2.0958e+00
Epoch 3/10
34/34 - 17s - loss: 620.6392 - loglik: -6.1736e+02 - logprior: -2.3853e+00
Epoch 4/10
34/34 - 16s - loss: 617.8351 - loglik: -6.1472e+02 - logprior: -2.3096e+00
Epoch 5/10
34/34 - 17s - loss: 615.5083 - loglik: -6.1245e+02 - logprior: -2.3060e+00
Epoch 6/10
34/34 - 17s - loss: 615.4846 - loglik: -6.1247e+02 - logprior: -2.3205e+00
Epoch 7/10
34/34 - 17s - loss: 615.2506 - loglik: -6.1222e+02 - logprior: -2.3308e+00
Epoch 8/10
34/34 - 17s - loss: 614.3648 - loglik: -6.1132e+02 - logprior: -2.3629e+00
Epoch 9/10
34/34 - 17s - loss: 613.8483 - loglik: -6.1081e+02 - logprior: -2.3655e+00
Epoch 10/10
34/34 - 17s - loss: 613.5902 - loglik: -6.1056e+02 - logprior: -2.3629e+00
Fitted a model with MAP estimate = -612.7133
expansions: [(12, 1), (13, 1), (14, 1), (15, 3), (16, 1), (17, 4), (18, 2), (20, 1), (22, 2), (29, 2), (38, 1), (50, 3), (54, 2), (55, 3), (64, 1), (66, 1), (70, 1), (73, 2), (89, 1), (94, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (172, 2), (173, 1), (176, 1), (178, 1), (186, 3), (188, 1), (190, 1), (202, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 28s - loss: 600.4754 - loglik: -5.9749e+02 - logprior: -2.7305e+00
Epoch 2/2
34/34 - 23s - loss: 575.6214 - loglik: -5.7367e+02 - logprior: -1.2641e+00
Fitted a model with MAP estimate = -570.4880
expansions: [(193, 1), (241, 3)]
discards: [ 18  26  27  35  44  77  78  79  80 102 103 126 244]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 588.3207 - loglik: -5.8566e+02 - logprior: -2.4258e+00
Epoch 2/2
34/34 - 23s - loss: 578.4840 - loglik: -5.7686e+02 - logprior: -7.8654e-01
Fitted a model with MAP estimate = -574.5213
expansions: [(231, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 26s - loss: 584.9406 - loglik: -5.8254e+02 - logprior: -2.1801e+00
Epoch 2/10
34/34 - 22s - loss: 576.8162 - loglik: -5.7575e+02 - logprior: -4.5803e-01
Epoch 3/10
34/34 - 23s - loss: 573.6091 - loglik: -5.7250e+02 - logprior: -3.7608e-01
Epoch 4/10
34/34 - 23s - loss: 572.3489 - loglik: -5.7124e+02 - logprior: -2.9148e-01
Epoch 5/10
34/34 - 23s - loss: 571.3610 - loglik: -5.7044e+02 - logprior: -1.1257e-01
Epoch 6/10
34/34 - 23s - loss: 568.3029 - loglik: -5.6751e+02 - logprior: 0.0059
Epoch 7/10
34/34 - 22s - loss: 565.1600 - loglik: -5.6452e+02 - logprior: 0.1910
Epoch 8/10
34/34 - 23s - loss: 564.2453 - loglik: -5.6384e+02 - logprior: 0.3724
Epoch 9/10
34/34 - 23s - loss: 562.7951 - loglik: -5.6256e+02 - logprior: 0.5420
Epoch 10/10
34/34 - 22s - loss: 562.6691 - loglik: -5.6258e+02 - logprior: 0.6681
Fitted a model with MAP estimate = -561.2360
Time for alignment: 606.0563
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 769.4341 - loglik: -7.6685e+02 - logprior: -2.3179e+00
Epoch 2/10
34/34 - 17s - loss: 647.0847 - loglik: -6.4458e+02 - logprior: -1.9713e+00
Epoch 3/10
34/34 - 16s - loss: 623.8203 - loglik: -6.2072e+02 - logprior: -2.2026e+00
Epoch 4/10
34/34 - 17s - loss: 620.6364 - loglik: -6.1765e+02 - logprior: -2.1572e+00
Epoch 5/10
34/34 - 17s - loss: 616.8729 - loglik: -6.1397e+02 - logprior: -2.1530e+00
Epoch 6/10
34/34 - 17s - loss: 616.3300 - loglik: -6.1347e+02 - logprior: -2.1723e+00
Epoch 7/10
34/34 - 17s - loss: 616.4659 - loglik: -6.1361e+02 - logprior: -2.1713e+00
Fitted a model with MAP estimate = -614.9168
expansions: [(13, 1), (14, 1), (16, 3), (18, 5), (30, 1), (31, 2), (35, 1), (49, 1), (52, 2), (56, 5), (57, 1), (61, 1), (65, 2), (66, 1), (70, 1), (73, 2), (92, 1), (95, 1), (96, 1), (100, 1), (101, 2), (103, 1), (106, 1), (111, 1), (131, 1), (132, 1), (135, 1), (140, 1), (143, 1), (144, 3), (145, 1), (148, 1), (165, 3), (168, 1), (172, 1), (173, 2), (175, 1), (176, 1), (183, 1), (186, 3), (188, 1), (189, 1), (203, 1), (208, 1), (211, 1), (212, 2), (224, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 28s - loss: 605.3463 - loglik: -6.0123e+02 - logprior: -3.8608e+00
Epoch 2/2
34/34 - 23s - loss: 578.2767 - loglik: -5.7544e+02 - logprior: -2.1722e+00
Fitted a model with MAP estimate = -571.6719
expansions: [(73, 2), (215, 1), (243, 3)]
discards: [ 23  24  40  66  74  75  76  77 100 101 278]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 587.0492 - loglik: -5.8424e+02 - logprior: -2.5767e+00
Epoch 2/2
34/34 - 23s - loss: 573.2612 - loglik: -5.7146e+02 - logprior: -9.5515e-01
Fitted a model with MAP estimate = -568.2256
expansions: [(71, 1), (94, 2), (103, 1), (182, 1), (237, 1)]
discards: [18 82]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 28s - loss: 578.4427 - loglik: -5.7600e+02 - logprior: -2.2148e+00
Epoch 2/10
34/34 - 23s - loss: 566.9567 - loglik: -5.6566e+02 - logprior: -5.5221e-01
Epoch 3/10
34/34 - 23s - loss: 565.2236 - loglik: -5.6375e+02 - logprior: -4.1091e-01
Epoch 4/10
34/34 - 24s - loss: 564.0340 - loglik: -5.6278e+02 - logprior: -2.5619e-01
Epoch 5/10
34/34 - 23s - loss: 562.4684 - loglik: -5.6144e+02 - logprior: -1.4030e-01
Epoch 6/10
34/34 - 23s - loss: 561.6534 - loglik: -5.6086e+02 - logprior: 0.0082
Epoch 7/10
34/34 - 23s - loss: 561.5269 - loglik: -5.6093e+02 - logprior: 0.1793
Epoch 8/10
34/34 - 23s - loss: 560.7902 - loglik: -5.6036e+02 - logprior: 0.3217
Epoch 9/10
34/34 - 23s - loss: 561.6439 - loglik: -5.6131e+02 - logprior: 0.4301
Fitted a model with MAP estimate = -559.5567
Time for alignment: 539.1672
Computed alignments with likelihoods: ['-560.7192', '-561.2360', '-559.5567']
Best model has likelihood: -559.5567
time for generating output: 0.4282
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.6454320987654321
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6c47ed9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb69fe73550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c5231880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c464ba90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c464ba30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb604209d60>, <__main__.SimpleDirichletPrior object at 0x7fb560e66af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4495 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7513 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7697 - loglik: -1.3251e+02 - logprior: -7.2409e+00
Epoch 5/10
10/10 - 1s - loss: 132.1185 - loglik: -1.2713e+02 - logprior: -4.9628e+00
Epoch 6/10
10/10 - 1s - loss: 128.1157 - loglik: -1.2415e+02 - logprior: -3.7759e+00
Epoch 7/10
10/10 - 1s - loss: 126.5198 - loglik: -1.2326e+02 - logprior: -2.8645e+00
Epoch 8/10
10/10 - 1s - loss: 125.6908 - loglik: -1.2294e+02 - logprior: -2.3424e+00
Epoch 9/10
10/10 - 1s - loss: 125.0978 - loglik: -1.2272e+02 - logprior: -2.0204e+00
Epoch 10/10
10/10 - 1s - loss: 124.6525 - loglik: -1.2251e+02 - logprior: -1.7971e+00
Fitted a model with MAP estimate = -124.1338
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 222.5541 - loglik: -1.2290e+02 - logprior: -9.9625e+01
Epoch 2/2
10/10 - 1s - loss: 157.3687 - loglik: -1.1602e+02 - logprior: -4.1187e+01
Fitted a model with MAP estimate = -145.4529
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.2508 - loglik: -1.1260e+02 - logprior: -7.9625e+01
Epoch 2/2
10/10 - 1s - loss: 131.4539 - loglik: -1.0971e+02 - logprior: -2.1604e+01
Fitted a model with MAP estimate = -121.9801
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.6675 - loglik: -1.0952e+02 - logprior: -7.8122e+01
Epoch 2/10
10/10 - 1s - loss: 129.3477 - loglik: -1.0835e+02 - logprior: -2.0878e+01
Epoch 3/10
10/10 - 1s - loss: 116.8721 - loglik: -1.0744e+02 - logprior: -9.1553e+00
Epoch 4/10
10/10 - 1s - loss: 111.9798 - loglik: -1.0748e+02 - logprior: -4.1772e+00
Epoch 5/10
10/10 - 1s - loss: 109.4674 - loglik: -1.0770e+02 - logprior: -1.3974e+00
Epoch 6/10
10/10 - 1s - loss: 108.1225 - loglik: -1.0794e+02 - logprior: 0.2117
Epoch 7/10
10/10 - 1s - loss: 107.3319 - loglik: -1.0811e+02 - logprior: 1.1829
Epoch 8/10
10/10 - 1s - loss: 106.8101 - loglik: -1.0822e+02 - logprior: 1.8230
Epoch 9/10
10/10 - 1s - loss: 106.4214 - loglik: -1.0831e+02 - logprior: 2.3048
Epoch 10/10
10/10 - 1s - loss: 106.1038 - loglik: -1.0835e+02 - logprior: 2.6760
Fitted a model with MAP estimate = -105.5134
Time for alignment: 36.6024
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7512 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7707 - loglik: -1.3251e+02 - logprior: -7.2403e+00
Epoch 5/10
10/10 - 1s - loss: 132.2090 - loglik: -1.2727e+02 - logprior: -4.9280e+00
Epoch 6/10
10/10 - 1s - loss: 128.0641 - loglik: -1.2419e+02 - logprior: -3.7520e+00
Epoch 7/10
10/10 - 1s - loss: 126.3608 - loglik: -1.2314e+02 - logprior: -2.8871e+00
Epoch 8/10
10/10 - 1s - loss: 125.5429 - loglik: -1.2281e+02 - logprior: -2.3414e+00
Epoch 9/10
10/10 - 1s - loss: 125.0235 - loglik: -1.2269e+02 - logprior: -2.0011e+00
Epoch 10/10
10/10 - 1s - loss: 124.6711 - loglik: -1.2261e+02 - logprior: -1.7713e+00
Fitted a model with MAP estimate = -124.2333
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.5334 - loglik: -1.2289e+02 - logprior: -9.9614e+01
Epoch 2/2
10/10 - 1s - loss: 157.3593 - loglik: -1.1601e+02 - logprior: -4.1188e+01
Fitted a model with MAP estimate = -145.4476
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.2552 - loglik: -1.1261e+02 - logprior: -7.9620e+01
Epoch 2/2
10/10 - 1s - loss: 131.4590 - loglik: -1.0971e+02 - logprior: -2.1610e+01
Fitted a model with MAP estimate = -121.9852
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.6632 - loglik: -1.0952e+02 - logprior: -7.8123e+01
Epoch 2/10
10/10 - 1s - loss: 129.3124 - loglik: -1.0831e+02 - logprior: -2.0882e+01
Epoch 3/10
10/10 - 1s - loss: 116.8676 - loglik: -1.0744e+02 - logprior: -9.1542e+00
Epoch 4/10
10/10 - 1s - loss: 111.9805 - loglik: -1.0748e+02 - logprior: -4.1756e+00
Epoch 5/10
10/10 - 1s - loss: 109.4683 - loglik: -1.0770e+02 - logprior: -1.3973e+00
Epoch 6/10
10/10 - 1s - loss: 108.1220 - loglik: -1.0794e+02 - logprior: 0.2126
Epoch 7/10
10/10 - 1s - loss: 107.3310 - loglik: -1.0811e+02 - logprior: 1.1828
Epoch 8/10
10/10 - 1s - loss: 106.8089 - loglik: -1.0822e+02 - logprior: 1.8247
Epoch 9/10
10/10 - 1s - loss: 106.4195 - loglik: -1.0831e+02 - logprior: 2.3065
Epoch 10/10
10/10 - 1s - loss: 106.1015 - loglik: -1.0835e+02 - logprior: 2.6786
Fitted a model with MAP estimate = -105.5111
Time for alignment: 34.2777
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7512 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7708 - loglik: -1.3251e+02 - logprior: -7.2403e+00
Epoch 5/10
10/10 - 1s - loss: 132.2459 - loglik: -1.2731e+02 - logprior: -4.9193e+00
Epoch 6/10
10/10 - 1s - loss: 128.1159 - loglik: -1.2427e+02 - logprior: -3.7400e+00
Epoch 7/10
10/10 - 1s - loss: 126.3650 - loglik: -1.2316e+02 - logprior: -2.8829e+00
Epoch 8/10
10/10 - 1s - loss: 125.5483 - loglik: -1.2282e+02 - logprior: -2.3409e+00
Epoch 9/10
10/10 - 1s - loss: 125.0249 - loglik: -1.2269e+02 - logprior: -2.0010e+00
Epoch 10/10
10/10 - 1s - loss: 124.6671 - loglik: -1.2259e+02 - logprior: -1.7764e+00
Fitted a model with MAP estimate = -124.2222
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.5327 - loglik: -1.2289e+02 - logprior: -9.9619e+01
Epoch 2/2
10/10 - 1s - loss: 157.3567 - loglik: -1.1601e+02 - logprior: -4.1188e+01
Fitted a model with MAP estimate = -145.4467
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.2535 - loglik: -1.1261e+02 - logprior: -7.9620e+01
Epoch 2/2
10/10 - 1s - loss: 131.4554 - loglik: -1.0971e+02 - logprior: -2.1611e+01
Fitted a model with MAP estimate = -121.9783
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.6644 - loglik: -1.0952e+02 - logprior: -7.8124e+01
Epoch 2/10
10/10 - 1s - loss: 129.3106 - loglik: -1.0830e+02 - logprior: -2.0883e+01
Epoch 3/10
10/10 - 1s - loss: 116.8550 - loglik: -1.0742e+02 - logprior: -9.1587e+00
Epoch 4/10
10/10 - 1s - loss: 111.9589 - loglik: -1.0744e+02 - logprior: -4.1891e+00
Epoch 5/10
10/10 - 1s - loss: 109.4603 - loglik: -1.0768e+02 - logprior: -1.4002e+00
Epoch 6/10
10/10 - 1s - loss: 108.1177 - loglik: -1.0794e+02 - logprior: 0.2164
Epoch 7/10
10/10 - 1s - loss: 107.3271 - loglik: -1.0811e+02 - logprior: 1.1854
Epoch 8/10
10/10 - 1s - loss: 106.8052 - loglik: -1.0822e+02 - logprior: 1.8256
Epoch 9/10
10/10 - 1s - loss: 106.4160 - loglik: -1.0830e+02 - logprior: 2.3079
Epoch 10/10
10/10 - 1s - loss: 106.0971 - loglik: -1.0835e+02 - logprior: 2.6800
Fitted a model with MAP estimate = -105.5048
Time for alignment: 35.3052
Computed alignments with likelihoods: ['-105.5134', '-105.5111', '-105.5048']
Best model has likelihood: -105.5048
time for generating output: 0.1296
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.9205930507549572
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb605edf400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57b04b550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57b04b580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6058b0af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb674a884c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6425d2490>, <__main__.SimpleDirichletPrior object at 0x7fb524f4a610>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.6457 - loglik: -3.0879e+02 - logprior: -7.8143e+00
Epoch 2/10
13/13 - 2s - loss: 284.9548 - loglik: -2.8282e+02 - logprior: -1.9909e+00
Epoch 3/10
13/13 - 2s - loss: 261.1535 - loglik: -2.5912e+02 - logprior: -1.8264e+00
Epoch 4/10
13/13 - 2s - loss: 250.7461 - loglik: -2.4811e+02 - logprior: -2.1605e+00
Epoch 5/10
13/13 - 2s - loss: 247.8445 - loglik: -2.4508e+02 - logprior: -2.0984e+00
Epoch 6/10
13/13 - 2s - loss: 247.3049 - loglik: -2.4467e+02 - logprior: -2.0030e+00
Epoch 7/10
13/13 - 2s - loss: 246.0541 - loglik: -2.4340e+02 - logprior: -2.0171e+00
Epoch 8/10
13/13 - 2s - loss: 245.9852 - loglik: -2.4335e+02 - logprior: -2.0123e+00
Epoch 9/10
13/13 - 2s - loss: 245.0406 - loglik: -2.4247e+02 - logprior: -1.9852e+00
Epoch 10/10
13/13 - 2s - loss: 244.8896 - loglik: -2.4232e+02 - logprior: -1.9831e+00
Fitted a model with MAP estimate = -244.1075
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (28, 2), (29, 1), (30, 3), (31, 1), (32, 1), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 266.1557 - loglik: -2.5680e+02 - logprior: -9.3188e+00
Epoch 2/2
13/13 - 2s - loss: 249.8099 - loglik: -2.4535e+02 - logprior: -4.2821e+00
Fitted a model with MAP estimate = -246.3139
expansions: [(0, 2)]
discards: [ 0 90 96]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.4472 - loglik: -2.4329e+02 - logprior: -7.1147e+00
Epoch 2/2
13/13 - 2s - loss: 243.1858 - loglik: -2.4086e+02 - logprior: -2.1033e+00
Fitted a model with MAP estimate = -242.0720
expansions: []
discards: [ 0 36 68]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 252.7345 - loglik: -2.4390e+02 - logprior: -8.7915e+00
Epoch 2/10
13/13 - 2s - loss: 245.2721 - loglik: -2.4216e+02 - logprior: -2.8841e+00
Epoch 3/10
13/13 - 2s - loss: 242.4188 - loglik: -2.4042e+02 - logprior: -1.5684e+00
Epoch 4/10
13/13 - 2s - loss: 240.9213 - loglik: -2.3903e+02 - logprior: -1.3044e+00
Epoch 5/10
13/13 - 2s - loss: 241.2214 - loglik: -2.3937e+02 - logprior: -1.1871e+00
Fitted a model with MAP estimate = -239.3655
Time for alignment: 62.9799
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.6844 - loglik: -3.0883e+02 - logprior: -7.8108e+00
Epoch 2/10
13/13 - 2s - loss: 284.5860 - loglik: -2.8246e+02 - logprior: -1.9760e+00
Epoch 3/10
13/13 - 2s - loss: 260.5365 - loglik: -2.5857e+02 - logprior: -1.7686e+00
Epoch 4/10
13/13 - 2s - loss: 250.8009 - loglik: -2.4832e+02 - logprior: -2.0818e+00
Epoch 5/10
13/13 - 2s - loss: 248.1696 - loglik: -2.4553e+02 - logprior: -2.0207e+00
Epoch 6/10
13/13 - 2s - loss: 247.1445 - loglik: -2.4463e+02 - logprior: -1.9158e+00
Epoch 7/10
13/13 - 2s - loss: 246.1648 - loglik: -2.4363e+02 - logprior: -1.9294e+00
Epoch 8/10
13/13 - 2s - loss: 245.4303 - loglik: -2.4285e+02 - logprior: -1.9542e+00
Epoch 9/10
13/13 - 2s - loss: 244.9756 - loglik: -2.4246e+02 - logprior: -1.9377e+00
Epoch 10/10
13/13 - 2s - loss: 244.9077 - loglik: -2.4242e+02 - logprior: -1.9381e+00
Fitted a model with MAP estimate = -243.9256
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 4), (31, 2), (32, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 265.3636 - loglik: -2.5604e+02 - logprior: -9.2858e+00
Epoch 2/2
13/13 - 2s - loss: 249.3643 - loglik: -2.4493e+02 - logprior: -4.2585e+00
Fitted a model with MAP estimate = -246.4332
expansions: [(0, 2)]
discards: [ 0 14 44 91 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.5101 - loglik: -2.4342e+02 - logprior: -7.0541e+00
Epoch 2/2
13/13 - 2s - loss: 243.5872 - loglik: -2.4129e+02 - logprior: -2.0730e+00
Fitted a model with MAP estimate = -241.9819
expansions: []
discards: [ 0 67]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 252.6668 - loglik: -2.4385e+02 - logprior: -8.7703e+00
Epoch 2/10
13/13 - 2s - loss: 244.6409 - loglik: -2.4155e+02 - logprior: -2.8602e+00
Epoch 3/10
13/13 - 2s - loss: 242.9766 - loglik: -2.4100e+02 - logprior: -1.5539e+00
Epoch 4/10
13/13 - 2s - loss: 241.2987 - loglik: -2.3940e+02 - logprior: -1.3059e+00
Epoch 5/10
13/13 - 2s - loss: 240.8712 - loglik: -2.3904e+02 - logprior: -1.1764e+00
Epoch 6/10
13/13 - 2s - loss: 239.7271 - loglik: -2.3791e+02 - logprior: -1.1599e+00
Epoch 7/10
13/13 - 2s - loss: 238.8202 - loglik: -2.3701e+02 - logprior: -1.1424e+00
Epoch 8/10
13/13 - 2s - loss: 238.8329 - loglik: -2.3703e+02 - logprior: -1.1325e+00
Fitted a model with MAP estimate = -237.4108
Time for alignment: 67.9395
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.6057 - loglik: -3.0875e+02 - logprior: -7.8156e+00
Epoch 2/10
13/13 - 2s - loss: 284.4366 - loglik: -2.8230e+02 - logprior: -1.9936e+00
Epoch 3/10
13/13 - 2s - loss: 260.9315 - loglik: -2.5885e+02 - logprior: -1.7952e+00
Epoch 4/10
13/13 - 2s - loss: 251.3589 - loglik: -2.4866e+02 - logprior: -2.0263e+00
Epoch 5/10
13/13 - 2s - loss: 248.1641 - loglik: -2.4553e+02 - logprior: -1.9718e+00
Epoch 6/10
13/13 - 2s - loss: 247.4961 - loglik: -2.4500e+02 - logprior: -1.9128e+00
Epoch 7/10
13/13 - 2s - loss: 246.4570 - loglik: -2.4392e+02 - logprior: -1.9400e+00
Epoch 8/10
13/13 - 2s - loss: 245.9216 - loglik: -2.4339e+02 - logprior: -1.9461e+00
Epoch 9/10
13/13 - 2s - loss: 244.7559 - loglik: -2.4225e+02 - logprior: -1.9408e+00
Epoch 10/10
13/13 - 2s - loss: 245.5280 - loglik: -2.4302e+02 - logprior: -1.9501e+00
Fitted a model with MAP estimate = -244.1919
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (30, 4), (31, 2), (32, 2), (39, 1), (40, 1), (50, 1), (57, 1), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 265.8103 - loglik: -2.5652e+02 - logprior: -9.2585e+00
Epoch 2/2
13/13 - 2s - loss: 249.0601 - loglik: -2.4464e+02 - logprior: -4.2369e+00
Fitted a model with MAP estimate = -246.3974
expansions: [(0, 2)]
discards: [ 0 14 89 94]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.9379 - loglik: -2.4382e+02 - logprior: -7.0719e+00
Epoch 2/2
13/13 - 2s - loss: 243.6136 - loglik: -2.4131e+02 - logprior: -2.0806e+00
Fitted a model with MAP estimate = -242.2653
expansions: []
discards: [ 0 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.6851 - loglik: -2.4390e+02 - logprior: -8.7461e+00
Epoch 2/10
13/13 - 2s - loss: 245.2274 - loglik: -2.4217e+02 - logprior: -2.8334e+00
Epoch 3/10
13/13 - 2s - loss: 242.8866 - loglik: -2.4098e+02 - logprior: -1.5090e+00
Epoch 4/10
13/13 - 2s - loss: 241.8678 - loglik: -2.4003e+02 - logprior: -1.2714e+00
Epoch 5/10
13/13 - 2s - loss: 240.5634 - loglik: -2.3877e+02 - logprior: -1.1595e+00
Epoch 6/10
13/13 - 2s - loss: 240.1530 - loglik: -2.3837e+02 - logprior: -1.1404e+00
Epoch 7/10
13/13 - 2s - loss: 239.5340 - loglik: -2.3775e+02 - logprior: -1.1289e+00
Epoch 8/10
13/13 - 2s - loss: 238.6305 - loglik: -2.3683e+02 - logprior: -1.1320e+00
Epoch 9/10
13/13 - 2s - loss: 238.0248 - loglik: -2.3622e+02 - logprior: -1.1146e+00
Epoch 10/10
13/13 - 2s - loss: 238.0485 - loglik: -2.3621e+02 - logprior: -1.1195e+00
Fitted a model with MAP estimate = -236.4472
Time for alignment: 69.1027
Computed alignments with likelihoods: ['-239.3655', '-237.4108', '-236.4472']
Best model has likelihood: -236.4472
time for generating output: 0.1872
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.924618320610687
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64ae34760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6032d6fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb571701e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5717014c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61fa000d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb61fa00880>, <__main__.SimpleDirichletPrior object at 0x7fb4e9ea2f40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 433.8089 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3673 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4340 - loglik: -2.6912e+02 - logprior: -9.2993e+00
Epoch 4/10
10/10 - 1s - loss: 247.0367 - loglik: -2.4123e+02 - logprior: -5.8072e+00
Epoch 5/10
10/10 - 1s - loss: 232.0815 - loglik: -2.2795e+02 - logprior: -4.0346e+00
Epoch 6/10
10/10 - 1s - loss: 225.9421 - loglik: -2.2268e+02 - logprior: -2.9189e+00
Epoch 7/10
10/10 - 1s - loss: 222.7863 - loglik: -2.2030e+02 - logprior: -2.0546e+00
Epoch 8/10
10/10 - 1s - loss: 221.1688 - loglik: -2.1937e+02 - logprior: -1.4239e+00
Epoch 9/10
10/10 - 1s - loss: 220.2715 - loglik: -2.1895e+02 - logprior: -9.8947e-01
Epoch 10/10
10/10 - 1s - loss: 219.7151 - loglik: -2.1873e+02 - logprior: -6.3390e-01
Fitted a model with MAP estimate = -219.1094
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 314.7715 - loglik: -2.1729e+02 - logprior: -9.7455e+01
Epoch 2/2
10/10 - 2s - loss: 237.8983 - loglik: -1.9997e+02 - logprior: -3.7744e+01
Fitted a model with MAP estimate = -223.7456
expansions: [(0, 2)]
discards: [  0 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.3550 - loglik: -1.9685e+02 - logprior: -7.6485e+01
Epoch 2/2
10/10 - 2s - loss: 210.0366 - loglik: -1.9257e+02 - logprior: -1.7338e+01
Fitted a model with MAP estimate = -199.6740
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.5543 - loglik: -1.9624e+02 - logprior: -9.1290e+01
Epoch 2/10
10/10 - 2s - loss: 218.3557 - loglik: -1.9417e+02 - logprior: -2.4062e+01
Epoch 3/10
10/10 - 2s - loss: 198.5417 - loglik: -1.9209e+02 - logprior: -6.1610e+00
Epoch 4/10
10/10 - 2s - loss: 191.1842 - loglik: -1.9092e+02 - logprior: 0.1419
Epoch 5/10
10/10 - 2s - loss: 187.6526 - loglik: -1.9058e+02 - logprior: 3.3198
Epoch 6/10
10/10 - 2s - loss: 185.6618 - loglik: -1.9042e+02 - logprior: 5.1656
Epoch 7/10
10/10 - 2s - loss: 184.4486 - loglik: -1.9040e+02 - logprior: 6.3683
Epoch 8/10
10/10 - 2s - loss: 183.6071 - loglik: -1.9042e+02 - logprior: 7.2184
Epoch 9/10
10/10 - 2s - loss: 182.9502 - loglik: -1.9048e+02 - logprior: 7.9368
Epoch 10/10
10/10 - 2s - loss: 182.3864 - loglik: -1.9056e+02 - logprior: 8.5877
Fitted a model with MAP estimate = -181.6788
Time for alignment: 56.3512
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.8089 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4339 - loglik: -2.6912e+02 - logprior: -9.2993e+00
Epoch 4/10
10/10 - 1s - loss: 247.0363 - loglik: -2.4122e+02 - logprior: -5.8116e+00
Epoch 5/10
10/10 - 1s - loss: 232.1614 - loglik: -2.2800e+02 - logprior: -4.0550e+00
Epoch 6/10
10/10 - 1s - loss: 226.1063 - loglik: -2.2283e+02 - logprior: -2.8979e+00
Epoch 7/10
10/10 - 1s - loss: 222.9936 - loglik: -2.2050e+02 - logprior: -2.0124e+00
Epoch 8/10
10/10 - 1s - loss: 221.4270 - loglik: -2.1959e+02 - logprior: -1.4105e+00
Epoch 9/10
10/10 - 1s - loss: 220.5145 - loglik: -2.1917e+02 - logprior: -9.7686e-01
Epoch 10/10
10/10 - 1s - loss: 219.8293 - loglik: -2.1882e+02 - logprior: -6.2968e-01
Fitted a model with MAP estimate = -219.1552
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 315.2297 - loglik: -2.1773e+02 - logprior: -9.7471e+01
Epoch 2/2
10/10 - 2s - loss: 238.1043 - loglik: -2.0007e+02 - logprior: -3.7851e+01
Fitted a model with MAP estimate = -223.8599
expansions: [(0, 2)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.4512 - loglik: -1.9692e+02 - logprior: -7.6511e+01
Epoch 2/2
10/10 - 2s - loss: 210.1123 - loglik: -1.9263e+02 - logprior: -1.7358e+01
Fitted a model with MAP estimate = -199.7809
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.7159 - loglik: -1.9631e+02 - logprior: -9.1379e+01
Epoch 2/10
10/10 - 2s - loss: 218.5800 - loglik: -1.9426e+02 - logprior: -2.4188e+01
Epoch 3/10
10/10 - 2s - loss: 198.6462 - loglik: -1.9217e+02 - logprior: -6.1780e+00
Epoch 4/10
10/10 - 2s - loss: 191.2789 - loglik: -1.9101e+02 - logprior: 0.1522
Epoch 5/10
10/10 - 2s - loss: 187.8275 - loglik: -1.9077e+02 - logprior: 3.3670
Epoch 6/10
10/10 - 2s - loss: 185.9083 - loglik: -1.9073e+02 - logprior: 5.2414
Epoch 7/10
10/10 - 2s - loss: 184.6565 - loglik: -1.9061e+02 - logprior: 6.3895
Epoch 8/10
10/10 - 2s - loss: 183.7190 - loglik: -1.9049e+02 - logprior: 7.1967
Epoch 9/10
10/10 - 2s - loss: 183.0098 - loglik: -1.9054e+02 - logprior: 7.9312
Epoch 10/10
10/10 - 2s - loss: 182.4166 - loglik: -1.9059e+02 - logprior: 8.5891
Fitted a model with MAP estimate = -181.7011
Time for alignment: 55.6954
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.8088 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4341 - loglik: -2.6912e+02 - logprior: -9.2992e+00
Epoch 4/10
10/10 - 1s - loss: 247.1693 - loglik: -2.4139e+02 - logprior: -5.7771e+00
Epoch 5/10
10/10 - 1s - loss: 232.9354 - loglik: -2.2906e+02 - logprior: -3.8603e+00
Epoch 6/10
10/10 - 1s - loss: 226.0847 - loglik: -2.2306e+02 - logprior: -2.8329e+00
Epoch 7/10
10/10 - 1s - loss: 222.7499 - loglik: -2.2033e+02 - logprior: -2.0059e+00
Epoch 8/10
10/10 - 1s - loss: 221.2015 - loglik: -2.1941e+02 - logprior: -1.3728e+00
Epoch 9/10
10/10 - 1s - loss: 220.3316 - loglik: -2.1910e+02 - logprior: -8.9431e-01
Epoch 10/10
10/10 - 1s - loss: 219.7682 - loglik: -2.1896e+02 - logprior: -5.0917e-01
Fitted a model with MAP estimate = -219.2362
expansions: [(13, 3), (14, 1), (24, 1), (29, 1), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 293.3563 - loglik: -2.1525e+02 - logprior: -7.8076e+01
Epoch 2/2
10/10 - 2s - loss: 216.6765 - loglik: -1.9829e+02 - logprior: -1.8198e+01
Fitted a model with MAP estimate = -203.2494
expansions: []
discards: [  0  47 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 296.0687 - loglik: -1.9981e+02 - logprior: -9.6244e+01
Epoch 2/2
10/10 - 2s - loss: 233.7859 - loglik: -1.9608e+02 - logprior: -3.7606e+01
Fitted a model with MAP estimate = -223.6377
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 273.1574 - loglik: -1.9634e+02 - logprior: -7.6797e+01
Epoch 2/10
10/10 - 2s - loss: 210.7718 - loglik: -1.9367e+02 - logprior: -1.7015e+01
Epoch 3/10
10/10 - 2s - loss: 196.4547 - loglik: -1.9142e+02 - logprior: -4.8145e+00
Epoch 4/10
10/10 - 2s - loss: 190.3925 - loglik: -1.9036e+02 - logprior: 0.3671
Epoch 5/10
10/10 - 2s - loss: 187.3177 - loglik: -1.9029e+02 - logprior: 3.4134
Epoch 6/10
10/10 - 2s - loss: 185.5294 - loglik: -1.9035e+02 - logprior: 5.2479
Epoch 7/10
10/10 - 2s - loss: 184.4372 - loglik: -1.9051e+02 - logprior: 6.5010
Epoch 8/10
10/10 - 2s - loss: 183.6586 - loglik: -1.9064e+02 - logprior: 7.4016
Epoch 9/10
10/10 - 2s - loss: 183.0444 - loglik: -1.9072e+02 - logprior: 8.0935
Epoch 10/10
10/10 - 2s - loss: 182.5110 - loglik: -1.9077e+02 - logprior: 8.6873
Fitted a model with MAP estimate = -181.8105
Time for alignment: 56.4388
Computed alignments with likelihoods: ['-181.6788', '-181.7011', '-181.8105']
Best model has likelihood: -181.6788
time for generating output: 0.1757
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8740298507462687
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb57a3fd580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb63905f160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f4b6c10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686a04520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb686a04c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb60edfb3a0>, <__main__.SimpleDirichletPrior object at 0x7fb57aef61c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.3481 - loglik: -1.3549e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.4446 - loglik: -1.1531e+02 - logprior: -9.1267e+00
Epoch 3/10
10/10 - 1s - loss: 98.7748 - loglik: -9.3696e+01 - logprior: -5.0463e+00
Epoch 4/10
10/10 - 1s - loss: 83.6453 - loglik: -7.9680e+01 - logprior: -3.9475e+00
Epoch 5/10
10/10 - 1s - loss: 78.6444 - loglik: -7.4981e+01 - logprior: -3.6425e+00
Epoch 6/10
10/10 - 1s - loss: 76.7057 - loglik: -7.3159e+01 - logprior: -3.3984e+00
Epoch 7/10
10/10 - 1s - loss: 75.9403 - loglik: -7.2686e+01 - logprior: -2.9845e+00
Epoch 8/10
10/10 - 1s - loss: 75.6396 - loglik: -7.2657e+01 - logprior: -2.7140e+00
Epoch 9/10
10/10 - 1s - loss: 75.2222 - loglik: -7.2368e+01 - logprior: -2.6131e+00
Epoch 10/10
10/10 - 1s - loss: 75.0992 - loglik: -7.2275e+01 - logprior: -2.5780e+00
Fitted a model with MAP estimate = -74.7258
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.8417 - loglik: -7.2251e+01 - logprior: -4.2573e+01
Epoch 2/2
10/10 - 1s - loss: 78.2970 - loglik: -6.4154e+01 - logprior: -1.4046e+01
Fitted a model with MAP estimate = -71.0039
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 91.2109 - loglik: -6.1100e+01 - logprior: -3.0090e+01
Epoch 2/2
10/10 - 1s - loss: 69.3906 - loglik: -6.0643e+01 - logprior: -8.6698e+00
Fitted a model with MAP estimate = -66.2279
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.7703 - loglik: -6.0298e+01 - logprior: -2.8451e+01
Epoch 2/10
10/10 - 1s - loss: 68.8597 - loglik: -6.0541e+01 - logprior: -8.2266e+00
Epoch 3/10
10/10 - 1s - loss: 64.9942 - loglik: -6.0608e+01 - logprior: -4.2600e+00
Epoch 4/10
10/10 - 1s - loss: 63.7559 - loglik: -6.0849e+01 - logprior: -2.7730e+00
Epoch 5/10
10/10 - 1s - loss: 62.7079 - loglik: -6.0464e+01 - logprior: -2.0684e+00
Epoch 6/10
10/10 - 1s - loss: 62.5841 - loglik: -6.0667e+01 - logprior: -1.7152e+00
Epoch 7/10
10/10 - 1s - loss: 62.1719 - loglik: -6.0478e+01 - logprior: -1.4815e+00
Epoch 8/10
10/10 - 1s - loss: 62.1670 - loglik: -6.0639e+01 - logprior: -1.2949e+00
Epoch 9/10
10/10 - 1s - loss: 61.9378 - loglik: -6.0551e+01 - logprior: -1.1406e+00
Epoch 10/10
10/10 - 1s - loss: 61.9931 - loglik: -6.0692e+01 - logprior: -1.0490e+00
Fitted a model with MAP estimate = -61.5855
Time for alignment: 31.9598
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.4028 - loglik: -1.3554e+02 - logprior: -3.1848e+01
Epoch 2/10
10/10 - 1s - loss: 124.3435 - loglik: -1.1521e+02 - logprior: -9.1273e+00
Epoch 3/10
10/10 - 1s - loss: 99.4640 - loglik: -9.4402e+01 - logprior: -5.0303e+00
Epoch 4/10
10/10 - 1s - loss: 84.6714 - loglik: -8.0763e+01 - logprior: -3.8925e+00
Epoch 5/10
10/10 - 1s - loss: 79.1075 - loglik: -7.5504e+01 - logprior: -3.5942e+00
Epoch 6/10
10/10 - 1s - loss: 76.7880 - loglik: -7.3303e+01 - logprior: -3.3916e+00
Epoch 7/10
10/10 - 1s - loss: 76.1078 - loglik: -7.2858e+01 - logprior: -3.0082e+00
Epoch 8/10
10/10 - 1s - loss: 75.6629 - loglik: -7.2670e+01 - logprior: -2.7216e+00
Epoch 9/10
10/10 - 1s - loss: 75.2741 - loglik: -7.2415e+01 - logprior: -2.6188e+00
Epoch 10/10
10/10 - 1s - loss: 75.0318 - loglik: -7.2213e+01 - logprior: -2.5845e+00
Fitted a model with MAP estimate = -74.7417
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.8414 - loglik: -7.2238e+01 - logprior: -4.2587e+01
Epoch 2/2
10/10 - 1s - loss: 78.3604 - loglik: -6.4221e+01 - logprior: -1.4048e+01
Fitted a model with MAP estimate = -71.0045
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.2516 - loglik: -6.1146e+01 - logprior: -3.0087e+01
Epoch 2/2
10/10 - 1s - loss: 69.3158 - loglik: -6.0565e+01 - logprior: -8.6763e+00
Fitted a model with MAP estimate = -66.2325
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.9197 - loglik: -6.0439e+01 - logprior: -2.8457e+01
Epoch 2/10
10/10 - 1s - loss: 68.7383 - loglik: -6.0417e+01 - logprior: -8.2274e+00
Epoch 3/10
10/10 - 1s - loss: 64.9731 - loglik: -6.0590e+01 - logprior: -4.2605e+00
Epoch 4/10
10/10 - 1s - loss: 63.6218 - loglik: -6.0700e+01 - logprior: -2.7822e+00
Epoch 5/10
10/10 - 1s - loss: 62.8752 - loglik: -6.0633e+01 - logprior: -2.0649e+00
Epoch 6/10
10/10 - 1s - loss: 62.4982 - loglik: -6.0588e+01 - logprior: -1.7100e+00
Epoch 7/10
10/10 - 1s - loss: 62.1951 - loglik: -6.0494e+01 - logprior: -1.4867e+00
Epoch 8/10
10/10 - 1s - loss: 62.0814 - loglik: -6.0557e+01 - logprior: -1.2878e+00
Epoch 9/10
10/10 - 1s - loss: 61.9894 - loglik: -6.0604e+01 - logprior: -1.1396e+00
Epoch 10/10
10/10 - 1s - loss: 61.9103 - loglik: -6.0608e+01 - logprior: -1.0502e+00
Fitted a model with MAP estimate = -61.5817
Time for alignment: 30.8930
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.4131 - loglik: -1.3555e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.4160 - loglik: -1.1528e+02 - logprior: -9.1279e+00
Epoch 3/10
10/10 - 1s - loss: 99.0031 - loglik: -9.3928e+01 - logprior: -5.0430e+00
Epoch 4/10
10/10 - 1s - loss: 84.1403 - loglik: -8.0182e+01 - logprior: -3.9399e+00
Epoch 5/10
10/10 - 1s - loss: 78.9455 - loglik: -7.5308e+01 - logprior: -3.6278e+00
Epoch 6/10
10/10 - 1s - loss: 76.8080 - loglik: -7.3305e+01 - logprior: -3.3995e+00
Epoch 7/10
10/10 - 1s - loss: 76.1229 - loglik: -7.2879e+01 - logprior: -2.9934e+00
Epoch 8/10
10/10 - 1s - loss: 75.6256 - loglik: -7.2641e+01 - logprior: -2.7139e+00
Epoch 9/10
10/10 - 1s - loss: 75.2207 - loglik: -7.2376e+01 - logprior: -2.6073e+00
Epoch 10/10
10/10 - 1s - loss: 75.2035 - loglik: -7.2404e+01 - logprior: -2.5650e+00
Fitted a model with MAP estimate = -74.8233
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.8957 - loglik: -7.2308e+01 - logprior: -4.2562e+01
Epoch 2/2
10/10 - 1s - loss: 78.2535 - loglik: -6.4100e+01 - logprior: -1.4036e+01
Fitted a model with MAP estimate = -71.0131
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.3033 - loglik: -6.1202e+01 - logprior: -3.0078e+01
Epoch 2/2
10/10 - 1s - loss: 69.2901 - loglik: -6.0508e+01 - logprior: -8.6841e+00
Fitted a model with MAP estimate = -66.2358
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.8404 - loglik: -6.0367e+01 - logprior: -2.8451e+01
Epoch 2/10
10/10 - 1s - loss: 68.7811 - loglik: -6.0452e+01 - logprior: -8.2303e+00
Epoch 3/10
10/10 - 1s - loss: 65.1723 - loglik: -6.0785e+01 - logprior: -4.2573e+00
Epoch 4/10
10/10 - 1s - loss: 63.5988 - loglik: -6.0692e+01 - logprior: -2.7639e+00
Epoch 5/10
10/10 - 1s - loss: 62.7707 - loglik: -6.0533e+01 - logprior: -2.0490e+00
Epoch 6/10
10/10 - 1s - loss: 62.6383 - loglik: -6.0731e+01 - logprior: -1.6910e+00
Epoch 7/10
10/10 - 1s - loss: 62.2662 - loglik: -6.0555e+01 - logprior: -1.4748e+00
Epoch 8/10
10/10 - 1s - loss: 62.1266 - loglik: -6.0592e+01 - logprior: -1.2883e+00
Epoch 9/10
10/10 - 1s - loss: 62.0169 - loglik: -6.0643e+01 - logprior: -1.1409e+00
Epoch 10/10
10/10 - 1s - loss: 61.8029 - loglik: -6.0502e+01 - logprior: -1.0535e+00
Fitted a model with MAP estimate = -61.5928
Time for alignment: 32.5245
Computed alignments with likelihoods: ['-61.5855', '-61.5817', '-61.5928']
Best model has likelihood: -61.5817
time for generating output: 0.1102
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb69f81ab80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60ec33fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60ec33f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c432a250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c432a640>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c432a550>, <__main__.SimpleDirichletPrior object at 0x7fb57aa849d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.3837 - loglik: -4.3991e+02 - logprior: -5.4655e+00
Epoch 2/10
15/15 - 4s - loss: 365.0038 - loglik: -3.6356e+02 - logprior: -1.4251e+00
Epoch 3/10
15/15 - 4s - loss: 318.9595 - loglik: -3.1726e+02 - logprior: -1.6761e+00
Epoch 4/10
15/15 - 4s - loss: 304.1441 - loglik: -3.0208e+02 - logprior: -1.8308e+00
Epoch 5/10
15/15 - 4s - loss: 301.6122 - loglik: -2.9954e+02 - logprior: -1.7185e+00
Epoch 6/10
15/15 - 4s - loss: 299.2567 - loglik: -2.9724e+02 - logprior: -1.7326e+00
Epoch 7/10
15/15 - 4s - loss: 300.1076 - loglik: -2.9814e+02 - logprior: -1.7282e+00
Fitted a model with MAP estimate = -298.6137
expansions: [(7, 3), (12, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (69, 1), (91, 2), (92, 1), (112, 1), (114, 4), (116, 3), (119, 2), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 302.9746 - loglik: -2.9621e+02 - logprior: -6.7185e+00
Epoch 2/2
15/15 - 5s - loss: 287.5724 - loglik: -2.8401e+02 - logprior: -3.3985e+00
Fitted a model with MAP estimate = -285.0157
expansions: [(0, 2)]
discards: [  0   7 106 138 147]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 288.2710 - loglik: -2.8307e+02 - logprior: -5.1631e+00
Epoch 2/2
15/15 - 5s - loss: 282.3745 - loglik: -2.8054e+02 - logprior: -1.6484e+00
Fitted a model with MAP estimate = -280.4615
expansions: []
discards: [ 0 65 66 67]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 294.3211 - loglik: -2.8775e+02 - logprior: -6.5288e+00
Epoch 2/10
15/15 - 5s - loss: 288.0020 - loglik: -2.8552e+02 - logprior: -2.3092e+00
Epoch 3/10
15/15 - 5s - loss: 284.7447 - loglik: -2.8317e+02 - logprior: -1.2732e+00
Epoch 4/10
15/15 - 5s - loss: 283.7371 - loglik: -2.8225e+02 - logprior: -1.1153e+00
Epoch 5/10
15/15 - 5s - loss: 282.9622 - loglik: -2.8150e+02 - logprior: -1.0814e+00
Epoch 6/10
15/15 - 5s - loss: 281.6889 - loglik: -2.8030e+02 - logprior: -1.0483e+00
Epoch 7/10
15/15 - 5s - loss: 282.0316 - loglik: -2.8070e+02 - logprior: -1.0011e+00
Fitted a model with MAP estimate = -281.0868
Time for alignment: 128.1444
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.1219 - loglik: -4.3965e+02 - logprior: -5.4582e+00
Epoch 2/10
15/15 - 4s - loss: 365.3850 - loglik: -3.6394e+02 - logprior: -1.4195e+00
Epoch 3/10
15/15 - 4s - loss: 318.6293 - loglik: -3.1691e+02 - logprior: -1.6962e+00
Epoch 4/10
15/15 - 4s - loss: 304.5972 - loglik: -3.0248e+02 - logprior: -1.9544e+00
Epoch 5/10
15/15 - 4s - loss: 300.4076 - loglik: -2.9829e+02 - logprior: -1.8499e+00
Epoch 6/10
15/15 - 4s - loss: 299.0752 - loglik: -2.9701e+02 - logprior: -1.8157e+00
Epoch 7/10
15/15 - 4s - loss: 298.1712 - loglik: -2.9615e+02 - logprior: -1.7867e+00
Epoch 8/10
15/15 - 4s - loss: 297.7026 - loglik: -2.9571e+02 - logprior: -1.7627e+00
Epoch 9/10
15/15 - 4s - loss: 296.9199 - loglik: -2.9491e+02 - logprior: -1.7779e+00
Epoch 10/10
15/15 - 4s - loss: 296.3496 - loglik: -2.9432e+02 - logprior: -1.8013e+00
Fitted a model with MAP estimate = -295.5154
expansions: [(8, 2), (9, 1), (12, 1), (16, 1), (24, 2), (27, 1), (49, 2), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 2), (115, 1), (116, 4), (117, 1), (119, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 301.8558 - loglik: -2.9508e+02 - logprior: -6.7380e+00
Epoch 2/2
15/15 - 5s - loss: 285.6767 - loglik: -2.8208e+02 - logprior: -3.4312e+00
Fitted a model with MAP estimate = -282.8682
expansions: [(0, 2)]
discards: [  0   7  57  77  84 109 136]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 287.3906 - loglik: -2.8221e+02 - logprior: -5.1454e+00
Epoch 2/2
15/15 - 5s - loss: 281.7001 - loglik: -2.7988e+02 - logprior: -1.6541e+00
Fitted a model with MAP estimate = -279.9023
expansions: []
discards: [ 0 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 292.7535 - loglik: -2.8618e+02 - logprior: -6.5339e+00
Epoch 2/10
15/15 - 5s - loss: 286.2599 - loglik: -2.8373e+02 - logprior: -2.3421e+00
Epoch 3/10
15/15 - 5s - loss: 282.6331 - loglik: -2.8103e+02 - logprior: -1.2874e+00
Epoch 4/10
15/15 - 5s - loss: 282.3805 - loglik: -2.8089e+02 - logprior: -1.1001e+00
Epoch 5/10
15/15 - 5s - loss: 280.3595 - loglik: -2.7891e+02 - logprior: -1.0678e+00
Epoch 6/10
15/15 - 5s - loss: 281.0803 - loglik: -2.7971e+02 - logprior: -1.0247e+00
Fitted a model with MAP estimate = -279.6883
Time for alignment: 135.6860
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.9003 - loglik: -4.4043e+02 - logprior: -5.4615e+00
Epoch 2/10
15/15 - 4s - loss: 366.8106 - loglik: -3.6537e+02 - logprior: -1.4181e+00
Epoch 3/10
15/15 - 4s - loss: 320.8323 - loglik: -3.1915e+02 - logprior: -1.6558e+00
Epoch 4/10
15/15 - 4s - loss: 306.9935 - loglik: -3.0490e+02 - logprior: -1.8572e+00
Epoch 5/10
15/15 - 4s - loss: 302.2210 - loglik: -3.0018e+02 - logprior: -1.7530e+00
Epoch 6/10
15/15 - 4s - loss: 301.0530 - loglik: -2.9904e+02 - logprior: -1.7703e+00
Epoch 7/10
15/15 - 4s - loss: 299.3904 - loglik: -2.9741e+02 - logprior: -1.7644e+00
Epoch 8/10
15/15 - 4s - loss: 298.7232 - loglik: -2.9677e+02 - logprior: -1.7453e+00
Epoch 9/10
15/15 - 4s - loss: 299.1732 - loglik: -2.9723e+02 - logprior: -1.7418e+00
Fitted a model with MAP estimate = -298.3046
expansions: [(7, 3), (12, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (67, 1), (69, 1), (91, 1), (92, 2), (105, 1), (112, 1), (114, 3), (116, 3), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 303.3141 - loglik: -2.9656e+02 - logprior: -6.7119e+00
Epoch 2/2
15/15 - 5s - loss: 287.5468 - loglik: -2.8396e+02 - logprior: -3.4097e+00
Fitted a model with MAP estimate = -285.7274
expansions: [(0, 2)]
discards: [  0   7  57  77  78 108]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 290.0229 - loglik: -2.8482e+02 - logprior: -5.1616e+00
Epoch 2/2
15/15 - 5s - loss: 284.3264 - loglik: -2.8248e+02 - logprior: -1.6609e+00
Fitted a model with MAP estimate = -282.7547
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 292.2891 - loglik: -2.8567e+02 - logprior: -6.5818e+00
Epoch 2/10
15/15 - 5s - loss: 285.1820 - loglik: -2.8265e+02 - logprior: -2.3507e+00
Epoch 3/10
15/15 - 5s - loss: 283.1541 - loglik: -2.8154e+02 - logprior: -1.3103e+00
Epoch 4/10
15/15 - 5s - loss: 282.1920 - loglik: -2.8066e+02 - logprior: -1.1561e+00
Epoch 5/10
15/15 - 5s - loss: 280.3570 - loglik: -2.7888e+02 - logprior: -1.1172e+00
Epoch 6/10
15/15 - 5s - loss: 281.1122 - loglik: -2.7970e+02 - logprior: -1.0796e+00
Fitted a model with MAP estimate = -279.7269
Time for alignment: 129.6155
Computed alignments with likelihoods: ['-280.4615', '-279.6883', '-279.7269']
Best model has likelihood: -279.6883
time for generating output: 0.2051
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9126254180602007
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6748353a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb52dc31b80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6046325e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60ee30100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60ee30220>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb60ee30ac0>, <__main__.SimpleDirichletPrior object at 0x7fb57a9e4820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 301.5485 - loglik: -2.9854e+02 - logprior: -2.9289e+00
Epoch 2/10
34/34 - 6s - loss: 213.6194 - loglik: -2.1172e+02 - logprior: -1.8404e+00
Epoch 3/10
34/34 - 7s - loss: 201.2483 - loglik: -1.9907e+02 - logprior: -1.9205e+00
Epoch 4/10
34/34 - 7s - loss: 201.1580 - loglik: -1.9904e+02 - logprior: -1.8601e+00
Epoch 5/10
34/34 - 6s - loss: 199.5697 - loglik: -1.9742e+02 - logprior: -1.8794e+00
Epoch 6/10
34/34 - 7s - loss: 200.0310 - loglik: -1.9788e+02 - logprior: -1.8663e+00
Fitted a model with MAP estimate = -199.1017
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 187.2394 - loglik: -1.8389e+02 - logprior: -3.2269e+00
Epoch 2/2
34/34 - 8s - loss: 176.3341 - loglik: -1.7462e+02 - logprior: -1.4663e+00
Fitted a model with MAP estimate = -174.9219
expansions: []
discards: [ 34 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 179.0507 - loglik: -1.7610e+02 - logprior: -2.8248e+00
Epoch 2/2
34/34 - 7s - loss: 177.3048 - loglik: -1.7576e+02 - logprior: -1.2812e+00
Fitted a model with MAP estimate = -175.0767
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.4700 - loglik: -1.7665e+02 - logprior: -2.6957e+00
Epoch 2/10
34/34 - 7s - loss: 176.1882 - loglik: -1.7478e+02 - logprior: -1.1371e+00
Epoch 3/10
34/34 - 7s - loss: 174.8057 - loglik: -1.7345e+02 - logprior: -1.0428e+00
Epoch 4/10
34/34 - 7s - loss: 174.7672 - loglik: -1.7349e+02 - logprior: -9.6086e-01
Epoch 5/10
34/34 - 7s - loss: 173.5211 - loglik: -1.7232e+02 - logprior: -8.8484e-01
Epoch 6/10
34/34 - 7s - loss: 173.5077 - loglik: -1.7240e+02 - logprior: -7.9815e-01
Epoch 7/10
34/34 - 7s - loss: 173.7342 - loglik: -1.7269e+02 - logprior: -7.2186e-01
Fitted a model with MAP estimate = -172.9132
Time for alignment: 168.6266
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 299.8522 - loglik: -2.9685e+02 - logprior: -2.9182e+00
Epoch 2/10
34/34 - 6s - loss: 207.4290 - loglik: -2.0554e+02 - logprior: -1.8224e+00
Epoch 3/10
34/34 - 7s - loss: 202.3442 - loglik: -2.0026e+02 - logprior: -1.8287e+00
Epoch 4/10
34/34 - 7s - loss: 200.8294 - loglik: -1.9877e+02 - logprior: -1.7907e+00
Epoch 5/10
34/34 - 7s - loss: 200.8163 - loglik: -1.9874e+02 - logprior: -1.7813e+00
Epoch 6/10
34/34 - 6s - loss: 200.3881 - loglik: -1.9828e+02 - logprior: -1.8026e+00
Epoch 7/10
34/34 - 7s - loss: 199.6321 - loglik: -1.9753e+02 - logprior: -1.7962e+00
Epoch 8/10
34/34 - 7s - loss: 199.7023 - loglik: -1.9757e+02 - logprior: -1.8185e+00
Fitted a model with MAP estimate = -199.2885
expansions: [(0, 2), (15, 1), (16, 4), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (45, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 3), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 188.0817 - loglik: -1.8469e+02 - logprior: -3.2563e+00
Epoch 2/2
34/34 - 7s - loss: 177.3401 - loglik: -1.7563e+02 - logprior: -1.4501e+00
Fitted a model with MAP estimate = -176.1347
expansions: []
discards: [138]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 180.4496 - loglik: -1.7751e+02 - logprior: -2.8238e+00
Epoch 2/2
34/34 - 7s - loss: 176.7513 - loglik: -1.7521e+02 - logprior: -1.2791e+00
Fitted a model with MAP estimate = -175.6510
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.8633 - loglik: -1.7704e+02 - logprior: -2.7006e+00
Epoch 2/10
34/34 - 7s - loss: 176.8427 - loglik: -1.7544e+02 - logprior: -1.1416e+00
Epoch 3/10
34/34 - 7s - loss: 175.2950 - loglik: -1.7394e+02 - logprior: -1.0457e+00
Epoch 4/10
34/34 - 7s - loss: 175.0506 - loglik: -1.7377e+02 - logprior: -9.6053e-01
Epoch 5/10
34/34 - 7s - loss: 174.4820 - loglik: -1.7329e+02 - logprior: -8.7047e-01
Epoch 6/10
34/34 - 8s - loss: 175.0450 - loglik: -1.7392e+02 - logprior: -8.0035e-01
Fitted a model with MAP estimate = -173.6195
Time for alignment: 172.2089
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 298.6576 - loglik: -2.9564e+02 - logprior: -2.9293e+00
Epoch 2/10
34/34 - 6s - loss: 205.9073 - loglik: -2.0393e+02 - logprior: -1.8907e+00
Epoch 3/10
34/34 - 7s - loss: 199.6048 - loglik: -1.9747e+02 - logprior: -1.8814e+00
Epoch 4/10
34/34 - 6s - loss: 199.6767 - loglik: -1.9759e+02 - logprior: -1.8484e+00
Fitted a model with MAP estimate = -198.0015
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (59, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 187.0947 - loglik: -1.8378e+02 - logprior: -3.1960e+00
Epoch 2/2
34/34 - 7s - loss: 177.1338 - loglik: -1.7539e+02 - logprior: -1.4911e+00
Fitted a model with MAP estimate = -175.5264
expansions: []
discards: [ 34 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 180.4179 - loglik: -1.7746e+02 - logprior: -2.8378e+00
Epoch 2/2
34/34 - 7s - loss: 176.6543 - loglik: -1.7509e+02 - logprior: -1.2983e+00
Fitted a model with MAP estimate = -175.7077
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 180.2189 - loglik: -1.7739e+02 - logprior: -2.7155e+00
Epoch 2/10
34/34 - 7s - loss: 176.7063 - loglik: -1.7529e+02 - logprior: -1.1575e+00
Epoch 3/10
34/34 - 7s - loss: 175.2523 - loglik: -1.7388e+02 - logprior: -1.0669e+00
Epoch 4/10
34/34 - 7s - loss: 174.7412 - loglik: -1.7343e+02 - logprior: -9.9065e-01
Epoch 5/10
34/34 - 7s - loss: 175.4048 - loglik: -1.7419e+02 - logprior: -9.0263e-01
Fitted a model with MAP estimate = -173.8465
Time for alignment: 140.0972
Computed alignments with likelihoods: ['-172.9132', '-173.6195', '-173.8465']
Best model has likelihood: -172.9132
time for generating output: 0.3298
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6271460965338517
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb69ffa82b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6868837f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb68628d5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb639292760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb639292040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb64a8261c0>, <__main__.SimpleDirichletPrior object at 0x7fb630ccb520>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.3105 - loglik: -3.9922e+02 - logprior: -2.1080e+01
Epoch 2/10
10/10 - 2s - loss: 365.1768 - loglik: -3.6006e+02 - logprior: -5.1156e+00
Epoch 3/10
10/10 - 2s - loss: 322.7130 - loglik: -3.1985e+02 - logprior: -2.8588e+00
Epoch 4/10
10/10 - 2s - loss: 293.9706 - loglik: -2.9140e+02 - logprior: -2.5070e+00
Epoch 5/10
10/10 - 2s - loss: 282.6744 - loglik: -2.7981e+02 - logprior: -2.6432e+00
Epoch 6/10
10/10 - 2s - loss: 278.3003 - loglik: -2.7530e+02 - logprior: -2.5809e+00
Epoch 7/10
10/10 - 2s - loss: 275.9937 - loglik: -2.7304e+02 - logprior: -2.4867e+00
Epoch 8/10
10/10 - 2s - loss: 276.0937 - loglik: -2.7326e+02 - logprior: -2.4000e+00
Fitted a model with MAP estimate = -274.4989
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 2), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (94, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 294.5973 - loglik: -2.7506e+02 - logprior: -1.9512e+01
Epoch 2/2
10/10 - 3s - loss: 266.9453 - loglik: -2.6151e+02 - logprior: -5.2622e+00
Fitted a model with MAP estimate = -262.0171
expansions: []
discards: [  0  66 104 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 288.9641 - loglik: -2.6498e+02 - logprior: -2.3959e+01
Epoch 2/2
10/10 - 2s - loss: 269.8024 - loglik: -2.5993e+02 - logprior: -9.7133e+00
Fitted a model with MAP estimate = -266.2410
expansions: [(0, 5), (121, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 278.8066 - loglik: -2.5972e+02 - logprior: -1.9061e+01
Epoch 2/10
10/10 - 3s - loss: 259.5055 - loglik: -2.5471e+02 - logprior: -4.6424e+00
Epoch 3/10
10/10 - 3s - loss: 254.2283 - loglik: -2.5191e+02 - logprior: -1.9617e+00
Epoch 4/10
10/10 - 3s - loss: 251.1911 - loglik: -2.4965e+02 - logprior: -1.0435e+00
Epoch 5/10
10/10 - 3s - loss: 248.6545 - loglik: -2.4751e+02 - logprior: -6.2212e-01
Epoch 6/10
10/10 - 3s - loss: 249.9527 - loglik: -2.4907e+02 - logprior: -4.0547e-01
Fitted a model with MAP estimate = -247.8742
Time for alignment: 65.5063
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.0729 - loglik: -3.9898e+02 - logprior: -2.1080e+01
Epoch 2/10
10/10 - 2s - loss: 363.9715 - loglik: -3.5884e+02 - logprior: -5.1236e+00
Epoch 3/10
10/10 - 2s - loss: 319.2202 - loglik: -3.1624e+02 - logprior: -2.9815e+00
Epoch 4/10
10/10 - 2s - loss: 288.8886 - loglik: -2.8607e+02 - logprior: -2.7596e+00
Epoch 5/10
10/10 - 2s - loss: 277.5526 - loglik: -2.7441e+02 - logprior: -2.9399e+00
Epoch 6/10
10/10 - 2s - loss: 273.6193 - loglik: -2.7033e+02 - logprior: -2.8973e+00
Epoch 7/10
10/10 - 2s - loss: 271.6087 - loglik: -2.6840e+02 - logprior: -2.7580e+00
Epoch 8/10
10/10 - 2s - loss: 270.4421 - loglik: -2.6733e+02 - logprior: -2.6799e+00
Epoch 9/10
10/10 - 2s - loss: 270.0432 - loglik: -2.6700e+02 - logprior: -2.6318e+00
Epoch 10/10
10/10 - 2s - loss: 269.9611 - loglik: -2.6696e+02 - logprior: -2.6041e+00
Fitted a model with MAP estimate = -269.1887
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (17, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 288.3792 - loglik: -2.6891e+02 - logprior: -1.9446e+01
Epoch 2/2
10/10 - 3s - loss: 262.7648 - loglik: -2.5755e+02 - logprior: -5.0484e+00
Fitted a model with MAP estimate = -258.1273
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 283.0605 - loglik: -2.5913e+02 - logprior: -2.3906e+01
Epoch 2/2
10/10 - 2s - loss: 266.1499 - loglik: -2.5631e+02 - logprior: -9.6833e+00
Fitted a model with MAP estimate = -262.6389
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.8344 - loglik: -2.5683e+02 - logprior: -1.8980e+01
Epoch 2/10
10/10 - 3s - loss: 259.0932 - loglik: -2.5435e+02 - logprior: -4.5908e+00
Epoch 3/10
10/10 - 3s - loss: 255.3893 - loglik: -2.5318e+02 - logprior: -1.8521e+00
Epoch 4/10
10/10 - 3s - loss: 251.8184 - loglik: -2.5044e+02 - logprior: -8.6839e-01
Epoch 5/10
10/10 - 3s - loss: 250.9895 - loglik: -2.5001e+02 - logprior: -4.4765e-01
Epoch 6/10
10/10 - 3s - loss: 249.4338 - loglik: -2.4873e+02 - logprior: -2.1057e-01
Epoch 7/10
10/10 - 3s - loss: 250.2384 - loglik: -2.4971e+02 - logprior: -7.6780e-02
Fitted a model with MAP estimate = -248.6768
Time for alignment: 72.0715
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.2693 - loglik: -3.9918e+02 - logprior: -2.1082e+01
Epoch 2/10
10/10 - 2s - loss: 364.7848 - loglik: -3.5966e+02 - logprior: -5.1184e+00
Epoch 3/10
10/10 - 2s - loss: 321.1258 - loglik: -3.1818e+02 - logprior: -2.9436e+00
Epoch 4/10
10/10 - 2s - loss: 290.7892 - loglik: -2.8806e+02 - logprior: -2.6462e+00
Epoch 5/10
10/10 - 2s - loss: 279.7704 - loglik: -2.7672e+02 - logprior: -2.7597e+00
Epoch 6/10
10/10 - 2s - loss: 275.1505 - loglik: -2.7193e+02 - logprior: -2.7155e+00
Epoch 7/10
10/10 - 2s - loss: 273.6535 - loglik: -2.7052e+02 - logprior: -2.6392e+00
Epoch 8/10
10/10 - 2s - loss: 271.7082 - loglik: -2.6879e+02 - logprior: -2.5451e+00
Epoch 9/10
10/10 - 2s - loss: 271.4319 - loglik: -2.6863e+02 - logprior: -2.4738e+00
Epoch 10/10
10/10 - 2s - loss: 269.7740 - loglik: -2.6694e+02 - logprior: -2.4953e+00
Fitted a model with MAP estimate = -269.6834
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.3780 - loglik: -2.6987e+02 - logprior: -1.9486e+01
Epoch 2/2
10/10 - 2s - loss: 265.4286 - loglik: -2.6018e+02 - logprior: -5.0705e+00
Fitted a model with MAP estimate = -260.9738
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 285.9542 - loglik: -2.6199e+02 - logprior: -2.3939e+01
Epoch 2/2
10/10 - 2s - loss: 269.1585 - loglik: -2.5928e+02 - logprior: -9.7099e+00
Fitted a model with MAP estimate = -265.7317
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 278.5204 - loglik: -2.5950e+02 - logprior: -1.9000e+01
Epoch 2/10
10/10 - 3s - loss: 262.5437 - loglik: -2.5775e+02 - logprior: -4.6417e+00
Epoch 3/10
10/10 - 3s - loss: 258.5181 - loglik: -2.5628e+02 - logprior: -1.8789e+00
Epoch 4/10
10/10 - 2s - loss: 254.9780 - loglik: -2.5353e+02 - logprior: -9.2253e-01
Epoch 5/10
10/10 - 3s - loss: 254.0930 - loglik: -2.5307e+02 - logprior: -4.8649e-01
Epoch 6/10
10/10 - 3s - loss: 252.8445 - loglik: -2.5207e+02 - logprior: -2.7428e-01
Epoch 7/10
10/10 - 2s - loss: 253.1386 - loglik: -2.5257e+02 - logprior: -1.0801e-01
Fitted a model with MAP estimate = -251.9268
Time for alignment: 70.6991
Computed alignments with likelihoods: ['-247.8742', '-248.6768', '-251.9268']
Best model has likelihood: -247.8742
time for generating output: 0.2082
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8824110671936759
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb560f13640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60e8bae50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6053c61c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6418054f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb641a16d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb630400550>, <__main__.SimpleDirichletPrior object at 0x7fb4e9bd0970>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 270.2330 - loglik: -2.3253e+02 - logprior: -3.7672e+01
Epoch 2/10
10/10 - 1s - loss: 230.9194 - loglik: -2.2080e+02 - logprior: -9.8931e+00
Epoch 3/10
10/10 - 1s - loss: 213.5009 - loglik: -2.0839e+02 - logprior: -4.6397e+00
Epoch 4/10
10/10 - 1s - loss: 202.7656 - loglik: -1.9958e+02 - logprior: -2.8416e+00
Epoch 5/10
10/10 - 1s - loss: 196.8018 - loglik: -1.9409e+02 - logprior: -2.4186e+00
Epoch 6/10
10/10 - 1s - loss: 193.1951 - loglik: -1.9034e+02 - logprior: -2.4571e+00
Epoch 7/10
10/10 - 1s - loss: 191.6251 - loglik: -1.8922e+02 - logprior: -1.9920e+00
Epoch 8/10
10/10 - 1s - loss: 190.8665 - loglik: -1.8903e+02 - logprior: -1.5031e+00
Epoch 9/10
10/10 - 1s - loss: 190.1874 - loglik: -1.8852e+02 - logprior: -1.3595e+00
Epoch 10/10
10/10 - 1s - loss: 189.9320 - loglik: -1.8833e+02 - logprior: -1.2930e+00
Fitted a model with MAP estimate = -189.3931
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.4948 - loglik: -2.0670e+02 - logprior: -4.9769e+01
Epoch 2/2
10/10 - 1s - loss: 205.1983 - loglik: -1.9015e+02 - logprior: -1.4857e+01
Fitted a model with MAP estimate = -195.2467
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 232.4105 - loglik: -1.9002e+02 - logprior: -4.2362e+01
Epoch 2/2
10/10 - 1s - loss: 203.3444 - loglik: -1.8678e+02 - logprior: -1.6417e+01
Fitted a model with MAP estimate = -198.2616
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.9619 - loglik: -1.8616e+02 - logprior: -3.7778e+01
Epoch 2/10
10/10 - 1s - loss: 194.1026 - loglik: -1.8423e+02 - logprior: -9.7286e+00
Epoch 3/10
10/10 - 1s - loss: 187.5555 - loglik: -1.8375e+02 - logprior: -3.5650e+00
Epoch 4/10
10/10 - 1s - loss: 184.7580 - loglik: -1.8305e+02 - logprior: -1.4265e+00
Epoch 5/10
10/10 - 1s - loss: 183.1684 - loglik: -1.8247e+02 - logprior: -3.5196e-01
Epoch 6/10
10/10 - 1s - loss: 182.2360 - loglik: -1.8208e+02 - logprior: 0.2128
Epoch 7/10
10/10 - 1s - loss: 181.6918 - loglik: -1.8186e+02 - logprior: 0.5097
Epoch 8/10
10/10 - 1s - loss: 181.0632 - loglik: -1.8144e+02 - logprior: 0.6781
Epoch 9/10
10/10 - 1s - loss: 180.8678 - loglik: -1.8139e+02 - logprior: 0.7961
Epoch 10/10
10/10 - 1s - loss: 180.5196 - loglik: -1.8115e+02 - logprior: 0.9036
Fitted a model with MAP estimate = -180.0853
Time for alignment: 48.4921
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3471 - loglik: -2.3265e+02 - logprior: -3.7671e+01
Epoch 2/10
10/10 - 1s - loss: 231.0197 - loglik: -2.2089e+02 - logprior: -9.8979e+00
Epoch 3/10
10/10 - 1s - loss: 214.3413 - loglik: -2.0919e+02 - logprior: -4.6597e+00
Epoch 4/10
10/10 - 1s - loss: 203.8436 - loglik: -2.0061e+02 - logprior: -2.8510e+00
Epoch 5/10
10/10 - 1s - loss: 197.5277 - loglik: -1.9478e+02 - logprior: -2.3402e+00
Epoch 6/10
10/10 - 1s - loss: 193.6516 - loglik: -1.9081e+02 - logprior: -2.2955e+00
Epoch 7/10
10/10 - 1s - loss: 191.6301 - loglik: -1.8917e+02 - logprior: -2.0049e+00
Epoch 8/10
10/10 - 1s - loss: 190.5295 - loglik: -1.8870e+02 - logprior: -1.5059e+00
Epoch 9/10
10/10 - 1s - loss: 190.0301 - loglik: -1.8844e+02 - logprior: -1.3059e+00
Epoch 10/10
10/10 - 1s - loss: 189.3636 - loglik: -1.8781e+02 - logprior: -1.2728e+00
Fitted a model with MAP estimate = -188.9497
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (56, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 256.4839 - loglik: -2.0668e+02 - logprior: -4.9772e+01
Epoch 2/2
10/10 - 1s - loss: 205.0037 - loglik: -1.8993e+02 - logprior: -1.4890e+01
Fitted a model with MAP estimate = -195.1206
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 232.4952 - loglik: -1.9007e+02 - logprior: -4.2399e+01
Epoch 2/2
10/10 - 1s - loss: 203.3899 - loglik: -1.8683e+02 - logprior: -1.6409e+01
Fitted a model with MAP estimate = -198.4314
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 224.4251 - loglik: -1.8659e+02 - logprior: -3.7809e+01
Epoch 2/10
10/10 - 1s - loss: 194.8929 - loglik: -1.8496e+02 - logprior: -9.7884e+00
Epoch 3/10
10/10 - 1s - loss: 188.0686 - loglik: -1.8416e+02 - logprior: -3.6558e+00
Epoch 4/10
10/10 - 1s - loss: 185.5043 - loglik: -1.8370e+02 - logprior: -1.5014e+00
Epoch 5/10
10/10 - 1s - loss: 184.1926 - loglik: -1.8339e+02 - logprior: -4.4435e-01
Epoch 6/10
10/10 - 1s - loss: 182.8868 - loglik: -1.8264e+02 - logprior: 0.1097
Epoch 7/10
10/10 - 1s - loss: 182.1695 - loglik: -1.8223e+02 - logprior: 0.3751
Epoch 8/10
10/10 - 1s - loss: 181.7184 - loglik: -1.8193e+02 - logprior: 0.4998
Epoch 9/10
10/10 - 1s - loss: 181.4575 - loglik: -1.8177e+02 - logprior: 0.5980
Epoch 10/10
10/10 - 1s - loss: 181.0662 - loglik: -1.8151e+02 - logprior: 0.7288
Fitted a model with MAP estimate = -180.5225
Time for alignment: 49.8598
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.4086 - loglik: -2.3271e+02 - logprior: -3.7672e+01
Epoch 2/10
10/10 - 1s - loss: 230.4949 - loglik: -2.2037e+02 - logprior: -9.8966e+00
Epoch 3/10
10/10 - 1s - loss: 213.1425 - loglik: -2.0803e+02 - logprior: -4.6505e+00
Epoch 4/10
10/10 - 1s - loss: 202.6897 - loglik: -1.9948e+02 - logprior: -2.8819e+00
Epoch 5/10
10/10 - 1s - loss: 197.1340 - loglik: -1.9447e+02 - logprior: -2.3416e+00
Epoch 6/10
10/10 - 1s - loss: 193.3010 - loglik: -1.9048e+02 - logprior: -2.3260e+00
Epoch 7/10
10/10 - 1s - loss: 191.6792 - loglik: -1.8930e+02 - logprior: -1.9400e+00
Epoch 8/10
10/10 - 1s - loss: 190.6915 - loglik: -1.8894e+02 - logprior: -1.4299e+00
Epoch 9/10
10/10 - 1s - loss: 190.0751 - loglik: -1.8851e+02 - logprior: -1.2789e+00
Epoch 10/10
10/10 - 1s - loss: 189.7652 - loglik: -1.8827e+02 - logprior: -1.2277e+00
Fitted a model with MAP estimate = -189.1536
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.6116 - loglik: -2.0585e+02 - logprior: -4.9729e+01
Epoch 2/2
10/10 - 1s - loss: 204.8225 - loglik: -1.8978e+02 - logprior: -1.4860e+01
Fitted a model with MAP estimate = -195.0423
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 232.3087 - loglik: -1.8988e+02 - logprior: -4.2400e+01
Epoch 2/2
10/10 - 1s - loss: 203.5205 - loglik: -1.8696e+02 - logprior: -1.6406e+01
Fitted a model with MAP estimate = -198.5576
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.1377 - loglik: -1.8633e+02 - logprior: -3.7783e+01
Epoch 2/10
10/10 - 1s - loss: 194.5184 - loglik: -1.8466e+02 - logprior: -9.7114e+00
Epoch 3/10
10/10 - 1s - loss: 187.6699 - loglik: -1.8386e+02 - logprior: -3.5552e+00
Epoch 4/10
10/10 - 1s - loss: 184.9490 - loglik: -1.8322e+02 - logprior: -1.4025e+00
Epoch 5/10
10/10 - 1s - loss: 183.4549 - loglik: -1.8275e+02 - logprior: -3.3921e-01
Epoch 6/10
10/10 - 1s - loss: 182.4387 - loglik: -1.8230e+02 - logprior: 0.2229
Epoch 7/10
10/10 - 1s - loss: 181.6783 - loglik: -1.8186e+02 - logprior: 0.5062
Epoch 8/10
10/10 - 1s - loss: 181.3611 - loglik: -1.8173e+02 - logprior: 0.6650
Epoch 9/10
10/10 - 1s - loss: 181.0091 - loglik: -1.8151e+02 - logprior: 0.7814
Epoch 10/10
10/10 - 1s - loss: 180.6242 - loglik: -1.8123e+02 - logprior: 0.8893
Fitted a model with MAP estimate = -180.1575
Time for alignment: 47.1903
Computed alignments with likelihoods: ['-180.0853', '-180.5225', '-180.1575']
Best model has likelihood: -180.0853
time for generating output: 0.1883
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7885117493472585
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6052bd910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4c93430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb604925640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb64af00400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64af000d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6061775b0>, <__main__.SimpleDirichletPrior object at 0x7fb65306ca00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 952.5560 - loglik: -9.5050e+02 - logprior: -1.4261e+00
Epoch 2/10
43/43 - 31s - loss: 838.9619 - loglik: -8.3504e+02 - logprior: -1.9335e+00
Epoch 3/10
43/43 - 32s - loss: 821.6945 - loglik: -8.1756e+02 - logprior: -2.0850e+00
Epoch 4/10
43/43 - 32s - loss: 818.9769 - loglik: -8.1474e+02 - logprior: -2.1326e+00
Epoch 5/10
43/43 - 31s - loss: 814.0387 - loglik: -8.1001e+02 - logprior: -2.1581e+00
Epoch 6/10
43/43 - 32s - loss: 813.7850 - loglik: -8.0991e+02 - logprior: -2.1617e+00
Epoch 7/10
43/43 - 31s - loss: 812.4379 - loglik: -8.0864e+02 - logprior: -2.1945e+00
Epoch 8/10
43/43 - 32s - loss: 812.2163 - loglik: -8.0851e+02 - logprior: -2.2042e+00
Epoch 9/10
43/43 - 32s - loss: 811.6448 - loglik: -8.0803e+02 - logprior: -2.2275e+00
Epoch 10/10
43/43 - 32s - loss: 810.7545 - loglik: -8.0712e+02 - logprior: -2.3346e+00
Fitted a model with MAP estimate = -821.7631
expansions: [(14, 1), (16, 3), (21, 1), (22, 1), (23, 3), (24, 2), (25, 1), (29, 1), (39, 1), (40, 1), (41, 1), (45, 1), (46, 2), (49, 2), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 2), (93, 1), (95, 1), (96, 1), (98, 1), (104, 2), (121, 1), (122, 1), (123, 1), (125, 1), (130, 2), (142, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (168, 1), (183, 2), (185, 3), (186, 1), (187, 1), (188, 2), (203, 1), (205, 3), (206, 1), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (236, 1), (239, 2), (242, 1), (245, 2), (248, 1), (250, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 373 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 831.5887 - loglik: -8.2855e+02 - logprior: -2.5638e+00
Epoch 2/2
43/43 - 48s - loss: 788.1682 - loglik: -7.8492e+02 - logprior: -1.6357e+00
Fitted a model with MAP estimate = -779.6689
expansions: [(0, 2)]
discards: [  0  17  28  34  62  67 113 122 140 172 201 210 240 245 251 272 273 303
 318 328 361 364]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 49s - loss: 802.2513 - loglik: -8.0060e+02 - logprior: -1.3350e+00
Epoch 2/2
43/43 - 44s - loss: 784.8738 - loglik: -7.8280e+02 - logprior: -6.7460e-01
Fitted a model with MAP estimate = -778.9699
expansions: [(282, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 778.1618 - loglik: -7.7663e+02 - logprior: -9.8733e-01
Epoch 2/10
61/61 - 61s - loss: 761.2209 - loglik: -7.5839e+02 - logprior: -6.5824e-01
Epoch 3/10
61/61 - 61s - loss: 755.9782 - loglik: -7.5255e+02 - logprior: -6.2641e-01
Epoch 4/10
61/61 - 61s - loss: 753.5693 - loglik: -7.5004e+02 - logprior: -5.4206e-01
Epoch 5/10
61/61 - 61s - loss: 754.1393 - loglik: -7.5074e+02 - logprior: -4.7085e-01
Fitted a model with MAP estimate = -748.6496
Time for alignment: 1138.8931
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 37s - loss: 948.3044 - loglik: -9.4622e+02 - logprior: -1.4527e+00
Epoch 2/10
43/43 - 32s - loss: 833.2031 - loglik: -8.2916e+02 - logprior: -2.0983e+00
Epoch 3/10
43/43 - 32s - loss: 817.2410 - loglik: -8.1302e+02 - logprior: -2.2721e+00
Epoch 4/10
43/43 - 32s - loss: 814.2268 - loglik: -8.1002e+02 - logprior: -2.2700e+00
Epoch 5/10
43/43 - 32s - loss: 811.2110 - loglik: -8.0714e+02 - logprior: -2.2547e+00
Epoch 6/10
43/43 - 31s - loss: 811.2930 - loglik: -8.0738e+02 - logprior: -2.2597e+00
Fitted a model with MAP estimate = -812.9211
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (101, 1), (103, 2), (120, 1), (122, 1), (123, 1), (125, 1), (130, 2), (132, 2), (144, 1), (147, 1), (149, 1), (152, 2), (155, 2), (156, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (203, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (244, 2), (245, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 51s - loss: 820.6588 - loglik: -8.1777e+02 - logprior: -2.4351e+00
Epoch 2/2
43/43 - 47s - loss: 786.3657 - loglik: -7.8346e+02 - logprior: -1.3992e+00
Fitted a model with MAP estimate = -779.6905
expansions: [(0, 2)]
discards: [  0  27  31  60 110 135 169 172 198 205 264 265 266 297 321 355 357]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 801.3278 - loglik: -7.9966e+02 - logprior: -1.3835e+00
Epoch 2/2
43/43 - 44s - loss: 784.4117 - loglik: -7.8231e+02 - logprior: -7.3488e-01
Fitted a model with MAP estimate = -779.2975
expansions: [(256, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 779.2444 - loglik: -7.7763e+02 - logprior: -1.0874e+00
Epoch 2/10
61/61 - 60s - loss: 759.9973 - loglik: -7.5725e+02 - logprior: -5.7594e-01
Epoch 3/10
61/61 - 61s - loss: 756.6926 - loglik: -7.5339e+02 - logprior: -5.2556e-01
Epoch 4/10
61/61 - 61s - loss: 754.3344 - loglik: -7.5087e+02 - logprior: -4.7118e-01
Epoch 5/10
61/61 - 61s - loss: 754.2573 - loglik: -7.5091e+02 - logprior: -3.9468e-01
Epoch 6/10
61/61 - 61s - loss: 751.5303 - loglik: -7.4838e+02 - logprior: -3.4468e-01
Epoch 7/10
61/61 - 61s - loss: 751.4648 - loglik: -7.4831e+02 - logprior: -2.6306e-01
Epoch 8/10
61/61 - 61s - loss: 751.0638 - loglik: -7.4829e+02 - logprior: -1.8622e-01
Epoch 9/10
61/61 - 61s - loss: 748.6424 - loglik: -7.4618e+02 - logprior: -1.3151e-01
Epoch 10/10
61/61 - 61s - loss: 748.5066 - loglik: -7.4611e+02 - logprior: -7.4524e-02
Fitted a model with MAP estimate = -744.4594
Time for alignment: 1309.3308
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 36s - loss: 950.7408 - loglik: -9.4869e+02 - logprior: -1.4271e+00
Epoch 2/10
43/43 - 32s - loss: 833.7459 - loglik: -8.2972e+02 - logprior: -2.0770e+00
Epoch 3/10
43/43 - 31s - loss: 817.7723 - loglik: -8.1359e+02 - logprior: -2.2296e+00
Epoch 4/10
43/43 - 32s - loss: 814.9194 - loglik: -8.1092e+02 - logprior: -2.1837e+00
Epoch 5/10
43/43 - 32s - loss: 812.9715 - loglik: -8.0915e+02 - logprior: -2.1562e+00
Epoch 6/10
43/43 - 32s - loss: 812.6645 - loglik: -8.0899e+02 - logprior: -2.1690e+00
Epoch 7/10
43/43 - 31s - loss: 811.5670 - loglik: -8.0795e+02 - logprior: -2.1869e+00
Epoch 8/10
43/43 - 32s - loss: 810.9985 - loglik: -8.0745e+02 - logprior: -2.1933e+00
Epoch 9/10
43/43 - 32s - loss: 809.7982 - loglik: -8.0633e+02 - logprior: -2.1999e+00
Epoch 10/10
43/43 - 31s - loss: 810.9221 - loglik: -8.0746e+02 - logprior: -2.2379e+00
Fitted a model with MAP estimate = -820.2116
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 2), (93, 1), (95, 1), (97, 1), (101, 1), (103, 2), (120, 1), (121, 1), (123, 1), (128, 2), (130, 1), (132, 2), (148, 2), (153, 1), (154, 1), (155, 3), (157, 2), (181, 1), (183, 2), (185, 3), (186, 1), (188, 1), (197, 1), (203, 1), (205, 3), (206, 2), (207, 1), (210, 1), (221, 1), (222, 1), (223, 2), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 2), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 51s - loss: 826.3110 - loglik: -8.2341e+02 - logprior: -2.4255e+00
Epoch 2/2
43/43 - 48s - loss: 788.0469 - loglik: -7.8500e+02 - logprior: -1.4519e+00
Fitted a model with MAP estimate = -779.3040
expansions: [(0, 2)]
discards: [ 24  59 109 118 135 166 172 206 236 241 266 267 268 300 315 325 358 361]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 801.2504 - loglik: -7.9956e+02 - logprior: -1.3873e+00
Epoch 2/2
43/43 - 44s - loss: 783.8408 - loglik: -7.8174e+02 - logprior: -7.2489e-01
Fitted a model with MAP estimate = -778.8004
expansions: [(258, 2)]
discards: [  0 283]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 779.4724 - loglik: -7.7790e+02 - logprior: -1.0374e+00
Epoch 2/10
61/61 - 62s - loss: 760.9426 - loglik: -7.5809e+02 - logprior: -6.8170e-01
Epoch 3/10
61/61 - 61s - loss: 754.8387 - loglik: -7.5143e+02 - logprior: -6.1774e-01
Epoch 4/10
61/61 - 62s - loss: 753.0712 - loglik: -7.4950e+02 - logprior: -5.4830e-01
Epoch 5/10
61/61 - 62s - loss: 754.4785 - loglik: -7.5103e+02 - logprior: -4.7794e-01
Fitted a model with MAP estimate = -748.4899
Time for alignment: 1132.0614
Computed alignments with likelihoods: ['-748.6496', '-744.4594', '-748.4899']
Best model has likelihood: -744.4594
time for generating output: 0.4201
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.7987310539302079
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6c522b700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c435ba90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f216b50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb603c001f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb603c00b80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb64ad547c0>, <__main__.SimpleDirichletPrior object at 0x7fb6037101c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 250.2579 - loglik: -1.9318e+02 - logprior: -5.7061e+01
Epoch 2/10
10/10 - 2s - loss: 188.4985 - loglik: -1.7251e+02 - logprior: -1.5960e+01
Epoch 3/10
10/10 - 2s - loss: 163.2139 - loglik: -1.5524e+02 - logprior: -7.9715e+00
Epoch 4/10
10/10 - 2s - loss: 152.9260 - loglik: -1.4794e+02 - logprior: -4.9661e+00
Epoch 5/10
10/10 - 2s - loss: 149.2242 - loglik: -1.4539e+02 - logprior: -3.6700e+00
Epoch 6/10
10/10 - 2s - loss: 148.1824 - loglik: -1.4502e+02 - logprior: -2.8259e+00
Epoch 7/10
10/10 - 2s - loss: 146.8752 - loglik: -1.4426e+02 - logprior: -2.2638e+00
Epoch 8/10
10/10 - 2s - loss: 146.1705 - loglik: -1.4384e+02 - logprior: -2.0196e+00
Epoch 9/10
10/10 - 2s - loss: 146.7199 - loglik: -1.4446e+02 - logprior: -1.9257e+00
Fitted a model with MAP estimate = -145.4963
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.9516 - loglik: -1.4540e+02 - logprior: -6.4531e+01
Epoch 2/2
10/10 - 2s - loss: 169.9658 - loglik: -1.4244e+02 - logprior: -2.7348e+01
Fitted a model with MAP estimate = -163.5095
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 193.7185 - loglik: -1.4109e+02 - logprior: -5.2602e+01
Epoch 2/2
10/10 - 2s - loss: 155.5277 - loglik: -1.4055e+02 - logprior: -1.4843e+01
Fitted a model with MAP estimate = -149.8296
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 203.3761 - loglik: -1.4078e+02 - logprior: -6.2577e+01
Epoch 2/10
10/10 - 2s - loss: 163.9656 - loglik: -1.4240e+02 - logprior: -2.1441e+01
Epoch 3/10
10/10 - 2s - loss: 150.4378 - loglik: -1.4140e+02 - logprior: -8.7721e+00
Epoch 4/10
10/10 - 2s - loss: 145.2991 - loglik: -1.4061e+02 - logprior: -4.3700e+00
Epoch 5/10
10/10 - 2s - loss: 143.4487 - loglik: -1.4052e+02 - logprior: -2.5874e+00
Epoch 6/10
10/10 - 2s - loss: 143.4128 - loglik: -1.4131e+02 - logprior: -1.7316e+00
Epoch 7/10
10/10 - 2s - loss: 142.1297 - loglik: -1.4044e+02 - logprior: -1.2908e+00
Epoch 8/10
10/10 - 2s - loss: 141.9787 - loglik: -1.4054e+02 - logprior: -1.0039e+00
Epoch 9/10
10/10 - 2s - loss: 141.1978 - loglik: -1.4000e+02 - logprior: -7.4336e-01
Epoch 10/10
10/10 - 2s - loss: 141.5025 - loglik: -1.4048e+02 - logprior: -5.4961e-01
Fitted a model with MAP estimate = -140.6246
Time for alignment: 54.3984
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 250.2030 - loglik: -1.9312e+02 - logprior: -5.7063e+01
Epoch 2/10
10/10 - 2s - loss: 188.4150 - loglik: -1.7242e+02 - logprior: -1.5961e+01
Epoch 3/10
10/10 - 2s - loss: 162.5779 - loglik: -1.5460e+02 - logprior: -7.9765e+00
Epoch 4/10
10/10 - 2s - loss: 152.8459 - loglik: -1.4775e+02 - logprior: -5.0238e+00
Epoch 5/10
10/10 - 2s - loss: 149.4122 - loglik: -1.4540e+02 - logprior: -3.7442e+00
Epoch 6/10
10/10 - 2s - loss: 147.9610 - loglik: -1.4470e+02 - logprior: -2.9054e+00
Epoch 7/10
10/10 - 2s - loss: 146.8821 - loglik: -1.4420e+02 - logprior: -2.3628e+00
Epoch 8/10
10/10 - 2s - loss: 146.5370 - loglik: -1.4412e+02 - logprior: -2.1070e+00
Epoch 9/10
10/10 - 2s - loss: 145.8473 - loglik: -1.4353e+02 - logprior: -1.9835e+00
Epoch 10/10
10/10 - 2s - loss: 146.0239 - loglik: -1.4379e+02 - logprior: -1.8563e+00
Fitted a model with MAP estimate = -145.2641
expansions: [(11, 1), (12, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.3365 - loglik: -1.4439e+02 - logprior: -5.1923e+01
Epoch 2/2
10/10 - 2s - loss: 156.3056 - loglik: -1.4118e+02 - logprior: -1.4962e+01
Fitted a model with MAP estimate = -150.1067
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 207.7753 - loglik: -1.4280e+02 - logprior: -6.4953e+01
Epoch 2/2
10/10 - 1s - loss: 169.0429 - loglik: -1.4114e+02 - logprior: -2.7767e+01
Fitted a model with MAP estimate = -163.6648
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 194.9824 - loglik: -1.4128e+02 - logprior: -5.3677e+01
Epoch 2/10
10/10 - 2s - loss: 154.7390 - loglik: -1.3972e+02 - logprior: -1.4900e+01
Epoch 3/10
10/10 - 2s - loss: 148.1091 - loglik: -1.4079e+02 - logprior: -7.0693e+00
Epoch 4/10
10/10 - 2s - loss: 143.8707 - loglik: -1.3958e+02 - logprior: -3.9888e+00
Epoch 5/10
10/10 - 2s - loss: 143.6176 - loglik: -1.4087e+02 - logprior: -2.4264e+00
Epoch 6/10
10/10 - 2s - loss: 141.9755 - loglik: -1.4005e+02 - logprior: -1.5759e+00
Epoch 7/10
10/10 - 2s - loss: 141.2598 - loglik: -1.3983e+02 - logprior: -1.0405e+00
Epoch 8/10
10/10 - 2s - loss: 140.7263 - loglik: -1.3962e+02 - logprior: -6.8948e-01
Epoch 9/10
10/10 - 2s - loss: 141.1180 - loglik: -1.4023e+02 - logprior: -4.5864e-01
Fitted a model with MAP estimate = -140.2784
Time for alignment: 54.5726
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.2904 - loglik: -1.9321e+02 - logprior: -5.7058e+01
Epoch 2/10
10/10 - 2s - loss: 187.9877 - loglik: -1.7201e+02 - logprior: -1.5944e+01
Epoch 3/10
10/10 - 2s - loss: 162.1334 - loglik: -1.5415e+02 - logprior: -7.9802e+00
Epoch 4/10
10/10 - 2s - loss: 153.9161 - loglik: -1.4894e+02 - logprior: -4.9331e+00
Epoch 5/10
10/10 - 2s - loss: 149.7462 - loglik: -1.4602e+02 - logprior: -3.5067e+00
Epoch 6/10
10/10 - 2s - loss: 147.1667 - loglik: -1.4416e+02 - logprior: -2.6630e+00
Epoch 7/10
10/10 - 1s - loss: 146.7278 - loglik: -1.4425e+02 - logprior: -2.1402e+00
Epoch 8/10
10/10 - 2s - loss: 145.7361 - loglik: -1.4353e+02 - logprior: -1.8977e+00
Epoch 9/10
10/10 - 2s - loss: 146.1060 - loglik: -1.4403e+02 - logprior: -1.7538e+00
Fitted a model with MAP estimate = -145.3061
expansions: [(13, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 209.4892 - loglik: -1.4489e+02 - logprior: -6.4575e+01
Epoch 2/2
10/10 - 2s - loss: 170.9912 - loglik: -1.4346e+02 - logprior: -2.7374e+01
Fitted a model with MAP estimate = -164.2464
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 194.3925 - loglik: -1.4178e+02 - logprior: -5.2590e+01
Epoch 2/2
10/10 - 2s - loss: 156.2993 - loglik: -1.4129e+02 - logprior: -1.4876e+01
Fitted a model with MAP estimate = -150.4934
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.7935 - loglik: -1.4260e+02 - logprior: -6.2173e+01
Epoch 2/10
10/10 - 1s - loss: 162.0627 - loglik: -1.4135e+02 - logprior: -2.0589e+01
Epoch 3/10
10/10 - 1s - loss: 150.1068 - loglik: -1.4139e+02 - logprior: -8.4469e+00
Epoch 4/10
10/10 - 2s - loss: 146.5711 - loglik: -1.4191e+02 - logprior: -4.3354e+00
Epoch 5/10
10/10 - 1s - loss: 144.4704 - loglik: -1.4154e+02 - logprior: -2.5932e+00
Epoch 6/10
10/10 - 2s - loss: 143.5340 - loglik: -1.4141e+02 - logprior: -1.7572e+00
Epoch 7/10
10/10 - 2s - loss: 143.0588 - loglik: -1.4133e+02 - logprior: -1.3265e+00
Epoch 8/10
10/10 - 1s - loss: 142.2976 - loglik: -1.4085e+02 - logprior: -1.0198e+00
Epoch 9/10
10/10 - 2s - loss: 142.0390 - loglik: -1.4083e+02 - logprior: -7.6389e-01
Epoch 10/10
10/10 - 2s - loss: 142.3892 - loglik: -1.4138e+02 - logprior: -5.4940e-01
Fitted a model with MAP estimate = -141.3848
Time for alignment: 53.0338
Computed alignments with likelihoods: ['-140.6246', '-140.2784', '-141.3848']
Best model has likelihood: -140.2784
time for generating output: 0.1273
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9472954230235784
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4e9c4d400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686d9e850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e0c89190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4ea03e8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e0b29df0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57ac53190>, <__main__.SimpleDirichletPrior object at 0x7fb6641c1b20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 540.2656 - loglik: -4.7204e+02 - logprior: -6.8215e+01
Epoch 2/10
10/10 - 2s - loss: 441.6895 - loglik: -4.2696e+02 - logprior: -1.4719e+01
Epoch 3/10
10/10 - 2s - loss: 389.3447 - loglik: -3.8355e+02 - logprior: -5.7912e+00
Epoch 4/10
10/10 - 2s - loss: 359.9748 - loglik: -3.5708e+02 - logprior: -2.8840e+00
Epoch 5/10
10/10 - 2s - loss: 344.5973 - loglik: -3.4263e+02 - logprior: -1.7952e+00
Epoch 6/10
10/10 - 2s - loss: 336.4042 - loglik: -3.3484e+02 - logprior: -1.1668e+00
Epoch 7/10
10/10 - 2s - loss: 332.7330 - loglik: -3.3177e+02 - logprior: -5.5852e-01
Epoch 8/10
10/10 - 2s - loss: 330.6718 - loglik: -3.3024e+02 - logprior: -9.7901e-02
Epoch 9/10
10/10 - 2s - loss: 329.7811 - loglik: -3.2962e+02 - logprior: 0.1569
Epoch 10/10
10/10 - 2s - loss: 329.2202 - loglik: -3.2928e+02 - logprior: 0.3977
Fitted a model with MAP estimate = -328.3443
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 1), (38, 1), (40, 2), (49, 1), (65, 2), (69, 1), (77, 3), (80, 2), (89, 1), (90, 3), (103, 1), (110, 2), (114, 2), (115, 1), (120, 1), (122, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 409.6095 - loglik: -3.3214e+02 - logprior: -7.7449e+01
Epoch 2/2
10/10 - 3s - loss: 343.8857 - loglik: -3.1423e+02 - logprior: -2.9481e+01
Fitted a model with MAP estimate = -331.0790
expansions: [(0, 2), (150, 1)]
discards: [  0  11  44 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 370.9854 - loglik: -3.1076e+02 - logprior: -6.0197e+01
Epoch 2/2
10/10 - 3s - loss: 316.6938 - loglik: -3.0424e+02 - logprior: -1.2305e+01
Fitted a model with MAP estimate = -306.9836
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 380.6022 - loglik: -3.0753e+02 - logprior: -7.3050e+01
Epoch 2/10
10/10 - 3s - loss: 322.9573 - loglik: -3.0390e+02 - logprior: -1.8903e+01
Epoch 3/10
10/10 - 3s - loss: 305.1315 - loglik: -3.0125e+02 - logprior: -3.5464e+00
Epoch 4/10
10/10 - 3s - loss: 297.9485 - loglik: -2.9934e+02 - logprior: 1.7810
Epoch 5/10
10/10 - 3s - loss: 294.1209 - loglik: -2.9802e+02 - logprior: 4.2904
Epoch 6/10
10/10 - 3s - loss: 292.0607 - loglik: -2.9740e+02 - logprior: 5.7715
Epoch 7/10
10/10 - 3s - loss: 290.7989 - loglik: -2.9704e+02 - logprior: 6.6956
Epoch 8/10
10/10 - 3s - loss: 289.7726 - loglik: -2.9677e+02 - logprior: 7.4719
Epoch 9/10
10/10 - 3s - loss: 288.9829 - loglik: -2.9668e+02 - logprior: 8.1695
Epoch 10/10
10/10 - 3s - loss: 288.5990 - loglik: -2.9691e+02 - logprior: 8.7895
Fitted a model with MAP estimate = -287.6300
Time for alignment: 90.2569
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 539.9726 - loglik: -4.7175e+02 - logprior: -6.8213e+01
Epoch 2/10
10/10 - 2s - loss: 440.4214 - loglik: -4.2567e+02 - logprior: -1.4741e+01
Epoch 3/10
10/10 - 2s - loss: 386.7242 - loglik: -3.8081e+02 - logprior: -5.9032e+00
Epoch 4/10
10/10 - 2s - loss: 355.7016 - loglik: -3.5256e+02 - logprior: -3.1423e+00
Epoch 5/10
10/10 - 2s - loss: 340.8562 - loglik: -3.3880e+02 - logprior: -1.9750e+00
Epoch 6/10
10/10 - 2s - loss: 332.8311 - loglik: -3.3115e+02 - logprior: -1.3603e+00
Epoch 7/10
10/10 - 2s - loss: 329.0945 - loglik: -3.2786e+02 - logprior: -8.0826e-01
Epoch 8/10
10/10 - 2s - loss: 327.1953 - loglik: -3.2648e+02 - logprior: -3.2813e-01
Epoch 9/10
10/10 - 2s - loss: 325.0145 - loglik: -3.2454e+02 - logprior: -1.0886e-01
Epoch 10/10
10/10 - 2s - loss: 323.7553 - loglik: -3.2336e+02 - logprior: 0.0201
Fitted a model with MAP estimate = -322.7087
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (49, 1), (51, 1), (55, 1), (66, 1), (71, 2), (77, 2), (78, 2), (80, 2), (89, 1), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 404.4900 - loglik: -3.2687e+02 - logprior: -7.7593e+01
Epoch 2/2
10/10 - 3s - loss: 338.1453 - loglik: -3.0853e+02 - logprior: -2.9438e+01
Fitted a model with MAP estimate = -326.8841
expansions: [(0, 2), (81, 1)]
discards: [ 0 22 85]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 365.6837 - loglik: -3.0554e+02 - logprior: -6.0122e+01
Epoch 2/2
10/10 - 3s - loss: 312.2719 - loglik: -3.0005e+02 - logprior: -1.2070e+01
Fitted a model with MAP estimate = -302.6380
expansions: []
discards: [  0 161]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 377.1216 - loglik: -3.0433e+02 - logprior: -7.2767e+01
Epoch 2/10
10/10 - 3s - loss: 319.6054 - loglik: -3.0084e+02 - logprior: -1.8619e+01
Epoch 3/10
10/10 - 3s - loss: 301.3716 - loglik: -2.9774e+02 - logprior: -3.2897e+00
Epoch 4/10
10/10 - 3s - loss: 294.7250 - loglik: -2.9642e+02 - logprior: 2.0837
Epoch 5/10
10/10 - 3s - loss: 290.9172 - loglik: -2.9525e+02 - logprior: 4.7025
Epoch 6/10
10/10 - 3s - loss: 289.5438 - loglik: -2.9531e+02 - logprior: 6.1494
Epoch 7/10
10/10 - 3s - loss: 287.7730 - loglik: -2.9442e+02 - logprior: 7.0702
Epoch 8/10
10/10 - 3s - loss: 287.1102 - loglik: -2.9451e+02 - logprior: 7.8496
Epoch 9/10
10/10 - 3s - loss: 286.5490 - loglik: -2.9463e+02 - logprior: 8.5544
Epoch 10/10
10/10 - 3s - loss: 285.8964 - loglik: -2.9461e+02 - logprior: 9.1911
Fitted a model with MAP estimate = -285.1825
Time for alignment: 89.9061
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.1382 - loglik: -4.7191e+02 - logprior: -6.8214e+01
Epoch 2/10
10/10 - 2s - loss: 440.9821 - loglik: -4.2625e+02 - logprior: -1.4723e+01
Epoch 3/10
10/10 - 2s - loss: 388.7157 - loglik: -3.8298e+02 - logprior: -5.7335e+00
Epoch 4/10
10/10 - 2s - loss: 358.9733 - loglik: -3.5596e+02 - logprior: -3.0007e+00
Epoch 5/10
10/10 - 2s - loss: 343.8196 - loglik: -3.4172e+02 - logprior: -1.9070e+00
Epoch 6/10
10/10 - 2s - loss: 336.4701 - loglik: -3.3485e+02 - logprior: -1.1323e+00
Epoch 7/10
10/10 - 2s - loss: 332.6288 - loglik: -3.3152e+02 - logprior: -5.5952e-01
Epoch 8/10
10/10 - 2s - loss: 330.7663 - loglik: -3.3007e+02 - logprior: -2.1565e-01
Epoch 9/10
10/10 - 2s - loss: 329.7474 - loglik: -3.2940e+02 - logprior: 0.0921
Epoch 10/10
10/10 - 2s - loss: 328.5451 - loglik: -3.2837e+02 - logprior: 0.2985
Fitted a model with MAP estimate = -327.7307
expansions: [(11, 3), (12, 2), (16, 3), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (55, 1), (66, 1), (77, 3), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 408.2703 - loglik: -3.3083e+02 - logprior: -7.7416e+01
Epoch 2/2
10/10 - 3s - loss: 339.3492 - loglik: -3.0961e+02 - logprior: -2.9553e+01
Fitted a model with MAP estimate = -326.9089
expansions: [(0, 1), (26, 1), (62, 1), (129, 1)]
discards: [  0  11  51 115]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 366.9546 - loglik: -3.0652e+02 - logprior: -6.0414e+01
Epoch 2/2
10/10 - 3s - loss: 312.8286 - loglik: -3.0023e+02 - logprior: -1.2443e+01
Fitted a model with MAP estimate = -303.3945
expansions: [(46, 1)]
discards: [142]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 361.1799 - loglik: -3.0211e+02 - logprior: -5.9048e+01
Epoch 2/10
10/10 - 3s - loss: 310.1755 - loglik: -2.9856e+02 - logprior: -1.1464e+01
Epoch 3/10
10/10 - 3s - loss: 297.8395 - loglik: -2.9558e+02 - logprior: -1.9119e+00
Epoch 4/10
10/10 - 3s - loss: 293.2666 - loglik: -2.9505e+02 - logprior: 2.2139
Epoch 5/10
10/10 - 3s - loss: 289.8537 - loglik: -2.9412e+02 - logprior: 4.7005
Epoch 6/10
10/10 - 3s - loss: 288.5374 - loglik: -2.9432e+02 - logprior: 6.2271
Epoch 7/10
10/10 - 3s - loss: 287.2134 - loglik: -2.9400e+02 - logprior: 7.2529
Epoch 8/10
10/10 - 3s - loss: 287.0738 - loglik: -2.9462e+02 - logprior: 8.0233
Epoch 9/10
10/10 - 3s - loss: 285.2764 - loglik: -2.9342e+02 - logprior: 8.6333
Epoch 10/10
10/10 - 3s - loss: 285.4333 - loglik: -2.9407e+02 - logprior: 9.1198
Fitted a model with MAP estimate = -284.3504
Time for alignment: 92.7639
Computed alignments with likelihoods: ['-287.6300', '-285.1825', '-284.3504']
Best model has likelihood: -284.3504
time for generating output: 0.2497
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.7755734655920645
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb603275340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb674a1aac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb674a1aeb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6860abac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6860ab370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4e0c89190>, <__main__.SimpleDirichletPrior object at 0x7fb6c5537e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.8372 - loglik: -2.8277e+02 - logprior: -1.0054e+01
Epoch 2/10
12/12 - 2s - loss: 253.8580 - loglik: -2.5127e+02 - logprior: -2.5399e+00
Epoch 3/10
12/12 - 2s - loss: 225.5239 - loglik: -2.2341e+02 - logprior: -1.8484e+00
Epoch 4/10
12/12 - 2s - loss: 214.4847 - loglik: -2.1209e+02 - logprior: -1.9413e+00
Epoch 5/10
12/12 - 2s - loss: 210.3096 - loglik: -2.0761e+02 - logprior: -2.0112e+00
Epoch 6/10
12/12 - 2s - loss: 207.3523 - loglik: -2.0475e+02 - logprior: -1.9348e+00
Epoch 7/10
12/12 - 2s - loss: 207.2391 - loglik: -2.0480e+02 - logprior: -1.8839e+00
Epoch 8/10
12/12 - 2s - loss: 205.3784 - loglik: -2.0299e+02 - logprior: -1.9055e+00
Epoch 9/10
12/12 - 2s - loss: 204.5295 - loglik: -2.0214e+02 - logprior: -1.9352e+00
Epoch 10/10
12/12 - 2s - loss: 203.7081 - loglik: -2.0132e+02 - logprior: -1.9402e+00
Fitted a model with MAP estimate = -203.1398
expansions: [(6, 3), (10, 4), (11, 2), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 222.8668 - loglik: -2.1108e+02 - logprior: -1.1760e+01
Epoch 2/2
12/12 - 2s - loss: 200.5799 - loglik: -1.9539e+02 - logprior: -5.0071e+00
Fitted a model with MAP estimate = -195.2232
expansions: [(0, 4)]
discards: [ 0 62 80]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 202.0585 - loglik: -1.9260e+02 - logprior: -9.4230e+00
Epoch 2/2
12/12 - 2s - loss: 190.6677 - loglik: -1.8783e+02 - logprior: -2.6647e+00
Fitted a model with MAP estimate = -187.4442
expansions: []
discards: [ 0  1  2 48]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 203.4099 - loglik: -1.9194e+02 - logprior: -1.1432e+01
Epoch 2/10
12/12 - 2s - loss: 192.9523 - loglik: -1.8896e+02 - logprior: -3.7940e+00
Epoch 3/10
12/12 - 2s - loss: 187.2682 - loglik: -1.8474e+02 - logprior: -2.1119e+00
Epoch 4/10
12/12 - 2s - loss: 185.3569 - loglik: -1.8311e+02 - logprior: -1.5945e+00
Epoch 5/10
12/12 - 2s - loss: 183.4980 - loglik: -1.8140e+02 - logprior: -1.3687e+00
Epoch 6/10
12/12 - 2s - loss: 181.7732 - loglik: -1.7973e+02 - logprior: -1.3700e+00
Epoch 7/10
12/12 - 2s - loss: 181.3069 - loglik: -1.7939e+02 - logprior: -1.3279e+00
Epoch 8/10
12/12 - 2s - loss: 181.1741 - loglik: -1.7931e+02 - logprior: -1.3397e+00
Epoch 9/10
12/12 - 2s - loss: 179.7865 - loglik: -1.7800e+02 - logprior: -1.2992e+00
Epoch 10/10
12/12 - 2s - loss: 180.0932 - loglik: -1.7837e+02 - logprior: -1.2667e+00
Fitted a model with MAP estimate = -179.1681
Time for alignment: 67.9132
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 292.7519 - loglik: -2.8269e+02 - logprior: -1.0051e+01
Epoch 2/10
12/12 - 2s - loss: 254.2889 - loglik: -2.5171e+02 - logprior: -2.5364e+00
Epoch 3/10
12/12 - 2s - loss: 227.1483 - loglik: -2.2504e+02 - logprior: -1.8387e+00
Epoch 4/10
12/12 - 2s - loss: 215.4352 - loglik: -2.1282e+02 - logprior: -1.9156e+00
Epoch 5/10
12/12 - 2s - loss: 210.9660 - loglik: -2.0814e+02 - logprior: -1.9447e+00
Epoch 6/10
12/12 - 2s - loss: 208.1622 - loglik: -2.0563e+02 - logprior: -1.8589e+00
Epoch 7/10
12/12 - 2s - loss: 207.5666 - loglik: -2.0526e+02 - logprior: -1.7911e+00
Epoch 8/10
12/12 - 2s - loss: 206.5583 - loglik: -2.0431e+02 - logprior: -1.8209e+00
Epoch 9/10
12/12 - 2s - loss: 206.8519 - loglik: -2.0462e+02 - logprior: -1.8402e+00
Fitted a model with MAP estimate = -205.8098
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 3), (49, 1), (50, 3), (58, 3), (59, 5), (61, 1), (64, 1), (68, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 223.2304 - loglik: -2.1153e+02 - logprior: -1.1673e+01
Epoch 2/2
12/12 - 2s - loss: 202.1410 - loglik: -1.9698e+02 - logprior: -4.9812e+00
Fitted a model with MAP estimate = -197.3102
expansions: [(0, 5), (45, 1), (46, 1)]
discards: [ 0 74 75 77]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 204.7562 - loglik: -1.9537e+02 - logprior: -9.3535e+00
Epoch 2/2
12/12 - 2s - loss: 191.8803 - loglik: -1.8918e+02 - logprior: -2.5152e+00
Fitted a model with MAP estimate = -189.0465
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 199.4778 - loglik: -1.9041e+02 - logprior: -9.0303e+00
Epoch 2/10
12/12 - 2s - loss: 191.8533 - loglik: -1.8919e+02 - logprior: -2.4844e+00
Epoch 3/10
12/12 - 2s - loss: 187.7990 - loglik: -1.8563e+02 - logprior: -1.7820e+00
Epoch 4/10
12/12 - 2s - loss: 185.3922 - loglik: -1.8327e+02 - logprior: -1.5163e+00
Epoch 5/10
12/12 - 2s - loss: 183.6197 - loglik: -1.8163e+02 - logprior: -1.3043e+00
Epoch 6/10
12/12 - 2s - loss: 183.0551 - loglik: -1.8116e+02 - logprior: -1.2515e+00
Epoch 7/10
12/12 - 2s - loss: 183.1333 - loglik: -1.8139e+02 - logprior: -1.1968e+00
Fitted a model with MAP estimate = -181.5666
Time for alignment: 60.8366
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.7861 - loglik: -2.8272e+02 - logprior: -1.0051e+01
Epoch 2/10
12/12 - 2s - loss: 254.9902 - loglik: -2.5240e+02 - logprior: -2.5449e+00
Epoch 3/10
12/12 - 2s - loss: 228.1162 - loglik: -2.2602e+02 - logprior: -1.8708e+00
Epoch 4/10
12/12 - 2s - loss: 215.0551 - loglik: -2.1268e+02 - logprior: -1.9585e+00
Epoch 5/10
12/12 - 2s - loss: 210.7490 - loglik: -2.0803e+02 - logprior: -2.0165e+00
Epoch 6/10
12/12 - 2s - loss: 207.4363 - loglik: -2.0473e+02 - logprior: -1.9942e+00
Epoch 7/10
12/12 - 2s - loss: 205.7455 - loglik: -2.0314e+02 - logprior: -1.9768e+00
Epoch 8/10
12/12 - 2s - loss: 204.9359 - loglik: -2.0237e+02 - logprior: -2.0049e+00
Epoch 9/10
12/12 - 2s - loss: 204.1542 - loglik: -2.0163e+02 - logprior: -1.9989e+00
Epoch 10/10
12/12 - 2s - loss: 203.9650 - loglik: -2.0148e+02 - logprior: -1.9887e+00
Fitted a model with MAP estimate = -203.0624
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (20, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 4), (59, 3), (61, 1), (64, 1), (70, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 219.5983 - loglik: -2.1016e+02 - logprior: -9.4085e+00
Epoch 2/2
12/12 - 2s - loss: 194.7097 - loglik: -1.9174e+02 - logprior: -2.7944e+00
Fitted a model with MAP estimate = -189.3842
expansions: [(47, 1)]
discards: [ 0 13 14 65 78 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 207.0719 - loglik: -1.9534e+02 - logprior: -1.1701e+01
Epoch 2/2
12/12 - 2s - loss: 196.3270 - loglik: -1.9113e+02 - logprior: -5.0158e+00
Fitted a model with MAP estimate = -192.9162
expansions: [(0, 2), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 198.2355 - loglik: -1.8910e+02 - logprior: -9.1015e+00
Epoch 2/10
12/12 - 2s - loss: 187.8087 - loglik: -1.8517e+02 - logprior: -2.4529e+00
Epoch 3/10
12/12 - 2s - loss: 184.6554 - loglik: -1.8259e+02 - logprior: -1.6713e+00
Epoch 4/10
12/12 - 2s - loss: 181.2358 - loglik: -1.7927e+02 - logprior: -1.3484e+00
Epoch 5/10
12/12 - 2s - loss: 180.6144 - loglik: -1.7875e+02 - logprior: -1.1848e+00
Epoch 6/10
12/12 - 2s - loss: 179.4030 - loglik: -1.7767e+02 - logprior: -1.1348e+00
Epoch 7/10
12/12 - 2s - loss: 178.3918 - loglik: -1.7678e+02 - logprior: -1.0905e+00
Epoch 8/10
12/12 - 2s - loss: 178.7933 - loglik: -1.7726e+02 - logprior: -1.0719e+00
Fitted a model with MAP estimate = -177.9117
Time for alignment: 62.5263
Computed alignments with likelihoods: ['-179.1681', '-181.5666', '-177.9117']
Best model has likelihood: -177.9117
time for generating output: 0.2026
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.9003219080686389
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb579fe5a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60389ffd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60389f670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60389f820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60389ff10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb664722af0>, <__main__.SimpleDirichletPrior object at 0x7fb603980d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.7779 - loglik: -2.8906e+02 - logprior: -4.0701e+01
Epoch 2/10
10/10 - 1s - loss: 277.0189 - loglik: -2.6648e+02 - logprior: -1.0530e+01
Epoch 3/10
10/10 - 1s - loss: 250.8608 - loglik: -2.4620e+02 - logprior: -4.6534e+00
Epoch 4/10
10/10 - 1s - loss: 236.9228 - loglik: -2.3419e+02 - logprior: -2.5988e+00
Epoch 5/10
10/10 - 1s - loss: 231.2546 - loglik: -2.2922e+02 - logprior: -1.6904e+00
Epoch 6/10
10/10 - 1s - loss: 228.1422 - loglik: -2.2652e+02 - logprior: -1.2504e+00
Epoch 7/10
10/10 - 1s - loss: 226.8699 - loglik: -2.2550e+02 - logprior: -1.0594e+00
Epoch 8/10
10/10 - 1s - loss: 225.6706 - loglik: -2.2442e+02 - logprior: -9.4150e-01
Epoch 9/10
10/10 - 1s - loss: 224.5971 - loglik: -2.2340e+02 - logprior: -8.6953e-01
Epoch 10/10
10/10 - 1s - loss: 224.6699 - loglik: -2.2352e+02 - logprior: -8.1353e-01
Fitted a model with MAP estimate = -223.9149
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (37, 2), (43, 11), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.1764 - loglik: -2.2357e+02 - logprior: -5.2584e+01
Epoch 2/2
10/10 - 2s - loss: 233.0020 - loglik: -2.1681e+02 - logprior: -1.6052e+01
Fitted a model with MAP estimate = -224.4627
expansions: []
discards: [ 0  1 57 74]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.3788 - loglik: -2.1713e+02 - logprior: -4.6226e+01
Epoch 2/2
10/10 - 1s - loss: 234.0927 - loglik: -2.1569e+02 - logprior: -1.8253e+01
Fitted a model with MAP estimate = -228.7412
expansions: [(0, 3), (40, 1)]
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 257.4975 - loglik: -2.1612e+02 - logprior: -4.1355e+01
Epoch 2/10
10/10 - 2s - loss: 224.6447 - loglik: -2.1369e+02 - logprior: -1.0805e+01
Epoch 3/10
10/10 - 2s - loss: 216.9395 - loglik: -2.1255e+02 - logprior: -4.0929e+00
Epoch 4/10
10/10 - 1s - loss: 214.2778 - loglik: -2.1224e+02 - logprior: -1.6832e+00
Epoch 5/10
10/10 - 1s - loss: 212.9330 - loglik: -2.1217e+02 - logprior: -3.9404e-01
Epoch 6/10
10/10 - 2s - loss: 212.3820 - loglik: -2.1229e+02 - logprior: 0.2860
Epoch 7/10
10/10 - 1s - loss: 211.4895 - loglik: -2.1178e+02 - logprior: 0.6723
Epoch 8/10
10/10 - 1s - loss: 211.2755 - loglik: -2.1180e+02 - logprior: 0.9095
Epoch 9/10
10/10 - 1s - loss: 211.5482 - loglik: -2.1223e+02 - logprior: 1.0830
Fitted a model with MAP estimate = -210.6090
Time for alignment: 53.7063
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.7466 - loglik: -2.8903e+02 - logprior: -4.0701e+01
Epoch 2/10
10/10 - 1s - loss: 276.9025 - loglik: -2.6637e+02 - logprior: -1.0528e+01
Epoch 3/10
10/10 - 1s - loss: 250.7614 - loglik: -2.4612e+02 - logprior: -4.6282e+00
Epoch 4/10
10/10 - 1s - loss: 236.7336 - loglik: -2.3399e+02 - logprior: -2.6019e+00
Epoch 5/10
10/10 - 1s - loss: 231.3869 - loglik: -2.2936e+02 - logprior: -1.6815e+00
Epoch 6/10
10/10 - 1s - loss: 228.0379 - loglik: -2.2651e+02 - logprior: -1.1692e+00
Epoch 7/10
10/10 - 1s - loss: 226.9351 - loglik: -2.2564e+02 - logprior: -9.8963e-01
Epoch 8/10
10/10 - 1s - loss: 225.6062 - loglik: -2.2441e+02 - logprior: -8.8832e-01
Epoch 9/10
10/10 - 1s - loss: 225.2640 - loglik: -2.2413e+02 - logprior: -8.0581e-01
Epoch 10/10
10/10 - 1s - loss: 224.8097 - loglik: -2.2375e+02 - logprior: -7.1904e-01
Fitted a model with MAP estimate = -224.1178
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.0631 - loglik: -2.2342e+02 - logprior: -5.2624e+01
Epoch 2/2
10/10 - 2s - loss: 233.2773 - loglik: -2.1709e+02 - logprior: -1.6057e+01
Fitted a model with MAP estimate = -224.5358
expansions: [(7, 1)]
discards: [ 0  1 74]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 262.2952 - loglik: -2.1611e+02 - logprior: -4.6158e+01
Epoch 2/2
10/10 - 1s - loss: 233.1522 - loglik: -2.1477e+02 - logprior: -1.8233e+01
Fitted a model with MAP estimate = -227.7313
expansions: [(0, 3)]
discards: [ 0 40 55 56 57]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 257.8387 - loglik: -2.1664e+02 - logprior: -4.1175e+01
Epoch 2/10
10/10 - 1s - loss: 225.9460 - loglik: -2.1511e+02 - logprior: -1.0696e+01
Epoch 3/10
10/10 - 1s - loss: 218.3643 - loglik: -2.1403e+02 - logprior: -4.0589e+00
Epoch 4/10
10/10 - 1s - loss: 216.0580 - loglik: -2.1414e+02 - logprior: -1.5812e+00
Epoch 5/10
10/10 - 1s - loss: 214.5412 - loglik: -2.1385e+02 - logprior: -3.4043e-01
Epoch 6/10
10/10 - 1s - loss: 213.6988 - loglik: -2.1368e+02 - logprior: 0.3433
Epoch 7/10
10/10 - 1s - loss: 213.3539 - loglik: -2.1371e+02 - logprior: 0.7338
Epoch 8/10
10/10 - 1s - loss: 212.8984 - loglik: -2.1347e+02 - logprior: 0.9666
Epoch 9/10
10/10 - 1s - loss: 212.3949 - loglik: -2.1313e+02 - logprior: 1.1438
Epoch 10/10
10/10 - 1s - loss: 212.8978 - loglik: -2.1378e+02 - logprior: 1.2933
Fitted a model with MAP estimate = -211.8952
Time for alignment: 55.4350
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.7320 - loglik: -2.8902e+02 - logprior: -4.0699e+01
Epoch 2/10
10/10 - 1s - loss: 276.5714 - loglik: -2.6604e+02 - logprior: -1.0521e+01
Epoch 3/10
10/10 - 1s - loss: 250.3076 - loglik: -2.4566e+02 - logprior: -4.6406e+00
Epoch 4/10
10/10 - 1s - loss: 236.7430 - loglik: -2.3396e+02 - logprior: -2.6449e+00
Epoch 5/10
10/10 - 1s - loss: 231.3475 - loglik: -2.2927e+02 - logprior: -1.7428e+00
Epoch 6/10
10/10 - 1s - loss: 227.9696 - loglik: -2.2635e+02 - logprior: -1.2795e+00
Epoch 7/10
10/10 - 1s - loss: 226.9354 - loglik: -2.2555e+02 - logprior: -1.0790e+00
Epoch 8/10
10/10 - 1s - loss: 225.6303 - loglik: -2.2435e+02 - logprior: -9.6177e-01
Epoch 9/10
10/10 - 1s - loss: 224.9895 - loglik: -2.2381e+02 - logprior: -8.4228e-01
Epoch 10/10
10/10 - 1s - loss: 224.7176 - loglik: -2.2361e+02 - logprior: -7.6227e-01
Fitted a model with MAP estimate = -223.9776
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 276.3807 - loglik: -2.2380e+02 - logprior: -5.2563e+01
Epoch 2/2
10/10 - 2s - loss: 232.4472 - loglik: -2.1623e+02 - logprior: -1.6074e+01
Fitted a model with MAP estimate = -224.0135
expansions: []
discards: [ 0  1 57 58 59 75]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.7850 - loglik: -2.1746e+02 - logprior: -4.6296e+01
Epoch 2/2
10/10 - 1s - loss: 234.6118 - loglik: -2.1615e+02 - logprior: -1.8309e+01
Fitted a model with MAP estimate = -228.9582
expansions: [(0, 3), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 256.5578 - loglik: -2.1514e+02 - logprior: -4.1389e+01
Epoch 2/10
10/10 - 1s - loss: 224.4233 - loglik: -2.1354e+02 - logprior: -1.0735e+01
Epoch 3/10
10/10 - 2s - loss: 217.1327 - loglik: -2.1287e+02 - logprior: -3.9703e+00
Epoch 4/10
10/10 - 2s - loss: 214.5383 - loglik: -2.1271e+02 - logprior: -1.4888e+00
Epoch 5/10
10/10 - 1s - loss: 213.4113 - loglik: -2.1284e+02 - logprior: -2.2996e-01
Epoch 6/10
10/10 - 1s - loss: 212.5135 - loglik: -2.1263e+02 - logprior: 0.4658
Epoch 7/10
10/10 - 1s - loss: 211.7312 - loglik: -2.1224e+02 - logprior: 0.8619
Epoch 8/10
10/10 - 1s - loss: 211.6665 - loglik: -2.1241e+02 - logprior: 1.1033
Epoch 9/10
10/10 - 1s - loss: 211.5059 - loglik: -2.1242e+02 - logprior: 1.2835
Epoch 10/10
10/10 - 2s - loss: 211.1024 - loglik: -2.1216e+02 - logprior: 1.4321
Fitted a model with MAP estimate = -210.7713
Time for alignment: 54.0429
Computed alignments with likelihoods: ['-210.6090', '-211.8952', '-210.7713']
Best model has likelihood: -210.6090
time for generating output: 0.2137
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7290782726397168
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4ea29df10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e9bd0430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb561204eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb65369a580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb686829b80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb653508790>, <__main__.SimpleDirichletPrior object at 0x7fb603070280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 553.4118 - loglik: -4.7379e+02 - logprior: -7.9618e+01
Epoch 2/10
10/10 - 3s - loss: 420.8491 - loglik: -4.0377e+02 - logprior: -1.7059e+01
Epoch 3/10
10/10 - 3s - loss: 343.0463 - loglik: -3.3577e+02 - logprior: -7.1795e+00
Epoch 4/10
10/10 - 3s - loss: 294.9166 - loglik: -2.8967e+02 - logprior: -5.1938e+00
Epoch 5/10
10/10 - 3s - loss: 278.2932 - loglik: -2.7368e+02 - logprior: -4.5903e+00
Epoch 6/10
10/10 - 3s - loss: 270.7195 - loglik: -2.6688e+02 - logprior: -3.7733e+00
Epoch 7/10
10/10 - 3s - loss: 266.6629 - loglik: -2.6353e+02 - logprior: -2.8936e+00
Epoch 8/10
10/10 - 3s - loss: 265.3147 - loglik: -2.6244e+02 - logprior: -2.4969e+00
Epoch 9/10
10/10 - 3s - loss: 264.0753 - loglik: -2.6149e+02 - logprior: -2.2286e+00
Epoch 10/10
10/10 - 3s - loss: 263.9974 - loglik: -2.6192e+02 - logprior: -1.7731e+00
Fitted a model with MAP estimate = -263.0673
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.5907 - loglik: -2.5672e+02 - logprior: -9.0852e+01
Epoch 2/2
10/10 - 3s - loss: 271.0539 - loglik: -2.3669e+02 - logprior: -3.4258e+01
Fitted a model with MAP estimate = -258.3373
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53  93 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 301.9404 - loglik: -2.3016e+02 - logprior: -7.1759e+01
Epoch 2/2
10/10 - 3s - loss: 236.9996 - loglik: -2.2284e+02 - logprior: -1.4061e+01
Fitted a model with MAP estimate = -226.9440
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 311.2668 - loglik: -2.2419e+02 - logprior: -8.7052e+01
Epoch 2/10
10/10 - 3s - loss: 248.8685 - loglik: -2.2114e+02 - logprior: -2.7630e+01
Epoch 3/10
10/10 - 3s - loss: 228.3740 - loglik: -2.2168e+02 - logprior: -6.6056e+00
Epoch 4/10
10/10 - 3s - loss: 215.9591 - loglik: -2.1965e+02 - logprior: 3.7279
Epoch 5/10
10/10 - 2s - loss: 212.3990 - loglik: -2.1971e+02 - logprior: 7.3407
Epoch 6/10
10/10 - 2s - loss: 211.4140 - loglik: -2.2046e+02 - logprior: 9.1414
Epoch 7/10
10/10 - 2s - loss: 209.1729 - loglik: -2.1926e+02 - logprior: 10.3037
Epoch 8/10
10/10 - 2s - loss: 208.1816 - loglik: -2.1906e+02 - logprior: 11.1564
Epoch 9/10
10/10 - 2s - loss: 207.8617 - loglik: -2.1951e+02 - logprior: 11.9014
Epoch 10/10
10/10 - 2s - loss: 207.0434 - loglik: -2.1939e+02 - logprior: 12.5767
Fitted a model with MAP estimate = -206.3493
Time for alignment: 88.3222
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 553.3812 - loglik: -4.7376e+02 - logprior: -7.9619e+01
Epoch 2/10
10/10 - 3s - loss: 421.1470 - loglik: -4.0407e+02 - logprior: -1.7060e+01
Epoch 3/10
10/10 - 3s - loss: 343.1384 - loglik: -3.3585e+02 - logprior: -7.1904e+00
Epoch 4/10
10/10 - 3s - loss: 295.2830 - loglik: -2.8998e+02 - logprior: -5.2444e+00
Epoch 5/10
10/10 - 3s - loss: 277.7487 - loglik: -2.7307e+02 - logprior: -4.6554e+00
Epoch 6/10
10/10 - 3s - loss: 270.3698 - loglik: -2.6670e+02 - logprior: -3.6320e+00
Epoch 7/10
10/10 - 3s - loss: 267.0450 - loglik: -2.6404e+02 - logprior: -2.8703e+00
Epoch 8/10
10/10 - 3s - loss: 264.4893 - loglik: -2.6163e+02 - logprior: -2.5437e+00
Epoch 9/10
10/10 - 3s - loss: 264.8909 - loglik: -2.6224e+02 - logprior: -2.2837e+00
Fitted a model with MAP estimate = -263.4220
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 2), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.8945 - loglik: -2.5880e+02 - logprior: -9.1080e+01
Epoch 2/2
10/10 - 2s - loss: 272.3582 - loglik: -2.3766e+02 - logprior: -3.4615e+01
Fitted a model with MAP estimate = -259.9200
expansions: [(0, 3), (14, 1), (15, 4), (16, 2), (86, 1)]
discards: [  0  43  53  93 107 117 121]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 301.7208 - loglik: -2.2993e+02 - logprior: -7.1767e+01
Epoch 2/2
10/10 - 3s - loss: 235.7361 - loglik: -2.2172e+02 - logprior: -1.3914e+01
Fitted a model with MAP estimate = -225.2449
expansions: []
discards: [ 0  1 23]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.8524 - loglik: -2.2272e+02 - logprior: -8.7112e+01
Epoch 2/10
10/10 - 2s - loss: 249.5047 - loglik: -2.2183e+02 - logprior: -2.7568e+01
Epoch 3/10
10/10 - 2s - loss: 227.3035 - loglik: -2.2073e+02 - logprior: -6.4877e+00
Epoch 4/10
10/10 - 3s - loss: 216.5344 - loglik: -2.2017e+02 - logprior: 3.6755
Epoch 5/10
10/10 - 3s - loss: 213.2004 - loglik: -2.2050e+02 - logprior: 7.3257
Epoch 6/10
10/10 - 3s - loss: 210.5658 - loglik: -2.1968e+02 - logprior: 9.1708
Epoch 7/10
10/10 - 3s - loss: 209.9283 - loglik: -2.2004e+02 - logprior: 10.2857
Epoch 8/10
10/10 - 3s - loss: 208.6391 - loglik: -2.1947e+02 - logprior: 11.1267
Epoch 9/10
10/10 - 3s - loss: 207.9727 - loglik: -2.1949e+02 - logprior: 11.8207
Epoch 10/10
10/10 - 3s - loss: 206.0358 - loglik: -2.1828e+02 - logprior: 12.5143
Fitted a model with MAP estimate = -206.3946
Time for alignment: 85.0152
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 553.3804 - loglik: -4.7375e+02 - logprior: -7.9620e+01
Epoch 2/10
10/10 - 3s - loss: 420.8990 - loglik: -4.0382e+02 - logprior: -1.7062e+01
Epoch 3/10
10/10 - 3s - loss: 343.6209 - loglik: -3.3632e+02 - logprior: -7.2068e+00
Epoch 4/10
10/10 - 3s - loss: 295.9929 - loglik: -2.9072e+02 - logprior: -5.2206e+00
Epoch 5/10
10/10 - 3s - loss: 277.0314 - loglik: -2.7223e+02 - logprior: -4.7790e+00
Epoch 6/10
10/10 - 3s - loss: 269.7178 - loglik: -2.6553e+02 - logprior: -4.0739e+00
Epoch 7/10
10/10 - 3s - loss: 266.4317 - loglik: -2.6296e+02 - logprior: -3.1450e+00
Epoch 8/10
10/10 - 3s - loss: 263.8939 - loglik: -2.6086e+02 - logprior: -2.6239e+00
Epoch 9/10
10/10 - 3s - loss: 263.2602 - loglik: -2.6049e+02 - logprior: -2.4206e+00
Epoch 10/10
10/10 - 3s - loss: 262.7254 - loglik: -2.6033e+02 - logprior: -2.0857e+00
Fitted a model with MAP estimate = -262.0979
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.2072 - loglik: -2.5631e+02 - logprior: -9.0875e+01
Epoch 2/2
10/10 - 2s - loss: 271.0739 - loglik: -2.3683e+02 - logprior: -3.4136e+01
Fitted a model with MAP estimate = -259.2143
expansions: [(0, 3), (15, 3), (86, 1)]
discards: [  0  43  53  93 119]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 303.1139 - loglik: -2.3151e+02 - logprior: -7.1584e+01
Epoch 2/2
10/10 - 2s - loss: 239.8150 - loglik: -2.2578e+02 - logprior: -1.3945e+01
Fitted a model with MAP estimate = -229.2997
expansions: [(16, 1), (17, 1), (21, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 312.2473 - loglik: -2.2524e+02 - logprior: -8.6978e+01
Epoch 2/10
10/10 - 2s - loss: 249.1518 - loglik: -2.2113e+02 - logprior: -2.7920e+01
Epoch 3/10
10/10 - 2s - loss: 227.8580 - loglik: -2.2069e+02 - logprior: -7.0892e+00
Epoch 4/10
10/10 - 3s - loss: 215.8642 - loglik: -2.1956e+02 - logprior: 3.7292
Epoch 5/10
10/10 - 3s - loss: 214.3991 - loglik: -2.2191e+02 - logprior: 7.5330
Epoch 6/10
10/10 - 2s - loss: 209.9389 - loglik: -2.1926e+02 - logprior: 9.3843
Epoch 7/10
10/10 - 2s - loss: 207.8290 - loglik: -2.1811e+02 - logprior: 10.4745
Epoch 8/10
10/10 - 3s - loss: 208.6814 - loglik: -2.1970e+02 - logprior: 11.3274
Fitted a model with MAP estimate = -207.1488
Time for alignment: 82.8558
Computed alignments with likelihoods: ['-206.3493', '-206.3946', '-207.1488']
Best model has likelihood: -206.3493
time for generating output: 0.2295
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb57ad1c4c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60e992130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60e992370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb603607550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb603607ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb65b8b3ca0>, <__main__.SimpleDirichletPrior object at 0x7fb664528ee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 812.3849 - loglik: -8.0988e+02 - logprior: -2.1980e+00
Epoch 2/10
33/33 - 25s - loss: 717.7279 - loglik: -7.1575e+02 - logprior: -9.7933e-01
Epoch 3/10
33/33 - 25s - loss: 706.6345 - loglik: -7.0422e+02 - logprior: -9.6638e-01
Epoch 4/10
33/33 - 25s - loss: 702.2747 - loglik: -6.9992e+02 - logprior: -9.5327e-01
Epoch 5/10
33/33 - 25s - loss: 698.7540 - loglik: -6.9647e+02 - logprior: -9.4662e-01
Epoch 6/10
33/33 - 25s - loss: 703.0596 - loglik: -7.0079e+02 - logprior: -9.6743e-01
Fitted a model with MAP estimate = -698.5041
expansions: [(0, 4), (7, 1), (9, 2), (60, 1), (61, 2), (72, 1), (78, 2), (109, 2), (110, 1), (113, 1), (116, 1), (135, 1), (155, 6), (204, 1), (221, 1), (224, 2), (225, 1)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 717.8542 - loglik: -7.1436e+02 - logprior: -3.3275e+00
Epoch 2/2
33/33 - 29s - loss: 701.2178 - loglik: -6.9975e+02 - logprior: -7.9408e-01
Fitted a model with MAP estimate = -696.5642
expansions: [(39, 2), (40, 9), (257, 4)]
discards: [  1   2   3   7   8  16  67 122 176 177 178 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 708.9957 - loglik: -7.0637e+02 - logprior: -2.4210e+00
Epoch 2/2
33/33 - 29s - loss: 698.6694 - loglik: -6.9726e+02 - logprior: -5.9038e-01
Fitted a model with MAP estimate = -693.8224
expansions: [(0, 4), (251, 2), (252, 1), (259, 3)]
discards: [ 34  38  39  40  41  42 253 254 255 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 707.2550 - loglik: -7.0371e+02 - logprior: -3.3520e+00
Epoch 2/10
33/33 - 28s - loss: 697.8676 - loglik: -6.9673e+02 - logprior: -3.7504e-01
Epoch 3/10
33/33 - 29s - loss: 693.9216 - loglik: -6.9230e+02 - logprior: -2.4979e-01
Epoch 4/10
33/33 - 28s - loss: 693.9091 - loglik: -6.9214e+02 - logprior: -1.9109e-01
Epoch 5/10
33/33 - 29s - loss: 690.6225 - loglik: -6.8903e+02 - logprior: -8.7300e-02
Epoch 6/10
33/33 - 28s - loss: 689.6576 - loglik: -6.8822e+02 - logprior: 0.0016
Epoch 7/10
33/33 - 29s - loss: 687.1373 - loglik: -6.8584e+02 - logprior: 0.0855
Epoch 8/10
33/33 - 29s - loss: 688.5428 - loglik: -6.8736e+02 - logprior: 0.1556
Fitted a model with MAP estimate = -685.7233
Time for alignment: 614.7303
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 813.1799 - loglik: -8.1067e+02 - logprior: -2.2052e+00
Epoch 2/10
33/33 - 25s - loss: 715.5767 - loglik: -7.1377e+02 - logprior: -1.0614e+00
Epoch 3/10
33/33 - 25s - loss: 706.6213 - loglik: -7.0423e+02 - logprior: -1.1680e+00
Epoch 4/10
33/33 - 25s - loss: 705.9021 - loglik: -7.0352e+02 - logprior: -1.1180e+00
Epoch 5/10
33/33 - 25s - loss: 703.0495 - loglik: -7.0068e+02 - logprior: -1.1361e+00
Epoch 6/10
33/33 - 25s - loss: 699.3880 - loglik: -6.9695e+02 - logprior: -1.1711e+00
Epoch 7/10
33/33 - 25s - loss: 700.4324 - loglik: -6.9799e+02 - logprior: -1.1872e+00
Fitted a model with MAP estimate = -697.6938
expansions: [(0, 4), (7, 1), (9, 2), (10, 1), (34, 4), (42, 1), (44, 2), (71, 1), (72, 1), (73, 1), (79, 2), (83, 1), (113, 1), (116, 1), (118, 1), (135, 1), (155, 4), (156, 1), (183, 1), (204, 1), (220, 2), (221, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 716.1827 - loglik: -7.1268e+02 - logprior: -3.3429e+00
Epoch 2/2
33/33 - 30s - loss: 697.7134 - loglik: -6.9632e+02 - logprior: -7.9420e-01
Fitted a model with MAP estimate = -694.3494
expansions: [(159, 1), (265, 4)]
discards: [  1   2   3   7   8  43  57 182 183 253 261 262 263]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 706.8584 - loglik: -7.0432e+02 - logprior: -2.3308e+00
Epoch 2/2
33/33 - 28s - loss: 696.5421 - loglik: -6.9522e+02 - logprior: -4.8928e-01
Fitted a model with MAP estimate = -692.9503
expansions: [(0, 4), (1, 1), (257, 4)]
discards: [250 251 252 253 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 704.3823 - loglik: -7.0091e+02 - logprior: -3.2849e+00
Epoch 2/10
33/33 - 29s - loss: 699.7903 - loglik: -6.9868e+02 - logprior: -3.3951e-01
Epoch 3/10
33/33 - 28s - loss: 690.6500 - loglik: -6.8906e+02 - logprior: -2.2322e-01
Epoch 4/10
33/33 - 29s - loss: 692.2549 - loglik: -6.9058e+02 - logprior: -1.5243e-01
Fitted a model with MAP estimate = -688.5265
Time for alignment: 529.1188
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 813.0723 - loglik: -8.1056e+02 - logprior: -2.1959e+00
Epoch 2/10
33/33 - 25s - loss: 721.3311 - loglik: -7.1961e+02 - logprior: -8.9146e-01
Epoch 3/10
33/33 - 25s - loss: 707.1427 - loglik: -7.0479e+02 - logprior: -9.9254e-01
Epoch 4/10
33/33 - 25s - loss: 705.9341 - loglik: -7.0346e+02 - logprior: -1.0162e+00
Epoch 5/10
33/33 - 25s - loss: 703.7461 - loglik: -7.0131e+02 - logprior: -1.0051e+00
Epoch 6/10
33/33 - 25s - loss: 701.0939 - loglik: -6.9872e+02 - logprior: -1.0131e+00
Epoch 7/10
33/33 - 25s - loss: 702.2361 - loglik: -6.9993e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -699.0460
expansions: [(0, 5), (5, 1), (8, 1), (9, 1), (33, 4), (72, 1), (73, 1), (74, 3), (78, 1), (111, 1), (114, 1), (119, 1), (133, 1), (155, 4), (163, 5), (172, 1), (204, 1), (221, 1), (223, 2), (230, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 716.2855 - loglik: -7.1278e+02 - logprior: -3.3433e+00
Epoch 2/2
33/33 - 30s - loss: 696.6635 - loglik: -6.9518e+02 - logprior: -9.0211e-01
Fitted a model with MAP estimate = -692.8053
expansions: [(267, 4)]
discards: [  1   5  42 178 190 191 192 193 194 260 261 262 263 264 265 266]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 34s - loss: 707.4713 - loglik: -7.0491e+02 - logprior: -2.3479e+00
Epoch 2/2
33/33 - 28s - loss: 694.5821 - loglik: -6.9315e+02 - logprior: -5.8459e-01
Fitted a model with MAP estimate = -692.9743
expansions: [(0, 4), (255, 3)]
discards: [  4   5 251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 706.6300 - loglik: -7.0324e+02 - logprior: -3.2042e+00
Epoch 2/10
33/33 - 29s - loss: 696.9133 - loglik: -6.9575e+02 - logprior: -4.1423e-01
Epoch 3/10
33/33 - 28s - loss: 692.7466 - loglik: -6.9110e+02 - logprior: -2.7704e-01
Epoch 4/10
33/33 - 29s - loss: 691.9456 - loglik: -6.9018e+02 - logprior: -1.9778e-01
Epoch 5/10
33/33 - 28s - loss: 688.3545 - loglik: -6.8674e+02 - logprior: -1.0479e-01
Epoch 6/10
33/33 - 29s - loss: 692.0663 - loglik: -6.9062e+02 - logprior: -2.4330e-02
Fitted a model with MAP estimate = -686.7746
Time for alignment: 587.3428
Computed alignments with likelihoods: ['-685.7233', '-688.5265', '-686.7746']
Best model has likelihood: -685.7233
time for generating output: 0.4030
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7804642166344294
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb686cb6df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb64aa3d490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6977b8ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb69fcda730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb674e337c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb50bff5b20>, <__main__.SimpleDirichletPrior object at 0x7fb561037eb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 411.4830 - loglik: -3.4859e+02 - logprior: -6.2885e+01
Epoch 2/10
10/10 - 2s - loss: 322.1391 - loglik: -3.0688e+02 - logprior: -1.5258e+01
Epoch 3/10
10/10 - 2s - loss: 273.5874 - loglik: -2.6640e+02 - logprior: -7.1264e+00
Epoch 4/10
10/10 - 2s - loss: 248.7746 - loglik: -2.4385e+02 - logprior: -4.7683e+00
Epoch 5/10
10/10 - 2s - loss: 238.9063 - loglik: -2.3499e+02 - logprior: -3.6675e+00
Epoch 6/10
10/10 - 2s - loss: 235.1136 - loglik: -2.3181e+02 - logprior: -2.8605e+00
Epoch 7/10
10/10 - 2s - loss: 232.8909 - loglik: -2.3027e+02 - logprior: -2.2334e+00
Epoch 8/10
10/10 - 2s - loss: 230.9982 - loglik: -2.2888e+02 - logprior: -1.8572e+00
Epoch 9/10
10/10 - 2s - loss: 230.9910 - loglik: -2.2910e+02 - logprior: -1.6684e+00
Epoch 10/10
10/10 - 2s - loss: 229.9274 - loglik: -2.2822e+02 - logprior: -1.4675e+00
Fitted a model with MAP estimate = -229.8831
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 303.8770 - loglik: -2.3307e+02 - logprior: -7.0779e+01
Epoch 2/2
10/10 - 2s - loss: 241.0767 - loglik: -2.1354e+02 - logprior: -2.7375e+01
Fitted a model with MAP estimate = -229.1384
expansions: [(0, 2)]
discards: [  0  13 107]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.5774 - loglik: -2.1126e+02 - logprior: -5.5288e+01
Epoch 2/2
10/10 - 2s - loss: 218.1418 - loglik: -2.0539e+02 - logprior: -1.2582e+01
Fitted a model with MAP estimate = -209.4927
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.8761 - loglik: -2.1050e+02 - logprior: -6.6355e+01
Epoch 2/10
10/10 - 2s - loss: 223.9085 - loglik: -2.0599e+02 - logprior: -1.7773e+01
Epoch 3/10
10/10 - 2s - loss: 208.0804 - loglik: -2.0292e+02 - logprior: -4.8149e+00
Epoch 4/10
10/10 - 2s - loss: 201.8465 - loglik: -2.0087e+02 - logprior: -4.3881e-01
Epoch 5/10
10/10 - 2s - loss: 199.2520 - loglik: -2.0037e+02 - logprior: 1.6736
Epoch 6/10
10/10 - 2s - loss: 196.8748 - loglik: -1.9928e+02 - logprior: 2.8438
Epoch 7/10
10/10 - 2s - loss: 195.8394 - loglik: -1.9906e+02 - logprior: 3.5631
Epoch 8/10
10/10 - 2s - loss: 195.2797 - loglik: -1.9913e+02 - logprior: 4.1576
Epoch 9/10
10/10 - 2s - loss: 195.1446 - loglik: -1.9951e+02 - logprior: 4.6739
Epoch 10/10
10/10 - 2s - loss: 194.5219 - loglik: -1.9934e+02 - logprior: 5.1186
Fitted a model with MAP estimate = -193.9716
Time for alignment: 62.6988
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 411.5916 - loglik: -3.4870e+02 - logprior: -6.2884e+01
Epoch 2/10
10/10 - 2s - loss: 322.3191 - loglik: -3.0706e+02 - logprior: -1.5254e+01
Epoch 3/10
10/10 - 2s - loss: 274.9669 - loglik: -2.6781e+02 - logprior: -7.0836e+00
Epoch 4/10
10/10 - 2s - loss: 250.3961 - loglik: -2.4536e+02 - logprior: -4.8563e+00
Epoch 5/10
10/10 - 2s - loss: 239.2829 - loglik: -2.3486e+02 - logprior: -4.1353e+00
Epoch 6/10
10/10 - 2s - loss: 235.3402 - loglik: -2.3163e+02 - logprior: -3.2702e+00
Epoch 7/10
10/10 - 2s - loss: 233.3845 - loglik: -2.3046e+02 - logprior: -2.5231e+00
Epoch 8/10
10/10 - 2s - loss: 232.5030 - loglik: -2.3002e+02 - logprior: -2.1803e+00
Epoch 9/10
10/10 - 2s - loss: 231.4572 - loglik: -2.2918e+02 - logprior: -1.9763e+00
Epoch 10/10
10/10 - 2s - loss: 231.1317 - loglik: -2.2907e+02 - logprior: -1.7388e+00
Fitted a model with MAP estimate = -230.2960
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (59, 1), (60, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (87, 4), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 305.7730 - loglik: -2.3491e+02 - logprior: -7.0832e+01
Epoch 2/2
10/10 - 2s - loss: 242.7388 - loglik: -2.1521e+02 - logprior: -2.7372e+01
Fitted a model with MAP estimate = -230.1023
expansions: [(0, 3)]
discards: [  0 107]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.4775 - loglik: -2.1091e+02 - logprior: -5.5540e+01
Epoch 2/2
10/10 - 2s - loss: 218.3582 - loglik: -2.0531e+02 - logprior: -1.2888e+01
Fitted a model with MAP estimate = -209.0052
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 277.4912 - loglik: -2.1091e+02 - logprior: -6.6559e+01
Epoch 2/10
10/10 - 2s - loss: 224.9257 - loglik: -2.0667e+02 - logprior: -1.8128e+01
Epoch 3/10
10/10 - 2s - loss: 208.7645 - loglik: -2.0355e+02 - logprior: -4.9016e+00
Epoch 4/10
10/10 - 2s - loss: 202.2457 - loglik: -2.0124e+02 - logprior: -5.2010e-01
Epoch 5/10
10/10 - 2s - loss: 198.5450 - loglik: -1.9961e+02 - logprior: 1.5918
Epoch 6/10
10/10 - 2s - loss: 197.4823 - loglik: -1.9981e+02 - logprior: 2.7789
Epoch 7/10
10/10 - 2s - loss: 195.8911 - loglik: -1.9903e+02 - logprior: 3.4965
Epoch 8/10
10/10 - 2s - loss: 195.0690 - loglik: -1.9885e+02 - logprior: 4.0970
Epoch 9/10
10/10 - 2s - loss: 195.2141 - loglik: -1.9953e+02 - logprior: 4.6209
Fitted a model with MAP estimate = -194.4012
Time for alignment: 58.3983
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 411.7427 - loglik: -3.4885e+02 - logprior: -6.2883e+01
Epoch 2/10
10/10 - 2s - loss: 321.8034 - loglik: -3.0653e+02 - logprior: -1.5267e+01
Epoch 3/10
10/10 - 2s - loss: 274.3043 - loglik: -2.6710e+02 - logprior: -7.1375e+00
Epoch 4/10
10/10 - 2s - loss: 248.4392 - loglik: -2.4343e+02 - logprior: -4.8345e+00
Epoch 5/10
10/10 - 2s - loss: 237.6883 - loglik: -2.3344e+02 - logprior: -3.9066e+00
Epoch 6/10
10/10 - 2s - loss: 234.2479 - loglik: -2.3064e+02 - logprior: -3.1458e+00
Epoch 7/10
10/10 - 2s - loss: 232.2271 - loglik: -2.2929e+02 - logprior: -2.5380e+00
Epoch 8/10
10/10 - 2s - loss: 231.0306 - loglik: -2.2853e+02 - logprior: -2.1622e+00
Epoch 9/10
10/10 - 2s - loss: 230.5749 - loglik: -2.2834e+02 - logprior: -1.9150e+00
Epoch 10/10
10/10 - 2s - loss: 230.2030 - loglik: -2.2818e+02 - logprior: -1.7124e+00
Fitted a model with MAP estimate = -229.5488
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 2), (85, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 305.5346 - loglik: -2.3460e+02 - logprior: -7.0909e+01
Epoch 2/2
10/10 - 2s - loss: 241.4715 - loglik: -2.1371e+02 - logprior: -2.7605e+01
Fitted a model with MAP estimate = -229.6739
expansions: [(0, 3)]
discards: [  0  97 109]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 266.4510 - loglik: -2.1083e+02 - logprior: -5.5593e+01
Epoch 2/2
10/10 - 2s - loss: 217.6347 - loglik: -2.0456e+02 - logprior: -1.2907e+01
Fitted a model with MAP estimate = -208.8360
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 277.9393 - loglik: -2.1128e+02 - logprior: -6.6642e+01
Epoch 2/10
10/10 - 2s - loss: 224.7777 - loglik: -2.0638e+02 - logprior: -1.8259e+01
Epoch 3/10
10/10 - 2s - loss: 207.7374 - loglik: -2.0246e+02 - logprior: -4.9309e+00
Epoch 4/10
10/10 - 2s - loss: 202.4174 - loglik: -2.0137e+02 - logprior: -5.0721e-01
Epoch 5/10
10/10 - 2s - loss: 198.6597 - loglik: -1.9976e+02 - logprior: 1.6368
Epoch 6/10
10/10 - 2s - loss: 196.9766 - loglik: -1.9938e+02 - logprior: 2.8245
Epoch 7/10
10/10 - 2s - loss: 195.7623 - loglik: -1.9896e+02 - logprior: 3.5432
Epoch 8/10
10/10 - 2s - loss: 196.1965 - loglik: -2.0001e+02 - logprior: 4.1331
Fitted a model with MAP estimate = -194.7142
Time for alignment: 57.1212
Computed alignments with likelihoods: ['-193.9716', '-194.4012', '-194.7142']
Best model has likelihood: -193.9716
time for generating output: 0.1771
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9378029079159935
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb630dc6d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60eda0370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb603d3bc10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57a7568e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a756790>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6066f34f0>, <__main__.SimpleDirichletPrior object at 0x7fb524c73e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.3312 - loglik: -1.9306e+02 - logprior: -2.2218e+00
Epoch 2/10
22/22 - 2s - loss: 162.6017 - loglik: -1.6088e+02 - logprior: -1.3504e+00
Epoch 3/10
22/22 - 2s - loss: 154.6466 - loglik: -1.5262e+02 - logprior: -1.4853e+00
Epoch 4/10
22/22 - 2s - loss: 153.2276 - loglik: -1.5143e+02 - logprior: -1.3838e+00
Epoch 5/10
22/22 - 2s - loss: 152.6185 - loglik: -1.5087e+02 - logprior: -1.3813e+00
Epoch 6/10
22/22 - 2s - loss: 152.2812 - loglik: -1.5059e+02 - logprior: -1.3526e+00
Epoch 7/10
22/22 - 2s - loss: 152.1921 - loglik: -1.5053e+02 - logprior: -1.3417e+00
Epoch 8/10
22/22 - 2s - loss: 151.9396 - loglik: -1.5028e+02 - logprior: -1.3319e+00
Epoch 9/10
22/22 - 2s - loss: 151.8187 - loglik: -1.5015e+02 - logprior: -1.3310e+00
Epoch 10/10
22/22 - 2s - loss: 151.4683 - loglik: -1.4980e+02 - logprior: -1.3301e+00
Fitted a model with MAP estimate = -153.6300
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 158.4448 - loglik: -1.5539e+02 - logprior: -2.9636e+00
Epoch 2/2
22/22 - 2s - loss: 147.6193 - loglik: -1.4572e+02 - logprior: -1.6050e+00
Fitted a model with MAP estimate = -145.2280
expansions: [(0, 2)]
discards: [ 0  9 17 22 31 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 147.1871 - loglik: -1.4497e+02 - logprior: -2.1568e+00
Epoch 2/2
22/22 - 2s - loss: 144.3693 - loglik: -1.4312e+02 - logprior: -1.0652e+00
Fitted a model with MAP estimate = -143.7381
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.0622 - loglik: -1.4376e+02 - logprior: -1.2286e+00
Epoch 2/10
32/32 - 3s - loss: 142.3563 - loglik: -1.4114e+02 - logprior: -9.2495e-01
Epoch 3/10
32/32 - 3s - loss: 141.5939 - loglik: -1.4024e+02 - logprior: -9.0668e-01
Epoch 4/10
32/32 - 3s - loss: 140.9485 - loglik: -1.3952e+02 - logprior: -9.0575e-01
Epoch 5/10
32/32 - 3s - loss: 140.8295 - loglik: -1.3942e+02 - logprior: -9.1162e-01
Epoch 6/10
32/32 - 3s - loss: 140.3778 - loglik: -1.3900e+02 - logprior: -9.0543e-01
Epoch 7/10
32/32 - 3s - loss: 140.1056 - loglik: -1.3868e+02 - logprior: -8.9514e-01
Epoch 8/10
32/32 - 3s - loss: 139.7274 - loglik: -1.3835e+02 - logprior: -8.9364e-01
Epoch 9/10
32/32 - 3s - loss: 139.7045 - loglik: -1.3832e+02 - logprior: -8.8592e-01
Epoch 10/10
32/32 - 3s - loss: 139.3436 - loglik: -1.3791e+02 - logprior: -8.9025e-01
Fitted a model with MAP estimate = -138.5859
Time for alignment: 91.5193
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.4134 - loglik: -1.9314e+02 - logprior: -2.2253e+00
Epoch 2/10
22/22 - 2s - loss: 162.8660 - loglik: -1.6119e+02 - logprior: -1.3655e+00
Epoch 3/10
22/22 - 2s - loss: 155.6603 - loglik: -1.5362e+02 - logprior: -1.4978e+00
Epoch 4/10
22/22 - 2s - loss: 153.5976 - loglik: -1.5173e+02 - logprior: -1.4176e+00
Epoch 5/10
22/22 - 2s - loss: 152.8304 - loglik: -1.5103e+02 - logprior: -1.4064e+00
Epoch 6/10
22/22 - 2s - loss: 152.6715 - loglik: -1.5093e+02 - logprior: -1.3798e+00
Epoch 7/10
22/22 - 2s - loss: 152.4071 - loglik: -1.5070e+02 - logprior: -1.3668e+00
Epoch 8/10
22/22 - 2s - loss: 152.0358 - loglik: -1.5033e+02 - logprior: -1.3609e+00
Epoch 9/10
22/22 - 2s - loss: 152.1654 - loglik: -1.5046e+02 - logprior: -1.3563e+00
Fitted a model with MAP estimate = -153.4167
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (17, 2), (21, 1), (22, 2), (24, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 158.1806 - loglik: -1.5513e+02 - logprior: -2.9640e+00
Epoch 2/2
22/22 - 2s - loss: 147.8740 - loglik: -1.4599e+02 - logprior: -1.5910e+00
Fitted a model with MAP estimate = -145.0719
expansions: [(0, 2)]
discards: [ 0  9 18 22 31 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.3657 - loglik: -1.4515e+02 - logprior: -2.1595e+00
Epoch 2/2
22/22 - 2s - loss: 144.3168 - loglik: -1.4306e+02 - logprior: -1.0681e+00
Fitted a model with MAP estimate = -143.8363
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1649 - loglik: -1.4386e+02 - logprior: -1.2316e+00
Epoch 2/10
32/32 - 3s - loss: 142.3270 - loglik: -1.4110e+02 - logprior: -9.3233e-01
Epoch 3/10
32/32 - 3s - loss: 141.5825 - loglik: -1.4023e+02 - logprior: -9.0994e-01
Epoch 4/10
32/32 - 3s - loss: 140.9347 - loglik: -1.3950e+02 - logprior: -9.1369e-01
Epoch 5/10
32/32 - 3s - loss: 140.7751 - loglik: -1.3937e+02 - logprior: -9.0483e-01
Epoch 6/10
32/32 - 3s - loss: 140.4049 - loglik: -1.3903e+02 - logprior: -9.0705e-01
Epoch 7/10
32/32 - 3s - loss: 139.9402 - loglik: -1.3853e+02 - logprior: -8.9813e-01
Epoch 8/10
32/32 - 3s - loss: 140.0046 - loglik: -1.3864e+02 - logprior: -8.9415e-01
Fitted a model with MAP estimate = -139.0892
Time for alignment: 85.6911
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.3380 - loglik: -1.9307e+02 - logprior: -2.2245e+00
Epoch 2/10
22/22 - 2s - loss: 162.5258 - loglik: -1.6077e+02 - logprior: -1.3590e+00
Epoch 3/10
22/22 - 2s - loss: 154.5677 - loglik: -1.5254e+02 - logprior: -1.4988e+00
Epoch 4/10
22/22 - 2s - loss: 153.3265 - loglik: -1.5152e+02 - logprior: -1.4030e+00
Epoch 5/10
22/22 - 2s - loss: 152.8749 - loglik: -1.5110e+02 - logprior: -1.3970e+00
Epoch 6/10
22/22 - 2s - loss: 152.6684 - loglik: -1.5097e+02 - logprior: -1.3668e+00
Epoch 7/10
22/22 - 2s - loss: 152.1565 - loglik: -1.5046e+02 - logprior: -1.3600e+00
Epoch 8/10
22/22 - 2s - loss: 151.9815 - loglik: -1.5028e+02 - logprior: -1.3544e+00
Epoch 9/10
22/22 - 2s - loss: 152.1431 - loglik: -1.5044e+02 - logprior: -1.3500e+00
Fitted a model with MAP estimate = -153.4873
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 157.8320 - loglik: -1.5478e+02 - logprior: -2.9627e+00
Epoch 2/2
22/22 - 2s - loss: 147.8112 - loglik: -1.4592e+02 - logprior: -1.5886e+00
Fitted a model with MAP estimate = -145.1091
expansions: [(0, 2)]
discards: [ 0  9 17 22 31 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.3411 - loglik: -1.4513e+02 - logprior: -2.1552e+00
Epoch 2/2
22/22 - 2s - loss: 144.2133 - loglik: -1.4296e+02 - logprior: -1.0655e+00
Fitted a model with MAP estimate = -143.7216
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.0027 - loglik: -1.4369e+02 - logprior: -1.2356e+00
Epoch 2/10
32/32 - 3s - loss: 142.5486 - loglik: -1.4133e+02 - logprior: -9.2771e-01
Epoch 3/10
32/32 - 3s - loss: 141.5889 - loglik: -1.4023e+02 - logprior: -9.1151e-01
Epoch 4/10
32/32 - 3s - loss: 140.9333 - loglik: -1.3950e+02 - logprior: -9.0826e-01
Epoch 5/10
32/32 - 3s - loss: 140.7404 - loglik: -1.3934e+02 - logprior: -9.0402e-01
Epoch 6/10
32/32 - 3s - loss: 140.3415 - loglik: -1.3897e+02 - logprior: -8.9905e-01
Epoch 7/10
32/32 - 3s - loss: 140.1329 - loglik: -1.3872e+02 - logprior: -8.9565e-01
Epoch 8/10
32/32 - 3s - loss: 139.7630 - loglik: -1.3839e+02 - logprior: -8.9285e-01
Epoch 9/10
32/32 - 3s - loss: 139.6672 - loglik: -1.3828e+02 - logprior: -8.8346e-01
Epoch 10/10
32/32 - 3s - loss: 139.5036 - loglik: -1.3809e+02 - logprior: -8.8253e-01
Fitted a model with MAP estimate = -138.5573
Time for alignment: 88.6329
Computed alignments with likelihoods: ['-138.5859', '-139.0892', '-138.5573']
Best model has likelihood: -138.5573
time for generating output: 0.1676
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.83876392684465
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6309ffe80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5717894f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb571789b80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb63079fc40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb63079f940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6390e1ee0>, <__main__.SimpleDirichletPrior object at 0x7fb4e9c1d2b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 463.6794 - loglik: -4.5025e+02 - logprior: -1.3421e+01
Epoch 2/10
17/17 - 5s - loss: 323.0244 - loglik: -3.2124e+02 - logprior: -1.7174e+00
Epoch 3/10
17/17 - 5s - loss: 267.8558 - loglik: -2.6652e+02 - logprior: -1.3054e+00
Epoch 4/10
17/17 - 5s - loss: 257.4950 - loglik: -2.5609e+02 - logprior: -1.2178e+00
Epoch 5/10
17/17 - 5s - loss: 252.6634 - loglik: -2.5109e+02 - logprior: -1.2171e+00
Epoch 6/10
17/17 - 5s - loss: 252.6185 - loglik: -2.5105e+02 - logprior: -1.2066e+00
Epoch 7/10
17/17 - 5s - loss: 251.7954 - loglik: -2.5019e+02 - logprior: -1.2194e+00
Epoch 8/10
17/17 - 5s - loss: 251.7493 - loglik: -2.5013e+02 - logprior: -1.2218e+00
Epoch 9/10
17/17 - 5s - loss: 249.5855 - loglik: -2.4798e+02 - logprior: -1.2167e+00
Epoch 10/10
17/17 - 5s - loss: 249.6765 - loglik: -2.4811e+02 - logprior: -1.1912e+00
Fitted a model with MAP estimate = -249.6578
expansions: [(0, 32), (1, 1), (7, 1), (25, 1), (26, 1), (28, 1), (50, 1), (61, 1), (80, 1), (89, 1), (92, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 266.5459 - loglik: -2.4893e+02 - logprior: -1.7555e+01
Epoch 2/2
17/17 - 6s - loss: 222.6807 - loglik: -2.1966e+02 - logprior: -2.7543e+00
Fitted a model with MAP estimate = -213.4121
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 255.5842 - loglik: -2.4132e+02 - logprior: -1.4218e+01
Epoch 2/2
17/17 - 6s - loss: 225.9715 - loglik: -2.2458e+02 - logprior: -1.1505e+00
Fitted a model with MAP estimate = -220.5820
expansions: [(0, 25)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 250.3952 - loglik: -2.3781e+02 - logprior: -1.2535e+01
Epoch 2/10
17/17 - 6s - loss: 222.0433 - loglik: -2.2115e+02 - logprior: -6.6928e-01
Epoch 3/10
17/17 - 6s - loss: 212.0645 - loglik: -2.1232e+02 - logprior: 0.6013
Epoch 4/10
17/17 - 7s - loss: 211.2549 - loglik: -2.1202e+02 - logprior: 1.1574
Epoch 5/10
17/17 - 6s - loss: 207.1254 - loglik: -2.0807e+02 - logprior: 1.3587
Epoch 6/10
17/17 - 6s - loss: 206.2112 - loglik: -2.0733e+02 - logprior: 1.5583
Epoch 7/10
17/17 - 6s - loss: 206.9077 - loglik: -2.0824e+02 - logprior: 1.7599
Fitted a model with MAP estimate = -205.2880
Time for alignment: 156.6546
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 463.5132 - loglik: -4.5008e+02 - logprior: -1.3423e+01
Epoch 2/10
17/17 - 5s - loss: 320.4559 - loglik: -3.1858e+02 - logprior: -1.8075e+00
Epoch 3/10
17/17 - 6s - loss: 262.6371 - loglik: -2.6085e+02 - logprior: -1.7537e+00
Epoch 4/10
17/17 - 5s - loss: 251.3869 - loglik: -2.4948e+02 - logprior: -1.7287e+00
Epoch 5/10
17/17 - 5s - loss: 248.4340 - loglik: -2.4621e+02 - logprior: -1.8504e+00
Epoch 6/10
17/17 - 5s - loss: 246.0905 - loglik: -2.4382e+02 - logprior: -1.8643e+00
Epoch 7/10
17/17 - 5s - loss: 246.0417 - loglik: -2.4376e+02 - logprior: -1.8895e+00
Epoch 8/10
17/17 - 5s - loss: 245.6225 - loglik: -2.4334e+02 - logprior: -1.9103e+00
Epoch 9/10
17/17 - 5s - loss: 244.5920 - loglik: -2.4230e+02 - logprior: -1.9258e+00
Epoch 10/10
17/17 - 5s - loss: 245.2546 - loglik: -2.4297e+02 - logprior: -1.9214e+00
Fitted a model with MAP estimate = -244.3934
expansions: [(50, 1), (59, 1), (80, 1), (89, 1), (96, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 275.6307 - loglik: -2.5735e+02 - logprior: -1.8214e+01
Epoch 2/2
17/17 - 5s - loss: 253.2414 - loglik: -2.4633e+02 - logprior: -6.6320e+00
Fitted a model with MAP estimate = -249.7579
expansions: [(0, 16)]
discards: [ 0 41 42 43 44]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 266.9222 - loglik: -2.5291e+02 - logprior: -1.3964e+01
Epoch 2/2
17/17 - 5s - loss: 242.8601 - loglik: -2.4028e+02 - logprior: -2.3648e+00
Fitted a model with MAP estimate = -238.3183
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 56]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 272.6707 - loglik: -2.5630e+02 - logprior: -1.6323e+01
Epoch 2/10
17/17 - 5s - loss: 253.5207 - loglik: -2.5118e+02 - logprior: -2.1288e+00
Epoch 3/10
17/17 - 5s - loss: 250.3284 - loglik: -2.4981e+02 - logprior: -1.7638e-01
Epoch 4/10
17/17 - 5s - loss: 248.0070 - loglik: -2.4769e+02 - logprior: 0.0530
Epoch 5/10
17/17 - 5s - loss: 246.1843 - loglik: -2.4605e+02 - logprior: 0.2333
Epoch 6/10
17/17 - 5s - loss: 244.8019 - loglik: -2.4485e+02 - logprior: 0.4386
Epoch 7/10
17/17 - 5s - loss: 244.8959 - loglik: -2.4507e+02 - logprior: 0.5567
Fitted a model with MAP estimate = -243.5075
Time for alignment: 145.4955
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 464.2496 - loglik: -4.5084e+02 - logprior: -1.3407e+01
Epoch 2/10
17/17 - 5s - loss: 318.7573 - loglik: -3.1702e+02 - logprior: -1.6620e+00
Epoch 3/10
17/17 - 5s - loss: 268.4748 - loglik: -2.6729e+02 - logprior: -1.1572e+00
Epoch 4/10
17/17 - 5s - loss: 257.5736 - loglik: -2.5626e+02 - logprior: -1.0493e+00
Epoch 5/10
17/17 - 5s - loss: 254.8287 - loglik: -2.5341e+02 - logprior: -1.0149e+00
Epoch 6/10
17/17 - 5s - loss: 253.7034 - loglik: -2.5233e+02 - logprior: -1.0165e+00
Epoch 7/10
17/17 - 5s - loss: 250.1151 - loglik: -2.4868e+02 - logprior: -1.0619e+00
Epoch 8/10
17/17 - 5s - loss: 250.5290 - loglik: -2.4906e+02 - logprior: -1.0827e+00
Fitted a model with MAP estimate = -250.4465
expansions: [(0, 32), (1, 1), (25, 2), (26, 1), (28, 1), (50, 1), (59, 1), (80, 1), (89, 1), (92, 1), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 266.7367 - loglik: -2.4918e+02 - logprior: -1.7499e+01
Epoch 2/2
17/17 - 6s - loss: 223.4765 - loglik: -2.2047e+02 - logprior: -2.7453e+00
Fitted a model with MAP estimate = -214.0929
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 256.1120 - loglik: -2.4183e+02 - logprior: -1.4232e+01
Epoch 2/2
17/17 - 6s - loss: 226.2021 - loglik: -2.2477e+02 - logprior: -1.2033e+00
Fitted a model with MAP estimate = -221.4060
expansions: [(0, 26)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 251.3667 - loglik: -2.3878e+02 - logprior: -1.2547e+01
Epoch 2/10
17/17 - 6s - loss: 221.4650 - loglik: -2.2052e+02 - logprior: -7.4795e-01
Epoch 3/10
17/17 - 6s - loss: 212.8536 - loglik: -2.1296e+02 - logprior: 0.4443
Epoch 4/10
17/17 - 6s - loss: 206.5801 - loglik: -2.0712e+02 - logprior: 0.9290
Epoch 5/10
17/17 - 6s - loss: 206.2074 - loglik: -2.0690e+02 - logprior: 1.1111
Epoch 6/10
17/17 - 6s - loss: 205.4191 - loglik: -2.0629e+02 - logprior: 1.3190
Epoch 7/10
17/17 - 6s - loss: 204.4079 - loglik: -2.0548e+02 - logprior: 1.5077
Epoch 8/10
17/17 - 6s - loss: 204.4720 - loglik: -2.0578e+02 - logprior: 1.7294
Fitted a model with MAP estimate = -203.4477
Time for alignment: 148.1819
Computed alignments with likelihoods: ['-205.2880', '-238.3183', '-203.4477']
Best model has likelihood: -203.4477
time for generating output: 0.3702
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.5330119471808845
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb604c5bc40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4bbb37490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4bbb37100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb675540f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6303dbc10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb50bc94d90>, <__main__.SimpleDirichletPrior object at 0x7fb52e0f3790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 246.0606 - loglik: -2.2559e+02 - logprior: -2.0450e+01
Epoch 2/10
10/10 - 1s - loss: 216.2433 - loglik: -2.1046e+02 - logprior: -5.6061e+00
Epoch 3/10
10/10 - 1s - loss: 198.1518 - loglik: -1.9482e+02 - logprior: -3.0553e+00
Epoch 4/10
10/10 - 1s - loss: 189.7863 - loglik: -1.8695e+02 - logprior: -2.4606e+00
Epoch 5/10
10/10 - 1s - loss: 186.7323 - loglik: -1.8407e+02 - logprior: -2.3221e+00
Epoch 6/10
10/10 - 1s - loss: 185.0947 - loglik: -1.8269e+02 - logprior: -2.1612e+00
Epoch 7/10
10/10 - 1s - loss: 184.0968 - loglik: -1.8202e+02 - logprior: -1.8698e+00
Epoch 8/10
10/10 - 1s - loss: 183.5040 - loglik: -1.8152e+02 - logprior: -1.7580e+00
Epoch 9/10
10/10 - 1s - loss: 183.5282 - loglik: -1.8153e+02 - logprior: -1.7866e+00
Fitted a model with MAP estimate = -182.8924
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (23, 1), (41, 2), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.1709 - loglik: -1.8430e+02 - logprior: -2.6845e+01
Epoch 2/2
10/10 - 1s - loss: 188.1182 - loglik: -1.7963e+02 - logprior: -8.3475e+00
Fitted a model with MAP estimate = -183.3829
expansions: []
discards: [ 0 12 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.8870 - loglik: -1.8041e+02 - logprior: -2.3452e+01
Epoch 2/2
10/10 - 1s - loss: 188.4309 - loglik: -1.7899e+02 - logprior: -9.3147e+00
Fitted a model with MAP estimate = -185.3839
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.0633 - loglik: -1.7855e+02 - logprior: -2.0485e+01
Epoch 2/10
10/10 - 1s - loss: 182.7822 - loglik: -1.7707e+02 - logprior: -5.5893e+00
Epoch 3/10
10/10 - 1s - loss: 179.2917 - loglik: -1.7654e+02 - logprior: -2.5337e+00
Epoch 4/10
10/10 - 1s - loss: 177.6380 - loglik: -1.7578e+02 - logprior: -1.5708e+00
Epoch 5/10
10/10 - 1s - loss: 176.6983 - loglik: -1.7520e+02 - logprior: -1.1899e+00
Epoch 6/10
10/10 - 1s - loss: 176.1744 - loglik: -1.7485e+02 - logprior: -1.0468e+00
Epoch 7/10
10/10 - 1s - loss: 175.8807 - loglik: -1.7473e+02 - logprior: -9.0792e-01
Epoch 8/10
10/10 - 1s - loss: 175.7332 - loglik: -1.7476e+02 - logprior: -7.2513e-01
Epoch 9/10
10/10 - 1s - loss: 175.3448 - loglik: -1.7448e+02 - logprior: -6.1517e-01
Epoch 10/10
10/10 - 1s - loss: 175.0356 - loglik: -1.7420e+02 - logprior: -5.7931e-01
Fitted a model with MAP estimate = -174.8726
Time for alignment: 47.5105
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.0909 - loglik: -2.2561e+02 - logprior: -2.0453e+01
Epoch 2/10
10/10 - 1s - loss: 216.7563 - loglik: -2.1097e+02 - logprior: -5.6061e+00
Epoch 3/10
10/10 - 1s - loss: 198.2922 - loglik: -1.9499e+02 - logprior: -3.0332e+00
Epoch 4/10
10/10 - 1s - loss: 190.1162 - loglik: -1.8751e+02 - logprior: -2.3758e+00
Epoch 5/10
10/10 - 1s - loss: 186.6129 - loglik: -1.8408e+02 - logprior: -2.2579e+00
Epoch 6/10
10/10 - 1s - loss: 185.3525 - loglik: -1.8297e+02 - logprior: -2.0941e+00
Epoch 7/10
10/10 - 1s - loss: 184.3772 - loglik: -1.8233e+02 - logprior: -1.8067e+00
Epoch 8/10
10/10 - 1s - loss: 183.5695 - loglik: -1.8165e+02 - logprior: -1.6900e+00
Epoch 9/10
10/10 - 1s - loss: 183.4401 - loglik: -1.8150e+02 - logprior: -1.7094e+00
Epoch 10/10
10/10 - 1s - loss: 183.0583 - loglik: -1.8115e+02 - logprior: -1.6840e+00
Fitted a model with MAP estimate = -182.7674
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 3), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.3401 - loglik: -1.8428e+02 - logprior: -2.7030e+01
Epoch 2/2
10/10 - 1s - loss: 188.0010 - loglik: -1.7937e+02 - logprior: -8.4853e+00
Fitted a model with MAP estimate = -183.3648
expansions: []
discards: [ 0 10 12 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.4585 - loglik: -1.7996e+02 - logprior: -2.3471e+01
Epoch 2/2
10/10 - 1s - loss: 188.3878 - loglik: -1.7889e+02 - logprior: -9.3597e+00
Fitted a model with MAP estimate = -185.3087
expansions: [(0, 2)]
discards: [ 0 52]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 198.9627 - loglik: -1.7844e+02 - logprior: -2.0498e+01
Epoch 2/10
10/10 - 1s - loss: 183.0457 - loglik: -1.7731e+02 - logprior: -5.6069e+00
Epoch 3/10
10/10 - 1s - loss: 179.5475 - loglik: -1.7674e+02 - logprior: -2.5696e+00
Epoch 4/10
10/10 - 1s - loss: 177.8515 - loglik: -1.7598e+02 - logprior: -1.5800e+00
Epoch 5/10
10/10 - 1s - loss: 177.3074 - loglik: -1.7583e+02 - logprior: -1.1964e+00
Epoch 6/10
10/10 - 1s - loss: 176.6743 - loglik: -1.7536e+02 - logprior: -1.0512e+00
Epoch 7/10
10/10 - 1s - loss: 176.4728 - loglik: -1.7534e+02 - logprior: -8.8782e-01
Epoch 8/10
10/10 - 1s - loss: 175.8714 - loglik: -1.7492e+02 - logprior: -7.0752e-01
Epoch 9/10
10/10 - 1s - loss: 175.8322 - loglik: -1.7498e+02 - logprior: -6.0651e-01
Epoch 10/10
10/10 - 1s - loss: 175.5437 - loglik: -1.7472e+02 - logprior: -5.7291e-01
Fitted a model with MAP estimate = -175.2922
Time for alignment: 49.2128
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.1767 - loglik: -2.2570e+02 - logprior: -2.0451e+01
Epoch 2/10
10/10 - 1s - loss: 215.9515 - loglik: -2.1017e+02 - logprior: -5.6094e+00
Epoch 3/10
10/10 - 1s - loss: 198.9386 - loglik: -1.9560e+02 - logprior: -3.0551e+00
Epoch 4/10
10/10 - 1s - loss: 190.1655 - loglik: -1.8729e+02 - logprior: -2.4797e+00
Epoch 5/10
10/10 - 1s - loss: 186.9521 - loglik: -1.8417e+02 - logprior: -2.3736e+00
Epoch 6/10
10/10 - 1s - loss: 185.0840 - loglik: -1.8258e+02 - logprior: -2.1997e+00
Epoch 7/10
10/10 - 1s - loss: 184.4264 - loglik: -1.8228e+02 - logprior: -1.9000e+00
Epoch 8/10
10/10 - 1s - loss: 183.6628 - loglik: -1.8164e+02 - logprior: -1.8051e+00
Epoch 9/10
10/10 - 1s - loss: 183.9159 - loglik: -1.8188e+02 - logprior: -1.8302e+00
Fitted a model with MAP estimate = -183.3069
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (34, 1), (41, 2), (42, 2), (43, 1), (44, 2), (46, 1), (49, 1), (50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.8600 - loglik: -1.8495e+02 - logprior: -2.6884e+01
Epoch 2/2
10/10 - 1s - loss: 188.4921 - loglik: -1.7998e+02 - logprior: -8.3660e+00
Fitted a model with MAP estimate = -183.6820
expansions: []
discards: [ 0 12 55]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 204.0275 - loglik: -1.8058e+02 - logprior: -2.3423e+01
Epoch 2/2
10/10 - 1s - loss: 188.7833 - loglik: -1.7943e+02 - logprior: -9.2143e+00
Fitted a model with MAP estimate = -185.5110
expansions: [(0, 2)]
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.2504 - loglik: -1.7886e+02 - logprior: -2.0363e+01
Epoch 2/10
10/10 - 1s - loss: 183.2150 - loglik: -1.7760e+02 - logprior: -5.4926e+00
Epoch 3/10
10/10 - 1s - loss: 179.7604 - loglik: -1.7705e+02 - logprior: -2.4801e+00
Epoch 4/10
10/10 - 1s - loss: 178.3849 - loglik: -1.7655e+02 - logprior: -1.5327e+00
Epoch 5/10
10/10 - 1s - loss: 177.1815 - loglik: -1.7571e+02 - logprior: -1.1770e+00
Epoch 6/10
10/10 - 1s - loss: 177.2138 - loglik: -1.7590e+02 - logprior: -1.0483e+00
Fitted a model with MAP estimate = -176.4974
Time for alignment: 40.5042
Computed alignments with likelihoods: ['-174.8726', '-175.2922', '-176.4974']
Best model has likelihood: -174.8726
time for generating output: 0.1698
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6786069651741293
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb57a959be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb50bde2c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64a84d430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57a763b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb5586f55b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb35463bf40>, <__main__.SimpleDirichletPrior object at 0x7fb68699d190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.9043 - loglik: -1.8730e+02 - logprior: -1.1587e+01
Epoch 2/10
11/11 - 1s - loss: 157.7849 - loglik: -1.5446e+02 - logprior: -3.3125e+00
Epoch 3/10
11/11 - 1s - loss: 123.8059 - loglik: -1.2128e+02 - logprior: -2.4773e+00
Epoch 4/10
11/11 - 1s - loss: 109.5577 - loglik: -1.0718e+02 - logprior: -2.3315e+00
Epoch 5/10
11/11 - 1s - loss: 104.9271 - loglik: -1.0285e+02 - logprior: -2.0605e+00
Epoch 6/10
11/11 - 1s - loss: 102.8981 - loglik: -1.0064e+02 - logprior: -2.0937e+00
Epoch 7/10
11/11 - 1s - loss: 101.5640 - loglik: -9.9102e+01 - logprior: -2.1240e+00
Epoch 8/10
11/11 - 1s - loss: 101.2514 - loglik: -9.8791e+01 - logprior: -2.1124e+00
Epoch 9/10
11/11 - 1s - loss: 100.5456 - loglik: -9.8066e+01 - logprior: -2.1660e+00
Epoch 10/10
11/11 - 1s - loss: 100.0520 - loglik: -9.7560e+01 - logprior: -2.1781e+00
Fitted a model with MAP estimate = -99.6547
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (38, 1), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 112.2615 - loglik: -9.8370e+01 - logprior: -1.3868e+01
Epoch 2/2
11/11 - 1s - loss: 95.2577 - loglik: -9.0782e+01 - logprior: -4.3538e+00
Fitted a model with MAP estimate = -92.4897
expansions: []
discards: [ 0 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.5260 - loglik: -9.2238e+01 - logprior: -1.3268e+01
Epoch 2/2
11/11 - 1s - loss: 96.3822 - loglik: -9.0741e+01 - logprior: -5.5190e+00
Fitted a model with MAP estimate = -93.9245
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.6431 - loglik: -9.1610e+01 - logprior: -1.2013e+01
Epoch 2/10
11/11 - 1s - loss: 94.3987 - loglik: -9.0718e+01 - logprior: -3.5686e+00
Epoch 3/10
11/11 - 1s - loss: 92.0193 - loglik: -8.9447e+01 - logprior: -2.3314e+00
Epoch 4/10
11/11 - 1s - loss: 91.4764 - loglik: -8.9432e+01 - logprior: -1.7383e+00
Epoch 5/10
11/11 - 1s - loss: 90.6699 - loglik: -8.8870e+01 - logprior: -1.4768e+00
Epoch 6/10
11/11 - 1s - loss: 89.8072 - loglik: -8.8042e+01 - logprior: -1.4346e+00
Epoch 7/10
11/11 - 1s - loss: 89.5889 - loglik: -8.7870e+01 - logprior: -1.3702e+00
Epoch 8/10
11/11 - 1s - loss: 90.0215 - loglik: -8.8303e+01 - logprior: -1.3649e+00
Fitted a model with MAP estimate = -89.0652
Time for alignment: 41.0769
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 199.1604 - loglik: -1.8755e+02 - logprior: -1.1587e+01
Epoch 2/10
11/11 - 1s - loss: 158.3540 - loglik: -1.5504e+02 - logprior: -3.3053e+00
Epoch 3/10
11/11 - 1s - loss: 123.7307 - loglik: -1.2123e+02 - logprior: -2.4561e+00
Epoch 4/10
11/11 - 1s - loss: 109.2153 - loglik: -1.0678e+02 - logprior: -2.3843e+00
Epoch 5/10
11/11 - 1s - loss: 104.2503 - loglik: -1.0205e+02 - logprior: -2.1396e+00
Epoch 6/10
11/11 - 1s - loss: 102.4623 - loglik: -1.0007e+02 - logprior: -2.1456e+00
Epoch 7/10
11/11 - 1s - loss: 101.6141 - loglik: -9.9106e+01 - logprior: -2.1348e+00
Epoch 8/10
11/11 - 1s - loss: 101.5421 - loglik: -9.9109e+01 - logprior: -2.0674e+00
Epoch 9/10
11/11 - 1s - loss: 101.0631 - loglik: -9.8634e+01 - logprior: -2.0835e+00
Epoch 10/10
11/11 - 1s - loss: 100.9766 - loglik: -9.8555e+01 - logprior: -2.0758e+00
Fitted a model with MAP estimate = -100.4520
expansions: [(0, 3), (15, 1), (26, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 111.4913 - loglik: -9.7613e+01 - logprior: -1.3854e+01
Epoch 2/2
11/11 - 1s - loss: 95.5173 - loglik: -9.1084e+01 - logprior: -4.3122e+00
Fitted a model with MAP estimate = -92.6961
expansions: []
discards: [ 0 33]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 105.5556 - loglik: -9.2260e+01 - logprior: -1.3268e+01
Epoch 2/2
11/11 - 1s - loss: 96.6446 - loglik: -9.0980e+01 - logprior: -5.5359e+00
Fitted a model with MAP estimate = -93.9494
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.6417 - loglik: -9.1596e+01 - logprior: -1.2016e+01
Epoch 2/10
11/11 - 1s - loss: 94.4683 - loglik: -9.0762e+01 - logprior: -3.5660e+00
Epoch 3/10
11/11 - 1s - loss: 92.0756 - loglik: -8.9497e+01 - logprior: -2.3368e+00
Epoch 4/10
11/11 - 1s - loss: 91.3530 - loglik: -8.9308e+01 - logprior: -1.7444e+00
Epoch 5/10
11/11 - 1s - loss: 90.5024 - loglik: -8.8699e+01 - logprior: -1.4823e+00
Epoch 6/10
11/11 - 1s - loss: 90.0473 - loglik: -8.8281e+01 - logprior: -1.4346e+00
Epoch 7/10
11/11 - 1s - loss: 89.6856 - loglik: -8.7962e+01 - logprior: -1.3738e+00
Epoch 8/10
11/11 - 1s - loss: 89.7836 - loglik: -8.8063e+01 - logprior: -1.3627e+00
Fitted a model with MAP estimate = -89.0731
Time for alignment: 42.1197
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.2756 - loglik: -1.8767e+02 - logprior: -1.1587e+01
Epoch 2/10
11/11 - 1s - loss: 158.1973 - loglik: -1.5487e+02 - logprior: -3.3183e+00
Epoch 3/10
11/11 - 1s - loss: 125.4403 - loglik: -1.2290e+02 - logprior: -2.4929e+00
Epoch 4/10
11/11 - 1s - loss: 109.5541 - loglik: -1.0713e+02 - logprior: -2.3785e+00
Epoch 5/10
11/11 - 1s - loss: 104.7054 - loglik: -1.0252e+02 - logprior: -2.1364e+00
Epoch 6/10
11/11 - 1s - loss: 103.3023 - loglik: -1.0088e+02 - logprior: -2.1662e+00
Epoch 7/10
11/11 - 1s - loss: 102.2020 - loglik: -9.9670e+01 - logprior: -2.1610e+00
Epoch 8/10
11/11 - 1s - loss: 101.6640 - loglik: -9.9210e+01 - logprior: -2.1139e+00
Epoch 9/10
11/11 - 1s - loss: 101.6951 - loglik: -9.9240e+01 - logprior: -2.1443e+00
Fitted a model with MAP estimate = -101.0080
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 113.1553 - loglik: -9.9267e+01 - logprior: -1.3866e+01
Epoch 2/2
11/11 - 1s - loss: 96.2630 - loglik: -9.1744e+01 - logprior: -4.3995e+00
Fitted a model with MAP estimate = -92.6944
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6247 - loglik: -9.2344e+01 - logprior: -1.3262e+01
Epoch 2/2
11/11 - 1s - loss: 96.5339 - loglik: -9.0882e+01 - logprior: -5.5406e+00
Fitted a model with MAP estimate = -94.0193
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.8814 - loglik: -9.0923e+01 - logprior: -1.0935e+01
Epoch 2/10
11/11 - 1s - loss: 93.0404 - loglik: -8.9827e+01 - logprior: -3.0890e+00
Epoch 3/10
11/11 - 1s - loss: 91.2220 - loglik: -8.9032e+01 - logprior: -1.9405e+00
Epoch 4/10
11/11 - 1s - loss: 90.5407 - loglik: -8.8476e+01 - logprior: -1.7563e+00
Epoch 5/10
11/11 - 1s - loss: 89.4129 - loglik: -8.7400e+01 - logprior: -1.6913e+00
Epoch 6/10
11/11 - 1s - loss: 89.4518 - loglik: -8.7578e+01 - logprior: -1.5429e+00
Fitted a model with MAP estimate = -88.6755
Time for alignment: 37.2070
Computed alignments with likelihoods: ['-89.0652', '-89.0731', '-88.6755']
Best model has likelihood: -88.6755
time for generating output: 0.1290
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb63075e100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb604f18040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a0887c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb65bcb8be0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c531a910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb603134370>, <__main__.SimpleDirichletPrior object at 0x7fb52deb5940>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 172.0943 - loglik: -1.7115e+02 - logprior: -7.3444e-01
Epoch 2/10
42/42 - 5s - loss: 82.6360 - loglik: -8.1678e+01 - logprior: -6.4560e-01
Epoch 3/10
42/42 - 5s - loss: 79.3431 - loglik: -7.8447e+01 - logprior: -6.4419e-01
Epoch 4/10
42/42 - 5s - loss: 79.3988 - loglik: -7.8539e+01 - logprior: -6.2394e-01
Fitted a model with MAP estimate = -77.8489
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (33, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.9825 - loglik: -4.5939e+01 - logprior: -8.2501e-01
Epoch 2/2
42/42 - 5s - loss: 33.6384 - loglik: -3.2708e+01 - logprior: -6.3551e-01
Fitted a model with MAP estimate = -32.6188
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 37.4955 - loglik: -3.6420e+01 - logprior: -9.6329e-01
Epoch 2/2
42/42 - 5s - loss: 34.3080 - loglik: -3.3554e+01 - logprior: -5.0604e-01
Fitted a model with MAP estimate = -33.6761
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.5979 - loglik: -3.2865e+01 - logprior: -5.9655e-01
Epoch 2/10
59/59 - 7s - loss: 32.5170 - loglik: -3.1716e+01 - logprior: -5.7502e-01
Epoch 3/10
59/59 - 7s - loss: 32.2621 - loglik: -3.1467e+01 - logprior: -5.6414e-01
Epoch 4/10
59/59 - 7s - loss: 31.6698 - loglik: -3.0939e+01 - logprior: -5.5510e-01
Epoch 5/10
59/59 - 7s - loss: 32.0894 - loglik: -3.1386e+01 - logprior: -5.4696e-01
Fitted a model with MAP estimate = -31.3623
Time for alignment: 188.1146
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 170.9512 - loglik: -1.7001e+02 - logprior: -7.3319e-01
Epoch 2/10
42/42 - 5s - loss: 81.1798 - loglik: -8.0213e+01 - logprior: -6.5674e-01
Epoch 3/10
42/42 - 5s - loss: 79.0054 - loglik: -7.8124e+01 - logprior: -6.3560e-01
Epoch 4/10
42/42 - 5s - loss: 78.5748 - loglik: -7.7703e+01 - logprior: -6.2362e-01
Epoch 5/10
42/42 - 5s - loss: 77.9052 - loglik: -7.7092e+01 - logprior: -6.2516e-01
Epoch 6/10
42/42 - 4s - loss: 77.4546 - loglik: -7.6650e+01 - logprior: -6.1373e-01
Epoch 7/10
42/42 - 5s - loss: 77.5634 - loglik: -7.6770e+01 - logprior: -6.0492e-01
Fitted a model with MAP estimate = -76.7591
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 47.1990 - loglik: -4.6183e+01 - logprior: -8.4215e-01
Epoch 2/2
42/42 - 5s - loss: 33.3821 - loglik: -3.2492e+01 - logprior: -6.3386e-01
Fitted a model with MAP estimate = -32.6541
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 32.1269 - loglik: -3.1514e+01 - logprior: -4.7547e-01
Epoch 2/10
59/59 - 7s - loss: 32.2713 - loglik: -3.1634e+01 - logprior: -4.0920e-01
Fitted a model with MAP estimate = -31.4285
Time for alignment: 136.0368
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 171.1628 - loglik: -1.7022e+02 - logprior: -7.3207e-01
Epoch 2/10
42/42 - 5s - loss: 81.5475 - loglik: -8.0634e+01 - logprior: -6.3918e-01
Epoch 3/10
42/42 - 5s - loss: 78.7224 - loglik: -7.7864e+01 - logprior: -6.4500e-01
Epoch 4/10
42/42 - 5s - loss: 78.1260 - loglik: -7.7308e+01 - logprior: -6.2801e-01
Epoch 5/10
42/42 - 5s - loss: 78.1241 - loglik: -7.7322e+01 - logprior: -6.1464e-01
Epoch 6/10
42/42 - 5s - loss: 77.6449 - loglik: -7.6836e+01 - logprior: -6.1125e-01
Epoch 7/10
42/42 - 5s - loss: 77.7906 - loglik: -7.6992e+01 - logprior: -6.0342e-01
Fitted a model with MAP estimate = -76.7688
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.7392 - loglik: -4.5728e+01 - logprior: -8.4348e-01
Epoch 2/2
42/42 - 5s - loss: 33.5463 - loglik: -3.2656e+01 - logprior: -6.3555e-01
Fitted a model with MAP estimate = -32.6292
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.6234 - loglik: -3.6557e+01 - logprior: -9.5553e-01
Epoch 2/2
42/42 - 5s - loss: 34.1942 - loglik: -3.3404e+01 - logprior: -5.4182e-01
Fitted a model with MAP estimate = -33.1373
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.4625 - loglik: -3.2661e+01 - logprior: -6.6893e-01
Epoch 2/10
59/59 - 7s - loss: 32.5226 - loglik: -3.1726e+01 - logprior: -5.6867e-01
Epoch 3/10
59/59 - 7s - loss: 32.1939 - loglik: -3.1405e+01 - logprior: -5.5659e-01
Epoch 4/10
59/59 - 7s - loss: 32.1196 - loglik: -3.1390e+01 - logprior: -5.4941e-01
Epoch 5/10
59/59 - 7s - loss: 31.6008 - loglik: -3.0902e+01 - logprior: -5.4055e-01
Epoch 6/10
59/59 - 7s - loss: 31.6710 - loglik: -3.0977e+01 - logprior: -5.3333e-01
Fitted a model with MAP estimate = -31.2492
Time for alignment: 204.8149
Computed alignments with likelihoods: ['-31.3623', '-31.4285', '-31.2492']
Best model has likelihood: -31.2492
time for generating output: 0.1849
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.2090663058186739
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb50bf8ea30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c679dfd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e9c00820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb53ed6d5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb603684070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6054ac820>, <__main__.SimpleDirichletPrior object at 0x7fb69758dfd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.0720 - loglik: -4.5016e+02 - logprior: -2.3891e+01
Epoch 2/10
14/14 - 4s - loss: 411.4500 - loglik: -4.0737e+02 - logprior: -4.0399e+00
Epoch 3/10
14/14 - 4s - loss: 374.6812 - loglik: -3.7242e+02 - logprior: -2.1057e+00
Epoch 4/10
14/14 - 4s - loss: 356.7481 - loglik: -3.5433e+02 - logprior: -1.9423e+00
Epoch 5/10
14/14 - 4s - loss: 354.9746 - loglik: -3.5244e+02 - logprior: -1.8270e+00
Epoch 6/10
14/14 - 4s - loss: 352.1548 - loglik: -3.4987e+02 - logprior: -1.6403e+00
Epoch 7/10
14/14 - 4s - loss: 350.7882 - loglik: -3.4865e+02 - logprior: -1.5549e+00
Epoch 8/10
14/14 - 4s - loss: 350.5432 - loglik: -3.4847e+02 - logprior: -1.4882e+00
Epoch 9/10
14/14 - 4s - loss: 351.6108 - loglik: -3.4958e+02 - logprior: -1.4396e+00
Fitted a model with MAP estimate = -349.5851
expansions: [(3, 1), (5, 1), (15, 1), (16, 5), (35, 1), (36, 3), (38, 2), (41, 1), (44, 2), (45, 1), (66, 2), (79, 6), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 381.5618 - loglik: -3.5294e+02 - logprior: -2.8577e+01
Epoch 2/2
14/14 - 5s - loss: 353.5221 - loglik: -3.4297e+02 - logprior: -1.0285e+01
Fitted a model with MAP estimate = -348.6054
expansions: [(0, 22)]
discards: [  0  16  17  45  49  60  61  62  63  64 100]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 373.4405 - loglik: -3.5176e+02 - logprior: -2.1634e+01
Epoch 2/2
14/14 - 5s - loss: 349.4527 - loglik: -3.4563e+02 - logprior: -3.5805e+00
Fitted a model with MAP estimate = -346.4228
expansions: [(37, 2), (77, 2), (78, 2), (97, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 371.0136 - loglik: -3.4514e+02 - logprior: -2.5825e+01
Epoch 2/10
14/14 - 4s - loss: 344.6642 - loglik: -3.4048e+02 - logprior: -3.9276e+00
Epoch 3/10
14/14 - 4s - loss: 338.9204 - loglik: -3.3848e+02 - logprior: 0.0310
Epoch 4/10
14/14 - 4s - loss: 337.6276 - loglik: -3.3817e+02 - logprior: 1.1170
Epoch 5/10
14/14 - 5s - loss: 334.8860 - loglik: -3.3576e+02 - logprior: 1.4830
Epoch 6/10
14/14 - 4s - loss: 333.6209 - loglik: -3.3477e+02 - logprior: 1.7493
Epoch 7/10
14/14 - 5s - loss: 333.4614 - loglik: -3.3491e+02 - logprior: 2.0229
Epoch 8/10
14/14 - 5s - loss: 332.8720 - loglik: -3.3457e+02 - logprior: 2.2611
Epoch 9/10
14/14 - 4s - loss: 330.6472 - loglik: -3.3254e+02 - logprior: 2.4629
Epoch 10/10
14/14 - 5s - loss: 333.0872 - loglik: -3.3516e+02 - logprior: 2.6435
Fitted a model with MAP estimate = -330.9983
Time for alignment: 126.3702
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 475.0337 - loglik: -4.5112e+02 - logprior: -2.3885e+01
Epoch 2/10
14/14 - 4s - loss: 411.6821 - loglik: -4.0762e+02 - logprior: -4.0149e+00
Epoch 3/10
14/14 - 4s - loss: 373.0974 - loglik: -3.7068e+02 - logprior: -2.2632e+00
Epoch 4/10
14/14 - 4s - loss: 360.4566 - loglik: -3.5776e+02 - logprior: -2.2143e+00
Epoch 5/10
14/14 - 4s - loss: 356.1921 - loglik: -3.5349e+02 - logprior: -1.9975e+00
Epoch 6/10
14/14 - 4s - loss: 353.8018 - loglik: -3.5130e+02 - logprior: -1.8719e+00
Epoch 7/10
14/14 - 4s - loss: 352.2493 - loglik: -3.4983e+02 - logprior: -1.8080e+00
Epoch 8/10
14/14 - 4s - loss: 351.1963 - loglik: -3.4889e+02 - logprior: -1.7239e+00
Epoch 9/10
14/14 - 4s - loss: 352.4650 - loglik: -3.5020e+02 - logprior: -1.6697e+00
Fitted a model with MAP estimate = -350.8809
expansions: [(3, 1), (5, 1), (10, 1), (16, 4), (17, 1), (34, 1), (35, 3), (41, 1), (45, 1), (66, 3), (76, 1), (78, 2), (80, 1), (81, 1), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 384.4321 - loglik: -3.5541e+02 - logprior: -2.8985e+01
Epoch 2/2
14/14 - 4s - loss: 354.8315 - loglik: -3.4385e+02 - logprior: -1.0762e+01
Fitted a model with MAP estimate = -350.1458
expansions: [(0, 24)]
discards: [ 0 12 13 44 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 373.4082 - loglik: -3.5136e+02 - logprior: -2.2002e+01
Epoch 2/2
14/14 - 5s - loss: 347.7848 - loglik: -3.4392e+02 - logprior: -3.5931e+00
Fitted a model with MAP estimate = -345.5108
expansions: [(40, 1), (78, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22 100]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 371.5911 - loglik: -3.4561e+02 - logprior: -2.5929e+01
Epoch 2/10
14/14 - 4s - loss: 346.9589 - loglik: -3.4254e+02 - logprior: -4.1679e+00
Epoch 3/10
14/14 - 4s - loss: 341.5421 - loglik: -3.4069e+02 - logprior: -3.6635e-01
Epoch 4/10
14/14 - 4s - loss: 338.2207 - loglik: -3.3827e+02 - logprior: 0.6446
Epoch 5/10
14/14 - 4s - loss: 337.2861 - loglik: -3.3769e+02 - logprior: 1.0304
Epoch 6/10
14/14 - 4s - loss: 337.0867 - loglik: -3.3784e+02 - logprior: 1.3377
Epoch 7/10
14/14 - 4s - loss: 335.0554 - loglik: -3.3614e+02 - logprior: 1.6358
Epoch 8/10
14/14 - 4s - loss: 336.1342 - loglik: -3.3747e+02 - logprior: 1.8760
Fitted a model with MAP estimate = -334.4520
Time for alignment: 117.2464
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.1649 - loglik: -4.5024e+02 - logprior: -2.3897e+01
Epoch 2/10
14/14 - 4s - loss: 410.2705 - loglik: -4.0618e+02 - logprior: -4.0438e+00
Epoch 3/10
14/14 - 4s - loss: 370.9830 - loglik: -3.6857e+02 - logprior: -2.2712e+00
Epoch 4/10
14/14 - 4s - loss: 357.2110 - loglik: -3.5452e+02 - logprior: -2.1797e+00
Epoch 5/10
14/14 - 4s - loss: 355.6265 - loglik: -3.5304e+02 - logprior: -1.9048e+00
Epoch 6/10
14/14 - 4s - loss: 352.7778 - loglik: -3.5040e+02 - logprior: -1.7575e+00
Epoch 7/10
14/14 - 4s - loss: 353.3782 - loglik: -3.5109e+02 - logprior: -1.7103e+00
Fitted a model with MAP estimate = -351.3011
expansions: [(12, 1), (13, 1), (16, 5), (17, 1), (36, 3), (42, 1), (43, 1), (44, 1), (45, 1), (66, 2), (76, 1), (78, 2), (79, 4), (100, 1), (102, 2), (111, 3), (112, 1), (113, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 385.3547 - loglik: -3.5646e+02 - logprior: -2.8852e+01
Epoch 2/2
14/14 - 5s - loss: 353.6668 - loglik: -3.4285e+02 - logprior: -1.0571e+01
Fitted a model with MAP estimate = -349.9418
expansions: [(0, 25), (57, 1), (82, 1)]
discards: [  0  14  44  59  60  98  99 126]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 371.5562 - loglik: -3.4981e+02 - logprior: -2.1696e+01
Epoch 2/2
14/14 - 5s - loss: 346.7769 - loglik: -3.4293e+02 - logprior: -3.6042e+00
Fitted a model with MAP estimate = -343.9558
expansions: [(82, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 368.3107 - loglik: -3.4266e+02 - logprior: -2.5605e+01
Epoch 2/10
14/14 - 4s - loss: 345.5334 - loglik: -3.4130e+02 - logprior: -3.9781e+00
Epoch 3/10
14/14 - 4s - loss: 339.3115 - loglik: -3.3853e+02 - logprior: -2.8940e-01
Epoch 4/10
14/14 - 4s - loss: 338.5314 - loglik: -3.3866e+02 - logprior: 0.7366
Epoch 5/10
14/14 - 5s - loss: 336.3648 - loglik: -3.3682e+02 - logprior: 1.0851
Epoch 6/10
14/14 - 4s - loss: 333.4597 - loglik: -3.3422e+02 - logprior: 1.3712
Epoch 7/10
14/14 - 4s - loss: 333.6227 - loglik: -3.3470e+02 - logprior: 1.6681
Fitted a model with MAP estimate = -332.4832
Time for alignment: 103.4988
Computed alignments with likelihoods: ['-330.9983', '-334.4520', '-332.4832']
Best model has likelihood: -330.9983
time for generating output: 0.2130
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.980243799915931
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb697543940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c4450760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c4450b50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb67d8a3f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e9bd0520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6063b57f0>, <__main__.SimpleDirichletPrior object at 0x7fb4ea03fc40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 855.9511 - loglik: -8.5281e+02 - logprior: -3.0321e+00
Epoch 2/10
29/29 - 22s - loss: 674.2305 - loglik: -6.7179e+02 - logprior: -2.2182e+00
Epoch 3/10
29/29 - 22s - loss: 644.0604 - loglik: -6.4079e+02 - logprior: -2.6963e+00
Epoch 4/10
29/29 - 22s - loss: 640.6688 - loglik: -6.3734e+02 - logprior: -2.7173e+00
Epoch 5/10
29/29 - 22s - loss: 635.7543 - loglik: -6.3243e+02 - logprior: -2.7440e+00
Epoch 6/10
29/29 - 22s - loss: 636.1692 - loglik: -6.3284e+02 - logprior: -2.7753e+00
Fitted a model with MAP estimate = -634.2237
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 1), (50, 1), (72, 1), (76, 1), (87, 1), (88, 1), (89, 1), (94, 1), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (126, 1), (142, 1), (148, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2), (269, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 622.4045 - loglik: -6.1682e+02 - logprior: -5.4049e+00
Epoch 2/2
29/29 - 29s - loss: 595.1953 - loglik: -5.9212e+02 - logprior: -2.5927e+00
Fitted a model with MAP estimate = -587.5743
expansions: [(0, 2), (139, 1)]
discards: [  0 262 326 327]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 595.4130 - loglik: -5.9164e+02 - logprior: -3.6209e+00
Epoch 2/2
29/29 - 29s - loss: 586.2276 - loglik: -5.8490e+02 - logprior: -8.8103e-01
Fitted a model with MAP estimate = -583.5355
expansions: [(80, 4), (173, 1), (327, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 33s - loss: 593.8126 - loglik: -5.8873e+02 - logprior: -4.9296e+00
Epoch 2/10
29/29 - 30s - loss: 584.1707 - loglik: -5.8240e+02 - logprior: -1.3267e+00
Epoch 3/10
29/29 - 29s - loss: 580.7793 - loglik: -5.7974e+02 - logprior: -4.8541e-01
Epoch 4/10
29/29 - 30s - loss: 577.4897 - loglik: -5.7662e+02 - logprior: -2.5578e-01
Epoch 5/10
29/29 - 30s - loss: 576.1062 - loglik: -5.7551e+02 - logprior: 0.0138
Epoch 6/10
29/29 - 30s - loss: 577.1651 - loglik: -5.7668e+02 - logprior: 0.0991
Fitted a model with MAP estimate = -574.9187
Time for alignment: 550.6537
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 856.7787 - loglik: -8.5363e+02 - logprior: -3.0282e+00
Epoch 2/10
29/29 - 22s - loss: 676.0047 - loglik: -6.7352e+02 - logprior: -2.2615e+00
Epoch 3/10
29/29 - 22s - loss: 642.3755 - loglik: -6.3918e+02 - logprior: -2.6282e+00
Epoch 4/10
29/29 - 22s - loss: 636.3107 - loglik: -6.3311e+02 - logprior: -2.6106e+00
Epoch 5/10
29/29 - 22s - loss: 633.9356 - loglik: -6.3074e+02 - logprior: -2.6456e+00
Epoch 6/10
29/29 - 22s - loss: 633.2612 - loglik: -6.3006e+02 - logprior: -2.6696e+00
Epoch 7/10
29/29 - 22s - loss: 631.2452 - loglik: -6.2803e+02 - logprior: -2.6868e+00
Epoch 8/10
29/29 - 22s - loss: 628.9086 - loglik: -6.2566e+02 - logprior: -2.7284e+00
Epoch 9/10
29/29 - 22s - loss: 629.3842 - loglik: -6.2614e+02 - logprior: -2.7349e+00
Fitted a model with MAP estimate = -627.4681
expansions: [(16, 1), (22, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 2), (38, 1), (48, 2), (49, 2), (63, 1), (76, 1), (86, 1), (88, 2), (89, 2), (96, 1), (119, 1), (120, 2), (121, 3), (123, 1), (124, 2), (127, 1), (144, 1), (147, 1), (151, 1), (154, 2), (155, 1), (162, 1), (173, 1), (174, 1), (181, 1), (184, 1), (185, 1), (186, 1), (191, 1), (205, 1), (217, 2), (218, 2), (233, 1), (240, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [ 0 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 621.4371 - loglik: -6.1583e+02 - logprior: -5.4171e+00
Epoch 2/2
29/29 - 29s - loss: 593.6448 - loglik: -5.9047e+02 - logprior: -2.6576e+00
Fitted a model with MAP estimate = -587.6410
expansions: [(0, 2), (107, 1)]
discards: [  0  58  59 104 143 144 185 263 328]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 597.8185 - loglik: -5.9437e+02 - logprior: -3.2848e+00
Epoch 2/2
29/29 - 29s - loss: 588.7569 - loglik: -5.8763e+02 - logprior: -6.2349e-01
Fitted a model with MAP estimate = -584.5326
expansions: []
discards: [  0 326]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 598.6251 - loglik: -5.9368e+02 - logprior: -4.7870e+00
Epoch 2/10
29/29 - 28s - loss: 586.8397 - loglik: -5.8533e+02 - logprior: -9.4610e-01
Epoch 3/10
29/29 - 28s - loss: 582.2722 - loglik: -5.8153e+02 - logprior: -3.3172e-03
Epoch 4/10
29/29 - 28s - loss: 581.8790 - loglik: -5.8123e+02 - logprior: 0.0429
Epoch 5/10
29/29 - 28s - loss: 581.2482 - loglik: -5.8094e+02 - logprior: 0.3266
Epoch 6/10
29/29 - 28s - loss: 579.2145 - loglik: -5.7908e+02 - logprior: 0.4525
Epoch 7/10
29/29 - 29s - loss: 579.0409 - loglik: -5.7910e+02 - logprior: 0.6215
Epoch 8/10
29/29 - 28s - loss: 579.6609 - loglik: -5.7991e+02 - logprior: 0.7914
Fitted a model with MAP estimate = -578.0800
Time for alignment: 665.1915
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 857.5965 - loglik: -8.5445e+02 - logprior: -3.0262e+00
Epoch 2/10
29/29 - 22s - loss: 674.3419 - loglik: -6.7185e+02 - logprior: -2.2568e+00
Epoch 3/10
29/29 - 22s - loss: 642.8851 - loglik: -6.3953e+02 - logprior: -2.7484e+00
Epoch 4/10
29/29 - 22s - loss: 636.1873 - loglik: -6.3286e+02 - logprior: -2.7285e+00
Epoch 5/10
29/29 - 22s - loss: 634.0428 - loglik: -6.3074e+02 - logprior: -2.7027e+00
Epoch 6/10
29/29 - 22s - loss: 632.1440 - loglik: -6.2887e+02 - logprior: -2.7303e+00
Epoch 7/10
29/29 - 22s - loss: 632.3217 - loglik: -6.2903e+02 - logprior: -2.7701e+00
Fitted a model with MAP estimate = -630.7356
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 2), (41, 1), (48, 2), (49, 1), (52, 1), (65, 1), (76, 1), (85, 1), (86, 1), (87, 1), (88, 1), (93, 1), (121, 2), (122, 1), (123, 1), (124, 1), (126, 1), (141, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (263, 1), (264, 1), (269, 2), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 620.1961 - loglik: -6.1442e+02 - logprior: -5.5928e+00
Epoch 2/2
29/29 - 29s - loss: 595.9031 - loglik: -5.9265e+02 - logprior: -2.7635e+00
Fitted a model with MAP estimate = -589.9315
expansions: [(0, 2), (25, 1)]
discards: [  0  57 259 298 328]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 598.6434 - loglik: -5.9515e+02 - logprior: -3.3418e+00
Epoch 2/2
29/29 - 29s - loss: 587.6530 - loglik: -5.8659e+02 - logprior: -6.1958e-01
Fitted a model with MAP estimate = -585.1511
expansions: [(138, 2), (323, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 34s - loss: 597.6865 - loglik: -5.9294e+02 - logprior: -4.5966e+00
Epoch 2/10
29/29 - 29s - loss: 585.8602 - loglik: -5.8433e+02 - logprior: -1.1006e+00
Epoch 3/10
29/29 - 29s - loss: 585.2027 - loglik: -5.8476e+02 - logprior: 0.1300
Epoch 4/10
29/29 - 29s - loss: 580.4290 - loglik: -5.8008e+02 - logprior: 0.2809
Epoch 5/10
29/29 - 29s - loss: 581.2515 - loglik: -5.8039e+02 - logprior: -2.4760e-01
Fitted a model with MAP estimate = -579.5073
Time for alignment: 536.1197
Computed alignments with likelihoods: ['-574.9187', '-578.0800', '-579.5073']
Best model has likelihood: -574.9187
time for generating output: 0.4371
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.874095513748191
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb603ff7160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6749cd580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4cfb75ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6304f6790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6047bb4f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6c475adc0>, <__main__.SimpleDirichletPrior object at 0x7fb4c6b53fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2985 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4831 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7554 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3559 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 1s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7941e-01
Epoch 10/10
10/10 - 1s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6819
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 717.3940 - loglik: -6.2173e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 1s - loss: 261.2884 - loglik: -5.4720e+01 - logprior: -2.0649e+02
Fitted a model with MAP estimate = -174.7435
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 508.4308 - loglik: -4.9911e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 1s - loss: 173.9856 - loglik: -4.9680e+01 - logprior: -1.2422e+02
Fitted a model with MAP estimate = -124.0347
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 479.7808 - loglik: -4.8924e+01 - logprior: -4.3083e+02
Epoch 2/10
10/10 - 1s - loss: 166.4693 - loglik: -4.9611e+01 - logprior: -1.1677e+02
Epoch 3/10
10/10 - 1s - loss: 100.6160 - loglik: -5.0422e+01 - logprior: -5.0137e+01
Epoch 4/10
10/10 - 1s - loss: 72.6013 - loglik: -5.1029e+01 - logprior: -2.1561e+01
Epoch 5/10
10/10 - 1s - loss: 57.1750 - loglik: -5.1527e+01 - logprior: -5.6455e+00
Epoch 6/10
10/10 - 1s - loss: 48.1583 - loglik: -5.1969e+01 - logprior: 3.8152
Epoch 7/10
10/10 - 1s - loss: 42.5376 - loglik: -5.2304e+01 - logprior: 9.7709
Epoch 8/10
10/10 - 1s - loss: 38.7077 - loglik: -5.2572e+01 - logprior: 13.8678
Epoch 9/10
10/10 - 1s - loss: 35.8511 - loglik: -5.2790e+01 - logprior: 16.9421
Epoch 10/10
10/10 - 1s - loss: 33.5607 - loglik: -5.2969e+01 - logprior: 19.4106
Fitted a model with MAP estimate = -32.4452
Time for alignment: 28.7067
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2982 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4828 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7561 - loglik: -7.3483e+01 - logprior: -3.7202e+01
Epoch 5/10
10/10 - 1s - loss: 92.3561 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4429 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 1s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6995e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7941e-01
Epoch 10/10
10/10 - 0s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 717.3942 - loglik: -6.2173e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 1s - loss: 261.2884 - loglik: -5.4720e+01 - logprior: -2.0649e+02
Fitted a model with MAP estimate = -174.7435
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 508.4308 - loglik: -4.9911e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 0s - loss: 173.9856 - loglik: -4.9680e+01 - logprior: -1.2422e+02
Fitted a model with MAP estimate = -124.0348
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 479.7808 - loglik: -4.8924e+01 - logprior: -4.3083e+02
Epoch 2/10
10/10 - 0s - loss: 166.4693 - loglik: -4.9612e+01 - logprior: -1.1677e+02
Epoch 3/10
10/10 - 0s - loss: 100.6160 - loglik: -5.0422e+01 - logprior: -5.0137e+01
Epoch 4/10
10/10 - 1s - loss: 72.6013 - loglik: -5.1029e+01 - logprior: -2.1561e+01
Epoch 5/10
10/10 - 1s - loss: 57.1750 - loglik: -5.1527e+01 - logprior: -5.6455e+00
Epoch 6/10
10/10 - 1s - loss: 48.1583 - loglik: -5.1969e+01 - logprior: 3.8153
Epoch 7/10
10/10 - 1s - loss: 42.5376 - loglik: -5.2304e+01 - logprior: 9.7710
Epoch 8/10
10/10 - 1s - loss: 38.7077 - loglik: -5.2572e+01 - logprior: 13.8679
Epoch 9/10
10/10 - 1s - loss: 35.8511 - loglik: -5.2790e+01 - logprior: 16.9422
Epoch 10/10
10/10 - 1s - loss: 33.5606 - loglik: -5.2969e+01 - logprior: 19.4107
Fitted a model with MAP estimate = -32.4452
Time for alignment: 27.9811
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2985 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4831 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7554 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3559 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 0s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 1s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6819
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 717.3940 - loglik: -6.2173e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 1s - loss: 261.2883 - loglik: -5.4720e+01 - logprior: -2.0649e+02
Fitted a model with MAP estimate = -174.7435
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.4308 - loglik: -4.9911e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 1s - loss: 173.9856 - loglik: -4.9680e+01 - logprior: -1.2422e+02
Fitted a model with MAP estimate = -124.0348
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 479.7808 - loglik: -4.8924e+01 - logprior: -4.3083e+02
Epoch 2/10
10/10 - 1s - loss: 166.4693 - loglik: -4.9612e+01 - logprior: -1.1677e+02
Epoch 3/10
10/10 - 1s - loss: 100.6160 - loglik: -5.0422e+01 - logprior: -5.0137e+01
Epoch 4/10
10/10 - 1s - loss: 72.6014 - loglik: -5.1029e+01 - logprior: -2.1561e+01
Epoch 5/10
10/10 - 1s - loss: 57.1750 - loglik: -5.1527e+01 - logprior: -5.6455e+00
Epoch 6/10
10/10 - 1s - loss: 48.1583 - loglik: -5.1969e+01 - logprior: 3.8153
Epoch 7/10
10/10 - 1s - loss: 42.5376 - loglik: -5.2304e+01 - logprior: 9.7710
Epoch 8/10
10/10 - 1s - loss: 38.7077 - loglik: -5.2572e+01 - logprior: 13.8679
Epoch 9/10
10/10 - 1s - loss: 35.8511 - loglik: -5.2790e+01 - logprior: 16.9422
Epoch 10/10
10/10 - 1s - loss: 33.5606 - loglik: -5.2969e+01 - logprior: 19.4107
Fitted a model with MAP estimate = -32.4451
Time for alignment: 29.0060
Computed alignments with likelihoods: ['-32.4452', '-32.4452', '-32.4451']
Best model has likelihood: -32.4451
time for generating output: 0.1092
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb52dd83130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686a43190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb686a43040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb61f3499a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f3495e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb61f098d60>, <__main__.SimpleDirichletPrior object at 0x7fb560fc01c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 476.8578 - loglik: -4.7247e+02 - logprior: -4.3039e+00
Epoch 2/10
16/16 - 5s - loss: 440.3019 - loglik: -4.3856e+02 - logprior: -1.1397e+00
Epoch 3/10
16/16 - 5s - loss: 413.5344 - loglik: -4.1079e+02 - logprior: -1.5075e+00
Epoch 4/10
16/16 - 5s - loss: 404.4712 - loglik: -4.0128e+02 - logprior: -1.6159e+00
Epoch 5/10
16/16 - 5s - loss: 400.6680 - loglik: -3.9774e+02 - logprior: -1.5560e+00
Epoch 6/10
16/16 - 5s - loss: 399.7018 - loglik: -3.9703e+02 - logprior: -1.6104e+00
Epoch 7/10
16/16 - 5s - loss: 397.0242 - loglik: -3.9446e+02 - logprior: -1.6526e+00
Epoch 8/10
16/16 - 5s - loss: 396.3051 - loglik: -3.9380e+02 - logprior: -1.6954e+00
Epoch 9/10
16/16 - 5s - loss: 395.5310 - loglik: -3.9304e+02 - logprior: -1.7266e+00
Epoch 10/10
16/16 - 5s - loss: 395.6420 - loglik: -3.9320e+02 - logprior: -1.7325e+00
Fitted a model with MAP estimate = -394.5246
expansions: [(15, 4), (17, 1), (19, 1), (28, 3), (45, 1), (48, 3), (50, 2), (56, 1), (58, 1), (69, 1), (73, 2), (74, 2), (75, 2), (93, 1), (94, 6), (105, 1), (116, 1), (117, 2), (119, 1), (122, 1), (125, 1), (129, 1), (139, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 418.4932 - loglik: -4.1401e+02 - logprior: -4.2038e+00
Epoch 2/2
33/33 - 9s - loss: 394.3622 - loglik: -3.9169e+02 - logprior: -1.5198e+00
Fitted a model with MAP estimate = -387.3430
expansions: [(179, 2)]
discards: [  0  15  16  34  35  62 148 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 406.7581 - loglik: -4.0272e+02 - logprior: -3.8362e+00
Epoch 2/2
33/33 - 8s - loss: 394.4965 - loglik: -3.9227e+02 - logprior: -1.2606e+00
Fitted a model with MAP estimate = -389.2352
expansions: [(0, 1), (31, 2), (32, 1), (172, 2)]
discards: [170 171]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 400.6572 - loglik: -3.9796e+02 - logprior: -2.5306e+00
Epoch 2/10
33/33 - 9s - loss: 392.3872 - loglik: -3.9066e+02 - logprior: -8.1424e-01
Epoch 3/10
33/33 - 9s - loss: 386.6066 - loglik: -3.8427e+02 - logprior: -7.1623e-01
Epoch 4/10
33/33 - 9s - loss: 383.5822 - loglik: -3.8124e+02 - logprior: -6.8358e-01
Epoch 5/10
33/33 - 9s - loss: 382.7643 - loglik: -3.8063e+02 - logprior: -6.5817e-01
Epoch 6/10
33/33 - 9s - loss: 381.2040 - loglik: -3.7928e+02 - logprior: -6.1200e-01
Epoch 7/10
33/33 - 9s - loss: 380.0349 - loglik: -3.7832e+02 - logprior: -5.6515e-01
Epoch 8/10
33/33 - 9s - loss: 380.0145 - loglik: -3.7852e+02 - logprior: -4.9840e-01
Epoch 9/10
33/33 - 9s - loss: 379.3543 - loglik: -3.7802e+02 - logprior: -4.4083e-01
Epoch 10/10
33/33 - 9s - loss: 379.1490 - loglik: -3.7795e+02 - logprior: -3.7609e-01
Fitted a model with MAP estimate = -377.8814
Time for alignment: 227.5994
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 477.1600 - loglik: -4.7277e+02 - logprior: -4.3094e+00
Epoch 2/10
16/16 - 5s - loss: 439.5016 - loglik: -4.3774e+02 - logprior: -1.1601e+00
Epoch 3/10
16/16 - 5s - loss: 412.8495 - loglik: -4.1024e+02 - logprior: -1.5279e+00
Epoch 4/10
16/16 - 5s - loss: 404.3090 - loglik: -4.0153e+02 - logprior: -1.6477e+00
Epoch 5/10
16/16 - 5s - loss: 402.4163 - loglik: -3.9965e+02 - logprior: -1.5903e+00
Epoch 6/10
16/16 - 5s - loss: 399.4209 - loglik: -3.9678e+02 - logprior: -1.6244e+00
Epoch 7/10
16/16 - 5s - loss: 398.3033 - loglik: -3.9579e+02 - logprior: -1.6306e+00
Epoch 8/10
16/16 - 5s - loss: 397.7276 - loglik: -3.9523e+02 - logprior: -1.6762e+00
Epoch 9/10
16/16 - 5s - loss: 397.1132 - loglik: -3.9465e+02 - logprior: -1.6775e+00
Epoch 10/10
16/16 - 5s - loss: 397.1149 - loglik: -3.9470e+02 - logprior: -1.6869e+00
Fitted a model with MAP estimate = -395.5360
expansions: [(15, 4), (17, 1), (23, 1), (28, 3), (42, 1), (43, 1), (48, 2), (50, 1), (53, 1), (56, 1), (69, 1), (71, 1), (72, 2), (80, 1), (94, 4), (95, 1), (98, 2), (116, 3), (119, 1), (122, 1), (125, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 419.7659 - loglik: -4.1526e+02 - logprior: -4.2248e+00
Epoch 2/2
33/33 - 9s - loss: 395.8850 - loglik: -3.9319e+02 - logprior: -1.5871e+00
Fitted a model with MAP estimate = -388.8815
expansions: [(93, 2), (162, 1), (175, 2)]
discards: [  0  15  16  34  35  89 115 124 145 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 408.5410 - loglik: -4.0451e+02 - logprior: -3.8169e+00
Epoch 2/2
33/33 - 8s - loss: 395.2827 - loglik: -3.9300e+02 - logprior: -1.2898e+00
Fitted a model with MAP estimate = -389.6151
expansions: [(0, 1), (31, 2), (32, 1), (168, 2)]
discards: [166 167]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 401.3985 - loglik: -3.9871e+02 - logprior: -2.5368e+00
Epoch 2/10
33/33 - 9s - loss: 392.0242 - loglik: -3.9040e+02 - logprior: -8.0293e-01
Epoch 3/10
33/33 - 9s - loss: 387.4791 - loglik: -3.8519e+02 - logprior: -7.2252e-01
Epoch 4/10
33/33 - 9s - loss: 384.4118 - loglik: -3.8214e+02 - logprior: -6.8363e-01
Epoch 5/10
33/33 - 9s - loss: 384.4065 - loglik: -3.8236e+02 - logprior: -6.4729e-01
Epoch 6/10
33/33 - 9s - loss: 382.1231 - loglik: -3.8030e+02 - logprior: -5.9752e-01
Epoch 7/10
33/33 - 8s - loss: 381.1358 - loglik: -3.7952e+02 - logprior: -5.3780e-01
Epoch 8/10
33/33 - 9s - loss: 381.3881 - loglik: -3.7996e+02 - logprior: -4.7099e-01
Fitted a model with MAP estimate = -379.5640
Time for alignment: 208.6628
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.2260 - loglik: -4.7284e+02 - logprior: -4.3103e+00
Epoch 2/10
16/16 - 5s - loss: 439.2794 - loglik: -4.3753e+02 - logprior: -1.1460e+00
Epoch 3/10
16/16 - 5s - loss: 412.5949 - loglik: -4.0995e+02 - logprior: -1.5305e+00
Epoch 4/10
16/16 - 5s - loss: 403.7553 - loglik: -4.0084e+02 - logprior: -1.6941e+00
Epoch 5/10
16/16 - 5s - loss: 400.1363 - loglik: -3.9728e+02 - logprior: -1.6173e+00
Epoch 6/10
16/16 - 5s - loss: 399.0682 - loglik: -3.9642e+02 - logprior: -1.6265e+00
Epoch 7/10
16/16 - 5s - loss: 397.6242 - loglik: -3.9517e+02 - logprior: -1.5873e+00
Epoch 8/10
16/16 - 5s - loss: 397.0917 - loglik: -3.9474e+02 - logprior: -1.5972e+00
Epoch 9/10
16/16 - 5s - loss: 397.4712 - loglik: -3.9519e+02 - logprior: -1.5939e+00
Fitted a model with MAP estimate = -396.0461
expansions: [(13, 1), (14, 1), (20, 1), (23, 1), (28, 3), (29, 1), (45, 1), (48, 3), (50, 2), (56, 1), (58, 1), (69, 1), (72, 2), (74, 3), (94, 4), (95, 2), (98, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 14s - loss: 416.0860 - loglik: -4.1168e+02 - logprior: -4.1183e+00
Epoch 2/2
33/33 - 9s - loss: 394.6908 - loglik: -3.9222e+02 - logprior: -1.4685e+00
Fitted a model with MAP estimate = -388.2498
expansions: [(177, 2)]
discards: [ 32  33  61  95 146 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 401.9872 - loglik: -3.9909e+02 - logprior: -2.7101e+00
Epoch 2/2
33/33 - 9s - loss: 393.9374 - loglik: -3.9254e+02 - logprior: -9.0990e-01
Fitted a model with MAP estimate = -389.7075
expansions: [(171, 2)]
discards: [169 170]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 11s - loss: 399.9143 - loglik: -3.9710e+02 - logprior: -2.6442e+00
Epoch 2/10
33/33 - 8s - loss: 392.5669 - loglik: -3.9068e+02 - logprior: -8.4595e-01
Epoch 3/10
33/33 - 9s - loss: 387.9350 - loglik: -3.8551e+02 - logprior: -7.7395e-01
Epoch 4/10
33/33 - 9s - loss: 385.2345 - loglik: -3.8294e+02 - logprior: -7.2692e-01
Epoch 5/10
33/33 - 8s - loss: 383.8409 - loglik: -3.8176e+02 - logprior: -7.0357e-01
Epoch 6/10
33/33 - 9s - loss: 383.1196 - loglik: -3.8125e+02 - logprior: -6.5103e-01
Epoch 7/10
33/33 - 9s - loss: 381.3527 - loglik: -3.7967e+02 - logprior: -5.9718e-01
Epoch 8/10
33/33 - 8s - loss: 381.6938 - loglik: -3.8020e+02 - logprior: -5.3858e-01
Fitted a model with MAP estimate = -379.9000
Time for alignment: 200.8688
Computed alignments with likelihoods: ['-377.8814', '-379.5640', '-379.9000']
Best model has likelihood: -377.8814
time for generating output: 0.2366
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7906458797327395
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb50bfd5e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4ea284250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb5259e39a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6303ac1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb524bcff70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57a4b5430>, <__main__.SimpleDirichletPrior object at 0x7fb50bb2f3d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 292.3649 - loglik: -2.7728e+02 - logprior: -1.5083e+01
Epoch 2/10
10/10 - 2s - loss: 251.4052 - loglik: -2.4736e+02 - logprior: -4.0390e+00
Epoch 3/10
10/10 - 2s - loss: 226.7614 - loglik: -2.2441e+02 - logprior: -2.3399e+00
Epoch 4/10
10/10 - 2s - loss: 212.2340 - loglik: -2.1001e+02 - logprior: -2.0651e+00
Epoch 5/10
10/10 - 2s - loss: 205.3418 - loglik: -2.0284e+02 - logprior: -2.1358e+00
Epoch 6/10
10/10 - 2s - loss: 205.4526 - loglik: -2.0282e+02 - logprior: -2.2558e+00
Fitted a model with MAP estimate = -203.3039
expansions: [(25, 1), (26, 1), (34, 1), (45, 2), (46, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.3400 - loglik: -2.0981e+02 - logprior: -1.7513e+01
Epoch 2/2
10/10 - 2s - loss: 208.5527 - loglik: -2.0078e+02 - logprior: -7.6533e+00
Fitted a model with MAP estimate = -206.5350
expansions: []
discards: [ 0 26]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.2210 - loglik: -2.0406e+02 - logprior: -1.7150e+01
Epoch 2/2
10/10 - 2s - loss: 207.0271 - loglik: -2.0074e+02 - logprior: -6.2270e+00
Fitted a model with MAP estimate = -203.9575
expansions: [(0, 12)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 215.4696 - loglik: -2.0070e+02 - logprior: -1.4764e+01
Epoch 2/10
10/10 - 2s - loss: 198.9730 - loglik: -1.9403e+02 - logprior: -4.8910e+00
Epoch 3/10
10/10 - 2s - loss: 194.6753 - loglik: -1.9161e+02 - logprior: -2.9130e+00
Epoch 4/10
10/10 - 2s - loss: 191.6793 - loglik: -1.8921e+02 - logprior: -2.2560e+00
Epoch 5/10
10/10 - 2s - loss: 189.5612 - loglik: -1.8733e+02 - logprior: -2.0076e+00
Epoch 6/10
10/10 - 2s - loss: 188.7540 - loglik: -1.8666e+02 - logprior: -1.8866e+00
Epoch 7/10
10/10 - 2s - loss: 187.9219 - loglik: -1.8600e+02 - logprior: -1.7377e+00
Epoch 8/10
10/10 - 2s - loss: 187.4752 - loglik: -1.8565e+02 - logprior: -1.6252e+00
Epoch 9/10
10/10 - 2s - loss: 186.6463 - loglik: -1.8485e+02 - logprior: -1.5794e+00
Epoch 10/10
10/10 - 2s - loss: 186.1952 - loglik: -1.8441e+02 - logprior: -1.5682e+00
Fitted a model with MAP estimate = -186.2950
Time for alignment: 55.3244
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 292.3424 - loglik: -2.7726e+02 - logprior: -1.5083e+01
Epoch 2/10
10/10 - 2s - loss: 251.7783 - loglik: -2.4777e+02 - logprior: -4.0017e+00
Epoch 3/10
10/10 - 2s - loss: 227.1146 - loglik: -2.2493e+02 - logprior: -2.1781e+00
Epoch 4/10
10/10 - 2s - loss: 210.5123 - loglik: -2.0850e+02 - logprior: -1.8677e+00
Epoch 5/10
10/10 - 2s - loss: 204.0059 - loglik: -2.0184e+02 - logprior: -1.8447e+00
Epoch 6/10
10/10 - 2s - loss: 199.1211 - loglik: -1.9691e+02 - logprior: -1.8405e+00
Epoch 7/10
10/10 - 2s - loss: 200.8048 - loglik: -1.9877e+02 - logprior: -1.7480e+00
Fitted a model with MAP estimate = -199.0438
expansions: [(32, 1), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 226.8109 - loglik: -2.0943e+02 - logprior: -1.7368e+01
Epoch 2/2
10/10 - 2s - loss: 208.6135 - loglik: -2.0101e+02 - logprior: -7.5258e+00
Fitted a model with MAP estimate = -206.6295
expansions: [(0, 12)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.7173 - loglik: -2.0011e+02 - logprior: -1.4592e+01
Epoch 2/2
10/10 - 2s - loss: 197.1505 - loglik: -1.9238e+02 - logprior: -4.6818e+00
Fitted a model with MAP estimate = -194.2414
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 216.9410 - loglik: -2.0109e+02 - logprior: -1.5832e+01
Epoch 2/10
10/10 - 2s - loss: 202.3059 - loglik: -1.9754e+02 - logprior: -4.6746e+00
Epoch 3/10
10/10 - 2s - loss: 200.5278 - loglik: -1.9800e+02 - logprior: -2.3817e+00
Epoch 4/10
10/10 - 2s - loss: 198.7928 - loglik: -1.9698e+02 - logprior: -1.6373e+00
Epoch 5/10
10/10 - 2s - loss: 198.3468 - loglik: -1.9683e+02 - logprior: -1.3161e+00
Epoch 6/10
10/10 - 2s - loss: 196.5044 - loglik: -1.9521e+02 - logprior: -1.0876e+00
Epoch 7/10
10/10 - 2s - loss: 195.3932 - loglik: -1.9427e+02 - logprior: -9.2590e-01
Epoch 8/10
10/10 - 2s - loss: 196.4576 - loglik: -1.9539e+02 - logprior: -8.6388e-01
Fitted a model with MAP estimate = -195.1591
Time for alignment: 53.0115
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 291.1222 - loglik: -2.7604e+02 - logprior: -1.5082e+01
Epoch 2/10
10/10 - 2s - loss: 253.6240 - loglik: -2.4957e+02 - logprior: -4.0467e+00
Epoch 3/10
10/10 - 2s - loss: 227.2193 - loglik: -2.2479e+02 - logprior: -2.4160e+00
Epoch 4/10
10/10 - 2s - loss: 212.6170 - loglik: -2.1038e+02 - logprior: -2.0954e+00
Epoch 5/10
10/10 - 2s - loss: 207.5130 - loglik: -2.0517e+02 - logprior: -2.0666e+00
Epoch 6/10
10/10 - 2s - loss: 204.8539 - loglik: -2.0230e+02 - logprior: -2.2353e+00
Epoch 7/10
10/10 - 2s - loss: 200.6584 - loglik: -1.9825e+02 - logprior: -2.1652e+00
Epoch 8/10
10/10 - 2s - loss: 200.0833 - loglik: -1.9776e+02 - logprior: -2.1496e+00
Epoch 9/10
10/10 - 2s - loss: 201.3007 - loglik: -1.9895e+02 - logprior: -2.1897e+00
Fitted a model with MAP estimate = -199.9192
expansions: [(20, 1), (32, 1), (34, 1), (36, 1), (37, 1), (44, 1), (45, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 225.6282 - loglik: -2.0810e+02 - logprior: -1.7501e+01
Epoch 2/2
10/10 - 2s - loss: 206.1189 - loglik: -1.9837e+02 - logprior: -7.6017e+00
Fitted a model with MAP estimate = -202.4408
expansions: [(0, 8)]
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 212.1434 - loglik: -1.9767e+02 - logprior: -1.4453e+01
Epoch 2/2
10/10 - 2s - loss: 196.2987 - loglik: -1.9172e+02 - logprior: -4.4720e+00
Fitted a model with MAP estimate = -192.3039
expansions: []
discards: [0 1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 211.6925 - loglik: -1.9587e+02 - logprior: -1.5806e+01
Epoch 2/10
10/10 - 2s - loss: 200.2294 - loglik: -1.9540e+02 - logprior: -4.7522e+00
Epoch 3/10
10/10 - 2s - loss: 196.4545 - loglik: -1.9377e+02 - logprior: -2.5288e+00
Epoch 4/10
10/10 - 2s - loss: 193.8969 - loglik: -1.9180e+02 - logprior: -1.8681e+00
Epoch 5/10
10/10 - 2s - loss: 194.3637 - loglik: -1.9261e+02 - logprior: -1.5023e+00
Fitted a model with MAP estimate = -192.5362
Time for alignment: 52.4323
Computed alignments with likelihoods: ['-186.2950', '-194.2414', '-192.3039']
Best model has likelihood: -186.2950
time for generating output: 0.2189
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3826578699340245
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb639729c10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e9de9ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57a15b730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb604f88bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb604f88a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb604f88520>, <__main__.SimpleDirichletPrior object at 0x7fb5367b3340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 614.9470 - loglik: -5.6064e+02 - logprior: -5.4293e+01
Epoch 2/10
10/10 - 3s - loss: 519.5051 - loglik: -5.0921e+02 - logprior: -1.0277e+01
Epoch 3/10
10/10 - 3s - loss: 457.6187 - loglik: -4.5424e+02 - logprior: -3.2945e+00
Epoch 4/10
10/10 - 3s - loss: 420.2436 - loglik: -4.1842e+02 - logprior: -1.6175e+00
Epoch 5/10
10/10 - 3s - loss: 406.4145 - loglik: -4.0504e+02 - logprior: -9.8849e-01
Epoch 6/10
10/10 - 3s - loss: 400.7270 - loglik: -3.9976e+02 - logprior: -4.0149e-01
Epoch 7/10
10/10 - 3s - loss: 397.8126 - loglik: -3.9710e+02 - logprior: -1.6596e-01
Epoch 8/10
10/10 - 3s - loss: 396.6779 - loglik: -3.9608e+02 - logprior: -6.5682e-02
Epoch 9/10
10/10 - 3s - loss: 395.9366 - loglik: -3.9553e+02 - logprior: 0.1405
Epoch 10/10
10/10 - 3s - loss: 395.3552 - loglik: -3.9511e+02 - logprior: 0.2961
Fitted a model with MAP estimate = -394.5103
expansions: [(7, 3), (19, 1), (22, 1), (23, 2), (32, 1), (43, 1), (44, 1), (45, 2), (46, 1), (52, 2), (81, 1), (82, 1), (94, 1), (103, 1), (105, 2), (106, 1), (116, 4), (150, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 480.6520 - loglik: -4.1754e+02 - logprior: -6.3089e+01
Epoch 2/2
10/10 - 3s - loss: 415.2717 - loglik: -3.9195e+02 - logprior: -2.3144e+01
Fitted a model with MAP estimate = -403.7394
expansions: [(0, 11), (119, 1)]
discards: [  0  27  64 139 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 446.4398 - loglik: -3.9755e+02 - logprior: -4.8868e+01
Epoch 2/2
10/10 - 3s - loss: 396.8234 - loglik: -3.8716e+02 - logprior: -9.5356e+00
Fitted a model with MAP estimate = -387.7759
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 63]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 440.6291 - loglik: -3.9301e+02 - logprior: -4.7608e+01
Epoch 2/10
10/10 - 3s - loss: 396.1903 - loglik: -3.8759e+02 - logprior: -8.5526e+00
Epoch 3/10
10/10 - 3s - loss: 384.2192 - loglik: -3.8345e+02 - logprior: -6.0753e-01
Epoch 4/10
10/10 - 3s - loss: 375.7332 - loglik: -3.7788e+02 - logprior: 2.5205
Epoch 5/10
10/10 - 3s - loss: 372.7944 - loglik: -3.7657e+02 - logprior: 4.3272
Epoch 6/10
10/10 - 3s - loss: 369.1515 - loglik: -3.7390e+02 - logprior: 5.3957
Epoch 7/10
10/10 - 3s - loss: 367.1443 - loglik: -3.7259e+02 - logprior: 6.1348
Epoch 8/10
10/10 - 3s - loss: 365.2952 - loglik: -3.7132e+02 - logprior: 6.7005
Epoch 9/10
10/10 - 3s - loss: 365.9189 - loglik: -3.7245e+02 - logprior: 7.1827
Fitted a model with MAP estimate = -364.3518
Time for alignment: 91.2510
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 615.1305 - loglik: -5.6082e+02 - logprior: -5.4292e+01
Epoch 2/10
10/10 - 3s - loss: 518.5186 - loglik: -5.0822e+02 - logprior: -1.0276e+01
Epoch 3/10
10/10 - 3s - loss: 459.5341 - loglik: -4.5612e+02 - logprior: -3.3221e+00
Epoch 4/10
10/10 - 3s - loss: 425.6034 - loglik: -4.2384e+02 - logprior: -1.4731e+00
Epoch 5/10
10/10 - 3s - loss: 411.9610 - loglik: -4.1057e+02 - logprior: -7.9186e-01
Epoch 6/10
10/10 - 3s - loss: 405.5807 - loglik: -4.0447e+02 - logprior: -3.1403e-01
Epoch 7/10
10/10 - 3s - loss: 402.6569 - loglik: -4.0191e+02 - logprior: -1.2677e-01
Epoch 8/10
10/10 - 3s - loss: 399.7929 - loglik: -3.9937e+02 - logprior: 0.0513
Epoch 9/10
10/10 - 3s - loss: 399.6908 - loglik: -3.9949e+02 - logprior: 0.2661
Epoch 10/10
10/10 - 3s - loss: 398.4594 - loglik: -3.9838e+02 - logprior: 0.4054
Fitted a model with MAP estimate = -397.5882
expansions: [(9, 2), (15, 1), (19, 2), (20, 2), (22, 1), (23, 1), (25, 1), (30, 3), (44, 2), (46, 1), (47, 1), (53, 1), (75, 1), (81, 2), (104, 1), (107, 3), (113, 1), (117, 3), (119, 1), (150, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 482.4117 - loglik: -4.1958e+02 - logprior: -6.2805e+01
Epoch 2/2
10/10 - 3s - loss: 420.1938 - loglik: -3.9666e+02 - logprior: -2.3348e+01
Fitted a model with MAP estimate = -407.0831
expansions: [(0, 14), (94, 2)]
discards: [  0  22  25 100 128 129 130 143 180 181 182]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 451.4213 - loglik: -4.0229e+02 - logprior: -4.9120e+01
Epoch 2/2
10/10 - 3s - loss: 403.6448 - loglik: -3.9366e+02 - logprior: -9.8931e+00
Fitted a model with MAP estimate = -394.1970
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 50 68]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 445.9942 - loglik: -3.9837e+02 - logprior: -4.7604e+01
Epoch 2/10
10/10 - 3s - loss: 403.1862 - loglik: -3.9458e+02 - logprior: -8.5512e+00
Epoch 3/10
10/10 - 3s - loss: 391.0359 - loglik: -3.9034e+02 - logprior: -5.8797e-01
Epoch 4/10
10/10 - 3s - loss: 383.5339 - loglik: -3.8587e+02 - logprior: 2.5978
Epoch 5/10
10/10 - 3s - loss: 378.4237 - loglik: -3.8242e+02 - logprior: 4.4498
Epoch 6/10
10/10 - 3s - loss: 376.4042 - loglik: -3.8139e+02 - logprior: 5.5602
Epoch 7/10
10/10 - 3s - loss: 373.0128 - loglik: -3.7870e+02 - logprior: 6.3109
Epoch 8/10
10/10 - 3s - loss: 373.3517 - loglik: -3.7961e+02 - logprior: 6.8845
Fitted a model with MAP estimate = -371.5052
Time for alignment: 87.1366
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 615.1715 - loglik: -5.6086e+02 - logprior: -5.4294e+01
Epoch 2/10
10/10 - 3s - loss: 519.1151 - loglik: -5.0884e+02 - logprior: -1.0254e+01
Epoch 3/10
10/10 - 3s - loss: 455.1104 - loglik: -4.5192e+02 - logprior: -3.1072e+00
Epoch 4/10
10/10 - 3s - loss: 419.1572 - loglik: -4.1779e+02 - logprior: -1.1534e+00
Epoch 5/10
10/10 - 3s - loss: 406.6380 - loglik: -4.0573e+02 - logprior: -5.3379e-01
Epoch 6/10
10/10 - 3s - loss: 399.5523 - loglik: -3.9899e+02 - logprior: -7.3661e-03
Epoch 7/10
10/10 - 3s - loss: 398.5882 - loglik: -3.9840e+02 - logprior: 0.3462
Epoch 8/10
10/10 - 3s - loss: 396.5116 - loglik: -3.9650e+02 - logprior: 0.4873
Epoch 9/10
10/10 - 3s - loss: 395.3535 - loglik: -3.9551e+02 - logprior: 0.6595
Epoch 10/10
10/10 - 3s - loss: 394.7414 - loglik: -3.9504e+02 - logprior: 0.7994
Fitted a model with MAP estimate = -393.8550
expansions: [(0, 3), (19, 2), (20, 2), (22, 2), (23, 2), (25, 1), (29, 3), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (75, 3), (81, 2), (97, 2), (106, 1), (107, 2), (114, 1), (116, 4), (145, 3), (150, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 211 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 483.7839 - loglik: -4.1346e+02 - logprior: -7.0303e+01
Epoch 2/2
10/10 - 3s - loss: 406.0834 - loglik: -3.8807e+02 - logprior: -1.7847e+01
Fitted a model with MAP estimate = -388.6232
expansions: []
discards: [  0   1   2  23  25  30  32  36  41  71  99 106 138 149 183]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 447.1866 - loglik: -3.9629e+02 - logprior: -5.0877e+01
Epoch 2/2
10/10 - 3s - loss: 394.9293 - loglik: -3.8504e+02 - logprior: -9.7584e+00
Fitted a model with MAP estimate = -387.0946
expansions: [(0, 10), (177, 2)]
discards: [ 52 169]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 458.1568 - loglik: -3.9177e+02 - logprior: -6.6370e+01
Epoch 2/10
10/10 - 3s - loss: 400.8286 - loglik: -3.8575e+02 - logprior: -1.4975e+01
Epoch 3/10
10/10 - 3s - loss: 383.8077 - loglik: -3.8094e+02 - logprior: -2.5645e+00
Epoch 4/10
10/10 - 3s - loss: 374.0515 - loglik: -3.7563e+02 - logprior: 2.1726
Epoch 5/10
10/10 - 3s - loss: 370.0172 - loglik: -3.7324e+02 - logprior: 3.9821
Epoch 6/10
10/10 - 3s - loss: 365.0422 - loglik: -3.6943e+02 - logprior: 5.1047
Epoch 7/10
10/10 - 3s - loss: 361.9967 - loglik: -3.6716e+02 - logprior: 5.8170
Epoch 8/10
10/10 - 3s - loss: 361.1072 - loglik: -3.6687e+02 - logprior: 6.4140
Epoch 9/10
10/10 - 3s - loss: 360.2508 - loglik: -3.6657e+02 - logprior: 6.9595
Epoch 10/10
10/10 - 3s - loss: 359.7739 - loglik: -3.6659e+02 - logprior: 7.4396
Fitted a model with MAP estimate = -358.0633
Time for alignment: 97.8027
Computed alignments with likelihoods: ['-364.3518', '-371.5052', '-358.0633']
Best model has likelihood: -358.0633
time for generating output: 0.2601
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7734558248631743
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb641c04ac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb65b9df7c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb65b9df250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb571a3d970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb65bd42910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb524bf3c10>, <__main__.SimpleDirichletPrior object at 0x7fb52e28bf70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.2398 - loglik: -2.3503e+02 - logprior: -3.1554e+00
Epoch 2/10
19/19 - 2s - loss: 206.2059 - loglik: -2.0471e+02 - logprior: -1.2291e+00
Epoch 3/10
19/19 - 2s - loss: 196.0218 - loglik: -1.9415e+02 - logprior: -1.3667e+00
Epoch 4/10
19/19 - 2s - loss: 193.3825 - loglik: -1.9170e+02 - logprior: -1.3131e+00
Epoch 5/10
19/19 - 2s - loss: 192.4891 - loglik: -1.9088e+02 - logprior: -1.3004e+00
Epoch 6/10
19/19 - 2s - loss: 192.4216 - loglik: -1.9086e+02 - logprior: -1.2723e+00
Epoch 7/10
19/19 - 2s - loss: 192.2675 - loglik: -1.9072e+02 - logprior: -1.2651e+00
Epoch 8/10
19/19 - 2s - loss: 191.8822 - loglik: -1.9035e+02 - logprior: -1.2586e+00
Epoch 9/10
19/19 - 2s - loss: 191.7094 - loglik: -1.9018e+02 - logprior: -1.2533e+00
Epoch 10/10
19/19 - 2s - loss: 191.9094 - loglik: -1.9040e+02 - logprior: -1.2418e+00
Fitted a model with MAP estimate = -185.7986
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (22, 1), (46, 1), (47, 1), (48, 2), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 194.6646 - loglik: -1.9170e+02 - logprior: -2.8949e+00
Epoch 2/2
19/19 - 2s - loss: 186.6256 - loglik: -1.8510e+02 - logprior: -1.1895e+00
Fitted a model with MAP estimate = -178.4427
expansions: [(20, 1), (30, 2)]
discards: [ 0 23 24 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 192.6846 - loglik: -1.8878e+02 - logprior: -3.8337e+00
Epoch 2/2
19/19 - 2s - loss: 188.1444 - loglik: -1.8578e+02 - logprior: -2.0585e+00
Fitted a model with MAP estimate = -179.3965
expansions: [(0, 4), (23, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 180.4249 - loglik: -1.7841e+02 - logprior: -1.9426e+00
Epoch 2/10
23/23 - 3s - loss: 177.4092 - loglik: -1.7610e+02 - logprior: -1.0764e+00
Epoch 3/10
23/23 - 3s - loss: 175.7525 - loglik: -1.7427e+02 - logprior: -1.0728e+00
Epoch 4/10
23/23 - 3s - loss: 174.8796 - loglik: -1.7333e+02 - logprior: -1.0476e+00
Epoch 5/10
23/23 - 3s - loss: 174.6178 - loglik: -1.7307e+02 - logprior: -1.0353e+00
Epoch 6/10
23/23 - 3s - loss: 174.1283 - loglik: -1.7262e+02 - logprior: -1.0194e+00
Epoch 7/10
23/23 - 3s - loss: 174.2991 - loglik: -1.7281e+02 - logprior: -1.0046e+00
Fitted a model with MAP estimate = -173.2522
Time for alignment: 84.2898
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.3817 - loglik: -2.3516e+02 - logprior: -3.1624e+00
Epoch 2/10
19/19 - 2s - loss: 206.4681 - loglik: -2.0489e+02 - logprior: -1.2310e+00
Epoch 3/10
19/19 - 2s - loss: 195.7664 - loglik: -1.9381e+02 - logprior: -1.3889e+00
Epoch 4/10
19/19 - 2s - loss: 193.3696 - loglik: -1.9164e+02 - logprior: -1.3559e+00
Epoch 5/10
19/19 - 2s - loss: 192.5696 - loglik: -1.9094e+02 - logprior: -1.3358e+00
Epoch 6/10
19/19 - 2s - loss: 192.2736 - loglik: -1.9069e+02 - logprior: -1.3067e+00
Epoch 7/10
19/19 - 2s - loss: 191.9435 - loglik: -1.9038e+02 - logprior: -1.2902e+00
Epoch 8/10
19/19 - 2s - loss: 191.7660 - loglik: -1.9022e+02 - logprior: -1.2852e+00
Epoch 9/10
19/19 - 2s - loss: 191.7747 - loglik: -1.9025e+02 - logprior: -1.2759e+00
Fitted a model with MAP estimate = -185.3288
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (18, 1), (23, 2), (24, 1), (46, 1), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.9937 - loglik: -1.9265e+02 - logprior: -4.2732e+00
Epoch 2/2
19/19 - 2s - loss: 188.0535 - loglik: -1.8622e+02 - logprior: -1.4990e+00
Fitted a model with MAP estimate = -178.9975
expansions: [(9, 2), (23, 3)]
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 190.9147 - loglik: -1.8692e+02 - logprior: -3.9267e+00
Epoch 2/2
19/19 - 2s - loss: 185.6360 - loglik: -1.8378e+02 - logprior: -1.5791e+00
Fitted a model with MAP estimate = -177.3084
expansions: [(31, 1)]
discards: [ 0  1  9 10 24 25 26 27 28]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 182.8078 - loglik: -1.8044e+02 - logprior: -2.2958e+00
Epoch 2/10
23/23 - 3s - loss: 178.9126 - loglik: -1.7769e+02 - logprior: -9.9718e-01
Epoch 3/10
23/23 - 3s - loss: 178.2189 - loglik: -1.7686e+02 - logprior: -9.7164e-01
Epoch 4/10
23/23 - 3s - loss: 177.0860 - loglik: -1.7566e+02 - logprior: -9.4574e-01
Epoch 5/10
23/23 - 3s - loss: 176.9208 - loglik: -1.7551e+02 - logprior: -9.3802e-01
Epoch 6/10
23/23 - 3s - loss: 176.5674 - loglik: -1.7520e+02 - logprior: -9.2039e-01
Epoch 7/10
23/23 - 3s - loss: 176.0759 - loglik: -1.7475e+02 - logprior: -8.9557e-01
Epoch 8/10
23/23 - 3s - loss: 176.2121 - loglik: -1.7490e+02 - logprior: -8.8040e-01
Fitted a model with MAP estimate = -175.3622
Time for alignment: 85.5643
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.2990 - loglik: -2.3509e+02 - logprior: -3.1626e+00
Epoch 2/10
19/19 - 2s - loss: 206.4642 - loglik: -2.0491e+02 - logprior: -1.2336e+00
Epoch 3/10
19/19 - 2s - loss: 195.6516 - loglik: -1.9371e+02 - logprior: -1.4191e+00
Epoch 4/10
19/19 - 2s - loss: 193.2662 - loglik: -1.9154e+02 - logprior: -1.3433e+00
Epoch 5/10
19/19 - 2s - loss: 192.5092 - loglik: -1.9083e+02 - logprior: -1.3417e+00
Epoch 6/10
19/19 - 2s - loss: 191.6641 - loglik: -1.9002e+02 - logprior: -1.3414e+00
Epoch 7/10
19/19 - 2s - loss: 191.8128 - loglik: -1.9021e+02 - logprior: -1.3119e+00
Fitted a model with MAP estimate = -184.7825
expansions: [(0, 2), (3, 1), (5, 2), (7, 1), (15, 1), (17, 4), (18, 2), (22, 2), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.7202 - loglik: -1.9244e+02 - logprior: -4.2139e+00
Epoch 2/2
19/19 - 2s - loss: 186.8663 - loglik: -1.8493e+02 - logprior: -1.6030e+00
Fitted a model with MAP estimate = -178.2202
expansions: [(7, 1), (30, 1)]
discards: [ 0  1  2 24 25 26 27 36 63]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 192.7779 - loglik: -1.8888e+02 - logprior: -3.8320e+00
Epoch 2/2
19/19 - 2s - loss: 187.5483 - loglik: -1.8566e+02 - logprior: -1.6087e+00
Fitted a model with MAP estimate = -178.8443
expansions: [(0, 3), (22, 3)]
discards: [0 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 181.2820 - loglik: -1.7935e+02 - logprior: -1.8525e+00
Epoch 2/10
23/23 - 3s - loss: 177.7666 - loglik: -1.7648e+02 - logprior: -1.0563e+00
Epoch 3/10
23/23 - 3s - loss: 176.8985 - loglik: -1.7545e+02 - logprior: -1.0541e+00
Epoch 4/10
23/23 - 3s - loss: 176.0669 - loglik: -1.7455e+02 - logprior: -1.0129e+00
Epoch 5/10
23/23 - 3s - loss: 175.6143 - loglik: -1.7410e+02 - logprior: -1.0058e+00
Epoch 6/10
23/23 - 3s - loss: 175.3975 - loglik: -1.7392e+02 - logprior: -9.9046e-01
Epoch 7/10
23/23 - 3s - loss: 175.2250 - loglik: -1.7377e+02 - logprior: -9.7458e-01
Epoch 8/10
23/23 - 3s - loss: 174.6592 - loglik: -1.7322e+02 - logprior: -9.6673e-01
Epoch 9/10
23/23 - 3s - loss: 174.6507 - loglik: -1.7321e+02 - logprior: -9.4818e-01
Epoch 10/10
23/23 - 3s - loss: 174.5123 - loglik: -1.7306e+02 - logprior: -9.4696e-01
Fitted a model with MAP estimate = -173.5909
Time for alignment: 84.6887
Computed alignments with likelihoods: ['-173.2522', '-175.3622', '-173.5909']
Best model has likelihood: -173.2522
time for generating output: 0.1697
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8932411674347158
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb560bcbd60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb571869850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb571869550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c53389a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f619a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69f960dc0>, <__main__.SimpleDirichletPrior object at 0x7fb4c69cae20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 795.5178 - loglik: -7.8915e+02 - logprior: -6.2478e+00
Epoch 2/10
22/22 - 14s - loss: 685.8494 - loglik: -6.8412e+02 - logprior: -1.1918e+00
Epoch 3/10
22/22 - 14s - loss: 645.8950 - loglik: -6.4205e+02 - logprior: -2.5763e+00
Epoch 4/10
22/22 - 14s - loss: 637.8485 - loglik: -6.3419e+02 - logprior: -2.5585e+00
Epoch 5/10
22/22 - 14s - loss: 636.4668 - loglik: -6.3294e+02 - logprior: -2.6406e+00
Epoch 6/10
22/22 - 14s - loss: 634.0491 - loglik: -6.3047e+02 - logprior: -2.7018e+00
Epoch 7/10
22/22 - 14s - loss: 631.5060 - loglik: -6.2789e+02 - logprior: -2.7410e+00
Epoch 8/10
22/22 - 14s - loss: 631.6708 - loglik: -6.2811e+02 - logprior: -2.7378e+00
Fitted a model with MAP estimate = -631.0030
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (45, 1), (48, 2), (49, 1), (66, 1), (70, 1), (71, 1), (76, 1), (77, 2), (78, 1), (79, 1), (81, 1), (83, 1), (97, 1), (104, 1), (105, 2), (107, 1), (119, 1), (121, 2), (137, 1), (143, 2), (144, 1), (149, 1), (151, 1), (152, 1), (156, 1), (157, 1), (176, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (186, 1), (194, 2), (206, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 649.3965 - loglik: -6.4019e+02 - logprior: -9.1003e+00
Epoch 2/2
22/22 - 19s - loss: 618.3029 - loglik: -6.1476e+02 - logprior: -3.1631e+00
Fitted a model with MAP estimate = -613.9734
expansions: [(0, 3), (247, 1)]
discards: [  0  34  99 175 220 230]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 626.4781 - loglik: -6.2086e+02 - logprior: -5.5229e+00
Epoch 2/2
22/22 - 19s - loss: 613.9571 - loglik: -6.1373e+02 - logprior: 0.2981
Fitted a model with MAP estimate = -608.6820
expansions: []
discards: [  0   1   2 269]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 24s - loss: 631.9106 - loglik: -6.2365e+02 - logprior: -8.1541e+00
Epoch 2/10
22/22 - 19s - loss: 615.9104 - loglik: -6.1310e+02 - logprior: -2.3055e+00
Epoch 3/10
22/22 - 19s - loss: 613.0490 - loglik: -6.1120e+02 - logprior: -8.8272e-01
Epoch 4/10
22/22 - 19s - loss: 604.9675 - loglik: -6.0528e+02 - logprior: 1.5362
Epoch 5/10
22/22 - 19s - loss: 604.9199 - loglik: -6.0548e+02 - logprior: 1.7704
Epoch 6/10
22/22 - 19s - loss: 603.9572 - loglik: -6.0475e+02 - logprior: 1.8987
Epoch 7/10
22/22 - 19s - loss: 604.2621 - loglik: -6.0535e+02 - logprior: 2.0930
Fitted a model with MAP estimate = -601.4431
Time for alignment: 393.0275
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 18s - loss: 798.7792 - loglik: -7.9242e+02 - logprior: -6.2300e+00
Epoch 2/10
22/22 - 14s - loss: 684.2250 - loglik: -6.8241e+02 - logprior: -1.2599e+00
Epoch 3/10
22/22 - 14s - loss: 645.0583 - loglik: -6.4101e+02 - logprior: -2.7928e+00
Epoch 4/10
22/22 - 14s - loss: 637.4235 - loglik: -6.3358e+02 - logprior: -2.7499e+00
Epoch 5/10
22/22 - 14s - loss: 635.2759 - loglik: -6.3167e+02 - logprior: -2.7422e+00
Epoch 6/10
22/22 - 14s - loss: 632.7914 - loglik: -6.2917e+02 - logprior: -2.7766e+00
Epoch 7/10
22/22 - 14s - loss: 632.7193 - loglik: -6.2910e+02 - logprior: -2.7824e+00
Epoch 8/10
22/22 - 14s - loss: 630.6345 - loglik: -6.2703e+02 - logprior: -2.7844e+00
Epoch 9/10
22/22 - 14s - loss: 633.6101 - loglik: -6.3006e+02 - logprior: -2.7777e+00
Fitted a model with MAP estimate = -630.1943
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 3), (35, 2), (44, 1), (45, 1), (47, 1), (48, 1), (49, 1), (60, 1), (70, 2), (71, 1), (75, 2), (78, 1), (80, 1), (82, 1), (104, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (176, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 1), (207, 1), (208, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (235, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 648.3600 - loglik: -6.3908e+02 - logprior: -9.1783e+00
Epoch 2/2
22/22 - 20s - loss: 618.6179 - loglik: -6.1499e+02 - logprior: -3.2373e+00
Fitted a model with MAP estimate = -613.1266
expansions: [(0, 3)]
discards: [  0  35  36  37  40 176 196 198 223 233 275]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 628.0286 - loglik: -6.2246e+02 - logprior: -5.4688e+00
Epoch 2/2
22/22 - 19s - loss: 612.6829 - loglik: -6.1251e+02 - logprior: 0.3433
Fitted a model with MAP estimate = -609.4172
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 630.9012 - loglik: -6.2282e+02 - logprior: -7.9663e+00
Epoch 2/10
22/22 - 19s - loss: 618.4934 - loglik: -6.1579e+02 - logprior: -2.2066e+00
Epoch 3/10
22/22 - 19s - loss: 612.0761 - loglik: -6.1021e+02 - logprior: -8.9450e-01
Epoch 4/10
22/22 - 19s - loss: 607.7095 - loglik: -6.0804e+02 - logprior: 1.5485
Epoch 5/10
22/22 - 19s - loss: 604.4630 - loglik: -6.0504e+02 - logprior: 1.7884
Epoch 6/10
22/22 - 19s - loss: 603.6736 - loglik: -6.0447e+02 - logprior: 1.9221
Epoch 7/10
22/22 - 19s - loss: 605.1970 - loglik: -6.0632e+02 - logprior: 2.1403
Fitted a model with MAP estimate = -602.0307
Time for alignment: 406.0795
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 799.0285 - loglik: -7.9268e+02 - logprior: -6.2253e+00
Epoch 2/10
22/22 - 14s - loss: 686.4236 - loglik: -6.8477e+02 - logprior: -1.1616e+00
Epoch 3/10
22/22 - 14s - loss: 644.3608 - loglik: -6.4091e+02 - logprior: -2.6297e+00
Epoch 4/10
22/22 - 14s - loss: 639.8060 - loglik: -6.3629e+02 - logprior: -2.5571e+00
Epoch 5/10
22/22 - 14s - loss: 634.9898 - loglik: -6.3151e+02 - logprior: -2.5627e+00
Epoch 6/10
22/22 - 14s - loss: 634.4149 - loglik: -6.3095e+02 - logprior: -2.5647e+00
Epoch 7/10
22/22 - 14s - loss: 633.6718 - loglik: -6.3022e+02 - logprior: -2.5880e+00
Epoch 8/10
22/22 - 14s - loss: 634.9174 - loglik: -6.3152e+02 - logprior: -2.5829e+00
Fitted a model with MAP estimate = -631.6549
expansions: [(12, 1), (13, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 7), (48, 1), (49, 1), (50, 1), (69, 1), (70, 1), (71, 1), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (105, 2), (106, 1), (107, 1), (108, 1), (143, 4), (144, 2), (148, 1), (149, 1), (155, 2), (158, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 1), (194, 1), (206, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 3), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 650.2585 - loglik: -6.4091e+02 - logprior: -9.2551e+00
Epoch 2/2
22/22 - 20s - loss: 617.9564 - loglik: -6.1417e+02 - logprior: -3.4027e+00
Fitted a model with MAP estimate = -612.2750
expansions: [(0, 3), (153, 2), (199, 1), (292, 1)]
discards: [  0  34  55  56  57  58 178 179 181 223 233 275 288 289 290]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 627.6014 - loglik: -6.2196e+02 - logprior: -5.5346e+00
Epoch 2/2
22/22 - 19s - loss: 611.0167 - loglik: -6.1083e+02 - logprior: 0.3381
Fitted a model with MAP estimate = -608.1454
expansions: []
discards: [  0   1   2 282]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 24s - loss: 631.2925 - loglik: -6.2317e+02 - logprior: -8.0076e+00
Epoch 2/10
22/22 - 19s - loss: 616.8020 - loglik: -6.1408e+02 - logprior: -2.2329e+00
Epoch 3/10
22/22 - 19s - loss: 611.8076 - loglik: -6.0994e+02 - logprior: -9.0075e-01
Epoch 4/10
22/22 - 19s - loss: 605.2602 - loglik: -6.0554e+02 - logprior: 1.5058
Epoch 5/10
22/22 - 19s - loss: 605.6832 - loglik: -6.0622e+02 - logprior: 1.7576
Fitted a model with MAP estimate = -602.6758
Time for alignment: 357.0255
Computed alignments with likelihoods: ['-601.4431', '-602.0307', '-602.6758']
Best model has likelihood: -601.4431
time for generating output: 0.4088
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9080260827564
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb605cc56a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5610e8490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb603754a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb560caf1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb69714fac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb579c2fe80>, <__main__.SimpleDirichletPrior object at 0x7fb4bc0fc190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6167 - loglik: -5.3775e+01 - logprior: -5.8246e+00
Epoch 6/10
10/10 - 1s - loss: 57.4746 - loglik: -5.3063e+01 - logprior: -4.3854e+00
Epoch 7/10
10/10 - 1s - loss: 56.3188 - loglik: -5.2641e+01 - logprior: -3.5631e+00
Epoch 8/10
10/10 - 1s - loss: 55.6869 - loglik: -5.2468e+01 - logprior: -3.0361e+00
Epoch 9/10
10/10 - 1s - loss: 55.3504 - loglik: -5.2489e+01 - logprior: -2.7107e+00
Epoch 10/10
10/10 - 1s - loss: 55.1281 - loglik: -5.2515e+01 - logprior: -2.4797e+00
Fitted a model with MAP estimate = -54.8997
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3456 - loglik: -4.9285e+01 - logprior: -1.2404e+02
Epoch 2/2
10/10 - 1s - loss: 86.6340 - loglik: -4.6074e+01 - logprior: -4.0491e+01
Fitted a model with MAP estimate = -69.6085
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 130.8081 - loglik: -4.3509e+01 - logprior: -8.7283e+01
Epoch 2/10
10/10 - 1s - loss: 69.2402 - loglik: -4.3789e+01 - logprior: -2.5390e+01
Epoch 3/10
10/10 - 1s - loss: 56.8005 - loglik: -4.4028e+01 - logprior: -1.2665e+01
Epoch 4/10
10/10 - 1s - loss: 51.8620 - loglik: -4.4166e+01 - logprior: -7.5461e+00
Epoch 5/10
10/10 - 1s - loss: 49.3295 - loglik: -4.4312e+01 - logprior: -4.8053e+00
Epoch 6/10
10/10 - 1s - loss: 47.8538 - loglik: -4.4365e+01 - logprior: -3.2671e+00
Epoch 7/10
10/10 - 1s - loss: 46.4210 - loglik: -4.3763e+01 - logprior: -2.4070e+00
Epoch 8/10
10/10 - 1s - loss: 45.6298 - loglik: -4.3454e+01 - logprior: -1.9204e+00
Epoch 9/10
10/10 - 1s - loss: 45.1983 - loglik: -4.3379e+01 - logprior: -1.5768e+00
Epoch 10/10
10/10 - 1s - loss: 44.9225 - loglik: -4.3424e+01 - logprior: -1.2575e+00
Fitted a model with MAP estimate = -44.5469
Time for alignment: 31.2239
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6173 - loglik: -5.3776e+01 - logprior: -5.8242e+00
Epoch 6/10
10/10 - 1s - loss: 57.5006 - loglik: -5.3107e+01 - logprior: -4.3745e+00
Epoch 7/10
10/10 - 1s - loss: 56.3321 - loglik: -5.2691e+01 - logprior: -3.5563e+00
Epoch 8/10
10/10 - 1s - loss: 55.6881 - loglik: -5.2483e+01 - logprior: -3.0327e+00
Epoch 9/10
10/10 - 1s - loss: 55.3547 - loglik: -5.2492e+01 - logprior: -2.7095e+00
Epoch 10/10
10/10 - 1s - loss: 55.1218 - loglik: -5.2507e+01 - logprior: -2.4846e+00
Fitted a model with MAP estimate = -54.8761
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3415 - loglik: -4.9279e+01 - logprior: -1.2404e+02
Epoch 2/2
10/10 - 1s - loss: 86.6356 - loglik: -4.6077e+01 - logprior: -4.0492e+01
Fitted a model with MAP estimate = -69.6008
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 130.8064 - loglik: -4.3508e+01 - logprior: -8.7282e+01
Epoch 2/10
10/10 - 1s - loss: 69.2408 - loglik: -4.3783e+01 - logprior: -2.5393e+01
Epoch 3/10
10/10 - 1s - loss: 56.7965 - loglik: -4.4014e+01 - logprior: -1.2669e+01
Epoch 4/10
10/10 - 1s - loss: 51.8366 - loglik: -4.4115e+01 - logprior: -7.5631e+00
Epoch 5/10
10/10 - 1s - loss: 49.2772 - loglik: -4.4227e+01 - logprior: -4.8384e+00
Epoch 6/10
10/10 - 1s - loss: 47.7024 - loglik: -4.4199e+01 - logprior: -3.2938e+00
Epoch 7/10
10/10 - 1s - loss: 46.2921 - loglik: -4.3607e+01 - logprior: -2.4437e+00
Epoch 8/10
10/10 - 1s - loss: 45.5561 - loglik: -4.3381e+01 - logprior: -1.9320e+00
Epoch 9/10
10/10 - 1s - loss: 45.1708 - loglik: -4.3349e+01 - logprior: -1.5769e+00
Epoch 10/10
10/10 - 1s - loss: 44.9147 - loglik: -4.3407e+01 - logprior: -1.2546e+00
Fitted a model with MAP estimate = -44.5407
Time for alignment: 30.7989
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6047 - loglik: -5.3757e+01 - logprior: -5.8302e+00
Epoch 6/10
10/10 - 1s - loss: 57.3738 - loglik: -5.2896e+01 - logprior: -4.4199e+00
Epoch 7/10
10/10 - 1s - loss: 56.2572 - loglik: -5.2490e+01 - logprior: -3.5906e+00
Epoch 8/10
10/10 - 1s - loss: 55.6209 - loglik: -5.2366e+01 - logprior: -3.0616e+00
Epoch 9/10
10/10 - 1s - loss: 55.2741 - loglik: -5.2380e+01 - logprior: -2.7369e+00
Epoch 10/10
10/10 - 1s - loss: 55.0427 - loglik: -5.2365e+01 - logprior: -2.5086e+00
Fitted a model with MAP estimate = -54.7643
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3971 - loglik: -4.9313e+01 - logprior: -1.2406e+02
Epoch 2/2
10/10 - 1s - loss: 86.6133 - loglik: -4.6047e+01 - logprior: -4.0496e+01
Fitted a model with MAP estimate = -69.5837
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.8121 - loglik: -4.3515e+01 - logprior: -8.7284e+01
Epoch 2/10
10/10 - 1s - loss: 69.2333 - loglik: -4.3801e+01 - logprior: -2.5388e+01
Epoch 3/10
10/10 - 1s - loss: 56.7855 - loglik: -4.4019e+01 - logprior: -1.2666e+01
Epoch 4/10
10/10 - 1s - loss: 51.8388 - loglik: -4.4132e+01 - logprior: -7.5618e+00
Epoch 5/10
10/10 - 1s - loss: 49.2712 - loglik: -4.4248e+01 - logprior: -4.8300e+00
Epoch 6/10
10/10 - 1s - loss: 47.8019 - loglik: -4.4289e+01 - logprior: -3.2978e+00
Epoch 7/10
10/10 - 1s - loss: 46.3629 - loglik: -4.3695e+01 - logprior: -2.4365e+00
Epoch 8/10
10/10 - 1s - loss: 45.5770 - loglik: -4.3395e+01 - logprior: -1.9369e+00
Epoch 9/10
10/10 - 1s - loss: 45.1727 - loglik: -4.3353e+01 - logprior: -1.5768e+00
Epoch 10/10
10/10 - 1s - loss: 44.9164 - loglik: -4.3411e+01 - logprior: -1.2539e+00
Fitted a model with MAP estimate = -44.5380
Time for alignment: 29.1560
Computed alignments with likelihoods: ['-44.5469', '-44.5407', '-44.5380']
Best model has likelihood: -44.5380
time for generating output: 0.1192
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb5612b5850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb639068820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb664421a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57b0b4f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57b0b4af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57b08d1c0>, <__main__.SimpleDirichletPrior object at 0x7fb50bdc4d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 362.7737 - loglik: -3.5026e+02 - logprior: -1.2487e+01
Epoch 2/10
11/11 - 3s - loss: 310.0193 - loglik: -3.0687e+02 - logprior: -3.1258e+00
Epoch 3/10
11/11 - 2s - loss: 268.3566 - loglik: -2.6618e+02 - logprior: -2.1559e+00
Epoch 4/10
11/11 - 3s - loss: 249.3617 - loglik: -2.4685e+02 - logprior: -2.3650e+00
Epoch 5/10
11/11 - 3s - loss: 243.5512 - loglik: -2.4063e+02 - logprior: -2.5305e+00
Epoch 6/10
11/11 - 3s - loss: 240.0631 - loglik: -2.3698e+02 - logprior: -2.5533e+00
Epoch 7/10
11/11 - 3s - loss: 238.2340 - loglik: -2.3516e+02 - logprior: -2.5600e+00
Epoch 8/10
11/11 - 3s - loss: 237.4383 - loglik: -2.3439e+02 - logprior: -2.5568e+00
Epoch 9/10
11/11 - 3s - loss: 236.5248 - loglik: -2.3348e+02 - logprior: -2.5432e+00
Epoch 10/10
11/11 - 3s - loss: 235.9424 - loglik: -2.3294e+02 - logprior: -2.5317e+00
Fitted a model with MAP estimate = -235.7800
expansions: [(8, 2), (9, 4), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 256.4684 - loglik: -2.4187e+02 - logprior: -1.4567e+01
Epoch 2/2
11/11 - 3s - loss: 231.7050 - loglik: -2.2517e+02 - logprior: -6.3514e+00
Fitted a model with MAP estimate = -225.5202
expansions: [(0, 20)]
discards: [ 0  8 11 12 74 85 88]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 236.5854 - loglik: -2.2440e+02 - logprior: -1.2161e+01
Epoch 2/2
11/11 - 4s - loss: 221.1406 - loglik: -2.1707e+02 - logprior: -3.9068e+00
Fitted a model with MAP estimate = -218.9603
expansions: [(29, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 92]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 231.1730 - loglik: -2.1793e+02 - logprior: -1.3216e+01
Epoch 2/10
11/11 - 3s - loss: 218.2737 - loglik: -2.1462e+02 - logprior: -3.4705e+00
Epoch 3/10
11/11 - 3s - loss: 217.7532 - loglik: -2.1565e+02 - logprior: -1.7187e+00
Epoch 4/10
11/11 - 3s - loss: 214.6932 - loglik: -2.1285e+02 - logprior: -1.3142e+00
Epoch 5/10
11/11 - 3s - loss: 212.8743 - loglik: -2.1117e+02 - logprior: -1.1547e+00
Epoch 6/10
11/11 - 3s - loss: 212.1608 - loglik: -2.1061e+02 - logprior: -1.0347e+00
Epoch 7/10
11/11 - 3s - loss: 209.9627 - loglik: -2.0865e+02 - logprior: -8.4859e-01
Epoch 8/10
11/11 - 3s - loss: 209.9219 - loglik: -2.0869e+02 - logprior: -7.9462e-01
Epoch 9/10
11/11 - 3s - loss: 211.1099 - loglik: -2.0995e+02 - logprior: -7.2743e-01
Fitted a model with MAP estimate = -209.1623
Time for alignment: 95.1652
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.4444 - loglik: -3.4994e+02 - logprior: -1.2485e+01
Epoch 2/10
11/11 - 2s - loss: 309.6128 - loglik: -3.0645e+02 - logprior: -3.1355e+00
Epoch 3/10
11/11 - 3s - loss: 269.1836 - loglik: -2.6697e+02 - logprior: -2.1937e+00
Epoch 4/10
11/11 - 3s - loss: 249.4149 - loglik: -2.4684e+02 - logprior: -2.4280e+00
Epoch 5/10
11/11 - 3s - loss: 241.2117 - loglik: -2.3819e+02 - logprior: -2.6216e+00
Epoch 6/10
11/11 - 3s - loss: 240.1301 - loglik: -2.3692e+02 - logprior: -2.6848e+00
Epoch 7/10
11/11 - 3s - loss: 237.7049 - loglik: -2.3452e+02 - logprior: -2.6871e+00
Epoch 8/10
11/11 - 3s - loss: 235.3952 - loglik: -2.3227e+02 - logprior: -2.6550e+00
Epoch 9/10
11/11 - 3s - loss: 236.5215 - loglik: -2.3344e+02 - logprior: -2.6375e+00
Fitted a model with MAP estimate = -235.5687
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (63, 1), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 253.7733 - loglik: -2.3921e+02 - logprior: -1.4529e+01
Epoch 2/2
11/11 - 3s - loss: 229.8647 - loglik: -2.2347e+02 - logprior: -6.2169e+00
Fitted a model with MAP estimate = -224.0669
expansions: [(0, 20)]
discards: [ 0  8 73 76]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 233.7590 - loglik: -2.2163e+02 - logprior: -1.2103e+01
Epoch 2/2
11/11 - 3s - loss: 218.4650 - loglik: -2.1461e+02 - logprior: -3.6788e+00
Fitted a model with MAP estimate = -215.3961
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 231.6027 - loglik: -2.1898e+02 - logprior: -1.2590e+01
Epoch 2/10
11/11 - 3s - loss: 218.9959 - loglik: -2.1558e+02 - logprior: -3.2422e+00
Epoch 3/10
11/11 - 3s - loss: 216.3198 - loglik: -2.1428e+02 - logprior: -1.6560e+00
Epoch 4/10
11/11 - 3s - loss: 215.5365 - loglik: -2.1377e+02 - logprior: -1.2364e+00
Epoch 5/10
11/11 - 3s - loss: 211.3727 - loglik: -2.0974e+02 - logprior: -1.0974e+00
Epoch 6/10
11/11 - 3s - loss: 213.7022 - loglik: -2.1226e+02 - logprior: -9.6440e-01
Fitted a model with MAP estimate = -211.6579
Time for alignment: 80.8089
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 362.9056 - loglik: -3.5039e+02 - logprior: -1.2487e+01
Epoch 2/10
11/11 - 3s - loss: 308.8421 - loglik: -3.0569e+02 - logprior: -3.1258e+00
Epoch 3/10
11/11 - 3s - loss: 268.6990 - loglik: -2.6653e+02 - logprior: -2.1468e+00
Epoch 4/10
11/11 - 3s - loss: 248.1426 - loglik: -2.4562e+02 - logprior: -2.3822e+00
Epoch 5/10
11/11 - 3s - loss: 241.1230 - loglik: -2.3817e+02 - logprior: -2.5522e+00
Epoch 6/10
11/11 - 3s - loss: 237.7009 - loglik: -2.3455e+02 - logprior: -2.5919e+00
Epoch 7/10
11/11 - 3s - loss: 237.0607 - loglik: -2.3387e+02 - logprior: -2.6175e+00
Epoch 8/10
11/11 - 2s - loss: 236.0655 - loglik: -2.3287e+02 - logprior: -2.6316e+00
Epoch 9/10
11/11 - 3s - loss: 234.7895 - loglik: -2.3163e+02 - logprior: -2.6399e+00
Epoch 10/10
11/11 - 2s - loss: 233.7986 - loglik: -2.3067e+02 - logprior: -2.6405e+00
Fitted a model with MAP estimate = -233.8773
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 253.3454 - loglik: -2.3881e+02 - logprior: -1.4508e+01
Epoch 2/2
11/11 - 3s - loss: 228.4391 - loglik: -2.2210e+02 - logprior: -6.1524e+00
Fitted a model with MAP estimate = -222.4508
expansions: [(0, 21)]
discards: [ 0  8 76]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 8s - loss: 226.7375 - loglik: -2.1871e+02 - logprior: -7.9440e+00
Epoch 2/2
22/22 - 5s - loss: 214.8990 - loglik: -2.1249e+02 - logprior: -2.0969e+00
Fitted a model with MAP estimate = -213.8181
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 229.4529 - loglik: -2.1652e+02 - logprior: -1.2903e+01
Epoch 2/10
11/11 - 3s - loss: 217.1412 - loglik: -2.1383e+02 - logprior: -3.1673e+00
Epoch 3/10
11/11 - 3s - loss: 214.2153 - loglik: -2.1251e+02 - logprior: -1.4281e+00
Epoch 4/10
11/11 - 3s - loss: 212.7785 - loglik: -2.1145e+02 - logprior: -9.7052e-01
Epoch 5/10
11/11 - 3s - loss: 210.3744 - loglik: -2.0907e+02 - logprior: -8.5583e-01
Epoch 6/10
11/11 - 3s - loss: 210.9522 - loglik: -2.0961e+02 - logprior: -8.3722e-01
Fitted a model with MAP estimate = -208.9823
Time for alignment: 88.0547
Computed alignments with likelihoods: ['-209.1623', '-211.6579', '-208.9823']
Best model has likelihood: -208.9823
time for generating output: 0.3043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.48023255813953486
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4c681b550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb560fd7e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb560fbf2e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5367a5490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb3547b2370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb536472760>, <__main__.SimpleDirichletPrior object at 0x7fb579f45c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 757.4927 - loglik: -7.5522e+02 - logprior: -1.7978e+00
Epoch 2/10
39/39 - 20s - loss: 638.6295 - loglik: -6.3561e+02 - logprior: -1.8573e+00
Epoch 3/10
39/39 - 19s - loss: 625.3217 - loglik: -6.2189e+02 - logprior: -2.0285e+00
Epoch 4/10
39/39 - 19s - loss: 622.6542 - loglik: -6.1941e+02 - logprior: -2.0104e+00
Epoch 5/10
39/39 - 19s - loss: 620.9155 - loglik: -6.1780e+02 - logprior: -2.0311e+00
Epoch 6/10
39/39 - 20s - loss: 620.4012 - loglik: -6.1737e+02 - logprior: -2.0381e+00
Epoch 7/10
39/39 - 19s - loss: 619.9321 - loglik: -6.1690e+02 - logprior: -2.0398e+00
Epoch 8/10
39/39 - 19s - loss: 619.1658 - loglik: -6.1612e+02 - logprior: -2.0582e+00
Epoch 9/10
39/39 - 20s - loss: 618.6064 - loglik: -6.1557e+02 - logprior: -2.0723e+00
Epoch 10/10
39/39 - 19s - loss: 618.6242 - loglik: -6.1562e+02 - logprior: -2.0743e+00
Fitted a model with MAP estimate = -578.9666
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (19, 1), (32, 1), (36, 1), (39, 1), (45, 2), (46, 2), (58, 1), (59, 2), (61, 4), (64, 1), (65, 1), (66, 1), (67, 1), (90, 1), (126, 2), (127, 5), (128, 2), (129, 2), (131, 1), (135, 2), (136, 4), (137, 10), (156, 2), (157, 2), (162, 1), (163, 1), (167, 2), (168, 3), (169, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (203, 1), (209, 1), (210, 1), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 621.1920 - loglik: -6.1780e+02 - logprior: -3.1529e+00
Epoch 2/2
39/39 - 28s - loss: 592.4346 - loglik: -5.9011e+02 - logprior: -1.5447e+00
Fitted a model with MAP estimate = -547.9318
expansions: [(0, 2), (231, 1), (232, 1)]
discards: [  0  11  12  58  78 156 157 158 161 173 176 177 208 209 210 213 228 248
 288]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 601.8492 - loglik: -5.9976e+02 - logprior: -1.8381e+00
Epoch 2/2
39/39 - 27s - loss: 591.3961 - loglik: -5.8949e+02 - logprior: -9.0733e-01
Fitted a model with MAP estimate = -546.9264
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 555.7148 - loglik: -5.5352e+02 - logprior: -1.9209e+00
Epoch 2/10
45/45 - 30s - loss: 547.0704 - loglik: -5.4548e+02 - logprior: -8.7576e-01
Epoch 3/10
45/45 - 30s - loss: 544.0043 - loglik: -5.4231e+02 - logprior: -7.8693e-01
Epoch 4/10
45/45 - 29s - loss: 541.1760 - loglik: -5.3932e+02 - logprior: -7.7697e-01
Epoch 5/10
45/45 - 30s - loss: 542.1013 - loglik: -5.4032e+02 - logprior: -6.3112e-01
Fitted a model with MAP estimate = -539.3541
Time for alignment: 611.0049
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 756.0930 - loglik: -7.5381e+02 - logprior: -1.8183e+00
Epoch 2/10
39/39 - 19s - loss: 638.5400 - loglik: -6.3528e+02 - logprior: -1.9329e+00
Epoch 3/10
39/39 - 19s - loss: 624.2427 - loglik: -6.2060e+02 - logprior: -2.1304e+00
Epoch 4/10
39/39 - 19s - loss: 621.0345 - loglik: -6.1767e+02 - logprior: -2.0964e+00
Epoch 5/10
39/39 - 19s - loss: 619.6733 - loglik: -6.1646e+02 - logprior: -2.0870e+00
Epoch 6/10
39/39 - 20s - loss: 618.8761 - loglik: -6.1574e+02 - logprior: -2.0928e+00
Epoch 7/10
39/39 - 20s - loss: 618.0928 - loglik: -6.1497e+02 - logprior: -2.1035e+00
Epoch 8/10
39/39 - 20s - loss: 617.5873 - loglik: -6.1447e+02 - logprior: -2.1161e+00
Epoch 9/10
39/39 - 20s - loss: 617.0654 - loglik: -6.1397e+02 - logprior: -2.1289e+00
Epoch 10/10
39/39 - 20s - loss: 616.8619 - loglik: -6.1377e+02 - logprior: -2.1387e+00
Fitted a model with MAP estimate = -576.0083
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (42, 1), (45, 2), (46, 1), (58, 2), (59, 2), (61, 5), (66, 2), (67, 2), (68, 2), (90, 1), (119, 1), (124, 3), (125, 5), (128, 2), (135, 1), (137, 2), (138, 2), (140, 1), (141, 1), (142, 4), (144, 2), (145, 1), (151, 1), (156, 1), (158, 2), (163, 3), (166, 4), (167, 3), (168, 2), (180, 2), (181, 1), (193, 1), (194, 1), (202, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 623.9667 - loglik: -6.2055e+02 - logprior: -3.1709e+00
Epoch 2/2
39/39 - 29s - loss: 592.8581 - loglik: -5.9039e+02 - logprior: -1.6901e+00
Fitted a model with MAP estimate = -548.0662
expansions: [(0, 2), (213, 1)]
discards: [  0  11  12  58  78  79  89  94 159 160 166 179 181 190 196 216 228 229
 232 250 285]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 600.9236 - loglik: -5.9882e+02 - logprior: -1.8521e+00
Epoch 2/2
39/39 - 26s - loss: 591.0076 - loglik: -5.8906e+02 - logprior: -9.8039e-01
Fitted a model with MAP estimate = -546.5006
expansions: [(203, 2)]
discards: [  0  71 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 556.5478 - loglik: -5.5445e+02 - logprior: -1.8256e+00
Epoch 2/10
45/45 - 29s - loss: 547.5202 - loglik: -5.4562e+02 - logprior: -8.8444e-01
Epoch 3/10
45/45 - 30s - loss: 543.6355 - loglik: -5.4129e+02 - logprior: -8.9148e-01
Epoch 4/10
45/45 - 29s - loss: 542.2260 - loglik: -5.3999e+02 - logprior: -8.0193e-01
Epoch 5/10
45/45 - 30s - loss: 542.8671 - loglik: -5.4085e+02 - logprior: -6.9116e-01
Fitted a model with MAP estimate = -539.3548
Time for alignment: 611.2391
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 759.2587 - loglik: -7.5696e+02 - logprior: -1.8256e+00
Epoch 2/10
39/39 - 20s - loss: 639.7450 - loglik: -6.3637e+02 - logprior: -1.9872e+00
Epoch 3/10
39/39 - 20s - loss: 625.8453 - loglik: -6.2185e+02 - logprior: -2.2259e+00
Epoch 4/10
39/39 - 20s - loss: 622.4249 - loglik: -6.1876e+02 - logprior: -2.2073e+00
Epoch 5/10
39/39 - 20s - loss: 621.1961 - loglik: -6.1768e+02 - logprior: -2.2078e+00
Epoch 6/10
39/39 - 20s - loss: 619.6765 - loglik: -6.1619e+02 - logprior: -2.2351e+00
Epoch 7/10
39/39 - 19s - loss: 618.6357 - loglik: -6.1514e+02 - logprior: -2.2725e+00
Epoch 8/10
39/39 - 20s - loss: 617.9208 - loglik: -6.1450e+02 - logprior: -2.2841e+00
Epoch 9/10
39/39 - 19s - loss: 617.2772 - loglik: -6.1391e+02 - logprior: -2.2886e+00
Epoch 10/10
39/39 - 20s - loss: 616.9528 - loglik: -6.1359e+02 - logprior: -2.2989e+00
Fitted a model with MAP estimate = -579.7940
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (42, 1), (45, 2), (46, 1), (58, 2), (59, 2), (61, 5), (66, 1), (67, 1), (68, 1), (70, 1), (97, 2), (119, 1), (121, 1), (123, 4), (124, 6), (125, 2), (127, 2), (129, 1), (130, 2), (131, 1), (138, 1), (139, 2), (140, 1), (143, 1), (144, 2), (156, 2), (157, 2), (163, 1), (164, 2), (166, 1), (168, 3), (169, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 627.0152 - loglik: -6.2354e+02 - logprior: -3.2294e+00
Epoch 2/2
39/39 - 29s - loss: 592.6409 - loglik: -5.9021e+02 - logprior: -1.6546e+00
Fitted a model with MAP estimate = -547.0188
expansions: [(0, 2), (232, 1), (233, 1)]
discards: [  0  11  12  58  78  79 155 159 160 170 173 189 196 210 211 212 225 249
 288 289]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 602.3950 - loglik: -6.0027e+02 - logprior: -1.8815e+00
Epoch 2/2
39/39 - 27s - loss: 591.9440 - loglik: -5.8999e+02 - logprior: -9.3537e-01
Fitted a model with MAP estimate = -547.0342
expansions: [(200, 2)]
discards: [  0 120]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 555.2739 - loglik: -5.5314e+02 - logprior: -1.8529e+00
Epoch 2/10
45/45 - 30s - loss: 544.9401 - loglik: -5.4304e+02 - logprior: -8.5899e-01
Epoch 3/10
45/45 - 31s - loss: 542.9042 - loglik: -5.4057e+02 - logprior: -8.6779e-01
Epoch 4/10
45/45 - 30s - loss: 540.2845 - loglik: -5.3811e+02 - logprior: -7.2697e-01
Epoch 5/10
45/45 - 31s - loss: 541.6499 - loglik: -5.3956e+02 - logprior: -7.5158e-01
Fitted a model with MAP estimate = -538.2769
Time for alignment: 614.6966
Computed alignments with likelihoods: ['-539.3541', '-539.3548', '-538.2769']
Best model has likelihood: -538.2769
time for generating output: 0.3179
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8907549052211506
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb52df419a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c7677c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54ff1a610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb50c1ab880>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6042b33a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4f2a93e80>, <__main__.SimpleDirichletPrior object at 0x7fb4c5ded160>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.7686 - loglik: -4.4442e+02 - logprior: -1.0917e+00
Epoch 2/10
30/30 - 8s - loss: 373.6282 - loglik: -3.7102e+02 - logprior: -1.2089e+00
Epoch 3/10
30/30 - 8s - loss: 360.8641 - loglik: -3.5837e+02 - logprior: -1.2140e+00
Epoch 4/10
30/30 - 8s - loss: 358.4420 - loglik: -3.5618e+02 - logprior: -1.2224e+00
Epoch 5/10
30/30 - 8s - loss: 357.5031 - loglik: -3.5542e+02 - logprior: -1.1986e+00
Epoch 6/10
30/30 - 8s - loss: 356.6372 - loglik: -3.5468e+02 - logprior: -1.1806e+00
Epoch 7/10
30/30 - 8s - loss: 356.6476 - loglik: -3.5469e+02 - logprior: -1.1685e+00
Fitted a model with MAP estimate = -346.4822
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 352.8694 - loglik: -3.5116e+02 - logprior: -1.2815e+00
Epoch 2/2
61/61 - 14s - loss: 342.3893 - loglik: -3.4008e+02 - logprior: -1.0111e+00
Fitted a model with MAP estimate = -330.3911
expansions: []
discards: [ 29  47  51  92  95 103 133 147 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 346.9593 - loglik: -3.4545e+02 - logprior: -1.1510e+00
Epoch 2/2
61/61 - 14s - loss: 342.6249 - loglik: -3.4055e+02 - logprior: -9.1381e-01
Fitted a model with MAP estimate = -330.4575
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 332.6266 - loglik: -3.3126e+02 - logprior: -7.6920e-01
Epoch 2/10
87/87 - 19s - loss: 329.1610 - loglik: -3.2684e+02 - logprior: -6.8335e-01
Epoch 3/10
87/87 - 19s - loss: 328.3202 - loglik: -3.2635e+02 - logprior: -6.5705e-01
Epoch 4/10
87/87 - 19s - loss: 327.7414 - loglik: -3.2608e+02 - logprior: -6.2817e-01
Epoch 5/10
87/87 - 19s - loss: 327.3634 - loglik: -3.2577e+02 - logprior: -6.0680e-01
Epoch 6/10
87/87 - 19s - loss: 327.0936 - loglik: -3.2553e+02 - logprior: -5.8037e-01
Epoch 7/10
87/87 - 19s - loss: 326.9713 - loglik: -3.2543e+02 - logprior: -5.5519e-01
Epoch 8/10
87/87 - 19s - loss: 326.2253 - loglik: -3.2467e+02 - logprior: -5.3854e-01
Epoch 9/10
87/87 - 19s - loss: 325.6510 - loglik: -3.2403e+02 - logprior: -5.1478e-01
Epoch 10/10
87/87 - 19s - loss: 325.5178 - loglik: -3.2395e+02 - logprior: -4.9444e-01
Fitted a model with MAP estimate = -323.9064
Time for alignment: 484.1515
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.6765 - loglik: -4.4432e+02 - logprior: -1.0866e+00
Epoch 2/10
30/30 - 8s - loss: 373.0284 - loglik: -3.7084e+02 - logprior: -1.2134e+00
Epoch 3/10
30/30 - 8s - loss: 361.6348 - loglik: -3.5938e+02 - logprior: -1.2184e+00
Epoch 4/10
30/30 - 8s - loss: 359.3320 - loglik: -3.5718e+02 - logprior: -1.2042e+00
Epoch 5/10
30/30 - 8s - loss: 358.8050 - loglik: -3.5674e+02 - logprior: -1.1760e+00
Epoch 6/10
30/30 - 8s - loss: 357.9031 - loglik: -3.5594e+02 - logprior: -1.1639e+00
Epoch 7/10
30/30 - 8s - loss: 357.5247 - loglik: -3.5558e+02 - logprior: -1.1530e+00
Epoch 8/10
30/30 - 8s - loss: 357.4049 - loglik: -3.5554e+02 - logprior: -1.1461e+00
Epoch 9/10
30/30 - 8s - loss: 356.7606 - loglik: -3.5493e+02 - logprior: -1.1459e+00
Epoch 10/10
30/30 - 8s - loss: 356.9221 - loglik: -3.5510e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -348.7594
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (51, 1), (52, 1), (54, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 353.5772 - loglik: -3.5182e+02 - logprior: -1.2935e+00
Epoch 2/2
61/61 - 15s - loss: 342.7435 - loglik: -3.4039e+02 - logprior: -1.0221e+00
Fitted a model with MAP estimate = -330.2654
expansions: []
discards: [ 18  30  48  52  93  95 104 134 148 152 156]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 347.0959 - loglik: -3.4558e+02 - logprior: -1.1291e+00
Epoch 2/2
61/61 - 14s - loss: 342.9398 - loglik: -3.4087e+02 - logprior: -8.8123e-01
Fitted a model with MAP estimate = -330.2868
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 332.4458 - loglik: -3.3127e+02 - logprior: -7.6014e-01
Epoch 2/10
87/87 - 19s - loss: 329.6876 - loglik: -3.2803e+02 - logprior: -6.6582e-01
Epoch 3/10
87/87 - 19s - loss: 327.7912 - loglik: -3.2595e+02 - logprior: -6.4515e-01
Epoch 4/10
87/87 - 19s - loss: 328.0884 - loglik: -3.2644e+02 - logprior: -6.2201e-01
Fitted a model with MAP estimate = -326.2578
Time for alignment: 392.1237
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.4083 - loglik: -4.4406e+02 - logprior: -1.0869e+00
Epoch 2/10
30/30 - 8s - loss: 373.4511 - loglik: -3.7088e+02 - logprior: -1.2050e+00
Epoch 3/10
30/30 - 8s - loss: 361.3784 - loglik: -3.5894e+02 - logprior: -1.2034e+00
Epoch 4/10
30/30 - 8s - loss: 358.2259 - loglik: -3.5600e+02 - logprior: -1.2173e+00
Epoch 5/10
30/30 - 8s - loss: 357.3334 - loglik: -3.5530e+02 - logprior: -1.1948e+00
Epoch 6/10
30/30 - 8s - loss: 356.8375 - loglik: -3.5492e+02 - logprior: -1.1787e+00
Epoch 7/10
30/30 - 9s - loss: 356.2284 - loglik: -3.5432e+02 - logprior: -1.1709e+00
Epoch 8/10
30/30 - 8s - loss: 356.0817 - loglik: -3.5424e+02 - logprior: -1.1668e+00
Epoch 9/10
30/30 - 8s - loss: 355.5757 - loglik: -3.5378e+02 - logprior: -1.1624e+00
Epoch 10/10
30/30 - 8s - loss: 355.8757 - loglik: -3.5410e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -347.8176
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (51, 1), (52, 1), (54, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 353.5371 - loglik: -3.5189e+02 - logprior: -1.2879e+00
Epoch 2/2
61/61 - 14s - loss: 342.9345 - loglik: -3.4119e+02 - logprior: -9.9505e-01
Fitted a model with MAP estimate = -330.4154
expansions: []
discards: [ 18  30  48  52  93  95 133 148 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 346.9068 - loglik: -3.4548e+02 - logprior: -1.1409e+00
Epoch 2/2
61/61 - 14s - loss: 342.6609 - loglik: -3.4109e+02 - logprior: -9.0156e-01
Fitted a model with MAP estimate = -330.5712
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 332.0115 - loglik: -3.3089e+02 - logprior: -7.6395e-01
Epoch 2/10
87/87 - 19s - loss: 329.2665 - loglik: -3.2757e+02 - logprior: -6.7215e-01
Epoch 3/10
87/87 - 19s - loss: 328.7818 - loglik: -3.2691e+02 - logprior: -6.5112e-01
Epoch 4/10
87/87 - 19s - loss: 327.7115 - loglik: -3.2606e+02 - logprior: -6.2288e-01
Epoch 5/10
87/87 - 19s - loss: 326.9630 - loglik: -3.2537e+02 - logprior: -6.0397e-01
Epoch 6/10
87/87 - 19s - loss: 327.6519 - loglik: -3.2612e+02 - logprior: -5.7163e-01
Fitted a model with MAP estimate = -325.5273
Time for alignment: 427.2677
Computed alignments with likelihoods: ['-323.9064', '-326.2578', '-325.5273']
Best model has likelihood: -323.9064
time for generating output: 0.3351
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6608710051333002
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4c6b90250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb606267b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb3441f2f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e1682520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54fd97760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb57a2e00a0>, <__main__.SimpleDirichletPrior object at 0x7fb4fb247070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1230.8440 - loglik: -1.2284e+03 - logprior: -1.9370e+00
Epoch 2/10
40/40 - 45s - loss: 1099.3298 - loglik: -1.0938e+03 - logprior: -3.0863e+00
Epoch 3/10
40/40 - 44s - loss: 1075.9399 - loglik: -1.0682e+03 - logprior: -3.2367e+00
Epoch 4/10
40/40 - 45s - loss: 1061.1667 - loglik: -1.0523e+03 - logprior: -3.3234e+00
Epoch 5/10
40/40 - 45s - loss: 1055.0419 - loglik: -1.0460e+03 - logprior: -3.4476e+00
Epoch 6/10
40/40 - 45s - loss: 1049.3271 - loglik: -1.0408e+03 - logprior: -3.5351e+00
Epoch 7/10
40/40 - 45s - loss: 1047.8069 - loglik: -1.0399e+03 - logprior: -3.5607e+00
Epoch 8/10
40/40 - 45s - loss: 1044.5564 - loglik: -1.0371e+03 - logprior: -3.5476e+00
Epoch 9/10
40/40 - 45s - loss: 1042.2955 - loglik: -1.0348e+03 - logprior: -3.6400e+00
Epoch 10/10
40/40 - 45s - loss: 1039.0194 - loglik: -1.0313e+03 - logprior: -3.7105e+00
Fitted a model with MAP estimate = -811.2267
expansions: [(122, 1), (125, 1), (169, 2), (173, 1), (202, 1), (229, 2), (330, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 123 159 160 161 162 163 164 179 180 181 182 183 184 185 186
 187 188 232 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285
 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303
 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321
 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 23s - loss: 1336.0424 - loglik: -1.3321e+03 - logprior: -3.3727e+00
Epoch 2/2
40/40 - 17s - loss: 1254.9054 - loglik: -1.2510e+03 - logprior: -1.2051e+00
Fitted a model with MAP estimate = -903.5798
expansions: [(0, 317), (14, 2), (18, 2), (19, 2), (20, 5), (21, 5), (49, 1), (52, 24), (80, 1)]
discards: [ 11  12  15  55  56  57  60  61  62  63  64  81 114 116]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 84s - loss: 1087.3895 - loglik: -1.0843e+03 - logprior: -1.8040e+00
Epoch 2/2
80/80 - 80s - loss: 1011.4241 - loglik: -1.0065e+03 - logprior: -1.0331e+00
Fitted a model with MAP estimate = -748.9263
expansions: [(256, 1), (265, 1), (289, 1), (294, 1), (301, 1), (307, 1)]
discards: [  0  21  22  89  90 134 195 197 330 336 340 346 392 393 394]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 114s - loss: 769.1995 - loglik: -7.6625e+02 - logprior: -9.6653e-01
Epoch 2/10
113/113 - 110s - loss: 745.5884 - loglik: -7.3976e+02 - logprior: -8.7139e-01
Epoch 3/10
113/113 - 110s - loss: 742.6645 - loglik: -7.3755e+02 - logprior: -6.8313e-01
Epoch 4/10
113/113 - 109s - loss: 743.3090 - loglik: -7.3849e+02 - logprior: -5.0405e-01
Fitted a model with MAP estimate = -734.4233
Time for alignment: 1443.2612
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1229.9143 - loglik: -1.2276e+03 - logprior: -1.7999e+00
Epoch 2/10
40/40 - 45s - loss: 1099.1263 - loglik: -1.0941e+03 - logprior: -2.7604e+00
Epoch 3/10
40/40 - 45s - loss: 1077.7064 - loglik: -1.0709e+03 - logprior: -3.0076e+00
Epoch 4/10
40/40 - 44s - loss: 1062.8956 - loglik: -1.0545e+03 - logprior: -3.3943e+00
Epoch 5/10
40/40 - 45s - loss: 1056.7307 - loglik: -1.0480e+03 - logprior: -3.4976e+00
Epoch 6/10
40/40 - 45s - loss: 1052.3257 - loglik: -1.0440e+03 - logprior: -3.6225e+00
Epoch 7/10
40/40 - 45s - loss: 1048.7742 - loglik: -1.0411e+03 - logprior: -3.6381e+00
Epoch 8/10
40/40 - 45s - loss: 1047.2924 - loglik: -1.0400e+03 - logprior: -3.6800e+00
Epoch 9/10
40/40 - 45s - loss: 1044.3751 - loglik: -1.0372e+03 - logprior: -3.7030e+00
Epoch 10/10
40/40 - 45s - loss: 1042.6350 - loglik: -1.0351e+03 - logprior: -3.7556e+00
Fitted a model with MAP estimate = -810.6015
expansions: [(118, 1), (125, 1), (170, 9), (171, 1), (330, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 109 110 123 183
 184 185 186 187 211 212 213 214 215 216 217 218 243 248 249 250 251 252
 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270
 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288
 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306
 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324
 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 22s - loss: 1309.4840 - loglik: -1.3050e+03 - logprior: -3.9977e+00
Epoch 2/2
40/40 - 18s - loss: 1234.0100 - loglik: -1.2294e+03 - logprior: -1.7674e+00
Fitted a model with MAP estimate = -890.6979
expansions: [(0, 312), (5, 1), (20, 1), (23, 2), (24, 9), (69, 2), (70, 1), (87, 1), (88, 4), (135, 1)]
discards: [  0  18  65  74  75  76  77  78  79  80  81  82 117 118 119 122]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 83s - loss: 1082.1465 - loglik: -1.0797e+03 - logprior: -1.4940e+00
Epoch 2/2
80/80 - 80s - loss: 1011.8883 - loglik: -1.0077e+03 - logprior: -9.9490e-01
Fitted a model with MAP estimate = -750.0569
expansions: [(256, 1), (265, 1), (281, 1), (288, 1), (301, 1), (304, 2), (440, 1)]
discards: [  0  21  22  35  68  69  70 106 107 202 203 205 336]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 113s - loss: 766.0783 - loglik: -7.6321e+02 - logprior: -1.0298e+00
Epoch 2/10
113/113 - 109s - loss: 745.8619 - loglik: -7.4036e+02 - logprior: -6.9055e-01
Epoch 3/10
113/113 - 109s - loss: 744.2928 - loglik: -7.3910e+02 - logprior: -6.8401e-01
Epoch 4/10
113/113 - 109s - loss: 741.9058 - loglik: -7.3710e+02 - logprior: -5.0563e-01
Epoch 5/10
113/113 - 109s - loss: 739.4805 - loglik: -7.3498e+02 - logprior: -4.6085e-01
Epoch 6/10
113/113 - 110s - loss: 736.7342 - loglik: -7.3240e+02 - logprior: -2.9281e-01
Epoch 7/10
113/113 - 109s - loss: 730.5196 - loglik: -7.2704e+02 - logprior: -1.2682e-01
Epoch 8/10
113/113 - 109s - loss: 732.9564 - loglik: -7.3003e+02 - logprior: 0.0102
Fitted a model with MAP estimate = -725.5159
Time for alignment: 1883.5930
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1242.4031 - loglik: -1.2406e+03 - logprior: -1.2466e+00
Epoch 2/10
40/40 - 45s - loss: 1130.2644 - loglik: -1.1274e+03 - logprior: -3.3131e-01
Epoch 3/10
40/40 - 45s - loss: 1113.3020 - loglik: -1.1084e+03 - logprior: -4.0756e-01
Epoch 4/10
40/40 - 45s - loss: 1105.9912 - loglik: -1.1006e+03 - logprior: -4.7452e-01
Epoch 5/10
40/40 - 45s - loss: 1099.1350 - loglik: -1.0942e+03 - logprior: -6.5554e-01
Epoch 6/10
40/40 - 45s - loss: 1095.6104 - loglik: -1.0912e+03 - logprior: -7.2968e-01
Epoch 7/10
40/40 - 45s - loss: 1094.1117 - loglik: -1.0903e+03 - logprior: -7.6811e-01
Epoch 8/10
40/40 - 45s - loss: 1092.0255 - loglik: -1.0887e+03 - logprior: -7.9288e-01
Epoch 9/10
40/40 - 45s - loss: 1090.7410 - loglik: -1.0876e+03 - logprior: -8.2440e-01
Epoch 10/10
40/40 - 45s - loss: 1089.8455 - loglik: -1.0868e+03 - logprior: -8.4046e-01
Fitted a model with MAP estimate = -833.2240
expansions: [(18, 2), (122, 1), (125, 1), (126, 1), (194, 1), (297, 7), (298, 5), (299, 2), (300, 4), (314, 1), (317, 1), (318, 2), (319, 5), (327, 1), (329, 8), (330, 88)]
discards: [  1 217 218 219]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 84s - loss: 1105.2034 - loglik: -1.1024e+03 - logprior: -2.0046e+00
Epoch 2/2
80/80 - 79s - loss: 1015.1740 - loglik: -1.0118e+03 - logprior: -1.2358e+00
Fitted a model with MAP estimate = -753.8571
expansions: [(161, 1), (165, 1), (222, 1), (358, 1), (385, 7), (387, 1), (450, 1)]
discards: [ 18 163 225 226 241 307 314 316 335 343 415 416 417 418 419 420 421 433
 455]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 81s - loss: 1041.5959 - loglik: -1.0400e+03 - logprior: -8.1600e-01
Epoch 2/2
80/80 - 78s - loss: 1008.1611 - loglik: -1.0048e+03 - logprior: -1.6591e-01
Fitted a model with MAP estimate = -749.9586
expansions: [(450, 2)]
discards: [449]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 113s - loss: 769.0234 - loglik: -7.6730e+02 - logprior: -3.9977e-01
Epoch 2/10
113/113 - 109s - loss: 742.3907 - loglik: -7.3825e+02 - logprior: -2.3145e-01
Epoch 3/10
113/113 - 109s - loss: 744.8923 - loglik: -7.4057e+02 - logprior: -6.4264e-02
Fitted a model with MAP estimate = -736.6808
Time for alignment: 1565.2588
Computed alignments with likelihoods: ['-734.4233', '-725.5159', '-736.6808']
Best model has likelihood: -725.5159
time for generating output: 0.5183
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.6906025087394613
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb69fcda670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6973ccd60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54ffd43a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c76ef580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb630ba24c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb603f8dbe0>, <__main__.SimpleDirichletPrior object at 0x7fb2dc0845e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.2551 - loglik: -4.4626e+02 - logprior: -2.9816e+00
Epoch 2/10
19/19 - 5s - loss: 280.1075 - loglik: -2.7842e+02 - logprior: -1.4580e+00
Epoch 3/10
19/19 - 5s - loss: 210.9574 - loglik: -2.0896e+02 - logprior: -1.8817e+00
Epoch 4/10
19/19 - 5s - loss: 200.5303 - loglik: -1.9819e+02 - logprior: -2.1686e+00
Epoch 5/10
19/19 - 5s - loss: 194.5684 - loglik: -1.9195e+02 - logprior: -2.3133e+00
Epoch 6/10
19/19 - 5s - loss: 192.5757 - loglik: -1.8972e+02 - logprior: -2.4904e+00
Epoch 7/10
19/19 - 5s - loss: 190.5943 - loglik: -1.8780e+02 - logprior: -2.4127e+00
Epoch 8/10
19/19 - 5s - loss: 190.0676 - loglik: -1.8727e+02 - logprior: -2.4144e+00
Epoch 9/10
19/19 - 5s - loss: 190.6496 - loglik: -1.8786e+02 - logprior: -2.4100e+00
Fitted a model with MAP estimate = -184.1762
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (102, 1), (105, 1), (111, 2), (114, 1), (121, 2), (122, 2), (125, 1), (126, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 181.9132 - loglik: -1.7891e+02 - logprior: -2.9406e+00
Epoch 2/2
19/19 - 6s - loss: 144.1190 - loglik: -1.4281e+02 - logprior: -1.0995e+00
Fitted a model with MAP estimate = -142.8969
expansions: []
discards: [ 50  76 138 152 154]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 146.5109 - loglik: -1.4357e+02 - logprior: -2.8767e+00
Epoch 2/2
19/19 - 6s - loss: 141.3153 - loglik: -1.4015e+02 - logprior: -9.4826e-01
Fitted a model with MAP estimate = -141.2238
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 144.3365 - loglik: -1.4202e+02 - logprior: -2.2383e+00
Epoch 2/10
22/22 - 7s - loss: 140.5296 - loglik: -1.3915e+02 - logprior: -1.1467e+00
Epoch 3/10
22/22 - 7s - loss: 135.7211 - loglik: -1.3428e+02 - logprior: -1.1950e+00
Epoch 4/10
22/22 - 7s - loss: 133.8245 - loglik: -1.3231e+02 - logprior: -1.1643e+00
Epoch 5/10
22/22 - 7s - loss: 131.3613 - loglik: -1.2976e+02 - logprior: -1.1609e+00
Epoch 6/10
22/22 - 7s - loss: 128.8851 - loglik: -1.2731e+02 - logprior: -1.1175e+00
Epoch 7/10
22/22 - 7s - loss: 127.3545 - loglik: -1.2583e+02 - logprior: -1.0828e+00
Epoch 8/10
22/22 - 7s - loss: 126.4481 - loglik: -1.2498e+02 - logprior: -1.0487e+00
Epoch 9/10
22/22 - 7s - loss: 127.4332 - loglik: -1.2603e+02 - logprior: -1.0011e+00
Fitted a model with MAP estimate = -125.9951
Time for alignment: 192.7806
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 449.6551 - loglik: -4.4666e+02 - logprior: -2.9800e+00
Epoch 2/10
19/19 - 5s - loss: 282.0008 - loglik: -2.8033e+02 - logprior: -1.4284e+00
Epoch 3/10
19/19 - 5s - loss: 213.3592 - loglik: -2.1146e+02 - logprior: -1.8067e+00
Epoch 4/10
19/19 - 5s - loss: 200.8876 - loglik: -1.9861e+02 - logprior: -2.1357e+00
Epoch 5/10
19/19 - 5s - loss: 197.0871 - loglik: -1.9446e+02 - logprior: -2.2953e+00
Epoch 6/10
19/19 - 5s - loss: 193.9768 - loglik: -1.9116e+02 - logprior: -2.4299e+00
Epoch 7/10
19/19 - 5s - loss: 192.7817 - loglik: -1.9003e+02 - logprior: -2.3486e+00
Epoch 8/10
19/19 - 5s - loss: 190.9718 - loglik: -1.8824e+02 - logprior: -2.3440e+00
Epoch 9/10
19/19 - 5s - loss: 189.9556 - loglik: -1.8723e+02 - logprior: -2.3432e+00
Epoch 10/10
19/19 - 5s - loss: 192.1821 - loglik: -1.8947e+02 - logprior: -2.3379e+00
Fitted a model with MAP estimate = -185.4856
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (111, 2), (114, 1), (121, 1), (122, 3), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 180.2131 - loglik: -1.7722e+02 - logprior: -2.9213e+00
Epoch 2/2
19/19 - 6s - loss: 145.2755 - loglik: -1.4399e+02 - logprior: -1.0843e+00
Fitted a model with MAP estimate = -142.8717
expansions: []
discards: [ 50  76 138 154]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 146.3089 - loglik: -1.4347e+02 - logprior: -2.7706e+00
Epoch 2/2
19/19 - 6s - loss: 141.3593 - loglik: -1.4025e+02 - logprior: -8.8777e-01
Fitted a model with MAP estimate = -141.3584
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 12s - loss: 143.6322 - loglik: -1.4153e+02 - logprior: -2.0153e+00
Epoch 2/10
22/22 - 7s - loss: 141.0019 - loglik: -1.3988e+02 - logprior: -8.8276e-01
Epoch 3/10
22/22 - 7s - loss: 136.3177 - loglik: -1.3512e+02 - logprior: -9.3902e-01
Epoch 4/10
22/22 - 7s - loss: 134.2261 - loglik: -1.3265e+02 - logprior: -1.2133e+00
Epoch 5/10
22/22 - 7s - loss: 130.2740 - loglik: -1.2867e+02 - logprior: -1.1451e+00
Epoch 6/10
22/22 - 7s - loss: 128.7909 - loglik: -1.2723e+02 - logprior: -1.0959e+00
Epoch 7/10
22/22 - 7s - loss: 127.3254 - loglik: -1.2580e+02 - logprior: -1.0780e+00
Epoch 8/10
22/22 - 7s - loss: 127.5298 - loglik: -1.2607e+02 - logprior: -1.0377e+00
Fitted a model with MAP estimate = -126.2188
Time for alignment: 191.0702
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.7862 - loglik: -4.4678e+02 - logprior: -2.9875e+00
Epoch 2/10
19/19 - 5s - loss: 286.6379 - loglik: -2.8497e+02 - logprior: -1.4338e+00
Epoch 3/10
19/19 - 5s - loss: 214.9385 - loglik: -2.1296e+02 - logprior: -1.8625e+00
Epoch 4/10
19/19 - 5s - loss: 202.3627 - loglik: -1.9998e+02 - logprior: -2.1341e+00
Epoch 5/10
19/19 - 5s - loss: 196.7550 - loglik: -1.9401e+02 - logprior: -2.3017e+00
Epoch 6/10
19/19 - 5s - loss: 193.2224 - loglik: -1.9020e+02 - logprior: -2.5224e+00
Epoch 7/10
19/19 - 5s - loss: 192.2502 - loglik: -1.8932e+02 - logprior: -2.4350e+00
Epoch 8/10
19/19 - 5s - loss: 192.3148 - loglik: -1.8943e+02 - logprior: -2.4234e+00
Fitted a model with MAP estimate = -185.8898
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (81, 1), (91, 1), (92, 1), (99, 1), (101, 1), (104, 1), (114, 1), (120, 2), (121, 1), (122, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 180.8838 - loglik: -1.7791e+02 - logprior: -2.9233e+00
Epoch 2/2
19/19 - 6s - loss: 145.0869 - loglik: -1.4389e+02 - logprior: -1.0256e+00
Fitted a model with MAP estimate = -142.8867
expansions: []
discards: [ 75 148]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 146.5556 - loglik: -1.4368e+02 - logprior: -2.8118e+00
Epoch 2/2
19/19 - 6s - loss: 141.6620 - loglik: -1.4057e+02 - logprior: -8.7679e-01
Fitted a model with MAP estimate = -141.1070
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 144.4823 - loglik: -1.4240e+02 - logprior: -1.9972e+00
Epoch 2/10
22/22 - 7s - loss: 140.0389 - loglik: -1.3894e+02 - logprior: -8.5472e-01
Epoch 3/10
22/22 - 7s - loss: 136.8004 - loglik: -1.3567e+02 - logprior: -8.9463e-01
Epoch 4/10
22/22 - 7s - loss: 133.8737 - loglik: -1.3236e+02 - logprior: -1.1708e+00
Epoch 5/10
22/22 - 7s - loss: 131.6353 - loglik: -1.3002e+02 - logprior: -1.1612e+00
Epoch 6/10
22/22 - 7s - loss: 127.7815 - loglik: -1.2620e+02 - logprior: -1.1179e+00
Epoch 7/10
22/22 - 7s - loss: 127.3783 - loglik: -1.2584e+02 - logprior: -1.0856e+00
Epoch 8/10
22/22 - 7s - loss: 127.4334 - loglik: -1.2597e+02 - logprior: -1.0455e+00
Fitted a model with MAP estimate = -126.1814
Time for alignment: 178.4068
Computed alignments with likelihoods: ['-125.9951', '-126.2188', '-126.1814']
Best model has likelihood: -125.9951
time for generating output: 0.2015
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb536394a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6756f9550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb50bc79be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb641957580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb579f95c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb69fe663d0>, <__main__.SimpleDirichletPrior object at 0x7fb604326670>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 609.4462 - loglik: -5.7618e+02 - logprior: -3.3244e+01
Epoch 2/10
12/12 - 4s - loss: 525.1897 - loglik: -5.2066e+02 - logprior: -4.5004e+00
Epoch 3/10
12/12 - 4s - loss: 467.9882 - loglik: -4.6608e+02 - logprior: -1.8008e+00
Epoch 4/10
12/12 - 4s - loss: 439.8205 - loglik: -4.3839e+02 - logprior: -1.1050e+00
Epoch 5/10
12/12 - 4s - loss: 430.3372 - loglik: -4.2889e+02 - logprior: -9.0444e-01
Epoch 6/10
12/12 - 4s - loss: 426.0870 - loglik: -4.2491e+02 - logprior: -6.3289e-01
Epoch 7/10
12/12 - 4s - loss: 422.8201 - loglik: -4.2171e+02 - logprior: -5.9300e-01
Epoch 8/10
12/12 - 4s - loss: 422.3142 - loglik: -4.2128e+02 - logprior: -5.5452e-01
Epoch 9/10
12/12 - 4s - loss: 420.8498 - loglik: -4.1996e+02 - logprior: -4.2865e-01
Epoch 10/10
12/12 - 4s - loss: 420.2226 - loglik: -4.1945e+02 - logprior: -3.2926e-01
Fitted a model with MAP estimate = -419.6407
expansions: [(11, 3), (19, 1), (32, 2), (33, 1), (42, 1), (44, 1), (45, 1), (60, 1), (63, 1), (64, 1), (75, 2), (77, 2), (78, 3), (86, 1), (90, 3), (92, 1), (102, 1), (107, 2), (126, 5), (127, 1), (129, 1), (130, 1), (138, 3), (145, 1), (146, 2), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 458.8153 - loglik: -4.2004e+02 - logprior: -3.8748e+01
Epoch 2/2
12/12 - 4s - loss: 418.1397 - loglik: -4.0511e+02 - logprior: -1.2819e+01
Fitted a model with MAP estimate = -411.2324
expansions: [(0, 4)]
discards: [  0  92 132 155 156 157 174 186 190 219]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 434.6537 - loglik: -4.0595e+02 - logprior: -2.8664e+01
Epoch 2/2
12/12 - 4s - loss: 405.6391 - loglik: -4.0281e+02 - logprior: -2.6243e+00
Fitted a model with MAP estimate = -399.8926
expansions: []
discards: [ 1  2  3 91]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 430.9608 - loglik: -4.0313e+02 - logprior: -2.7799e+01
Epoch 2/10
12/12 - 4s - loss: 403.5029 - loglik: -4.0118e+02 - logprior: -2.1157e+00
Epoch 3/10
12/12 - 4s - loss: 398.2866 - loglik: -4.0041e+02 - logprior: 2.5290
Epoch 4/10
12/12 - 4s - loss: 394.1917 - loglik: -3.9820e+02 - logprior: 4.5223
Epoch 5/10
12/12 - 4s - loss: 394.6469 - loglik: -3.9971e+02 - logprior: 5.5741
Fitted a model with MAP estimate = -392.2634
Time for alignment: 103.9289
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 608.0385 - loglik: -5.7478e+02 - logprior: -3.3236e+01
Epoch 2/10
12/12 - 4s - loss: 525.4265 - loglik: -5.2090e+02 - logprior: -4.5005e+00
Epoch 3/10
12/12 - 4s - loss: 464.3941 - loglik: -4.6255e+02 - logprior: -1.7491e+00
Epoch 4/10
12/12 - 4s - loss: 433.4207 - loglik: -4.3192e+02 - logprior: -1.1892e+00
Epoch 5/10
12/12 - 4s - loss: 424.0248 - loglik: -4.2257e+02 - logprior: -9.2050e-01
Epoch 6/10
12/12 - 4s - loss: 424.3730 - loglik: -4.2327e+02 - logprior: -5.6085e-01
Fitted a model with MAP estimate = -420.6731
expansions: [(11, 3), (19, 1), (24, 1), (30, 4), (31, 2), (40, 1), (47, 1), (59, 1), (60, 1), (61, 1), (62, 1), (76, 2), (77, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (131, 5), (138, 1), (139, 1), (146, 3), (148, 1), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 459.0935 - loglik: -4.2003e+02 - logprior: -3.9042e+01
Epoch 2/2
12/12 - 4s - loss: 420.1606 - loglik: -4.0712e+02 - logprior: -1.2899e+01
Fitted a model with MAP estimate = -412.4326
expansions: [(0, 4)]
discards: [  0  36  37  39  93 157 212]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 434.3958 - loglik: -4.0543e+02 - logprior: -2.8934e+01
Epoch 2/2
12/12 - 4s - loss: 406.2625 - loglik: -4.0305e+02 - logprior: -3.0121e+00
Fitted a model with MAP estimate = -400.5212
expansions: [(105, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 431.2668 - loglik: -4.0316e+02 - logprior: -2.8074e+01
Epoch 2/10
12/12 - 4s - loss: 403.4785 - loglik: -4.0087e+02 - logprior: -2.4086e+00
Epoch 3/10
12/12 - 4s - loss: 398.6583 - loglik: -4.0045e+02 - logprior: 2.1897
Epoch 4/10
12/12 - 4s - loss: 396.6267 - loglik: -4.0029e+02 - logprior: 4.1694
Epoch 5/10
12/12 - 4s - loss: 393.8907 - loglik: -3.9857e+02 - logprior: 5.1914
Epoch 6/10
12/12 - 4s - loss: 391.8933 - loglik: -3.9718e+02 - logprior: 5.7684
Epoch 7/10
12/12 - 4s - loss: 392.6261 - loglik: -3.9840e+02 - logprior: 6.2173
Fitted a model with MAP estimate = -391.3868
Time for alignment: 97.3796
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 608.1550 - loglik: -5.7490e+02 - logprior: -3.3240e+01
Epoch 2/10
12/12 - 4s - loss: 525.1684 - loglik: -5.2063e+02 - logprior: -4.5065e+00
Epoch 3/10
12/12 - 4s - loss: 461.4102 - loglik: -4.5936e+02 - logprior: -1.8987e+00
Epoch 4/10
12/12 - 4s - loss: 434.3473 - loglik: -4.3231e+02 - logprior: -1.4819e+00
Epoch 5/10
12/12 - 4s - loss: 428.5472 - loglik: -4.2683e+02 - logprior: -1.0316e+00
Epoch 6/10
12/12 - 4s - loss: 426.5306 - loglik: -4.2523e+02 - logprior: -7.7266e-01
Epoch 7/10
12/12 - 4s - loss: 423.1760 - loglik: -4.2201e+02 - logprior: -7.2372e-01
Epoch 8/10
12/12 - 4s - loss: 422.4781 - loglik: -4.2139e+02 - logprior: -6.4431e-01
Epoch 9/10
12/12 - 4s - loss: 421.3517 - loglik: -4.2030e+02 - logprior: -5.7161e-01
Epoch 10/10
12/12 - 4s - loss: 422.3281 - loglik: -4.2137e+02 - logprior: -4.8816e-01
Fitted a model with MAP estimate = -419.9490
expansions: [(11, 3), (21, 1), (30, 1), (31, 3), (32, 1), (41, 1), (45, 1), (60, 2), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 2), (78, 2), (86, 1), (87, 1), (90, 1), (91, 1), (92, 1), (100, 1), (101, 1), (120, 2), (126, 5), (127, 1), (129, 2), (130, 1), (138, 1), (140, 1), (146, 3), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 459.2354 - loglik: -4.2050e+02 - logprior: -3.8705e+01
Epoch 2/2
12/12 - 4s - loss: 416.9073 - loglik: -4.0393e+02 - logprior: -1.2778e+01
Fitted a model with MAP estimate = -409.7741
expansions: [(0, 4)]
discards: [  0  34  71  96 131 150 159 167 170 223]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 434.5059 - loglik: -4.0551e+02 - logprior: -2.8962e+01
Epoch 2/2
12/12 - 4s - loss: 404.1833 - loglik: -4.0100e+02 - logprior: -2.9700e+00
Fitted a model with MAP estimate = -399.2307
expansions: []
discards: [  1   2   3 155 156 188]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 432.0800 - loglik: -4.0411e+02 - logprior: -2.7937e+01
Epoch 2/10
12/12 - 4s - loss: 403.0134 - loglik: -4.0051e+02 - logprior: -2.2978e+00
Epoch 3/10
12/12 - 4s - loss: 397.1923 - loglik: -3.9918e+02 - logprior: 2.3979
Epoch 4/10
12/12 - 4s - loss: 392.0983 - loglik: -3.9595e+02 - logprior: 4.3794
Epoch 5/10
12/12 - 4s - loss: 393.5252 - loglik: -3.9839e+02 - logprior: 5.4154
Fitted a model with MAP estimate = -390.9607
Time for alignment: 101.7611
Computed alignments with likelihoods: ['-392.2634', '-391.3868', '-390.9607']
Best model has likelihood: -390.9607
time for generating output: 0.2594
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9425717004213674
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb64af264f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb579f45eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb3441f23a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5254e2040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb50bb88af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4f275a370>, <__main__.SimpleDirichletPrior object at 0x7fb52deb6850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 605.6263 - loglik: -5.9796e+02 - logprior: -7.6417e+00
Epoch 2/10
21/21 - 8s - loss: 483.7375 - loglik: -4.8156e+02 - logprior: -2.1277e+00
Epoch 3/10
21/21 - 9s - loss: 444.5938 - loglik: -4.4119e+02 - logprior: -2.8623e+00
Epoch 4/10
21/21 - 9s - loss: 436.7345 - loglik: -4.3345e+02 - logprior: -2.7182e+00
Epoch 5/10
21/21 - 8s - loss: 433.8718 - loglik: -4.3068e+02 - logprior: -2.6993e+00
Epoch 6/10
21/21 - 9s - loss: 434.9514 - loglik: -4.3173e+02 - logprior: -2.7205e+00
Fitted a model with MAP estimate = -432.4256
expansions: [(14, 1), (17, 2), (20, 1), (21, 2), (22, 5), (38, 2), (39, 1), (40, 2), (46, 1), (50, 1), (64, 1), (65, 1), (66, 2), (76, 8), (78, 1), (80, 1), (96, 1), (99, 1), (115, 1), (116, 1), (119, 1), (121, 4), (145, 1), (146, 1), (147, 1), (149, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 3), (171, 1), (178, 1), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 17s - loss: 430.6896 - loglik: -4.2009e+02 - logprior: -1.0503e+01
Epoch 2/2
21/21 - 11s - loss: 403.8146 - loglik: -3.9959e+02 - logprior: -3.8535e+00
Fitted a model with MAP estimate = -397.4595
expansions: [(0, 5)]
discards: [  0  13  17  29  97  98  99 158 220]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 410.2193 - loglik: -4.0297e+02 - logprior: -7.1572e+00
Epoch 2/2
21/21 - 10s - loss: 396.7944 - loglik: -3.9574e+02 - logprior: -7.1133e-01
Fitted a model with MAP estimate = -394.1167
expansions: []
discards: [ 1  2  3  4 53 87]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 407.3483 - loglik: -4.0065e+02 - logprior: -6.6005e+00
Epoch 2/10
21/21 - 10s - loss: 397.4540 - loglik: -3.9701e+02 - logprior: -3.7888e-02
Epoch 3/10
21/21 - 10s - loss: 393.2716 - loglik: -3.9342e+02 - logprior: 0.6711
Epoch 4/10
21/21 - 11s - loss: 393.2034 - loglik: -3.9371e+02 - logprior: 1.0082
Epoch 5/10
21/21 - 10s - loss: 391.9274 - loglik: -3.9256e+02 - logprior: 1.1301
Epoch 6/10
21/21 - 10s - loss: 390.1383 - loglik: -3.9084e+02 - logprior: 1.2080
Epoch 7/10
21/21 - 10s - loss: 389.1104 - loglik: -3.8987e+02 - logprior: 1.2970
Epoch 8/10
21/21 - 10s - loss: 389.4545 - loglik: -3.9040e+02 - logprior: 1.4886
Fitted a model with MAP estimate = -388.0303
Time for alignment: 225.6469
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 605.2192 - loglik: -5.9753e+02 - logprior: -7.6624e+00
Epoch 2/10
21/21 - 9s - loss: 478.5246 - loglik: -4.7626e+02 - logprior: -2.2358e+00
Epoch 3/10
21/21 - 9s - loss: 438.5745 - loglik: -4.3508e+02 - logprior: -3.0991e+00
Epoch 4/10
21/21 - 8s - loss: 430.8507 - loglik: -4.2731e+02 - logprior: -3.0051e+00
Epoch 5/10
21/21 - 8s - loss: 429.8445 - loglik: -4.2642e+02 - logprior: -2.9010e+00
Epoch 6/10
21/21 - 9s - loss: 426.1370 - loglik: -4.2273e+02 - logprior: -2.8769e+00
Epoch 7/10
21/21 - 8s - loss: 426.9128 - loglik: -4.2350e+02 - logprior: -2.8724e+00
Fitted a model with MAP estimate = -426.0303
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (75, 9), (97, 1), (98, 1), (99, 1), (119, 2), (121, 1), (122, 1), (144, 1), (146, 2), (147, 1), (149, 1), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (177, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 429.6781 - loglik: -4.1897e+02 - logprior: -1.0616e+01
Epoch 2/2
21/21 - 11s - loss: 403.9290 - loglik: -3.9956e+02 - logprior: -4.0366e+00
Fitted a model with MAP estimate = -398.5760
expansions: [(0, 5)]
discards: [  0  13  18  28  50  52  83 153 220]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 16s - loss: 410.6077 - loglik: -4.0324e+02 - logprior: -7.2809e+00
Epoch 2/2
21/21 - 10s - loss: 397.2155 - loglik: -3.9601e+02 - logprior: -8.4905e-01
Fitted a model with MAP estimate = -394.2406
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 407.5134 - loglik: -4.0065e+02 - logprior: -6.7683e+00
Epoch 2/10
21/21 - 10s - loss: 395.7649 - loglik: -3.9520e+02 - logprior: -1.6595e-01
Epoch 3/10
21/21 - 10s - loss: 395.1682 - loglik: -3.9508e+02 - logprior: 0.5126
Epoch 4/10
21/21 - 10s - loss: 392.6819 - loglik: -3.9293e+02 - logprior: 0.8484
Epoch 5/10
21/21 - 10s - loss: 391.1240 - loglik: -3.9154e+02 - logprior: 0.9677
Epoch 6/10
21/21 - 10s - loss: 390.1526 - loglik: -3.9069e+02 - logprior: 1.0879
Epoch 7/10
21/21 - 10s - loss: 389.2148 - loglik: -3.8981e+02 - logprior: 1.1596
Epoch 8/10
21/21 - 11s - loss: 389.5911 - loglik: -3.9037e+02 - logprior: 1.3546
Fitted a model with MAP estimate = -387.8196
Time for alignment: 231.9512
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 605.4451 - loglik: -5.9774e+02 - logprior: -7.6750e+00
Epoch 2/10
21/21 - 8s - loss: 475.9903 - loglik: -4.7373e+02 - logprior: -2.2033e+00
Epoch 3/10
21/21 - 9s - loss: 436.0164 - loglik: -4.3248e+02 - logprior: -3.0109e+00
Epoch 4/10
21/21 - 9s - loss: 426.4031 - loglik: -4.2289e+02 - logprior: -2.9670e+00
Epoch 5/10
21/21 - 8s - loss: 424.9595 - loglik: -4.2156e+02 - logprior: -2.8991e+00
Epoch 6/10
21/21 - 8s - loss: 422.8055 - loglik: -4.1936e+02 - logprior: -2.9298e+00
Epoch 7/10
21/21 - 8s - loss: 422.7011 - loglik: -4.1923e+02 - logprior: -2.9429e+00
Epoch 8/10
21/21 - 9s - loss: 422.3163 - loglik: -4.1883e+02 - logprior: -2.9486e+00
Epoch 9/10
21/21 - 9s - loss: 421.6609 - loglik: -4.1815e+02 - logprior: -2.9642e+00
Epoch 10/10
21/21 - 8s - loss: 421.3998 - loglik: -4.1788e+02 - logprior: -2.9687e+00
Fitted a model with MAP estimate = -420.8158
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (38, 2), (39, 2), (40, 1), (50, 1), (64, 4), (66, 1), (76, 7), (78, 1), (97, 1), (98, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (147, 1), (150, 1), (158, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 423.8513 - loglik: -4.1330e+02 - logprior: -1.0462e+01
Epoch 2/2
21/21 - 11s - loss: 394.6547 - loglik: -3.9027e+02 - logprior: -4.0299e+00
Fitted a model with MAP estimate = -391.1551
expansions: [(0, 5), (101, 2)]
discards: [  0  18  28  49  83 152 218]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 401.6931 - loglik: -3.9455e+02 - logprior: -7.0518e+00
Epoch 2/2
21/21 - 11s - loss: 386.8054 - loglik: -3.8587e+02 - logprior: -5.6867e-01
Fitted a model with MAP estimate = -384.4350
expansions: []
discards: [  1   2   3   4 101]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 396.1474 - loglik: -3.8956e+02 - logprior: -6.4911e+00
Epoch 2/10
21/21 - 10s - loss: 387.7662 - loglik: -3.8740e+02 - logprior: 0.0746
Epoch 3/10
21/21 - 10s - loss: 383.7297 - loglik: -3.8383e+02 - logprior: 0.7614
Epoch 4/10
21/21 - 10s - loss: 381.7074 - loglik: -3.8215e+02 - logprior: 1.0773
Epoch 5/10
21/21 - 10s - loss: 380.3254 - loglik: -3.8097e+02 - logprior: 1.2333
Epoch 6/10
21/21 - 10s - loss: 379.7641 - loglik: -3.8048e+02 - logprior: 1.2943
Epoch 7/10
21/21 - 10s - loss: 378.8958 - loglik: -3.7968e+02 - logprior: 1.3812
Epoch 8/10
21/21 - 10s - loss: 377.2244 - loglik: -3.7814e+02 - logprior: 1.5237
Epoch 9/10
21/21 - 10s - loss: 378.4718 - loglik: -3.7959e+02 - logprior: 1.7358
Fitted a model with MAP estimate = -376.7471
Time for alignment: 266.5933
Computed alignments with likelihoods: ['-388.0303', '-387.8196', '-376.7471']
Best model has likelihood: -376.7471
time for generating output: 0.5057
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.6162681227640746
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7facf99ef160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c5368100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb653794fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad01466eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad01466340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4bd0faf70>, <__main__.SimpleDirichletPrior object at 0x7fb57a091fd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 283.3910 - loglik: -2.8022e+02 - logprior: -3.0558e+00
Epoch 2/10
19/19 - 3s - loss: 253.3380 - loglik: -2.5172e+02 - logprior: -9.4001e-01
Epoch 3/10
19/19 - 3s - loss: 242.5790 - loglik: -2.4045e+02 - logprior: -1.0020e+00
Epoch 4/10
19/19 - 3s - loss: 239.0252 - loglik: -2.3690e+02 - logprior: -9.3460e-01
Epoch 5/10
19/19 - 4s - loss: 237.1595 - loglik: -2.3520e+02 - logprior: -9.3845e-01
Epoch 6/10
19/19 - 3s - loss: 235.7059 - loglik: -2.3391e+02 - logprior: -9.3904e-01
Epoch 7/10
19/19 - 3s - loss: 235.2693 - loglik: -2.3353e+02 - logprior: -9.2960e-01
Epoch 8/10
19/19 - 4s - loss: 234.5175 - loglik: -2.3277e+02 - logprior: -9.2313e-01
Epoch 9/10
19/19 - 3s - loss: 233.6907 - loglik: -2.3191e+02 - logprior: -9.3579e-01
Epoch 10/10
19/19 - 3s - loss: 233.0376 - loglik: -2.3118e+02 - logprior: -9.5543e-01
Fitted a model with MAP estimate = -230.7498
expansions: [(0, 12), (12, 1), (14, 1), (15, 2), (36, 1), (54, 1), (56, 3), (57, 1), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 266.2861 - loglik: -2.6199e+02 - logprior: -4.2001e+00
Epoch 2/2
19/19 - 4s - loss: 241.8852 - loglik: -2.4019e+02 - logprior: -1.3057e+00
Fitted a model with MAP estimate = -236.4576
expansions: [(0, 10)]
discards: [10 11 12 13 14 15 16 17 18 19 20 21 22 23 75 77]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.8737 - loglik: -2.4091e+02 - logprior: -3.8987e+00
Epoch 2/2
19/19 - 3s - loss: 237.8654 - loglik: -2.3612e+02 - logprior: -1.3556e+00
Fitted a model with MAP estimate = -234.4774
expansions: [(0, 5), (25, 2), (26, 1)]
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 241.8082 - loglik: -2.3804e+02 - logprior: -3.7061e+00
Epoch 2/10
19/19 - 4s - loss: 236.0497 - loglik: -2.3425e+02 - logprior: -1.4023e+00
Epoch 3/10
19/19 - 4s - loss: 233.6617 - loglik: -2.3169e+02 - logprior: -1.0567e+00
Epoch 4/10
19/19 - 3s - loss: 230.9599 - loglik: -2.2885e+02 - logprior: -9.5696e-01
Epoch 5/10
19/19 - 4s - loss: 230.1127 - loglik: -2.2814e+02 - logprior: -9.2083e-01
Epoch 6/10
19/19 - 4s - loss: 229.2593 - loglik: -2.2742e+02 - logprior: -9.0212e-01
Epoch 7/10
19/19 - 3s - loss: 227.9807 - loglik: -2.2622e+02 - logprior: -8.9672e-01
Epoch 8/10
19/19 - 4s - loss: 228.5246 - loglik: -2.2682e+02 - logprior: -8.7886e-01
Fitted a model with MAP estimate = -226.6410
Time for alignment: 115.8800
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.0844 - loglik: -2.7991e+02 - logprior: -3.0597e+00
Epoch 2/10
19/19 - 3s - loss: 252.0252 - loglik: -2.5045e+02 - logprior: -9.2162e-01
Epoch 3/10
19/19 - 3s - loss: 242.1624 - loglik: -2.4032e+02 - logprior: -9.8262e-01
Epoch 4/10
19/19 - 3s - loss: 237.9855 - loglik: -2.3600e+02 - logprior: -9.4782e-01
Epoch 5/10
19/19 - 3s - loss: 236.4277 - loglik: -2.3455e+02 - logprior: -9.3466e-01
Epoch 6/10
19/19 - 3s - loss: 235.5671 - loglik: -2.3386e+02 - logprior: -9.1583e-01
Epoch 7/10
19/19 - 3s - loss: 235.1550 - loglik: -2.3351e+02 - logprior: -9.0840e-01
Epoch 8/10
19/19 - 3s - loss: 234.8834 - loglik: -2.3323e+02 - logprior: -9.0160e-01
Epoch 9/10
19/19 - 3s - loss: 233.7319 - loglik: -2.3201e+02 - logprior: -9.1002e-01
Epoch 10/10
19/19 - 3s - loss: 232.3356 - loglik: -2.3052e+02 - logprior: -9.2414e-01
Fitted a model with MAP estimate = -230.5872
expansions: [(0, 11), (11, 3), (12, 2), (13, 1), (14, 1), (21, 1), (57, 3), (59, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 265.8836 - loglik: -2.6163e+02 - logprior: -4.1545e+00
Epoch 2/2
19/19 - 4s - loss: 241.3757 - loglik: -2.3951e+02 - logprior: -1.3108e+00
Fitted a model with MAP estimate = -235.9573
expansions: [(0, 11), (23, 1)]
discards: [10 11 12 13 14 15 16 17 18 19 20 78 79 83]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 243.7172 - loglik: -2.3977e+02 - logprior: -3.8635e+00
Epoch 2/2
19/19 - 4s - loss: 237.2062 - loglik: -2.3542e+02 - logprior: -1.3697e+00
Fitted a model with MAP estimate = -233.5342
expansions: [(0, 5)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 20]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.2082 - loglik: -2.3782e+02 - logprior: -3.3304e+00
Epoch 2/10
19/19 - 4s - loss: 236.0855 - loglik: -2.3469e+02 - logprior: -1.0189e+00
Epoch 3/10
19/19 - 3s - loss: 233.6901 - loglik: -2.3183e+02 - logprior: -8.9602e-01
Epoch 4/10
19/19 - 3s - loss: 231.7237 - loglik: -2.2964e+02 - logprior: -8.7530e-01
Epoch 5/10
19/19 - 3s - loss: 230.6766 - loglik: -2.2878e+02 - logprior: -8.5486e-01
Epoch 6/10
19/19 - 4s - loss: 229.7111 - loglik: -2.2796e+02 - logprior: -8.3396e-01
Epoch 7/10
19/19 - 4s - loss: 229.1520 - loglik: -2.2748e+02 - logprior: -8.3131e-01
Epoch 8/10
19/19 - 3s - loss: 228.1696 - loglik: -2.2653e+02 - logprior: -8.0670e-01
Epoch 9/10
19/19 - 3s - loss: 227.9679 - loglik: -2.2634e+02 - logprior: -7.9248e-01
Epoch 10/10
19/19 - 3s - loss: 227.1038 - loglik: -2.2547e+02 - logprior: -7.8061e-01
Fitted a model with MAP estimate = -225.7583
Time for alignment: 118.0324
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.4263 - loglik: -2.8025e+02 - logprior: -3.0602e+00
Epoch 2/10
19/19 - 3s - loss: 252.2246 - loglik: -2.5064e+02 - logprior: -9.2546e-01
Epoch 3/10
19/19 - 3s - loss: 242.5950 - loglik: -2.4056e+02 - logprior: -9.7749e-01
Epoch 4/10
19/19 - 3s - loss: 238.8575 - loglik: -2.3673e+02 - logprior: -9.3630e-01
Epoch 5/10
19/19 - 4s - loss: 237.4060 - loglik: -2.3548e+02 - logprior: -9.4306e-01
Epoch 6/10
19/19 - 3s - loss: 235.9612 - loglik: -2.3420e+02 - logprior: -9.3696e-01
Epoch 7/10
19/19 - 3s - loss: 235.4229 - loglik: -2.3372e+02 - logprior: -9.1477e-01
Epoch 8/10
19/19 - 4s - loss: 235.1572 - loglik: -2.3347e+02 - logprior: -9.1507e-01
Epoch 9/10
19/19 - 3s - loss: 234.1733 - loglik: -2.3246e+02 - logprior: -9.1896e-01
Epoch 10/10
19/19 - 3s - loss: 233.7024 - loglik: -2.3192e+02 - logprior: -9.2393e-01
Fitted a model with MAP estimate = -231.5425
expansions: [(0, 12), (16, 3), (17, 2), (29, 2), (56, 2), (57, 1), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 264.7316 - loglik: -2.6053e+02 - logprior: -4.1134e+00
Epoch 2/2
19/19 - 4s - loss: 242.1846 - loglik: -2.4045e+02 - logprior: -1.3633e+00
Fitted a model with MAP estimate = -237.0881
expansions: [(0, 11), (10, 1), (36, 2)]
discards: [11 12 13 14 15 16 17 18 19 20 21 22 23 47 77]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.6345 - loglik: -2.4066e+02 - logprior: -3.9071e+00
Epoch 2/2
19/19 - 4s - loss: 237.7365 - loglik: -2.3590e+02 - logprior: -1.4422e+00
Fitted a model with MAP estimate = -233.8772
expansions: [(0, 5), (17, 1), (36, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 21 33]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.8565 - loglik: -2.3802e+02 - logprior: -3.7718e+00
Epoch 2/10
19/19 - 4s - loss: 236.5595 - loglik: -2.3465e+02 - logprior: -1.5147e+00
Epoch 3/10
19/19 - 4s - loss: 233.9608 - loglik: -2.3191e+02 - logprior: -1.1681e+00
Epoch 4/10
19/19 - 4s - loss: 232.1714 - loglik: -2.2997e+02 - logprior: -1.0530e+00
Epoch 5/10
19/19 - 4s - loss: 230.9407 - loglik: -2.2890e+02 - logprior: -9.4871e-01
Epoch 6/10
19/19 - 3s - loss: 229.3067 - loglik: -2.2738e+02 - logprior: -9.1693e-01
Epoch 7/10
19/19 - 4s - loss: 228.7336 - loglik: -2.2689e+02 - logprior: -8.8137e-01
Epoch 8/10
19/19 - 4s - loss: 228.4619 - loglik: -2.2665e+02 - logprior: -8.5892e-01
Epoch 9/10
19/19 - 4s - loss: 227.3002 - loglik: -2.2550e+02 - logprior: -8.3008e-01
Epoch 10/10
19/19 - 4s - loss: 225.9540 - loglik: -2.2412e+02 - logprior: -8.2068e-01
Fitted a model with MAP estimate = -224.8326
Time for alignment: 119.2813
Computed alignments with likelihoods: ['-226.6410', '-225.7583', '-224.8326']
Best model has likelihood: -224.8326
time for generating output: 0.2409
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.42005263157894734
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb54f4e6f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5035c8b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6869eeee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c5cd1520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4c5cd1550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb320375bb0>, <__main__.SimpleDirichletPrior object at 0x7fb6c505e4f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 733.2839 - loglik: -7.2073e+02 - logprior: -1.2533e+01
Epoch 2/10
17/17 - 8s - loss: 552.8496 - loglik: -5.5037e+02 - logprior: -2.4600e+00
Epoch 3/10
17/17 - 8s - loss: 453.5710 - loglik: -4.4926e+02 - logprior: -4.2316e+00
Epoch 4/10
17/17 - 8s - loss: 432.6617 - loglik: -4.2735e+02 - logprior: -4.8192e+00
Epoch 5/10
17/17 - 8s - loss: 428.4840 - loglik: -4.2345e+02 - logprior: -4.4259e+00
Epoch 6/10
17/17 - 8s - loss: 426.7132 - loglik: -4.2196e+02 - logprior: -4.2086e+00
Epoch 7/10
17/17 - 8s - loss: 423.1852 - loglik: -4.1844e+02 - logprior: -4.2255e+00
Epoch 8/10
17/17 - 8s - loss: 424.8814 - loglik: -4.2013e+02 - logprior: -4.2290e+00
Fitted a model with MAP estimate = -423.3954
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (54, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (62, 1), (85, 1), (86, 1), (94, 2), (97, 1), (109, 1), (113, 1), (114, 1), (115, 1), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (173, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 16s - loss: 431.7463 - loglik: -4.1494e+02 - logprior: -1.6737e+01
Epoch 2/2
17/17 - 10s - loss: 391.4939 - loglik: -3.8578e+02 - logprior: -5.4000e+00
Fitted a model with MAP estimate = -386.4431
expansions: [(0, 9)]
discards: [  0 154]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 398.5519 - loglik: -3.8679e+02 - logprior: -1.1718e+01
Epoch 2/2
17/17 - 10s - loss: 381.0839 - loglik: -3.8020e+02 - logprior: -6.4516e-01
Fitted a model with MAP estimate = -378.5051
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 395.6098 - loglik: -3.8467e+02 - logprior: -1.0878e+01
Epoch 2/10
17/17 - 10s - loss: 380.1306 - loglik: -3.8010e+02 - logprior: 0.2461
Epoch 3/10
17/17 - 10s - loss: 377.6562 - loglik: -3.7894e+02 - logprior: 1.7047
Epoch 4/10
17/17 - 10s - loss: 374.5525 - loglik: -3.7650e+02 - logprior: 2.4199
Epoch 5/10
17/17 - 10s - loss: 372.4987 - loglik: -3.7481e+02 - logprior: 2.7950
Epoch 6/10
17/17 - 10s - loss: 371.4039 - loglik: -3.7395e+02 - logprior: 3.0200
Epoch 7/10
17/17 - 10s - loss: 370.0709 - loglik: -3.7284e+02 - logprior: 3.2645
Epoch 8/10
17/17 - 10s - loss: 369.8828 - loglik: -3.7293e+02 - logprior: 3.5566
Epoch 9/10
17/17 - 10s - loss: 369.0423 - loglik: -3.7239e+02 - logprior: 3.8536
Epoch 10/10
17/17 - 10s - loss: 369.1748 - loglik: -3.7281e+02 - logprior: 4.1457
Fitted a model with MAP estimate = -367.6928
Time for alignment: 251.6161
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 733.0472 - loglik: -7.2048e+02 - logprior: -1.2552e+01
Epoch 2/10
17/17 - 8s - loss: 552.7833 - loglik: -5.5037e+02 - logprior: -2.3822e+00
Epoch 3/10
17/17 - 8s - loss: 454.8820 - loglik: -4.5109e+02 - logprior: -3.7746e+00
Epoch 4/10
17/17 - 8s - loss: 436.0490 - loglik: -4.3139e+02 - logprior: -4.3959e+00
Epoch 5/10
17/17 - 8s - loss: 431.3836 - loglik: -4.2671e+02 - logprior: -4.1520e+00
Epoch 6/10
17/17 - 8s - loss: 428.3899 - loglik: -4.2389e+02 - logprior: -3.9925e+00
Epoch 7/10
17/17 - 8s - loss: 426.9687 - loglik: -4.2253e+02 - logprior: -3.9750e+00
Epoch 8/10
17/17 - 8s - loss: 425.3127 - loglik: -4.2084e+02 - logprior: -3.9803e+00
Epoch 9/10
17/17 - 8s - loss: 425.1622 - loglik: -4.2071e+02 - logprior: -3.9464e+00
Epoch 10/10
17/17 - 8s - loss: 426.7937 - loglik: -4.2234e+02 - logprior: -3.9388e+00
Fitted a model with MAP estimate = -424.6110
expansions: [(9, 1), (12, 1), (14, 3), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (110, 1), (114, 1), (115, 2), (116, 1), (130, 3), (131, 2), (142, 1), (151, 1), (160, 2), (161, 1), (162, 1), (176, 1), (179, 1), (180, 1), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 432.7636 - loglik: -4.1584e+02 - logprior: -1.6876e+01
Epoch 2/2
17/17 - 10s - loss: 392.2895 - loglik: -3.8632e+02 - logprior: -5.7096e+00
Fitted a model with MAP estimate = -387.0593
expansions: [(0, 10)]
discards: [  0  16 138 157 160]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 16s - loss: 398.9691 - loglik: -3.8713e+02 - logprior: -1.1763e+01
Epoch 2/2
17/17 - 10s - loss: 382.3194 - loglik: -3.8135e+02 - logprior: -6.3596e-01
Fitted a model with MAP estimate = -378.6161
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 394.9380 - loglik: -3.8413e+02 - logprior: -1.0741e+01
Epoch 2/10
17/17 - 10s - loss: 381.2287 - loglik: -3.8127e+02 - logprior: 0.3600
Epoch 3/10
17/17 - 10s - loss: 378.0272 - loglik: -3.7938e+02 - logprior: 1.8399
Epoch 4/10
17/17 - 10s - loss: 374.4453 - loglik: -3.7648e+02 - logprior: 2.5349
Epoch 5/10
17/17 - 10s - loss: 373.6917 - loglik: -3.7614e+02 - logprior: 2.9029
Epoch 6/10
17/17 - 10s - loss: 369.6494 - loglik: -3.7232e+02 - logprior: 3.1055
Epoch 7/10
17/17 - 10s - loss: 372.4423 - loglik: -3.7530e+02 - logprior: 3.3450
Fitted a model with MAP estimate = -369.1834
Time for alignment: 237.7099
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 732.1474 - loglik: -7.1958e+02 - logprior: -1.2553e+01
Epoch 2/10
17/17 - 8s - loss: 550.4485 - loglik: -5.4804e+02 - logprior: -2.3824e+00
Epoch 3/10
17/17 - 8s - loss: 454.3484 - loglik: -4.5037e+02 - logprior: -3.9282e+00
Epoch 4/10
17/17 - 8s - loss: 433.3827 - loglik: -4.2840e+02 - logprior: -4.5517e+00
Epoch 5/10
17/17 - 8s - loss: 430.5099 - loglik: -4.2566e+02 - logprior: -4.2599e+00
Epoch 6/10
17/17 - 8s - loss: 426.7916 - loglik: -4.2222e+02 - logprior: -4.0460e+00
Epoch 7/10
17/17 - 8s - loss: 426.1076 - loglik: -4.2159e+02 - logprior: -4.0407e+00
Epoch 8/10
17/17 - 8s - loss: 425.7415 - loglik: -4.2117e+02 - logprior: -4.0734e+00
Epoch 9/10
17/17 - 8s - loss: 424.6781 - loglik: -4.2005e+02 - logprior: -4.0801e+00
Epoch 10/10
17/17 - 8s - loss: 425.9786 - loglik: -4.2136e+02 - logprior: -4.0600e+00
Fitted a model with MAP estimate = -423.7964
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (87, 4), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 2), (140, 1), (141, 1), (150, 1), (160, 1), (161, 1), (162, 1), (176, 1), (179, 1), (181, 1), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 432.3945 - loglik: -4.1546e+02 - logprior: -1.6869e+01
Epoch 2/2
17/17 - 10s - loss: 392.8349 - loglik: -3.8689e+02 - logprior: -5.6381e+00
Fitted a model with MAP estimate = -386.1846
expansions: [(0, 9)]
discards: [  0 104 105 142 157 161]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 399.4179 - loglik: -3.8768e+02 - logprior: -1.1690e+01
Epoch 2/2
17/17 - 10s - loss: 383.1148 - loglik: -3.8227e+02 - logprior: -6.0536e-01
Fitted a model with MAP estimate = -378.8149
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 394.9743 - loglik: -3.8416e+02 - logprior: -1.0745e+01
Epoch 2/10
17/17 - 10s - loss: 381.5114 - loglik: -3.8159e+02 - logprior: 0.3850
Epoch 3/10
17/17 - 10s - loss: 377.8587 - loglik: -3.7925e+02 - logprior: 1.8367
Epoch 4/10
17/17 - 10s - loss: 373.4523 - loglik: -3.7550e+02 - logprior: 2.5005
Epoch 5/10
17/17 - 10s - loss: 374.0568 - loglik: -3.7644e+02 - logprior: 2.8464
Fitted a model with MAP estimate = -371.0725
Time for alignment: 216.8062
Computed alignments with likelihoods: ['-367.6928', '-369.1834', '-371.0725']
Best model has likelihood: -367.6928
time for generating output: 0.3535
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.948295672156262
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4c5e33880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e18cdd30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f1c9a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4bbcfa8b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4c6383e20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6973e13a0>, <__main__.SimpleDirichletPrior object at 0x7fb5033ef250>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5587 - loglik: -1.5940e+02 - logprior: -2.0140e+01
Epoch 2/10
10/10 - 1s - loss: 148.2859 - loglik: -1.4243e+02 - logprior: -5.8353e+00
Epoch 3/10
10/10 - 1s - loss: 131.2007 - loglik: -1.2779e+02 - logprior: -3.4065e+00
Epoch 4/10
10/10 - 1s - loss: 119.8809 - loglik: -1.1699e+02 - logprior: -2.8826e+00
Epoch 5/10
10/10 - 1s - loss: 113.7044 - loglik: -1.1073e+02 - logprior: -2.8526e+00
Epoch 6/10
10/10 - 1s - loss: 111.9717 - loglik: -1.0877e+02 - logprior: -2.8512e+00
Epoch 7/10
10/10 - 1s - loss: 110.6014 - loglik: -1.0747e+02 - logprior: -2.7371e+00
Epoch 8/10
10/10 - 1s - loss: 110.2509 - loglik: -1.0733e+02 - logprior: -2.6066e+00
Epoch 9/10
10/10 - 1s - loss: 110.0733 - loglik: -1.0728e+02 - logprior: -2.5213e+00
Epoch 10/10
10/10 - 1s - loss: 109.9283 - loglik: -1.0713e+02 - logprior: -2.4957e+00
Fitted a model with MAP estimate = -109.4149
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 135.8124 - loglik: -1.1312e+02 - logprior: -2.2663e+01
Epoch 2/2
10/10 - 1s - loss: 116.0449 - loglik: -1.0605e+02 - logprior: -9.8362e+00
Fitted a model with MAP estimate = -112.5303
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.6888 - loglik: -1.0369e+02 - logprior: -1.7974e+01
Epoch 2/2
10/10 - 1s - loss: 108.0413 - loglik: -1.0283e+02 - logprior: -5.0638e+00
Fitted a model with MAP estimate = -105.7259
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 125.0202 - loglik: -1.0457e+02 - logprior: -2.0427e+01
Epoch 2/10
10/10 - 1s - loss: 110.1491 - loglik: -1.0403e+02 - logprior: -5.9768e+00
Epoch 3/10
10/10 - 1s - loss: 106.4765 - loglik: -1.0310e+02 - logprior: -3.1217e+00
Epoch 4/10
10/10 - 1s - loss: 105.1244 - loglik: -1.0251e+02 - logprior: -2.3081e+00
Epoch 5/10
10/10 - 1s - loss: 104.5137 - loglik: -1.0244e+02 - logprior: -1.7352e+00
Epoch 6/10
10/10 - 1s - loss: 103.8710 - loglik: -1.0207e+02 - logprior: -1.4679e+00
Epoch 7/10
10/10 - 1s - loss: 103.7635 - loglik: -1.0209e+02 - logprior: -1.3437e+00
Epoch 8/10
10/10 - 1s - loss: 103.4437 - loglik: -1.0189e+02 - logprior: -1.2108e+00
Epoch 9/10
10/10 - 1s - loss: 103.5159 - loglik: -1.0203e+02 - logprior: -1.1490e+00
Fitted a model with MAP estimate = -102.9487
Time for alignment: 36.1806
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 179.5579 - loglik: -1.5940e+02 - logprior: -2.0141e+01
Epoch 2/10
10/10 - 1s - loss: 148.6076 - loglik: -1.4276e+02 - logprior: -5.8313e+00
Epoch 3/10
10/10 - 1s - loss: 131.2100 - loglik: -1.2783e+02 - logprior: -3.3760e+00
Epoch 4/10
10/10 - 1s - loss: 119.3427 - loglik: -1.1647e+02 - logprior: -2.8677e+00
Epoch 5/10
10/10 - 1s - loss: 113.5658 - loglik: -1.1057e+02 - logprior: -2.8877e+00
Epoch 6/10
10/10 - 1s - loss: 111.6858 - loglik: -1.0846e+02 - logprior: -2.8813e+00
Epoch 7/10
10/10 - 1s - loss: 110.8114 - loglik: -1.0762e+02 - logprior: -2.7658e+00
Epoch 8/10
10/10 - 1s - loss: 110.2344 - loglik: -1.0725e+02 - logprior: -2.6177e+00
Epoch 9/10
10/10 - 1s - loss: 109.9163 - loglik: -1.0706e+02 - logprior: -2.5233e+00
Epoch 10/10
10/10 - 1s - loss: 109.9363 - loglik: -1.0711e+02 - logprior: -2.5043e+00
Fitted a model with MAP estimate = -109.4184
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.8770 - loglik: -1.1316e+02 - logprior: -2.2687e+01
Epoch 2/2
10/10 - 1s - loss: 115.8223 - loglik: -1.0586e+02 - logprior: -9.8283e+00
Fitted a model with MAP estimate = -112.4709
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.8246 - loglik: -1.0383e+02 - logprior: -1.7967e+01
Epoch 2/2
10/10 - 1s - loss: 107.7545 - loglik: -1.0254e+02 - logprior: -5.0603e+00
Fitted a model with MAP estimate = -105.7197
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 125.1214 - loglik: -1.0469e+02 - logprior: -2.0412e+01
Epoch 2/10
10/10 - 1s - loss: 109.8284 - loglik: -1.0373e+02 - logprior: -5.9662e+00
Epoch 3/10
10/10 - 1s - loss: 106.6832 - loglik: -1.0331e+02 - logprior: -3.1166e+00
Epoch 4/10
10/10 - 1s - loss: 105.1665 - loglik: -1.0253e+02 - logprior: -2.3118e+00
Epoch 5/10
10/10 - 1s - loss: 104.4058 - loglik: -1.0235e+02 - logprior: -1.7243e+00
Epoch 6/10
10/10 - 1s - loss: 103.8677 - loglik: -1.0208e+02 - logprior: -1.4597e+00
Epoch 7/10
10/10 - 1s - loss: 103.7837 - loglik: -1.0210e+02 - logprior: -1.3463e+00
Epoch 8/10
10/10 - 1s - loss: 103.4952 - loglik: -1.0196e+02 - logprior: -1.2017e+00
Epoch 9/10
10/10 - 1s - loss: 103.4825 - loglik: -1.0200e+02 - logprior: -1.1394e+00
Epoch 10/10
10/10 - 1s - loss: 103.1823 - loglik: -1.0172e+02 - logprior: -1.1119e+00
Fitted a model with MAP estimate = -102.8331
Time for alignment: 38.5639
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5767 - loglik: -1.5942e+02 - logprior: -2.0139e+01
Epoch 2/10
10/10 - 1s - loss: 148.2391 - loglik: -1.4239e+02 - logprior: -5.8279e+00
Epoch 3/10
10/10 - 1s - loss: 131.6159 - loglik: -1.2826e+02 - logprior: -3.3582e+00
Epoch 4/10
10/10 - 1s - loss: 120.5450 - loglik: -1.1775e+02 - logprior: -2.7880e+00
Epoch 5/10
10/10 - 1s - loss: 114.2407 - loglik: -1.1133e+02 - logprior: -2.7535e+00
Epoch 6/10
10/10 - 1s - loss: 111.9456 - loglik: -1.0881e+02 - logprior: -2.7471e+00
Epoch 7/10
10/10 - 1s - loss: 110.6092 - loglik: -1.0754e+02 - logprior: -2.6772e+00
Epoch 8/10
10/10 - 1s - loss: 110.0149 - loglik: -1.0712e+02 - logprior: -2.5883e+00
Epoch 9/10
10/10 - 1s - loss: 109.5503 - loglik: -1.0673e+02 - logprior: -2.5291e+00
Epoch 10/10
10/10 - 1s - loss: 109.7320 - loglik: -1.0691e+02 - logprior: -2.5075e+00
Fitted a model with MAP estimate = -109.1016
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.5158 - loglik: -1.1281e+02 - logprior: -2.2679e+01
Epoch 2/2
10/10 - 1s - loss: 115.9561 - loglik: -1.0598e+02 - logprior: -9.8249e+00
Fitted a model with MAP estimate = -112.4726
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.8440 - loglik: -1.0385e+02 - logprior: -1.7964e+01
Epoch 2/2
10/10 - 1s - loss: 107.6791 - loglik: -1.0248e+02 - logprior: -5.0574e+00
Fitted a model with MAP estimate = -105.7423
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 125.0747 - loglik: -1.0463e+02 - logprior: -2.0425e+01
Epoch 2/10
10/10 - 1s - loss: 110.0576 - loglik: -1.0395e+02 - logprior: -5.9687e+00
Epoch 3/10
10/10 - 1s - loss: 106.5033 - loglik: -1.0312e+02 - logprior: -3.1232e+00
Epoch 4/10
10/10 - 1s - loss: 105.1769 - loglik: -1.0250e+02 - logprior: -2.3431e+00
Epoch 5/10
10/10 - 1s - loss: 104.3888 - loglik: -1.0230e+02 - logprior: -1.7583e+00
Epoch 6/10
10/10 - 1s - loss: 103.8903 - loglik: -1.0211e+02 - logprior: -1.4523e+00
Epoch 7/10
10/10 - 1s - loss: 103.6509 - loglik: -1.0199e+02 - logprior: -1.3251e+00
Epoch 8/10
10/10 - 1s - loss: 103.4895 - loglik: -1.0195e+02 - logprior: -1.2015e+00
Epoch 9/10
10/10 - 1s - loss: 103.5373 - loglik: -1.0206e+02 - logprior: -1.1346e+00
Fitted a model with MAP estimate = -102.9489
Time for alignment: 34.3132
Computed alignments with likelihoods: ['-102.9487', '-102.8331', '-102.9489']
Best model has likelihood: -102.8331
time for generating output: 0.1208
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9928712871287129
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb65bf50b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6053bfb50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4fb10d340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb54f3eca30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54f3ecd00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb61f726bb0>, <__main__.SimpleDirichletPrior object at 0x7fb69f8f6640>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 468.3009 - loglik: -4.6640e+02 - logprior: -1.8784e+00
Epoch 2/10
39/39 - 11s - loss: 383.9753 - loglik: -3.8236e+02 - logprior: -1.3253e+00
Epoch 3/10
39/39 - 11s - loss: 374.7649 - loglik: -3.7299e+02 - logprior: -1.3066e+00
Epoch 4/10
39/39 - 10s - loss: 372.0269 - loglik: -3.7022e+02 - logprior: -1.3083e+00
Epoch 5/10
39/39 - 11s - loss: 370.9884 - loglik: -3.6918e+02 - logprior: -1.3011e+00
Epoch 6/10
39/39 - 11s - loss: 370.3127 - loglik: -3.6849e+02 - logprior: -1.3122e+00
Epoch 7/10
39/39 - 10s - loss: 369.4984 - loglik: -3.6770e+02 - logprior: -1.3093e+00
Epoch 8/10
39/39 - 11s - loss: 368.9319 - loglik: -3.6712e+02 - logprior: -1.3312e+00
Epoch 9/10
39/39 - 10s - loss: 369.0905 - loglik: -3.6729e+02 - logprior: -1.3279e+00
Fitted a model with MAP estimate = -305.6816
expansions: [(0, 14), (10, 1), (15, 1), (18, 1), (28, 2), (29, 1), (33, 2), (34, 1), (35, 1), (38, 1), (43, 2), (44, 1), (45, 1), (56, 1), (69, 1), (71, 3), (87, 1), (88, 1), (90, 1), (102, 2), (107, 1), (112, 2), (125, 8), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 356.0879 - loglik: -3.5302e+02 - logprior: -2.8720e+00
Epoch 2/2
39/39 - 14s - loss: 338.4289 - loglik: -3.3646e+02 - logprior: -1.6226e+00
Fitted a model with MAP estimate = -280.9793
expansions: [(171, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  46  53  54  68 102 152]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 345.3934 - loglik: -3.4308e+02 - logprior: -2.1568e+00
Epoch 2/2
39/39 - 12s - loss: 340.7458 - loglik: -3.3932e+02 - logprior: -1.0354e+00
Fitted a model with MAP estimate = -284.6149
expansions: [(0, 14)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 282.6260 - loglik: -2.8070e+02 - logprior: -1.7301e+00
Epoch 2/10
52/52 - 16s - loss: 276.1577 - loglik: -2.7442e+02 - logprior: -1.4444e+00
Epoch 3/10
52/52 - 16s - loss: 275.3855 - loglik: -2.7364e+02 - logprior: -1.4022e+00
Epoch 4/10
52/52 - 17s - loss: 273.4178 - loglik: -2.7165e+02 - logprior: -1.3344e+00
Epoch 5/10
52/52 - 16s - loss: 272.1012 - loglik: -2.7041e+02 - logprior: -1.2664e+00
Epoch 6/10
52/52 - 16s - loss: 271.2549 - loglik: -2.6962e+02 - logprior: -1.2098e+00
Epoch 7/10
52/52 - 17s - loss: 270.3675 - loglik: -2.6878e+02 - logprior: -1.1493e+00
Epoch 8/10
52/52 - 17s - loss: 269.8595 - loglik: -2.6833e+02 - logprior: -1.0917e+00
Epoch 9/10
52/52 - 15s - loss: 269.9081 - loglik: -2.6843e+02 - logprior: -1.0321e+00
Fitted a model with MAP estimate = -269.0233
Time for alignment: 396.9819
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 468.5574 - loglik: -4.6670e+02 - logprior: -1.8327e+00
Epoch 2/10
39/39 - 10s - loss: 385.3250 - loglik: -3.8388e+02 - logprior: -1.2130e+00
Epoch 3/10
39/39 - 11s - loss: 376.6288 - loglik: -3.7487e+02 - logprior: -1.2559e+00
Epoch 4/10
39/39 - 11s - loss: 374.2540 - loglik: -3.7248e+02 - logprior: -1.2079e+00
Epoch 5/10
39/39 - 11s - loss: 372.8475 - loglik: -3.7106e+02 - logprior: -1.2023e+00
Epoch 6/10
39/39 - 11s - loss: 372.2663 - loglik: -3.7047e+02 - logprior: -1.2099e+00
Epoch 7/10
39/39 - 11s - loss: 371.7283 - loglik: -3.6994e+02 - logprior: -1.2052e+00
Epoch 8/10
39/39 - 11s - loss: 371.5690 - loglik: -3.6979e+02 - logprior: -1.2128e+00
Epoch 9/10
39/39 - 10s - loss: 371.1430 - loglik: -3.6938e+02 - logprior: -1.2096e+00
Epoch 10/10
39/39 - 10s - loss: 370.8056 - loglik: -3.6900e+02 - logprior: -1.2450e+00
Fitted a model with MAP estimate = -305.7008
expansions: [(0, 15), (10, 1), (15, 1), (18, 1), (28, 2), (29, 3), (32, 1), (37, 1), (44, 2), (45, 1), (56, 1), (70, 3), (72, 1), (89, 6), (91, 1), (103, 1), (107, 1), (112, 2), (125, 2), (127, 11), (134, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 355.7505 - loglik: -3.5241e+02 - logprior: -3.1499e+00
Epoch 2/2
39/39 - 15s - loss: 335.9876 - loglik: -3.3371e+02 - logprior: -1.9620e+00
Fitted a model with MAP estimate = -278.0104
expansions: [(177, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  69 101 124 125 155
 173 174 175 196]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 344.1419 - loglik: -3.4161e+02 - logprior: -2.3598e+00
Epoch 2/2
39/39 - 13s - loss: 339.2358 - loglik: -3.3765e+02 - logprior: -1.2248e+00
Fitted a model with MAP estimate = -283.0875
expansions: [(0, 14), (45, 2), (86, 1), (155, 1)]
discards: [36 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 21s - loss: 281.3516 - loglik: -2.7926e+02 - logprior: -1.9053e+00
Epoch 2/10
52/52 - 16s - loss: 274.4843 - loglik: -2.7266e+02 - logprior: -1.5359e+00
Epoch 3/10
52/52 - 16s - loss: 274.4970 - loglik: -2.7267e+02 - logprior: -1.4989e+00
Fitted a model with MAP estimate = -272.6135
Time for alignment: 317.7950
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.2386 - loglik: -4.6735e+02 - logprior: -1.8630e+00
Epoch 2/10
39/39 - 11s - loss: 390.3828 - loglik: -3.8903e+02 - logprior: -1.1828e+00
Epoch 3/10
39/39 - 11s - loss: 382.9552 - loglik: -3.8122e+02 - logprior: -1.2112e+00
Epoch 4/10
39/39 - 11s - loss: 380.4426 - loglik: -3.7866e+02 - logprior: -1.1872e+00
Epoch 5/10
39/39 - 11s - loss: 379.0888 - loglik: -3.7734e+02 - logprior: -1.1878e+00
Epoch 6/10
39/39 - 11s - loss: 378.3451 - loglik: -3.7663e+02 - logprior: -1.1968e+00
Epoch 7/10
39/39 - 10s - loss: 377.2199 - loglik: -3.7547e+02 - logprior: -1.2186e+00
Epoch 8/10
39/39 - 11s - loss: 376.4684 - loglik: -3.7472e+02 - logprior: -1.2502e+00
Epoch 9/10
39/39 - 10s - loss: 376.1741 - loglik: -3.7436e+02 - logprior: -1.3489e+00
Epoch 10/10
39/39 - 11s - loss: 375.3578 - loglik: -3.7351e+02 - logprior: -1.3923e+00
Fitted a model with MAP estimate = -310.7397
expansions: [(0, 5), (10, 1), (15, 1), (18, 1), (19, 1), (27, 2), (29, 4), (34, 3), (43, 4), (44, 1), (56, 1), (71, 1), (88, 3), (90, 1), (102, 2), (104, 1), (107, 2), (125, 2), (126, 7), (129, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 362.0836 - loglik: -3.5872e+02 - logprior: -3.1780e+00
Epoch 2/2
39/39 - 14s - loss: 343.1553 - loglik: -3.4120e+02 - logprior: -1.5779e+00
Fitted a model with MAP estimate = -283.9378
expansions: [(52, 1), (92, 2)]
discards: [  0   3   4  36  40  49  50  61  62 139]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 351.6808 - loglik: -3.4831e+02 - logprior: -3.1852e+00
Epoch 2/2
39/39 - 13s - loss: 345.7593 - loglik: -3.4415e+02 - logprior: -1.1957e+00
Fitted a model with MAP estimate = -286.2156
expansions: [(85, 1)]
discards: [  0   1 129]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 285.1953 - loglik: -2.8365e+02 - logprior: -1.3392e+00
Epoch 2/10
52/52 - 16s - loss: 282.2105 - loglik: -2.8106e+02 - logprior: -8.1261e-01
Epoch 3/10
52/52 - 15s - loss: 281.1122 - loglik: -2.7999e+02 - logprior: -7.5349e-01
Epoch 4/10
52/52 - 16s - loss: 279.3559 - loglik: -2.7827e+02 - logprior: -6.9516e-01
Epoch 5/10
52/52 - 16s - loss: 277.3549 - loglik: -2.7638e+02 - logprior: -6.4297e-01
Epoch 6/10
52/52 - 14s - loss: 277.8996 - loglik: -2.7696e+02 - logprior: -6.4012e-01
Fitted a model with MAP estimate = -276.0929
Time for alignment: 352.6727
Computed alignments with likelihoods: ['-269.0233', '-272.6135', '-276.0929']
Best model has likelihood: -269.0233
time for generating output: 0.6954
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.19820935327174016
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6390c5be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb558ad3ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb630598af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e139ceb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e139c820>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fad100b6310>, <__main__.SimpleDirichletPrior object at 0x7fb3547af4f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 817.2565 - loglik: -8.1509e+02 - logprior: -1.7035e+00
Epoch 2/10
37/37 - 23s - loss: 731.8279 - loglik: -7.2921e+02 - logprior: -7.9781e-01
Epoch 3/10
37/37 - 24s - loss: 718.0487 - loglik: -7.1460e+02 - logprior: -8.9765e-01
Epoch 4/10
37/37 - 23s - loss: 708.2597 - loglik: -7.0504e+02 - logprior: -9.7660e-01
Epoch 5/10
37/37 - 24s - loss: 708.5508 - loglik: -7.0560e+02 - logprior: -1.0787e+00
Fitted a model with MAP estimate = -703.9365
expansions: [(0, 4), (30, 1), (33, 5), (52, 1), (73, 3), (93, 8), (116, 1), (121, 1), (122, 2), (133, 5), (134, 1), (198, 10), (206, 4), (240, 1)]
discards: [ 99 100 101 102 103 147 150]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 34s - loss: 739.0886 - loglik: -7.3585e+02 - logprior: -3.0237e+00
Epoch 2/2
37/37 - 28s - loss: 708.2653 - loglik: -7.0620e+02 - logprior: -1.5025e+00
Fitted a model with MAP estimate = -700.3403
expansions: [(0, 2), (37, 1), (38, 1), (39, 1), (157, 1), (227, 1), (228, 4), (244, 3), (281, 1)]
discards: [  0  86 113 114 178 179 180 214 215 245 246 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 717.2830 - loglik: -7.1474e+02 - logprior: -2.3193e+00
Epoch 2/2
37/37 - 29s - loss: 704.0397 - loglik: -7.0173e+02 - logprior: -1.1234e+00
Fitted a model with MAP estimate = -696.0762
expansions: [(0, 2), (112, 5), (113, 3), (180, 4), (227, 1), (243, 1), (244, 1)]
discards: [  1 106 107 108 109 208 249]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 36s - loss: 716.8980 - loglik: -7.1411e+02 - logprior: -2.5990e+00
Epoch 2/10
37/37 - 30s - loss: 702.4174 - loglik: -7.0064e+02 - logprior: -8.2440e-01
Epoch 3/10
37/37 - 30s - loss: 692.9885 - loglik: -6.9035e+02 - logprior: -7.5954e-01
Epoch 4/10
37/37 - 30s - loss: 691.0021 - loglik: -6.8810e+02 - logprior: -7.3552e-01
Epoch 5/10
37/37 - 30s - loss: 685.7962 - loglik: -6.8308e+02 - logprior: -7.0868e-01
Epoch 6/10
37/37 - 30s - loss: 685.8088 - loglik: -6.8343e+02 - logprior: -5.7210e-01
Fitted a model with MAP estimate = -682.2583
Time for alignment: 561.9679
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 814.5213 - loglik: -8.1236e+02 - logprior: -1.6909e+00
Epoch 2/10
37/37 - 23s - loss: 727.2488 - loglik: -7.2489e+02 - logprior: -8.3148e-01
Epoch 3/10
37/37 - 24s - loss: 709.8306 - loglik: -7.0716e+02 - logprior: -9.7118e-01
Epoch 4/10
37/37 - 23s - loss: 704.4944 - loglik: -7.0175e+02 - logprior: -9.3298e-01
Epoch 5/10
37/37 - 24s - loss: 701.3409 - loglik: -6.9873e+02 - logprior: -9.6463e-01
Epoch 6/10
37/37 - 23s - loss: 700.3401 - loglik: -6.9788e+02 - logprior: -9.9390e-01
Epoch 7/10
37/37 - 24s - loss: 696.8341 - loglik: -6.9449e+02 - logprior: -1.0259e+00
Epoch 8/10
37/37 - 23s - loss: 699.0947 - loglik: -6.9687e+02 - logprior: -1.0315e+00
Fitted a model with MAP estimate = -696.1484
expansions: [(0, 4), (30, 1), (33, 5), (58, 1), (65, 1), (91, 4), (92, 10), (93, 4), (94, 1), (138, 1), (199, 8), (209, 4), (210, 2), (240, 1), (242, 1)]
discards: [122 155 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 739.6217 - loglik: -7.3646e+02 - logprior: -2.8326e+00
Epoch 2/2
37/37 - 29s - loss: 702.4225 - loglik: -6.9980e+02 - logprior: -1.3407e+00
Fitted a model with MAP estimate = -693.6502
expansions: [(37, 1), (38, 1), (39, 1), (175, 6), (226, 1), (250, 3)]
discards: [  0 103 235 248 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 35s - loss: 715.3271 - loglik: -7.1199e+02 - logprior: -3.1125e+00
Epoch 2/2
37/37 - 30s - loss: 696.8609 - loglik: -6.9488e+02 - logprior: -9.1702e-01
Fitted a model with MAP estimate = -688.9675
expansions: [(0, 2), (111, 1), (180, 1), (241, 6)]
discards: [242]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 35s - loss: 708.4779 - loglik: -7.0622e+02 - logprior: -2.0649e+00
Epoch 2/10
37/37 - 31s - loss: 694.7189 - loglik: -6.9295e+02 - logprior: -7.4175e-01
Epoch 3/10
37/37 - 31s - loss: 687.9273 - loglik: -6.8515e+02 - logprior: -7.7601e-01
Epoch 4/10
37/37 - 31s - loss: 680.2888 - loglik: -6.7740e+02 - logprior: -6.2565e-01
Epoch 5/10
37/37 - 31s - loss: 681.2758 - loglik: -6.7856e+02 - logprior: -5.5705e-01
Fitted a model with MAP estimate = -675.5826
Time for alignment: 609.0556
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 815.2808 - loglik: -8.1313e+02 - logprior: -1.6876e+00
Epoch 2/10
37/37 - 24s - loss: 726.8254 - loglik: -7.2408e+02 - logprior: -8.5879e-01
Epoch 3/10
37/37 - 24s - loss: 712.0218 - loglik: -7.0862e+02 - logprior: -9.2241e-01
Epoch 4/10
37/37 - 24s - loss: 705.8372 - loglik: -7.0279e+02 - logprior: -9.4304e-01
Epoch 5/10
37/37 - 23s - loss: 702.0063 - loglik: -6.9930e+02 - logprior: -9.7285e-01
Epoch 6/10
37/37 - 24s - loss: 702.1163 - loglik: -6.9963e+02 - logprior: -9.9457e-01
Fitted a model with MAP estimate = -699.8777
expansions: [(0, 4), (30, 1), (33, 3), (34, 2), (66, 1), (73, 3), (92, 4), (93, 8), (107, 1), (124, 3), (129, 1), (198, 9), (206, 1), (210, 3), (239, 1), (240, 1)]
discards: [156]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 34s - loss: 737.5300 - loglik: -7.3448e+02 - logprior: -2.7503e+00
Epoch 2/2
37/37 - 29s - loss: 703.4752 - loglik: -7.0115e+02 - logprior: -1.1243e+00
Fitted a model with MAP estimate = -693.5577
expansions: [(0, 2), (37, 1), (42, 1), (43, 1), (113, 6), (231, 6), (252, 1)]
discards: [  0  86 106 219 220 228 253]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 34s - loss: 713.8773 - loglik: -7.1140e+02 - logprior: -2.2766e+00
Epoch 2/2
37/37 - 30s - loss: 698.3489 - loglik: -6.9634e+02 - logprior: -9.6821e-01
Fitted a model with MAP estimate = -689.6601
expansions: [(0, 2), (234, 1), (239, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 708.2503 - loglik: -7.0548e+02 - logprior: -2.5782e+00
Epoch 2/10
37/37 - 31s - loss: 698.4434 - loglik: -6.9665e+02 - logprior: -7.8637e-01
Epoch 3/10
37/37 - 31s - loss: 689.0114 - loglik: -6.8631e+02 - logprior: -7.2440e-01
Epoch 4/10
37/37 - 31s - loss: 681.2593 - loglik: -6.7838e+02 - logprior: -6.6247e-01
Epoch 5/10
37/37 - 31s - loss: 681.6561 - loglik: -6.7895e+02 - logprior: -6.4374e-01
Fitted a model with MAP estimate = -677.8984
Time for alignment: 604.3983
Computed alignments with likelihoods: ['-682.2583', '-675.5826', '-677.8984']
Best model has likelihood: -675.5826
time for generating output: 0.3296
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.9132915796798886
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb686a923d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf9cd4700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bc2c4670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb686cd90d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bd597fa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb686a65850>, <__main__.SimpleDirichletPrior object at 0x7fb2bc04dfd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3579 - loglik: -2.6162e+02 - logprior: -2.8563e+01
Epoch 3/10
10/10 - 1s - loss: 262.0309 - loglik: -2.5084e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.7197 - loglik: -2.4194e+02 - logprior: -4.7204e+00
Epoch 5/10
10/10 - 1s - loss: 237.4039 - loglik: -2.3544e+02 - logprior: -1.7869e+00
Epoch 6/10
10/10 - 1s - loss: 231.7841 - loglik: -2.3107e+02 - logprior: -2.0256e-01
Epoch 7/10
10/10 - 1s - loss: 228.5344 - loglik: -2.2844e+02 - logprior: 0.6565
Epoch 8/10
10/10 - 1s - loss: 226.5568 - loglik: -2.2703e+02 - logprior: 1.2458
Epoch 9/10
10/10 - 1s - loss: 225.3463 - loglik: -2.2632e+02 - logprior: 1.6628
Epoch 10/10
10/10 - 1s - loss: 224.5649 - loglik: -2.2599e+02 - logprior: 2.0638
Fitted a model with MAP estimate = -223.6061
expansions: [(0, 6), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 387.5421 - loglik: -2.3509e+02 - logprior: -1.5243e+02
Epoch 2/2
10/10 - 1s - loss: 272.6664 - loglik: -2.2812e+02 - logprior: -4.4415e+01
Fitted a model with MAP estimate = -251.0999
expansions: [(0, 4), (43, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 348.2797 - loglik: -2.2788e+02 - logprior: -1.2037e+02
Epoch 2/2
10/10 - 1s - loss: 257.8160 - loglik: -2.2560e+02 - logprior: -3.2043e+01
Fitted a model with MAP estimate = -241.5094
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 353.4296 - loglik: -2.2522e+02 - logprior: -1.2819e+02
Epoch 2/10
10/10 - 1s - loss: 261.8708 - loglik: -2.2396e+02 - logprior: -3.7753e+01
Epoch 3/10
10/10 - 1s - loss: 236.9762 - loglik: -2.2339e+02 - logprior: -1.3226e+01
Epoch 4/10
10/10 - 1s - loss: 226.6748 - loglik: -2.2329e+02 - logprior: -2.8483e+00
Epoch 5/10
10/10 - 1s - loss: 221.9808 - loglik: -2.2331e+02 - logprior: 1.8876
Epoch 6/10
10/10 - 1s - loss: 219.4052 - loglik: -2.2329e+02 - logprior: 4.4091
Epoch 7/10
10/10 - 1s - loss: 217.7465 - loglik: -2.2311e+02 - logprior: 5.9031
Epoch 8/10
10/10 - 1s - loss: 216.5319 - loglik: -2.2282e+02 - logprior: 6.8770
Epoch 9/10
10/10 - 1s - loss: 215.6286 - loglik: -2.2263e+02 - logprior: 7.6292
Epoch 10/10
10/10 - 1s - loss: 214.8878 - loglik: -2.2253e+02 - logprior: 8.2617
Fitted a model with MAP estimate = -213.9028
Time for alignment: 48.1753
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2055 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3582 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0302 - loglik: -2.5083e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.5251 - loglik: -2.4170e+02 - logprior: -4.7563e+00
Epoch 5/10
10/10 - 1s - loss: 237.1018 - loglik: -2.3502e+02 - logprior: -1.8381e+00
Epoch 6/10
10/10 - 1s - loss: 231.6462 - loglik: -2.3084e+02 - logprior: -2.2992e-01
Epoch 7/10
10/10 - 1s - loss: 228.4634 - loglik: -2.2834e+02 - logprior: 0.6423
Epoch 8/10
10/10 - 1s - loss: 226.5200 - loglik: -2.2700e+02 - logprior: 1.2387
Epoch 9/10
10/10 - 1s - loss: 225.3326 - loglik: -2.2631e+02 - logprior: 1.6543
Epoch 10/10
10/10 - 1s - loss: 224.5341 - loglik: -2.2596e+02 - logprior: 2.0618
Fitted a model with MAP estimate = -223.5530
expansions: [(0, 6), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.7927 - loglik: -2.3530e+02 - logprior: -1.5248e+02
Epoch 2/2
10/10 - 1s - loss: 272.7155 - loglik: -2.2816e+02 - logprior: -4.4415e+01
Fitted a model with MAP estimate = -251.0494
expansions: [(0, 4), (43, 3)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 347.6280 - loglik: -2.2752e+02 - logprior: -1.2008e+02
Epoch 2/2
10/10 - 1s - loss: 257.3854 - loglik: -2.2521e+02 - logprior: -3.2002e+01
Fitted a model with MAP estimate = -240.7811
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 352.4355 - loglik: -2.2540e+02 - logprior: -1.2701e+02
Epoch 2/10
10/10 - 1s - loss: 274.0998 - loglik: -2.2430e+02 - logprior: -4.9641e+01
Epoch 3/10
10/10 - 1s - loss: 256.5511 - loglik: -2.2360e+02 - logprior: -3.2588e+01
Epoch 4/10
10/10 - 1s - loss: 243.7878 - loglik: -2.2331e+02 - logprior: -1.9961e+01
Epoch 5/10
10/10 - 1s - loss: 226.8470 - loglik: -2.2342e+02 - logprior: -2.8853e+00
Epoch 6/10
10/10 - 1s - loss: 220.0478 - loglik: -2.2356e+02 - logprior: 4.0343
Epoch 7/10
10/10 - 1s - loss: 217.9829 - loglik: -2.2324e+02 - logprior: 5.8228
Epoch 8/10
10/10 - 1s - loss: 216.6780 - loglik: -2.2287e+02 - logprior: 6.8296
Epoch 9/10
10/10 - 1s - loss: 215.7074 - loglik: -2.2263e+02 - logprior: 7.5664
Epoch 10/10
10/10 - 1s - loss: 214.9303 - loglik: -2.2247e+02 - logprior: 8.1745
Fitted a model with MAP estimate = -213.9014
Time for alignment: 46.5762
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3582 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0309 - loglik: -2.5084e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.8171 - loglik: -2.4206e+02 - logprior: -4.7029e+00
Epoch 5/10
10/10 - 1s - loss: 237.6658 - loglik: -2.3579e+02 - logprior: -1.7437e+00
Epoch 6/10
10/10 - 1s - loss: 231.9115 - loglik: -2.3127e+02 - logprior: -1.7834e-01
Epoch 7/10
10/10 - 1s - loss: 228.5965 - loglik: -2.2852e+02 - logprior: 0.6632
Epoch 8/10
10/10 - 1s - loss: 226.6010 - loglik: -2.2708e+02 - logprior: 1.2488
Epoch 9/10
10/10 - 1s - loss: 225.3620 - loglik: -2.2633e+02 - logprior: 1.6632
Epoch 10/10
10/10 - 1s - loss: 224.5440 - loglik: -2.2593e+02 - logprior: 2.0295
Fitted a model with MAP estimate = -223.5427
expansions: [(0, 6), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.4383 - loglik: -2.3495e+02 - logprior: -1.5247e+02
Epoch 2/2
10/10 - 1s - loss: 272.5302 - loglik: -2.2791e+02 - logprior: -4.4480e+01
Fitted a model with MAP estimate = -250.9657
expansions: [(0, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.5623 - loglik: -2.2784e+02 - logprior: -1.2070e+02
Epoch 2/2
10/10 - 1s - loss: 258.6169 - loglik: -2.2590e+02 - logprior: -3.2543e+01
Fitted a model with MAP estimate = -242.5703
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 354.5864 - loglik: -2.2587e+02 - logprior: -1.2869e+02
Epoch 2/10
10/10 - 1s - loss: 263.1321 - loglik: -2.2480e+02 - logprior: -3.8179e+01
Epoch 3/10
10/10 - 1s - loss: 238.3013 - loglik: -2.2424e+02 - logprior: -1.3688e+01
Epoch 4/10
10/10 - 1s - loss: 228.0776 - loglik: -2.2420e+02 - logprior: -3.3430e+00
Epoch 5/10
10/10 - 1s - loss: 223.4131 - loglik: -2.2426e+02 - logprior: 1.3962
Epoch 6/10
10/10 - 1s - loss: 220.8223 - loglik: -2.2418e+02 - logprior: 3.8817
Epoch 7/10
10/10 - 1s - loss: 219.1697 - loglik: -2.2397e+02 - logprior: 5.3578
Epoch 8/10
10/10 - 1s - loss: 217.9994 - loglik: -2.2373e+02 - logprior: 6.3381
Epoch 9/10
10/10 - 1s - loss: 217.0911 - loglik: -2.2355e+02 - logprior: 7.0822
Epoch 10/10
10/10 - 1s - loss: 216.3318 - loglik: -2.2339e+02 - logprior: 7.6887
Fitted a model with MAP estimate = -215.3281
Time for alignment: 45.3481
Computed alignments with likelihoods: ['-213.9028', '-213.9014', '-215.3281']
Best model has likelihood: -213.9014
time for generating output: 0.1672
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7229152987524623
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb604c44e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb550260c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb63011cfd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5035c88b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb5035c8ac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb674dd5af0>, <__main__.SimpleDirichletPrior object at 0x7fb525169970>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 997.1023 - loglik: -9.8867e+02 - logprior: -8.3194e+00
Epoch 2/10
19/19 - 20s - loss: 873.2306 - loglik: -8.7304e+02 - logprior: 0.4091
Epoch 3/10
19/19 - 20s - loss: 812.2002 - loglik: -8.1031e+02 - logprior: -1.1002e+00
Epoch 4/10
19/19 - 20s - loss: 799.0287 - loglik: -7.9639e+02 - logprior: -1.6167e+00
Epoch 5/10
19/19 - 20s - loss: 793.3618 - loglik: -7.9086e+02 - logprior: -1.5384e+00
Epoch 6/10
19/19 - 20s - loss: 789.5718 - loglik: -7.8713e+02 - logprior: -1.5566e+00
Epoch 7/10
19/19 - 20s - loss: 788.6004 - loglik: -7.8615e+02 - logprior: -1.6148e+00
Epoch 8/10
19/19 - 20s - loss: 786.7349 - loglik: -7.8430e+02 - logprior: -1.6339e+00
Epoch 9/10
19/19 - 20s - loss: 787.4040 - loglik: -7.8500e+02 - logprior: -1.6535e+00
Fitted a model with MAP estimate = -785.5340
expansions: [(14, 1), (33, 1), (40, 2), (43, 1), (106, 1), (109, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 9), (122, 2), (142, 1), (157, 1), (167, 3), (170, 1), (173, 1), (174, 1), (176, 1), (177, 1), (178, 1), (190, 1), (191, 2), (196, 1), (199, 1), (205, 1), (219, 2), (220, 1), (223, 2), (224, 2), (227, 1), (237, 1), (238, 1), (242, 2), (268, 9), (285, 1), (301, 1), (302, 3), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 827.6288 - loglik: -8.1536e+02 - logprior: -1.2179e+01
Epoch 2/2
19/19 - 26s - loss: 769.6252 - loglik: -7.6502e+02 - logprior: -4.2444e+00
Fitted a model with MAP estimate = -761.2923
expansions: [(0, 2)]
discards: [  0  42 145 146 147 269 293 320 321 322]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 777.3066 - loglik: -7.6992e+02 - logprior: -7.3170e+00
Epoch 2/2
19/19 - 25s - loss: 754.4835 - loglik: -7.5564e+02 - logprior: 1.4653
Fitted a model with MAP estimate = -750.1370
expansions: [(144, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 777.8540 - loglik: -7.6709e+02 - logprior: -1.0691e+01
Epoch 2/10
19/19 - 26s - loss: 759.0888 - loglik: -7.5800e+02 - logprior: -6.7045e-01
Epoch 3/10
19/19 - 25s - loss: 746.9891 - loglik: -7.4871e+02 - logprior: 2.5160
Epoch 4/10
19/19 - 25s - loss: 742.9210 - loglik: -7.4515e+02 - logprior: 3.2398
Epoch 5/10
19/19 - 25s - loss: 740.4590 - loglik: -7.4296e+02 - logprior: 3.5012
Epoch 6/10
19/19 - 26s - loss: 740.0040 - loglik: -7.4283e+02 - logprior: 3.7371
Epoch 7/10
19/19 - 25s - loss: 740.1701 - loglik: -7.4339e+02 - logprior: 4.0403
Fitted a model with MAP estimate = -737.6771
Time for alignment: 545.5594
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 998.2648 - loglik: -9.8983e+02 - logprior: -8.3259e+00
Epoch 2/10
19/19 - 20s - loss: 877.3141 - loglik: -8.7710e+02 - logprior: 0.3832
Epoch 3/10
19/19 - 20s - loss: 818.3188 - loglik: -8.1644e+02 - logprior: -1.1082e+00
Epoch 4/10
19/19 - 20s - loss: 795.0489 - loglik: -7.9225e+02 - logprior: -1.7394e+00
Epoch 5/10
19/19 - 20s - loss: 788.5980 - loglik: -7.8578e+02 - logprior: -1.7022e+00
Epoch 6/10
19/19 - 20s - loss: 787.1890 - loglik: -7.8434e+02 - logprior: -1.8080e+00
Epoch 7/10
19/19 - 20s - loss: 787.0455 - loglik: -7.8422e+02 - logprior: -1.9114e+00
Epoch 8/10
19/19 - 20s - loss: 783.2819 - loglik: -7.8054e+02 - logprior: -1.9235e+00
Epoch 9/10
19/19 - 20s - loss: 783.8513 - loglik: -7.8120e+02 - logprior: -1.8957e+00
Fitted a model with MAP estimate = -780.9584
expansions: [(30, 1), (66, 1), (68, 1), (99, 1), (100, 3), (108, 1), (114, 1), (115, 1), (118, 1), (120, 1), (121, 1), (122, 8), (123, 2), (143, 1), (144, 1), (158, 1), (164, 2), (167, 1), (168, 1), (169, 1), (170, 1), (174, 1), (177, 1), (178, 1), (179, 1), (191, 2), (197, 1), (199, 1), (206, 1), (211, 1), (212, 1), (220, 2), (221, 1), (222, 1), (223, 1), (224, 2), (237, 1), (238, 1), (241, 1), (243, 1), (261, 1), (268, 10), (300, 1), (301, 1), (302, 5), (303, 1)]
discards: [ 0 85]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 822.2407 - loglik: -8.1025e+02 - logprior: -1.1889e+01
Epoch 2/2
19/19 - 26s - loss: 772.8373 - loglik: -7.6909e+02 - logprior: -3.2515e+00
Fitted a model with MAP estimate = -761.1339
expansions: [(0, 2), (228, 1)]
discards: [  0 103 130 144 188 261 266]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 774.6487 - loglik: -7.6753e+02 - logprior: -7.0349e+00
Epoch 2/2
19/19 - 26s - loss: 753.9169 - loglik: -7.5504e+02 - logprior: 1.4306
Fitted a model with MAP estimate = -748.9992
expansions: [(139, 1)]
discards: [  0 143 144 319 320]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 776.7491 - loglik: -7.6640e+02 - logprior: -1.0291e+01
Epoch 2/10
19/19 - 25s - loss: 758.7412 - loglik: -7.5801e+02 - logprior: -3.6121e-01
Epoch 3/10
19/19 - 26s - loss: 749.2092 - loglik: -7.5097e+02 - logprior: 2.6287
Epoch 4/10
19/19 - 25s - loss: 743.0164 - loglik: -7.4522e+02 - logprior: 3.3091
Epoch 5/10
19/19 - 26s - loss: 739.4361 - loglik: -7.4193e+02 - logprior: 3.5624
Epoch 6/10
19/19 - 25s - loss: 741.4273 - loglik: -7.4429e+02 - logprior: 3.8108
Fitted a model with MAP estimate = -738.4790
Time for alignment: 524.1290
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 1000.7358 - loglik: -9.9229e+02 - logprior: -8.3289e+00
Epoch 2/10
19/19 - 20s - loss: 874.0676 - loglik: -8.7391e+02 - logprior: 0.4457
Epoch 3/10
19/19 - 20s - loss: 816.8829 - loglik: -8.1577e+02 - logprior: -4.6710e-01
Epoch 4/10
19/19 - 20s - loss: 797.8993 - loglik: -7.9618e+02 - logprior: -8.4580e-01
Epoch 5/10
19/19 - 20s - loss: 791.1569 - loglik: -7.8923e+02 - logprior: -8.4105e-01
Epoch 6/10
19/19 - 20s - loss: 787.8345 - loglik: -7.8606e+02 - logprior: -9.2150e-01
Epoch 7/10
19/19 - 20s - loss: 790.9490 - loglik: -7.8921e+02 - logprior: -1.0072e+00
Fitted a model with MAP estimate = -785.3112
expansions: [(14, 1), (33, 1), (57, 1), (116, 1), (117, 3), (120, 1), (122, 12), (124, 2), (142, 1), (144, 1), (163, 1), (164, 1), (166, 1), (167, 1), (168, 2), (169, 1), (176, 1), (177, 2), (190, 4), (195, 1), (198, 1), (210, 1), (211, 1), (221, 1), (222, 1), (223, 1), (224, 1), (237, 1), (241, 1), (242, 2), (266, 7), (301, 1), (302, 3), (311, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 809.3228 - loglik: -8.0074e+02 - logprior: -8.4884e+00
Epoch 2/2
19/19 - 25s - loss: 765.1017 - loglik: -7.6489e+02 - logprior: 0.1407
Fitted a model with MAP estimate = -754.2046
expansions: [(374, 1)]
discards: [  0 119 196 291]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 781.4313 - loglik: -7.7028e+02 - logprior: -1.1088e+01
Epoch 2/2
19/19 - 25s - loss: 762.2333 - loglik: -7.5941e+02 - logprior: -2.5133e+00
Fitted a model with MAP estimate = -756.7920
expansions: [(0, 2)]
discards: [  0 143]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 770.7930 - loglik: -7.6371e+02 - logprior: -7.0194e+00
Epoch 2/10
19/19 - 25s - loss: 754.8303 - loglik: -7.5598e+02 - logprior: 1.5489
Epoch 3/10
19/19 - 25s - loss: 747.2244 - loglik: -7.4914e+02 - logprior: 2.7024
Epoch 4/10
19/19 - 25s - loss: 743.6304 - loglik: -7.4578e+02 - logprior: 3.1312
Epoch 5/10
19/19 - 25s - loss: 745.2049 - loglik: -7.4713e+02 - logprior: 2.8956
Fitted a model with MAP estimate = -740.1847
Time for alignment: 450.1089
Computed alignments with likelihoods: ['-737.6771', '-738.4790', '-740.1847']
Best model has likelihood: -737.6771
time for generating output: 0.5133
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.6299579326923077
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb32034fb50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2bc315fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bc315eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb561203c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bc3194c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb2bd77be80>, <__main__.SimpleDirichletPrior object at 0x7fad006b2190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 369.5590 - loglik: -3.1244e+02 - logprior: -5.7095e+01
Epoch 2/10
10/10 - 1s - loss: 291.3383 - loglik: -2.7713e+02 - logprior: -1.4193e+01
Epoch 3/10
10/10 - 1s - loss: 242.6716 - loglik: -2.3588e+02 - logprior: -6.7673e+00
Epoch 4/10
10/10 - 1s - loss: 215.0589 - loglik: -2.1030e+02 - logprior: -4.7008e+00
Epoch 5/10
10/10 - 1s - loss: 204.0140 - loglik: -2.0038e+02 - logprior: -3.5883e+00
Epoch 6/10
10/10 - 1s - loss: 198.4661 - loglik: -1.9530e+02 - logprior: -3.0095e+00
Epoch 7/10
10/10 - 1s - loss: 196.6641 - loglik: -1.9385e+02 - logprior: -2.5078e+00
Epoch 8/10
10/10 - 1s - loss: 195.2150 - loglik: -1.9267e+02 - logprior: -2.2247e+00
Epoch 9/10
10/10 - 1s - loss: 194.7161 - loglik: -1.9242e+02 - logprior: -2.0212e+00
Epoch 10/10
10/10 - 1s - loss: 195.0801 - loglik: -1.9298e+02 - logprior: -1.8498e+00
Fitted a model with MAP estimate = -194.0801
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.2981 - loglik: -1.8584e+02 - logprior: -6.4434e+01
Epoch 2/2
10/10 - 2s - loss: 198.8946 - loglik: -1.7317e+02 - logprior: -2.5602e+01
Fitted a model with MAP estimate = -189.9361
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.5067 - loglik: -1.7062e+02 - logprior: -5.0867e+01
Epoch 2/2
10/10 - 2s - loss: 180.1133 - loglik: -1.6810e+02 - logprior: -1.1936e+01
Fitted a model with MAP estimate = -173.6675
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 231.6267 - loglik: -1.7028e+02 - logprior: -6.1320e+01
Epoch 2/10
10/10 - 2s - loss: 187.0301 - loglik: -1.6898e+02 - logprior: -1.7968e+01
Epoch 3/10
10/10 - 2s - loss: 174.0343 - loglik: -1.6895e+02 - logprior: -5.0236e+00
Epoch 4/10
10/10 - 2s - loss: 169.3768 - loglik: -1.6869e+02 - logprior: -6.1556e-01
Epoch 5/10
10/10 - 2s - loss: 167.1658 - loglik: -1.6822e+02 - logprior: 1.2552
Epoch 6/10
10/10 - 2s - loss: 165.5219 - loglik: -1.6751e+02 - logprior: 2.2734
Epoch 7/10
10/10 - 2s - loss: 165.2929 - loglik: -1.6796e+02 - logprior: 2.9321
Epoch 8/10
10/10 - 2s - loss: 164.6416 - loglik: -1.6787e+02 - logprior: 3.4756
Epoch 9/10
10/10 - 2s - loss: 164.4058 - loglik: -1.6809e+02 - logprior: 3.9430
Epoch 10/10
10/10 - 2s - loss: 164.0961 - loglik: -1.6815e+02 - logprior: 4.3169
Fitted a model with MAP estimate = -163.4835
Time for alignment: 57.4269
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 369.5446 - loglik: -3.1243e+02 - logprior: -5.7095e+01
Epoch 2/10
10/10 - 1s - loss: 291.3577 - loglik: -2.7716e+02 - logprior: -1.4182e+01
Epoch 3/10
10/10 - 1s - loss: 242.8167 - loglik: -2.3612e+02 - logprior: -6.6733e+00
Epoch 4/10
10/10 - 1s - loss: 217.2716 - loglik: -2.1280e+02 - logprior: -4.4131e+00
Epoch 5/10
10/10 - 1s - loss: 206.1925 - loglik: -2.0294e+02 - logprior: -3.2028e+00
Epoch 6/10
10/10 - 1s - loss: 200.4497 - loglik: -1.9763e+02 - logprior: -2.6616e+00
Epoch 7/10
10/10 - 1s - loss: 198.0933 - loglik: -1.9552e+02 - logprior: -2.2858e+00
Epoch 8/10
10/10 - 1s - loss: 196.2974 - loglik: -1.9396e+02 - logprior: -2.0612e+00
Epoch 9/10
10/10 - 1s - loss: 195.7737 - loglik: -1.9360e+02 - logprior: -1.9404e+00
Epoch 10/10
10/10 - 1s - loss: 195.3418 - loglik: -1.9337e+02 - logprior: -1.7528e+00
Fitted a model with MAP estimate = -195.0233
expansions: [(13, 5), (17, 2), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 2), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.7013 - loglik: -1.8627e+02 - logprior: -6.4426e+01
Epoch 2/2
10/10 - 2s - loss: 199.0247 - loglik: -1.7338e+02 - logprior: -2.5590e+01
Fitted a model with MAP estimate = -190.0024
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.4841 - loglik: -1.7061e+02 - logprior: -5.0848e+01
Epoch 2/2
10/10 - 2s - loss: 180.1311 - loglik: -1.6815e+02 - logprior: -1.1912e+01
Fitted a model with MAP estimate = -173.8906
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.4151 - loglik: -1.7011e+02 - logprior: -6.1282e+01
Epoch 2/10
10/10 - 2s - loss: 187.1984 - loglik: -1.6921e+02 - logprior: -1.7907e+01
Epoch 3/10
10/10 - 2s - loss: 173.9876 - loglik: -1.6892e+02 - logprior: -5.0054e+00
Epoch 4/10
10/10 - 2s - loss: 169.4698 - loglik: -1.6884e+02 - logprior: -5.8176e-01
Epoch 5/10
10/10 - 2s - loss: 166.8897 - loglik: -1.6801e+02 - logprior: 1.2543
Epoch 6/10
10/10 - 2s - loss: 165.8077 - loglik: -1.6779e+02 - logprior: 2.2347
Epoch 7/10
10/10 - 2s - loss: 165.4254 - loglik: -1.6805e+02 - logprior: 2.8880
Epoch 8/10
10/10 - 2s - loss: 164.6538 - loglik: -1.6787e+02 - logprior: 3.4471
Epoch 9/10
10/10 - 2s - loss: 164.3478 - loglik: -1.6804e+02 - logprior: 3.9271
Epoch 10/10
10/10 - 2s - loss: 163.9865 - loglik: -1.6804e+02 - logprior: 4.3073
Fitted a model with MAP estimate = -163.5224
Time for alignment: 53.6928
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 369.5055 - loglik: -3.1239e+02 - logprior: -5.7096e+01
Epoch 2/10
10/10 - 1s - loss: 291.0443 - loglik: -2.7684e+02 - logprior: -1.4192e+01
Epoch 3/10
10/10 - 1s - loss: 241.9760 - loglik: -2.3517e+02 - logprior: -6.7831e+00
Epoch 4/10
10/10 - 1s - loss: 213.6759 - loglik: -2.0886e+02 - logprior: -4.7560e+00
Epoch 5/10
10/10 - 1s - loss: 202.8930 - loglik: -1.9927e+02 - logprior: -3.6030e+00
Epoch 6/10
10/10 - 1s - loss: 198.0627 - loglik: -1.9510e+02 - logprior: -2.9272e+00
Epoch 7/10
10/10 - 1s - loss: 195.8370 - loglik: -1.9319e+02 - logprior: -2.5232e+00
Epoch 8/10
10/10 - 1s - loss: 194.6569 - loglik: -1.9213e+02 - logprior: -2.2785e+00
Epoch 9/10
10/10 - 1s - loss: 194.1517 - loglik: -1.9175e+02 - logprior: -2.1365e+00
Epoch 10/10
10/10 - 1s - loss: 193.9157 - loglik: -1.9167e+02 - logprior: -2.0167e+00
Fitted a model with MAP estimate = -193.4345
expansions: [(13, 5), (17, 2), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.6280 - loglik: -1.8614e+02 - logprior: -6.4479e+01
Epoch 2/2
10/10 - 2s - loss: 198.7008 - loglik: -1.7305e+02 - logprior: -2.5597e+01
Fitted a model with MAP estimate = -190.1083
expansions: [(0, 2)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 221.7773 - loglik: -1.7077e+02 - logprior: -5.0987e+01
Epoch 2/2
10/10 - 2s - loss: 180.5662 - loglik: -1.6843e+02 - logprior: -1.2066e+01
Fitted a model with MAP estimate = -174.6538
expansions: [(69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 232.6246 - loglik: -1.7072e+02 - logprior: -6.1877e+01
Epoch 2/10
10/10 - 2s - loss: 189.3179 - loglik: -1.6994e+02 - logprior: -1.9295e+01
Epoch 3/10
10/10 - 2s - loss: 175.1107 - loglik: -1.6943e+02 - logprior: -5.6148e+00
Epoch 4/10
10/10 - 2s - loss: 169.8290 - loglik: -1.6911e+02 - logprior: -6.6332e-01
Epoch 5/10
10/10 - 2s - loss: 167.3018 - loglik: -1.6838e+02 - logprior: 1.2380
Epoch 6/10
10/10 - 2s - loss: 166.1958 - loglik: -1.6818e+02 - logprior: 2.2707
Epoch 7/10
10/10 - 2s - loss: 165.4134 - loglik: -1.6806e+02 - logprior: 2.9323
Epoch 8/10
10/10 - 2s - loss: 165.0181 - loglik: -1.6824e+02 - logprior: 3.4783
Epoch 9/10
10/10 - 2s - loss: 164.6276 - loglik: -1.6831e+02 - logprior: 3.9395
Epoch 10/10
10/10 - 2s - loss: 164.4026 - loglik: -1.6846e+02 - logprior: 4.3144
Fitted a model with MAP estimate = -163.7930
Time for alignment: 52.8625
Computed alignments with likelihoods: ['-163.4835', '-163.5224', '-163.7930']
Best model has likelihood: -163.4835
time for generating output: 0.1807
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9311943393924607
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb52e099cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb53ec097c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bcbbd280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2be7aef70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2be7ae190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4c7977a60>, <__main__.SimpleDirichletPrior object at 0x7fad101e8b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.6925 - loglik: -3.8843e+02 - logprior: -8.2168e+00
Epoch 2/10
13/13 - 3s - loss: 354.6527 - loglik: -3.5236e+02 - logprior: -1.9828e+00
Epoch 3/10
13/13 - 3s - loss: 327.6813 - loglik: -3.2536e+02 - logprior: -1.7435e+00
Epoch 4/10
13/13 - 3s - loss: 312.1227 - loglik: -3.0908e+02 - logprior: -2.0958e+00
Epoch 5/10
13/13 - 3s - loss: 306.9101 - loglik: -3.0394e+02 - logprior: -2.1154e+00
Epoch 6/10
13/13 - 3s - loss: 303.9308 - loglik: -3.0130e+02 - logprior: -2.0663e+00
Epoch 7/10
13/13 - 3s - loss: 303.0977 - loglik: -3.0056e+02 - logprior: -2.0811e+00
Epoch 8/10
13/13 - 3s - loss: 302.9094 - loglik: -3.0042e+02 - logprior: -2.0679e+00
Epoch 9/10
13/13 - 3s - loss: 301.9813 - loglik: -2.9950e+02 - logprior: -2.0475e+00
Epoch 10/10
13/13 - 3s - loss: 301.4355 - loglik: -2.9896e+02 - logprior: -2.0518e+00
Fitted a model with MAP estimate = -301.3728
expansions: [(12, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (45, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (82, 1), (83, 1), (84, 1), (93, 4), (100, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 323.5743 - loglik: -3.1378e+02 - logprior: -9.7587e+00
Epoch 2/2
13/13 - 3s - loss: 302.2689 - loglik: -2.9746e+02 - logprior: -4.6086e+00
Fitted a model with MAP estimate = -297.3389
expansions: [(0, 2)]
discards: [  0  30  34  40  83 120 121 129]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 304.3362 - loglik: -2.9697e+02 - logprior: -7.3368e+00
Epoch 2/2
13/13 - 3s - loss: 294.1150 - loglik: -2.9190e+02 - logprior: -1.9954e+00
Fitted a model with MAP estimate = -291.3557
expansions: [(124, 1)]
discards: [ 0 24 74]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 305.6003 - loglik: -2.9644e+02 - logprior: -9.1127e+00
Epoch 2/10
13/13 - 3s - loss: 294.9323 - loglik: -2.9183e+02 - logprior: -2.8687e+00
Epoch 3/10
13/13 - 3s - loss: 290.7667 - loglik: -2.8901e+02 - logprior: -1.3206e+00
Epoch 4/10
13/13 - 3s - loss: 289.3063 - loglik: -2.8770e+02 - logprior: -1.0042e+00
Epoch 5/10
13/13 - 3s - loss: 287.3063 - loglik: -2.8587e+02 - logprior: -8.1230e-01
Epoch 6/10
13/13 - 3s - loss: 287.3283 - loglik: -2.8603e+02 - logprior: -7.2845e-01
Fitted a model with MAP estimate = -286.4306
Time for alignment: 88.5082
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 396.6316 - loglik: -3.8836e+02 - logprior: -8.2262e+00
Epoch 2/10
13/13 - 3s - loss: 354.4990 - loglik: -3.5220e+02 - logprior: -1.9868e+00
Epoch 3/10
13/13 - 3s - loss: 325.5571 - loglik: -3.2337e+02 - logprior: -1.7079e+00
Epoch 4/10
13/13 - 3s - loss: 313.0862 - loglik: -3.1026e+02 - logprior: -2.0179e+00
Epoch 5/10
13/13 - 3s - loss: 307.2362 - loglik: -3.0429e+02 - logprior: -2.0639e+00
Epoch 6/10
13/13 - 3s - loss: 304.4425 - loglik: -3.0183e+02 - logprior: -2.0071e+00
Epoch 7/10
13/13 - 3s - loss: 303.0908 - loglik: -3.0062e+02 - logprior: -2.0017e+00
Epoch 8/10
13/13 - 3s - loss: 302.3696 - loglik: -2.9998e+02 - logprior: -1.9692e+00
Epoch 9/10
13/13 - 3s - loss: 302.0956 - loglik: -2.9974e+02 - logprior: -1.9699e+00
Epoch 10/10
13/13 - 3s - loss: 301.6731 - loglik: -2.9933e+02 - logprior: -1.9739e+00
Fitted a model with MAP estimate = -301.1589
expansions: [(12, 1), (19, 1), (22, 2), (24, 1), (25, 2), (26, 1), (27, 3), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (80, 1), (81, 1), (82, 2), (83, 2), (100, 2), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 322.0482 - loglik: -3.1228e+02 - logprior: -9.7314e+00
Epoch 2/2
13/13 - 3s - loss: 301.3025 - loglik: -2.9652e+02 - logprior: -4.5787e+00
Fitted a model with MAP estimate = -296.5808
expansions: [(0, 2)]
discards: [ 0 23 30 35 76 77 83]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 302.0255 - loglik: -2.9465e+02 - logprior: -7.3386e+00
Epoch 2/2
13/13 - 3s - loss: 292.2338 - loglik: -2.9003e+02 - logprior: -1.9733e+00
Fitted a model with MAP estimate = -289.8008
expansions: []
discards: [ 0 98]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.3498 - loglik: -2.9535e+02 - logprior: -8.9610e+00
Epoch 2/10
13/13 - 3s - loss: 293.5668 - loglik: -2.9063e+02 - logprior: -2.7113e+00
Epoch 3/10
13/13 - 3s - loss: 289.8160 - loglik: -2.8810e+02 - logprior: -1.2808e+00
Epoch 4/10
13/13 - 3s - loss: 288.3279 - loglik: -2.8675e+02 - logprior: -9.9201e-01
Epoch 5/10
13/13 - 3s - loss: 286.6590 - loglik: -2.8524e+02 - logprior: -8.0406e-01
Epoch 6/10
13/13 - 3s - loss: 286.9252 - loglik: -2.8562e+02 - logprior: -7.4858e-01
Fitted a model with MAP estimate = -285.8020
Time for alignment: 88.4181
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.8280 - loglik: -3.8856e+02 - logprior: -8.2236e+00
Epoch 2/10
13/13 - 3s - loss: 354.8691 - loglik: -3.5257e+02 - logprior: -1.9791e+00
Epoch 3/10
13/13 - 3s - loss: 326.5656 - loglik: -3.2424e+02 - logprior: -1.7043e+00
Epoch 4/10
13/13 - 3s - loss: 312.2007 - loglik: -3.0917e+02 - logprior: -2.0066e+00
Epoch 5/10
13/13 - 3s - loss: 307.2596 - loglik: -3.0444e+02 - logprior: -1.9526e+00
Epoch 6/10
13/13 - 3s - loss: 304.9460 - loglik: -3.0245e+02 - logprior: -1.8952e+00
Epoch 7/10
13/13 - 3s - loss: 303.5314 - loglik: -3.0110e+02 - logprior: -1.9464e+00
Epoch 8/10
13/13 - 3s - loss: 302.6223 - loglik: -3.0028e+02 - logprior: -1.9334e+00
Epoch 9/10
13/13 - 3s - loss: 302.6269 - loglik: -3.0033e+02 - logprior: -1.9086e+00
Fitted a model with MAP estimate = -301.8050
expansions: [(12, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 2), (80, 1), (81, 2), (82, 1), (99, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 322.2208 - loglik: -3.1248e+02 - logprior: -9.7039e+00
Epoch 2/2
13/13 - 3s - loss: 301.9138 - loglik: -2.9718e+02 - logprior: -4.5345e+00
Fitted a model with MAP estimate = -296.4974
expansions: [(0, 2)]
discards: [  0  35  39  82 102]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 301.1433 - loglik: -2.9375e+02 - logprior: -7.3534e+00
Epoch 2/2
13/13 - 3s - loss: 292.3081 - loglik: -2.9007e+02 - logprior: -2.0134e+00
Fitted a model with MAP estimate = -289.1197
expansions: []
discards: [ 0 23 74]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 303.2286 - loglik: -2.9424e+02 - logprior: -8.9468e+00
Epoch 2/10
13/13 - 3s - loss: 293.3994 - loglik: -2.9049e+02 - logprior: -2.6771e+00
Epoch 3/10
13/13 - 3s - loss: 289.2980 - loglik: -2.8758e+02 - logprior: -1.2773e+00
Epoch 4/10
13/13 - 3s - loss: 287.3058 - loglik: -2.8575e+02 - logprior: -9.5857e-01
Epoch 5/10
13/13 - 3s - loss: 286.4813 - loglik: -2.8509e+02 - logprior: -7.7071e-01
Epoch 6/10
13/13 - 3s - loss: 286.7321 - loglik: -2.8545e+02 - logprior: -7.1742e-01
Fitted a model with MAP estimate = -285.3824
Time for alignment: 87.3967
Computed alignments with likelihoods: ['-286.4306', '-285.8020', '-285.3824']
Best model has likelihood: -285.3824
time for generating output: 0.2374
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8602581219403649
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6040bc6d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb525154580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb60ecdcd30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb60ecdce50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54fef1250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6061a53d0>, <__main__.SimpleDirichletPrior object at 0x7fb4c7525c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.5818 - loglik: -2.1389e+02 - logprior: -8.6883e+00
Epoch 2/10
13/13 - 2s - loss: 173.1835 - loglik: -1.7080e+02 - logprior: -2.3188e+00
Epoch 3/10
13/13 - 2s - loss: 144.0370 - loglik: -1.4209e+02 - logprior: -1.8794e+00
Epoch 4/10
13/13 - 2s - loss: 136.6905 - loglik: -1.3487e+02 - logprior: -1.7971e+00
Epoch 5/10
13/13 - 2s - loss: 134.5478 - loglik: -1.3267e+02 - logprior: -1.7072e+00
Epoch 6/10
13/13 - 2s - loss: 133.5692 - loglik: -1.3162e+02 - logprior: -1.7212e+00
Epoch 7/10
13/13 - 2s - loss: 133.1335 - loglik: -1.3126e+02 - logprior: -1.6809e+00
Epoch 8/10
13/13 - 2s - loss: 132.9451 - loglik: -1.3105e+02 - logprior: -1.6978e+00
Epoch 9/10
13/13 - 2s - loss: 132.5279 - loglik: -1.3066e+02 - logprior: -1.6737e+00
Epoch 10/10
13/13 - 2s - loss: 132.5018 - loglik: -1.3064e+02 - logprior: -1.6805e+00
Fitted a model with MAP estimate = -132.1664
expansions: [(0, 4), (13, 1), (16, 1), (34, 1), (35, 2), (36, 2), (37, 2), (42, 1), (43, 2), (44, 2), (45, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 137.8992 - loglik: -1.2747e+02 - logprior: -1.0395e+01
Epoch 2/2
13/13 - 2s - loss: 119.5674 - loglik: -1.1605e+02 - logprior: -3.3385e+00
Fitted a model with MAP estimate = -115.4963
expansions: [(0, 2)]
discards: [57 61 62 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 128.3222 - loglik: -1.1810e+02 - logprior: -1.0197e+01
Epoch 2/2
13/13 - 2s - loss: 118.6391 - loglik: -1.1512e+02 - logprior: -3.3752e+00
Fitted a model with MAP estimate = -116.4621
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 122.8928 - loglik: -1.1479e+02 - logprior: -8.0771e+00
Epoch 2/10
13/13 - 2s - loss: 116.8180 - loglik: -1.1428e+02 - logprior: -2.3942e+00
Epoch 3/10
13/13 - 2s - loss: 115.4561 - loglik: -1.1336e+02 - logprior: -1.8745e+00
Epoch 4/10
13/13 - 2s - loss: 114.7224 - loglik: -1.1297e+02 - logprior: -1.5215e+00
Epoch 5/10
13/13 - 2s - loss: 114.4180 - loglik: -1.1274e+02 - logprior: -1.4522e+00
Epoch 6/10
13/13 - 2s - loss: 114.1277 - loglik: -1.1251e+02 - logprior: -1.3878e+00
Epoch 7/10
13/13 - 2s - loss: 113.7322 - loglik: -1.1213e+02 - logprior: -1.3645e+00
Epoch 8/10
13/13 - 2s - loss: 113.9622 - loglik: -1.1239e+02 - logprior: -1.3282e+00
Fitted a model with MAP estimate = -113.4860
Time for alignment: 66.0898
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.8875 - loglik: -2.1420e+02 - logprior: -8.6874e+00
Epoch 2/10
13/13 - 2s - loss: 172.9758 - loglik: -1.7060e+02 - logprior: -2.3181e+00
Epoch 3/10
13/13 - 2s - loss: 145.0550 - loglik: -1.4313e+02 - logprior: -1.8518e+00
Epoch 4/10
13/13 - 2s - loss: 137.1651 - loglik: -1.3539e+02 - logprior: -1.7489e+00
Epoch 5/10
13/13 - 2s - loss: 134.5359 - loglik: -1.3268e+02 - logprior: -1.6789e+00
Epoch 6/10
13/13 - 2s - loss: 133.5903 - loglik: -1.3165e+02 - logprior: -1.6905e+00
Epoch 7/10
13/13 - 2s - loss: 132.9588 - loglik: -1.3110e+02 - logprior: -1.6500e+00
Epoch 8/10
13/13 - 2s - loss: 132.8533 - loglik: -1.3100e+02 - logprior: -1.6423e+00
Epoch 9/10
13/13 - 2s - loss: 132.4175 - loglik: -1.3054e+02 - logprior: -1.6514e+00
Epoch 10/10
13/13 - 2s - loss: 132.4938 - loglik: -1.3063e+02 - logprior: -1.6396e+00
Fitted a model with MAP estimate = -132.0211
expansions: [(0, 4), (13, 1), (34, 1), (35, 3), (36, 1), (37, 1), (39, 1), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 137.9186 - loglik: -1.2752e+02 - logprior: -1.0365e+01
Epoch 2/2
13/13 - 2s - loss: 121.4087 - loglik: -1.1802e+02 - logprior: -3.2268e+00
Fitted a model with MAP estimate = -118.4338
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 126.6965 - loglik: -1.1645e+02 - logprior: -1.0213e+01
Epoch 2/2
13/13 - 2s - loss: 117.5468 - loglik: -1.1402e+02 - logprior: -3.3791e+00
Fitted a model with MAP estimate = -115.4991
expansions: []
discards: [58]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 123.1307 - loglik: -1.1501e+02 - logprior: -8.0923e+00
Epoch 2/10
13/13 - 2s - loss: 116.5209 - loglik: -1.1395e+02 - logprior: -2.4292e+00
Epoch 3/10
13/13 - 2s - loss: 115.1617 - loglik: -1.1302e+02 - logprior: -1.9143e+00
Epoch 4/10
13/13 - 2s - loss: 114.5878 - loglik: -1.1278e+02 - logprior: -1.5653e+00
Epoch 5/10
13/13 - 2s - loss: 114.3102 - loglik: -1.1259e+02 - logprior: -1.4871e+00
Epoch 6/10
13/13 - 2s - loss: 113.8438 - loglik: -1.1217e+02 - logprior: -1.4335e+00
Epoch 7/10
13/13 - 2s - loss: 113.4127 - loglik: -1.1176e+02 - logprior: -1.4055e+00
Epoch 8/10
13/13 - 2s - loss: 114.0835 - loglik: -1.1245e+02 - logprior: -1.3724e+00
Fitted a model with MAP estimate = -113.2765
Time for alignment: 67.2366
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.9827 - loglik: -2.1429e+02 - logprior: -8.6858e+00
Epoch 2/10
13/13 - 2s - loss: 174.6618 - loglik: -1.7228e+02 - logprior: -2.3225e+00
Epoch 3/10
13/13 - 2s - loss: 146.2603 - loglik: -1.4431e+02 - logprior: -1.8852e+00
Epoch 4/10
13/13 - 2s - loss: 137.7953 - loglik: -1.3593e+02 - logprior: -1.7983e+00
Epoch 5/10
13/13 - 2s - loss: 134.2676 - loglik: -1.3234e+02 - logprior: -1.6957e+00
Epoch 6/10
13/13 - 2s - loss: 134.1585 - loglik: -1.3220e+02 - logprior: -1.7407e+00
Epoch 7/10
13/13 - 2s - loss: 133.1730 - loglik: -1.3129e+02 - logprior: -1.7104e+00
Epoch 8/10
13/13 - 2s - loss: 132.5641 - loglik: -1.3065e+02 - logprior: -1.7249e+00
Epoch 9/10
13/13 - 2s - loss: 132.7039 - loglik: -1.3080e+02 - logprior: -1.7099e+00
Fitted a model with MAP estimate = -132.0137
expansions: [(0, 4), (13, 1), (16, 1), (34, 1), (35, 2), (36, 2), (37, 1), (42, 1), (43, 2), (44, 2), (45, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 137.9796 - loglik: -1.2760e+02 - logprior: -1.0342e+01
Epoch 2/2
13/13 - 2s - loss: 120.4512 - loglik: -1.1694e+02 - logprior: -3.3384e+00
Fitted a model with MAP estimate = -116.4996
expansions: [(0, 2), (46, 1), (63, 1)]
discards: [42 56]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 124.4235 - loglik: -1.1412e+02 - logprior: -1.0278e+01
Epoch 2/2
13/13 - 2s - loss: 115.2971 - loglik: -1.1167e+02 - logprior: -3.4733e+00
Fitted a model with MAP estimate = -112.6072
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 119.5200 - loglik: -1.1128e+02 - logprior: -8.2096e+00
Epoch 2/10
13/13 - 2s - loss: 112.7240 - loglik: -1.1005e+02 - logprior: -2.5289e+00
Epoch 3/10
13/13 - 2s - loss: 111.4810 - loglik: -1.0923e+02 - logprior: -2.0103e+00
Epoch 4/10
13/13 - 2s - loss: 111.0673 - loglik: -1.0916e+02 - logprior: -1.6572e+00
Epoch 5/10
13/13 - 2s - loss: 110.6131 - loglik: -1.0878e+02 - logprior: -1.5848e+00
Epoch 6/10
13/13 - 2s - loss: 110.2254 - loglik: -1.0844e+02 - logprior: -1.5272e+00
Epoch 7/10
13/13 - 2s - loss: 110.1298 - loglik: -1.0837e+02 - logprior: -1.5008e+00
Epoch 8/10
13/13 - 2s - loss: 110.1170 - loglik: -1.0839e+02 - logprior: -1.4615e+00
Epoch 9/10
13/13 - 2s - loss: 109.8925 - loglik: -1.0821e+02 - logprior: -1.4172e+00
Epoch 10/10
13/13 - 2s - loss: 109.8718 - loglik: -1.0822e+02 - logprior: -1.3793e+00
Fitted a model with MAP estimate = -109.4534
Time for alignment: 67.8587
Computed alignments with likelihoods: ['-113.4860', '-113.2765', '-109.4534']
Best model has likelihood: -109.4534
time for generating output: 0.2213
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5005645464809936
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb605ebadf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6646515b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2dc2717c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57af937c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64ad9ac70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb60550bb80>, <__main__.SimpleDirichletPrior object at 0x7fb524ed2e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.3660 - loglik: -5.8469e+01 - logprior: -8.8627e-01
Epoch 2/10
41/41 - 2s - loss: 46.3787 - loglik: -4.5464e+01 - logprior: -8.9750e-01
Epoch 3/10
41/41 - 2s - loss: 45.6143 - loglik: -4.4674e+01 - logprior: -8.8521e-01
Epoch 4/10
41/41 - 2s - loss: 45.4357 - loglik: -4.4426e+01 - logprior: -8.8333e-01
Epoch 5/10
41/41 - 2s - loss: 45.0202 - loglik: -4.3983e+01 - logprior: -8.8155e-01
Epoch 6/10
41/41 - 2s - loss: 45.1483 - loglik: -4.4095e+01 - logprior: -8.7953e-01
Fitted a model with MAP estimate = -44.5964
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 45.4756 - loglik: -4.4302e+01 - logprior: -1.0891e+00
Epoch 2/2
41/41 - 2s - loss: 43.5567 - loglik: -4.2578e+01 - logprior: -8.7294e-01
Fitted a model with MAP estimate = -42.7341
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.0996 - loglik: -4.2946e+01 - logprior: -1.0616e+00
Epoch 2/2
41/41 - 2s - loss: 43.5070 - loglik: -4.2554e+01 - logprior: -8.4202e-01
Fitted a model with MAP estimate = -42.6957
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.7704 - loglik: -4.1987e+01 - logprior: -6.8261e-01
Epoch 2/10
58/58 - 2s - loss: 42.2926 - loglik: -4.1592e+01 - logprior: -5.8689e-01
Epoch 3/10
58/58 - 3s - loss: 42.2455 - loglik: -4.1549e+01 - logprior: -5.8163e-01
Epoch 4/10
58/58 - 3s - loss: 41.8845 - loglik: -4.1141e+01 - logprior: -5.7724e-01
Epoch 5/10
58/58 - 2s - loss: 41.8697 - loglik: -4.1119e+01 - logprior: -5.7482e-01
Epoch 6/10
58/58 - 3s - loss: 41.7686 - loglik: -4.1021e+01 - logprior: -5.7224e-01
Epoch 7/10
58/58 - 3s - loss: 41.5619 - loglik: -4.0805e+01 - logprior: -5.7249e-01
Epoch 8/10
58/58 - 2s - loss: 41.6096 - loglik: -4.0849e+01 - logprior: -5.7043e-01
Fitted a model with MAP estimate = -41.3374
Time for alignment: 86.3169
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 7s - loss: 59.4351 - loglik: -5.8621e+01 - logprior: -8.0411e-01
Epoch 2/10
41/41 - 2s - loss: 46.3416 - loglik: -4.5592e+01 - logprior: -7.3965e-01
Epoch 3/10
41/41 - 2s - loss: 45.5922 - loglik: -4.4854e+01 - logprior: -6.9593e-01
Epoch 4/10
41/41 - 2s - loss: 45.4638 - loglik: -4.4678e+01 - logprior: -6.9363e-01
Epoch 5/10
41/41 - 2s - loss: 45.1640 - loglik: -4.4344e+01 - logprior: -6.9213e-01
Epoch 6/10
41/41 - 2s - loss: 45.0594 - loglik: -4.4219e+01 - logprior: -6.9055e-01
Epoch 7/10
41/41 - 2s - loss: 44.9759 - loglik: -4.4124e+01 - logprior: -6.8966e-01
Epoch 8/10
41/41 - 2s - loss: 44.9196 - loglik: -4.4058e+01 - logprior: -6.8914e-01
Epoch 9/10
41/41 - 2s - loss: 44.8620 - loglik: -4.3995e+01 - logprior: -6.8839e-01
Epoch 10/10
41/41 - 2s - loss: 44.8090 - loglik: -4.3932e+01 - logprior: -6.8786e-01
Fitted a model with MAP estimate = -44.6732
expansions: [(4, 1), (8, 2), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 47.6399 - loglik: -4.6425e+01 - logprior: -1.1175e+00
Epoch 2/2
41/41 - 2s - loss: 43.6035 - loglik: -4.2616e+01 - logprior: -8.8146e-01
Fitted a model with MAP estimate = -42.6761
expansions: []
discards: [ 8 12 16]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1613 - loglik: -4.3013e+01 - logprior: -1.0595e+00
Epoch 2/2
41/41 - 2s - loss: 43.5186 - loglik: -4.2567e+01 - logprior: -8.4215e-01
Fitted a model with MAP estimate = -42.6655
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7010 - loglik: -4.1917e+01 - logprior: -6.8427e-01
Epoch 2/10
58/58 - 3s - loss: 42.2628 - loglik: -4.1561e+01 - logprior: -5.8598e-01
Epoch 3/10
58/58 - 2s - loss: 42.3266 - loglik: -4.1631e+01 - logprior: -5.7893e-01
Fitted a model with MAP estimate = -41.7879
Time for alignment: 80.1787
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 7s - loss: 60.1620 - loglik: -5.9307e+01 - logprior: -8.4454e-01
Epoch 2/10
41/41 - 2s - loss: 46.8159 - loglik: -4.5763e+01 - logprior: -9.1481e-01
Epoch 3/10
41/41 - 2s - loss: 45.5392 - loglik: -4.4533e+01 - logprior: -8.8760e-01
Epoch 4/10
41/41 - 2s - loss: 45.4371 - loglik: -4.4433e+01 - logprior: -8.8076e-01
Epoch 5/10
41/41 - 2s - loss: 45.2003 - loglik: -4.4159e+01 - logprior: -8.8049e-01
Epoch 6/10
41/41 - 2s - loss: 44.9442 - loglik: -4.3879e+01 - logprior: -8.7919e-01
Epoch 7/10
41/41 - 2s - loss: 44.9941 - loglik: -4.3927e+01 - logprior: -8.7613e-01
Fitted a model with MAP estimate = -44.7266
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 45.5297 - loglik: -4.4346e+01 - logprior: -1.0880e+00
Epoch 2/2
41/41 - 1s - loss: 43.5490 - loglik: -4.2565e+01 - logprior: -8.7261e-01
Fitted a model with MAP estimate = -42.6870
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1711 - loglik: -4.3020e+01 - logprior: -1.0598e+00
Epoch 2/2
41/41 - 2s - loss: 43.4954 - loglik: -4.2538e+01 - logprior: -8.4411e-01
Fitted a model with MAP estimate = -42.6784
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 8s - loss: 42.7477 - loglik: -4.1964e+01 - logprior: -6.8412e-01
Epoch 2/10
58/58 - 2s - loss: 42.2787 - loglik: -4.1580e+01 - logprior: -5.8674e-01
Epoch 3/10
58/58 - 3s - loss: 42.2826 - loglik: -4.1590e+01 - logprior: -5.7882e-01
Fitted a model with MAP estimate = -41.7926
Time for alignment: 76.6467
Computed alignments with likelihoods: ['-41.3374', '-41.7879', '-41.7926']
Best model has likelihood: -41.3374
time for generating output: 0.1041
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb6032ff850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb54f7d4a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4d0215100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2bc4ecbe0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bc4ec3a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb65b838880>, <__main__.SimpleDirichletPrior object at 0x7fb4e9f741c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7861 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9486 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9368 - loglik: -1.0408e+02 - logprior: -6.8504e+00
Epoch 6/10
10/10 - 1s - loss: 107.0554 - loglik: -1.0219e+02 - logprior: -4.8416e+00
Epoch 7/10
10/10 - 1s - loss: 104.9860 - loglik: -1.0118e+02 - logprior: -3.6111e+00
Epoch 8/10
10/10 - 1s - loss: 104.0239 - loglik: -1.0095e+02 - logprior: -2.7470e+00
Epoch 9/10
10/10 - 1s - loss: 103.4343 - loglik: -1.0099e+02 - logprior: -2.1725e+00
Epoch 10/10
10/10 - 1s - loss: 103.0281 - loglik: -1.0106e+02 - logprior: -1.7384e+00
Fitted a model with MAP estimate = -102.6342
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.8920 - loglik: -1.0169e+02 - logprior: -1.4018e+02
Epoch 2/2
10/10 - 1s - loss: 153.7401 - loglik: -9.5051e+01 - logprior: -5.8602e+01
Fitted a model with MAP estimate = -138.9087
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.4424 - loglik: -9.1629e+01 - logprior: -1.1280e+02
Epoch 2/2
10/10 - 1s - loss: 120.8508 - loglik: -8.9699e+01 - logprior: -3.1096e+01
Fitted a model with MAP estimate = -108.4150
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 222.7250 - loglik: -9.2224e+01 - logprior: -1.3048e+02
Epoch 2/10
10/10 - 1s - loss: 129.8095 - loglik: -9.1505e+01 - logprior: -3.8228e+01
Epoch 3/10
10/10 - 1s - loss: 106.9362 - loglik: -9.1395e+01 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 98.8546 - loglik: -9.1475e+01 - logprior: -7.1735e+00
Epoch 5/10
10/10 - 1s - loss: 94.8876 - loglik: -9.1598e+01 - logprior: -3.0516e+00
Epoch 6/10
10/10 - 1s - loss: 92.6898 - loglik: -9.1692e+01 - logprior: -7.4353e-01
Epoch 7/10
10/10 - 1s - loss: 91.3886 - loglik: -9.1755e+01 - logprior: 0.6399
Epoch 8/10
10/10 - 1s - loss: 90.5434 - loglik: -9.1805e+01 - logprior: 1.5511
Epoch 9/10
10/10 - 1s - loss: 89.9489 - loglik: -9.1874e+01 - logprior: 2.2247
Epoch 10/10
10/10 - 1s - loss: 89.4808 - loglik: -9.1964e+01 - logprior: 2.7858
Fitted a model with MAP estimate = -88.9492
Time for alignment: 33.5570
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6814 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7861 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9370 - loglik: -1.0408e+02 - logprior: -6.8503e+00
Epoch 6/10
10/10 - 1s - loss: 107.0791 - loglik: -1.0221e+02 - logprior: -4.8380e+00
Epoch 7/10
10/10 - 1s - loss: 105.0411 - loglik: -1.0125e+02 - logprior: -3.5861e+00
Epoch 8/10
10/10 - 1s - loss: 104.0867 - loglik: -1.0101e+02 - logprior: -2.7184e+00
Epoch 9/10
10/10 - 1s - loss: 103.5112 - loglik: -1.0107e+02 - logprior: -2.1354e+00
Epoch 10/10
10/10 - 1s - loss: 103.1129 - loglik: -1.0114e+02 - logprior: -1.7055e+00
Fitted a model with MAP estimate = -102.6795
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.8760 - loglik: -1.0169e+02 - logprior: -1.4016e+02
Epoch 2/2
10/10 - 1s - loss: 153.7498 - loglik: -9.5036e+01 - logprior: -5.8602e+01
Fitted a model with MAP estimate = -138.8885
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.4636 - loglik: -9.1628e+01 - logprior: -1.1281e+02
Epoch 2/2
10/10 - 1s - loss: 120.8620 - loglik: -8.9667e+01 - logprior: -3.1101e+01
Fitted a model with MAP estimate = -108.3932
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 222.7415 - loglik: -9.2221e+01 - logprior: -1.3050e+02
Epoch 2/10
10/10 - 1s - loss: 129.8095 - loglik: -9.1482e+01 - logprior: -3.8245e+01
Epoch 3/10
10/10 - 1s - loss: 106.9258 - loglik: -9.1385e+01 - logprior: -1.5392e+01
Epoch 4/10
10/10 - 1s - loss: 98.8342 - loglik: -9.1460e+01 - logprior: -7.1713e+00
Epoch 5/10
10/10 - 1s - loss: 94.8690 - loglik: -9.1577e+01 - logprior: -3.0500e+00
Epoch 6/10
10/10 - 1s - loss: 92.6811 - loglik: -9.1690e+01 - logprior: -7.3954e-01
Epoch 7/10
10/10 - 1s - loss: 91.3832 - loglik: -9.1753e+01 - logprior: 0.6452
Epoch 8/10
10/10 - 1s - loss: 90.5389 - loglik: -9.1800e+01 - logprior: 1.5524
Epoch 9/10
10/10 - 1s - loss: 89.9455 - loglik: -9.1873e+01 - logprior: 2.2274
Epoch 10/10
10/10 - 1s - loss: 89.4776 - loglik: -9.1961e+01 - logprior: 2.7866
Fitted a model with MAP estimate = -88.9466
Time for alignment: 33.1567
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7861 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9310 - loglik: -1.0407e+02 - logprior: -6.8533e+00
Epoch 6/10
10/10 - 1s - loss: 106.9958 - loglik: -1.0208e+02 - logprior: -4.8646e+00
Epoch 7/10
10/10 - 1s - loss: 105.0225 - loglik: -1.0118e+02 - logprior: -3.5889e+00
Epoch 8/10
10/10 - 1s - loss: 104.0827 - loglik: -1.0100e+02 - logprior: -2.7247e+00
Epoch 9/10
10/10 - 1s - loss: 103.5136 - loglik: -1.0107e+02 - logprior: -2.1425e+00
Epoch 10/10
10/10 - 1s - loss: 103.1157 - loglik: -1.0114e+02 - logprior: -1.7081e+00
Fitted a model with MAP estimate = -102.6719
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (25, 1), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.9753 - loglik: -1.0183e+02 - logprior: -1.4012e+02
Epoch 2/2
10/10 - 1s - loss: 153.4814 - loglik: -9.4775e+01 - logprior: -5.8594e+01
Fitted a model with MAP estimate = -138.7280
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 204.8288 - loglik: -9.1984e+01 - logprior: -1.1282e+02
Epoch 2/2
10/10 - 1s - loss: 121.5301 - loglik: -9.0436e+01 - logprior: -3.1007e+01
Fitted a model with MAP estimate = -109.1319
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.1958 - loglik: -9.3254e+01 - logprior: -1.3092e+02
Epoch 2/10
10/10 - 1s - loss: 131.4000 - loglik: -9.2705e+01 - logprior: -3.8620e+01
Epoch 3/10
10/10 - 1s - loss: 108.0661 - loglik: -9.2611e+01 - logprior: -1.5325e+01
Epoch 4/10
10/10 - 1s - loss: 99.8331 - loglik: -9.2647e+01 - logprior: -7.0086e+00
Epoch 5/10
10/10 - 1s - loss: 95.8560 - loglik: -9.2759e+01 - logprior: -2.8796e+00
Epoch 6/10
10/10 - 1s - loss: 93.6606 - loglik: -9.2869e+01 - logprior: -5.5731e-01
Epoch 7/10
10/10 - 1s - loss: 92.3643 - loglik: -9.2962e+01 - logprior: 0.8478
Epoch 8/10
10/10 - 1s - loss: 91.5234 - loglik: -9.3020e+01 - logprior: 1.7600
Epoch 9/10
10/10 - 1s - loss: 90.9186 - loglik: -9.3086e+01 - logprior: 2.4416
Epoch 10/10
10/10 - 1s - loss: 90.4436 - loglik: -9.3165e+01 - logprior: 2.9984
Fitted a model with MAP estimate = -89.9362
Time for alignment: 32.4577
Computed alignments with likelihoods: ['-88.9492', '-88.9466', '-89.9362']
Best model has likelihood: -88.9466
time for generating output: 0.1187
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8942992874109263
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb65b8e14f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4fb19b0a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61f4b61c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb50382ad90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb64adf9760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6039ae430>, <__main__.SimpleDirichletPrior object at 0x7fb6038e7e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.4901 - loglik: -1.9232e+02 - logprior: -3.1573e+00
Epoch 2/10
19/19 - 2s - loss: 155.0710 - loglik: -1.5377e+02 - logprior: -1.2746e+00
Epoch 3/10
19/19 - 2s - loss: 141.2988 - loglik: -1.3973e+02 - logprior: -1.3728e+00
Epoch 4/10
19/19 - 2s - loss: 139.3436 - loglik: -1.3789e+02 - logprior: -1.3398e+00
Epoch 5/10
19/19 - 2s - loss: 138.6343 - loglik: -1.3724e+02 - logprior: -1.2844e+00
Epoch 6/10
19/19 - 2s - loss: 138.3047 - loglik: -1.3693e+02 - logprior: -1.2624e+00
Epoch 7/10
19/19 - 2s - loss: 138.1839 - loglik: -1.3683e+02 - logprior: -1.2417e+00
Epoch 8/10
19/19 - 2s - loss: 138.1802 - loglik: -1.3683e+02 - logprior: -1.2250e+00
Epoch 9/10
19/19 - 2s - loss: 137.8760 - loglik: -1.3654e+02 - logprior: -1.2187e+00
Epoch 10/10
19/19 - 2s - loss: 137.7750 - loglik: -1.3644e+02 - logprior: -1.2118e+00
Fitted a model with MAP estimate = -138.5155
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 143.2274 - loglik: -1.3874e+02 - logprior: -4.4320e+00
Epoch 2/2
19/19 - 2s - loss: 132.0492 - loglik: -1.3028e+02 - logprior: -1.6560e+00
Fitted a model with MAP estimate = -131.6995
expansions: [(0, 2)]
discards: [19 26 29 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 134.2238 - loglik: -1.3018e+02 - logprior: -3.9855e+00
Epoch 2/2
19/19 - 2s - loss: 128.6745 - loglik: -1.2705e+02 - logprior: -1.4930e+00
Fitted a model with MAP estimate = -128.3932
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.9624 - loglik: -1.2941e+02 - logprior: -3.4953e+00
Epoch 2/10
21/21 - 2s - loss: 129.7090 - loglik: -1.2766e+02 - logprior: -1.8976e+00
Epoch 3/10
21/21 - 2s - loss: 128.1797 - loglik: -1.2670e+02 - logprior: -1.2764e+00
Epoch 4/10
21/21 - 2s - loss: 127.5874 - loglik: -1.2620e+02 - logprior: -1.1575e+00
Epoch 5/10
21/21 - 2s - loss: 127.0891 - loglik: -1.2569e+02 - logprior: -1.1693e+00
Epoch 6/10
21/21 - 2s - loss: 127.0399 - loglik: -1.2566e+02 - logprior: -1.1506e+00
Epoch 7/10
21/21 - 2s - loss: 126.7118 - loglik: -1.2534e+02 - logprior: -1.1338e+00
Epoch 8/10
21/21 - 2s - loss: 126.6769 - loglik: -1.2533e+02 - logprior: -1.1124e+00
Epoch 9/10
21/21 - 2s - loss: 126.7859 - loglik: -1.2546e+02 - logprior: -1.0988e+00
Fitted a model with MAP estimate = -126.3485
Time for alignment: 67.4809
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.4251 - loglik: -1.9226e+02 - logprior: -3.1578e+00
Epoch 2/10
19/19 - 2s - loss: 155.4828 - loglik: -1.5419e+02 - logprior: -1.2732e+00
Epoch 3/10
19/19 - 2s - loss: 141.7362 - loglik: -1.4021e+02 - logprior: -1.3781e+00
Epoch 4/10
19/19 - 2s - loss: 139.7252 - loglik: -1.3826e+02 - logprior: -1.3402e+00
Epoch 5/10
19/19 - 2s - loss: 139.1399 - loglik: -1.3775e+02 - logprior: -1.2877e+00
Epoch 6/10
19/19 - 2s - loss: 138.8372 - loglik: -1.3747e+02 - logprior: -1.2632e+00
Epoch 7/10
19/19 - 2s - loss: 138.7405 - loglik: -1.3739e+02 - logprior: -1.2350e+00
Epoch 8/10
19/19 - 2s - loss: 138.6343 - loglik: -1.3729e+02 - logprior: -1.2239e+00
Epoch 9/10
19/19 - 2s - loss: 138.5869 - loglik: -1.3725e+02 - logprior: -1.2142e+00
Epoch 10/10
19/19 - 2s - loss: 138.3719 - loglik: -1.3703e+02 - logprior: -1.2136e+00
Fitted a model with MAP estimate = -138.9954
expansions: [(0, 3), (13, 2), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 142.6540 - loglik: -1.3818e+02 - logprior: -4.4119e+00
Epoch 2/2
19/19 - 2s - loss: 132.1370 - loglik: -1.3042e+02 - logprior: -1.5996e+00
Fitted a model with MAP estimate = -131.7137
expansions: [(0, 2)]
discards: [27 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.1370 - loglik: -1.3008e+02 - logprior: -4.0087e+00
Epoch 2/2
19/19 - 2s - loss: 128.3679 - loglik: -1.2682e+02 - logprior: -1.4070e+00
Fitted a model with MAP estimate = -128.2802
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.6681 - loglik: -1.2912e+02 - logprior: -3.4902e+00
Epoch 2/10
21/21 - 2s - loss: 129.3933 - loglik: -1.2755e+02 - logprior: -1.6904e+00
Epoch 3/10
21/21 - 2s - loss: 127.7150 - loglik: -1.2628e+02 - logprior: -1.2294e+00
Epoch 4/10
21/21 - 2s - loss: 127.5175 - loglik: -1.2611e+02 - logprior: -1.1760e+00
Epoch 5/10
21/21 - 2s - loss: 126.8838 - loglik: -1.2548e+02 - logprior: -1.1702e+00
Epoch 6/10
21/21 - 2s - loss: 126.9318 - loglik: -1.2555e+02 - logprior: -1.1537e+00
Fitted a model with MAP estimate = -126.4925
Time for alignment: 60.7029
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.5153 - loglik: -1.9235e+02 - logprior: -3.1586e+00
Epoch 2/10
19/19 - 2s - loss: 156.0839 - loglik: -1.5479e+02 - logprior: -1.2709e+00
Epoch 3/10
19/19 - 2s - loss: 141.3040 - loglik: -1.3977e+02 - logprior: -1.3776e+00
Epoch 4/10
19/19 - 2s - loss: 139.1272 - loglik: -1.3766e+02 - logprior: -1.3441e+00
Epoch 5/10
19/19 - 2s - loss: 138.6170 - loglik: -1.3722e+02 - logprior: -1.2890e+00
Epoch 6/10
19/19 - 2s - loss: 138.1713 - loglik: -1.3680e+02 - logprior: -1.2625e+00
Epoch 7/10
19/19 - 2s - loss: 138.1676 - loglik: -1.3682e+02 - logprior: -1.2386e+00
Epoch 8/10
19/19 - 2s - loss: 137.8463 - loglik: -1.3650e+02 - logprior: -1.2270e+00
Epoch 9/10
19/19 - 2s - loss: 137.9589 - loglik: -1.3663e+02 - logprior: -1.2153e+00
Fitted a model with MAP estimate = -138.4429
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 143.2831 - loglik: -1.3884e+02 - logprior: -4.3876e+00
Epoch 2/2
19/19 - 2s - loss: 132.3091 - loglik: -1.3054e+02 - logprior: -1.6564e+00
Fitted a model with MAP estimate = -131.9906
expansions: [(0, 2)]
discards: [19 26 29 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 134.4367 - loglik: -1.3036e+02 - logprior: -4.0272e+00
Epoch 2/2
19/19 - 2s - loss: 128.7845 - loglik: -1.2722e+02 - logprior: -1.4268e+00
Fitted a model with MAP estimate = -128.5848
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.7950 - loglik: -1.2926e+02 - logprior: -3.4777e+00
Epoch 2/10
21/21 - 2s - loss: 129.4699 - loglik: -1.2763e+02 - logprior: -1.6842e+00
Epoch 3/10
21/21 - 2s - loss: 128.1370 - loglik: -1.2671e+02 - logprior: -1.2212e+00
Epoch 4/10
21/21 - 2s - loss: 127.5707 - loglik: -1.2617e+02 - logprior: -1.1637e+00
Epoch 5/10
21/21 - 2s - loss: 126.9655 - loglik: -1.2557e+02 - logprior: -1.1607e+00
Epoch 6/10
21/21 - 2s - loss: 127.0235 - loglik: -1.2565e+02 - logprior: -1.1432e+00
Fitted a model with MAP estimate = -126.5802
Time for alignment: 58.9733
Computed alignments with likelihoods: ['-126.3485', '-126.4925', '-126.5802']
Best model has likelihood: -126.3485
time for generating output: 0.1529
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.941983780411728
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb50c28e400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c5d99970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bccaad90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad1000d190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad1000d700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6537946a0>, <__main__.SimpleDirichletPrior object at 0x7fb50c2707f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.5078 - loglik: -3.8320e+02 - logprior: -8.2706e+00
Epoch 2/10
13/13 - 2s - loss: 336.8660 - loglik: -3.3469e+02 - logprior: -2.0377e+00
Epoch 3/10
13/13 - 3s - loss: 293.8640 - loglik: -2.9160e+02 - logprior: -2.0025e+00
Epoch 4/10
13/13 - 3s - loss: 281.1505 - loglik: -2.7827e+02 - logprior: -2.3971e+00
Epoch 5/10
13/13 - 2s - loss: 277.9468 - loglik: -2.7499e+02 - logprior: -2.4420e+00
Epoch 6/10
13/13 - 2s - loss: 276.9612 - loglik: -2.7417e+02 - logprior: -2.3484e+00
Epoch 7/10
13/13 - 2s - loss: 276.6993 - loglik: -2.7394e+02 - logprior: -2.3315e+00
Epoch 8/10
13/13 - 3s - loss: 275.5395 - loglik: -2.7283e+02 - logprior: -2.3273e+00
Epoch 9/10
13/13 - 3s - loss: 275.2971 - loglik: -2.7263e+02 - logprior: -2.3102e+00
Epoch 10/10
13/13 - 3s - loss: 274.9170 - loglik: -2.7227e+02 - logprior: -2.3097e+00
Fitted a model with MAP estimate = -274.7809
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 286.2249 - loglik: -2.7639e+02 - logprior: -9.8076e+00
Epoch 2/2
13/13 - 3s - loss: 265.0249 - loglik: -2.6061e+02 - logprior: -4.1968e+00
Fitted a model with MAP estimate = -261.1904
expansions: [(0, 3)]
discards: [  0  67  87  88 127]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 268.4579 - loglik: -2.6086e+02 - logprior: -7.5499e+00
Epoch 2/2
13/13 - 3s - loss: 257.5478 - loglik: -2.5534e+02 - logprior: -1.9657e+00
Fitted a model with MAP estimate = -255.5767
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.2544 - loglik: -2.5979e+02 - logprior: -9.4230e+00
Epoch 2/10
13/13 - 3s - loss: 260.2471 - loglik: -2.5702e+02 - logprior: -3.0011e+00
Epoch 3/10
13/13 - 3s - loss: 255.6619 - loglik: -2.5387e+02 - logprior: -1.3395e+00
Epoch 4/10
13/13 - 3s - loss: 254.8075 - loglik: -2.5341e+02 - logprior: -7.9541e-01
Epoch 5/10
13/13 - 3s - loss: 252.5676 - loglik: -2.5131e+02 - logprior: -6.8370e-01
Epoch 6/10
13/13 - 3s - loss: 252.6541 - loglik: -2.5151e+02 - logprior: -6.4821e-01
Fitted a model with MAP estimate = -251.8210
Time for alignment: 86.0708
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 391.0207 - loglik: -3.8272e+02 - logprior: -8.2685e+00
Epoch 2/10
13/13 - 3s - loss: 337.9414 - loglik: -3.3578e+02 - logprior: -2.0304e+00
Epoch 3/10
13/13 - 2s - loss: 297.0341 - loglik: -2.9468e+02 - logprior: -1.9712e+00
Epoch 4/10
13/13 - 3s - loss: 282.2579 - loglik: -2.7915e+02 - logprior: -2.3342e+00
Epoch 5/10
13/13 - 2s - loss: 277.7717 - loglik: -2.7477e+02 - logprior: -2.3523e+00
Epoch 6/10
13/13 - 2s - loss: 277.0969 - loglik: -2.7433e+02 - logprior: -2.2675e+00
Epoch 7/10
13/13 - 2s - loss: 276.0554 - loglik: -2.7336e+02 - logprior: -2.2744e+00
Epoch 8/10
13/13 - 3s - loss: 275.2446 - loglik: -2.7258e+02 - logprior: -2.2977e+00
Epoch 9/10
13/13 - 2s - loss: 274.9425 - loglik: -2.7229e+02 - logprior: -2.3090e+00
Epoch 10/10
13/13 - 2s - loss: 274.4141 - loglik: -2.7178e+02 - logprior: -2.3050e+00
Fitted a model with MAP estimate = -274.3777
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 286.6611 - loglik: -2.7680e+02 - logprior: -9.8287e+00
Epoch 2/2
13/13 - 3s - loss: 264.8742 - loglik: -2.6038e+02 - logprior: -4.2746e+00
Fitted a model with MAP estimate = -261.2219
expansions: [(0, 3)]
discards: [  0  65  87  88 127]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.9109 - loglik: -2.6026e+02 - logprior: -7.6020e+00
Epoch 2/2
13/13 - 3s - loss: 258.1882 - loglik: -2.5593e+02 - logprior: -2.0301e+00
Fitted a model with MAP estimate = -255.7236
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 269.0052 - loglik: -2.5951e+02 - logprior: -9.4514e+00
Epoch 2/10
13/13 - 3s - loss: 260.3428 - loglik: -2.5708e+02 - logprior: -3.0290e+00
Epoch 3/10
13/13 - 3s - loss: 256.5290 - loglik: -2.5466e+02 - logprior: -1.4079e+00
Epoch 4/10
13/13 - 3s - loss: 253.1310 - loglik: -2.5166e+02 - logprior: -8.7680e-01
Epoch 5/10
13/13 - 3s - loss: 253.1851 - loglik: -2.5179e+02 - logprior: -8.1725e-01
Fitted a model with MAP estimate = -252.1164
Time for alignment: 84.4820
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 390.6491 - loglik: -3.8235e+02 - logprior: -8.2634e+00
Epoch 2/10
13/13 - 2s - loss: 339.5807 - loglik: -3.3742e+02 - logprior: -2.0195e+00
Epoch 3/10
13/13 - 2s - loss: 295.6929 - loglik: -2.9341e+02 - logprior: -1.9029e+00
Epoch 4/10
13/13 - 3s - loss: 282.4122 - loglik: -2.7939e+02 - logprior: -2.2494e+00
Epoch 5/10
13/13 - 2s - loss: 277.7012 - loglik: -2.7476e+02 - logprior: -2.2587e+00
Epoch 6/10
13/13 - 2s - loss: 276.1629 - loglik: -2.7356e+02 - logprior: -2.1376e+00
Epoch 7/10
13/13 - 3s - loss: 275.8409 - loglik: -2.7332e+02 - logprior: -2.1160e+00
Epoch 8/10
13/13 - 2s - loss: 275.5490 - loglik: -2.7307e+02 - logprior: -2.1171e+00
Epoch 9/10
13/13 - 3s - loss: 274.3252 - loglik: -2.7186e+02 - logprior: -2.1267e+00
Epoch 10/10
13/13 - 3s - loss: 274.4044 - loglik: -2.7194e+02 - logprior: -2.1486e+00
Fitted a model with MAP estimate = -274.0230
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 4), (99, 2), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 285.8518 - loglik: -2.7607e+02 - logprior: -9.7464e+00
Epoch 2/2
13/13 - 3s - loss: 264.5891 - loglik: -2.6032e+02 - logprior: -4.0793e+00
Fitted a model with MAP estimate = -261.0682
expansions: [(0, 3)]
discards: [ 0 76 87 88]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.9230 - loglik: -2.6037e+02 - logprior: -7.5063e+00
Epoch 2/2
13/13 - 3s - loss: 258.1848 - loglik: -2.5600e+02 - logprior: -1.9428e+00
Fitted a model with MAP estimate = -255.5427
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 268.9618 - loglik: -2.5956e+02 - logprior: -9.3598e+00
Epoch 2/10
13/13 - 3s - loss: 260.4370 - loglik: -2.5734e+02 - logprior: -2.8624e+00
Epoch 3/10
13/13 - 3s - loss: 255.8468 - loglik: -2.5406e+02 - logprior: -1.3157e+00
Epoch 4/10
13/13 - 3s - loss: 254.1492 - loglik: -2.5274e+02 - logprior: -7.9970e-01
Epoch 5/10
13/13 - 3s - loss: 253.1044 - loglik: -2.5178e+02 - logprior: -7.4511e-01
Epoch 6/10
13/13 - 3s - loss: 252.5648 - loglik: -2.5134e+02 - logprior: -7.2497e-01
Epoch 7/10
13/13 - 3s - loss: 251.5720 - loglik: -2.5040e+02 - logprior: -7.3912e-01
Epoch 8/10
13/13 - 3s - loss: 251.7295 - loglik: -2.5062e+02 - logprior: -7.1536e-01
Fitted a model with MAP estimate = -251.3243
Time for alignment: 91.1948
Computed alignments with likelihoods: ['-251.8210', '-252.1164', '-251.3243']
Best model has likelihood: -251.3243
time for generating output: 0.2047
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.944811753902663
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb5250dda30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb630a2a100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7facd8710040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facd8710a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4e1609610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb35463a070>, <__main__.SimpleDirichletPrior object at 0x7fad01103df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1082.4537 - loglik: -1.0785e+03 - logprior: -3.9241e+00
Epoch 2/10
25/25 - 32s - loss: 824.0917 - loglik: -8.2229e+02 - logprior: -1.7380e+00
Epoch 3/10
25/25 - 32s - loss: 762.9177 - loglik: -7.5912e+02 - logprior: -3.2507e+00
Epoch 4/10
25/25 - 32s - loss: 750.6184 - loglik: -7.4640e+02 - logprior: -3.5004e+00
Epoch 5/10
25/25 - 32s - loss: 746.6705 - loglik: -7.4244e+02 - logprior: -3.4505e+00
Epoch 6/10
25/25 - 32s - loss: 744.6584 - loglik: -7.4036e+02 - logprior: -3.4986e+00
Epoch 7/10
25/25 - 32s - loss: 743.6205 - loglik: -7.3930e+02 - logprior: -3.5170e+00
Epoch 8/10
25/25 - 32s - loss: 742.8382 - loglik: -7.3847e+02 - logprior: -3.5774e+00
Epoch 9/10
25/25 - 32s - loss: 744.6696 - loglik: -7.4034e+02 - logprior: -3.5611e+00
Fitted a model with MAP estimate = -741.7659
expansions: [(52, 1), (132, 1), (134, 1), (143, 1), (163, 2), (164, 2), (175, 6), (176, 2), (177, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 2), (195, 1), (197, 1), (198, 1), (199, 2), (200, 1), (202, 1), (203, 1), (205, 1), (207, 1), (209, 1), (210, 1), (213, 1), (214, 1), (216, 1), (218, 1), (219, 1), (223, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (236, 1), (238, 2), (239, 1), (250, 1), (256, 4), (257, 2), (259, 5), (267, 1), (281, 1), (283, 1), (298, 1), (300, 1), (301, 1), (302, 2), (304, 1), (305, 1), (317, 1), (318, 1), (319, 1), (321, 2), (326, 1), (328, 2), (329, 1), (354, 3), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 735.6968 - loglik: -7.3031e+02 - logprior: -5.2294e+00
Epoch 2/2
25/25 - 45s - loss: 698.9947 - loglik: -6.9697e+02 - logprior: -1.4536e+00
Fitted a model with MAP estimate = -691.4119
expansions: [(437, 2)]
discards: [168 185 186 215 315 319 320 399 438 439]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 708.0075 - loglik: -7.0397e+02 - logprior: -3.8958e+00
Epoch 2/2
25/25 - 44s - loss: 694.5611 - loglik: -6.9340e+02 - logprior: -5.4168e-01
Fitted a model with MAP estimate = -689.1606
expansions: []
discards: [429 430]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 706.4264 - loglik: -7.0320e+02 - logprior: -3.0884e+00
Epoch 2/10
25/25 - 44s - loss: 695.2311 - loglik: -6.9540e+02 - logprior: 0.7730
Epoch 3/10
25/25 - 44s - loss: 692.6526 - loglik: -6.9277e+02 - logprior: 1.1877
Epoch 4/10
25/25 - 44s - loss: 686.8434 - loglik: -6.8698e+02 - logprior: 1.3818
Epoch 5/10
25/25 - 44s - loss: 687.5250 - loglik: -6.8793e+02 - logprior: 1.5960
Fitted a model with MAP estimate = -684.1366
Time for alignment: 855.0241
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1082.2493 - loglik: -1.0783e+03 - logprior: -3.8977e+00
Epoch 2/10
25/25 - 32s - loss: 823.0861 - loglik: -8.2128e+02 - logprior: -1.7159e+00
Epoch 3/10
25/25 - 32s - loss: 767.1740 - loglik: -7.6316e+02 - logprior: -3.4354e+00
Epoch 4/10
25/25 - 32s - loss: 757.7511 - loglik: -7.5334e+02 - logprior: -3.6768e+00
Epoch 5/10
25/25 - 32s - loss: 754.4470 - loglik: -7.5006e+02 - logprior: -3.5628e+00
Epoch 6/10
25/25 - 32s - loss: 753.3694 - loglik: -7.4894e+02 - logprior: -3.5692e+00
Epoch 7/10
25/25 - 32s - loss: 751.9459 - loglik: -7.4750e+02 - logprior: -3.6014e+00
Epoch 8/10
25/25 - 32s - loss: 752.0125 - loglik: -7.4762e+02 - logprior: -3.5760e+00
Fitted a model with MAP estimate = -750.2375
expansions: [(127, 1), (145, 1), (146, 1), (165, 1), (166, 1), (172, 1), (176, 4), (177, 2), (178, 2), (179, 1), (180, 1), (191, 1), (192, 1), (193, 1), (194, 3), (195, 2), (198, 1), (199, 1), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (207, 2), (209, 1), (210, 2), (212, 1), (213, 1), (215, 1), (217, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (235, 1), (237, 2), (238, 2), (249, 1), (252, 2), (254, 1), (255, 7), (257, 2), (266, 1), (280, 1), (282, 1), (297, 1), (299, 1), (300, 1), (301, 2), (303, 1), (314, 1), (316, 1), (317, 1), (318, 1), (327, 2), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 743.8393 - loglik: -7.3839e+02 - logprior: -5.2929e+00
Epoch 2/2
25/25 - 45s - loss: 707.0658 - loglik: -7.0528e+02 - logprior: -1.1949e+00
Fitted a model with MAP estimate = -699.8391
expansions: [(406, 2)]
discards: [184 185 214 291]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 709.0330 - loglik: -7.0544e+02 - logprior: -3.4500e+00
Epoch 2/2
25/25 - 45s - loss: 701.6537 - loglik: -7.0142e+02 - logprior: 0.3792
Fitted a model with MAP estimate = -693.3925
expansions: []
discards: [310 311 312]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 708.7422 - loglik: -7.0558e+02 - logprior: -3.0198e+00
Epoch 2/10
25/25 - 44s - loss: 699.2394 - loglik: -6.9954e+02 - logprior: 0.9090
Epoch 3/10
25/25 - 44s - loss: 693.1320 - loglik: -6.9333e+02 - logprior: 1.2866
Epoch 4/10
25/25 - 44s - loss: 690.7749 - loglik: -6.9100e+02 - logprior: 1.4760
Epoch 5/10
25/25 - 44s - loss: 688.0625 - loglik: -6.8855e+02 - logprior: 1.6654
Epoch 6/10
25/25 - 44s - loss: 689.4196 - loglik: -6.9033e+02 - logprior: 1.9695
Fitted a model with MAP estimate = -686.0294
Time for alignment: 866.5194
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1080.2048 - loglik: -1.0763e+03 - logprior: -3.8450e+00
Epoch 2/10
25/25 - 32s - loss: 821.0734 - loglik: -8.1923e+02 - logprior: -1.6943e+00
Epoch 3/10
25/25 - 32s - loss: 767.2385 - loglik: -7.6344e+02 - logprior: -3.1832e+00
Epoch 4/10
25/25 - 32s - loss: 757.1817 - loglik: -7.5326e+02 - logprior: -3.2123e+00
Epoch 5/10
25/25 - 32s - loss: 757.2584 - loglik: -7.5316e+02 - logprior: -3.3275e+00
Fitted a model with MAP estimate = -753.6406
expansions: [(146, 1), (147, 1), (165, 1), (166, 2), (172, 1), (176, 4), (177, 2), (178, 2), (179, 1), (192, 1), (193, 1), (194, 4), (195, 1), (196, 1), (199, 2), (200, 3), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (210, 1), (211, 1), (212, 1), (213, 1), (217, 1), (219, 1), (220, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (238, 2), (239, 1), (250, 1), (251, 1), (255, 3), (256, 2), (257, 1), (259, 5), (267, 1), (281, 1), (283, 1), (298, 1), (300, 1), (301, 1), (302, 2), (304, 1), (305, 1), (317, 1), (318, 1), (319, 1), (327, 1), (328, 2), (340, 2), (355, 1), (356, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 741.7499 - loglik: -7.3643e+02 - logprior: -5.1678e+00
Epoch 2/2
25/25 - 45s - loss: 707.8790 - loglik: -7.0617e+02 - logprior: -1.1641e+00
Fitted a model with MAP estimate = -700.8029
expansions: [(410, 1)]
discards: [185 191 213 223 321 322 348 422]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 716.4464 - loglik: -7.1224e+02 - logprior: -4.0685e+00
Epoch 2/2
25/25 - 44s - loss: 704.5709 - loglik: -7.0385e+02 - logprior: -1.0709e-01
Fitted a model with MAP estimate = -699.3832
expansions: []
discards: [308]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 712.7358 - loglik: -7.0923e+02 - logprior: -3.3665e+00
Epoch 2/10
25/25 - 44s - loss: 705.3910 - loglik: -7.0498e+02 - logprior: 0.1954
Epoch 3/10
25/25 - 44s - loss: 697.2287 - loglik: -6.9706e+02 - logprior: 0.9095
Epoch 4/10
25/25 - 44s - loss: 696.7644 - loglik: -6.9659e+02 - logprior: 1.0700
Epoch 5/10
25/25 - 44s - loss: 695.8751 - loglik: -6.9597e+02 - logprior: 1.2894
Epoch 6/10
25/25 - 44s - loss: 693.3768 - loglik: -6.9386e+02 - logprior: 1.5507
Epoch 7/10
25/25 - 44s - loss: 690.4011 - loglik: -6.9128e+02 - logprior: 1.8557
Epoch 8/10
25/25 - 44s - loss: 694.0426 - loglik: -6.9519e+02 - logprior: 2.0490
Fitted a model with MAP estimate = -690.5455
Time for alignment: 860.4148
Computed alignments with likelihoods: ['-684.1366', '-686.0294', '-690.5455']
Best model has likelihood: -684.1366
time for generating output: 0.4349
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9100881136486243
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb57a222f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6c54cbfd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6c54cba60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2bc5b0fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb675030c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4c73e0100>, <__main__.SimpleDirichletPrior object at 0x7fb5033ed130>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 327.6718 - loglik: -3.2306e+02 - logprior: -4.6051e+00
Epoch 2/10
16/16 - 4s - loss: 255.4761 - loglik: -2.5388e+02 - logprior: -1.5871e+00
Epoch 3/10
16/16 - 5s - loss: 217.7851 - loglik: -2.1540e+02 - logprior: -2.0688e+00
Epoch 4/10
16/16 - 4s - loss: 207.8997 - loglik: -2.0513e+02 - logprior: -2.0920e+00
Epoch 5/10
16/16 - 5s - loss: 203.1193 - loglik: -2.0055e+02 - logprior: -1.9982e+00
Epoch 6/10
16/16 - 4s - loss: 201.4165 - loglik: -1.9885e+02 - logprior: -2.0058e+00
Epoch 7/10
16/16 - 4s - loss: 200.4083 - loglik: -1.9784e+02 - logprior: -2.0076e+00
Epoch 8/10
16/16 - 4s - loss: 200.1641 - loglik: -1.9758e+02 - logprior: -2.0104e+00
Epoch 9/10
16/16 - 5s - loss: 198.9009 - loglik: -1.9633e+02 - logprior: -2.0147e+00
Epoch 10/10
16/16 - 4s - loss: 199.1843 - loglik: -1.9662e+02 - logprior: -2.0080e+00
Fitted a model with MAP estimate = -198.3777
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 4), (19, 1), (38, 10), (72, 1), (73, 1), (74, 1), (80, 1), (97, 1), (98, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 209.1651 - loglik: -2.0467e+02 - logprior: -4.3019e+00
Epoch 2/2
33/33 - 7s - loss: 191.5358 - loglik: -1.8865e+02 - logprior: -2.5536e+00
Fitted a model with MAP estimate = -187.6453
expansions: [(0, 1), (48, 1), (50, 2), (51, 1)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 192.2161 - loglik: -1.8896e+02 - logprior: -3.1210e+00
Epoch 2/2
33/33 - 7s - loss: 184.9620 - loglik: -1.8311e+02 - logprior: -1.5994e+00
Fitted a model with MAP estimate = -182.6514
expansions: [(0, 1), (49, 1), (50, 1), (53, 1)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 187.9030 - loglik: -1.8444e+02 - logprior: -3.3404e+00
Epoch 2/10
33/33 - 7s - loss: 183.2168 - loglik: -1.8139e+02 - logprior: -1.5883e+00
Epoch 3/10
33/33 - 7s - loss: 179.3653 - loglik: -1.7755e+02 - logprior: -1.4743e+00
Epoch 4/10
33/33 - 6s - loss: 177.1980 - loglik: -1.7546e+02 - logprior: -1.3593e+00
Epoch 5/10
33/33 - 8s - loss: 177.9302 - loglik: -1.7629e+02 - logprior: -1.2682e+00
Fitted a model with MAP estimate = -176.4240
Time for alignment: 153.4317
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 326.1915 - loglik: -3.2160e+02 - logprior: -4.5856e+00
Epoch 2/10
16/16 - 5s - loss: 253.7620 - loglik: -2.5220e+02 - logprior: -1.5513e+00
Epoch 3/10
16/16 - 5s - loss: 223.2459 - loglik: -2.2111e+02 - logprior: -1.9017e+00
Epoch 4/10
16/16 - 4s - loss: 213.6394 - loglik: -2.1119e+02 - logprior: -1.9249e+00
Epoch 5/10
16/16 - 5s - loss: 207.2628 - loglik: -2.0481e+02 - logprior: -1.8343e+00
Epoch 6/10
16/16 - 4s - loss: 202.9042 - loglik: -2.0028e+02 - logprior: -1.8762e+00
Epoch 7/10
16/16 - 4s - loss: 198.9155 - loglik: -1.9622e+02 - logprior: -1.9065e+00
Epoch 8/10
16/16 - 5s - loss: 197.8168 - loglik: -1.9521e+02 - logprior: -1.9155e+00
Epoch 9/10
16/16 - 5s - loss: 196.7964 - loglik: -1.9426e+02 - logprior: -1.9200e+00
Epoch 10/10
16/16 - 4s - loss: 195.6519 - loglik: -1.9315e+02 - logprior: -1.9109e+00
Fitted a model with MAP estimate = -195.8522
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (77, 1), (97, 1), (98, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 10s - loss: 220.9579 - loglik: -2.1503e+02 - logprior: -5.8620e+00
Epoch 2/2
16/16 - 5s - loss: 202.0426 - loglik: -1.9875e+02 - logprior: -2.9712e+00
Fitted a model with MAP estimate = -197.7742
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 198.0325 - loglik: -1.9476e+02 - logprior: -3.1693e+00
Epoch 2/2
33/33 - 6s - loss: 192.5388 - loglik: -1.9072e+02 - logprior: -1.6002e+00
Fitted a model with MAP estimate = -191.1617
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 196.5544 - loglik: -1.9331e+02 - logprior: -3.1389e+00
Epoch 2/10
33/33 - 7s - loss: 193.4061 - loglik: -1.9172e+02 - logprior: -1.4571e+00
Epoch 3/10
33/33 - 6s - loss: 189.6646 - loglik: -1.8792e+02 - logprior: -1.3931e+00
Epoch 4/10
33/33 - 7s - loss: 189.9438 - loglik: -1.8826e+02 - logprior: -1.2875e+00
Fitted a model with MAP estimate = -187.9678
Time for alignment: 136.6176
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 326.3533 - loglik: -3.2177e+02 - logprior: -4.5787e+00
Epoch 2/10
16/16 - 5s - loss: 256.1321 - loglik: -2.5459e+02 - logprior: -1.5374e+00
Epoch 3/10
16/16 - 4s - loss: 221.0641 - loglik: -2.1895e+02 - logprior: -1.9545e+00
Epoch 4/10
16/16 - 4s - loss: 205.1966 - loglik: -2.0277e+02 - logprior: -1.9616e+00
Epoch 5/10
16/16 - 4s - loss: 201.7348 - loglik: -1.9939e+02 - logprior: -1.8407e+00
Epoch 6/10
16/16 - 4s - loss: 200.0061 - loglik: -1.9767e+02 - logprior: -1.8674e+00
Epoch 7/10
16/16 - 4s - loss: 197.4077 - loglik: -1.9510e+02 - logprior: -1.8542e+00
Epoch 8/10
16/16 - 4s - loss: 197.2106 - loglik: -1.9491e+02 - logprior: -1.8447e+00
Epoch 9/10
16/16 - 5s - loss: 196.3148 - loglik: -1.9401e+02 - logprior: -1.8464e+00
Epoch 10/10
16/16 - 5s - loss: 196.5247 - loglik: -1.9423e+02 - logprior: -1.8464e+00
Fitted a model with MAP estimate = -195.7412
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 10s - loss: 216.8534 - loglik: -2.1096e+02 - logprior: -5.8229e+00
Epoch 2/2
16/16 - 4s - loss: 201.2190 - loglik: -1.9802e+02 - logprior: -2.9123e+00
Fitted a model with MAP estimate = -197.4997
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 198.3179 - loglik: -1.9505e+02 - logprior: -3.1622e+00
Epoch 2/2
33/33 - 7s - loss: 193.2219 - loglik: -1.9139e+02 - logprior: -1.6042e+00
Fitted a model with MAP estimate = -191.6627
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 197.0407 - loglik: -1.9379e+02 - logprior: -3.1410e+00
Epoch 2/10
33/33 - 7s - loss: 193.8160 - loglik: -1.9213e+02 - logprior: -1.4603e+00
Epoch 3/10
33/33 - 7s - loss: 190.9348 - loglik: -1.8922e+02 - logprior: -1.3772e+00
Epoch 4/10
33/33 - 6s - loss: 188.7752 - loglik: -1.8710e+02 - logprior: -1.2775e+00
Epoch 5/10
33/33 - 7s - loss: 189.0919 - loglik: -1.8748e+02 - logprior: -1.1996e+00
Fitted a model with MAP estimate = -187.8206
Time for alignment: 142.6813
Computed alignments with likelihoods: ['-176.4240', '-187.9678', '-187.8206']
Best model has likelihood: -176.4240
time for generating output: 0.3254
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5751115336795144
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4ea012040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2bd9af5b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7facf00d1fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf00d1f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2dc323760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faccd69e040>, <__main__.SimpleDirichletPrior object at 0x7fb4bc9230d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.5153 - loglik: -3.0523e+02 - logprior: -3.1835e+00
Epoch 2/10
19/19 - 3s - loss: 276.9996 - loglik: -2.7511e+02 - logprior: -1.2234e+00
Epoch 3/10
19/19 - 3s - loss: 260.9199 - loglik: -2.5845e+02 - logprior: -1.5864e+00
Epoch 4/10
19/19 - 3s - loss: 256.6026 - loglik: -2.5421e+02 - logprior: -1.5627e+00
Epoch 5/10
19/19 - 3s - loss: 255.4452 - loglik: -2.5324e+02 - logprior: -1.5067e+00
Epoch 6/10
19/19 - 3s - loss: 254.6846 - loglik: -2.5257e+02 - logprior: -1.4838e+00
Epoch 7/10
19/19 - 3s - loss: 253.8211 - loglik: -2.5176e+02 - logprior: -1.4587e+00
Epoch 8/10
19/19 - 3s - loss: 254.0859 - loglik: -2.5207e+02 - logprior: -1.4486e+00
Fitted a model with MAP estimate = -240.2561
expansions: [(6, 3), (7, 2), (10, 2), (39, 5), (55, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 264.5656 - loglik: -2.6134e+02 - logprior: -3.1281e+00
Epoch 2/2
19/19 - 3s - loss: 252.0946 - loglik: -2.5030e+02 - logprior: -1.3471e+00
Fitted a model with MAP estimate = -234.3550
expansions: []
discards: [ 0  9 15 51 68 73 80]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 257.2305 - loglik: -2.5319e+02 - logprior: -3.9630e+00
Epoch 2/2
19/19 - 3s - loss: 251.9665 - loglik: -2.4944e+02 - logprior: -2.1623e+00
Fitted a model with MAP estimate = -235.0163
expansions: [(0, 2), (38, 9)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 236.5160 - loglik: -2.3437e+02 - logprior: -2.0494e+00
Epoch 2/10
23/23 - 4s - loss: 231.4485 - loglik: -2.2985e+02 - logprior: -1.0923e+00
Epoch 3/10
23/23 - 4s - loss: 229.4098 - loglik: -2.2738e+02 - logprior: -1.1152e+00
Epoch 4/10
23/23 - 4s - loss: 227.7246 - loglik: -2.2560e+02 - logprior: -1.0829e+00
Epoch 5/10
23/23 - 4s - loss: 226.8989 - loglik: -2.2486e+02 - logprior: -1.0705e+00
Epoch 6/10
23/23 - 4s - loss: 226.8364 - loglik: -2.2488e+02 - logprior: -1.0574e+00
Epoch 7/10
23/23 - 4s - loss: 225.7859 - loglik: -2.2389e+02 - logprior: -1.0512e+00
Epoch 8/10
23/23 - 4s - loss: 225.1535 - loglik: -2.2332e+02 - logprior: -1.0392e+00
Epoch 9/10
23/23 - 4s - loss: 224.7955 - loglik: -2.2302e+02 - logprior: -1.0314e+00
Epoch 10/10
23/23 - 4s - loss: 224.4335 - loglik: -2.2271e+02 - logprior: -1.0192e+00
Fitted a model with MAP estimate = -223.6303
Time for alignment: 109.7850
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 308.6314 - loglik: -3.0535e+02 - logprior: -3.1829e+00
Epoch 2/10
19/19 - 3s - loss: 276.5983 - loglik: -2.7470e+02 - logprior: -1.2129e+00
Epoch 3/10
19/19 - 3s - loss: 260.7783 - loglik: -2.5803e+02 - logprior: -1.5626e+00
Epoch 4/10
19/19 - 3s - loss: 257.1730 - loglik: -2.5477e+02 - logprior: -1.5093e+00
Epoch 5/10
19/19 - 3s - loss: 255.0973 - loglik: -2.5291e+02 - logprior: -1.4716e+00
Epoch 6/10
19/19 - 3s - loss: 254.1315 - loglik: -2.5205e+02 - logprior: -1.4646e+00
Epoch 7/10
19/19 - 3s - loss: 253.5230 - loglik: -2.5149e+02 - logprior: -1.4516e+00
Epoch 8/10
19/19 - 3s - loss: 253.2092 - loglik: -2.5120e+02 - logprior: -1.4341e+00
Epoch 9/10
19/19 - 3s - loss: 252.7886 - loglik: -2.5081e+02 - logprior: -1.4309e+00
Epoch 10/10
19/19 - 3s - loss: 252.6065 - loglik: -2.5066e+02 - logprior: -1.4266e+00
Fitted a model with MAP estimate = -240.6359
expansions: [(6, 3), (7, 2), (10, 2), (39, 9), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.1663 - loglik: -2.6503e+02 - logprior: -4.0559e+00
Epoch 2/2
19/19 - 3s - loss: 253.7392 - loglik: -2.5116e+02 - logprior: -2.2389e+00
Fitted a model with MAP estimate = -235.6410
expansions: [(0, 2)]
discards: [ 0  8 14 54 55 74 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.8419 - loglik: -2.5182e+02 - logprior: -2.9536e+00
Epoch 2/2
19/19 - 3s - loss: 249.6550 - loglik: -2.4814e+02 - logprior: -1.1934e+00
Fitted a model with MAP estimate = -233.9403
expansions: [(54, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 237.9368 - loglik: -2.3523e+02 - logprior: -2.6235e+00
Epoch 2/10
23/23 - 4s - loss: 232.8578 - loglik: -2.3143e+02 - logprior: -1.1572e+00
Epoch 3/10
23/23 - 4s - loss: 231.1461 - loglik: -2.2952e+02 - logprior: -1.1199e+00
Epoch 4/10
23/23 - 4s - loss: 230.5866 - loglik: -2.2874e+02 - logprior: -1.1018e+00
Epoch 5/10
23/23 - 3s - loss: 228.4833 - loglik: -2.2651e+02 - logprior: -1.1007e+00
Epoch 6/10
23/23 - 4s - loss: 228.3632 - loglik: -2.2639e+02 - logprior: -1.0896e+00
Epoch 7/10
23/23 - 4s - loss: 227.8190 - loglik: -2.2587e+02 - logprior: -1.0790e+00
Epoch 8/10
23/23 - 4s - loss: 226.7664 - loglik: -2.2489e+02 - logprior: -1.0730e+00
Epoch 9/10
23/23 - 4s - loss: 227.0332 - loglik: -2.2521e+02 - logprior: -1.0685e+00
Fitted a model with MAP estimate = -225.7736
Time for alignment: 113.6176
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.6001 - loglik: -3.0531e+02 - logprior: -3.1850e+00
Epoch 2/10
19/19 - 3s - loss: 276.7797 - loglik: -2.7487e+02 - logprior: -1.2274e+00
Epoch 3/10
19/19 - 3s - loss: 261.3118 - loglik: -2.5855e+02 - logprior: -1.5738e+00
Epoch 4/10
19/19 - 3s - loss: 257.3405 - loglik: -2.5492e+02 - logprior: -1.4999e+00
Epoch 5/10
19/19 - 3s - loss: 255.9508 - loglik: -2.5378e+02 - logprior: -1.4433e+00
Epoch 6/10
19/19 - 3s - loss: 255.2524 - loglik: -2.5313e+02 - logprior: -1.4517e+00
Epoch 7/10
19/19 - 3s - loss: 254.1618 - loglik: -2.5213e+02 - logprior: -1.4298e+00
Epoch 8/10
19/19 - 3s - loss: 253.6445 - loglik: -2.5162e+02 - logprior: -1.4174e+00
Epoch 9/10
19/19 - 3s - loss: 253.4712 - loglik: -2.5147e+02 - logprior: -1.4089e+00
Epoch 10/10
19/19 - 3s - loss: 253.3955 - loglik: -2.5141e+02 - logprior: -1.4081e+00
Fitted a model with MAP estimate = -241.3858
expansions: [(6, 3), (7, 2), (10, 2), (39, 8), (58, 2), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 270.7521 - loglik: -2.6661e+02 - logprior: -4.0520e+00
Epoch 2/2
19/19 - 3s - loss: 254.1243 - loglik: -2.5152e+02 - logprior: -2.2450e+00
Fitted a model with MAP estimate = -235.9161
expansions: [(0, 2)]
discards: [ 0  8 14 50 51 52 73 81 86]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.3918 - loglik: -2.5138e+02 - logprior: -2.9484e+00
Epoch 2/2
19/19 - 3s - loss: 249.8287 - loglik: -2.4825e+02 - logprior: -1.1813e+00
Fitted a model with MAP estimate = -233.3361
expansions: [(39, 10)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 238.2518 - loglik: -2.3548e+02 - logprior: -2.6769e+00
Epoch 2/10
23/23 - 4s - loss: 232.0186 - loglik: -2.3023e+02 - logprior: -1.2544e+00
Epoch 3/10
23/23 - 4s - loss: 229.7228 - loglik: -2.2759e+02 - logprior: -1.1605e+00
Epoch 4/10
23/23 - 4s - loss: 227.8933 - loglik: -2.2571e+02 - logprior: -1.1374e+00
Epoch 5/10
23/23 - 4s - loss: 227.7043 - loglik: -2.2560e+02 - logprior: -1.1296e+00
Epoch 6/10
23/23 - 4s - loss: 226.5471 - loglik: -2.2453e+02 - logprior: -1.1214e+00
Epoch 7/10
23/23 - 4s - loss: 226.2392 - loglik: -2.2429e+02 - logprior: -1.1152e+00
Epoch 8/10
23/23 - 4s - loss: 225.2620 - loglik: -2.2338e+02 - logprior: -1.1074e+00
Epoch 9/10
23/23 - 4s - loss: 225.3337 - loglik: -2.2350e+02 - logprior: -1.0955e+00
Fitted a model with MAP estimate = -224.1547
Time for alignment: 112.5279
Computed alignments with likelihoods: ['-223.6303', '-225.7736', '-224.1547']
Best model has likelihood: -223.6303
time for generating output: 0.2265
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.745119305856833
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb524fd38b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf02b8c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bcbace20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad10232400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bcefc190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb5259aad90>, <__main__.SimpleDirichletPrior object at 0x7facf92c7040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.3687 - loglik: -3.8711e+02 - logprior: -4.6228e+01
Epoch 2/10
10/10 - 2s - loss: 370.7231 - loglik: -3.5957e+02 - logprior: -1.1025e+01
Epoch 3/10
10/10 - 2s - loss: 333.5234 - loglik: -3.2853e+02 - logprior: -4.8819e+00
Epoch 4/10
10/10 - 2s - loss: 310.0344 - loglik: -3.0678e+02 - logprior: -3.1265e+00
Epoch 5/10
10/10 - 2s - loss: 300.7995 - loglik: -2.9809e+02 - logprior: -2.3691e+00
Epoch 6/10
10/10 - 2s - loss: 297.9518 - loglik: -2.9569e+02 - logprior: -1.8113e+00
Epoch 7/10
10/10 - 2s - loss: 296.2970 - loglik: -2.9455e+02 - logprior: -1.3143e+00
Epoch 8/10
10/10 - 2s - loss: 295.1093 - loglik: -2.9364e+02 - logprior: -1.0378e+00
Epoch 9/10
10/10 - 2s - loss: 295.2570 - loglik: -2.9392e+02 - logprior: -9.1786e-01
Fitted a model with MAP estimate = -294.0943
expansions: [(10, 4), (17, 1), (18, 1), (26, 2), (28, 3), (48, 2), (59, 3), (62, 1), (65, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.7303 - loglik: -2.9790e+02 - logprior: -5.1800e+01
Epoch 2/2
10/10 - 3s - loss: 305.7759 - loglik: -2.8569e+02 - logprior: -1.9952e+01
Fitted a model with MAP estimate = -298.0366
expansions: [(10, 2)]
discards: [35 59 72]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 332.1107 - loglik: -2.8294e+02 - logprior: -4.9144e+01
Epoch 2/2
10/10 - 3s - loss: 293.3327 - loglik: -2.7952e+02 - logprior: -1.3698e+01
Fitted a model with MAP estimate = -284.5358
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 319.3266 - loglik: -2.7866e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 3s - loss: 286.9839 - loglik: -2.7832e+02 - logprior: -8.5487e+00
Epoch 3/10
10/10 - 3s - loss: 279.5096 - loglik: -2.7723e+02 - logprior: -2.0932e+00
Epoch 4/10
10/10 - 3s - loss: 276.0600 - loglik: -2.7624e+02 - logprior: 0.4639
Epoch 5/10
10/10 - 3s - loss: 274.2350 - loglik: -2.7572e+02 - logprior: 1.8269
Epoch 6/10
10/10 - 3s - loss: 273.6492 - loglik: -2.7588e+02 - logprior: 2.5492
Epoch 7/10
10/10 - 3s - loss: 272.6769 - loglik: -2.7545e+02 - logprior: 3.0526
Epoch 8/10
10/10 - 3s - loss: 272.5033 - loglik: -2.7573e+02 - logprior: 3.5012
Epoch 9/10
10/10 - 3s - loss: 272.1609 - loglik: -2.7574e+02 - logprior: 3.8651
Epoch 10/10
10/10 - 3s - loss: 271.4180 - loglik: -2.7524e+02 - logprior: 4.1248
Fitted a model with MAP estimate = -271.2105
Time for alignment: 79.2188
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.3819 - loglik: -3.8713e+02 - logprior: -4.6228e+01
Epoch 2/10
10/10 - 2s - loss: 370.6221 - loglik: -3.5947e+02 - logprior: -1.1029e+01
Epoch 3/10
10/10 - 2s - loss: 334.3844 - loglik: -3.2937e+02 - logprior: -4.8978e+00
Epoch 4/10
10/10 - 2s - loss: 311.7311 - loglik: -3.0859e+02 - logprior: -2.9642e+00
Epoch 5/10
10/10 - 2s - loss: 302.8581 - loglik: -3.0025e+02 - logprior: -2.1659e+00
Epoch 6/10
10/10 - 2s - loss: 298.7993 - loglik: -2.9651e+02 - logprior: -1.7786e+00
Epoch 7/10
10/10 - 2s - loss: 296.8497 - loglik: -2.9510e+02 - logprior: -1.3428e+00
Epoch 8/10
10/10 - 2s - loss: 295.2137 - loglik: -2.9384e+02 - logprior: -1.0264e+00
Epoch 9/10
10/10 - 2s - loss: 294.6991 - loglik: -2.9349e+02 - logprior: -8.5539e-01
Epoch 10/10
10/10 - 2s - loss: 294.6483 - loglik: -2.9350e+02 - logprior: -7.7545e-01
Fitted a model with MAP estimate = -293.8507
expansions: [(10, 4), (17, 1), (18, 1), (29, 3), (42, 2), (49, 2), (59, 3), (62, 1), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 350.5740 - loglik: -2.9874e+02 - logprior: -5.1806e+01
Epoch 2/2
10/10 - 3s - loss: 306.5896 - loglik: -2.8638e+02 - logprior: -2.0078e+01
Fitted a model with MAP estimate = -299.0197
expansions: [(10, 1)]
discards: [ 0 59 72]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 336.1274 - loglik: -2.8554e+02 - logprior: -5.0560e+01
Epoch 2/2
10/10 - 3s - loss: 300.6217 - loglik: -2.8257e+02 - logprior: -1.7928e+01
Fitted a model with MAP estimate = -291.7622
expansions: [(0, 2), (2, 1), (9, 1)]
discards: [ 0 35 51]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 320.9957 - loglik: -2.8125e+02 - logprior: -3.9721e+01
Epoch 2/10
10/10 - 3s - loss: 286.0032 - loglik: -2.7761e+02 - logprior: -8.2790e+00
Epoch 3/10
10/10 - 3s - loss: 278.8967 - loglik: -2.7674e+02 - logprior: -1.9643e+00
Epoch 4/10
10/10 - 3s - loss: 275.8094 - loglik: -2.7611e+02 - logprior: 0.5842
Epoch 5/10
10/10 - 3s - loss: 273.7610 - loglik: -2.7544e+02 - logprior: 2.0399
Epoch 6/10
10/10 - 3s - loss: 273.3311 - loglik: -2.7585e+02 - logprior: 2.8655
Epoch 7/10
10/10 - 3s - loss: 272.2459 - loglik: -2.7530e+02 - logprior: 3.3545
Epoch 8/10
10/10 - 3s - loss: 272.6152 - loglik: -2.7604e+02 - logprior: 3.7050
Fitted a model with MAP estimate = -271.6722
Time for alignment: 74.8503
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.9949 - loglik: -3.8674e+02 - logprior: -4.6231e+01
Epoch 2/10
10/10 - 2s - loss: 371.2199 - loglik: -3.6006e+02 - logprior: -1.1031e+01
Epoch 3/10
10/10 - 2s - loss: 334.6885 - loglik: -3.2968e+02 - logprior: -4.8896e+00
Epoch 4/10
10/10 - 2s - loss: 312.1310 - loglik: -3.0895e+02 - logprior: -3.0361e+00
Epoch 5/10
10/10 - 2s - loss: 301.9837 - loglik: -2.9938e+02 - logprior: -2.2587e+00
Epoch 6/10
10/10 - 2s - loss: 297.8739 - loglik: -2.9574e+02 - logprior: -1.7130e+00
Epoch 7/10
10/10 - 2s - loss: 296.5272 - loglik: -2.9490e+02 - logprior: -1.2333e+00
Epoch 8/10
10/10 - 2s - loss: 295.6828 - loglik: -2.9433e+02 - logprior: -9.6564e-01
Epoch 9/10
10/10 - 2s - loss: 294.9461 - loglik: -2.9372e+02 - logprior: -8.3951e-01
Epoch 10/10
10/10 - 2s - loss: 294.3606 - loglik: -2.9320e+02 - logprior: -7.5488e-01
Fitted a model with MAP estimate = -293.7236
expansions: [(10, 4), (17, 1), (18, 1), (26, 1), (28, 3), (42, 1), (48, 2), (58, 3), (61, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.6219 - loglik: -2.9781e+02 - logprior: -5.1782e+01
Epoch 2/2
10/10 - 3s - loss: 306.6414 - loglik: -2.8651e+02 - logprior: -2.0006e+01
Fitted a model with MAP estimate = -298.9466
expansions: [(10, 1)]
discards: [ 0 34 58 71]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 336.8652 - loglik: -2.8633e+02 - logprior: -5.0511e+01
Epoch 2/2
10/10 - 3s - loss: 300.9268 - loglik: -2.8312e+02 - logprior: -1.7687e+01
Fitted a model with MAP estimate = -292.3343
expansions: [(2, 2), (13, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 333.0808 - loglik: -2.8230e+02 - logprior: -5.0757e+01
Epoch 2/10
10/10 - 3s - loss: 298.7787 - loglik: -2.7967e+02 - logprior: -1.8994e+01
Epoch 3/10
10/10 - 3s - loss: 290.0213 - loglik: -2.7786e+02 - logprior: -1.1980e+01
Epoch 4/10
10/10 - 3s - loss: 282.7690 - loglik: -2.7704e+02 - logprior: -5.4482e+00
Epoch 5/10
10/10 - 3s - loss: 276.0186 - loglik: -2.7645e+02 - logprior: 0.7891
Epoch 6/10
10/10 - 3s - loss: 273.9439 - loglik: -2.7616e+02 - logprior: 2.5683
Epoch 7/10
10/10 - 3s - loss: 273.5444 - loglik: -2.7644e+02 - logprior: 3.1992
Epoch 8/10
10/10 - 3s - loss: 272.4376 - loglik: -2.7572e+02 - logprior: 3.5678
Epoch 9/10
10/10 - 3s - loss: 271.7498 - loglik: -2.7528e+02 - logprior: 3.8116
Epoch 10/10
10/10 - 3s - loss: 272.3449 - loglik: -2.7609e+02 - logprior: 4.0324
Fitted a model with MAP estimate = -271.5619
Time for alignment: 79.9345
Computed alignments with likelihoods: ['-271.2105', '-271.6722', '-271.5619']
Best model has likelihood: -271.2105
time for generating output: 0.1794
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.8762567672080434
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb605f00370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad01e02fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb641f08b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf921e1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7facf921e100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb4e190a160>, <__main__.SimpleDirichletPrior object at 0x7facf15a8f40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1665 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4903 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7877 - loglik: -1.3991e+02 - logprior: -3.2880e+01
Epoch 4/10
10/10 - 1s - loss: 144.8596 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1867 - loglik: -1.2002e+02 - logprior: -1.1159e+01
Epoch 6/10
10/10 - 1s - loss: 123.3558 - loglik: -1.1711e+02 - logprior: -6.0955e+00
Epoch 7/10
10/10 - 1s - loss: 118.3712 - loglik: -1.1512e+02 - logprior: -2.8861e+00
Epoch 8/10
10/10 - 1s - loss: 115.5545 - loglik: -1.1449e+02 - logprior: -7.0987e-01
Epoch 9/10
10/10 - 1s - loss: 113.9521 - loglik: -1.1454e+02 - logprior: 0.8793
Epoch 10/10
10/10 - 1s - loss: 112.8792 - loglik: -1.1468e+02 - logprior: 2.0832
Fitted a model with MAP estimate = -112.1142
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.6315 - loglik: -1.0998e+02 - logprior: -3.5763e+02
Epoch 2/2
10/10 - 1s - loss: 206.7065 - loglik: -9.7277e+01 - logprior: -1.0934e+02
Fitted a model with MAP estimate = -158.0643
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 400.4558 - loglik: -9.7358e+01 - logprior: -3.0308e+02
Epoch 2/2
10/10 - 1s - loss: 212.7427 - loglik: -9.5763e+01 - logprior: -1.1696e+02
Fitted a model with MAP estimate = -182.8736
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 372.3244 - loglik: -9.4781e+01 - logprior: -2.7752e+02
Epoch 2/10
10/10 - 1s - loss: 169.9366 - loglik: -9.4164e+01 - logprior: -7.5735e+01
Epoch 3/10
10/10 - 1s - loss: 119.2383 - loglik: -9.4415e+01 - logprior: -2.4821e+01
Epoch 4/10
10/10 - 1s - loss: 101.0884 - loglik: -9.4869e+01 - logprior: -6.2132e+00
Epoch 5/10
10/10 - 1s - loss: 91.7111 - loglik: -9.4784e+01 - logprior: 3.1709
Epoch 6/10
10/10 - 1s - loss: 86.2114 - loglik: -9.4517e+01 - logprior: 8.5995
Epoch 7/10
10/10 - 1s - loss: 82.7342 - loglik: -9.4555e+01 - logprior: 12.1394
Epoch 8/10
10/10 - 1s - loss: 80.3192 - loglik: -9.4724e+01 - logprior: 14.6744
Epoch 9/10
10/10 - 1s - loss: 78.4840 - loglik: -9.4854e+01 - logprior: 16.6307
Epoch 10/10
10/10 - 1s - loss: 76.9732 - loglik: -9.4942e+01 - logprior: 18.2402
Fitted a model with MAP estimate = -75.9499
Time for alignment: 37.4396
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4905 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7878 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8583 - loglik: -1.2603e+02 - logprior: -1.8824e+01
Epoch 5/10
10/10 - 1s - loss: 131.0420 - loglik: -1.1977e+02 - logprior: -1.1236e+01
Epoch 6/10
10/10 - 1s - loss: 123.3375 - loglik: -1.1694e+02 - logprior: -6.1311e+00
Epoch 7/10
10/10 - 1s - loss: 118.4729 - loglik: -1.1514e+02 - logprior: -2.9094e+00
Epoch 8/10
10/10 - 1s - loss: 115.6678 - loglik: -1.1460e+02 - logprior: -7.3286e-01
Epoch 9/10
10/10 - 1s - loss: 114.0263 - loglik: -1.1459e+02 - logprior: 0.8585
Epoch 10/10
10/10 - 1s - loss: 112.8989 - loglik: -1.1462e+02 - logprior: 2.0500
Fitted a model with MAP estimate = -112.0849
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.3302 - loglik: -1.0924e+02 - logprior: -3.5806e+02
Epoch 2/2
10/10 - 1s - loss: 206.5481 - loglik: -9.7255e+01 - logprior: -1.0920e+02
Fitted a model with MAP estimate = -157.7428
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 400.4198 - loglik: -9.7384e+01 - logprior: -3.0302e+02
Epoch 2/2
10/10 - 1s - loss: 212.7396 - loglik: -9.5775e+01 - logprior: -1.1695e+02
Fitted a model with MAP estimate = -182.8671
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 372.3126 - loglik: -9.4809e+01 - logprior: -2.7748e+02
Epoch 2/10
10/10 - 1s - loss: 169.9278 - loglik: -9.4207e+01 - logprior: -7.5685e+01
Epoch 3/10
10/10 - 1s - loss: 119.2543 - loglik: -9.4426e+01 - logprior: -2.4826e+01
Epoch 4/10
10/10 - 1s - loss: 101.1073 - loglik: -9.4864e+01 - logprior: -6.2367e+00
Epoch 5/10
10/10 - 1s - loss: 91.7077 - loglik: -9.4744e+01 - logprior: 3.1353
Epoch 6/10
10/10 - 1s - loss: 86.1852 - loglik: -9.4472e+01 - logprior: 8.5754
Epoch 7/10
10/10 - 1s - loss: 82.7213 - loglik: -9.4523e+01 - logprior: 12.1257
Epoch 8/10
10/10 - 1s - loss: 80.3121 - loglik: -9.4717e+01 - logprior: 14.6700
Epoch 9/10
10/10 - 1s - loss: 78.4810 - loglik: -9.4847e+01 - logprior: 16.6210
Epoch 10/10
10/10 - 1s - loss: 76.9717 - loglik: -9.4919e+01 - logprior: 18.2241
Fitted a model with MAP estimate = -75.9394
Time for alignment: 36.5115
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4906 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7878 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8596 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1295 - loglik: -1.1992e+02 - logprior: -1.1194e+01
Epoch 6/10
10/10 - 1s - loss: 123.3728 - loglik: -1.1705e+02 - logprior: -6.1081e+00
Epoch 7/10
10/10 - 1s - loss: 118.4854 - loglik: -1.1518e+02 - logprior: -2.8880e+00
Epoch 8/10
10/10 - 1s - loss: 115.6751 - loglik: -1.1460e+02 - logprior: -7.2588e-01
Epoch 9/10
10/10 - 1s - loss: 114.0349 - loglik: -1.1460e+02 - logprior: 0.8671
Epoch 10/10
10/10 - 1s - loss: 112.9034 - loglik: -1.1464e+02 - logprior: 2.0632
Fitted a model with MAP estimate = -112.0910
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.6377 - loglik: -1.0999e+02 - logprior: -3.5763e+02
Epoch 2/2
10/10 - 1s - loss: 206.7057 - loglik: -9.7279e+01 - logprior: -1.0933e+02
Fitted a model with MAP estimate = -158.0575
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 400.4508 - loglik: -9.7354e+01 - logprior: -3.0308e+02
Epoch 2/2
10/10 - 1s - loss: 212.7400 - loglik: -9.5757e+01 - logprior: -1.1697e+02
Fitted a model with MAP estimate = -182.8716
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 372.3268 - loglik: -9.4781e+01 - logprior: -2.7752e+02
Epoch 2/10
10/10 - 1s - loss: 169.9350 - loglik: -9.4163e+01 - logprior: -7.5735e+01
Epoch 3/10
10/10 - 1s - loss: 119.2354 - loglik: -9.4414e+01 - logprior: -2.4818e+01
Epoch 4/10
10/10 - 1s - loss: 101.0824 - loglik: -9.4869e+01 - logprior: -6.2073e+00
Epoch 5/10
10/10 - 1s - loss: 91.6940 - loglik: -9.4771e+01 - logprior: 3.1710
Epoch 6/10
10/10 - 1s - loss: 86.1726 - loglik: -9.4487e+01 - logprior: 8.6010
Epoch 7/10
10/10 - 1s - loss: 82.7032 - loglik: -9.4526e+01 - logprior: 12.1478
Epoch 8/10
10/10 - 1s - loss: 80.2896 - loglik: -9.4714e+01 - logprior: 14.6926
Epoch 9/10
10/10 - 1s - loss: 78.4610 - loglik: -9.4852e+01 - logprior: 16.6454
Epoch 10/10
10/10 - 1s - loss: 76.9514 - loglik: -9.4929e+01 - logprior: 18.2516
Fitted a model with MAP estimate = -75.9209
Time for alignment: 36.4756
Computed alignments with likelihoods: ['-75.9499', '-75.9394', '-75.9209']
Best model has likelihood: -75.9209
time for generating output: 0.1421
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8474320241691843
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb5589bca00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4c6ab1640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad10314d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf02cfd00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7facf02cf040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb604509700>, <__main__.SimpleDirichletPrior object at 0x7fb4c5e9b2e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 688.8790 - loglik: -6.8445e+02 - logprior: -4.3183e+00
Epoch 2/10
26/26 - 11s - loss: 587.5458 - loglik: -5.8573e+02 - logprior: -1.6569e+00
Epoch 3/10
26/26 - 11s - loss: 572.9044 - loglik: -5.7069e+02 - logprior: -1.7675e+00
Epoch 4/10
26/26 - 11s - loss: 569.5805 - loglik: -5.6723e+02 - logprior: -1.7670e+00
Epoch 5/10
26/26 - 11s - loss: 566.7537 - loglik: -5.6428e+02 - logprior: -1.8203e+00
Epoch 6/10
26/26 - 11s - loss: 567.0085 - loglik: -5.6445e+02 - logprior: -1.8547e+00
Fitted a model with MAP estimate = -565.1342
expansions: [(76, 1), (95, 2), (174, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 585.9442 - loglik: -5.7854e+02 - logprior: -7.2746e+00
Epoch 2/2
26/26 - 12s - loss: 573.3951 - loglik: -5.6948e+02 - logprior: -3.5900e+00
Fitted a model with MAP estimate = -570.2651
expansions: [(0, 4)]
discards: [ 0 95 96]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 18s - loss: 578.0529 - loglik: -5.7305e+02 - logprior: -4.8846e+00
Epoch 2/2
26/26 - 11s - loss: 571.1749 - loglik: -5.6934e+02 - logprior: -1.4983e+00
Fitted a model with MAP estimate = -568.4109
expansions: []
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 575.4269 - loglik: -5.7071e+02 - logprior: -4.5902e+00
Epoch 2/10
26/26 - 11s - loss: 571.3811 - loglik: -5.6983e+02 - logprior: -1.1918e+00
Epoch 3/10
26/26 - 11s - loss: 568.9230 - loglik: -5.6735e+02 - logprior: -1.0704e+00
Epoch 4/10
26/26 - 11s - loss: 565.7648 - loglik: -5.6406e+02 - logprior: -1.0458e+00
Epoch 5/10
26/26 - 11s - loss: 564.2568 - loglik: -5.6258e+02 - logprior: -9.4299e-01
Epoch 6/10
26/26 - 11s - loss: 563.7632 - loglik: -5.6219e+02 - logprior: -8.3141e-01
Epoch 7/10
26/26 - 11s - loss: 563.9216 - loglik: -5.6248e+02 - logprior: -7.2320e-01
Fitted a model with MAP estimate = -562.5214
Time for alignment: 252.2716
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 691.0003 - loglik: -6.8659e+02 - logprior: -4.2967e+00
Epoch 2/10
26/26 - 11s - loss: 590.1707 - loglik: -5.8880e+02 - logprior: -1.2786e+00
Epoch 3/10
26/26 - 11s - loss: 576.2219 - loglik: -5.7433e+02 - logprior: -1.4964e+00
Epoch 4/10
26/26 - 11s - loss: 570.8044 - loglik: -5.6876e+02 - logprior: -1.5062e+00
Epoch 5/10
26/26 - 11s - loss: 569.7475 - loglik: -5.6762e+02 - logprior: -1.5177e+00
Epoch 6/10
26/26 - 11s - loss: 569.0358 - loglik: -5.6687e+02 - logprior: -1.5273e+00
Epoch 7/10
26/26 - 11s - loss: 569.1938 - loglik: -5.6700e+02 - logprior: -1.5590e+00
Fitted a model with MAP estimate = -566.9924
expansions: [(0, 3), (10, 1), (108, 4), (173, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 18s - loss: 583.0932 - loglik: -5.7626e+02 - logprior: -6.6900e+00
Epoch 2/2
26/26 - 12s - loss: 569.3462 - loglik: -5.6695e+02 - logprior: -1.9806e+00
Fitted a model with MAP estimate = -566.7192
expansions: [(114, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 575.8185 - loglik: -5.7101e+02 - logprior: -4.6747e+00
Epoch 2/2
26/26 - 12s - loss: 568.2673 - loglik: -5.6658e+02 - logprior: -1.2495e+00
Fitted a model with MAP estimate = -565.6890
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 573.8433 - loglik: -5.6927e+02 - logprior: -4.4486e+00
Epoch 2/10
26/26 - 12s - loss: 568.1271 - loglik: -5.6679e+02 - logprior: -9.4875e-01
Epoch 3/10
26/26 - 12s - loss: 564.7676 - loglik: -5.6331e+02 - logprior: -7.6086e-01
Epoch 4/10
26/26 - 12s - loss: 562.7095 - loglik: -5.6124e+02 - logprior: -6.9508e-01
Epoch 5/10
26/26 - 12s - loss: 561.1893 - loglik: -5.5985e+02 - logprior: -5.9934e-01
Epoch 6/10
26/26 - 12s - loss: 559.8546 - loglik: -5.5868e+02 - logprior: -4.5687e-01
Epoch 7/10
26/26 - 12s - loss: 560.6392 - loglik: -5.5961e+02 - logprior: -3.4206e-01
Fitted a model with MAP estimate = -558.6764
Time for alignment: 269.3032
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 691.3824 - loglik: -6.8697e+02 - logprior: -4.3053e+00
Epoch 2/10
26/26 - 11s - loss: 587.9801 - loglik: -5.8609e+02 - logprior: -1.6863e+00
Epoch 3/10
26/26 - 11s - loss: 571.7868 - loglik: -5.6937e+02 - logprior: -1.8365e+00
Epoch 4/10
26/26 - 11s - loss: 566.3931 - loglik: -5.6386e+02 - logprior: -1.9039e+00
Epoch 5/10
26/26 - 11s - loss: 565.4693 - loglik: -5.6288e+02 - logprior: -1.9229e+00
Epoch 6/10
26/26 - 11s - loss: 564.5016 - loglik: -5.6192e+02 - logprior: -1.9176e+00
Epoch 7/10
26/26 - 11s - loss: 564.1799 - loglik: -5.6157e+02 - logprior: -1.9406e+00
Epoch 8/10
26/26 - 11s - loss: 561.9022 - loglik: -5.5929e+02 - logprior: -1.9674e+00
Epoch 9/10
26/26 - 11s - loss: 563.2592 - loglik: -5.6064e+02 - logprior: -1.9684e+00
Fitted a model with MAP estimate = -561.7815
expansions: [(0, 3), (9, 1), (43, 1), (130, 1), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 17s - loss: 581.3477 - loglik: -5.7452e+02 - logprior: -6.6782e+00
Epoch 2/2
26/26 - 12s - loss: 566.1219 - loglik: -5.6368e+02 - logprior: -2.0283e+00
Fitted a model with MAP estimate = -563.6798
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 575.1912 - loglik: -5.7019e+02 - logprior: -4.8739e+00
Epoch 2/2
26/26 - 12s - loss: 566.9858 - loglik: -5.6516e+02 - logprior: -1.4197e+00
Fitted a model with MAP estimate = -564.8067
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 575.2590 - loglik: -5.7051e+02 - logprior: -4.6304e+00
Epoch 2/10
26/26 - 11s - loss: 565.7360 - loglik: -5.6418e+02 - logprior: -1.1796e+00
Epoch 3/10
26/26 - 11s - loss: 564.9766 - loglik: -5.6325e+02 - logprior: -1.0573e+00
Epoch 4/10
26/26 - 11s - loss: 561.7090 - loglik: -5.5991e+02 - logprior: -1.0339e+00
Epoch 5/10
26/26 - 12s - loss: 561.8804 - loglik: -5.6021e+02 - logprior: -9.2212e-01
Fitted a model with MAP estimate = -559.3057
Time for alignment: 262.7746
Computed alignments with likelihoods: ['-562.5214', '-558.6764', '-559.3057']
Best model has likelihood: -558.6764
time for generating output: 0.3832
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.6670967741935484
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4c5e9bf70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad00e11f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad00e114f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57b2709a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb57b270400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faccd7e8c40>, <__main__.SimpleDirichletPrior object at 0x7face87e22b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 422.9562 - loglik: -3.4503e+02 - logprior: -7.7899e+01
Epoch 2/10
10/10 - 2s - loss: 338.0176 - loglik: -3.1914e+02 - logprior: -1.8756e+01
Epoch 3/10
10/10 - 2s - loss: 299.4304 - loglik: -2.9180e+02 - logprior: -7.5347e+00
Epoch 4/10
10/10 - 2s - loss: 279.1547 - loglik: -2.7532e+02 - logprior: -3.7111e+00
Epoch 5/10
10/10 - 2s - loss: 270.6040 - loglik: -2.6834e+02 - logprior: -1.8930e+00
Epoch 6/10
10/10 - 2s - loss: 265.0217 - loglik: -2.6366e+02 - logprior: -8.4378e-01
Epoch 7/10
10/10 - 2s - loss: 261.6494 - loglik: -2.6093e+02 - logprior: -2.6059e-01
Epoch 8/10
10/10 - 2s - loss: 259.9689 - loglik: -2.5986e+02 - logprior: 0.3249
Epoch 9/10
10/10 - 2s - loss: 257.9769 - loglik: -2.5826e+02 - logprior: 0.7111
Epoch 10/10
10/10 - 2s - loss: 257.1131 - loglik: -2.5760e+02 - logprior: 0.9331
Fitted a model with MAP estimate = -256.3449
expansions: [(5, 3), (6, 2), (11, 5), (36, 4), (45, 3), (47, 2), (64, 3), (88, 7), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 351.4771 - loglik: -2.6433e+02 - logprior: -8.7119e+01
Epoch 2/2
10/10 - 2s - loss: 282.3980 - loglik: -2.4862e+02 - logprior: -3.3652e+01
Fitted a model with MAP estimate = -269.1458
expansions: [(0, 2), (17, 1), (111, 2)]
discards: [ 0  8 84 85]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 314.9450 - loglik: -2.4640e+02 - logprior: -6.8523e+01
Epoch 2/2
10/10 - 2s - loss: 254.9200 - loglik: -2.3971e+02 - logprior: -1.5098e+01
Fitted a model with MAP estimate = -244.9426
expansions: []
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 324.7139 - loglik: -2.4236e+02 - logprior: -8.2337e+01
Epoch 2/10
10/10 - 2s - loss: 261.6553 - loglik: -2.3880e+02 - logprior: -2.2742e+01
Epoch 3/10
10/10 - 2s - loss: 242.5806 - loglik: -2.3691e+02 - logprior: -5.4613e+00
Epoch 4/10
10/10 - 2s - loss: 235.8240 - loglik: -2.3593e+02 - logprior: 0.4278
Epoch 5/10
10/10 - 2s - loss: 231.9533 - loglik: -2.3464e+02 - logprior: 3.1512
Epoch 6/10
10/10 - 2s - loss: 229.8588 - loglik: -2.3415e+02 - logprior: 4.7613
Epoch 7/10
10/10 - 2s - loss: 228.8498 - loglik: -2.3415e+02 - logprior: 5.7393
Epoch 8/10
10/10 - 2s - loss: 227.7074 - loglik: -2.3371e+02 - logprior: 6.4261
Epoch 9/10
10/10 - 2s - loss: 226.7609 - loglik: -2.3334e+02 - logprior: 6.9932
Epoch 10/10
10/10 - 2s - loss: 226.7029 - loglik: -2.3381e+02 - logprior: 7.5238
Fitted a model with MAP estimate = -225.8134
Time for alignment: 63.2826
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.0201 - loglik: -3.4510e+02 - logprior: -7.7898e+01
Epoch 2/10
10/10 - 2s - loss: 338.2316 - loglik: -3.1936e+02 - logprior: -1.8748e+01
Epoch 3/10
10/10 - 2s - loss: 300.9127 - loglik: -2.9326e+02 - logprior: -7.5548e+00
Epoch 4/10
10/10 - 2s - loss: 280.5068 - loglik: -2.7651e+02 - logprior: -3.7675e+00
Epoch 5/10
10/10 - 2s - loss: 271.4396 - loglik: -2.6884e+02 - logprior: -2.0680e+00
Epoch 6/10
10/10 - 2s - loss: 267.3943 - loglik: -2.6590e+02 - logprior: -9.9461e-01
Epoch 7/10
10/10 - 2s - loss: 264.2994 - loglik: -2.6368e+02 - logprior: -2.8439e-01
Epoch 8/10
10/10 - 2s - loss: 262.6543 - loglik: -2.6261e+02 - logprior: 0.2566
Epoch 9/10
10/10 - 2s - loss: 261.4228 - loglik: -2.6171e+02 - logprior: 0.5983
Epoch 10/10
10/10 - 2s - loss: 260.9487 - loglik: -2.6148e+02 - logprior: 0.8375
Fitted a model with MAP estimate = -260.2267
expansions: [(5, 1), (6, 1), (10, 2), (12, 4), (18, 2), (36, 4), (50, 2), (62, 4), (78, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 352.8888 - loglik: -2.6563e+02 - logprior: -8.7238e+01
Epoch 2/2
10/10 - 2s - loss: 283.7330 - loglik: -2.4976e+02 - logprior: -3.3845e+01
Fitted a model with MAP estimate = -270.9886
expansions: [(0, 3)]
discards: [  0  97 105]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 316.3134 - loglik: -2.4758e+02 - logprior: -6.8708e+01
Epoch 2/2
10/10 - 2s - loss: 257.6238 - loglik: -2.4193e+02 - logprior: -1.5540e+01
Fitted a model with MAP estimate = -247.8102
expansions: [(115, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 327.7467 - loglik: -2.4355e+02 - logprior: -8.4177e+01
Epoch 2/10
10/10 - 2s - loss: 266.3694 - loglik: -2.3843e+02 - logprior: -2.7828e+01
Epoch 3/10
10/10 - 2s - loss: 244.0907 - loglik: -2.3553e+02 - logprior: -8.3210e+00
Epoch 4/10
10/10 - 2s - loss: 234.0277 - loglik: -2.3360e+02 - logprior: -1.0473e-02
Epoch 5/10
10/10 - 2s - loss: 230.1530 - loglik: -2.3273e+02 - logprior: 3.0850
Epoch 6/10
10/10 - 2s - loss: 227.8183 - loglik: -2.3208e+02 - logprior: 4.7227
Epoch 7/10
10/10 - 2s - loss: 227.1476 - loglik: -2.3244e+02 - logprior: 5.7106
Epoch 8/10
10/10 - 2s - loss: 225.9931 - loglik: -2.3200e+02 - logprior: 6.4239
Epoch 9/10
10/10 - 2s - loss: 224.8704 - loglik: -2.3146e+02 - logprior: 6.9956
Epoch 10/10
10/10 - 2s - loss: 224.8184 - loglik: -2.3192e+02 - logprior: 7.5115
Fitted a model with MAP estimate = -223.9725
Time for alignment: 64.8257
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.9792 - loglik: -3.4506e+02 - logprior: -7.7898e+01
Epoch 2/10
10/10 - 2s - loss: 337.9460 - loglik: -3.1908e+02 - logprior: -1.8748e+01
Epoch 3/10
10/10 - 2s - loss: 299.2419 - loglik: -2.9159e+02 - logprior: -7.5564e+00
Epoch 4/10
10/10 - 2s - loss: 279.5434 - loglik: -2.7559e+02 - logprior: -3.7457e+00
Epoch 5/10
10/10 - 2s - loss: 271.2263 - loglik: -2.6881e+02 - logprior: -1.9098e+00
Epoch 6/10
10/10 - 2s - loss: 267.2957 - loglik: -2.6593e+02 - logprior: -8.5972e-01
Epoch 7/10
10/10 - 2s - loss: 265.1779 - loglik: -2.6468e+02 - logprior: -1.5552e-01
Epoch 8/10
10/10 - 2s - loss: 263.5073 - loglik: -2.6361e+02 - logprior: 0.3966
Epoch 9/10
10/10 - 2s - loss: 263.1658 - loglik: -2.6370e+02 - logprior: 0.8267
Epoch 10/10
10/10 - 2s - loss: 262.0504 - loglik: -2.6282e+02 - logprior: 1.0756
Fitted a model with MAP estimate = -261.6715
expansions: [(5, 3), (6, 2), (12, 4), (18, 2), (36, 4), (45, 2), (79, 2), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 353.9299 - loglik: -2.6661e+02 - logprior: -8.7298e+01
Epoch 2/2
10/10 - 2s - loss: 286.9189 - loglik: -2.5276e+02 - logprior: -3.4036e+01
Fitted a model with MAP estimate = -274.4481
expansions: [(0, 3), (79, 3), (112, 2)]
discards: [  0   8  46  95 102]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.1973 - loglik: -2.4945e+02 - logprior: -6.8726e+01
Epoch 2/2
10/10 - 2s - loss: 258.5255 - loglik: -2.4316e+02 - logprior: -1.5214e+01
Fitted a model with MAP estimate = -248.3196
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 327.8979 - loglik: -2.4542e+02 - logprior: -8.2459e+01
Epoch 2/10
10/10 - 2s - loss: 263.5587 - loglik: -2.4041e+02 - logprior: -2.3028e+01
Epoch 3/10
10/10 - 2s - loss: 243.7937 - loglik: -2.3797e+02 - logprior: -5.5559e+00
Epoch 4/10
10/10 - 2s - loss: 236.1797 - loglik: -2.3619e+02 - logprior: 0.4638
Epoch 5/10
10/10 - 2s - loss: 232.7541 - loglik: -2.3558e+02 - logprior: 3.3203
Epoch 6/10
10/10 - 2s - loss: 230.6741 - loglik: -2.3515e+02 - logprior: 4.9141
Epoch 7/10
10/10 - 2s - loss: 229.3506 - loglik: -2.3474e+02 - logprior: 5.8283
Epoch 8/10
10/10 - 2s - loss: 228.5497 - loglik: -2.3461e+02 - logprior: 6.4931
Epoch 9/10
10/10 - 2s - loss: 227.7987 - loglik: -2.3444e+02 - logprior: 7.0690
Epoch 10/10
10/10 - 2s - loss: 227.4716 - loglik: -2.3464e+02 - logprior: 7.6007
Fitted a model with MAP estimate = -226.6403
Time for alignment: 61.3600
Computed alignments with likelihoods: ['-225.8134', '-223.9725', '-226.6403']
Best model has likelihood: -223.9725
time for generating output: 0.2161
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.8079136690647482
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb514a0a9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf05248b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb560d19670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6427c0e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4bc0c9cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fad10550850>, <__main__.SimpleDirichletPrior object at 0x7fb2bd2ae3a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2582 - loglik: -9.2792e+01 - logprior: -4.4452e+00
Epoch 2/10
17/17 - 1s - loss: 75.2577 - loglik: -7.3690e+01 - logprior: -1.5558e+00
Epoch 3/10
17/17 - 1s - loss: 65.1882 - loglik: -6.3463e+01 - logprior: -1.6774e+00
Epoch 4/10
17/17 - 1s - loss: 63.6280 - loglik: -6.1821e+01 - logprior: -1.6746e+00
Epoch 5/10
17/17 - 1s - loss: 63.0313 - loglik: -6.1348e+01 - logprior: -1.5749e+00
Epoch 6/10
17/17 - 1s - loss: 62.8740 - loglik: -6.1150e+01 - logprior: -1.5900e+00
Epoch 7/10
17/17 - 1s - loss: 62.7285 - loglik: -6.0995e+01 - logprior: -1.5762e+00
Epoch 8/10
17/17 - 1s - loss: 62.6920 - loglik: -6.0960e+01 - logprior: -1.5653e+00
Epoch 9/10
17/17 - 1s - loss: 62.6286 - loglik: -6.0885e+01 - logprior: -1.5567e+00
Epoch 10/10
17/17 - 1s - loss: 62.6022 - loglik: -6.0859e+01 - logprior: -1.5480e+00
Fitted a model with MAP estimate = -62.3760
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2886 - loglik: -6.4696e+01 - logprior: -5.5481e+00
Epoch 2/2
17/17 - 1s - loss: 62.0593 - loglik: -5.9288e+01 - logprior: -2.6313e+00
Fitted a model with MAP estimate = -59.4858
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 62.3228 - loglik: -5.7881e+01 - logprior: -4.3992e+00
Epoch 2/2
17/17 - 1s - loss: 58.5831 - loglik: -5.6769e+01 - logprior: -1.7017e+00
Fitted a model with MAP estimate = -58.0945
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.5860 - loglik: -5.7275e+01 - logprior: -4.2717e+00
Epoch 2/10
17/17 - 1s - loss: 58.4406 - loglik: -5.6651e+01 - logprior: -1.6838e+00
Epoch 3/10
17/17 - 1s - loss: 58.1234 - loglik: -5.6550e+01 - logprior: -1.4565e+00
Epoch 4/10
17/17 - 1s - loss: 57.8878 - loglik: -5.6335e+01 - logprior: -1.3954e+00
Epoch 5/10
17/17 - 1s - loss: 57.6550 - loglik: -5.6107e+01 - logprior: -1.3650e+00
Epoch 6/10
17/17 - 1s - loss: 57.5566 - loglik: -5.6017e+01 - logprior: -1.3414e+00
Epoch 7/10
17/17 - 1s - loss: 57.6775 - loglik: -5.6137e+01 - logprior: -1.3285e+00
Fitted a model with MAP estimate = -57.2428
Time for alignment: 41.7607
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1729 - loglik: -9.2709e+01 - logprior: -4.4430e+00
Epoch 2/10
17/17 - 1s - loss: 74.8163 - loglik: -7.3247e+01 - logprior: -1.5569e+00
Epoch 3/10
17/17 - 1s - loss: 65.2826 - loglik: -6.3600e+01 - logprior: -1.6781e+00
Epoch 4/10
17/17 - 1s - loss: 63.5731 - loglik: -6.1882e+01 - logprior: -1.6492e+00
Epoch 5/10
17/17 - 1s - loss: 63.0418 - loglik: -6.1337e+01 - logprior: -1.5734e+00
Epoch 6/10
17/17 - 1s - loss: 63.0195 - loglik: -6.1314e+01 - logprior: -1.5879e+00
Epoch 7/10
17/17 - 1s - loss: 62.8244 - loglik: -6.1110e+01 - logprior: -1.5711e+00
Epoch 8/10
17/17 - 1s - loss: 62.6729 - loglik: -6.0964e+01 - logprior: -1.5616e+00
Epoch 9/10
17/17 - 1s - loss: 62.7085 - loglik: -6.0983e+01 - logprior: -1.5532e+00
Fitted a model with MAP estimate = -62.4368
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.3192 - loglik: -6.4733e+01 - logprior: -5.5453e+00
Epoch 2/2
17/17 - 1s - loss: 61.7821 - loglik: -5.9045e+01 - logprior: -2.6084e+00
Fitted a model with MAP estimate = -59.3812
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 62.3376 - loglik: -5.7913e+01 - logprior: -4.3828e+00
Epoch 2/2
17/17 - 1s - loss: 58.5409 - loglik: -5.6728e+01 - logprior: -1.7079e+00
Fitted a model with MAP estimate = -58.1011
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6232 - loglik: -5.7314e+01 - logprior: -4.2693e+00
Epoch 2/10
17/17 - 1s - loss: 58.4264 - loglik: -5.6633e+01 - logprior: -1.6887e+00
Epoch 3/10
17/17 - 1s - loss: 58.1238 - loglik: -5.6550e+01 - logprior: -1.4545e+00
Epoch 4/10
17/17 - 1s - loss: 57.8071 - loglik: -5.6244e+01 - logprior: -1.4055e+00
Epoch 5/10
17/17 - 1s - loss: 57.8340 - loglik: -5.6292e+01 - logprior: -1.3597e+00
Fitted a model with MAP estimate = -57.4139
Time for alignment: 38.3969
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1200 - loglik: -9.2656e+01 - logprior: -4.4434e+00
Epoch 2/10
17/17 - 1s - loss: 75.3291 - loglik: -7.3748e+01 - logprior: -1.5681e+00
Epoch 3/10
17/17 - 1s - loss: 65.5505 - loglik: -6.3827e+01 - logprior: -1.6958e+00
Epoch 4/10
17/17 - 1s - loss: 63.2954 - loglik: -6.1471e+01 - logprior: -1.6791e+00
Epoch 5/10
17/17 - 1s - loss: 62.5675 - loglik: -6.0823e+01 - logprior: -1.6002e+00
Epoch 6/10
17/17 - 1s - loss: 62.4961 - loglik: -6.0721e+01 - logprior: -1.6152e+00
Epoch 7/10
17/17 - 1s - loss: 62.2100 - loglik: -6.0433e+01 - logprior: -1.5917e+00
Epoch 8/10
17/17 - 1s - loss: 62.1939 - loglik: -6.0436e+01 - logprior: -1.5830e+00
Epoch 9/10
17/17 - 1s - loss: 62.1185 - loglik: -6.0355e+01 - logprior: -1.5673e+00
Epoch 10/10
17/17 - 1s - loss: 62.1709 - loglik: -6.0404e+01 - logprior: -1.5631e+00
Fitted a model with MAP estimate = -61.8703
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.9385 - loglik: -6.4358e+01 - logprior: -5.5358e+00
Epoch 2/2
17/17 - 1s - loss: 61.8350 - loglik: -5.9060e+01 - logprior: -2.6291e+00
Fitted a model with MAP estimate = -59.4557
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 62.4520 - loglik: -5.8012e+01 - logprior: -4.3989e+00
Epoch 2/2
17/17 - 1s - loss: 58.5795 - loglik: -5.6768e+01 - logprior: -1.7011e+00
Fitted a model with MAP estimate = -58.1094
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6616 - loglik: -5.7355e+01 - logprior: -4.2663e+00
Epoch 2/10
17/17 - 1s - loss: 58.4158 - loglik: -5.6624e+01 - logprior: -1.6880e+00
Epoch 3/10
17/17 - 1s - loss: 58.0594 - loglik: -5.6477e+01 - logprior: -1.4617e+00
Epoch 4/10
17/17 - 1s - loss: 57.9011 - loglik: -5.6344e+01 - logprior: -1.4012e+00
Epoch 5/10
17/17 - 1s - loss: 57.6882 - loglik: -5.6142e+01 - logprior: -1.3621e+00
Epoch 6/10
17/17 - 1s - loss: 57.5100 - loglik: -5.5965e+01 - logprior: -1.3498e+00
Epoch 7/10
17/17 - 1s - loss: 57.5781 - loglik: -5.6042e+01 - logprior: -1.3236e+00
Fitted a model with MAP estimate = -57.2494
Time for alignment: 40.9849
Computed alignments with likelihoods: ['-57.2428', '-57.4139', '-57.2494']
Best model has likelihood: -57.2428
time for generating output: 0.1226
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7456580470860672
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4e157d910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb67da408e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4c79d6bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb54f365580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb6058b2a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb6058b28e0>, <__main__.SimpleDirichletPrior object at 0x7fb3544a5520>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.8090 - loglik: -1.8563e+02 - logprior: -8.1674e+00
Epoch 2/10
13/13 - 1s - loss: 163.8727 - loglik: -1.6158e+02 - logprior: -2.2880e+00
Epoch 3/10
13/13 - 1s - loss: 146.8162 - loglik: -1.4474e+02 - logprior: -2.0186e+00
Epoch 4/10
13/13 - 1s - loss: 140.5420 - loglik: -1.3817e+02 - logprior: -2.1506e+00
Epoch 5/10
13/13 - 1s - loss: 135.9982 - loglik: -1.3362e+02 - logprior: -2.0419e+00
Epoch 6/10
13/13 - 1s - loss: 135.0413 - loglik: -1.3280e+02 - logprior: -1.9739e+00
Epoch 7/10
13/13 - 1s - loss: 133.9581 - loglik: -1.3174e+02 - logprior: -1.9973e+00
Epoch 8/10
13/13 - 1s - loss: 134.0265 - loglik: -1.3187e+02 - logprior: -1.9659e+00
Fitted a model with MAP estimate = -133.6508
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.4583 - loglik: -1.3475e+02 - logprior: -9.6828e+00
Epoch 2/2
13/13 - 1s - loss: 129.8307 - loglik: -1.2501e+02 - logprior: -4.6390e+00
Fitted a model with MAP estimate = -127.1190
expansions: [(0, 2)]
discards: [ 0 25 56]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 130.9018 - loglik: -1.2342e+02 - logprior: -7.4408e+00
Epoch 2/2
13/13 - 1s - loss: 123.8853 - loglik: -1.2140e+02 - logprior: -2.3284e+00
Fitted a model with MAP estimate = -122.2618
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.8971 - loglik: -1.2448e+02 - logprior: -9.3796e+00
Epoch 2/10
13/13 - 1s - loss: 124.8201 - loglik: -1.2106e+02 - logprior: -3.6083e+00
Epoch 3/10
13/13 - 1s - loss: 121.9070 - loglik: -1.1975e+02 - logprior: -1.8541e+00
Epoch 4/10
13/13 - 1s - loss: 120.1617 - loglik: -1.1834e+02 - logprior: -1.5019e+00
Epoch 5/10
13/13 - 1s - loss: 119.5811 - loglik: -1.1795e+02 - logprior: -1.3740e+00
Epoch 6/10
13/13 - 1s - loss: 119.2160 - loglik: -1.1759e+02 - logprior: -1.3918e+00
Epoch 7/10
13/13 - 1s - loss: 118.4339 - loglik: -1.1681e+02 - logprior: -1.4075e+00
Epoch 8/10
13/13 - 1s - loss: 118.8138 - loglik: -1.1724e+02 - logprior: -1.3692e+00
Fitted a model with MAP estimate = -118.3048
Time for alignment: 43.3532
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.8077 - loglik: -1.8563e+02 - logprior: -8.1666e+00
Epoch 2/10
13/13 - 1s - loss: 163.0820 - loglik: -1.6080e+02 - logprior: -2.2810e+00
Epoch 3/10
13/13 - 1s - loss: 145.9773 - loglik: -1.4392e+02 - logprior: -1.9999e+00
Epoch 4/10
13/13 - 1s - loss: 139.2137 - loglik: -1.3681e+02 - logprior: -2.1893e+00
Epoch 5/10
13/13 - 1s - loss: 135.9413 - loglik: -1.3356e+02 - logprior: -2.1009e+00
Epoch 6/10
13/13 - 1s - loss: 134.6236 - loglik: -1.3239e+02 - logprior: -2.0002e+00
Epoch 7/10
13/13 - 1s - loss: 133.8707 - loglik: -1.3165e+02 - logprior: -2.0308e+00
Epoch 8/10
13/13 - 1s - loss: 133.2411 - loglik: -1.3102e+02 - logprior: -2.0420e+00
Epoch 9/10
13/13 - 1s - loss: 132.5317 - loglik: -1.3030e+02 - logprior: -2.0533e+00
Epoch 10/10
13/13 - 1s - loss: 132.4643 - loglik: -1.3024e+02 - logprior: -2.0477e+00
Fitted a model with MAP estimate = -132.2342
expansions: [(12, 1), (13, 1), (17, 5), (18, 2), (33, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.9467 - loglik: -1.3426e+02 - logprior: -9.6564e+00
Epoch 2/2
13/13 - 1s - loss: 128.9566 - loglik: -1.2420e+02 - logprior: -4.5746e+00
Fitted a model with MAP estimate = -126.2746
expansions: [(0, 2)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 129.9194 - loglik: -1.2248e+02 - logprior: -7.4026e+00
Epoch 2/2
13/13 - 1s - loss: 122.7831 - loglik: -1.2037e+02 - logprior: -2.2602e+00
Fitted a model with MAP estimate = -121.0758
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 132.4914 - loglik: -1.2310e+02 - logprior: -9.3537e+00
Epoch 2/10
13/13 - 1s - loss: 124.9451 - loglik: -1.2123e+02 - logprior: -3.5633e+00
Epoch 3/10
13/13 - 1s - loss: 121.1997 - loglik: -1.1909e+02 - logprior: -1.8376e+00
Epoch 4/10
13/13 - 1s - loss: 120.1818 - loglik: -1.1837e+02 - logprior: -1.4930e+00
Epoch 5/10
13/13 - 1s - loss: 119.5380 - loglik: -1.1788e+02 - logprior: -1.3768e+00
Epoch 6/10
13/13 - 1s - loss: 119.4533 - loglik: -1.1782e+02 - logprior: -1.3832e+00
Epoch 7/10
13/13 - 1s - loss: 118.8080 - loglik: -1.1718e+02 - logprior: -1.4011e+00
Epoch 8/10
13/13 - 1s - loss: 118.4846 - loglik: -1.1691e+02 - logprior: -1.3665e+00
Epoch 9/10
13/13 - 1s - loss: 118.5635 - loglik: -1.1702e+02 - logprior: -1.3387e+00
Fitted a model with MAP estimate = -118.2411
Time for alignment: 45.4811
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.8545 - loglik: -1.8568e+02 - logprior: -8.1675e+00
Epoch 2/10
13/13 - 1s - loss: 163.4377 - loglik: -1.6115e+02 - logprior: -2.2887e+00
Epoch 3/10
13/13 - 1s - loss: 145.9276 - loglik: -1.4384e+02 - logprior: -2.0101e+00
Epoch 4/10
13/13 - 1s - loss: 138.8091 - loglik: -1.3634e+02 - logprior: -2.1581e+00
Epoch 5/10
13/13 - 1s - loss: 136.0835 - loglik: -1.3372e+02 - logprior: -2.0461e+00
Epoch 6/10
13/13 - 1s - loss: 134.4705 - loglik: -1.3226e+02 - logprior: -1.9558e+00
Epoch 7/10
13/13 - 1s - loss: 134.3559 - loglik: -1.3219e+02 - logprior: -1.9663e+00
Epoch 8/10
13/13 - 1s - loss: 134.1112 - loglik: -1.3198e+02 - logprior: -1.9536e+00
Epoch 9/10
13/13 - 1s - loss: 133.3434 - loglik: -1.3123e+02 - logprior: -1.9466e+00
Epoch 10/10
13/13 - 1s - loss: 133.8082 - loglik: -1.3170e+02 - logprior: -1.9408e+00
Fitted a model with MAP estimate = -133.3344
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 144.5799 - loglik: -1.3487e+02 - logprior: -9.6809e+00
Epoch 2/2
13/13 - 1s - loss: 129.3136 - loglik: -1.2450e+02 - logprior: -4.6313e+00
Fitted a model with MAP estimate = -127.0880
expansions: [(0, 2)]
discards: [ 0 24 56]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.1382 - loglik: -1.2367e+02 - logprior: -7.4319e+00
Epoch 2/2
13/13 - 1s - loss: 123.9519 - loglik: -1.2151e+02 - logprior: -2.3034e+00
Fitted a model with MAP estimate = -122.5530
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.1170 - loglik: -1.2470e+02 - logprior: -9.3758e+00
Epoch 2/10
13/13 - 1s - loss: 125.5484 - loglik: -1.2177e+02 - logprior: -3.6169e+00
Epoch 3/10
13/13 - 1s - loss: 121.7782 - loglik: -1.1962e+02 - logprior: -1.8602e+00
Epoch 4/10
13/13 - 1s - loss: 120.6034 - loglik: -1.1878e+02 - logprior: -1.5056e+00
Epoch 5/10
13/13 - 1s - loss: 119.6778 - loglik: -1.1803e+02 - logprior: -1.3798e+00
Epoch 6/10
13/13 - 1s - loss: 119.3365 - loglik: -1.1769e+02 - logprior: -1.3985e+00
Epoch 7/10
13/13 - 1s - loss: 118.8110 - loglik: -1.1718e+02 - logprior: -1.4123e+00
Epoch 8/10
13/13 - 1s - loss: 119.0512 - loglik: -1.1747e+02 - logprior: -1.3690e+00
Fitted a model with MAP estimate = -118.5231
Time for alignment: 45.1673
Computed alignments with likelihoods: ['-118.3048', '-118.2411', '-118.5231']
Best model has likelihood: -118.2411
time for generating output: 0.1357
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7facf132ea30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb5037d9190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb54fb73fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2bd2870a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb630d98f70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb53f0269d0>, <__main__.SimpleDirichletPrior object at 0x7fb2bc7d55b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0688 - loglik: -2.2839e+02 - logprior: -1.2676e+01
Epoch 2/10
11/11 - 1s - loss: 208.4064 - loglik: -2.0502e+02 - logprior: -3.3826e+00
Epoch 3/10
11/11 - 1s - loss: 183.2898 - loglik: -1.8097e+02 - logprior: -2.1880e+00
Epoch 4/10
11/11 - 1s - loss: 166.1274 - loglik: -1.6380e+02 - logprior: -1.9874e+00
Epoch 5/10
11/11 - 1s - loss: 160.2971 - loglik: -1.5807e+02 - logprior: -1.7887e+00
Epoch 6/10
11/11 - 1s - loss: 157.7772 - loglik: -1.5570e+02 - logprior: -1.6511e+00
Epoch 7/10
11/11 - 1s - loss: 157.1621 - loglik: -1.5545e+02 - logprior: -1.4090e+00
Epoch 8/10
11/11 - 1s - loss: 156.6564 - loglik: -1.5514e+02 - logprior: -1.2705e+00
Epoch 9/10
11/11 - 1s - loss: 156.2875 - loglik: -1.5487e+02 - logprior: -1.1810e+00
Epoch 10/10
11/11 - 1s - loss: 155.7844 - loglik: -1.5440e+02 - logprior: -1.1442e+00
Fitted a model with MAP estimate = -155.5065
expansions: [(0, 6), (22, 1), (23, 2), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 173.3759 - loglik: -1.5746e+02 - logprior: -1.5891e+01
Epoch 2/2
11/11 - 1s - loss: 151.2168 - loglik: -1.4641e+02 - logprior: -4.6676e+00
Fitted a model with MAP estimate = -147.1489
expansions: []
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 162.4755 - loglik: -1.4814e+02 - logprior: -1.4308e+01
Epoch 2/2
11/11 - 1s - loss: 151.0571 - loglik: -1.4521e+02 - logprior: -5.6870e+00
Fitted a model with MAP estimate = -148.2922
expansions: []
discards: [37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.3863 - loglik: -1.4593e+02 - logprior: -1.2431e+01
Epoch 2/10
11/11 - 1s - loss: 147.8479 - loglik: -1.4444e+02 - logprior: -3.2589e+00
Epoch 3/10
11/11 - 1s - loss: 145.2348 - loglik: -1.4298e+02 - logprior: -2.0016e+00
Epoch 4/10
11/11 - 1s - loss: 143.8937 - loglik: -1.4221e+02 - logprior: -1.3867e+00
Epoch 5/10
11/11 - 1s - loss: 143.3159 - loglik: -1.4194e+02 - logprior: -1.0806e+00
Epoch 6/10
11/11 - 1s - loss: 143.0258 - loglik: -1.4175e+02 - logprior: -1.0050e+00
Epoch 7/10
11/11 - 1s - loss: 142.5421 - loglik: -1.4142e+02 - logprior: -8.6953e-01
Epoch 8/10
11/11 - 1s - loss: 142.4247 - loglik: -1.4133e+02 - logprior: -8.4754e-01
Epoch 9/10
11/11 - 1s - loss: 142.0460 - loglik: -1.4098e+02 - logprior: -8.0493e-01
Epoch 10/10
11/11 - 1s - loss: 141.9845 - loglik: -1.4096e+02 - logprior: -7.6948e-01
Fitted a model with MAP estimate = -141.6838
Time for alignment: 47.5426
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0760 - loglik: -2.2840e+02 - logprior: -1.2674e+01
Epoch 2/10
11/11 - 1s - loss: 208.3142 - loglik: -2.0493e+02 - logprior: -3.3795e+00
Epoch 3/10
11/11 - 1s - loss: 181.5459 - loglik: -1.7921e+02 - logprior: -2.1970e+00
Epoch 4/10
11/11 - 1s - loss: 165.5473 - loglik: -1.6315e+02 - logprior: -1.9965e+00
Epoch 5/10
11/11 - 1s - loss: 160.2496 - loglik: -1.5796e+02 - logprior: -1.7763e+00
Epoch 6/10
11/11 - 1s - loss: 157.7231 - loglik: -1.5572e+02 - logprior: -1.6303e+00
Epoch 7/10
11/11 - 1s - loss: 156.9449 - loglik: -1.5526e+02 - logprior: -1.4246e+00
Epoch 8/10
11/11 - 1s - loss: 156.3383 - loglik: -1.5482e+02 - logprior: -1.2942e+00
Epoch 9/10
11/11 - 1s - loss: 155.8137 - loglik: -1.5439e+02 - logprior: -1.2045e+00
Epoch 10/10
11/11 - 1s - loss: 155.7720 - loglik: -1.5438e+02 - logprior: -1.1628e+00
Fitted a model with MAP estimate = -155.3813
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 171.7606 - loglik: -1.5585e+02 - logprior: -1.5891e+01
Epoch 2/2
11/11 - 1s - loss: 150.5421 - loglik: -1.4585e+02 - logprior: -4.5590e+00
Fitted a model with MAP estimate = -146.9118
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.6171 - loglik: -1.4832e+02 - logprior: -1.4267e+01
Epoch 2/2
11/11 - 1s - loss: 151.0724 - loglik: -1.4529e+02 - logprior: -5.6300e+00
Fitted a model with MAP estimate = -148.4710
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.2934 - loglik: -1.4583e+02 - logprior: -1.2434e+01
Epoch 2/10
11/11 - 1s - loss: 147.9199 - loglik: -1.4451e+02 - logprior: -3.2646e+00
Epoch 3/10
11/11 - 1s - loss: 145.2822 - loglik: -1.4302e+02 - logprior: -2.0091e+00
Epoch 4/10
11/11 - 1s - loss: 144.1566 - loglik: -1.4247e+02 - logprior: -1.3894e+00
Epoch 5/10
11/11 - 1s - loss: 143.1923 - loglik: -1.4183e+02 - logprior: -1.0775e+00
Epoch 6/10
11/11 - 1s - loss: 142.7556 - loglik: -1.4149e+02 - logprior: -1.0100e+00
Epoch 7/10
11/11 - 1s - loss: 142.6762 - loglik: -1.4155e+02 - logprior: -8.7329e-01
Epoch 8/10
11/11 - 1s - loss: 142.4138 - loglik: -1.4130e+02 - logprior: -8.5342e-01
Epoch 9/10
11/11 - 1s - loss: 141.9704 - loglik: -1.4091e+02 - logprior: -8.0354e-01
Epoch 10/10
11/11 - 1s - loss: 141.9335 - loglik: -1.4090e+02 - logprior: -7.7883e-01
Fitted a model with MAP estimate = -141.6764
Time for alignment: 46.4459
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9810 - loglik: -2.2831e+02 - logprior: -1.2672e+01
Epoch 2/10
11/11 - 1s - loss: 208.2473 - loglik: -2.0487e+02 - logprior: -3.3740e+00
Epoch 3/10
11/11 - 1s - loss: 183.5702 - loglik: -1.8126e+02 - logprior: -2.1761e+00
Epoch 4/10
11/11 - 1s - loss: 170.2847 - loglik: -1.6800e+02 - logprior: -1.9381e+00
Epoch 5/10
11/11 - 1s - loss: 163.2453 - loglik: -1.6108e+02 - logprior: -1.7118e+00
Epoch 6/10
11/11 - 1s - loss: 160.1023 - loglik: -1.5806e+02 - logprior: -1.5817e+00
Epoch 7/10
11/11 - 1s - loss: 158.8360 - loglik: -1.5710e+02 - logprior: -1.4007e+00
Epoch 8/10
11/11 - 1s - loss: 157.7954 - loglik: -1.5624e+02 - logprior: -1.2905e+00
Epoch 9/10
11/11 - 1s - loss: 157.2793 - loglik: -1.5583e+02 - logprior: -1.2098e+00
Epoch 10/10
11/11 - 1s - loss: 156.9224 - loglik: -1.5554e+02 - logprior: -1.1593e+00
Fitted a model with MAP estimate = -156.5176
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (46, 1), (47, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 173.5510 - loglik: -1.5765e+02 - logprior: -1.5873e+01
Epoch 2/2
11/11 - 1s - loss: 151.2072 - loglik: -1.4648e+02 - logprior: -4.5924e+00
Fitted a model with MAP estimate = -147.2691
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.4657 - loglik: -1.4813e+02 - logprior: -1.4305e+01
Epoch 2/2
11/11 - 1s - loss: 151.3773 - loglik: -1.4554e+02 - logprior: -5.6847e+00
Fitted a model with MAP estimate = -148.3168
expansions: []
discards: [37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.5627 - loglik: -1.4611e+02 - logprior: -1.2420e+01
Epoch 2/10
11/11 - 1s - loss: 147.8071 - loglik: -1.4441e+02 - logprior: -3.2576e+00
Epoch 3/10
11/11 - 1s - loss: 145.2937 - loglik: -1.4303e+02 - logprior: -2.0168e+00
Epoch 4/10
11/11 - 1s - loss: 143.7061 - loglik: -1.4202e+02 - logprior: -1.3941e+00
Epoch 5/10
11/11 - 1s - loss: 143.6497 - loglik: -1.4227e+02 - logprior: -1.0857e+00
Epoch 6/10
11/11 - 1s - loss: 142.6680 - loglik: -1.4139e+02 - logprior: -1.0128e+00
Epoch 7/10
11/11 - 1s - loss: 142.6768 - loglik: -1.4154e+02 - logprior: -8.7825e-01
Fitted a model with MAP estimate = -142.1427
Time for alignment: 43.4799
Computed alignments with likelihoods: ['-141.6838', '-141.6764', '-142.1427']
Best model has likelihood: -141.6764
time for generating output: 0.1333
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fad013d39a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6972bfe50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb2bc576e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb2be6347c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb61ffd76a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb5037197f0>, <__main__.SimpleDirichletPrior object at 0x7fb51470ea30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 526.0032 - loglik: -5.2034e+02 - logprior: -5.6460e+00
Epoch 2/10
24/24 - 8s - loss: 374.3215 - loglik: -3.7160e+02 - logprior: -2.6395e+00
Epoch 3/10
24/24 - 8s - loss: 344.1204 - loglik: -3.4061e+02 - logprior: -3.1572e+00
Epoch 4/10
24/24 - 7s - loss: 342.4297 - loglik: -3.3913e+02 - logprior: -2.8886e+00
Epoch 5/10
24/24 - 8s - loss: 340.0677 - loglik: -3.3681e+02 - logprior: -2.8623e+00
Epoch 6/10
24/24 - 8s - loss: 339.2471 - loglik: -3.3596e+02 - logprior: -2.8892e+00
Epoch 7/10
24/24 - 8s - loss: 339.8508 - loglik: -3.3654e+02 - logprior: -2.9059e+00
Fitted a model with MAP estimate = -338.5585
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (34, 2), (35, 1), (36, 1), (37, 2), (39, 2), (40, 1), (47, 1), (48, 1), (61, 1), (65, 1), (75, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 3), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (155, 1), (157, 1), (171, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 335.3490 - loglik: -3.2739e+02 - logprior: -7.8612e+00
Epoch 2/2
24/24 - 9s - loss: 315.0567 - loglik: -3.1137e+02 - logprior: -3.2536e+00
Fitted a model with MAP estimate = -310.8318
expansions: [(0, 2), (197, 1)]
discards: [  0  13  25  50  54 152 221 222]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 320.1649 - loglik: -3.1502e+02 - logprior: -5.0443e+00
Epoch 2/2
24/24 - 9s - loss: 309.1481 - loglik: -3.0807e+02 - logprior: -6.8551e-01
Fitted a model with MAP estimate = -306.5017
expansions: [(218, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 315.9750 - loglik: -3.1119e+02 - logprior: -4.6886e+00
Epoch 2/10
24/24 - 9s - loss: 307.4158 - loglik: -3.0674e+02 - logprior: -2.9403e-01
Epoch 3/10
24/24 - 9s - loss: 305.6155 - loglik: -3.0523e+02 - logprior: 0.1668
Epoch 4/10
24/24 - 9s - loss: 302.3159 - loglik: -3.0206e+02 - logprior: 0.3466
Epoch 5/10
24/24 - 9s - loss: 303.6414 - loglik: -3.0356e+02 - logprior: 0.4907
Fitted a model with MAP estimate = -301.5418
Time for alignment: 185.8609
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 526.9931 - loglik: -5.2131e+02 - logprior: -5.6704e+00
Epoch 2/10
24/24 - 8s - loss: 382.8654 - loglik: -3.8018e+02 - logprior: -2.6492e+00
Epoch 3/10
24/24 - 8s - loss: 348.9340 - loglik: -3.4540e+02 - logprior: -3.2198e+00
Epoch 4/10
24/24 - 8s - loss: 343.6572 - loglik: -3.4013e+02 - logprior: -3.0627e+00
Epoch 5/10
24/24 - 8s - loss: 341.1325 - loglik: -3.3763e+02 - logprior: -3.0490e+00
Epoch 6/10
24/24 - 8s - loss: 342.5323 - loglik: -3.3904e+02 - logprior: -3.0536e+00
Fitted a model with MAP estimate = -340.5709
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 1), (18, 1), (19, 1), (33, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (65, 1), (76, 1), (85, 1), (88, 1), (91, 1), (109, 1), (110, 1), (113, 1), (116, 1), (117, 1), (119, 1), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 1), (172, 1), (173, 3), (174, 1), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 336.1292 - loglik: -3.2819e+02 - logprior: -7.8398e+00
Epoch 2/2
24/24 - 9s - loss: 314.5543 - loglik: -3.1095e+02 - logprior: -3.2021e+00
Fitted a model with MAP estimate = -310.6887
expansions: [(0, 2), (194, 1)]
discards: [ 0 12 63]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 316.8856 - loglik: -3.1177e+02 - logprior: -5.0170e+00
Epoch 2/2
24/24 - 9s - loss: 307.6798 - loglik: -3.0665e+02 - logprior: -6.4570e-01
Fitted a model with MAP estimate = -304.9375
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 15s - loss: 313.8150 - loglik: -3.0904e+02 - logprior: -4.6873e+00
Epoch 2/10
24/24 - 9s - loss: 308.0500 - loglik: -3.0739e+02 - logprior: -2.9283e-01
Epoch 3/10
24/24 - 9s - loss: 304.0205 - loglik: -3.0363e+02 - logprior: 0.1561
Epoch 4/10
24/24 - 9s - loss: 302.3400 - loglik: -3.0207e+02 - logprior: 0.3256
Epoch 5/10
24/24 - 9s - loss: 302.0923 - loglik: -3.0200e+02 - logprior: 0.4802
Epoch 6/10
24/24 - 9s - loss: 301.0851 - loglik: -3.0120e+02 - logprior: 0.6575
Epoch 7/10
24/24 - 9s - loss: 301.0279 - loglik: -3.0134e+02 - logprior: 0.8444
Epoch 8/10
24/24 - 9s - loss: 299.9962 - loglik: -3.0049e+02 - logprior: 1.0150
Epoch 9/10
24/24 - 9s - loss: 300.6978 - loglik: -3.0139e+02 - logprior: 1.2083
Fitted a model with MAP estimate = -299.3594
Time for alignment: 217.0057
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 528.9008 - loglik: -5.2326e+02 - logprior: -5.6262e+00
Epoch 2/10
24/24 - 8s - loss: 376.3036 - loglik: -3.7373e+02 - logprior: -2.5367e+00
Epoch 3/10
24/24 - 8s - loss: 348.0326 - loglik: -3.4467e+02 - logprior: -3.0274e+00
Epoch 4/10
24/24 - 8s - loss: 340.4945 - loglik: -3.3719e+02 - logprior: -2.8504e+00
Epoch 5/10
24/24 - 8s - loss: 341.8896 - loglik: -3.3861e+02 - logprior: -2.8669e+00
Fitted a model with MAP estimate = -340.1718
expansions: [(11, 1), (12, 3), (15, 1), (17, 1), (18, 2), (34, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (59, 1), (63, 1), (65, 1), (76, 1), (85, 1), (88, 1), (90, 1), (109, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 1), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 334.9012 - loglik: -3.2701e+02 - logprior: -7.7882e+00
Epoch 2/2
24/24 - 9s - loss: 316.6098 - loglik: -3.1299e+02 - logprior: -3.2042e+00
Fitted a model with MAP estimate = -311.2728
expansions: [(0, 2), (12, 1), (13, 1), (188, 1), (190, 1)]
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 317.1302 - loglik: -3.1200e+02 - logprior: -5.0360e+00
Epoch 2/2
24/24 - 9s - loss: 307.9606 - loglik: -3.0693e+02 - logprior: -6.5285e-01
Fitted a model with MAP estimate = -304.9873
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 315.4719 - loglik: -3.1068e+02 - logprior: -4.6969e+00
Epoch 2/10
24/24 - 9s - loss: 306.5730 - loglik: -3.0586e+02 - logprior: -3.4359e-01
Epoch 3/10
24/24 - 9s - loss: 304.0996 - loglik: -3.0367e+02 - logprior: 0.1258
Epoch 4/10
24/24 - 9s - loss: 302.9636 - loglik: -3.0265e+02 - logprior: 0.2893
Epoch 5/10
24/24 - 9s - loss: 301.8710 - loglik: -3.0175e+02 - logprior: 0.4573
Epoch 6/10
24/24 - 9s - loss: 300.6965 - loglik: -3.0078e+02 - logprior: 0.6222
Epoch 7/10
24/24 - 9s - loss: 301.4963 - loglik: -3.0178e+02 - logprior: 0.8104
Fitted a model with MAP estimate = -300.0298
Time for alignment: 187.5155
Computed alignments with likelihoods: ['-301.5418', '-299.3594', '-300.0298']
Best model has likelihood: -299.3594
time for generating output: 0.3128
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9724081780351004
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fad01f8e640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb630919880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb630919940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb6390f00a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb514ac0490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7facf97fd790>, <__main__.SimpleDirichletPrior object at 0x7fb51479ad00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.2421 - loglik: -2.7681e+02 - logprior: -2.7410e+01
Epoch 2/10
10/10 - 1s - loss: 247.6845 - loglik: -2.4044e+02 - logprior: -7.2242e+00
Epoch 3/10
10/10 - 1s - loss: 207.8904 - loglik: -2.0397e+02 - logprior: -3.9165e+00
Epoch 4/10
10/10 - 1s - loss: 187.5044 - loglik: -1.8454e+02 - logprior: -2.9380e+00
Epoch 5/10
10/10 - 1s - loss: 179.3121 - loglik: -1.7642e+02 - logprior: -2.7242e+00
Epoch 6/10
10/10 - 1s - loss: 175.5600 - loglik: -1.7259e+02 - logprior: -2.6085e+00
Epoch 7/10
10/10 - 1s - loss: 175.0323 - loglik: -1.7231e+02 - logprior: -2.3237e+00
Epoch 8/10
10/10 - 1s - loss: 173.7190 - loglik: -1.7120e+02 - logprior: -2.1520e+00
Epoch 9/10
10/10 - 1s - loss: 173.6242 - loglik: -1.7113e+02 - logprior: -2.1233e+00
Epoch 10/10
10/10 - 1s - loss: 173.7833 - loglik: -1.7127e+02 - logprior: -2.1240e+00
Fitted a model with MAP estimate = -172.7824
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (51, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.2743 - loglik: -1.6965e+02 - logprior: -3.4595e+01
Epoch 2/2
10/10 - 1s - loss: 168.5279 - loglik: -1.5787e+02 - logprior: -1.0488e+01
Fitted a model with MAP estimate = -161.8698
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 189.4921 - loglik: -1.5833e+02 - logprior: -3.1142e+01
Epoch 2/2
10/10 - 1s - loss: 167.9845 - loglik: -1.5530e+02 - logprior: -1.2539e+01
Fitted a model with MAP estimate = -163.4607
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 184.5531 - loglik: -1.5516e+02 - logprior: -2.9370e+01
Epoch 2/10
10/10 - 1s - loss: 161.8761 - loglik: -1.5317e+02 - logprior: -8.5779e+00
Epoch 3/10
10/10 - 1s - loss: 155.9035 - loglik: -1.5219e+02 - logprior: -3.3896e+00
Epoch 4/10
10/10 - 1s - loss: 152.8825 - loglik: -1.5072e+02 - logprior: -1.7305e+00
Epoch 5/10
10/10 - 1s - loss: 151.4873 - loglik: -1.5002e+02 - logprior: -1.0419e+00
Epoch 6/10
10/10 - 1s - loss: 150.2491 - loglik: -1.4913e+02 - logprior: -7.1600e-01
Epoch 7/10
10/10 - 1s - loss: 150.2635 - loglik: -1.4933e+02 - logprior: -5.3844e-01
Fitted a model with MAP estimate = -149.4783
Time for alignment: 44.2434
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8833 - loglik: -2.7645e+02 - logprior: -2.7412e+01
Epoch 2/10
10/10 - 1s - loss: 247.5605 - loglik: -2.4033e+02 - logprior: -7.2188e+00
Epoch 3/10
10/10 - 1s - loss: 210.1172 - loglik: -2.0618e+02 - logprior: -3.9333e+00
Epoch 4/10
10/10 - 1s - loss: 189.7684 - loglik: -1.8680e+02 - logprior: -2.9614e+00
Epoch 5/10
10/10 - 1s - loss: 181.0110 - loglik: -1.7825e+02 - logprior: -2.6845e+00
Epoch 6/10
10/10 - 1s - loss: 175.7041 - loglik: -1.7287e+02 - logprior: -2.5553e+00
Epoch 7/10
10/10 - 1s - loss: 173.8844 - loglik: -1.7118e+02 - logprior: -2.2774e+00
Epoch 8/10
10/10 - 1s - loss: 173.4178 - loglik: -1.7091e+02 - logprior: -2.1009e+00
Epoch 9/10
10/10 - 1s - loss: 172.5206 - loglik: -1.7012e+02 - logprior: -2.0422e+00
Epoch 10/10
10/10 - 1s - loss: 172.7727 - loglik: -1.7042e+02 - logprior: -2.0167e+00
Fitted a model with MAP estimate = -171.9843
expansions: [(0, 3), (10, 2), (21, 1), (35, 4), (36, 2), (48, 2), (49, 2), (58, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.5144 - loglik: -1.7051e+02 - logprior: -3.4978e+01
Epoch 2/2
10/10 - 1s - loss: 169.9788 - loglik: -1.5887e+02 - logprior: -1.0977e+01
Fitted a model with MAP estimate = -162.5591
expansions: []
discards: [13 44 91]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 182.7874 - loglik: -1.5690e+02 - logprior: -2.5872e+01
Epoch 2/2
10/10 - 1s - loss: 161.6205 - loglik: -1.5461e+02 - logprior: -6.8910e+00
Fitted a model with MAP estimate = -157.7540
expansions: [(60, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.3952 - loglik: -1.5485e+02 - logprior: -2.4528e+01
Epoch 2/10
10/10 - 1s - loss: 159.5069 - loglik: -1.5287e+02 - logprior: -6.5258e+00
Epoch 3/10
10/10 - 1s - loss: 155.2080 - loglik: -1.5187e+02 - logprior: -3.0777e+00
Epoch 4/10
10/10 - 1s - loss: 153.3925 - loglik: -1.5124e+02 - logprior: -1.7894e+00
Epoch 5/10
10/10 - 1s - loss: 151.5917 - loglik: -1.5007e+02 - logprior: -1.1379e+00
Epoch 6/10
10/10 - 1s - loss: 150.6244 - loglik: -1.4945e+02 - logprior: -7.9187e-01
Epoch 7/10
10/10 - 1s - loss: 150.7199 - loglik: -1.4979e+02 - logprior: -5.4221e-01
Fitted a model with MAP estimate = -150.0055
Time for alignment: 44.0756
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.1666 - loglik: -2.7674e+02 - logprior: -2.7412e+01
Epoch 2/10
10/10 - 1s - loss: 247.9049 - loglik: -2.4067e+02 - logprior: -7.2208e+00
Epoch 3/10
10/10 - 1s - loss: 209.7987 - loglik: -2.0589e+02 - logprior: -3.9012e+00
Epoch 4/10
10/10 - 1s - loss: 188.6743 - loglik: -1.8577e+02 - logprior: -2.9018e+00
Epoch 5/10
10/10 - 1s - loss: 179.9330 - loglik: -1.7723e+02 - logprior: -2.6231e+00
Epoch 6/10
10/10 - 1s - loss: 177.0397 - loglik: -1.7424e+02 - logprior: -2.5254e+00
Epoch 7/10
10/10 - 1s - loss: 175.5862 - loglik: -1.7288e+02 - logprior: -2.2655e+00
Epoch 8/10
10/10 - 1s - loss: 174.6726 - loglik: -1.7215e+02 - logprior: -2.0826e+00
Epoch 9/10
10/10 - 1s - loss: 174.8786 - loglik: -1.7242e+02 - logprior: -2.0745e+00
Fitted a model with MAP estimate = -173.9298
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 5), (50, 3), (51, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 206.1763 - loglik: -1.7170e+02 - logprior: -3.4456e+01
Epoch 2/2
10/10 - 1s - loss: 171.1404 - loglik: -1.6053e+02 - logprior: -1.0481e+01
Fitted a model with MAP estimate = -164.1411
expansions: []
discards: [ 0 89]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 191.6350 - loglik: -1.6034e+02 - logprior: -3.1273e+01
Epoch 2/2
10/10 - 1s - loss: 170.6742 - loglik: -1.5794e+02 - logprior: -1.2602e+01
Fitted a model with MAP estimate = -165.9331
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.0740 - loglik: -1.5761e+02 - logprior: -2.9439e+01
Epoch 2/10
10/10 - 1s - loss: 164.6574 - loglik: -1.5587e+02 - logprior: -8.6489e+00
Epoch 3/10
10/10 - 1s - loss: 157.9279 - loglik: -1.5421e+02 - logprior: -3.4508e+00
Epoch 4/10
10/10 - 1s - loss: 154.9701 - loglik: -1.5281e+02 - logprior: -1.7982e+00
Epoch 5/10
10/10 - 1s - loss: 154.2700 - loglik: -1.5278e+02 - logprior: -1.0968e+00
Epoch 6/10
10/10 - 1s - loss: 153.4446 - loglik: -1.5230e+02 - logprior: -7.6170e-01
Epoch 7/10
10/10 - 1s - loss: 152.3412 - loglik: -1.5138e+02 - logprior: -5.7974e-01
Epoch 8/10
10/10 - 1s - loss: 152.3866 - loglik: -1.5159e+02 - logprior: -4.2085e-01
Fitted a model with MAP estimate = -151.8470
Time for alignment: 43.5015
Computed alignments with likelihoods: ['-149.4783', '-150.0055', '-151.8470']
Best model has likelihood: -149.4783
time for generating output: 0.1695
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7527426160337553
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb4f24de5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fad01e57c70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4c6ea5850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb57aa9ffd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7facf942e1f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fb50bd5c7f0>, <__main__.SimpleDirichletPrior object at 0x7facf88e40d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 150.8577 - loglik: -1.4572e+02 - logprior: -5.1368e+00
Epoch 2/10
16/16 - 2s - loss: 126.7267 - loglik: -1.2505e+02 - logprior: -1.6125e+00
Epoch 3/10
16/16 - 2s - loss: 115.3342 - loglik: -1.1323e+02 - logprior: -1.7418e+00
Epoch 4/10
16/16 - 2s - loss: 110.7786 - loglik: -1.0876e+02 - logprior: -1.7926e+00
Epoch 5/10
16/16 - 2s - loss: 109.0058 - loglik: -1.0702e+02 - logprior: -1.7532e+00
Epoch 6/10
16/16 - 2s - loss: 108.3271 - loglik: -1.0640e+02 - logprior: -1.7247e+00
Epoch 7/10
16/16 - 2s - loss: 108.1659 - loglik: -1.0626e+02 - logprior: -1.6944e+00
Epoch 8/10
16/16 - 2s - loss: 108.0163 - loglik: -1.0613e+02 - logprior: -1.6905e+00
Epoch 9/10
16/16 - 2s - loss: 107.7293 - loglik: -1.0583e+02 - logprior: -1.6760e+00
Epoch 10/10
16/16 - 2s - loss: 107.5025 - loglik: -1.0560e+02 - logprior: -1.6672e+00
Fitted a model with MAP estimate = -107.3236
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (17, 1), (23, 6), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 115.1606 - loglik: -1.0876e+02 - logprior: -6.3423e+00
Epoch 2/2
16/16 - 2s - loss: 105.2187 - loglik: -1.0185e+02 - logprior: -3.1764e+00
Fitted a model with MAP estimate = -103.7933
expansions: [(0, 1)]
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 106.6146 - loglik: -1.0188e+02 - logprior: -4.6805e+00
Epoch 2/2
16/16 - 2s - loss: 102.3499 - loglik: -1.0046e+02 - logprior: -1.7080e+00
Fitted a model with MAP estimate = -101.4713
expansions: [(3, 1)]
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.2428 - loglik: -1.0293e+02 - logprior: -6.2611e+00
Epoch 2/10
16/16 - 2s - loss: 104.6663 - loglik: -1.0166e+02 - logprior: -2.8660e+00
Epoch 3/10
16/16 - 2s - loss: 102.3675 - loglik: -1.0046e+02 - logprior: -1.6643e+00
Epoch 4/10
16/16 - 2s - loss: 101.2970 - loglik: -9.9500e+01 - logprior: -1.4352e+00
Epoch 5/10
16/16 - 2s - loss: 100.7700 - loglik: -9.8950e+01 - logprior: -1.4325e+00
Epoch 6/10
16/16 - 2s - loss: 99.8313 - loglik: -9.8039e+01 - logprior: -1.4202e+00
Epoch 7/10
16/16 - 2s - loss: 99.7182 - loglik: -9.7952e+01 - logprior: -1.3982e+00
Epoch 8/10
16/16 - 2s - loss: 100.0492 - loglik: -9.8306e+01 - logprior: -1.3808e+00
Fitted a model with MAP estimate = -99.1234
Time for alignment: 63.7988
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.3232 - loglik: -1.4619e+02 - logprior: -5.1322e+00
Epoch 2/10
16/16 - 2s - loss: 127.4813 - loglik: -1.2580e+02 - logprior: -1.6170e+00
Epoch 3/10
16/16 - 2s - loss: 114.5894 - loglik: -1.1241e+02 - logprior: -1.7593e+00
Epoch 4/10
16/16 - 2s - loss: 110.9606 - loglik: -1.0886e+02 - logprior: -1.7976e+00
Epoch 5/10
16/16 - 2s - loss: 109.3234 - loglik: -1.0734e+02 - logprior: -1.7556e+00
Epoch 6/10
16/16 - 2s - loss: 108.9483 - loglik: -1.0699e+02 - logprior: -1.7235e+00
Epoch 7/10
16/16 - 2s - loss: 108.5147 - loglik: -1.0660e+02 - logprior: -1.6841e+00
Epoch 8/10
16/16 - 2s - loss: 108.1286 - loglik: -1.0621e+02 - logprior: -1.6717e+00
Epoch 9/10
16/16 - 2s - loss: 108.4371 - loglik: -1.0652e+02 - logprior: -1.6589e+00
Fitted a model with MAP estimate = -107.8826
expansions: [(3, 1), (6, 1), (12, 2), (15, 1), (17, 1), (23, 4), (25, 1), (26, 2), (29, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.6790 - loglik: -1.0825e+02 - logprior: -6.3669e+00
Epoch 2/2
16/16 - 2s - loss: 105.8238 - loglik: -1.0246e+02 - logprior: -3.1879e+00
Fitted a model with MAP estimate = -103.8125
expansions: [(0, 1)]
discards: [ 0 30 36]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 106.8470 - loglik: -1.0213e+02 - logprior: -4.6652e+00
Epoch 2/2
16/16 - 2s - loss: 102.9164 - loglik: -1.0106e+02 - logprior: -1.6885e+00
Fitted a model with MAP estimate = -101.6491
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.1134 - loglik: -1.0284e+02 - logprior: -6.2235e+00
Epoch 2/10
16/16 - 2s - loss: 104.3335 - loglik: -1.0147e+02 - logprior: -2.7239e+00
Epoch 3/10
16/16 - 2s - loss: 101.8334 - loglik: -1.0001e+02 - logprior: -1.5717e+00
Epoch 4/10
16/16 - 2s - loss: 101.5784 - loglik: -9.9807e+01 - logprior: -1.3976e+00
Epoch 5/10
16/16 - 2s - loss: 100.5645 - loglik: -9.8768e+01 - logprior: -1.3971e+00
Epoch 6/10
16/16 - 2s - loss: 99.8812 - loglik: -9.8131e+01 - logprior: -1.3753e+00
Epoch 7/10
16/16 - 2s - loss: 99.6500 - loglik: -9.7930e+01 - logprior: -1.3512e+00
Epoch 8/10
16/16 - 2s - loss: 99.3935 - loglik: -9.7680e+01 - logprior: -1.3415e+00
Epoch 9/10
16/16 - 2s - loss: 99.2850 - loglik: -9.7604e+01 - logprior: -1.3194e+00
Epoch 10/10
16/16 - 2s - loss: 99.2017 - loglik: -9.7537e+01 - logprior: -1.3017e+00
Fitted a model with MAP estimate = -98.7276
Time for alignment: 64.1764
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 150.9359 - loglik: -1.4580e+02 - logprior: -5.1329e+00
Epoch 2/10
16/16 - 2s - loss: 126.1931 - loglik: -1.2451e+02 - logprior: -1.6149e+00
Epoch 3/10
16/16 - 2s - loss: 114.8314 - loglik: -1.1278e+02 - logprior: -1.7453e+00
Epoch 4/10
16/16 - 2s - loss: 109.7888 - loglik: -1.0765e+02 - logprior: -1.8455e+00
Epoch 5/10
16/16 - 2s - loss: 108.0508 - loglik: -1.0594e+02 - logprior: -1.8172e+00
Epoch 6/10
16/16 - 2s - loss: 107.8243 - loglik: -1.0576e+02 - logprior: -1.7737e+00
Epoch 7/10
16/16 - 2s - loss: 107.4349 - loglik: -1.0541e+02 - logprior: -1.7346e+00
Epoch 8/10
16/16 - 2s - loss: 107.2143 - loglik: -1.0520e+02 - logprior: -1.7173e+00
Epoch 9/10
16/16 - 2s - loss: 107.2423 - loglik: -1.0524e+02 - logprior: -1.7059e+00
Fitted a model with MAP estimate = -106.7745
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (23, 5), (24, 2), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 115.0965 - loglik: -1.0868e+02 - logprior: -6.3582e+00
Epoch 2/2
16/16 - 2s - loss: 105.7528 - loglik: -1.0241e+02 - logprior: -3.1480e+00
Fitted a model with MAP estimate = -103.9281
expansions: [(0, 1)]
discards: [ 0 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.9349 - loglik: -1.0222e+02 - logprior: -4.6654e+00
Epoch 2/2
16/16 - 2s - loss: 102.4516 - loglik: -1.0057e+02 - logprior: -1.7039e+00
Fitted a model with MAP estimate = -101.7076
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 106.2642 - loglik: -1.0154e+02 - logprior: -4.6736e+00
Epoch 2/10
16/16 - 2s - loss: 101.9831 - loglik: -1.0017e+02 - logprior: -1.6747e+00
Epoch 3/10
16/16 - 2s - loss: 100.8204 - loglik: -9.9148e+01 - logprior: -1.4093e+00
Epoch 4/10
16/16 - 2s - loss: 100.5217 - loglik: -9.8789e+01 - logprior: -1.3538e+00
Epoch 5/10
16/16 - 2s - loss: 99.6999 - loglik: -9.7983e+01 - logprior: -1.3231e+00
Epoch 6/10
16/16 - 2s - loss: 99.2992 - loglik: -9.7609e+01 - logprior: -1.3056e+00
Epoch 7/10
16/16 - 2s - loss: 99.1132 - loglik: -9.7451e+01 - logprior: -1.2853e+00
Epoch 8/10
16/16 - 2s - loss: 98.5959 - loglik: -9.6948e+01 - logprior: -1.2670e+00
Epoch 9/10
16/16 - 2s - loss: 98.8887 - loglik: -9.7255e+01 - logprior: -1.2527e+00
Fitted a model with MAP estimate = -98.1580
Time for alignment: 62.1266
Computed alignments with likelihoods: ['-99.1234', '-98.7276', '-98.1580']
Best model has likelihood: -98.1580
time for generating output: 0.1409
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.8998384491114702
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb52ddac6d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf8a82f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7facf87bb4c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf8810400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad01ba03a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7facf1773850>, <__main__.SimpleDirichletPrior object at 0x7fb2dc691850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 586.7179 - loglik: -5.8444e+02 - logprior: -1.7704e+00
Epoch 2/10
39/39 - 13s - loss: 535.5664 - loglik: -5.3292e+02 - logprior: -1.1005e+00
Epoch 3/10
39/39 - 12s - loss: 523.7387 - loglik: -5.2041e+02 - logprior: -1.1944e+00
Epoch 4/10
39/39 - 13s - loss: 519.1370 - loglik: -5.1567e+02 - logprior: -1.1904e+00
Epoch 5/10
39/39 - 13s - loss: 516.2788 - loglik: -5.1296e+02 - logprior: -1.2249e+00
Epoch 6/10
39/39 - 12s - loss: 514.4534 - loglik: -5.1128e+02 - logprior: -1.2557e+00
Epoch 7/10
39/39 - 13s - loss: 512.9125 - loglik: -5.0990e+02 - logprior: -1.2693e+00
Epoch 8/10
39/39 - 13s - loss: 511.9847 - loglik: -5.0915e+02 - logprior: -1.2906e+00
Epoch 9/10
39/39 - 12s - loss: 511.2932 - loglik: -5.0857e+02 - logprior: -1.3041e+00
Epoch 10/10
39/39 - 13s - loss: 510.6282 - loglik: -5.0804e+02 - logprior: -1.3114e+00
Fitted a model with MAP estimate = -490.7686
expansions: [(4, 1), (5, 1), (6, 1), (10, 1), (20, 9), (25, 1), (54, 1), (55, 1), (70, 1), (72, 1), (78, 2), (79, 1), (112, 1), (113, 1), (122, 1), (124, 4), (126, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 559.9459 - loglik: -5.5720e+02 - logprior: -2.4911e+00
Epoch 2/2
39/39 - 15s - loss: 523.1556 - loglik: -5.2067e+02 - logprior: -1.4479e+00
Fitted a model with MAP estimate = -473.5472
expansions: [(29, 1), (95, 1), (99, 1), (135, 2), (143, 3), (144, 5), (145, 2), (146, 3)]
discards: [  0  33  34  35  36 136 155 156 157 158 159 160 161 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 531.7595 - loglik: -5.2867e+02 - logprior: -2.8108e+00
Epoch 2/2
39/39 - 14s - loss: 518.3654 - loglik: -5.1601e+02 - logprior: -1.0174e+00
Fitted a model with MAP estimate = -469.2387
expansions: [(0, 2), (32, 1), (144, 1)]
discards: [  0 133]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 22s - loss: 473.7449 - loglik: -4.7216e+02 - logprior: -1.2101e+00
Epoch 2/10
51/51 - 18s - loss: 464.1268 - loglik: -4.6169e+02 - logprior: -8.2924e-01
Epoch 3/10
51/51 - 18s - loss: 459.8466 - loglik: -4.5645e+02 - logprior: -8.1106e-01
Epoch 4/10
51/51 - 18s - loss: 456.8727 - loglik: -4.5342e+02 - logprior: -7.6707e-01
Epoch 5/10
51/51 - 18s - loss: 453.2891 - loglik: -4.5002e+02 - logprior: -7.3142e-01
Epoch 6/10
51/51 - 18s - loss: 452.8047 - loglik: -4.4970e+02 - logprior: -7.0057e-01
Epoch 7/10
51/51 - 19s - loss: 449.8937 - loglik: -4.4712e+02 - logprior: -6.6965e-01
Epoch 8/10
51/51 - 18s - loss: 450.8811 - loglik: -4.4845e+02 - logprior: -6.2356e-01
Fitted a model with MAP estimate = -447.0345
Time for alignment: 437.1134
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 587.0834 - loglik: -5.8482e+02 - logprior: -1.7688e+00
Epoch 2/10
39/39 - 12s - loss: 535.7856 - loglik: -5.3286e+02 - logprior: -1.1940e+00
Epoch 3/10
39/39 - 12s - loss: 525.0062 - loglik: -5.2119e+02 - logprior: -1.2787e+00
Epoch 4/10
39/39 - 12s - loss: 519.5141 - loglik: -5.1572e+02 - logprior: -1.2788e+00
Epoch 5/10
39/39 - 13s - loss: 516.3674 - loglik: -5.1283e+02 - logprior: -1.3074e+00
Epoch 6/10
39/39 - 13s - loss: 514.6092 - loglik: -5.1139e+02 - logprior: -1.3242e+00
Epoch 7/10
39/39 - 13s - loss: 513.6538 - loglik: -5.1068e+02 - logprior: -1.3484e+00
Epoch 8/10
39/39 - 13s - loss: 512.7689 - loglik: -5.0997e+02 - logprior: -1.3621e+00
Epoch 9/10
39/39 - 13s - loss: 511.9334 - loglik: -5.0921e+02 - logprior: -1.3818e+00
Epoch 10/10
39/39 - 12s - loss: 511.5439 - loglik: -5.0887e+02 - logprior: -1.3875e+00
Fitted a model with MAP estimate = -492.2569
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (25, 1), (27, 2), (56, 1), (71, 2), (78, 2), (88, 1), (93, 1), (110, 1), (112, 1), (122, 7), (123, 3), (125, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 562.0639 - loglik: -5.5939e+02 - logprior: -2.4030e+00
Epoch 2/2
39/39 - 15s - loss: 522.3810 - loglik: -5.1985e+02 - logprior: -1.4419e+00
Fitted a model with MAP estimate = -473.1338
expansions: [(2, 1), (15, 1), (25, 1), (32, 2), (34, 1), (90, 1), (144, 2), (145, 2), (147, 3), (149, 1)]
discards: [  0  16  91  92 153 154 155 156 157 158 159 160 161 162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 532.3580 - loglik: -5.2926e+02 - logprior: -2.8220e+00
Epoch 2/2
39/39 - 14s - loss: 519.8586 - loglik: -5.1731e+02 - logprior: -1.1673e+00
Fitted a model with MAP estimate = -470.8808
expansions: [(0, 1), (94, 1), (143, 2)]
discards: [  0 150]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 21s - loss: 475.3614 - loglik: -4.7383e+02 - logprior: -1.1635e+00
Epoch 2/10
51/51 - 18s - loss: 465.2411 - loglik: -4.6282e+02 - logprior: -8.5357e-01
Epoch 3/10
51/51 - 19s - loss: 459.8834 - loglik: -4.5653e+02 - logprior: -8.1113e-01
Epoch 4/10
51/51 - 18s - loss: 457.4247 - loglik: -4.5406e+02 - logprior: -7.4251e-01
Epoch 5/10
51/51 - 18s - loss: 454.9363 - loglik: -4.5179e+02 - logprior: -7.0120e-01
Epoch 6/10
51/51 - 18s - loss: 453.2791 - loglik: -4.5026e+02 - logprior: -6.6612e-01
Epoch 7/10
51/51 - 18s - loss: 451.5649 - loglik: -4.4887e+02 - logprior: -6.3739e-01
Epoch 8/10
51/51 - 19s - loss: 450.8808 - loglik: -4.4850e+02 - logprior: -6.1164e-01
Epoch 9/10
51/51 - 18s - loss: 449.5447 - loglik: -4.4745e+02 - logprior: -5.7456e-01
Epoch 10/10
51/51 - 18s - loss: 449.5046 - loglik: -4.4757e+02 - logprior: -5.3240e-01
Fitted a model with MAP estimate = -447.1384
Time for alignment: 469.9858
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 586.9767 - loglik: -5.8471e+02 - logprior: -1.7623e+00
Epoch 2/10
39/39 - 12s - loss: 538.3275 - loglik: -5.3590e+02 - logprior: -1.0191e+00
Epoch 3/10
39/39 - 12s - loss: 527.0508 - loglik: -5.2405e+02 - logprior: -1.1901e+00
Epoch 4/10
39/39 - 13s - loss: 522.7783 - loglik: -5.1945e+02 - logprior: -1.1696e+00
Epoch 5/10
39/39 - 12s - loss: 520.1508 - loglik: -5.1695e+02 - logprior: -1.1876e+00
Epoch 6/10
39/39 - 13s - loss: 518.3869 - loglik: -5.1535e+02 - logprior: -1.2030e+00
Epoch 7/10
39/39 - 12s - loss: 517.0612 - loglik: -5.1421e+02 - logprior: -1.2229e+00
Epoch 8/10
39/39 - 12s - loss: 516.2136 - loglik: -5.1350e+02 - logprior: -1.2378e+00
Epoch 9/10
39/39 - 12s - loss: 515.3852 - loglik: -5.1280e+02 - logprior: -1.2335e+00
Epoch 10/10
39/39 - 12s - loss: 514.9279 - loglik: -5.1245e+02 - logprior: -1.2299e+00
Fitted a model with MAP estimate = -493.1693
expansions: [(4, 1), (6, 1), (10, 1), (20, 3), (23, 7), (55, 1), (56, 2), (66, 2), (69, 1), (76, 4), (110, 2), (121, 1), (125, 1), (126, 1), (127, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 560.4299 - loglik: -5.5778e+02 - logprior: -2.3901e+00
Epoch 2/2
39/39 - 15s - loss: 525.9884 - loglik: -5.2366e+02 - logprior: -1.2784e+00
Fitted a model with MAP estimate = -474.3541
expansions: [(6, 1), (41, 1), (97, 2), (134, 1), (145, 6), (146, 3)]
discards: [  2  26  36  37  70  82 135 147 157 158 159 160 161 162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 533.1703 - loglik: -5.3098e+02 - logprior: -1.9151e+00
Epoch 2/2
39/39 - 15s - loss: 520.1472 - loglik: -5.1792e+02 - logprior: -8.9484e-01
Fitted a model with MAP estimate = -470.8452
expansions: [(28, 2), (41, 1), (162, 3)]
discards: [25 38 94]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 21s - loss: 473.7654 - loglik: -4.7216e+02 - logprior: -1.2416e+00
Epoch 2/10
51/51 - 18s - loss: 464.4600 - loglik: -4.6197e+02 - logprior: -8.8058e-01
Epoch 3/10
51/51 - 18s - loss: 460.3403 - loglik: -4.5695e+02 - logprior: -8.2705e-01
Epoch 4/10
51/51 - 18s - loss: 456.5731 - loglik: -4.5313e+02 - logprior: -7.8137e-01
Epoch 5/10
51/51 - 18s - loss: 454.0858 - loglik: -4.5083e+02 - logprior: -7.3032e-01
Epoch 6/10
51/51 - 18s - loss: 452.2903 - loglik: -4.4917e+02 - logprior: -7.0604e-01
Epoch 7/10
51/51 - 18s - loss: 450.7556 - loglik: -4.4798e+02 - logprior: -6.6266e-01
Epoch 8/10
51/51 - 18s - loss: 449.4973 - loglik: -4.4705e+02 - logprior: -6.2758e-01
Epoch 9/10
51/51 - 18s - loss: 448.7769 - loglik: -4.4666e+02 - logprior: -5.8210e-01
Epoch 10/10
51/51 - 18s - loss: 447.6525 - loglik: -4.4565e+02 - logprior: -5.4930e-01
Fitted a model with MAP estimate = -446.0740
Time for alignment: 470.5370
Computed alignments with likelihoods: ['-447.0345', '-447.1384', '-446.0740']
Best model has likelihood: -446.0740
time for generating output: 0.2887
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.7082790091264668
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fb4607bfe50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fb468016220>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045940>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045a30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045670>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680451c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fb468045f10>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680454c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045df0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045ee0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045850>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045760>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680458e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045250>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680457f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045730>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb468045910>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb468045610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fb2bc252df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7facf829de50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fad01db87c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fb4e13957c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fb460735e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680406d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb4680409d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fb686e683a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fb6e04bb550> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fb4607f3160>, <function make_default_emission_matrix at 0x7fb4607f3160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7facf9fc2d30>, <__main__.SimpleDirichletPrior object at 0x7facc5990d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 491.5912 - loglik: -4.8541e+02 - logprior: -6.1187e+00
Epoch 2/10
14/14 - 4s - loss: 435.2477 - loglik: -4.3338e+02 - logprior: -1.4233e+00
Epoch 3/10
14/14 - 4s - loss: 397.4552 - loglik: -3.9514e+02 - logprior: -1.4700e+00
Epoch 4/10
14/14 - 4s - loss: 386.1355 - loglik: -3.8366e+02 - logprior: -1.5037e+00
Epoch 5/10
14/14 - 4s - loss: 381.6905 - loglik: -3.7907e+02 - logprior: -1.5487e+00
Epoch 6/10
14/14 - 4s - loss: 378.9023 - loglik: -3.7659e+02 - logprior: -1.5379e+00
Epoch 7/10
14/14 - 4s - loss: 378.3530 - loglik: -3.7619e+02 - logprior: -1.5438e+00
Epoch 8/10
14/14 - 4s - loss: 377.7974 - loglik: -3.7571e+02 - logprior: -1.5511e+00
Epoch 9/10
14/14 - 4s - loss: 376.5474 - loglik: -3.7449e+02 - logprior: -1.5418e+00
Epoch 10/10
14/14 - 4s - loss: 376.3516 - loglik: -3.7431e+02 - logprior: -1.5425e+00
Fitted a model with MAP estimate = -375.8736
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (70, 4), (102, 5), (118, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 385.7218 - loglik: -3.8093e+02 - logprior: -4.6146e+00
Epoch 2/2
29/29 - 7s - loss: 363.2511 - loglik: -3.6140e+02 - logprior: -1.3317e+00
Fitted a model with MAP estimate = -359.8026
expansions: [(126, 1), (130, 1)]
discards: [ 41 148 167]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 369.8223 - loglik: -3.6631e+02 - logprior: -3.4036e+00
Epoch 2/2
29/29 - 7s - loss: 362.1465 - loglik: -3.6069e+02 - logprior: -9.2129e-01
Fitted a model with MAP estimate = -358.8265
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 12s - loss: 368.8456 - loglik: -3.6551e+02 - logprior: -3.2199e+00
Epoch 2/10
29/29 - 7s - loss: 361.4540 - loglik: -3.6025e+02 - logprior: -7.0749e-01
Epoch 3/10
29/29 - 7s - loss: 358.9413 - loglik: -3.5740e+02 - logprior: -6.2979e-01
Epoch 4/10
29/29 - 7s - loss: 357.0251 - loglik: -3.5558e+02 - logprior: -5.2803e-01
Epoch 5/10
29/29 - 7s - loss: 355.1124 - loglik: -3.5384e+02 - logprior: -4.4685e-01
Epoch 6/10
29/29 - 7s - loss: 355.2708 - loglik: -3.5419e+02 - logprior: -3.5701e-01
Fitted a model with MAP estimate = -353.9258
Time for alignment: 165.3981
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.6854 - loglik: -4.8552e+02 - logprior: -6.1084e+00
Epoch 2/10
14/14 - 4s - loss: 433.9241 - loglik: -4.3208e+02 - logprior: -1.4080e+00
Epoch 3/10
14/14 - 4s - loss: 396.6884 - loglik: -3.9425e+02 - logprior: -1.5172e+00
Epoch 4/10
14/14 - 4s - loss: 385.5962 - loglik: -3.8279e+02 - logprior: -1.5506e+00
Epoch 5/10
14/14 - 4s - loss: 380.5228 - loglik: -3.7791e+02 - logprior: -1.5426e+00
Epoch 6/10
14/14 - 4s - loss: 380.6052 - loglik: -3.7826e+02 - logprior: -1.5325e+00
Fitted a model with MAP estimate = -377.8727
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (70, 3), (89, 1), (102, 6), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 383.5606 - loglik: -3.7889e+02 - logprior: -4.5040e+00
Epoch 2/2
29/29 - 7s - loss: 363.6157 - loglik: -3.6177e+02 - logprior: -1.3349e+00
Fitted a model with MAP estimate = -359.8922
expansions: [(131, 1)]
discards: [ 41 149 168]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 370.0140 - loglik: -3.6644e+02 - logprior: -3.4719e+00
Epoch 2/2
29/29 - 7s - loss: 362.8212 - loglik: -3.6149e+02 - logprior: -9.7601e-01
Fitted a model with MAP estimate = -360.0829
expansions: [(89, 1)]
discards: [112]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 368.8720 - loglik: -3.6547e+02 - logprior: -3.2813e+00
Epoch 2/10
29/29 - 7s - loss: 362.1754 - loglik: -3.6079e+02 - logprior: -8.0982e-01
Epoch 3/10
29/29 - 7s - loss: 358.8886 - loglik: -3.5719e+02 - logprior: -7.3090e-01
Epoch 4/10
29/29 - 7s - loss: 357.7299 - loglik: -3.5615e+02 - logprior: -6.5492e-01
Epoch 5/10
29/29 - 7s - loss: 356.9487 - loglik: -3.5556e+02 - logprior: -5.6712e-01
Epoch 6/10
29/29 - 7s - loss: 354.8189 - loglik: -3.5363e+02 - logprior: -4.7506e-01
Epoch 7/10
29/29 - 7s - loss: 354.1459 - loglik: -3.5312e+02 - logprior: -3.8550e-01
Epoch 8/10
29/29 - 7s - loss: 354.4944 - loglik: -3.5360e+02 - logprior: -2.9719e-01
Fitted a model with MAP estimate = -353.5661
Time for alignment: 159.1943
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.0982 - loglik: -4.8493e+02 - logprior: -6.1103e+00
Epoch 2/10
14/14 - 4s - loss: 437.2603 - loglik: -4.3540e+02 - logprior: -1.4093e+00
Epoch 3/10
14/14 - 4s - loss: 400.3965 - loglik: -3.9794e+02 - logprior: -1.5342e+00
Epoch 4/10
14/14 - 4s - loss: 387.9782 - loglik: -3.8503e+02 - logprior: -1.6355e+00
Epoch 5/10
14/14 - 4s - loss: 383.6359 - loglik: -3.8095e+02 - logprior: -1.6178e+00
Epoch 6/10
14/14 - 4s - loss: 381.4128 - loglik: -3.7905e+02 - logprior: -1.5895e+00
Epoch 7/10
14/14 - 4s - loss: 380.3742 - loglik: -3.7815e+02 - logprior: -1.5802e+00
Epoch 8/10
14/14 - 4s - loss: 378.3664 - loglik: -3.7622e+02 - logprior: -1.5978e+00
Epoch 9/10
14/14 - 4s - loss: 378.0807 - loglik: -3.7596e+02 - logprior: -1.5984e+00
Epoch 10/10
14/14 - 4s - loss: 377.6888 - loglik: -3.7553e+02 - logprior: -1.6470e+00
Fitted a model with MAP estimate = -376.7764
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (30, 1), (32, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 4), (102, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 386.8411 - loglik: -3.8208e+02 - logprior: -4.5979e+00
Epoch 2/2
29/29 - 7s - loss: 364.3145 - loglik: -3.6250e+02 - logprior: -1.2884e+00
Fitted a model with MAP estimate = -361.1266
expansions: [(125, 1), (129, 1)]
discards: [ 91 147 166]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 371.1611 - loglik: -3.6766e+02 - logprior: -3.3668e+00
Epoch 2/2
29/29 - 7s - loss: 364.7148 - loglik: -3.6321e+02 - logprior: -9.0028e-01
Fitted a model with MAP estimate = -360.6401
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 371.3898 - loglik: -3.6807e+02 - logprior: -3.2031e+00
Epoch 2/10
29/29 - 7s - loss: 363.0549 - loglik: -3.6192e+02 - logprior: -6.6980e-01
Epoch 3/10
29/29 - 7s - loss: 360.5699 - loglik: -3.5903e+02 - logprior: -6.0222e-01
Epoch 4/10
29/29 - 7s - loss: 359.4781 - loglik: -3.5802e+02 - logprior: -5.1443e-01
Epoch 5/10
29/29 - 7s - loss: 357.8502 - loglik: -3.5661e+02 - logprior: -4.2373e-01
Epoch 6/10
29/29 - 7s - loss: 357.9010 - loglik: -3.5687e+02 - logprior: -3.2783e-01
Fitted a model with MAP estimate = -356.1126
Time for alignment: 160.9205
Computed alignments with likelihoods: ['-353.9258', '-353.5661', '-356.1126']
Best model has likelihood: -353.5661
time for generating output: 0.2969
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.791519030746371
