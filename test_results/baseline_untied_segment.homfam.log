Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 537.9064 - loglik: -5.3304e+02 - logprior: -4.8652e+00
Epoch 2/10
25/25 - 6s - loss: 427.1369 - loglik: -4.2547e+02 - logprior: -1.6687e+00
Epoch 3/10
25/25 - 6s - loss: 409.7537 - loglik: -4.0795e+02 - logprior: -1.8069e+00
Epoch 4/10
25/25 - 6s - loss: 405.7091 - loglik: -4.0406e+02 - logprior: -1.6512e+00
Epoch 5/10
25/25 - 6s - loss: 405.5088 - loglik: -4.0391e+02 - logprior: -1.5980e+00
Epoch 6/10
25/25 - 6s - loss: 404.8295 - loglik: -4.0324e+02 - logprior: -1.5941e+00
Epoch 7/10
25/25 - 6s - loss: 403.4032 - loglik: -4.0181e+02 - logprior: -1.5931e+00
Epoch 8/10
25/25 - 6s - loss: 405.2178 - loglik: -4.0363e+02 - logprior: -1.5868e+00
Fitted a model with MAP estimate = -403.9480
expansions: [(9, 3), (10, 1), (11, 2), (12, 1), (32, 2), (33, 2), (35, 2), (48, 3), (58, 2), (60, 1), (63, 1), (76, 1), (81, 2), (82, 2), (83, 2), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 2), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 404.1396 - loglik: -3.9723e+02 - logprior: -6.9057e+00
Epoch 2/2
25/25 - 8s - loss: 388.8020 - loglik: -3.8614e+02 - logprior: -2.6577e+00
Fitted a model with MAP estimate = -386.3865
expansions: [(0, 3), (215, 1)]
discards: [  0   9  42  46  62 102 107 144 157 159 179]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 393.2150 - loglik: -3.8894e+02 - logprior: -4.2784e+00
Epoch 2/2
25/25 - 7s - loss: 385.6855 - loglik: -3.8529e+02 - logprior: -3.9516e-01
Fitted a model with MAP estimate = -384.1516
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 393.8972 - loglik: -3.8764e+02 - logprior: -6.2539e+00
Epoch 2/10
25/25 - 7s - loss: 388.0371 - loglik: -3.8682e+02 - logprior: -1.2193e+00
Epoch 3/10
25/25 - 7s - loss: 383.1244 - loglik: -3.8358e+02 - logprior: 0.4554
Epoch 4/10
25/25 - 7s - loss: 383.2692 - loglik: -3.8381e+02 - logprior: 0.5446
Fitted a model with MAP estimate = -381.3328
Time for alignment: 144.2721
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 537.3166 - loglik: -5.3245e+02 - logprior: -4.8649e+00
Epoch 2/10
25/25 - 6s - loss: 427.2191 - loglik: -4.2562e+02 - logprior: -1.5981e+00
Epoch 3/10
25/25 - 6s - loss: 408.7003 - loglik: -4.0699e+02 - logprior: -1.7123e+00
Epoch 4/10
25/25 - 6s - loss: 406.1871 - loglik: -4.0460e+02 - logprior: -1.5895e+00
Epoch 5/10
25/25 - 6s - loss: 403.5283 - loglik: -4.0192e+02 - logprior: -1.6093e+00
Epoch 6/10
25/25 - 6s - loss: 404.7624 - loglik: -4.0313e+02 - logprior: -1.6332e+00
Fitted a model with MAP estimate = -403.9082
expansions: [(9, 5), (10, 2), (31, 3), (32, 2), (34, 2), (48, 1), (50, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (86, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (124, 2), (127, 2), (138, 1), (139, 2), (140, 1), (150, 1), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 401.1590 - loglik: -3.9436e+02 - logprior: -6.8018e+00
Epoch 2/2
25/25 - 8s - loss: 388.9542 - loglik: -3.8652e+02 - logprior: -2.4368e+00
Fitted a model with MAP estimate = -384.9122
expansions: [(0, 3)]
discards: [  0  11  41  81 143 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 391.2248 - loglik: -3.8688e+02 - logprior: -4.3424e+00
Epoch 2/2
25/25 - 7s - loss: 386.4113 - loglik: -3.8595e+02 - logprior: -4.6187e-01
Fitted a model with MAP estimate = -383.6797
expansions: []
discards: [  0   2  45 106]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 394.1506 - loglik: -3.8784e+02 - logprior: -6.3112e+00
Epoch 2/10
25/25 - 7s - loss: 387.6740 - loglik: -3.8642e+02 - logprior: -1.2533e+00
Epoch 3/10
25/25 - 7s - loss: 384.3075 - loglik: -3.8472e+02 - logprior: 0.4144
Epoch 4/10
25/25 - 7s - loss: 382.3711 - loglik: -3.8289e+02 - logprior: 0.5150
Epoch 5/10
25/25 - 7s - loss: 379.9431 - loglik: -3.8053e+02 - logprior: 0.5821
Epoch 6/10
25/25 - 7s - loss: 381.8754 - loglik: -3.8258e+02 - logprior: 0.7086
Fitted a model with MAP estimate = -380.1586
Time for alignment: 146.8062
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 536.6701 - loglik: -5.3182e+02 - logprior: -4.8484e+00
Epoch 2/10
25/25 - 6s - loss: 428.8217 - loglik: -4.2724e+02 - logprior: -1.5783e+00
Epoch 3/10
25/25 - 6s - loss: 408.6469 - loglik: -4.0697e+02 - logprior: -1.6732e+00
Epoch 4/10
25/25 - 6s - loss: 406.4791 - loglik: -4.0494e+02 - logprior: -1.5415e+00
Epoch 5/10
25/25 - 6s - loss: 404.5674 - loglik: -4.0305e+02 - logprior: -1.5167e+00
Epoch 6/10
25/25 - 6s - loss: 404.9431 - loglik: -4.0341e+02 - logprior: -1.5317e+00
Fitted a model with MAP estimate = -403.8628
expansions: [(9, 2), (10, 4), (31, 3), (32, 2), (34, 2), (47, 2), (50, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (85, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 402.1224 - loglik: -3.9532e+02 - logprior: -6.8008e+00
Epoch 2/2
25/25 - 8s - loss: 388.3860 - loglik: -3.8596e+02 - logprior: -2.4212e+00
Fitted a model with MAP estimate = -385.0319
expansions: [(0, 3), (214, 1)]
discards: [  0  41  61  81 106 143 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 392.3214 - loglik: -3.8801e+02 - logprior: -4.3115e+00
Epoch 2/2
25/25 - 7s - loss: 385.3893 - loglik: -3.8494e+02 - logprior: -4.4937e-01
Fitted a model with MAP estimate = -383.7665
expansions: []
discards: [ 0  2 45]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 394.8661 - loglik: -3.8859e+02 - logprior: -6.2754e+00
Epoch 2/10
25/25 - 7s - loss: 386.8210 - loglik: -3.8567e+02 - logprior: -1.1539e+00
Epoch 3/10
25/25 - 7s - loss: 382.9725 - loglik: -3.8340e+02 - logprior: 0.4280
Epoch 4/10
25/25 - 7s - loss: 383.5763 - loglik: -3.8408e+02 - logprior: 0.4996
Fitted a model with MAP estimate = -381.2065
Time for alignment: 132.4851
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 538.3651 - loglik: -5.3349e+02 - logprior: -4.8771e+00
Epoch 2/10
25/25 - 6s - loss: 427.6879 - loglik: -4.2589e+02 - logprior: -1.7931e+00
Epoch 3/10
25/25 - 6s - loss: 408.0879 - loglik: -4.0612e+02 - logprior: -1.9683e+00
Epoch 4/10
25/25 - 6s - loss: 405.1233 - loglik: -4.0331e+02 - logprior: -1.8141e+00
Epoch 5/10
25/25 - 6s - loss: 404.3360 - loglik: -4.0254e+02 - logprior: -1.7910e+00
Epoch 6/10
25/25 - 6s - loss: 402.2205 - loglik: -4.0045e+02 - logprior: -1.7726e+00
Epoch 7/10
25/25 - 6s - loss: 404.6411 - loglik: -4.0286e+02 - logprior: -1.7816e+00
Fitted a model with MAP estimate = -402.8653
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 1), (93, 2), (99, 1), (102, 1), (113, 2), (115, 1), (122, 2), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (163, 1), (167, 1), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 402.8087 - loglik: -3.9599e+02 - logprior: -6.8218e+00
Epoch 2/2
25/25 - 8s - loss: 388.3277 - loglik: -3.8586e+02 - logprior: -2.4627e+00
Fitted a model with MAP estimate = -385.4505
expansions: [(0, 3)]
discards: [  0   9  42  62  63  82 103 106 145 158 160 180]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 392.9005 - loglik: -3.8860e+02 - logprior: -4.3025e+00
Epoch 2/2
25/25 - 7s - loss: 387.0043 - loglik: -3.8658e+02 - logprior: -4.2529e-01
Fitted a model with MAP estimate = -384.3751
expansions: [(61, 2)]
discards: [ 0  2 45]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 395.4799 - loglik: -3.8922e+02 - logprior: -6.2608e+00
Epoch 2/10
25/25 - 7s - loss: 386.4950 - loglik: -3.8525e+02 - logprior: -1.2488e+00
Epoch 3/10
25/25 - 7s - loss: 382.8621 - loglik: -3.8333e+02 - logprior: 0.4640
Epoch 4/10
25/25 - 7s - loss: 382.7348 - loglik: -3.8329e+02 - logprior: 0.5555
Epoch 5/10
25/25 - 7s - loss: 381.9310 - loglik: -3.8259e+02 - logprior: 0.6562
Epoch 6/10
25/25 - 7s - loss: 379.8665 - loglik: -3.8062e+02 - logprior: 0.7537
Epoch 7/10
25/25 - 7s - loss: 379.9356 - loglik: -3.8081e+02 - logprior: 0.8761
Fitted a model with MAP estimate = -379.8475
Time for alignment: 161.1662
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 537.8769 - loglik: -5.3301e+02 - logprior: -4.8626e+00
Epoch 2/10
25/25 - 6s - loss: 430.3289 - loglik: -4.2864e+02 - logprior: -1.6871e+00
Epoch 3/10
25/25 - 6s - loss: 409.8651 - loglik: -4.0800e+02 - logprior: -1.8612e+00
Epoch 4/10
25/25 - 6s - loss: 407.4145 - loglik: -4.0568e+02 - logprior: -1.7317e+00
Epoch 5/10
25/25 - 6s - loss: 402.8770 - loglik: -4.0118e+02 - logprior: -1.6928e+00
Epoch 6/10
25/25 - 6s - loss: 406.4132 - loglik: -4.0472e+02 - logprior: -1.6958e+00
Fitted a model with MAP estimate = -404.9120
expansions: [(9, 3), (10, 1), (11, 2), (12, 1), (31, 1), (32, 1), (33, 1), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (62, 2), (66, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (102, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 402.2405 - loglik: -3.9540e+02 - logprior: -6.8371e+00
Epoch 2/2
25/25 - 8s - loss: 388.6113 - loglik: -3.8613e+02 - logprior: -2.4768e+00
Fitted a model with MAP estimate = -385.0050
expansions: [(0, 3), (213, 1)]
discards: [  0   9  44  61  81 143 205]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 392.2363 - loglik: -3.8791e+02 - logprior: -4.3252e+00
Epoch 2/2
25/25 - 7s - loss: 386.0325 - loglik: -3.8560e+02 - logprior: -4.2930e-01
Fitted a model with MAP estimate = -383.8346
expansions: []
discards: [  0   2 105]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 393.8616 - loglik: -3.8756e+02 - logprior: -6.3034e+00
Epoch 2/10
25/25 - 7s - loss: 388.2412 - loglik: -3.8704e+02 - logprior: -1.1963e+00
Epoch 3/10
25/25 - 7s - loss: 383.8807 - loglik: -3.8429e+02 - logprior: 0.4138
Epoch 4/10
25/25 - 7s - loss: 381.8767 - loglik: -3.8239e+02 - logprior: 0.5108
Epoch 5/10
25/25 - 7s - loss: 382.1802 - loglik: -3.8278e+02 - logprior: 0.5958
Fitted a model with MAP estimate = -380.6967
Time for alignment: 140.2661
Computed alignments with likelihoods: ['-381.3328', '-380.1586', '-381.2065', '-379.8475', '-380.6967']
Best model has likelihood: -379.8475  (prior= 0.9462 )
time for generating output: 0.2304
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9298310066127847
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.2510 - loglik: -3.8615e+02 - logprior: -2.0097e+01
Epoch 2/10
10/10 - 1s - loss: 341.6351 - loglik: -3.3703e+02 - logprior: -4.6043e+00
Epoch 3/10
10/10 - 1s - loss: 289.3085 - loglik: -2.8679e+02 - logprior: -2.5150e+00
Epoch 4/10
10/10 - 1s - loss: 255.4762 - loglik: -2.5324e+02 - logprior: -2.2404e+00
Epoch 5/10
10/10 - 1s - loss: 242.4916 - loglik: -2.4054e+02 - logprior: -1.9533e+00
Epoch 6/10
10/10 - 1s - loss: 238.2001 - loglik: -2.3653e+02 - logprior: -1.6651e+00
Epoch 7/10
10/10 - 1s - loss: 236.0596 - loglik: -2.3462e+02 - logprior: -1.4418e+00
Epoch 8/10
10/10 - 1s - loss: 234.8590 - loglik: -2.3366e+02 - logprior: -1.2020e+00
Epoch 9/10
10/10 - 1s - loss: 234.5858 - loglik: -2.3350e+02 - logprior: -1.0826e+00
Epoch 10/10
10/10 - 1s - loss: 234.0116 - loglik: -2.3302e+02 - logprior: -9.8929e-01
Fitted a model with MAP estimate = -233.9807
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.5007 - loglik: -2.3007e+02 - logprior: -2.6428e+01
Epoch 2/2
10/10 - 2s - loss: 219.4324 - loglik: -2.1208e+02 - logprior: -7.3529e+00
Fitted a model with MAP estimate = -212.6021
expansions: []
discards: [ 62 121 141]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.7248 - loglik: -2.0826e+02 - logprior: -1.8465e+01
Epoch 2/2
10/10 - 2s - loss: 210.0616 - loglik: -2.0615e+02 - logprior: -3.9137e+00
Fitted a model with MAP estimate = -207.5516
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.1767 - loglik: -2.0683e+02 - logprior: -1.7351e+01
Epoch 2/10
10/10 - 2s - loss: 209.2997 - loglik: -2.0578e+02 - logprior: -3.5227e+00
Epoch 3/10
10/10 - 2s - loss: 206.0857 - loglik: -2.0520e+02 - logprior: -8.8596e-01
Epoch 4/10
10/10 - 2s - loss: 204.6175 - loglik: -2.0463e+02 - logprior: 0.0102
Epoch 5/10
10/10 - 2s - loss: 203.3624 - loglik: -2.0381e+02 - logprior: 0.4448
Epoch 6/10
10/10 - 2s - loss: 203.1167 - loglik: -2.0389e+02 - logprior: 0.7753
Epoch 7/10
10/10 - 2s - loss: 203.0541 - loglik: -2.0410e+02 - logprior: 1.0482
Epoch 8/10
10/10 - 2s - loss: 202.4455 - loglik: -2.0367e+02 - logprior: 1.2257
Epoch 9/10
10/10 - 2s - loss: 202.2909 - loglik: -2.0361e+02 - logprior: 1.3206
Epoch 10/10
10/10 - 2s - loss: 202.3083 - loglik: -2.0372e+02 - logprior: 1.4094
Fitted a model with MAP estimate = -202.0302
Time for alignment: 57.2182
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 405.9973 - loglik: -3.8590e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 341.1010 - loglik: -3.3649e+02 - logprior: -4.6072e+00
Epoch 3/10
10/10 - 1s - loss: 289.1663 - loglik: -2.8669e+02 - logprior: -2.4757e+00
Epoch 4/10
10/10 - 1s - loss: 258.8842 - loglik: -2.5664e+02 - logprior: -2.2442e+00
Epoch 5/10
10/10 - 1s - loss: 246.6788 - loglik: -2.4470e+02 - logprior: -1.9807e+00
Epoch 6/10
10/10 - 1s - loss: 242.2220 - loglik: -2.4058e+02 - logprior: -1.6424e+00
Epoch 7/10
10/10 - 1s - loss: 240.7619 - loglik: -2.3926e+02 - logprior: -1.5036e+00
Epoch 8/10
10/10 - 1s - loss: 239.6370 - loglik: -2.3825e+02 - logprior: -1.3844e+00
Epoch 9/10
10/10 - 1s - loss: 238.7611 - loglik: -2.3748e+02 - logprior: -1.2861e+00
Epoch 10/10
10/10 - 1s - loss: 238.1970 - loglik: -2.3694e+02 - logprior: -1.2563e+00
Fitted a model with MAP estimate = -238.5952
expansions: [(0, 3), (11, 1), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 2), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.3713 - loglik: -2.3193e+02 - logprior: -2.6446e+01
Epoch 2/2
10/10 - 2s - loss: 221.6162 - loglik: -2.1426e+02 - logprior: -7.3589e+00
Fitted a model with MAP estimate = -213.7604
expansions: [(132, 1)]
discards: [61]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.6739 - loglik: -2.0918e+02 - logprior: -1.8496e+01
Epoch 2/2
10/10 - 2s - loss: 209.8717 - loglik: -2.0595e+02 - logprior: -3.9209e+00
Fitted a model with MAP estimate = -207.5954
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.8045 - loglik: -2.0743e+02 - logprior: -1.7371e+01
Epoch 2/10
10/10 - 2s - loss: 208.6999 - loglik: -2.0515e+02 - logprior: -3.5481e+00
Epoch 3/10
10/10 - 2s - loss: 206.1892 - loglik: -2.0527e+02 - logprior: -9.2175e-01
Epoch 4/10
10/10 - 2s - loss: 204.3341 - loglik: -2.0432e+02 - logprior: -1.8328e-02
Epoch 5/10
10/10 - 2s - loss: 204.2132 - loglik: -2.0463e+02 - logprior: 0.4177
Epoch 6/10
10/10 - 2s - loss: 202.8186 - loglik: -2.0357e+02 - logprior: 0.7480
Epoch 7/10
10/10 - 2s - loss: 202.7697 - loglik: -2.0378e+02 - logprior: 1.0148
Epoch 8/10
10/10 - 2s - loss: 202.3315 - loglik: -2.0352e+02 - logprior: 1.1913
Epoch 9/10
10/10 - 2s - loss: 202.1946 - loglik: -2.0348e+02 - logprior: 1.2898
Epoch 10/10
10/10 - 2s - loss: 202.5220 - loglik: -2.0389e+02 - logprior: 1.3730
Fitted a model with MAP estimate = -202.0181
Time for alignment: 57.0811
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.2088 - loglik: -3.8611e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 1s - loss: 341.0455 - loglik: -3.3643e+02 - logprior: -4.6123e+00
Epoch 3/10
10/10 - 1s - loss: 289.9460 - loglik: -2.8743e+02 - logprior: -2.5142e+00
Epoch 4/10
10/10 - 1s - loss: 258.2027 - loglik: -2.5594e+02 - logprior: -2.2589e+00
Epoch 5/10
10/10 - 1s - loss: 246.2641 - loglik: -2.4427e+02 - logprior: -1.9906e+00
Epoch 6/10
10/10 - 1s - loss: 242.8082 - loglik: -2.4108e+02 - logprior: -1.7302e+00
Epoch 7/10
10/10 - 1s - loss: 240.0086 - loglik: -2.3840e+02 - logprior: -1.6099e+00
Epoch 8/10
10/10 - 1s - loss: 239.7051 - loglik: -2.3823e+02 - logprior: -1.4770e+00
Epoch 9/10
10/10 - 1s - loss: 239.4693 - loglik: -2.3809e+02 - logprior: -1.3763e+00
Epoch 10/10
10/10 - 1s - loss: 238.4239 - loglik: -2.3707e+02 - logprior: -1.3508e+00
Fitted a model with MAP estimate = -238.7237
expansions: [(0, 3), (11, 1), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.5961 - loglik: -2.3215e+02 - logprior: -2.6441e+01
Epoch 2/2
10/10 - 2s - loss: 221.7162 - loglik: -2.1437e+02 - logprior: -7.3414e+00
Fitted a model with MAP estimate = -213.7095
expansions: [(132, 1)]
discards: [62]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.4883 - loglik: -2.0901e+02 - logprior: -1.8481e+01
Epoch 2/2
10/10 - 2s - loss: 210.2437 - loglik: -2.0634e+02 - logprior: -3.9000e+00
Fitted a model with MAP estimate = -207.5919
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.3369 - loglik: -2.0699e+02 - logprior: -1.7349e+01
Epoch 2/10
10/10 - 2s - loss: 208.7419 - loglik: -2.0521e+02 - logprior: -3.5338e+00
Epoch 3/10
10/10 - 2s - loss: 206.6994 - loglik: -2.0580e+02 - logprior: -9.0241e-01
Epoch 4/10
10/10 - 2s - loss: 203.9422 - loglik: -2.0394e+02 - logprior: -2.1501e-03
Epoch 5/10
10/10 - 2s - loss: 204.2152 - loglik: -2.0465e+02 - logprior: 0.4329
Fitted a model with MAP estimate = -203.2944
Time for alignment: 48.3548
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 405.9647 - loglik: -3.8586e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 341.4022 - loglik: -3.3679e+02 - logprior: -4.6107e+00
Epoch 3/10
10/10 - 1s - loss: 287.2752 - loglik: -2.8481e+02 - logprior: -2.4644e+00
Epoch 4/10
10/10 - 1s - loss: 256.9435 - loglik: -2.5490e+02 - logprior: -2.0463e+00
Epoch 5/10
10/10 - 1s - loss: 244.4503 - loglik: -2.4281e+02 - logprior: -1.6442e+00
Epoch 6/10
10/10 - 1s - loss: 239.1891 - loglik: -2.3785e+02 - logprior: -1.3371e+00
Epoch 7/10
10/10 - 1s - loss: 237.7331 - loglik: -2.3658e+02 - logprior: -1.1560e+00
Epoch 8/10
10/10 - 1s - loss: 236.5579 - loglik: -2.3560e+02 - logprior: -9.5437e-01
Epoch 9/10
10/10 - 1s - loss: 235.8293 - loglik: -2.3499e+02 - logprior: -8.3790e-01
Epoch 10/10
10/10 - 1s - loss: 235.2726 - loglik: -2.3450e+02 - logprior: -7.7193e-01
Fitted a model with MAP estimate = -235.4197
expansions: [(0, 4), (13, 1), (14, 2), (15, 2), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (76, 1), (77, 2), (78, 4), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.0357 - loglik: -2.3158e+02 - logprior: -2.6454e+01
Epoch 2/2
10/10 - 2s - loss: 220.5076 - loglik: -2.1296e+02 - logprior: -7.5480e+00
Fitted a model with MAP estimate = -213.2287
expansions: []
discards: [ 19  63  96 123 143]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.0156 - loglik: -2.0850e+02 - logprior: -1.8512e+01
Epoch 2/2
10/10 - 2s - loss: 210.5653 - loglik: -2.0664e+02 - logprior: -3.9205e+00
Fitted a model with MAP estimate = -207.6353
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.5871 - loglik: -2.0623e+02 - logprior: -1.7357e+01
Epoch 2/10
10/10 - 2s - loss: 210.2306 - loglik: -2.0670e+02 - logprior: -3.5344e+00
Epoch 3/10
10/10 - 2s - loss: 205.7774 - loglik: -2.0489e+02 - logprior: -8.9221e-01
Epoch 4/10
10/10 - 2s - loss: 204.5598 - loglik: -2.0457e+02 - logprior: 0.0100
Epoch 5/10
10/10 - 2s - loss: 203.6734 - loglik: -2.0411e+02 - logprior: 0.4415
Epoch 6/10
10/10 - 2s - loss: 202.9635 - loglik: -2.0374e+02 - logprior: 0.7758
Epoch 7/10
10/10 - 2s - loss: 202.7361 - loglik: -2.0378e+02 - logprior: 1.0409
Epoch 8/10
10/10 - 2s - loss: 202.8255 - loglik: -2.0405e+02 - logprior: 1.2223
Fitted a model with MAP estimate = -202.3347
Time for alignment: 53.5371
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.3125 - loglik: -3.8621e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 341.4481 - loglik: -3.3684e+02 - logprior: -4.6124e+00
Epoch 3/10
10/10 - 1s - loss: 288.5145 - loglik: -2.8601e+02 - logprior: -2.5037e+00
Epoch 4/10
10/10 - 1s - loss: 257.8066 - loglik: -2.5560e+02 - logprior: -2.2093e+00
Epoch 5/10
10/10 - 1s - loss: 246.5414 - loglik: -2.4478e+02 - logprior: -1.7575e+00
Epoch 6/10
10/10 - 1s - loss: 243.6200 - loglik: -2.4228e+02 - logprior: -1.3364e+00
Epoch 7/10
10/10 - 1s - loss: 241.4728 - loglik: -2.4032e+02 - logprior: -1.1518e+00
Epoch 8/10
10/10 - 1s - loss: 240.8057 - loglik: -2.3979e+02 - logprior: -1.0134e+00
Epoch 9/10
10/10 - 1s - loss: 240.7545 - loglik: -2.3984e+02 - logprior: -9.1176e-01
Epoch 10/10
10/10 - 1s - loss: 240.1343 - loglik: -2.3924e+02 - logprior: -8.9658e-01
Fitted a model with MAP estimate = -239.9306
expansions: [(0, 3), (14, 3), (15, 1), (22, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.5360 - loglik: -2.3209e+02 - logprior: -2.6449e+01
Epoch 2/2
10/10 - 2s - loss: 221.6634 - loglik: -2.1427e+02 - logprior: -7.3969e+00
Fitted a model with MAP estimate = -213.9240
expansions: [(133, 1)]
discards: [59 62]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.4410 - loglik: -2.0895e+02 - logprior: -1.8487e+01
Epoch 2/2
10/10 - 2s - loss: 210.4087 - loglik: -2.0649e+02 - logprior: -3.9146e+00
Fitted a model with MAP estimate = -207.6153
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.2749 - loglik: -2.0691e+02 - logprior: -1.7365e+01
Epoch 2/10
10/10 - 2s - loss: 209.1525 - loglik: -2.0561e+02 - logprior: -3.5436e+00
Epoch 3/10
10/10 - 2s - loss: 206.6178 - loglik: -2.0569e+02 - logprior: -9.2374e-01
Epoch 4/10
10/10 - 2s - loss: 204.2077 - loglik: -2.0419e+02 - logprior: -1.5996e-02
Epoch 5/10
10/10 - 2s - loss: 203.0436 - loglik: -2.0346e+02 - logprior: 0.4167
Epoch 6/10
10/10 - 2s - loss: 203.1560 - loglik: -2.0390e+02 - logprior: 0.7429
Fitted a model with MAP estimate = -202.8852
Time for alignment: 50.1336
Computed alignments with likelihoods: ['-202.0302', '-202.0181', '-203.2944', '-202.3347', '-202.8852']
Best model has likelihood: -202.0181  (prior= 1.4211 )
time for generating output: 0.1727
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.910580204778157
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9159 - loglik: -2.4845e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 236.2238 - loglik: -2.1902e+02 - logprior: -1.7207e+01
Epoch 3/10
10/10 - 1s - loss: 203.6738 - loglik: -1.9634e+02 - logprior: -7.3386e+00
Epoch 4/10
10/10 - 1s - loss: 189.4666 - loglik: -1.8572e+02 - logprior: -3.7490e+00
Epoch 5/10
10/10 - 1s - loss: 183.3405 - loglik: -1.8160e+02 - logprior: -1.7365e+00
Epoch 6/10
10/10 - 1s - loss: 179.8415 - loglik: -1.7921e+02 - logprior: -6.2747e-01
Epoch 7/10
10/10 - 1s - loss: 177.8331 - loglik: -1.7781e+02 - logprior: -2.6408e-02
Epoch 8/10
10/10 - 1s - loss: 176.4957 - loglik: -1.7685e+02 - logprior: 0.3572
Epoch 9/10
10/10 - 1s - loss: 175.6017 - loglik: -1.7622e+02 - logprior: 0.6157
Epoch 10/10
10/10 - 1s - loss: 175.1970 - loglik: -1.7599e+02 - logprior: 0.7892
Fitted a model with MAP estimate = -175.0120
expansions: [(9, 3), (14, 2), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 252.6193 - loglik: -1.7586e+02 - logprior: -7.6759e+01
Epoch 2/2
10/10 - 2s - loss: 198.8061 - loglik: -1.6813e+02 - logprior: -3.0673e+01
Fitted a model with MAP estimate = -189.2653
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 237.9949 - loglik: -1.6531e+02 - logprior: -7.2684e+01
Epoch 2/10
10/10 - 2s - loss: 183.5741 - loglik: -1.6324e+02 - logprior: -2.0334e+01
Epoch 3/10
10/10 - 2s - loss: 169.2125 - loglik: -1.6260e+02 - logprior: -6.6082e+00
Epoch 4/10
10/10 - 2s - loss: 164.4370 - loglik: -1.6260e+02 - logprior: -1.8342e+00
Epoch 5/10
10/10 - 2s - loss: 162.2140 - loglik: -1.6268e+02 - logprior: 0.4665
Epoch 6/10
10/10 - 1s - loss: 160.8324 - loglik: -1.6256e+02 - logprior: 1.7303
Epoch 7/10
10/10 - 1s - loss: 160.0516 - loglik: -1.6255e+02 - logprior: 2.4981
Epoch 8/10
10/10 - 1s - loss: 159.5700 - loglik: -1.6264e+02 - logprior: 3.0725
Epoch 9/10
10/10 - 2s - loss: 159.2156 - loglik: -1.6278e+02 - logprior: 3.5609
Epoch 10/10
10/10 - 2s - loss: 158.9099 - loglik: -1.6288e+02 - logprior: 3.9659
Fitted a model with MAP estimate = -158.7363
Time for alignment: 42.9644
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9178 - loglik: -2.4845e+02 - logprior: -6.8471e+01
Epoch 2/10
10/10 - 1s - loss: 236.4647 - loglik: -2.1926e+02 - logprior: -1.7208e+01
Epoch 3/10
10/10 - 1s - loss: 203.8451 - loglik: -1.9648e+02 - logprior: -7.3692e+00
Epoch 4/10
10/10 - 1s - loss: 189.7954 - loglik: -1.8590e+02 - logprior: -3.8960e+00
Epoch 5/10
10/10 - 1s - loss: 182.6740 - loglik: -1.8073e+02 - logprior: -1.9416e+00
Epoch 6/10
10/10 - 1s - loss: 178.1310 - loglik: -1.7715e+02 - logprior: -9.7909e-01
Epoch 7/10
10/10 - 1s - loss: 175.7047 - loglik: -1.7531e+02 - logprior: -3.9767e-01
Epoch 8/10
10/10 - 1s - loss: 174.4337 - loglik: -1.7440e+02 - logprior: -3.8623e-02
Epoch 9/10
10/10 - 1s - loss: 173.8827 - loglik: -1.7407e+02 - logprior: 0.1833
Epoch 10/10
10/10 - 2s - loss: 173.5386 - loglik: -1.7392e+02 - logprior: 0.3800
Fitted a model with MAP estimate = -173.3868
expansions: [(9, 3), (14, 1), (15, 1), (26, 1), (27, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.6501 - loglik: -1.7390e+02 - logprior: -7.6746e+01
Epoch 2/2
10/10 - 2s - loss: 197.5960 - loglik: -1.6684e+02 - logprior: -3.0757e+01
Fitted a model with MAP estimate = -188.6219
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.6935 - loglik: -1.6482e+02 - logprior: -7.2869e+01
Epoch 2/10
10/10 - 1s - loss: 183.6351 - loglik: -1.6298e+02 - logprior: -2.0655e+01
Epoch 3/10
10/10 - 2s - loss: 169.2664 - loglik: -1.6242e+02 - logprior: -6.8472e+00
Epoch 4/10
10/10 - 2s - loss: 164.4660 - loglik: -1.6244e+02 - logprior: -2.0249e+00
Epoch 5/10
10/10 - 2s - loss: 162.0685 - loglik: -1.6235e+02 - logprior: 0.2850
Epoch 6/10
10/10 - 2s - loss: 160.7039 - loglik: -1.6225e+02 - logprior: 1.5460
Epoch 7/10
10/10 - 1s - loss: 159.9680 - loglik: -1.6227e+02 - logprior: 2.3018
Epoch 8/10
10/10 - 2s - loss: 159.4955 - loglik: -1.6240e+02 - logprior: 2.9050
Epoch 9/10
10/10 - 1s - loss: 159.1301 - loglik: -1.6256e+02 - logprior: 3.4249
Epoch 10/10
10/10 - 2s - loss: 158.8220 - loglik: -1.6265e+02 - logprior: 3.8271
Fitted a model with MAP estimate = -158.6494
Time for alignment: 43.4239
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9046 - loglik: -2.4844e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 236.0765 - loglik: -2.1887e+02 - logprior: -1.7211e+01
Epoch 3/10
10/10 - 2s - loss: 203.5468 - loglik: -1.9620e+02 - logprior: -7.3507e+00
Epoch 4/10
10/10 - 1s - loss: 189.4018 - loglik: -1.8561e+02 - logprior: -3.7908e+00
Epoch 5/10
10/10 - 1s - loss: 181.8205 - loglik: -1.7984e+02 - logprior: -1.9768e+00
Epoch 6/10
10/10 - 1s - loss: 177.4670 - loglik: -1.7659e+02 - logprior: -8.7997e-01
Epoch 7/10
10/10 - 1s - loss: 175.4152 - loglik: -1.7514e+02 - logprior: -2.7219e-01
Epoch 8/10
10/10 - 1s - loss: 174.4698 - loglik: -1.7455e+02 - logprior: 0.0797
Epoch 9/10
10/10 - 1s - loss: 173.9746 - loglik: -1.7432e+02 - logprior: 0.3454
Epoch 10/10
10/10 - 1s - loss: 173.7071 - loglik: -1.7422e+02 - logprior: 0.5141
Fitted a model with MAP estimate = -173.5818
expansions: [(10, 2), (14, 2), (15, 1), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.3898 - loglik: -1.7457e+02 - logprior: -7.6821e+01
Epoch 2/2
10/10 - 2s - loss: 198.1014 - loglik: -1.6716e+02 - logprior: -3.0944e+01
Fitted a model with MAP estimate = -189.2252
expansions: []
discards: [ 0  9 15]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.0374 - loglik: -1.6763e+02 - logprior: -7.5404e+01
Epoch 2/2
10/10 - 1s - loss: 191.7159 - loglik: -1.6590e+02 - logprior: -2.5816e+01
Fitted a model with MAP estimate = -180.4890
expansions: [(5, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 245.2444 - loglik: -1.6754e+02 - logprior: -7.7700e+01
Epoch 2/10
10/10 - 2s - loss: 197.2836 - loglik: -1.6645e+02 - logprior: -3.0834e+01
Epoch 3/10
10/10 - 1s - loss: 187.6391 - loglik: -1.6651e+02 - logprior: -2.1128e+01
Epoch 4/10
10/10 - 2s - loss: 183.3961 - loglik: -1.6661e+02 - logprior: -1.6783e+01
Epoch 5/10
10/10 - 1s - loss: 178.8966 - loglik: -1.6663e+02 - logprior: -1.2264e+01
Epoch 6/10
10/10 - 1s - loss: 169.1406 - loglik: -1.6641e+02 - logprior: -2.7313e+00
Epoch 7/10
10/10 - 2s - loss: 163.8231 - loglik: -1.6591e+02 - logprior: 2.0899
Epoch 8/10
10/10 - 1s - loss: 162.9464 - loglik: -1.6605e+02 - logprior: 3.1072
Epoch 9/10
10/10 - 2s - loss: 162.5031 - loglik: -1.6624e+02 - logprior: 3.7381
Epoch 10/10
10/10 - 1s - loss: 162.1737 - loglik: -1.6634e+02 - logprior: 4.1641
Fitted a model with MAP estimate = -161.9846
Time for alignment: 49.2507
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.9015 - loglik: -2.4843e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 236.0717 - loglik: -2.1886e+02 - logprior: -1.7207e+01
Epoch 3/10
10/10 - 1s - loss: 202.2189 - loglik: -1.9487e+02 - logprior: -7.3508e+00
Epoch 4/10
10/10 - 1s - loss: 187.4034 - loglik: -1.8366e+02 - logprior: -3.7449e+00
Epoch 5/10
10/10 - 1s - loss: 181.7744 - loglik: -1.8004e+02 - logprior: -1.7312e+00
Epoch 6/10
10/10 - 1s - loss: 178.7703 - loglik: -1.7802e+02 - logprior: -7.4856e-01
Epoch 7/10
10/10 - 1s - loss: 177.0650 - loglik: -1.7691e+02 - logprior: -1.5205e-01
Epoch 8/10
10/10 - 1s - loss: 176.3163 - loglik: -1.7657e+02 - logprior: 0.2517
Epoch 9/10
10/10 - 1s - loss: 175.8199 - loglik: -1.7635e+02 - logprior: 0.5271
Epoch 10/10
10/10 - 1s - loss: 175.4558 - loglik: -1.7617e+02 - logprior: 0.7116
Fitted a model with MAP estimate = -175.2877
expansions: [(9, 3), (14, 2), (26, 2), (27, 3), (38, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.0421 - loglik: -1.7629e+02 - logprior: -7.6756e+01
Epoch 2/2
10/10 - 2s - loss: 198.7035 - loglik: -1.6794e+02 - logprior: -3.0765e+01
Fitted a model with MAP estimate = -188.9858
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.5642 - loglik: -1.6456e+02 - logprior: -7.3006e+01
Epoch 2/10
10/10 - 2s - loss: 182.7728 - loglik: -1.6202e+02 - logprior: -2.0748e+01
Epoch 3/10
10/10 - 2s - loss: 167.6855 - loglik: -1.6101e+02 - logprior: -6.6733e+00
Epoch 4/10
10/10 - 1s - loss: 162.5492 - loglik: -1.6074e+02 - logprior: -1.8054e+00
Epoch 5/10
10/10 - 2s - loss: 160.1825 - loglik: -1.6069e+02 - logprior: 0.5084
Epoch 6/10
10/10 - 2s - loss: 158.8474 - loglik: -1.6065e+02 - logprior: 1.8003
Epoch 7/10
10/10 - 2s - loss: 158.0817 - loglik: -1.6065e+02 - logprior: 2.5692
Epoch 8/10
10/10 - 2s - loss: 157.5935 - loglik: -1.6075e+02 - logprior: 3.1544
Epoch 9/10
10/10 - 2s - loss: 157.2012 - loglik: -1.6086e+02 - logprior: 3.6551
Epoch 10/10
10/10 - 2s - loss: 156.8664 - loglik: -1.6094e+02 - logprior: 4.0699
Fitted a model with MAP estimate = -156.6440
Time for alignment: 43.1947
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9193 - loglik: -2.4845e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 236.1943 - loglik: -2.1898e+02 - logprior: -1.7210e+01
Epoch 3/10
10/10 - 1s - loss: 203.2859 - loglik: -1.9592e+02 - logprior: -7.3703e+00
Epoch 4/10
10/10 - 1s - loss: 189.3866 - loglik: -1.8558e+02 - logprior: -3.8065e+00
Epoch 5/10
10/10 - 1s - loss: 183.6034 - loglik: -1.8174e+02 - logprior: -1.8671e+00
Epoch 6/10
10/10 - 2s - loss: 180.3763 - loglik: -1.7952e+02 - logprior: -8.5183e-01
Epoch 7/10
10/10 - 1s - loss: 178.3659 - loglik: -1.7813e+02 - logprior: -2.3366e-01
Epoch 8/10
10/10 - 1s - loss: 177.2731 - loglik: -1.7742e+02 - logprior: 0.1500
Epoch 9/10
10/10 - 1s - loss: 176.6493 - loglik: -1.7703e+02 - logprior: 0.3774
Epoch 10/10
10/10 - 1s - loss: 176.2981 - loglik: -1.7687e+02 - logprior: 0.5731
Fitted a model with MAP estimate = -176.1321
expansions: [(9, 3), (14, 1), (15, 1), (24, 1), (26, 1), (27, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.9761 - loglik: -1.7625e+02 - logprior: -7.6722e+01
Epoch 2/2
10/10 - 2s - loss: 198.4160 - loglik: -1.6757e+02 - logprior: -3.0845e+01
Fitted a model with MAP estimate = -188.8587
expansions: [(33, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 237.8038 - loglik: -1.6447e+02 - logprior: -7.3332e+01
Epoch 2/2
10/10 - 2s - loss: 183.1968 - loglik: -1.6192e+02 - logprior: -2.1280e+01
Fitted a model with MAP estimate = -172.0874
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.3354 - loglik: -1.6102e+02 - logprior: -6.1313e+01
Epoch 2/10
10/10 - 2s - loss: 175.4518 - loglik: -1.6063e+02 - logprior: -1.4826e+01
Epoch 3/10
10/10 - 2s - loss: 165.8713 - loglik: -1.6073e+02 - logprior: -5.1441e+00
Epoch 4/10
10/10 - 2s - loss: 161.8857 - loglik: -1.6073e+02 - logprior: -1.1537e+00
Epoch 5/10
10/10 - 2s - loss: 159.7293 - loglik: -1.6068e+02 - logprior: 0.9551
Epoch 6/10
10/10 - 2s - loss: 158.5314 - loglik: -1.6068e+02 - logprior: 2.1499
Epoch 7/10
10/10 - 2s - loss: 157.8462 - loglik: -1.6081e+02 - logprior: 2.9633
Epoch 8/10
10/10 - 2s - loss: 157.3663 - loglik: -1.6099e+02 - logprior: 3.6268
Epoch 9/10
10/10 - 2s - loss: 157.0002 - loglik: -1.6115e+02 - logprior: 4.1516
Epoch 10/10
10/10 - 2s - loss: 156.6870 - loglik: -1.6125e+02 - logprior: 4.5620
Fitted a model with MAP estimate = -156.5042
Time for alignment: 50.2484
Computed alignments with likelihoods: ['-158.7363', '-158.6494', '-161.9846', '-156.6440', '-156.5042']
Best model has likelihood: -156.5042  (prior= 4.7354 )
time for generating output: 0.2210
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.35792259345021504
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 757.1630 - loglik: -7.4930e+02 - logprior: -7.8629e+00
Epoch 2/10
20/20 - 9s - loss: 685.6215 - loglik: -6.8586e+02 - logprior: 0.2344
Epoch 3/10
20/20 - 9s - loss: 655.3766 - loglik: -6.5553e+02 - logprior: 0.1554
Epoch 4/10
20/20 - 9s - loss: 645.5734 - loglik: -6.4607e+02 - logprior: 0.4952
Epoch 5/10
20/20 - 9s - loss: 646.4678 - loglik: -6.4704e+02 - logprior: 0.5753
Fitted a model with MAP estimate = -644.0153
expansions: [(0, 4), (25, 1), (45, 1), (52, 3), (82, 3), (86, 12), (92, 1), (94, 1), (116, 1), (118, 1), (147, 1), (153, 2), (168, 1), (169, 8), (198, 2), (200, 1), (207, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 662.0302 - loglik: -6.5074e+02 - logprior: -1.1290e+01
Epoch 2/2
20/20 - 11s - loss: 641.8856 - loglik: -6.4145e+02 - logprior: -4.3083e-01
Fitted a model with MAP estimate = -637.9670
expansions: [(0, 3)]
discards: [  2   3  93  94  98  99 100 200 201 202 203 238]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 654.9127 - loglik: -6.4331e+02 - logprior: -1.1608e+01
Epoch 2/2
20/20 - 11s - loss: 641.3137 - loglik: -6.4053e+02 - logprior: -7.8020e-01
Fitted a model with MAP estimate = -638.2155
expansions: [(0, 4), (97, 3), (175, 1)]
discards: [ 0  1  2  4  5  6 94]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 650.3348 - loglik: -6.4190e+02 - logprior: -8.4306e+00
Epoch 2/10
20/20 - 11s - loss: 640.0832 - loglik: -6.4075e+02 - logprior: 0.6645
Epoch 3/10
20/20 - 11s - loss: 633.6022 - loglik: -6.3534e+02 - logprior: 1.7331
Epoch 4/10
20/20 - 11s - loss: 635.9565 - loglik: -6.3807e+02 - logprior: 2.1154
Fitted a model with MAP estimate = -632.2391
Time for alignment: 176.6851
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 756.7723 - loglik: -7.4890e+02 - logprior: -7.8763e+00
Epoch 2/10
20/20 - 9s - loss: 685.9880 - loglik: -6.8624e+02 - logprior: 0.2509
Epoch 3/10
20/20 - 9s - loss: 659.3033 - loglik: -6.5959e+02 - logprior: 0.2840
Epoch 4/10
20/20 - 9s - loss: 645.7509 - loglik: -6.4629e+02 - logprior: 0.5382
Epoch 5/10
20/20 - 9s - loss: 647.1646 - loglik: -6.4777e+02 - logprior: 0.6032
Fitted a model with MAP estimate = -644.7401
expansions: [(0, 4), (54, 4), (62, 1), (86, 9), (93, 1), (102, 1), (115, 2), (117, 1), (152, 2), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 663.7406 - loglik: -6.5253e+02 - logprior: -1.1214e+01
Epoch 2/2
20/20 - 11s - loss: 644.0258 - loglik: -6.4346e+02 - logprior: -5.6892e-01
Fitted a model with MAP estimate = -640.2597
expansions: [(0, 3), (170, 1)]
discards: [  2   3  59 135]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 655.8151 - loglik: -6.4423e+02 - logprior: -1.1583e+01
Epoch 2/2
20/20 - 11s - loss: 641.5380 - loglik: -6.4077e+02 - logprior: -7.6742e-01
Fitted a model with MAP estimate = -638.3958
expansions: [(0, 3), (26, 5), (50, 1), (98, 2)]
discards: [  1   2   4   5   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 653.9752 - loglik: -6.4249e+02 - logprior: -1.1482e+01
Epoch 2/10
20/20 - 11s - loss: 640.4667 - loglik: -6.3944e+02 - logprior: -1.0234e+00
Epoch 3/10
20/20 - 11s - loss: 632.4052 - loglik: -6.3396e+02 - logprior: 1.5529
Epoch 4/10
20/20 - 11s - loss: 632.8182 - loglik: -6.3487e+02 - logprior: 2.0565
Fitted a model with MAP estimate = -631.2023
Time for alignment: 171.5312
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 756.0952 - loglik: -7.4821e+02 - logprior: -7.8895e+00
Epoch 2/10
20/20 - 9s - loss: 687.5883 - loglik: -6.8784e+02 - logprior: 0.2516
Epoch 3/10
20/20 - 9s - loss: 663.4413 - loglik: -6.6374e+02 - logprior: 0.2998
Epoch 4/10
20/20 - 9s - loss: 648.9981 - loglik: -6.4946e+02 - logprior: 0.4570
Epoch 5/10
20/20 - 9s - loss: 651.1984 - loglik: -6.5165e+02 - logprior: 0.4562
Fitted a model with MAP estimate = -648.2015
expansions: [(0, 3), (10, 1), (25, 1), (27, 1), (51, 1), (53, 2), (55, 1), (85, 8), (86, 1), (93, 1), (110, 4), (112, 1), (115, 1), (117, 1), (146, 1), (199, 2), (201, 2), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 664.6302 - loglik: -6.5379e+02 - logprior: -1.0840e+01
Epoch 2/2
20/20 - 11s - loss: 643.8435 - loglik: -6.4353e+02 - logprior: -3.1331e-01
Fitted a model with MAP estimate = -641.3497
expansions: [(0, 3), (14, 1), (99, 1), (177, 1), (232, 1), (239, 1)]
discards: [  1   2   3   4   5   6 132 133]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 656.0826 - loglik: -6.4441e+02 - logprior: -1.1672e+01
Epoch 2/2
20/20 - 11s - loss: 642.4088 - loglik: -6.4201e+02 - logprior: -3.9780e-01
Fitted a model with MAP estimate = -637.9776
expansions: [(0, 3), (96, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 651.8690 - loglik: -6.4369e+02 - logprior: -8.1774e+00
Epoch 2/10
20/20 - 11s - loss: 639.2958 - loglik: -6.4000e+02 - logprior: 0.7018
Epoch 3/10
20/20 - 11s - loss: 634.3613 - loglik: -6.3614e+02 - logprior: 1.7786
Epoch 4/10
20/20 - 11s - loss: 630.3121 - loglik: -6.3245e+02 - logprior: 2.1370
Epoch 5/10
20/20 - 11s - loss: 634.4908 - loglik: -6.3683e+02 - logprior: 2.3414
Fitted a model with MAP estimate = -630.4138
Time for alignment: 183.6784
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 757.2633 - loglik: -7.4938e+02 - logprior: -7.8803e+00
Epoch 2/10
20/20 - 9s - loss: 686.7528 - loglik: -6.8706e+02 - logprior: 0.3035
Epoch 3/10
20/20 - 9s - loss: 661.0582 - loglik: -6.6139e+02 - logprior: 0.3332
Epoch 4/10
20/20 - 9s - loss: 651.7014 - loglik: -6.5219e+02 - logprior: 0.4848
Epoch 5/10
20/20 - 9s - loss: 647.8480 - loglik: -6.4841e+02 - logprior: 0.5578
Epoch 6/10
20/20 - 9s - loss: 647.2548 - loglik: -6.4783e+02 - logprior: 0.5798
Epoch 7/10
20/20 - 9s - loss: 643.4447 - loglik: -6.4404e+02 - logprior: 0.5948
Epoch 8/10
20/20 - 9s - loss: 647.4568 - loglik: -6.4804e+02 - logprior: 0.5861
Fitted a model with MAP estimate = -645.3079
expansions: [(0, 3), (45, 1), (52, 1), (54, 2), (59, 1), (82, 1), (83, 1), (85, 7), (92, 2), (93, 1), (110, 3), (114, 1), (115, 2), (116, 1), (154, 2), (155, 1), (199, 4), (207, 1), (208, 1)]
discards: [  1 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 668.6456 - loglik: -6.5735e+02 - logprior: -1.1291e+01
Epoch 2/2
20/20 - 11s - loss: 644.6940 - loglik: -6.4440e+02 - logprior: -2.9622e-01
Fitted a model with MAP estimate = -641.3444
expansions: [(0, 3)]
discards: [  3   4   5  91  96 108 130 131 138 181]
Re-initialized the encoder parameters.
Fitting a model of length 251 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 658.2164 - loglik: -6.4635e+02 - logprior: -1.1870e+01
Epoch 2/2
20/20 - 10s - loss: 643.8146 - loglik: -6.4335e+02 - logprior: -4.6044e-01
Fitted a model with MAP estimate = -638.9901
expansions: [(0, 3), (215, 2)]
discards: [5]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 654.6341 - loglik: -6.4356e+02 - logprior: -1.1077e+01
Epoch 2/10
20/20 - 10s - loss: 640.9871 - loglik: -6.4032e+02 - logprior: -6.6454e-01
Epoch 3/10
20/20 - 11s - loss: 637.0131 - loglik: -6.3887e+02 - logprior: 1.8531
Epoch 4/10
20/20 - 11s - loss: 636.2062 - loglik: -6.3859e+02 - logprior: 2.3857
Epoch 5/10
20/20 - 10s - loss: 630.3640 - loglik: -6.3299e+02 - logprior: 2.6249
Epoch 6/10
20/20 - 11s - loss: 630.7722 - loglik: -6.3355e+02 - logprior: 2.7751
Fitted a model with MAP estimate = -631.1961
Time for alignment: 218.9885
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 757.0127 - loglik: -7.4914e+02 - logprior: -7.8741e+00
Epoch 2/10
20/20 - 9s - loss: 690.4523 - loglik: -6.9072e+02 - logprior: 0.2628
Epoch 3/10
20/20 - 9s - loss: 658.8825 - loglik: -6.5923e+02 - logprior: 0.3437
Epoch 4/10
20/20 - 9s - loss: 653.8401 - loglik: -6.5440e+02 - logprior: 0.5566
Epoch 5/10
20/20 - 9s - loss: 650.0599 - loglik: -6.5070e+02 - logprior: 0.6359
Epoch 6/10
20/20 - 9s - loss: 651.4546 - loglik: -6.5213e+02 - logprior: 0.6785
Fitted a model with MAP estimate = -648.4264
expansions: [(0, 3), (42, 1), (43, 2), (57, 2), (62, 1), (81, 4), (83, 2), (86, 1), (93, 1), (115, 1), (117, 1), (146, 1), (152, 1), (170, 9), (198, 3), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 668.1647 - loglik: -6.5713e+02 - logprior: -1.1036e+01
Epoch 2/2
20/20 - 11s - loss: 649.0547 - loglik: -6.4880e+02 - logprior: -2.5336e-01
Fitted a model with MAP estimate = -643.0308
expansions: [(0, 3), (96, 2)]
discards: [  1   2   3   4   5   6 191 192 193 194 195 196 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 249 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 659.9719 - loglik: -6.4815e+02 - logprior: -1.1822e+01
Epoch 2/2
20/20 - 10s - loss: 645.3081 - loglik: -6.4488e+02 - logprior: -4.2664e-01
Fitted a model with MAP estimate = -642.9019
expansions: [(0, 4), (168, 1), (190, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 654.3924 - loglik: -6.4609e+02 - logprior: -8.2998e+00
Epoch 2/10
20/20 - 11s - loss: 641.5165 - loglik: -6.4222e+02 - logprior: 0.7085
Epoch 3/10
20/20 - 11s - loss: 636.3121 - loglik: -6.3809e+02 - logprior: 1.7814
Epoch 4/10
20/20 - 11s - loss: 632.2089 - loglik: -6.3439e+02 - logprior: 2.1779
Epoch 5/10
20/20 - 11s - loss: 634.1316 - loglik: -6.3652e+02 - logprior: 2.3882
Fitted a model with MAP estimate = -632.2119
Time for alignment: 192.0542
Computed alignments with likelihoods: ['-632.2391', '-631.2023', '-630.4138', '-631.1961', '-632.2119']
Best model has likelihood: -630.4138  (prior= 2.4366 )
time for generating output: 0.2883
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6350257542310522
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6691 - loglik: -1.3143e+02 - logprior: -3.2369e+00
Epoch 2/10
19/19 - 1s - loss: 113.7745 - loglik: -1.1243e+02 - logprior: -1.3457e+00
Epoch 3/10
19/19 - 1s - loss: 106.9526 - loglik: -1.0553e+02 - logprior: -1.4182e+00
Epoch 4/10
19/19 - 1s - loss: 105.5000 - loglik: -1.0417e+02 - logprior: -1.3276e+00
Epoch 5/10
19/19 - 1s - loss: 104.7508 - loglik: -1.0347e+02 - logprior: -1.2807e+00
Epoch 6/10
19/19 - 1s - loss: 104.3863 - loglik: -1.0308e+02 - logprior: -1.3079e+00
Epoch 7/10
19/19 - 1s - loss: 104.1396 - loglik: -1.0284e+02 - logprior: -1.2984e+00
Epoch 8/10
19/19 - 1s - loss: 104.0645 - loglik: -1.0278e+02 - logprior: -1.2875e+00
Epoch 9/10
19/19 - 1s - loss: 103.8422 - loglik: -1.0254e+02 - logprior: -1.2972e+00
Epoch 10/10
19/19 - 1s - loss: 103.7524 - loglik: -1.0247e+02 - logprior: -1.2862e+00
Fitted a model with MAP estimate = -102.3308
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.7457 - loglik: -1.0257e+02 - logprior: -4.1788e+00
Epoch 2/2
19/19 - 1s - loss: 97.7075 - loglik: -9.5704e+01 - logprior: -2.0038e+00
Fitted a model with MAP estimate = -93.9958
expansions: [(0, 1)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.6366 - loglik: -9.3461e+01 - logprior: -3.1756e+00
Epoch 2/2
19/19 - 1s - loss: 93.1488 - loglik: -9.1737e+01 - logprior: -1.4114e+00
Fitted a model with MAP estimate = -91.3822
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.5932 - loglik: -9.1429e+01 - logprior: -3.1646e+00
Epoch 2/10
19/19 - 1s - loss: 92.0896 - loglik: -9.0716e+01 - logprior: -1.3736e+00
Epoch 3/10
19/19 - 1s - loss: 91.5557 - loglik: -9.0318e+01 - logprior: -1.2380e+00
Epoch 4/10
19/19 - 1s - loss: 90.8594 - loglik: -8.9652e+01 - logprior: -1.2077e+00
Epoch 5/10
19/19 - 1s - loss: 90.4039 - loglik: -8.9217e+01 - logprior: -1.1865e+00
Epoch 6/10
19/19 - 1s - loss: 90.4032 - loglik: -8.9228e+01 - logprior: -1.1751e+00
Epoch 7/10
19/19 - 1s - loss: 90.5951 - loglik: -8.9427e+01 - logprior: -1.1677e+00
Fitted a model with MAP estimate = -90.2090
Time for alignment: 43.0676
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7466 - loglik: -1.3151e+02 - logprior: -3.2366e+00
Epoch 2/10
19/19 - 1s - loss: 114.9251 - loglik: -1.1357e+02 - logprior: -1.3588e+00
Epoch 3/10
19/19 - 1s - loss: 107.0642 - loglik: -1.0566e+02 - logprior: -1.4070e+00
Epoch 4/10
19/19 - 1s - loss: 104.1851 - loglik: -1.0274e+02 - logprior: -1.4434e+00
Epoch 5/10
19/19 - 1s - loss: 103.5184 - loglik: -1.0207e+02 - logprior: -1.4434e+00
Epoch 6/10
19/19 - 1s - loss: 102.9766 - loglik: -1.0155e+02 - logprior: -1.4295e+00
Epoch 7/10
19/19 - 1s - loss: 102.9265 - loglik: -1.0151e+02 - logprior: -1.4177e+00
Epoch 8/10
19/19 - 1s - loss: 102.5862 - loglik: -1.0117e+02 - logprior: -1.4192e+00
Epoch 9/10
19/19 - 1s - loss: 102.6686 - loglik: -1.0125e+02 - logprior: -1.4197e+00
Fitted a model with MAP estimate = -101.1338
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (28, 3), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.3574 - loglik: -1.0117e+02 - logprior: -4.1897e+00
Epoch 2/2
19/19 - 1s - loss: 96.1220 - loglik: -9.4137e+01 - logprior: -1.9855e+00
Fitted a model with MAP estimate = -93.0570
expansions: [(0, 1)]
discards: [ 0 29 37]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.6840 - loglik: -9.3509e+01 - logprior: -3.1747e+00
Epoch 2/2
19/19 - 1s - loss: 93.4864 - loglik: -9.2099e+01 - logprior: -1.3875e+00
Fitted a model with MAP estimate = -91.6074
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.1855 - loglik: -9.1008e+01 - logprior: -3.1779e+00
Epoch 2/10
19/19 - 1s - loss: 91.8002 - loglik: -9.0419e+01 - logprior: -1.3810e+00
Epoch 3/10
19/19 - 1s - loss: 91.3326 - loglik: -9.0090e+01 - logprior: -1.2430e+00
Epoch 4/10
19/19 - 1s - loss: 91.1577 - loglik: -8.9946e+01 - logprior: -1.2115e+00
Epoch 5/10
19/19 - 1s - loss: 90.9779 - loglik: -8.9793e+01 - logprior: -1.1846e+00
Epoch 6/10
19/19 - 1s - loss: 90.2417 - loglik: -8.9063e+01 - logprior: -1.1786e+00
Epoch 7/10
19/19 - 1s - loss: 90.4352 - loglik: -8.9267e+01 - logprior: -1.1681e+00
Fitted a model with MAP estimate = -90.3575
Time for alignment: 42.3953
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7118 - loglik: -1.3147e+02 - logprior: -3.2379e+00
Epoch 2/10
19/19 - 1s - loss: 113.9527 - loglik: -1.1261e+02 - logprior: -1.3393e+00
Epoch 3/10
19/19 - 1s - loss: 107.4739 - loglik: -1.0609e+02 - logprior: -1.3848e+00
Epoch 4/10
19/19 - 1s - loss: 105.7227 - loglik: -1.0441e+02 - logprior: -1.3091e+00
Epoch 5/10
19/19 - 1s - loss: 105.0735 - loglik: -1.0379e+02 - logprior: -1.2876e+00
Epoch 6/10
19/19 - 1s - loss: 104.5262 - loglik: -1.0322e+02 - logprior: -1.3027e+00
Epoch 7/10
19/19 - 1s - loss: 104.6402 - loglik: -1.0335e+02 - logprior: -1.2906e+00
Fitted a model with MAP estimate = -102.8841
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 1), (28, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.6309 - loglik: -1.0247e+02 - logprior: -4.1621e+00
Epoch 2/2
19/19 - 1s - loss: 97.1598 - loglik: -9.5254e+01 - logprior: -1.9054e+00
Fitted a model with MAP estimate = -93.5034
expansions: [(0, 1)]
discards: [0 9]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.6034 - loglik: -9.3421e+01 - logprior: -3.1825e+00
Epoch 2/2
19/19 - 1s - loss: 93.4519 - loglik: -9.2060e+01 - logprior: -1.3920e+00
Fitted a model with MAP estimate = -91.5919
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.2831 - loglik: -9.1105e+01 - logprior: -3.1783e+00
Epoch 2/10
19/19 - 1s - loss: 91.7328 - loglik: -9.0345e+01 - logprior: -1.3874e+00
Epoch 3/10
19/19 - 1s - loss: 91.4022 - loglik: -9.0159e+01 - logprior: -1.2433e+00
Epoch 4/10
19/19 - 1s - loss: 90.8674 - loglik: -8.9653e+01 - logprior: -1.2140e+00
Epoch 5/10
19/19 - 1s - loss: 90.9134 - loglik: -8.9719e+01 - logprior: -1.1948e+00
Fitted a model with MAP estimate = -90.5402
Time for alignment: 36.8795
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6447 - loglik: -1.3140e+02 - logprior: -3.2398e+00
Epoch 2/10
19/19 - 1s - loss: 115.2302 - loglik: -1.1389e+02 - logprior: -1.3364e+00
Epoch 3/10
19/19 - 1s - loss: 107.3444 - loglik: -1.0603e+02 - logprior: -1.3174e+00
Epoch 4/10
19/19 - 1s - loss: 105.0367 - loglik: -1.0367e+02 - logprior: -1.3652e+00
Epoch 5/10
19/19 - 1s - loss: 103.4610 - loglik: -1.0214e+02 - logprior: -1.3256e+00
Epoch 6/10
19/19 - 1s - loss: 103.1019 - loglik: -1.0174e+02 - logprior: -1.3602e+00
Epoch 7/10
19/19 - 1s - loss: 103.1636 - loglik: -1.0183e+02 - logprior: -1.3376e+00
Fitted a model with MAP estimate = -101.4705
expansions: [(6, 1), (9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (29, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.5956 - loglik: -1.0243e+02 - logprior: -4.1652e+00
Epoch 2/2
19/19 - 1s - loss: 98.9202 - loglik: -9.7107e+01 - logprior: -1.8130e+00
Fitted a model with MAP estimate = -96.1003
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.5698 - loglik: -9.6377e+01 - logprior: -3.1928e+00
Epoch 2/2
19/19 - 1s - loss: 95.2839 - loglik: -9.3904e+01 - logprior: -1.3800e+00
Fitted a model with MAP estimate = -92.9224
expansions: [(36, 1)]
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.7589 - loglik: -9.2581e+01 - logprior: -3.1783e+00
Epoch 2/10
19/19 - 1s - loss: 92.6982 - loglik: -9.1321e+01 - logprior: -1.3771e+00
Epoch 3/10
19/19 - 1s - loss: 92.4805 - loglik: -9.1240e+01 - logprior: -1.2408e+00
Epoch 4/10
19/19 - 1s - loss: 91.8924 - loglik: -9.0682e+01 - logprior: -1.2103e+00
Epoch 5/10
19/19 - 1s - loss: 91.2850 - loglik: -9.0099e+01 - logprior: -1.1855e+00
Epoch 6/10
19/19 - 1s - loss: 91.0451 - loglik: -8.9864e+01 - logprior: -1.1809e+00
Epoch 7/10
19/19 - 1s - loss: 90.7743 - loglik: -8.9603e+01 - logprior: -1.1711e+00
Epoch 8/10
19/19 - 1s - loss: 90.8727 - loglik: -8.9713e+01 - logprior: -1.1599e+00
Fitted a model with MAP estimate = -90.6690
Time for alignment: 40.1457
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6751 - loglik: -1.3144e+02 - logprior: -3.2354e+00
Epoch 2/10
19/19 - 1s - loss: 114.0394 - loglik: -1.1271e+02 - logprior: -1.3293e+00
Epoch 3/10
19/19 - 1s - loss: 107.2923 - loglik: -1.0592e+02 - logprior: -1.3745e+00
Epoch 4/10
19/19 - 1s - loss: 105.7345 - loglik: -1.0442e+02 - logprior: -1.3164e+00
Epoch 5/10
19/19 - 1s - loss: 104.7401 - loglik: -1.0346e+02 - logprior: -1.2834e+00
Epoch 6/10
19/19 - 1s - loss: 104.3265 - loglik: -1.0304e+02 - logprior: -1.2857e+00
Epoch 7/10
19/19 - 1s - loss: 104.2697 - loglik: -1.0301e+02 - logprior: -1.2632e+00
Epoch 8/10
19/19 - 1s - loss: 103.1631 - loglik: -1.0189e+02 - logprior: -1.2767e+00
Epoch 9/10
19/19 - 1s - loss: 102.5622 - loglik: -1.0135e+02 - logprior: -1.2141e+00
Epoch 10/10
19/19 - 1s - loss: 102.2471 - loglik: -1.0106e+02 - logprior: -1.1823e+00
Fitted a model with MAP estimate = -100.7939
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (29, 1), (35, 1), (36, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.7888 - loglik: -1.0157e+02 - logprior: -4.2167e+00
Epoch 2/2
19/19 - 1s - loss: 97.2703 - loglik: -9.5191e+01 - logprior: -2.0790e+00
Fitted a model with MAP estimate = -94.0793
expansions: [(0, 1)]
discards: [ 0 13 29 46]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.6247 - loglik: -9.4373e+01 - logprior: -3.2516e+00
Epoch 2/2
19/19 - 1s - loss: 93.8424 - loglik: -9.2445e+01 - logprior: -1.3973e+00
Fitted a model with MAP estimate = -92.1145
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.5216 - loglik: -9.1294e+01 - logprior: -3.2280e+00
Epoch 2/10
19/19 - 1s - loss: 92.0869 - loglik: -9.0719e+01 - logprior: -1.3680e+00
Epoch 3/10
19/19 - 1s - loss: 91.2847 - loglik: -9.0053e+01 - logprior: -1.2312e+00
Epoch 4/10
19/19 - 1s - loss: 91.1501 - loglik: -8.9953e+01 - logprior: -1.1975e+00
Epoch 5/10
19/19 - 1s - loss: 91.0976 - loglik: -8.9914e+01 - logprior: -1.1839e+00
Epoch 6/10
19/19 - 1s - loss: 90.5926 - loglik: -8.9420e+01 - logprior: -1.1728e+00
Epoch 7/10
19/19 - 1s - loss: 90.5436 - loglik: -8.9374e+01 - logprior: -1.1700e+00
Epoch 8/10
19/19 - 1s - loss: 90.4673 - loglik: -8.9316e+01 - logprior: -1.1517e+00
Epoch 9/10
19/19 - 1s - loss: 90.4178 - loglik: -8.9268e+01 - logprior: -1.1497e+00
Epoch 10/10
19/19 - 1s - loss: 90.5759 - loglik: -8.9444e+01 - logprior: -1.1315e+00
Fitted a model with MAP estimate = -90.3899
Time for alignment: 46.1291
Computed alignments with likelihoods: ['-90.2090', '-90.3575', '-90.5402', '-90.6690', '-90.3899']
Best model has likelihood: -90.2090  (prior= -1.1706 )
time for generating output: 0.0885
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9519038076152304
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 653.2137 - loglik: -6.3469e+02 - logprior: -1.8525e+01
Epoch 2/10
15/15 - 6s - loss: 569.6451 - loglik: -5.6924e+02 - logprior: -4.0174e-01
Epoch 3/10
15/15 - 6s - loss: 513.4450 - loglik: -5.1370e+02 - logprior: 0.2590
Epoch 4/10
15/15 - 6s - loss: 481.1497 - loglik: -4.8097e+02 - logprior: -1.7874e-01
Epoch 5/10
15/15 - 6s - loss: 473.4985 - loglik: -4.7306e+02 - logprior: -4.3922e-01
Epoch 6/10
15/15 - 6s - loss: 468.1438 - loglik: -4.6774e+02 - logprior: -4.0556e-01
Epoch 7/10
15/15 - 6s - loss: 467.4984 - loglik: -4.6727e+02 - logprior: -2.2588e-01
Epoch 8/10
15/15 - 6s - loss: 466.3901 - loglik: -4.6629e+02 - logprior: -9.5497e-02
Epoch 9/10
15/15 - 6s - loss: 466.1451 - loglik: -4.6611e+02 - logprior: -3.1472e-02
Epoch 10/10
15/15 - 6s - loss: 465.2957 - loglik: -4.6533e+02 - logprior: 0.0364
Fitted a model with MAP estimate = -465.0180
expansions: [(19, 1), (25, 4), (39, 2), (41, 1), (43, 1), (47, 1), (50, 1), (51, 1), (53, 2), (54, 2), (65, 1), (68, 5), (69, 2), (71, 1), (72, 1), (73, 1), (76, 2), (85, 1), (98, 1), (100, 1), (101, 2), (102, 4), (131, 2), (132, 2), (133, 4), (134, 2), (137, 1), (140, 1), (156, 1), (157, 5), (158, 1), (174, 7), (183, 4)]
discards: [  1   2   6 200 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 469.5693 - loglik: -4.5325e+02 - logprior: -1.6315e+01
Epoch 2/2
15/15 - 9s - loss: 436.5545 - loglik: -4.3695e+02 - logprior: 0.3988
Fitted a model with MAP estimate = -431.3590
expansions: [(0, 3), (62, 1), (243, 1), (245, 1)]
discards: [ 23 130 169 177 235]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 460.2124 - loglik: -4.3566e+02 - logprior: -2.4550e+01
Epoch 2/2
15/15 - 9s - loss: 430.2363 - loglik: -4.2813e+02 - logprior: -2.1070e+00
Fitted a model with MAP estimate = -427.2916
expansions: [(28, 1), (206, 1)]
discards: [  0   1   2  90 168 234]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 451.1662 - loglik: -4.3411e+02 - logprior: -1.7059e+01
Epoch 2/10
15/15 - 9s - loss: 426.5332 - loglik: -4.2876e+02 - logprior: 2.2251
Epoch 3/10
15/15 - 9s - loss: 423.1922 - loglik: -4.2823e+02 - logprior: 5.0427
Epoch 4/10
15/15 - 9s - loss: 426.0292 - loglik: -4.3203e+02 - logprior: 5.9982
Fitted a model with MAP estimate = -421.8245
Time for alignment: 165.2551
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 654.1635 - loglik: -6.3563e+02 - logprior: -1.8532e+01
Epoch 2/10
15/15 - 6s - loss: 567.6051 - loglik: -5.6728e+02 - logprior: -3.2357e-01
Epoch 3/10
15/15 - 6s - loss: 509.6518 - loglik: -5.1001e+02 - logprior: 0.3629
Epoch 4/10
15/15 - 6s - loss: 476.2902 - loglik: -4.7608e+02 - logprior: -2.1370e-01
Epoch 5/10
15/15 - 6s - loss: 472.9890 - loglik: -4.7266e+02 - logprior: -3.2693e-01
Epoch 6/10
15/15 - 6s - loss: 466.8956 - loglik: -4.6656e+02 - logprior: -3.3637e-01
Epoch 7/10
15/15 - 6s - loss: 468.9000 - loglik: -4.6865e+02 - logprior: -2.5032e-01
Fitted a model with MAP estimate = -466.8476
expansions: [(25, 4), (28, 1), (37, 3), (39, 1), (41, 1), (49, 1), (50, 1), (52, 2), (53, 1), (54, 1), (65, 1), (68, 5), (69, 2), (71, 1), (72, 1), (73, 1), (76, 2), (78, 1), (93, 1), (100, 4), (101, 1), (102, 1), (130, 1), (131, 2), (132, 2), (133, 4), (134, 2), (136, 2), (156, 1), (157, 5), (174, 7), (181, 1), (182, 2), (183, 1)]
discards: [  1   2   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 470.6322 - loglik: -4.5431e+02 - logprior: -1.6326e+01
Epoch 2/2
15/15 - 9s - loss: 434.7234 - loglik: -4.3513e+02 - logprior: 0.4110
Fitted a model with MAP estimate = -430.9056
expansions: [(0, 3), (21, 1), (206, 2), (207, 1), (240, 1)]
discards: [129 169 177 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 457.9007 - loglik: -4.3340e+02 - logprior: -2.4502e+01
Epoch 2/2
15/15 - 9s - loss: 433.5458 - loglik: -4.3157e+02 - logprior: -1.9759e+00
Fitted a model with MAP estimate = -427.6852
expansions: [(244, 1)]
discards: [  0   1   2  91 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 450.2647 - loglik: -4.3317e+02 - logprior: -1.7097e+01
Epoch 2/10
15/15 - 9s - loss: 427.1996 - loglik: -4.2916e+02 - logprior: 1.9616
Epoch 3/10
15/15 - 9s - loss: 424.9092 - loglik: -4.2975e+02 - logprior: 4.8370
Epoch 4/10
15/15 - 9s - loss: 424.2692 - loglik: -4.3007e+02 - logprior: 5.7995
Epoch 5/10
15/15 - 9s - loss: 419.7436 - loglik: -4.2614e+02 - logprior: 6.3982
Epoch 6/10
15/15 - 9s - loss: 420.0435 - loglik: -4.2690e+02 - logprior: 6.8608
Fitted a model with MAP estimate = -420.6615
Time for alignment: 162.9119
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 651.6599 - loglik: -6.3312e+02 - logprior: -1.8544e+01
Epoch 2/10
15/15 - 6s - loss: 565.6512 - loglik: -5.6527e+02 - logprior: -3.8054e-01
Epoch 3/10
15/15 - 6s - loss: 502.8630 - loglik: -5.0334e+02 - logprior: 0.4735
Epoch 4/10
15/15 - 6s - loss: 484.3677 - loglik: -4.8440e+02 - logprior: 0.0305
Epoch 5/10
15/15 - 6s - loss: 471.8098 - loglik: -4.7143e+02 - logprior: -3.7615e-01
Epoch 6/10
15/15 - 6s - loss: 465.7070 - loglik: -4.6558e+02 - logprior: -1.2905e-01
Epoch 7/10
15/15 - 6s - loss: 465.4416 - loglik: -4.6547e+02 - logprior: 0.0315
Epoch 8/10
15/15 - 6s - loss: 464.8245 - loglik: -4.6502e+02 - logprior: 0.1912
Epoch 9/10
15/15 - 6s - loss: 465.3482 - loglik: -4.6562e+02 - logprior: 0.2696
Fitted a model with MAP estimate = -464.3095
expansions: [(25, 3), (39, 1), (42, 1), (49, 1), (52, 1), (53, 1), (56, 1), (71, 1), (72, 1), (73, 8), (74, 3), (75, 1), (78, 2), (80, 1), (86, 1), (102, 1), (103, 2), (104, 4), (133, 2), (134, 2), (135, 4), (136, 2), (138, 1), (140, 1), (157, 7), (173, 7), (183, 4), (197, 1)]
discards: [  1   2 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 466.1147 - loglik: -4.5001e+02 - logprior: -1.6101e+01
Epoch 2/2
15/15 - 9s - loss: 437.4369 - loglik: -4.3828e+02 - logprior: 0.8472
Fitted a model with MAP estimate = -429.7249
expansions: [(202, 2)]
discards: [129 168 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 449.4115 - loglik: -4.3421e+02 - logprior: -1.5206e+01
Epoch 2/2
15/15 - 9s - loss: 430.0230 - loglik: -4.3153e+02 - logprior: 1.5052
Fitted a model with MAP estimate = -426.7747
expansions: [(0, 3), (200, 1), (238, 1)]
discards: [164]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 454.4813 - loglik: -4.3056e+02 - logprior: -2.3921e+01
Epoch 2/10
15/15 - 9s - loss: 429.6936 - loglik: -4.2853e+02 - logprior: -1.1606e+00
Epoch 3/10
15/15 - 9s - loss: 424.3810 - loglik: -4.2860e+02 - logprior: 4.2227
Epoch 4/10
15/15 - 9s - loss: 420.8483 - loglik: -4.2684e+02 - logprior: 5.9871
Epoch 5/10
15/15 - 9s - loss: 422.4149 - loglik: -4.2902e+02 - logprior: 6.6079
Fitted a model with MAP estimate = -420.1069
Time for alignment: 167.2475
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 653.4085 - loglik: -6.3490e+02 - logprior: -1.8512e+01
Epoch 2/10
15/15 - 6s - loss: 568.2756 - loglik: -5.6785e+02 - logprior: -4.2388e-01
Epoch 3/10
15/15 - 6s - loss: 503.7660 - loglik: -5.0414e+02 - logprior: 0.3768
Epoch 4/10
15/15 - 6s - loss: 483.9149 - loglik: -4.8376e+02 - logprior: -1.5941e-01
Epoch 5/10
15/15 - 6s - loss: 468.9546 - loglik: -4.6830e+02 - logprior: -6.5876e-01
Epoch 6/10
15/15 - 6s - loss: 470.5182 - loglik: -4.7016e+02 - logprior: -3.6218e-01
Fitted a model with MAP estimate = -468.4542
expansions: [(23, 1), (24, 2), (25, 2), (38, 3), (40, 1), (42, 1), (46, 1), (49, 1), (50, 1), (51, 2), (53, 2), (63, 1), (67, 2), (68, 4), (69, 1), (71, 1), (72, 1), (73, 1), (76, 1), (85, 1), (101, 4), (103, 2), (131, 1), (132, 2), (133, 2), (134, 4), (135, 2), (137, 1), (139, 1), (156, 1), (157, 6), (173, 8), (183, 2)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 474.7612 - loglik: -4.5821e+02 - logprior: -1.6553e+01
Epoch 2/2
15/15 - 9s - loss: 434.1097 - loglik: -4.3442e+02 - logprior: 0.3114
Fitted a model with MAP estimate = -431.0517
expansions: [(63, 1), (67, 1), (206, 2), (207, 1)]
discards: [ 84  85 131 170 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 449.2637 - loglik: -4.3387e+02 - logprior: -1.5392e+01
Epoch 2/2
15/15 - 9s - loss: 430.7342 - loglik: -4.3202e+02 - logprior: 1.2888
Fitted a model with MAP estimate = -426.6678
expansions: [(0, 3), (242, 2), (243, 1)]
discards: [ 22 204]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 454.5731 - loglik: -4.3116e+02 - logprior: -2.3411e+01
Epoch 2/10
15/15 - 9s - loss: 428.8623 - loglik: -4.2798e+02 - logprior: -8.7763e-01
Epoch 3/10
15/15 - 9s - loss: 421.7188 - loglik: -4.2592e+02 - logprior: 4.2021
Epoch 4/10
15/15 - 9s - loss: 423.1530 - loglik: -4.2891e+02 - logprior: 5.7602
Fitted a model with MAP estimate = -419.8955
Time for alignment: 140.1777
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 651.6937 - loglik: -6.3315e+02 - logprior: -1.8547e+01
Epoch 2/10
15/15 - 6s - loss: 573.6310 - loglik: -5.7318e+02 - logprior: -4.4874e-01
Epoch 3/10
15/15 - 6s - loss: 508.9427 - loglik: -5.0918e+02 - logprior: 0.2363
Epoch 4/10
15/15 - 6s - loss: 485.9367 - loglik: -4.8582e+02 - logprior: -1.1192e-01
Epoch 5/10
15/15 - 6s - loss: 471.3026 - loglik: -4.7080e+02 - logprior: -5.0549e-01
Epoch 6/10
15/15 - 6s - loss: 471.2868 - loglik: -4.7070e+02 - logprior: -5.8452e-01
Epoch 7/10
15/15 - 6s - loss: 469.1242 - loglik: -4.6879e+02 - logprior: -3.2965e-01
Epoch 8/10
15/15 - 6s - loss: 469.1089 - loglik: -4.6890e+02 - logprior: -2.1204e-01
Epoch 9/10
15/15 - 6s - loss: 467.1172 - loglik: -4.6691e+02 - logprior: -2.0758e-01
Epoch 10/10
15/15 - 6s - loss: 469.1143 - loglik: -4.6900e+02 - logprior: -1.1199e-01
Fitted a model with MAP estimate = -467.2150
expansions: [(19, 1), (23, 1), (24, 2), (25, 2), (37, 2), (39, 1), (49, 1), (51, 1), (53, 2), (54, 1), (55, 1), (68, 1), (70, 2), (71, 5), (73, 1), (74, 1), (75, 1), (77, 1), (79, 1), (86, 1), (102, 4), (104, 3), (110, 1), (130, 1), (131, 2), (132, 2), (133, 4), (134, 2), (136, 2), (137, 4), (151, 2), (153, 2), (154, 1), (174, 8), (198, 1)]
discards: [  1   2   6 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 471.9471 - loglik: -4.5557e+02 - logprior: -1.6379e+01
Epoch 2/2
15/15 - 9s - loss: 439.9982 - loglik: -4.4027e+02 - logprior: 0.2715
Fitted a model with MAP estimate = -433.6892
expansions: [(26, 1), (41, 1), (206, 1)]
discards: [ 83 129 169 177 185 202 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 454.1310 - loglik: -4.3892e+02 - logprior: -1.5215e+01
Epoch 2/2
15/15 - 9s - loss: 431.7785 - loglik: -4.3332e+02 - logprior: 1.5379
Fitted a model with MAP estimate = -429.7236
expansions: [(0, 3)]
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 459.3208 - loglik: -4.3560e+02 - logprior: -2.3719e+01
Epoch 2/10
15/15 - 9s - loss: 432.9298 - loglik: -4.3169e+02 - logprior: -1.2375e+00
Epoch 3/10
15/15 - 9s - loss: 428.9624 - loglik: -4.3298e+02 - logprior: 4.0207
Epoch 4/10
15/15 - 9s - loss: 424.8310 - loglik: -4.3041e+02 - logprior: 5.5830
Epoch 5/10
15/15 - 9s - loss: 425.2017 - loglik: -4.3132e+02 - logprior: 6.1230
Fitted a model with MAP estimate = -423.2126
Time for alignment: 171.5170
Computed alignments with likelihoods: ['-421.8245', '-420.6615', '-420.1069', '-419.8955', '-423.2126']
Best model has likelihood: -419.8955  (prior= 6.0695 )
time for generating output: 0.2765
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9238287832827772
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.7583 - loglik: -1.8509e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 180.2572 - loglik: -1.6906e+02 - logprior: -1.1193e+01
Epoch 3/10
10/10 - 1s - loss: 161.7299 - loglik: -1.5622e+02 - logprior: -5.5145e+00
Epoch 4/10
10/10 - 1s - loss: 151.6119 - loglik: -1.4805e+02 - logprior: -3.5601e+00
Epoch 5/10
10/10 - 1s - loss: 147.4894 - loglik: -1.4485e+02 - logprior: -2.6403e+00
Epoch 6/10
10/10 - 1s - loss: 145.4607 - loglik: -1.4326e+02 - logprior: -2.2023e+00
Epoch 7/10
10/10 - 1s - loss: 144.7036 - loglik: -1.4296e+02 - logprior: -1.7442e+00
Epoch 8/10
10/10 - 1s - loss: 144.1529 - loglik: -1.4278e+02 - logprior: -1.3712e+00
Epoch 9/10
10/10 - 1s - loss: 143.8649 - loglik: -1.4263e+02 - logprior: -1.2299e+00
Epoch 10/10
10/10 - 0s - loss: 143.8527 - loglik: -1.4269e+02 - logprior: -1.1596e+00
Fitted a model with MAP estimate = -143.6480
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 2), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 194.8700 - loglik: -1.4016e+02 - logprior: -5.4707e+01
Epoch 2/2
10/10 - 1s - loss: 152.2635 - loglik: -1.3565e+02 - logprior: -1.6611e+01
Fitted a model with MAP estimate = -144.5468
expansions: []
discards: [ 0 22 27]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 182.7744 - loglik: -1.3550e+02 - logprior: -4.7279e+01
Epoch 2/2
10/10 - 1s - loss: 153.3756 - loglik: -1.3508e+02 - logprior: -1.8299e+01
Fitted a model with MAP estimate = -148.2869
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 177.0153 - loglik: -1.3449e+02 - logprior: -4.2521e+01
Epoch 2/10
10/10 - 1s - loss: 145.1305 - loglik: -1.3363e+02 - logprior: -1.1498e+01
Epoch 3/10
10/10 - 1s - loss: 137.4388 - loglik: -1.3302e+02 - logprior: -4.4140e+00
Epoch 4/10
10/10 - 1s - loss: 134.8599 - loglik: -1.3290e+02 - logprior: -1.9616e+00
Epoch 5/10
10/10 - 1s - loss: 133.3422 - loglik: -1.3245e+02 - logprior: -8.9357e-01
Epoch 6/10
10/10 - 1s - loss: 132.4442 - loglik: -1.3208e+02 - logprior: -3.5925e-01
Epoch 7/10
10/10 - 1s - loss: 132.2525 - loglik: -1.3242e+02 - logprior: 0.1677
Epoch 8/10
10/10 - 1s - loss: 132.0484 - loglik: -1.3264e+02 - logprior: 0.5876
Epoch 9/10
10/10 - 1s - loss: 131.6472 - loglik: -1.3243e+02 - logprior: 0.7866
Epoch 10/10
10/10 - 0s - loss: 131.5002 - loglik: -1.3242e+02 - logprior: 0.9186
Fitted a model with MAP estimate = -131.4492
Time for alignment: 28.9599
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.9385 - loglik: -1.8527e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 180.3590 - loglik: -1.6916e+02 - logprior: -1.1195e+01
Epoch 3/10
10/10 - 1s - loss: 162.3153 - loglik: -1.5679e+02 - logprior: -5.5280e+00
Epoch 4/10
10/10 - 1s - loss: 152.5890 - loglik: -1.4902e+02 - logprior: -3.5661e+00
Epoch 5/10
10/10 - 1s - loss: 148.2344 - loglik: -1.4559e+02 - logprior: -2.6430e+00
Epoch 6/10
10/10 - 1s - loss: 146.2173 - loglik: -1.4401e+02 - logprior: -2.2094e+00
Epoch 7/10
10/10 - 1s - loss: 145.1434 - loglik: -1.4336e+02 - logprior: -1.7876e+00
Epoch 8/10
10/10 - 1s - loss: 144.4205 - loglik: -1.4302e+02 - logprior: -1.4054e+00
Epoch 9/10
10/10 - 1s - loss: 144.0689 - loglik: -1.4285e+02 - logprior: -1.2182e+00
Epoch 10/10
10/10 - 1s - loss: 143.8780 - loglik: -1.4274e+02 - logprior: -1.1338e+00
Fitted a model with MAP estimate = -143.7350
expansions: [(0, 2), (17, 1), (19, 2), (20, 1), (25, 1), (27, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 195.0919 - loglik: -1.4020e+02 - logprior: -5.4889e+01
Epoch 2/2
10/10 - 1s - loss: 152.1431 - loglik: -1.3560e+02 - logprior: -1.6546e+01
Fitted a model with MAP estimate = -144.1973
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 182.1638 - loglik: -1.3481e+02 - logprior: -4.7350e+01
Epoch 2/2
10/10 - 1s - loss: 152.7132 - loglik: -1.3442e+02 - logprior: -1.8297e+01
Fitted a model with MAP estimate = -147.6780
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.5177 - loglik: -1.3408e+02 - logprior: -4.2436e+01
Epoch 2/10
10/10 - 1s - loss: 144.7685 - loglik: -1.3329e+02 - logprior: -1.1474e+01
Epoch 3/10
10/10 - 1s - loss: 137.6243 - loglik: -1.3319e+02 - logprior: -4.4348e+00
Epoch 4/10
10/10 - 1s - loss: 134.7494 - loglik: -1.3276e+02 - logprior: -1.9908e+00
Epoch 5/10
10/10 - 1s - loss: 133.2070 - loglik: -1.3229e+02 - logprior: -9.1211e-01
Epoch 6/10
10/10 - 1s - loss: 132.5232 - loglik: -1.3215e+02 - logprior: -3.7459e-01
Epoch 7/10
10/10 - 1s - loss: 132.3647 - loglik: -1.3250e+02 - logprior: 0.1361
Epoch 8/10
10/10 - 1s - loss: 131.8233 - loglik: -1.3237e+02 - logprior: 0.5497
Epoch 9/10
10/10 - 1s - loss: 131.7672 - loglik: -1.3253e+02 - logprior: 0.7604
Epoch 10/10
10/10 - 1s - loss: 131.5748 - loglik: -1.3247e+02 - logprior: 0.8918
Fitted a model with MAP estimate = -131.4904
Time for alignment: 27.9037
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.6166 - loglik: -1.8495e+02 - logprior: -4.1665e+01
Epoch 2/10
10/10 - 1s - loss: 180.2362 - loglik: -1.6904e+02 - logprior: -1.1198e+01
Epoch 3/10
10/10 - 1s - loss: 160.8914 - loglik: -1.5538e+02 - logprior: -5.5119e+00
Epoch 4/10
10/10 - 1s - loss: 151.4891 - loglik: -1.4798e+02 - logprior: -3.5072e+00
Epoch 5/10
10/10 - 1s - loss: 147.4942 - loglik: -1.4486e+02 - logprior: -2.6318e+00
Epoch 6/10
10/10 - 1s - loss: 145.3939 - loglik: -1.4322e+02 - logprior: -2.1759e+00
Epoch 7/10
10/10 - 1s - loss: 144.6972 - loglik: -1.4296e+02 - logprior: -1.7365e+00
Epoch 8/10
10/10 - 1s - loss: 143.9657 - loglik: -1.4259e+02 - logprior: -1.3753e+00
Epoch 9/10
10/10 - 1s - loss: 143.9901 - loglik: -1.4278e+02 - logprior: -1.2071e+00
Fitted a model with MAP estimate = -143.7616
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.4150 - loglik: -1.3981e+02 - logprior: -5.4610e+01
Epoch 2/2
10/10 - 1s - loss: 151.2106 - loglik: -1.3465e+02 - logprior: -1.6556e+01
Fitted a model with MAP estimate = -143.7220
expansions: [(3, 1), (18, 1)]
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 181.9046 - loglik: -1.3466e+02 - logprior: -4.7249e+01
Epoch 2/2
10/10 - 1s - loss: 152.6570 - loglik: -1.3440e+02 - logprior: -1.8254e+01
Fitted a model with MAP estimate = -147.4358
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.3470 - loglik: -1.3391e+02 - logprior: -4.2432e+01
Epoch 2/10
10/10 - 1s - loss: 144.5967 - loglik: -1.3315e+02 - logprior: -1.1450e+01
Epoch 3/10
10/10 - 1s - loss: 137.4511 - loglik: -1.3303e+02 - logprior: -4.4235e+00
Epoch 4/10
10/10 - 1s - loss: 134.3806 - loglik: -1.3240e+02 - logprior: -1.9825e+00
Epoch 5/10
10/10 - 1s - loss: 133.0667 - loglik: -1.3213e+02 - logprior: -9.3665e-01
Epoch 6/10
10/10 - 1s - loss: 132.3150 - loglik: -1.3193e+02 - logprior: -3.8640e-01
Epoch 7/10
10/10 - 1s - loss: 132.1335 - loglik: -1.3230e+02 - logprior: 0.1628
Epoch 8/10
10/10 - 1s - loss: 131.7165 - loglik: -1.3230e+02 - logprior: 0.5799
Epoch 9/10
10/10 - 1s - loss: 131.5503 - loglik: -1.3233e+02 - logprior: 0.7793
Epoch 10/10
10/10 - 1s - loss: 131.4367 - loglik: -1.3235e+02 - logprior: 0.9140
Fitted a model with MAP estimate = -131.3323
Time for alignment: 28.1855
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.7115 - loglik: -1.8505e+02 - logprior: -4.1665e+01
Epoch 2/10
10/10 - 1s - loss: 180.3357 - loglik: -1.6915e+02 - logprior: -1.1187e+01
Epoch 3/10
10/10 - 1s - loss: 161.5057 - loglik: -1.5602e+02 - logprior: -5.4828e+00
Epoch 4/10
10/10 - 1s - loss: 152.2026 - loglik: -1.4869e+02 - logprior: -3.5119e+00
Epoch 5/10
10/10 - 1s - loss: 148.4276 - loglik: -1.4580e+02 - logprior: -2.6308e+00
Epoch 6/10
10/10 - 1s - loss: 146.8120 - loglik: -1.4462e+02 - logprior: -2.1968e+00
Epoch 7/10
10/10 - 1s - loss: 145.7445 - loglik: -1.4400e+02 - logprior: -1.7396e+00
Epoch 8/10
10/10 - 1s - loss: 145.5831 - loglik: -1.4422e+02 - logprior: -1.3678e+00
Epoch 9/10
10/10 - 1s - loss: 144.9200 - loglik: -1.4371e+02 - logprior: -1.2118e+00
Epoch 10/10
10/10 - 1s - loss: 144.7896 - loglik: -1.4365e+02 - logprior: -1.1378e+00
Fitted a model with MAP estimate = -144.7912
expansions: [(0, 2), (1, 1), (17, 1), (19, 2), (21, 1), (27, 2), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 196.5663 - loglik: -1.4175e+02 - logprior: -5.4816e+01
Epoch 2/2
10/10 - 1s - loss: 153.8340 - loglik: -1.3722e+02 - logprior: -1.6615e+01
Fitted a model with MAP estimate = -146.1753
expansions: [(26, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 183.9293 - loglik: -1.3668e+02 - logprior: -4.7252e+01
Epoch 2/2
10/10 - 1s - loss: 154.5232 - loglik: -1.3622e+02 - logprior: -1.8300e+01
Fitted a model with MAP estimate = -149.2870
expansions: [(18, 1)]
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 178.6748 - loglik: -1.3625e+02 - logprior: -4.2426e+01
Epoch 2/10
10/10 - 1s - loss: 146.6242 - loglik: -1.3514e+02 - logprior: -1.1487e+01
Epoch 3/10
10/10 - 1s - loss: 139.7274 - loglik: -1.3526e+02 - logprior: -4.4672e+00
Epoch 4/10
10/10 - 1s - loss: 136.5506 - loglik: -1.3452e+02 - logprior: -2.0260e+00
Epoch 5/10
10/10 - 1s - loss: 135.0263 - loglik: -1.3407e+02 - logprior: -9.5818e-01
Epoch 6/10
10/10 - 1s - loss: 134.2527 - loglik: -1.3383e+02 - logprior: -4.1967e-01
Epoch 7/10
10/10 - 1s - loss: 133.8383 - loglik: -1.3395e+02 - logprior: 0.1148
Epoch 8/10
10/10 - 1s - loss: 133.5266 - loglik: -1.3406e+02 - logprior: 0.5302
Epoch 9/10
10/10 - 1s - loss: 133.4394 - loglik: -1.3418e+02 - logprior: 0.7365
Epoch 10/10
10/10 - 1s - loss: 133.3139 - loglik: -1.3418e+02 - logprior: 0.8692
Fitted a model with MAP estimate = -133.1779
Time for alignment: 27.9888
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.8003 - loglik: -1.8514e+02 - logprior: -4.1663e+01
Epoch 2/10
10/10 - 1s - loss: 180.1418 - loglik: -1.6894e+02 - logprior: -1.1200e+01
Epoch 3/10
10/10 - 1s - loss: 160.6716 - loglik: -1.5512e+02 - logprior: -5.5505e+00
Epoch 4/10
10/10 - 1s - loss: 151.0904 - loglik: -1.4751e+02 - logprior: -3.5761e+00
Epoch 5/10
10/10 - 1s - loss: 147.0894 - loglik: -1.4442e+02 - logprior: -2.6727e+00
Epoch 6/10
10/10 - 1s - loss: 145.4879 - loglik: -1.4330e+02 - logprior: -2.1889e+00
Epoch 7/10
10/10 - 1s - loss: 144.9205 - loglik: -1.4319e+02 - logprior: -1.7301e+00
Epoch 8/10
10/10 - 1s - loss: 144.3072 - loglik: -1.4293e+02 - logprior: -1.3727e+00
Epoch 9/10
10/10 - 1s - loss: 144.0240 - loglik: -1.4280e+02 - logprior: -1.2204e+00
Epoch 10/10
10/10 - 1s - loss: 143.7359 - loglik: -1.4259e+02 - logprior: -1.1430e+00
Fitted a model with MAP estimate = -143.5822
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 194.1933 - loglik: -1.3944e+02 - logprior: -5.4751e+01
Epoch 2/2
10/10 - 1s - loss: 150.9468 - loglik: -1.3430e+02 - logprior: -1.6644e+01
Fitted a model with MAP estimate = -143.3868
expansions: [(19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.4426 - loglik: -1.3422e+02 - logprior: -4.7218e+01
Epoch 2/2
10/10 - 1s - loss: 152.3725 - loglik: -1.3392e+02 - logprior: -1.8457e+01
Fitted a model with MAP estimate = -147.3744
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.6405 - loglik: -1.3416e+02 - logprior: -4.2481e+01
Epoch 2/10
10/10 - 1s - loss: 144.5396 - loglik: -1.3306e+02 - logprior: -1.1480e+01
Epoch 3/10
10/10 - 1s - loss: 137.5129 - loglik: -1.3307e+02 - logprior: -4.4403e+00
Epoch 4/10
10/10 - 1s - loss: 134.6736 - loglik: -1.3267e+02 - logprior: -2.0046e+00
Epoch 5/10
10/10 - 1s - loss: 133.1585 - loglik: -1.3219e+02 - logprior: -9.7263e-01
Epoch 6/10
10/10 - 1s - loss: 132.4651 - loglik: -1.3204e+02 - logprior: -4.2067e-01
Epoch 7/10
10/10 - 1s - loss: 132.2468 - loglik: -1.3238e+02 - logprior: 0.1315
Epoch 8/10
10/10 - 1s - loss: 131.8549 - loglik: -1.3239e+02 - logprior: 0.5368
Epoch 9/10
10/10 - 1s - loss: 131.7434 - loglik: -1.3248e+02 - logprior: 0.7343
Epoch 10/10
10/10 - 1s - loss: 131.7650 - loglik: -1.3264e+02 - logprior: 0.8708
Fitted a model with MAP estimate = -131.4826
Time for alignment: 28.5047
Computed alignments with likelihoods: ['-131.4492', '-131.4904', '-131.3323', '-133.1779', '-131.4826']
Best model has likelihood: -131.3323  (prior= 0.9981 )
time for generating output: 0.1121
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.8957332579824809
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.9025 - loglik: -2.3771e+02 - logprior: -1.3193e+01
Epoch 2/10
11/11 - 1s - loss: 220.7787 - loglik: -2.1735e+02 - logprior: -3.4246e+00
Epoch 3/10
11/11 - 1s - loss: 200.7085 - loglik: -1.9857e+02 - logprior: -2.1427e+00
Epoch 4/10
11/11 - 1s - loss: 189.2122 - loglik: -1.8716e+02 - logprior: -2.0508e+00
Epoch 5/10
11/11 - 1s - loss: 186.6863 - loglik: -1.8468e+02 - logprior: -2.0082e+00
Epoch 6/10
11/11 - 1s - loss: 186.4798 - loglik: -1.8463e+02 - logprior: -1.8451e+00
Epoch 7/10
11/11 - 1s - loss: 184.8852 - loglik: -1.8320e+02 - logprior: -1.6859e+00
Epoch 8/10
11/11 - 1s - loss: 184.7813 - loglik: -1.8314e+02 - logprior: -1.6417e+00
Epoch 9/10
11/11 - 1s - loss: 184.2195 - loglik: -1.8255e+02 - logprior: -1.6656e+00
Epoch 10/10
11/11 - 1s - loss: 184.3023 - loglik: -1.8262e+02 - logprior: -1.6776e+00
Fitted a model with MAP estimate = -184.2893
expansions: [(8, 1), (10, 3), (11, 1), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.0563 - loglik: -1.8406e+02 - logprior: -1.5000e+01
Epoch 2/2
11/11 - 1s - loss: 185.5468 - loglik: -1.7908e+02 - logprior: -6.4673e+00
Fitted a model with MAP estimate = -182.0025
expansions: [(0, 2)]
discards: [ 0 14 16 44]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.2729 - loglik: -1.7648e+02 - logprior: -1.1795e+01
Epoch 2/2
11/11 - 1s - loss: 178.5923 - loglik: -1.7552e+02 - logprior: -3.0676e+00
Fitted a model with MAP estimate = -176.9387
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 190.1732 - loglik: -1.7604e+02 - logprior: -1.4133e+01
Epoch 2/10
11/11 - 1s - loss: 180.8385 - loglik: -1.7662e+02 - logprior: -4.2166e+00
Epoch 3/10
11/11 - 1s - loss: 177.5231 - loglik: -1.7544e+02 - logprior: -2.0822e+00
Epoch 4/10
11/11 - 1s - loss: 176.0430 - loglik: -1.7478e+02 - logprior: -1.2606e+00
Epoch 5/10
11/11 - 1s - loss: 175.6227 - loglik: -1.7460e+02 - logprior: -1.0193e+00
Epoch 6/10
11/11 - 1s - loss: 174.6601 - loglik: -1.7384e+02 - logprior: -8.2377e-01
Epoch 7/10
11/11 - 1s - loss: 173.9723 - loglik: -1.7322e+02 - logprior: -7.4943e-01
Epoch 8/10
11/11 - 1s - loss: 175.1138 - loglik: -1.7441e+02 - logprior: -7.0557e-01
Fitted a model with MAP estimate = -174.3005
Time for alignment: 45.4509
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.6106 - loglik: -2.3742e+02 - logprior: -1.3190e+01
Epoch 2/10
11/11 - 1s - loss: 220.9193 - loglik: -2.1749e+02 - logprior: -3.4262e+00
Epoch 3/10
11/11 - 1s - loss: 200.9230 - loglik: -1.9878e+02 - logprior: -2.1420e+00
Epoch 4/10
11/11 - 1s - loss: 190.9369 - loglik: -1.8890e+02 - logprior: -2.0338e+00
Epoch 5/10
11/11 - 1s - loss: 187.3925 - loglik: -1.8538e+02 - logprior: -2.0101e+00
Epoch 6/10
11/11 - 1s - loss: 187.2530 - loglik: -1.8540e+02 - logprior: -1.8556e+00
Epoch 7/10
11/11 - 1s - loss: 185.5639 - loglik: -1.8385e+02 - logprior: -1.7145e+00
Epoch 8/10
11/11 - 1s - loss: 185.9010 - loglik: -1.8422e+02 - logprior: -1.6792e+00
Fitted a model with MAP estimate = -185.2079
expansions: [(8, 1), (10, 2), (11, 2), (15, 1), (24, 1), (25, 1), (32, 1), (34, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.9670 - loglik: -1.8495e+02 - logprior: -1.5020e+01
Epoch 2/2
11/11 - 1s - loss: 185.3233 - loglik: -1.7894e+02 - logprior: -6.3813e+00
Fitted a model with MAP estimate = -182.5710
expansions: [(0, 2), (38, 1)]
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.0183 - loglik: -1.7623e+02 - logprior: -1.1792e+01
Epoch 2/2
11/11 - 1s - loss: 178.5352 - loglik: -1.7545e+02 - logprior: -3.0825e+00
Fitted a model with MAP estimate = -177.0450
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 190.8922 - loglik: -1.7671e+02 - logprior: -1.4182e+01
Epoch 2/10
11/11 - 1s - loss: 180.7698 - loglik: -1.7649e+02 - logprior: -4.2774e+00
Epoch 3/10
11/11 - 1s - loss: 176.8748 - loglik: -1.7477e+02 - logprior: -2.1022e+00
Epoch 4/10
11/11 - 1s - loss: 176.1297 - loglik: -1.7487e+02 - logprior: -1.2646e+00
Epoch 5/10
11/11 - 1s - loss: 174.9837 - loglik: -1.7397e+02 - logprior: -1.0155e+00
Epoch 6/10
11/11 - 1s - loss: 174.8525 - loglik: -1.7402e+02 - logprior: -8.3009e-01
Epoch 7/10
11/11 - 1s - loss: 174.2370 - loglik: -1.7348e+02 - logprior: -7.6040e-01
Epoch 8/10
11/11 - 1s - loss: 175.1626 - loglik: -1.7446e+02 - logprior: -7.0204e-01
Fitted a model with MAP estimate = -174.2789
Time for alignment: 41.7026
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.4392 - loglik: -2.3725e+02 - logprior: -1.3193e+01
Epoch 2/10
11/11 - 1s - loss: 221.1902 - loglik: -2.1776e+02 - logprior: -3.4292e+00
Epoch 3/10
11/11 - 1s - loss: 200.0126 - loglik: -1.9788e+02 - logprior: -2.1283e+00
Epoch 4/10
11/11 - 1s - loss: 190.1690 - loglik: -1.8816e+02 - logprior: -2.0042e+00
Epoch 5/10
11/11 - 1s - loss: 186.7820 - loglik: -1.8482e+02 - logprior: -1.9641e+00
Epoch 6/10
11/11 - 1s - loss: 185.8550 - loglik: -1.8404e+02 - logprior: -1.8116e+00
Epoch 7/10
11/11 - 1s - loss: 184.7360 - loglik: -1.8306e+02 - logprior: -1.6790e+00
Epoch 8/10
11/11 - 1s - loss: 184.4980 - loglik: -1.8285e+02 - logprior: -1.6442e+00
Epoch 9/10
11/11 - 1s - loss: 184.3855 - loglik: -1.8273e+02 - logprior: -1.6572e+00
Epoch 10/10
11/11 - 1s - loss: 184.5896 - loglik: -1.8293e+02 - logprior: -1.6557e+00
Fitted a model with MAP estimate = -184.2542
expansions: [(8, 1), (10, 3), (11, 1), (17, 1), (24, 1), (32, 1), (33, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.5036 - loglik: -1.8353e+02 - logprior: -1.4973e+01
Epoch 2/2
11/11 - 1s - loss: 184.1270 - loglik: -1.7774e+02 - logprior: -6.3902e+00
Fitted a model with MAP estimate = -181.7846
expansions: [(0, 2)]
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 187.3775 - loglik: -1.7558e+02 - logprior: -1.1794e+01
Epoch 2/2
11/11 - 1s - loss: 178.6273 - loglik: -1.7554e+02 - logprior: -3.0871e+00
Fitted a model with MAP estimate = -176.9243
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.1421 - loglik: -1.7700e+02 - logprior: -1.4145e+01
Epoch 2/10
11/11 - 1s - loss: 180.0551 - loglik: -1.7583e+02 - logprior: -4.2233e+00
Epoch 3/10
11/11 - 1s - loss: 176.8027 - loglik: -1.7472e+02 - logprior: -2.0789e+00
Epoch 4/10
11/11 - 1s - loss: 176.2121 - loglik: -1.7495e+02 - logprior: -1.2658e+00
Epoch 5/10
11/11 - 1s - loss: 174.9554 - loglik: -1.7394e+02 - logprior: -1.0145e+00
Epoch 6/10
11/11 - 1s - loss: 174.2487 - loglik: -1.7342e+02 - logprior: -8.3187e-01
Epoch 7/10
11/11 - 1s - loss: 175.2105 - loglik: -1.7446e+02 - logprior: -7.5441e-01
Fitted a model with MAP estimate = -174.4031
Time for alignment: 42.8365
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.8526 - loglik: -2.3766e+02 - logprior: -1.3195e+01
Epoch 2/10
11/11 - 1s - loss: 220.8883 - loglik: -2.1746e+02 - logprior: -3.4243e+00
Epoch 3/10
11/11 - 1s - loss: 198.8756 - loglik: -1.9674e+02 - logprior: -2.1400e+00
Epoch 4/10
11/11 - 1s - loss: 190.3606 - loglik: -1.8834e+02 - logprior: -2.0189e+00
Epoch 5/10
11/11 - 1s - loss: 187.1461 - loglik: -1.8520e+02 - logprior: -1.9438e+00
Epoch 6/10
11/11 - 1s - loss: 186.0768 - loglik: -1.8430e+02 - logprior: -1.7800e+00
Epoch 7/10
11/11 - 1s - loss: 186.3179 - loglik: -1.8467e+02 - logprior: -1.6513e+00
Fitted a model with MAP estimate = -184.9902
expansions: [(9, 2), (11, 3), (12, 2), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.5436 - loglik: -1.8358e+02 - logprior: -1.4968e+01
Epoch 2/2
11/11 - 1s - loss: 184.3070 - loglik: -1.7795e+02 - logprior: -6.3584e+00
Fitted a model with MAP estimate = -181.5621
expansions: [(0, 2)]
discards: [ 0 16]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.3536 - loglik: -1.7654e+02 - logprior: -1.1814e+01
Epoch 2/2
11/11 - 1s - loss: 177.8787 - loglik: -1.7478e+02 - logprior: -3.1018e+00
Fitted a model with MAP estimate = -177.0894
expansions: []
discards: [ 0 10]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 190.6912 - loglik: -1.7651e+02 - logprior: -1.4185e+01
Epoch 2/10
11/11 - 1s - loss: 180.4551 - loglik: -1.7621e+02 - logprior: -4.2474e+00
Epoch 3/10
11/11 - 1s - loss: 177.2888 - loglik: -1.7519e+02 - logprior: -2.0967e+00
Epoch 4/10
11/11 - 1s - loss: 176.1062 - loglik: -1.7483e+02 - logprior: -1.2797e+00
Epoch 5/10
11/11 - 1s - loss: 175.9651 - loglik: -1.7494e+02 - logprior: -1.0204e+00
Epoch 6/10
11/11 - 1s - loss: 174.5126 - loglik: -1.7367e+02 - logprior: -8.4076e-01
Epoch 7/10
11/11 - 1s - loss: 174.8124 - loglik: -1.7405e+02 - logprior: -7.5931e-01
Fitted a model with MAP estimate = -174.4376
Time for alignment: 39.6490
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.6592 - loglik: -2.3747e+02 - logprior: -1.3191e+01
Epoch 2/10
11/11 - 1s - loss: 220.3305 - loglik: -2.1692e+02 - logprior: -3.4102e+00
Epoch 3/10
11/11 - 1s - loss: 200.4357 - loglik: -1.9831e+02 - logprior: -2.1268e+00
Epoch 4/10
11/11 - 1s - loss: 189.4325 - loglik: -1.8737e+02 - logprior: -2.0595e+00
Epoch 5/10
11/11 - 1s - loss: 186.9521 - loglik: -1.8491e+02 - logprior: -2.0381e+00
Epoch 6/10
11/11 - 1s - loss: 186.4717 - loglik: -1.8460e+02 - logprior: -1.8754e+00
Epoch 7/10
11/11 - 1s - loss: 184.8547 - loglik: -1.8314e+02 - logprior: -1.7110e+00
Epoch 8/10
11/11 - 1s - loss: 184.8348 - loglik: -1.8316e+02 - logprior: -1.6739e+00
Epoch 9/10
11/11 - 1s - loss: 184.4724 - loglik: -1.8277e+02 - logprior: -1.7024e+00
Epoch 10/10
11/11 - 1s - loss: 185.3568 - loglik: -1.8364e+02 - logprior: -1.7148e+00
Fitted a model with MAP estimate = -184.6506
expansions: [(8, 3), (9, 1), (10, 1), (12, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.6281 - loglik: -1.8366e+02 - logprior: -1.4968e+01
Epoch 2/2
11/11 - 1s - loss: 184.0952 - loglik: -1.7771e+02 - logprior: -6.3809e+00
Fitted a model with MAP estimate = -181.8678
expansions: [(0, 2)]
discards: [0 8]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.1517 - loglik: -1.7639e+02 - logprior: -1.1765e+01
Epoch 2/2
11/11 - 1s - loss: 178.2032 - loglik: -1.7513e+02 - logprior: -3.0735e+00
Fitted a model with MAP estimate = -177.2900
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.6055 - loglik: -1.7744e+02 - logprior: -1.4164e+01
Epoch 2/10
11/11 - 1s - loss: 179.8547 - loglik: -1.7562e+02 - logprior: -4.2337e+00
Epoch 3/10
11/11 - 1s - loss: 177.7775 - loglik: -1.7571e+02 - logprior: -2.0707e+00
Epoch 4/10
11/11 - 1s - loss: 175.5517 - loglik: -1.7430e+02 - logprior: -1.2560e+00
Epoch 5/10
11/11 - 1s - loss: 174.9610 - loglik: -1.7394e+02 - logprior: -1.0191e+00
Epoch 6/10
11/11 - 1s - loss: 175.9465 - loglik: -1.7513e+02 - logprior: -8.1811e-01
Fitted a model with MAP estimate = -174.6746
Time for alignment: 41.6720
Computed alignments with likelihoods: ['-174.3005', '-174.2789', '-174.4031', '-174.4376', '-174.6746']
Best model has likelihood: -174.2789  (prior= -0.6766 )
time for generating output: 0.1262
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.829308909242298
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.2939 - loglik: -2.2365e+02 - logprior: -4.0647e+01
Epoch 2/10
10/10 - 1s - loss: 208.3404 - loglik: -1.9757e+02 - logprior: -1.0770e+01
Epoch 3/10
10/10 - 1s - loss: 176.7319 - loglik: -1.7128e+02 - logprior: -5.4517e+00
Epoch 4/10
10/10 - 1s - loss: 155.3433 - loglik: -1.5160e+02 - logprior: -3.7443e+00
Epoch 5/10
10/10 - 1s - loss: 147.9667 - loglik: -1.4497e+02 - logprior: -2.9926e+00
Epoch 6/10
10/10 - 1s - loss: 145.2033 - loglik: -1.4267e+02 - logprior: -2.5364e+00
Epoch 7/10
10/10 - 1s - loss: 144.3522 - loglik: -1.4215e+02 - logprior: -2.2026e+00
Epoch 8/10
10/10 - 1s - loss: 143.2516 - loglik: -1.4123e+02 - logprior: -2.0186e+00
Epoch 9/10
10/10 - 1s - loss: 142.9638 - loglik: -1.4106e+02 - logprior: -1.9064e+00
Epoch 10/10
10/10 - 1s - loss: 142.5773 - loglik: -1.4072e+02 - logprior: -1.8616e+00
Fitted a model with MAP estimate = -142.4374
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 170.4550 - loglik: -1.3368e+02 - logprior: -3.6773e+01
Epoch 2/2
10/10 - 1s - loss: 136.8458 - loglik: -1.2729e+02 - logprior: -9.5578e+00
Fitted a model with MAP estimate = -132.2228
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.7349 - loglik: -1.3153e+02 - logprior: -4.5209e+01
Epoch 2/2
10/10 - 1s - loss: 148.9381 - loglik: -1.3047e+02 - logprior: -1.8469e+01
Fitted a model with MAP estimate = -144.9213
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.5621 - loglik: -1.2825e+02 - logprior: -3.6309e+01
Epoch 2/10
10/10 - 1s - loss: 135.9340 - loglik: -1.2692e+02 - logprior: -9.0148e+00
Epoch 3/10
10/10 - 1s - loss: 129.9791 - loglik: -1.2651e+02 - logprior: -3.4688e+00
Epoch 4/10
10/10 - 1s - loss: 127.7107 - loglik: -1.2644e+02 - logprior: -1.2680e+00
Epoch 5/10
10/10 - 1s - loss: 127.1655 - loglik: -1.2704e+02 - logprior: -1.2121e-01
Epoch 6/10
10/10 - 1s - loss: 126.2478 - loglik: -1.2674e+02 - logprior: 0.4899
Epoch 7/10
10/10 - 1s - loss: 126.0950 - loglik: -1.2693e+02 - logprior: 0.8357
Epoch 8/10
10/10 - 1s - loss: 125.8075 - loglik: -1.2689e+02 - logprior: 1.0779
Epoch 9/10
10/10 - 1s - loss: 125.7462 - loglik: -1.2703e+02 - logprior: 1.2798
Epoch 10/10
10/10 - 1s - loss: 125.6987 - loglik: -1.2715e+02 - logprior: 1.4502
Fitted a model with MAP estimate = -125.5110
Time for alignment: 31.8259
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.4137 - loglik: -2.2377e+02 - logprior: -4.0645e+01
Epoch 2/10
10/10 - 1s - loss: 208.5849 - loglik: -1.9782e+02 - logprior: -1.0770e+01
Epoch 3/10
10/10 - 1s - loss: 178.6449 - loglik: -1.7312e+02 - logprior: -5.5213e+00
Epoch 4/10
10/10 - 1s - loss: 156.5881 - loglik: -1.5259e+02 - logprior: -3.9950e+00
Epoch 5/10
10/10 - 1s - loss: 146.8651 - loglik: -1.4345e+02 - logprior: -3.4169e+00
Epoch 6/10
10/10 - 1s - loss: 143.9267 - loglik: -1.4097e+02 - logprior: -2.9605e+00
Epoch 7/10
10/10 - 1s - loss: 142.6293 - loglik: -1.4006e+02 - logprior: -2.5706e+00
Epoch 8/10
10/10 - 1s - loss: 142.0221 - loglik: -1.3968e+02 - logprior: -2.3406e+00
Epoch 9/10
10/10 - 1s - loss: 141.3917 - loglik: -1.3918e+02 - logprior: -2.2086e+00
Epoch 10/10
10/10 - 1s - loss: 141.4821 - loglik: -1.3936e+02 - logprior: -2.1227e+00
Fitted a model with MAP estimate = -141.2655
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.3525 - loglik: -1.3451e+02 - logprior: -3.6843e+01
Epoch 2/2
10/10 - 1s - loss: 138.3385 - loglik: -1.2860e+02 - logprior: -9.7415e+00
Fitted a model with MAP estimate = -133.1972
expansions: []
discards: [45 58 65]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 162.9874 - loglik: -1.2709e+02 - logprior: -3.5897e+01
Epoch 2/2
10/10 - 1s - loss: 135.8495 - loglik: -1.2657e+02 - logprior: -9.2756e+00
Fitted a model with MAP estimate = -131.8917
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.8581 - loglik: -1.3091e+02 - logprior: -4.4946e+01
Epoch 2/10
10/10 - 1s - loss: 148.3476 - loglik: -1.3005e+02 - logprior: -1.8297e+01
Epoch 3/10
10/10 - 1s - loss: 142.0263 - loglik: -1.2920e+02 - logprior: -1.2828e+01
Epoch 4/10
10/10 - 1s - loss: 139.3590 - loglik: -1.2889e+02 - logprior: -1.0468e+01
Epoch 5/10
10/10 - 1s - loss: 136.9526 - loglik: -1.2924e+02 - logprior: -7.7145e+00
Epoch 6/10
10/10 - 1s - loss: 131.0648 - loglik: -1.2926e+02 - logprior: -1.8042e+00
Epoch 7/10
10/10 - 1s - loss: 128.9417 - loglik: -1.2949e+02 - logprior: 0.5488
Epoch 8/10
10/10 - 1s - loss: 128.5386 - loglik: -1.2947e+02 - logprior: 0.9338
Epoch 9/10
10/10 - 1s - loss: 128.7080 - loglik: -1.2983e+02 - logprior: 1.1246
Fitted a model with MAP estimate = -128.4051
Time for alignment: 30.2050
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.3426 - loglik: -2.2370e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 208.2966 - loglik: -1.9753e+02 - logprior: -1.0763e+01
Epoch 3/10
10/10 - 1s - loss: 177.3233 - loglik: -1.7190e+02 - logprior: -5.4264e+00
Epoch 4/10
10/10 - 1s - loss: 156.2900 - loglik: -1.5255e+02 - logprior: -3.7355e+00
Epoch 5/10
10/10 - 1s - loss: 147.7695 - loglik: -1.4474e+02 - logprior: -3.0327e+00
Epoch 6/10
10/10 - 1s - loss: 145.4501 - loglik: -1.4288e+02 - logprior: -2.5750e+00
Epoch 7/10
10/10 - 1s - loss: 144.2244 - loglik: -1.4200e+02 - logprior: -2.2266e+00
Epoch 8/10
10/10 - 1s - loss: 143.7240 - loglik: -1.4171e+02 - logprior: -2.0188e+00
Epoch 9/10
10/10 - 1s - loss: 143.2010 - loglik: -1.4130e+02 - logprior: -1.8999e+00
Epoch 10/10
10/10 - 1s - loss: 142.8460 - loglik: -1.4102e+02 - logprior: -1.8242e+00
Fitted a model with MAP estimate = -142.7025
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 170.3934 - loglik: -1.3363e+02 - logprior: -3.6763e+01
Epoch 2/2
10/10 - 1s - loss: 136.8402 - loglik: -1.2729e+02 - logprior: -9.5499e+00
Fitted a model with MAP estimate = -132.2011
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.9403 - loglik: -1.3174e+02 - logprior: -4.5200e+01
Epoch 2/2
10/10 - 1s - loss: 148.8873 - loglik: -1.3044e+02 - logprior: -1.8451e+01
Fitted a model with MAP estimate = -144.9135
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.5640 - loglik: -1.2827e+02 - logprior: -3.6296e+01
Epoch 2/10
10/10 - 1s - loss: 135.9967 - loglik: -1.2699e+02 - logprior: -9.0056e+00
Epoch 3/10
10/10 - 1s - loss: 129.9587 - loglik: -1.2649e+02 - logprior: -3.4702e+00
Epoch 4/10
10/10 - 1s - loss: 127.8220 - loglik: -1.2656e+02 - logprior: -1.2573e+00
Epoch 5/10
10/10 - 1s - loss: 126.9219 - loglik: -1.2681e+02 - logprior: -1.1303e-01
Epoch 6/10
10/10 - 1s - loss: 126.3434 - loglik: -1.2684e+02 - logprior: 0.5003
Epoch 7/10
10/10 - 1s - loss: 126.3134 - loglik: -1.2716e+02 - logprior: 0.8497
Epoch 8/10
10/10 - 1s - loss: 125.7815 - loglik: -1.2688e+02 - logprior: 1.0936
Epoch 9/10
10/10 - 1s - loss: 125.7989 - loglik: -1.2709e+02 - logprior: 1.2921
Fitted a model with MAP estimate = -125.6307
Time for alignment: 30.0598
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.4385 - loglik: -2.2379e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 208.3300 - loglik: -1.9756e+02 - logprior: -1.0769e+01
Epoch 3/10
10/10 - 1s - loss: 177.5221 - loglik: -1.7205e+02 - logprior: -5.4720e+00
Epoch 4/10
10/10 - 1s - loss: 156.0412 - loglik: -1.5223e+02 - logprior: -3.8089e+00
Epoch 5/10
10/10 - 1s - loss: 148.0298 - loglik: -1.4498e+02 - logprior: -3.0507e+00
Epoch 6/10
10/10 - 1s - loss: 145.3647 - loglik: -1.4277e+02 - logprior: -2.5909e+00
Epoch 7/10
10/10 - 1s - loss: 144.0900 - loglik: -1.4181e+02 - logprior: -2.2783e+00
Epoch 8/10
10/10 - 1s - loss: 143.2014 - loglik: -1.4114e+02 - logprior: -2.0642e+00
Epoch 9/10
10/10 - 1s - loss: 142.7379 - loglik: -1.4078e+02 - logprior: -1.9530e+00
Epoch 10/10
10/10 - 1s - loss: 142.5235 - loglik: -1.4063e+02 - logprior: -1.8921e+00
Fitted a model with MAP estimate = -142.3732
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 170.2868 - loglik: -1.3352e+02 - logprior: -3.6767e+01
Epoch 2/2
10/10 - 1s - loss: 136.6162 - loglik: -1.2706e+02 - logprior: -9.5552e+00
Fitted a model with MAP estimate = -132.1873
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.7949 - loglik: -1.3159e+02 - logprior: -4.5210e+01
Epoch 2/2
10/10 - 1s - loss: 149.1164 - loglik: -1.3066e+02 - logprior: -1.8457e+01
Fitted a model with MAP estimate = -144.9222
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.7304 - loglik: -1.2838e+02 - logprior: -3.6351e+01
Epoch 2/10
10/10 - 1s - loss: 135.7979 - loglik: -1.2678e+02 - logprior: -9.0158e+00
Epoch 3/10
10/10 - 1s - loss: 130.0895 - loglik: -1.2661e+02 - logprior: -3.4773e+00
Epoch 4/10
10/10 - 1s - loss: 127.8387 - loglik: -1.2658e+02 - logprior: -1.2619e+00
Epoch 5/10
10/10 - 1s - loss: 126.9115 - loglik: -1.2680e+02 - logprior: -1.1438e-01
Epoch 6/10
10/10 - 1s - loss: 126.3482 - loglik: -1.2685e+02 - logprior: 0.4972
Epoch 7/10
10/10 - 1s - loss: 125.9832 - loglik: -1.2683e+02 - logprior: 0.8482
Epoch 8/10
10/10 - 1s - loss: 125.9156 - loglik: -1.2700e+02 - logprior: 1.0865
Epoch 9/10
10/10 - 1s - loss: 126.0782 - loglik: -1.2737e+02 - logprior: 1.2918
Fitted a model with MAP estimate = -125.6442
Time for alignment: 29.8331
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.3376 - loglik: -2.2369e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 208.4989 - loglik: -1.9773e+02 - logprior: -1.0767e+01
Epoch 3/10
10/10 - 1s - loss: 177.7389 - loglik: -1.7226e+02 - logprior: -5.4767e+00
Epoch 4/10
10/10 - 1s - loss: 156.4049 - loglik: -1.5261e+02 - logprior: -3.7968e+00
Epoch 5/10
10/10 - 1s - loss: 148.1609 - loglik: -1.4507e+02 - logprior: -3.0863e+00
Epoch 6/10
10/10 - 1s - loss: 145.1837 - loglik: -1.4257e+02 - logprior: -2.6099e+00
Epoch 7/10
10/10 - 1s - loss: 144.0835 - loglik: -1.4182e+02 - logprior: -2.2672e+00
Epoch 8/10
10/10 - 1s - loss: 143.2412 - loglik: -1.4118e+02 - logprior: -2.0602e+00
Epoch 9/10
10/10 - 1s - loss: 142.5694 - loglik: -1.4061e+02 - logprior: -1.9620e+00
Epoch 10/10
10/10 - 1s - loss: 142.5580 - loglik: -1.4067e+02 - logprior: -1.8919e+00
Fitted a model with MAP estimate = -142.3698
expansions: [(2, 1), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 171.5223 - loglik: -1.3470e+02 - logprior: -3.6825e+01
Epoch 2/2
10/10 - 1s - loss: 137.9639 - loglik: -1.2822e+02 - logprior: -9.7417e+00
Fitted a model with MAP estimate = -133.2358
expansions: []
discards: [45 58 65]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.0070 - loglik: -1.2711e+02 - logprior: -3.5896e+01
Epoch 2/2
10/10 - 1s - loss: 135.7005 - loglik: -1.2642e+02 - logprior: -9.2791e+00
Fitted a model with MAP estimate = -131.9019
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9871 - loglik: -1.3105e+02 - logprior: -4.4933e+01
Epoch 2/10
10/10 - 1s - loss: 148.0999 - loglik: -1.2981e+02 - logprior: -1.8293e+01
Epoch 3/10
10/10 - 1s - loss: 142.1253 - loglik: -1.2931e+02 - logprior: -1.2813e+01
Epoch 4/10
10/10 - 1s - loss: 139.3517 - loglik: -1.2892e+02 - logprior: -1.0432e+01
Epoch 5/10
10/10 - 1s - loss: 136.6969 - loglik: -1.2924e+02 - logprior: -7.4574e+00
Epoch 6/10
10/10 - 1s - loss: 130.6242 - loglik: -1.2910e+02 - logprior: -1.5236e+00
Epoch 7/10
10/10 - 1s - loss: 128.8576 - loglik: -1.2944e+02 - logprior: 0.5787
Epoch 8/10
10/10 - 1s - loss: 128.9378 - loglik: -1.2988e+02 - logprior: 0.9457
Fitted a model with MAP estimate = -128.5508
Time for alignment: 29.6369
Computed alignments with likelihoods: ['-125.5110', '-128.4051', '-125.6307', '-125.6442', '-128.5508']
Best model has likelihood: -125.5110  (prior= 1.5336 )
time for generating output: 0.1157
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.8564831261101243
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.4852 - loglik: -1.6687e+02 - logprior: -5.6201e+00
Epoch 2/10
15/15 - 1s - loss: 144.7752 - loglik: -1.4300e+02 - logprior: -1.7712e+00
Epoch 3/10
15/15 - 1s - loss: 130.3350 - loglik: -1.2855e+02 - logprior: -1.7894e+00
Epoch 4/10
15/15 - 1s - loss: 127.3907 - loglik: -1.2566e+02 - logprior: -1.7341e+00
Epoch 5/10
15/15 - 1s - loss: 126.5070 - loglik: -1.2487e+02 - logprior: -1.6339e+00
Epoch 6/10
15/15 - 1s - loss: 126.3263 - loglik: -1.2468e+02 - logprior: -1.6505e+00
Epoch 7/10
15/15 - 1s - loss: 125.9865 - loglik: -1.2436e+02 - logprior: -1.6235e+00
Epoch 8/10
15/15 - 1s - loss: 126.0425 - loglik: -1.2444e+02 - logprior: -1.6019e+00
Fitted a model with MAP estimate = -125.9633
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (28, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 136.6403 - loglik: -1.2967e+02 - logprior: -6.9706e+00
Epoch 2/2
15/15 - 1s - loss: 127.2095 - loglik: -1.2382e+02 - logprior: -3.3893e+00
Fitted a model with MAP estimate = -124.9452
expansions: []
discards: [12 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.1021 - loglik: -1.2206e+02 - logprior: -6.0402e+00
Epoch 2/2
15/15 - 1s - loss: 121.9889 - loglik: -1.2004e+02 - logprior: -1.9520e+00
Fitted a model with MAP estimate = -121.0753
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.8608 - loglik: -1.2045e+02 - logprior: -5.4097e+00
Epoch 2/10
15/15 - 1s - loss: 121.4483 - loglik: -1.1959e+02 - logprior: -1.8618e+00
Epoch 3/10
15/15 - 1s - loss: 121.0194 - loglik: -1.1962e+02 - logprior: -1.3957e+00
Epoch 4/10
15/15 - 1s - loss: 120.6189 - loglik: -1.1940e+02 - logprior: -1.2231e+00
Epoch 5/10
15/15 - 1s - loss: 120.5434 - loglik: -1.1937e+02 - logprior: -1.1753e+00
Epoch 6/10
15/15 - 1s - loss: 120.2783 - loglik: -1.1913e+02 - logprior: -1.1453e+00
Epoch 7/10
15/15 - 1s - loss: 120.4211 - loglik: -1.1930e+02 - logprior: -1.1223e+00
Fitted a model with MAP estimate = -120.2492
Time for alignment: 33.6088
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.3317 - loglik: -1.6671e+02 - logprior: -5.6197e+00
Epoch 2/10
15/15 - 1s - loss: 145.5900 - loglik: -1.4381e+02 - logprior: -1.7752e+00
Epoch 3/10
15/15 - 1s - loss: 131.8484 - loglik: -1.3004e+02 - logprior: -1.8035e+00
Epoch 4/10
15/15 - 1s - loss: 127.6098 - loglik: -1.2587e+02 - logprior: -1.7436e+00
Epoch 5/10
15/15 - 1s - loss: 126.0745 - loglik: -1.2443e+02 - logprior: -1.6446e+00
Epoch 6/10
15/15 - 1s - loss: 125.6982 - loglik: -1.2402e+02 - logprior: -1.6741e+00
Epoch 7/10
15/15 - 1s - loss: 125.6132 - loglik: -1.2396e+02 - logprior: -1.6537e+00
Epoch 8/10
15/15 - 1s - loss: 125.2410 - loglik: -1.2361e+02 - logprior: -1.6296e+00
Epoch 9/10
15/15 - 1s - loss: 125.2936 - loglik: -1.2367e+02 - logprior: -1.6217e+00
Fitted a model with MAP estimate = -125.2402
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (24, 2), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 135.9790 - loglik: -1.2903e+02 - logprior: -6.9451e+00
Epoch 2/2
15/15 - 1s - loss: 126.8348 - loglik: -1.2347e+02 - logprior: -3.3650e+00
Fitted a model with MAP estimate = -124.8086
expansions: [(0, 2)]
discards: [ 0 12 15 31 46]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.4490 - loglik: -1.2233e+02 - logprior: -5.1160e+00
Epoch 2/2
15/15 - 1s - loss: 121.6075 - loglik: -1.1994e+02 - logprior: -1.6665e+00
Fitted a model with MAP estimate = -120.7185
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.1063 - loglik: -1.2253e+02 - logprior: -6.5716e+00
Epoch 2/10
15/15 - 1s - loss: 122.2982 - loglik: -1.2015e+02 - logprior: -2.1506e+00
Epoch 3/10
15/15 - 1s - loss: 120.8459 - loglik: -1.1943e+02 - logprior: -1.4145e+00
Epoch 4/10
15/15 - 1s - loss: 120.4131 - loglik: -1.1917e+02 - logprior: -1.2446e+00
Epoch 5/10
15/15 - 1s - loss: 120.3876 - loglik: -1.1921e+02 - logprior: -1.1816e+00
Epoch 6/10
15/15 - 1s - loss: 120.2494 - loglik: -1.1909e+02 - logprior: -1.1575e+00
Epoch 7/10
15/15 - 1s - loss: 120.1625 - loglik: -1.1903e+02 - logprior: -1.1330e+00
Epoch 8/10
15/15 - 1s - loss: 120.1802 - loglik: -1.1906e+02 - logprior: -1.1235e+00
Fitted a model with MAP estimate = -120.0631
Time for alignment: 34.7663
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.3362 - loglik: -1.6671e+02 - logprior: -5.6222e+00
Epoch 2/10
15/15 - 1s - loss: 144.9731 - loglik: -1.4320e+02 - logprior: -1.7755e+00
Epoch 3/10
15/15 - 1s - loss: 130.9334 - loglik: -1.2913e+02 - logprior: -1.7985e+00
Epoch 4/10
15/15 - 1s - loss: 127.6689 - loglik: -1.2593e+02 - logprior: -1.7344e+00
Epoch 5/10
15/15 - 1s - loss: 126.4588 - loglik: -1.2483e+02 - logprior: -1.6275e+00
Epoch 6/10
15/15 - 1s - loss: 126.2865 - loglik: -1.2464e+02 - logprior: -1.6498e+00
Epoch 7/10
15/15 - 1s - loss: 126.0541 - loglik: -1.2443e+02 - logprior: -1.6233e+00
Epoch 8/10
15/15 - 1s - loss: 125.8452 - loglik: -1.2424e+02 - logprior: -1.6012e+00
Epoch 9/10
15/15 - 1s - loss: 126.0392 - loglik: -1.2445e+02 - logprior: -1.5924e+00
Fitted a model with MAP estimate = -125.8539
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.8023 - loglik: -1.2884e+02 - logprior: -6.9606e+00
Epoch 2/2
15/15 - 1s - loss: 126.7200 - loglik: -1.2336e+02 - logprior: -3.3609e+00
Fitted a model with MAP estimate = -124.8628
expansions: [(0, 2)]
discards: [ 0 12 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 127.6171 - loglik: -1.2250e+02 - logprior: -5.1168e+00
Epoch 2/2
15/15 - 1s - loss: 121.5593 - loglik: -1.1989e+02 - logprior: -1.6672e+00
Fitted a model with MAP estimate = -120.7158
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.0802 - loglik: -1.2249e+02 - logprior: -6.5903e+00
Epoch 2/10
15/15 - 1s - loss: 122.3485 - loglik: -1.2021e+02 - logprior: -2.1423e+00
Epoch 3/10
15/15 - 1s - loss: 120.5171 - loglik: -1.1909e+02 - logprior: -1.4241e+00
Epoch 4/10
15/15 - 1s - loss: 120.8320 - loglik: -1.1959e+02 - logprior: -1.2439e+00
Fitted a model with MAP estimate = -120.3279
Time for alignment: 31.7331
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.4732 - loglik: -1.6685e+02 - logprior: -5.6217e+00
Epoch 2/10
15/15 - 1s - loss: 145.2334 - loglik: -1.4346e+02 - logprior: -1.7702e+00
Epoch 3/10
15/15 - 1s - loss: 130.8231 - loglik: -1.2904e+02 - logprior: -1.7787e+00
Epoch 4/10
15/15 - 1s - loss: 127.3653 - loglik: -1.2562e+02 - logprior: -1.7446e+00
Epoch 5/10
15/15 - 1s - loss: 126.5458 - loglik: -1.2490e+02 - logprior: -1.6461e+00
Epoch 6/10
15/15 - 1s - loss: 126.1001 - loglik: -1.2444e+02 - logprior: -1.6574e+00
Epoch 7/10
15/15 - 1s - loss: 126.0408 - loglik: -1.2441e+02 - logprior: -1.6291e+00
Epoch 8/10
15/15 - 1s - loss: 126.0060 - loglik: -1.2440e+02 - logprior: -1.6055e+00
Epoch 9/10
15/15 - 1s - loss: 125.8005 - loglik: -1.2420e+02 - logprior: -1.6022e+00
Epoch 10/10
15/15 - 1s - loss: 125.6897 - loglik: -1.2409e+02 - logprior: -1.6000e+00
Fitted a model with MAP estimate = -125.5678
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.7576 - loglik: -1.2979e+02 - logprior: -6.9720e+00
Epoch 2/2
15/15 - 1s - loss: 127.1411 - loglik: -1.2373e+02 - logprior: -3.4069e+00
Fitted a model with MAP estimate = -125.1526
expansions: [(0, 2)]
discards: [ 0 12 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 127.6024 - loglik: -1.2249e+02 - logprior: -5.1171e+00
Epoch 2/2
15/15 - 1s - loss: 121.6637 - loglik: -1.1999e+02 - logprior: -1.6698e+00
Fitted a model with MAP estimate = -120.7626
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 129.5376 - loglik: -1.2296e+02 - logprior: -6.5797e+00
Epoch 2/10
15/15 - 1s - loss: 122.4005 - loglik: -1.2025e+02 - logprior: -2.1520e+00
Epoch 3/10
15/15 - 1s - loss: 120.7992 - loglik: -1.1939e+02 - logprior: -1.4119e+00
Epoch 4/10
15/15 - 1s - loss: 120.7122 - loglik: -1.1948e+02 - logprior: -1.2327e+00
Epoch 5/10
15/15 - 1s - loss: 120.5496 - loglik: -1.1939e+02 - logprior: -1.1636e+00
Epoch 6/10
15/15 - 1s - loss: 120.4531 - loglik: -1.1931e+02 - logprior: -1.1480e+00
Epoch 7/10
15/15 - 1s - loss: 120.3011 - loglik: -1.1918e+02 - logprior: -1.1220e+00
Epoch 8/10
15/15 - 1s - loss: 120.2467 - loglik: -1.1913e+02 - logprior: -1.1176e+00
Epoch 9/10
15/15 - 1s - loss: 120.3207 - loglik: -1.1924e+02 - logprior: -1.0847e+00
Fitted a model with MAP estimate = -120.1716
Time for alignment: 35.4401
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4155 - loglik: -1.6679e+02 - logprior: -5.6210e+00
Epoch 2/10
15/15 - 1s - loss: 145.0059 - loglik: -1.4323e+02 - logprior: -1.7753e+00
Epoch 3/10
15/15 - 1s - loss: 130.9278 - loglik: -1.2914e+02 - logprior: -1.7879e+00
Epoch 4/10
15/15 - 1s - loss: 127.5975 - loglik: -1.2586e+02 - logprior: -1.7380e+00
Epoch 5/10
15/15 - 1s - loss: 126.2958 - loglik: -1.2464e+02 - logprior: -1.6549e+00
Epoch 6/10
15/15 - 1s - loss: 126.0105 - loglik: -1.2434e+02 - logprior: -1.6750e+00
Epoch 7/10
15/15 - 1s - loss: 125.4866 - loglik: -1.2383e+02 - logprior: -1.6516e+00
Epoch 8/10
15/15 - 1s - loss: 125.7540 - loglik: -1.2412e+02 - logprior: -1.6346e+00
Fitted a model with MAP estimate = -125.5748
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 136.2103 - loglik: -1.2925e+02 - logprior: -6.9593e+00
Epoch 2/2
15/15 - 1s - loss: 126.6990 - loglik: -1.2334e+02 - logprior: -3.3625e+00
Fitted a model with MAP estimate = -124.6558
expansions: []
discards: [12 15 31 46]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.8848 - loglik: -1.2186e+02 - logprior: -6.0226e+00
Epoch 2/2
15/15 - 1s - loss: 121.8798 - loglik: -1.1994e+02 - logprior: -1.9435e+00
Fitted a model with MAP estimate = -121.0408
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 125.4876 - loglik: -1.2006e+02 - logprior: -5.4273e+00
Epoch 2/10
15/15 - 1s - loss: 121.5693 - loglik: -1.1970e+02 - logprior: -1.8657e+00
Epoch 3/10
15/15 - 1s - loss: 120.6196 - loglik: -1.1922e+02 - logprior: -1.4005e+00
Epoch 4/10
15/15 - 1s - loss: 120.6194 - loglik: -1.1938e+02 - logprior: -1.2406e+00
Epoch 5/10
15/15 - 1s - loss: 120.1918 - loglik: -1.1901e+02 - logprior: -1.1829e+00
Epoch 6/10
15/15 - 1s - loss: 120.2530 - loglik: -1.1909e+02 - logprior: -1.1588e+00
Fitted a model with MAP estimate = -120.1154
Time for alignment: 31.9517
Computed alignments with likelihoods: ['-120.2492', '-120.0631', '-120.3279', '-120.1716', '-120.1154']
Best model has likelihood: -120.0631  (prior= -1.1165 )
time for generating output: 0.1174
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9661290322580646
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 197.8967 - loglik: -1.8357e+02 - logprior: -1.4331e+01
Epoch 2/10
10/10 - 1s - loss: 170.3553 - loglik: -1.6626e+02 - logprior: -4.0931e+00
Epoch 3/10
10/10 - 1s - loss: 153.3073 - loglik: -1.5083e+02 - logprior: -2.4804e+00
Epoch 4/10
10/10 - 1s - loss: 141.3763 - loglik: -1.3914e+02 - logprior: -2.2328e+00
Epoch 5/10
10/10 - 2s - loss: 137.1692 - loglik: -1.3494e+02 - logprior: -2.2271e+00
Epoch 6/10
10/10 - 2s - loss: 135.2635 - loglik: -1.3300e+02 - logprior: -2.2638e+00
Epoch 7/10
10/10 - 1s - loss: 134.1549 - loglik: -1.3196e+02 - logprior: -2.1932e+00
Epoch 8/10
10/10 - 1s - loss: 133.8080 - loglik: -1.3175e+02 - logprior: -2.0548e+00
Epoch 9/10
10/10 - 1s - loss: 133.5974 - loglik: -1.3163e+02 - logprior: -1.9696e+00
Epoch 10/10
10/10 - 2s - loss: 133.2520 - loglik: -1.3129e+02 - logprior: -1.9607e+00
Fitted a model with MAP estimate = -133.0520
expansions: [(4, 2), (5, 2), (6, 2), (8, 2), (24, 1), (27, 1), (32, 1), (33, 1), (38, 2), (39, 1), (40, 2), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.0257 - loglik: -1.3198e+02 - logprior: -1.6050e+01
Epoch 2/2
10/10 - 1s - loss: 129.4817 - loglik: -1.2243e+02 - logprior: -7.0536e+00
Fitted a model with MAP estimate = -126.2221
expansions: [(0, 2)]
discards: [ 0  9 13 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.5600 - loglik: -1.1988e+02 - logprior: -1.2683e+01
Epoch 2/2
10/10 - 1s - loss: 122.1282 - loglik: -1.1867e+02 - logprior: -3.4608e+00
Fitted a model with MAP estimate = -120.4117
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.2136 - loglik: -1.2013e+02 - logprior: -1.5084e+01
Epoch 2/10
10/10 - 1s - loss: 124.1362 - loglik: -1.1923e+02 - logprior: -4.9073e+00
Epoch 3/10
10/10 - 2s - loss: 120.7019 - loglik: -1.1818e+02 - logprior: -2.5204e+00
Epoch 4/10
10/10 - 1s - loss: 119.5262 - loglik: -1.1796e+02 - logprior: -1.5684e+00
Epoch 5/10
10/10 - 2s - loss: 118.9557 - loglik: -1.1766e+02 - logprior: -1.2949e+00
Epoch 6/10
10/10 - 2s - loss: 118.2140 - loglik: -1.1716e+02 - logprior: -1.0540e+00
Epoch 7/10
10/10 - 1s - loss: 118.2698 - loglik: -1.1729e+02 - logprior: -9.8126e-01
Fitted a model with MAP estimate = -118.1050
Time for alignment: 50.2648
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.8387 - loglik: -1.8351e+02 - logprior: -1.4332e+01
Epoch 2/10
10/10 - 2s - loss: 170.8009 - loglik: -1.6670e+02 - logprior: -4.1044e+00
Epoch 3/10
10/10 - 1s - loss: 153.6131 - loglik: -1.5109e+02 - logprior: -2.5277e+00
Epoch 4/10
10/10 - 2s - loss: 142.0427 - loglik: -1.3974e+02 - logprior: -2.3013e+00
Epoch 5/10
10/10 - 1s - loss: 136.2273 - loglik: -1.3389e+02 - logprior: -2.3341e+00
Epoch 6/10
10/10 - 1s - loss: 133.9635 - loglik: -1.3153e+02 - logprior: -2.4286e+00
Epoch 7/10
10/10 - 1s - loss: 132.7387 - loglik: -1.3035e+02 - logprior: -2.3908e+00
Epoch 8/10
10/10 - 1s - loss: 131.9652 - loglik: -1.2970e+02 - logprior: -2.2625e+00
Epoch 9/10
10/10 - 1s - loss: 131.7089 - loglik: -1.2954e+02 - logprior: -2.1661e+00
Epoch 10/10
10/10 - 1s - loss: 132.0018 - loglik: -1.2987e+02 - logprior: -2.1283e+00
Fitted a model with MAP estimate = -131.7387
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 147.2482 - loglik: -1.3123e+02 - logprior: -1.6021e+01
Epoch 2/2
10/10 - 1s - loss: 128.1008 - loglik: -1.2114e+02 - logprior: -6.9617e+00
Fitted a model with MAP estimate = -125.5127
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 131.6023 - loglik: -1.1895e+02 - logprior: -1.2653e+01
Epoch 2/2
10/10 - 2s - loss: 121.8859 - loglik: -1.1843e+02 - logprior: -3.4517e+00
Fitted a model with MAP estimate = -120.3387
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.1677 - loglik: -1.2008e+02 - logprior: -1.5088e+01
Epoch 2/10
10/10 - 2s - loss: 124.3007 - loglik: -1.1938e+02 - logprior: -4.9171e+00
Epoch 3/10
10/10 - 1s - loss: 120.7640 - loglik: -1.1826e+02 - logprior: -2.5075e+00
Epoch 4/10
10/10 - 1s - loss: 119.4209 - loglik: -1.1786e+02 - logprior: -1.5614e+00
Epoch 5/10
10/10 - 1s - loss: 118.6612 - loglik: -1.1737e+02 - logprior: -1.2885e+00
Epoch 6/10
10/10 - 1s - loss: 118.7067 - loglik: -1.1766e+02 - logprior: -1.0428e+00
Fitted a model with MAP estimate = -118.2771
Time for alignment: 44.9760
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 198.1209 - loglik: -1.8379e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 1s - loss: 169.9017 - loglik: -1.6580e+02 - logprior: -4.0977e+00
Epoch 3/10
10/10 - 2s - loss: 153.6704 - loglik: -1.5117e+02 - logprior: -2.4954e+00
Epoch 4/10
10/10 - 1s - loss: 142.8072 - loglik: -1.4057e+02 - logprior: -2.2396e+00
Epoch 5/10
10/10 - 1s - loss: 136.7019 - loglik: -1.3441e+02 - logprior: -2.2888e+00
Epoch 6/10
10/10 - 1s - loss: 133.8852 - loglik: -1.3147e+02 - logprior: -2.4201e+00
Epoch 7/10
10/10 - 2s - loss: 133.0363 - loglik: -1.3063e+02 - logprior: -2.4080e+00
Epoch 8/10
10/10 - 1s - loss: 132.2399 - loglik: -1.2994e+02 - logprior: -2.3041e+00
Epoch 9/10
10/10 - 1s - loss: 131.8231 - loglik: -1.2959e+02 - logprior: -2.2359e+00
Epoch 10/10
10/10 - 1s - loss: 132.0232 - loglik: -1.2981e+02 - logprior: -2.2157e+00
Fitted a model with MAP estimate = -131.7279
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 147.5141 - loglik: -1.3149e+02 - logprior: -1.6026e+01
Epoch 2/2
10/10 - 1s - loss: 129.1886 - loglik: -1.2222e+02 - logprior: -6.9646e+00
Fitted a model with MAP estimate = -125.7110
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 131.8683 - loglik: -1.1922e+02 - logprior: -1.2653e+01
Epoch 2/2
10/10 - 2s - loss: 122.0793 - loglik: -1.1863e+02 - logprior: -3.4541e+00
Fitted a model with MAP estimate = -120.3321
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.1564 - loglik: -1.2004e+02 - logprior: -1.5112e+01
Epoch 2/10
10/10 - 2s - loss: 124.3163 - loglik: -1.1939e+02 - logprior: -4.9288e+00
Epoch 3/10
10/10 - 1s - loss: 120.7726 - loglik: -1.1826e+02 - logprior: -2.5118e+00
Epoch 4/10
10/10 - 1s - loss: 119.6586 - loglik: -1.1810e+02 - logprior: -1.5621e+00
Epoch 5/10
10/10 - 1s - loss: 118.8216 - loglik: -1.1753e+02 - logprior: -1.2938e+00
Epoch 6/10
10/10 - 2s - loss: 118.5252 - loglik: -1.1748e+02 - logprior: -1.0484e+00
Epoch 7/10
10/10 - 1s - loss: 117.9691 - loglik: -1.1699e+02 - logprior: -9.7590e-01
Epoch 8/10
10/10 - 2s - loss: 118.3081 - loglik: -1.1740e+02 - logprior: -9.0730e-01
Fitted a model with MAP estimate = -118.0025
Time for alignment: 50.2997
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.5746 - loglik: -1.8324e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 1s - loss: 170.4559 - loglik: -1.6636e+02 - logprior: -4.0981e+00
Epoch 3/10
10/10 - 2s - loss: 152.7434 - loglik: -1.5027e+02 - logprior: -2.4688e+00
Epoch 4/10
10/10 - 1s - loss: 141.5882 - loglik: -1.3940e+02 - logprior: -2.1908e+00
Epoch 5/10
10/10 - 1s - loss: 137.4602 - loglik: -1.3528e+02 - logprior: -2.1808e+00
Epoch 6/10
10/10 - 1s - loss: 134.9344 - loglik: -1.3270e+02 - logprior: -2.2380e+00
Epoch 7/10
10/10 - 2s - loss: 134.3832 - loglik: -1.3220e+02 - logprior: -2.1865e+00
Epoch 8/10
10/10 - 1s - loss: 133.8589 - loglik: -1.3180e+02 - logprior: -2.0603e+00
Epoch 9/10
10/10 - 1s - loss: 133.7809 - loglik: -1.3180e+02 - logprior: -1.9810e+00
Epoch 10/10
10/10 - 1s - loss: 133.3067 - loglik: -1.3134e+02 - logprior: -1.9658e+00
Fitted a model with MAP estimate = -133.4218
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 2), (39, 2), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.1673 - loglik: -1.3214e+02 - logprior: -1.6031e+01
Epoch 2/2
10/10 - 1s - loss: 129.3003 - loglik: -1.2225e+02 - logprior: -7.0502e+00
Fitted a model with MAP estimate = -126.1290
expansions: [(0, 2)]
discards: [ 0 10 13 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.2502 - loglik: -1.1957e+02 - logprior: -1.2679e+01
Epoch 2/2
10/10 - 1s - loss: 122.0967 - loglik: -1.1865e+02 - logprior: -3.4513e+00
Fitted a model with MAP estimate = -120.4106
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.4560 - loglik: -1.2035e+02 - logprior: -1.5105e+01
Epoch 2/10
10/10 - 1s - loss: 123.9022 - loglik: -1.1897e+02 - logprior: -4.9353e+00
Epoch 3/10
10/10 - 1s - loss: 120.8543 - loglik: -1.1834e+02 - logprior: -2.5175e+00
Epoch 4/10
10/10 - 2s - loss: 119.6329 - loglik: -1.1806e+02 - logprior: -1.5693e+00
Epoch 5/10
10/10 - 2s - loss: 118.7487 - loglik: -1.1745e+02 - logprior: -1.2963e+00
Epoch 6/10
10/10 - 2s - loss: 118.6894 - loglik: -1.1764e+02 - logprior: -1.0513e+00
Epoch 7/10
10/10 - 2s - loss: 118.1613 - loglik: -1.1718e+02 - logprior: -9.8185e-01
Epoch 8/10
10/10 - 2s - loss: 118.1220 - loglik: -1.1721e+02 - logprior: -9.1465e-01
Epoch 9/10
10/10 - 1s - loss: 118.0913 - loglik: -1.1719e+02 - logprior: -8.9848e-01
Epoch 10/10
10/10 - 2s - loss: 117.7698 - loglik: -1.1689e+02 - logprior: -8.7706e-01
Fitted a model with MAP estimate = -117.8758
Time for alignment: 52.5413
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.3989 - loglik: -1.8307e+02 - logprior: -1.4331e+01
Epoch 2/10
10/10 - 2s - loss: 170.9725 - loglik: -1.6688e+02 - logprior: -4.0943e+00
Epoch 3/10
10/10 - 2s - loss: 154.3645 - loglik: -1.5191e+02 - logprior: -2.4588e+00
Epoch 4/10
10/10 - 1s - loss: 142.8116 - loglik: -1.4067e+02 - logprior: -2.1375e+00
Epoch 5/10
10/10 - 2s - loss: 138.1383 - loglik: -1.3600e+02 - logprior: -2.1345e+00
Epoch 6/10
10/10 - 1s - loss: 135.1335 - loglik: -1.3292e+02 - logprior: -2.2160e+00
Epoch 7/10
10/10 - 2s - loss: 134.6212 - loglik: -1.3243e+02 - logprior: -2.1871e+00
Epoch 8/10
10/10 - 1s - loss: 134.2010 - loglik: -1.3215e+02 - logprior: -2.0533e+00
Epoch 9/10
10/10 - 2s - loss: 133.9550 - loglik: -1.3200e+02 - logprior: -1.9553e+00
Epoch 10/10
10/10 - 1s - loss: 133.7559 - loglik: -1.3181e+02 - logprior: -1.9463e+00
Fitted a model with MAP estimate = -133.6556
expansions: [(4, 2), (5, 2), (6, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 2), (39, 2), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.3652 - loglik: -1.3233e+02 - logprior: -1.6030e+01
Epoch 2/2
10/10 - 1s - loss: 129.7430 - loglik: -1.2268e+02 - logprior: -7.0651e+00
Fitted a model with MAP estimate = -126.3895
expansions: [(0, 2)]
discards: [ 0  9 13 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.4476 - loglik: -1.1977e+02 - logprior: -1.2679e+01
Epoch 2/2
10/10 - 2s - loss: 122.3651 - loglik: -1.1892e+02 - logprior: -3.4479e+00
Fitted a model with MAP estimate = -120.5800
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.3100 - loglik: -1.2026e+02 - logprior: -1.5052e+01
Epoch 2/10
10/10 - 1s - loss: 124.2505 - loglik: -1.1936e+02 - logprior: -4.8886e+00
Epoch 3/10
10/10 - 1s - loss: 120.6783 - loglik: -1.1818e+02 - logprior: -2.4958e+00
Epoch 4/10
10/10 - 1s - loss: 119.5197 - loglik: -1.1795e+02 - logprior: -1.5661e+00
Epoch 5/10
10/10 - 1s - loss: 118.8841 - loglik: -1.1759e+02 - logprior: -1.2931e+00
Epoch 6/10
10/10 - 1s - loss: 118.4645 - loglik: -1.1742e+02 - logprior: -1.0486e+00
Epoch 7/10
10/10 - 1s - loss: 117.9177 - loglik: -1.1694e+02 - logprior: -9.7707e-01
Epoch 8/10
10/10 - 1s - loss: 118.5443 - loglik: -1.1763e+02 - logprior: -9.1227e-01
Fitted a model with MAP estimate = -118.0082
Time for alignment: 50.4325
Computed alignments with likelihoods: ['-118.1050', '-118.2771', '-118.0025', '-117.8758', '-118.0082']
Best model has likelihood: -117.8758  (prior= -0.8743 )
time for generating output: 0.2439
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 527.3958 - loglik: -5.2118e+02 - logprior: -6.2150e+00
Epoch 2/10
23/23 - 4s - loss: 459.0078 - loglik: -4.5839e+02 - logprior: -6.1996e-01
Epoch 3/10
23/23 - 4s - loss: 438.6039 - loglik: -4.3825e+02 - logprior: -3.5563e-01
Epoch 4/10
23/23 - 4s - loss: 432.0534 - loglik: -4.3181e+02 - logprior: -2.3859e-01
Epoch 5/10
23/23 - 4s - loss: 430.8548 - loglik: -4.3068e+02 - logprior: -1.7423e-01
Epoch 6/10
23/23 - 4s - loss: 428.2523 - loglik: -4.2807e+02 - logprior: -1.8155e-01
Epoch 7/10
23/23 - 4s - loss: 428.3191 - loglik: -4.2813e+02 - logprior: -1.8990e-01
Fitted a model with MAP estimate = -427.5284
expansions: [(0, 8), (9, 4), (15, 1), (39, 1), (54, 1), (56, 2), (57, 1), (59, 2), (66, 1), (70, 1), (71, 1), (72, 2), (107, 1), (114, 1), (119, 2), (121, 1), (124, 1), (127, 1), (149, 2), (150, 1), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 441.9795 - loglik: -4.3375e+02 - logprior: -8.2279e+00
Epoch 2/2
23/23 - 6s - loss: 424.0264 - loglik: -4.2353e+02 - logprior: -4.9173e-01
Fitted a model with MAP estimate = -421.1881
expansions: [(0, 18), (182, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  77  95 194 195 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.8512 - loglik: -4.2837e+02 - logprior: -8.4764e+00
Epoch 2/2
23/23 - 6s - loss: 424.4356 - loglik: -4.2403e+02 - logprior: -4.0261e-01
Fitted a model with MAP estimate = -420.7280
expansions: [(0, 9), (201, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 80]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 433.5373 - loglik: -4.2488e+02 - logprior: -8.6623e+00
Epoch 2/10
23/23 - 6s - loss: 423.8874 - loglik: -4.2302e+02 - logprior: -8.6436e-01
Epoch 3/10
23/23 - 6s - loss: 419.1533 - loglik: -4.1992e+02 - logprior: 0.7653
Epoch 4/10
23/23 - 6s - loss: 414.3202 - loglik: -4.1543e+02 - logprior: 1.1087
Epoch 5/10
23/23 - 6s - loss: 412.2008 - loglik: -4.1345e+02 - logprior: 1.2526
Epoch 6/10
23/23 - 6s - loss: 408.8359 - loglik: -4.1022e+02 - logprior: 1.3850
Epoch 7/10
23/23 - 6s - loss: 410.4663 - loglik: -4.1197e+02 - logprior: 1.5033
Fitted a model with MAP estimate = -408.6005
Time for alignment: 127.2426
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 527.9893 - loglik: -5.2181e+02 - logprior: -6.1765e+00
Epoch 2/10
23/23 - 4s - loss: 458.0361 - loglik: -4.5734e+02 - logprior: -6.9340e-01
Epoch 3/10
23/23 - 4s - loss: 435.3785 - loglik: -4.3484e+02 - logprior: -5.3739e-01
Epoch 4/10
23/23 - 4s - loss: 431.0302 - loglik: -4.3067e+02 - logprior: -3.6128e-01
Epoch 5/10
23/23 - 4s - loss: 428.4384 - loglik: -4.2815e+02 - logprior: -2.8577e-01
Epoch 6/10
23/23 - 4s - loss: 425.9038 - loglik: -4.2560e+02 - logprior: -3.0577e-01
Epoch 7/10
23/23 - 4s - loss: 424.9210 - loglik: -4.2459e+02 - logprior: -3.2834e-01
Epoch 8/10
23/23 - 4s - loss: 425.6669 - loglik: -4.2531e+02 - logprior: -3.5327e-01
Fitted a model with MAP estimate = -425.4681
expansions: [(0, 9), (10, 3), (18, 1), (57, 1), (58, 2), (59, 2), (71, 1), (72, 1), (73, 2), (83, 2), (85, 1), (88, 1), (108, 1), (113, 2), (114, 1), (120, 1), (122, 1), (125, 1), (127, 1), (128, 1), (149, 2), (150, 2), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 441.6126 - loglik: -4.3324e+02 - logprior: -8.3680e+00
Epoch 2/2
23/23 - 6s - loss: 422.9805 - loglik: -4.2238e+02 - logprior: -6.0214e-01
Fitted a model with MAP estimate = -420.1819
expansions: [(0, 18), (23, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  73  75  93 105 141 142 198 199
 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 437.3803 - loglik: -4.2881e+02 - logprior: -8.5694e+00
Epoch 2/2
23/23 - 6s - loss: 423.1178 - loglik: -4.2262e+02 - logprior: -4.9493e-01
Fitted a model with MAP estimate = -420.6433
expansions: [(0, 14), (28, 1), (146, 1), (201, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 22 23]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 432.6589 - loglik: -4.2657e+02 - logprior: -6.0895e+00
Epoch 2/10
23/23 - 6s - loss: 418.8125 - loglik: -4.1909e+02 - logprior: 0.2759
Epoch 3/10
23/23 - 6s - loss: 417.6930 - loglik: -4.1861e+02 - logprior: 0.9148
Epoch 4/10
23/23 - 6s - loss: 413.6071 - loglik: -4.1470e+02 - logprior: 1.0897
Epoch 5/10
23/23 - 6s - loss: 408.8558 - loglik: -4.1011e+02 - logprior: 1.2551
Epoch 6/10
23/23 - 6s - loss: 407.6055 - loglik: -4.0896e+02 - logprior: 1.3576
Epoch 7/10
23/23 - 6s - loss: 407.7682 - loglik: -4.0922e+02 - logprior: 1.4549
Fitted a model with MAP estimate = -406.4749
Time for alignment: 132.5399
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 527.8893 - loglik: -5.2169e+02 - logprior: -6.2003e+00
Epoch 2/10
23/23 - 4s - loss: 457.5988 - loglik: -4.5692e+02 - logprior: -6.7600e-01
Epoch 3/10
23/23 - 4s - loss: 437.8029 - loglik: -4.3737e+02 - logprior: -4.3675e-01
Epoch 4/10
23/23 - 4s - loss: 430.3428 - loglik: -4.3003e+02 - logprior: -3.0954e-01
Epoch 5/10
23/23 - 4s - loss: 427.1409 - loglik: -4.2690e+02 - logprior: -2.4278e-01
Epoch 6/10
23/23 - 4s - loss: 427.4638 - loglik: -4.2721e+02 - logprior: -2.5413e-01
Fitted a model with MAP estimate = -425.6075
expansions: [(0, 8), (9, 4), (15, 1), (18, 1), (42, 1), (56, 2), (57, 2), (58, 2), (70, 3), (71, 1), (79, 2), (80, 1), (83, 1), (107, 1), (112, 1), (120, 2), (121, 2), (124, 1), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 438.6234 - loglik: -4.3048e+02 - logprior: -8.1410e+00
Epoch 2/2
23/23 - 6s - loss: 421.3731 - loglik: -4.2094e+02 - logprior: -4.3181e-01
Fitted a model with MAP estimate = -418.1651
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6   7   8   9  10  72  77  93 104 152 200 201 202
 203 204]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 435.0304 - loglik: -4.2663e+02 - logprior: -8.4014e+00
Epoch 2/2
23/23 - 6s - loss: 421.4531 - loglik: -4.2117e+02 - logprior: -2.8505e-01
Fitted a model with MAP estimate = -418.9487
expansions: [(0, 17), (203, 7)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 429.6621 - loglik: -4.2326e+02 - logprior: -6.3972e+00
Epoch 2/10
23/23 - 6s - loss: 418.9860 - loglik: -4.1923e+02 - logprior: 0.2416
Epoch 3/10
23/23 - 6s - loss: 415.6798 - loglik: -4.1652e+02 - logprior: 0.8390
Epoch 4/10
23/23 - 6s - loss: 412.0569 - loglik: -4.1311e+02 - logprior: 1.0512
Epoch 5/10
23/23 - 6s - loss: 408.3582 - loglik: -4.0960e+02 - logprior: 1.2445
Epoch 6/10
23/23 - 6s - loss: 406.3330 - loglik: -4.0769e+02 - logprior: 1.3552
Epoch 7/10
23/23 - 6s - loss: 405.6131 - loglik: -4.0705e+02 - logprior: 1.4398
Epoch 8/10
23/23 - 6s - loss: 405.1252 - loglik: -4.0669e+02 - logprior: 1.5666
Epoch 9/10
23/23 - 6s - loss: 406.4601 - loglik: -4.0816e+02 - logprior: 1.7044
Fitted a model with MAP estimate = -404.9380
Time for alignment: 136.4672
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 528.6428 - loglik: -5.2246e+02 - logprior: -6.1866e+00
Epoch 2/10
23/23 - 4s - loss: 458.2725 - loglik: -4.5757e+02 - logprior: -7.0212e-01
Epoch 3/10
23/23 - 4s - loss: 439.3534 - loglik: -4.3879e+02 - logprior: -5.6646e-01
Epoch 4/10
23/23 - 4s - loss: 432.8555 - loglik: -4.3246e+02 - logprior: -3.9050e-01
Epoch 5/10
23/23 - 4s - loss: 428.9545 - loglik: -4.2862e+02 - logprior: -3.3348e-01
Epoch 6/10
23/23 - 4s - loss: 426.7477 - loglik: -4.2641e+02 - logprior: -3.4178e-01
Epoch 7/10
23/23 - 4s - loss: 427.7778 - loglik: -4.2743e+02 - logprior: -3.4768e-01
Fitted a model with MAP estimate = -426.5258
expansions: [(0, 9), (9, 2), (19, 1), (43, 1), (51, 1), (56, 2), (57, 1), (59, 1), (66, 1), (71, 1), (72, 3), (82, 2), (84, 1), (87, 1), (108, 1), (112, 1), (116, 1), (119, 1), (120, 2), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 441.6434 - loglik: -4.3346e+02 - logprior: -8.1854e+00
Epoch 2/2
23/23 - 6s - loss: 423.2264 - loglik: -4.2287e+02 - logprior: -3.5895e-01
Fitted a model with MAP estimate = -420.2117
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  10  18  71  93 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.5284 - loglik: -4.2810e+02 - logprior: -8.4285e+00
Epoch 2/2
23/23 - 6s - loss: 423.4348 - loglik: -4.2305e+02 - logprior: -3.8900e-01
Fitted a model with MAP estimate = -420.9929
expansions: [(0, 18), (203, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 434.7887 - loglik: -4.2619e+02 - logprior: -8.5941e+00
Epoch 2/10
23/23 - 6s - loss: 423.8596 - loglik: -4.2308e+02 - logprior: -7.8273e-01
Epoch 3/10
23/23 - 6s - loss: 417.1320 - loglik: -4.1776e+02 - logprior: 0.6329
Epoch 4/10
23/23 - 6s - loss: 413.2839 - loglik: -4.1420e+02 - logprior: 0.9133
Epoch 5/10
23/23 - 6s - loss: 410.8504 - loglik: -4.1193e+02 - logprior: 1.0825
Epoch 6/10
23/23 - 6s - loss: 408.1837 - loglik: -4.0938e+02 - logprior: 1.1913
Epoch 7/10
23/23 - 6s - loss: 407.8237 - loglik: -4.0912e+02 - logprior: 1.2957
Epoch 8/10
23/23 - 6s - loss: 405.8160 - loglik: -4.0723e+02 - logprior: 1.4169
Epoch 9/10
23/23 - 6s - loss: 407.6516 - loglik: -4.0918e+02 - logprior: 1.5235
Fitted a model with MAP estimate = -406.3963
Time for alignment: 141.4788
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 527.1081 - loglik: -5.2090e+02 - logprior: -6.2032e+00
Epoch 2/10
23/23 - 4s - loss: 457.1790 - loglik: -4.5643e+02 - logprior: -7.4570e-01
Epoch 3/10
23/23 - 4s - loss: 435.7407 - loglik: -4.3516e+02 - logprior: -5.8249e-01
Epoch 4/10
23/23 - 4s - loss: 431.2135 - loglik: -4.3078e+02 - logprior: -4.3421e-01
Epoch 5/10
23/23 - 4s - loss: 428.5422 - loglik: -4.2816e+02 - logprior: -3.8143e-01
Epoch 6/10
23/23 - 4s - loss: 425.7894 - loglik: -4.2540e+02 - logprior: -3.9233e-01
Epoch 7/10
23/23 - 4s - loss: 426.8044 - loglik: -4.2640e+02 - logprior: -4.0573e-01
Fitted a model with MAP estimate = -425.6851
expansions: [(0, 8), (11, 1), (19, 1), (39, 1), (51, 1), (56, 2), (57, 1), (59, 2), (66, 1), (71, 1), (72, 1), (78, 1), (79, 2), (80, 1), (83, 1), (86, 1), (112, 2), (113, 1), (119, 1), (121, 1), (124, 1), (127, 1), (149, 2), (150, 1), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 441.4478 - loglik: -4.3315e+02 - logprior: -8.2966e+00
Epoch 2/2
23/23 - 6s - loss: 424.0198 - loglik: -4.2356e+02 - logprior: -4.5558e-01
Fitted a model with MAP estimate = -420.1986
expansions: [(0, 19), (183, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  68  74 103 140 195 196 197 198
 199]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.2587 - loglik: -4.2788e+02 - logprior: -8.3824e+00
Epoch 2/2
23/23 - 6s - loss: 422.6266 - loglik: -4.2237e+02 - logprior: -2.6053e-01
Fitted a model with MAP estimate = -420.6162
expansions: [(0, 19), (27, 3), (201, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 432.1809 - loglik: -4.2612e+02 - logprior: -6.0604e+00
Epoch 2/10
23/23 - 6s - loss: 419.5537 - loglik: -4.1989e+02 - logprior: 0.3381
Epoch 3/10
23/23 - 6s - loss: 414.6866 - loglik: -4.1562e+02 - logprior: 0.9339
Epoch 4/10
23/23 - 6s - loss: 411.4798 - loglik: -4.1256e+02 - logprior: 1.0771
Epoch 5/10
23/23 - 6s - loss: 408.1949 - loglik: -4.0938e+02 - logprior: 1.1881
Epoch 6/10
23/23 - 6s - loss: 407.5013 - loglik: -4.0881e+02 - logprior: 1.3079
Epoch 7/10
23/23 - 6s - loss: 406.4673 - loglik: -4.0788e+02 - logprior: 1.4102
Epoch 8/10
23/23 - 6s - loss: 405.4877 - loglik: -4.0701e+02 - logprior: 1.5262
Epoch 9/10
23/23 - 6s - loss: 405.4485 - loglik: -4.0709e+02 - logprior: 1.6394
Epoch 10/10
23/23 - 6s - loss: 404.6973 - loglik: -4.0646e+02 - logprior: 1.7643
Fitted a model with MAP estimate = -404.7776
Time for alignment: 147.8260
Computed alignments with likelihoods: ['-408.6005', '-406.4749', '-404.9380', '-406.3963', '-404.7776']
Best model has likelihood: -404.7776  (prior= 1.8117 )
time for generating output: 0.2137
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.6994061215166743
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.4082 - loglik: -2.4926e+02 - logprior: -1.1476e+00
Epoch 2/10
29/29 - 3s - loss: 225.2602 - loglik: -2.2447e+02 - logprior: -7.9149e-01
Epoch 3/10
29/29 - 3s - loss: 221.0168 - loglik: -2.2024e+02 - logprior: -7.7755e-01
Epoch 4/10
29/29 - 3s - loss: 219.8889 - loglik: -2.1909e+02 - logprior: -8.0348e-01
Epoch 5/10
29/29 - 3s - loss: 219.4569 - loglik: -2.1866e+02 - logprior: -7.9380e-01
Epoch 6/10
29/29 - 3s - loss: 218.6867 - loglik: -2.1790e+02 - logprior: -7.8772e-01
Epoch 7/10
29/29 - 3s - loss: 217.9362 - loglik: -2.1715e+02 - logprior: -7.8654e-01
Epoch 8/10
29/29 - 3s - loss: 217.7617 - loglik: -2.1698e+02 - logprior: -7.8459e-01
Epoch 9/10
29/29 - 3s - loss: 217.8124 - loglik: -2.1703e+02 - logprior: -7.8409e-01
Fitted a model with MAP estimate = -203.6051
expansions: [(1, 1), (2, 1), (13, 2), (14, 2), (15, 1), (22, 1), (39, 2), (40, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.6701 - loglik: -2.1954e+02 - logprior: -1.1258e+00
Epoch 2/2
29/29 - 3s - loss: 215.6803 - loglik: -2.1493e+02 - logprior: -7.4626e-01
Fitted a model with MAP estimate = -197.2630
expansions: []
discards: [ 1 49 60 66]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.6227 - loglik: -2.1662e+02 - logprior: -1.0050e+00
Epoch 2/2
29/29 - 3s - loss: 215.4170 - loglik: -2.1471e+02 - logprior: -7.0308e-01
Fitted a model with MAP estimate = -197.5611
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 196.8636 - loglik: -1.9619e+02 - logprior: -6.6890e-01
Epoch 2/10
42/42 - 4s - loss: 196.5588 - loglik: -1.9608e+02 - logprior: -4.8263e-01
Epoch 3/10
42/42 - 4s - loss: 195.9726 - loglik: -1.9550e+02 - logprior: -4.7688e-01
Epoch 4/10
42/42 - 4s - loss: 195.7048 - loglik: -1.9523e+02 - logprior: -4.7019e-01
Epoch 5/10
42/42 - 4s - loss: 194.9548 - loglik: -1.9448e+02 - logprior: -4.7037e-01
Epoch 6/10
42/42 - 4s - loss: 194.5885 - loglik: -1.9412e+02 - logprior: -4.6862e-01
Epoch 7/10
42/42 - 4s - loss: 193.8049 - loglik: -1.9334e+02 - logprior: -4.6464e-01
Epoch 8/10
42/42 - 4s - loss: 193.3808 - loglik: -1.9292e+02 - logprior: -4.6034e-01
Epoch 9/10
42/42 - 4s - loss: 193.7070 - loglik: -1.9325e+02 - logprior: -4.5424e-01
Fitted a model with MAP estimate = -193.3641
Time for alignment: 119.9403
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 249.8277 - loglik: -2.4868e+02 - logprior: -1.1481e+00
Epoch 2/10
29/29 - 3s - loss: 225.1661 - loglik: -2.2435e+02 - logprior: -8.1284e-01
Epoch 3/10
29/29 - 3s - loss: 220.8356 - loglik: -2.2005e+02 - logprior: -7.8262e-01
Epoch 4/10
29/29 - 3s - loss: 219.9762 - loglik: -2.1919e+02 - logprior: -7.8197e-01
Epoch 5/10
29/29 - 3s - loss: 218.9536 - loglik: -2.1818e+02 - logprior: -7.7119e-01
Epoch 6/10
29/29 - 3s - loss: 218.1877 - loglik: -2.1741e+02 - logprior: -7.7491e-01
Epoch 7/10
29/29 - 3s - loss: 217.3485 - loglik: -2.1656e+02 - logprior: -7.8408e-01
Epoch 8/10
29/29 - 3s - loss: 217.5564 - loglik: -2.1677e+02 - logprior: -7.8416e-01
Fitted a model with MAP estimate = -202.4819
expansions: [(1, 1), (2, 1), (13, 4), (14, 2), (27, 2), (38, 2), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.6255 - loglik: -2.1949e+02 - logprior: -1.1317e+00
Epoch 2/2
29/29 - 3s - loss: 215.1562 - loglik: -2.1441e+02 - logprior: -7.4447e-01
Fitted a model with MAP estimate = -197.2955
expansions: []
discards: [ 1 36 49 57 60 68]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.5379 - loglik: -2.1653e+02 - logprior: -1.0099e+00
Epoch 2/2
29/29 - 3s - loss: 216.0670 - loglik: -2.1536e+02 - logprior: -7.0617e-01
Fitted a model with MAP estimate = -197.7001
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.0274 - loglik: -1.9635e+02 - logprior: -6.7304e-01
Epoch 2/10
42/42 - 4s - loss: 196.6072 - loglik: -1.9612e+02 - logprior: -4.8579e-01
Epoch 3/10
42/42 - 4s - loss: 196.0721 - loglik: -1.9559e+02 - logprior: -4.7977e-01
Epoch 4/10
42/42 - 4s - loss: 195.6792 - loglik: -1.9520e+02 - logprior: -4.7510e-01
Epoch 5/10
42/42 - 4s - loss: 195.2751 - loglik: -1.9480e+02 - logprior: -4.7025e-01
Epoch 6/10
42/42 - 4s - loss: 194.2380 - loglik: -1.9377e+02 - logprior: -4.6735e-01
Epoch 7/10
42/42 - 4s - loss: 194.0062 - loglik: -1.9354e+02 - logprior: -4.6482e-01
Epoch 8/10
42/42 - 4s - loss: 193.5130 - loglik: -1.9305e+02 - logprior: -4.6084e-01
Epoch 9/10
42/42 - 4s - loss: 193.6359 - loglik: -1.9317e+02 - logprior: -4.6205e-01
Fitted a model with MAP estimate = -193.3694
Time for alignment: 117.2289
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.4694 - loglik: -2.4932e+02 - logprior: -1.1466e+00
Epoch 2/10
29/29 - 3s - loss: 225.7151 - loglik: -2.2491e+02 - logprior: -8.0070e-01
Epoch 3/10
29/29 - 3s - loss: 221.1844 - loglik: -2.2040e+02 - logprior: -7.8194e-01
Epoch 4/10
29/29 - 3s - loss: 220.0244 - loglik: -2.1922e+02 - logprior: -7.9987e-01
Epoch 5/10
29/29 - 3s - loss: 219.3019 - loglik: -2.1851e+02 - logprior: -7.9646e-01
Epoch 6/10
29/29 - 3s - loss: 218.1799 - loglik: -2.1739e+02 - logprior: -7.8525e-01
Epoch 7/10
29/29 - 3s - loss: 217.6050 - loglik: -2.1682e+02 - logprior: -7.8345e-01
Epoch 8/10
29/29 - 3s - loss: 217.3359 - loglik: -2.1655e+02 - logprior: -7.8371e-01
Epoch 9/10
29/29 - 3s - loss: 217.4199 - loglik: -2.1664e+02 - logprior: -7.8188e-01
Fitted a model with MAP estimate = -203.4208
expansions: [(1, 1), (2, 1), (13, 4), (14, 3), (27, 2), (38, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.8861 - loglik: -2.1976e+02 - logprior: -1.1226e+00
Epoch 2/2
29/29 - 3s - loss: 215.3845 - loglik: -2.1465e+02 - logprior: -7.3225e-01
Fitted a model with MAP estimate = -197.2797
expansions: []
discards: [ 1 37 61 67]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.2938 - loglik: -2.1629e+02 - logprior: -1.0084e+00
Epoch 2/2
29/29 - 3s - loss: 215.9374 - loglik: -2.1523e+02 - logprior: -7.0448e-01
Fitted a model with MAP estimate = -197.5453
expansions: []
discards: [16]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.1115 - loglik: -1.9644e+02 - logprior: -6.6667e-01
Epoch 2/10
42/42 - 4s - loss: 196.5115 - loglik: -1.9603e+02 - logprior: -4.7994e-01
Epoch 3/10
42/42 - 4s - loss: 195.7265 - loglik: -1.9526e+02 - logprior: -4.7010e-01
Epoch 4/10
42/42 - 4s - loss: 196.0107 - loglik: -1.9554e+02 - logprior: -4.6805e-01
Fitted a model with MAP estimate = -195.1074
Time for alignment: 99.4005
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 249.9412 - loglik: -2.4879e+02 - logprior: -1.1486e+00
Epoch 2/10
29/29 - 3s - loss: 224.5687 - loglik: -2.2375e+02 - logprior: -8.1634e-01
Epoch 3/10
29/29 - 3s - loss: 220.1141 - loglik: -2.1933e+02 - logprior: -7.8576e-01
Epoch 4/10
29/29 - 3s - loss: 219.4541 - loglik: -2.1868e+02 - logprior: -7.7709e-01
Epoch 5/10
29/29 - 3s - loss: 218.7388 - loglik: -2.1797e+02 - logprior: -7.6971e-01
Epoch 6/10
29/29 - 3s - loss: 218.2031 - loglik: -2.1744e+02 - logprior: -7.6489e-01
Epoch 7/10
29/29 - 3s - loss: 217.1307 - loglik: -2.1637e+02 - logprior: -7.6107e-01
Epoch 8/10
29/29 - 3s - loss: 216.8437 - loglik: -2.1608e+02 - logprior: -7.6488e-01
Epoch 9/10
29/29 - 3s - loss: 216.8347 - loglik: -2.1605e+02 - logprior: -7.8005e-01
Epoch 10/10
29/29 - 3s - loss: 216.4552 - loglik: -2.1568e+02 - logprior: -7.7884e-01
Fitted a model with MAP estimate = -203.3217
expansions: [(1, 1), (2, 1), (14, 3), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.5178 - loglik: -2.1941e+02 - logprior: -1.1123e+00
Epoch 2/2
29/29 - 3s - loss: 215.4940 - loglik: -2.1477e+02 - logprior: -7.2775e-01
Fitted a model with MAP estimate = -197.2011
expansions: []
discards: [ 1 56 59 67]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.2805 - loglik: -2.1628e+02 - logprior: -1.0040e+00
Epoch 2/2
29/29 - 3s - loss: 216.1558 - loglik: -2.1545e+02 - logprior: -7.0352e-01
Fitted a model with MAP estimate = -197.6485
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 196.7810 - loglik: -1.9611e+02 - logprior: -6.6762e-01
Epoch 2/10
42/42 - 4s - loss: 196.7965 - loglik: -1.9631e+02 - logprior: -4.8162e-01
Fitted a model with MAP estimate = -195.9367
Time for alignment: 93.3481
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.0945 - loglik: -2.4895e+02 - logprior: -1.1463e+00
Epoch 2/10
29/29 - 3s - loss: 223.9399 - loglik: -2.2314e+02 - logprior: -8.0456e-01
Epoch 3/10
29/29 - 3s - loss: 220.7124 - loglik: -2.1992e+02 - logprior: -7.9043e-01
Epoch 4/10
29/29 - 3s - loss: 219.1524 - loglik: -2.1836e+02 - logprior: -7.8870e-01
Epoch 5/10
29/29 - 3s - loss: 218.5511 - loglik: -2.1776e+02 - logprior: -7.8891e-01
Epoch 6/10
29/29 - 3s - loss: 217.5728 - loglik: -2.1679e+02 - logprior: -7.8261e-01
Epoch 7/10
29/29 - 3s - loss: 217.2511 - loglik: -2.1647e+02 - logprior: -7.7928e-01
Epoch 8/10
29/29 - 3s - loss: 216.7553 - loglik: -2.1597e+02 - logprior: -7.8425e-01
Epoch 9/10
29/29 - 3s - loss: 216.8110 - loglik: -2.1603e+02 - logprior: -7.7989e-01
Fitted a model with MAP estimate = -202.5002
expansions: [(1, 1), (2, 1), (14, 2), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 219.9928 - loglik: -2.1888e+02 - logprior: -1.1177e+00
Epoch 2/2
29/29 - 3s - loss: 215.6088 - loglik: -2.1488e+02 - logprior: -7.2859e-01
Fitted a model with MAP estimate = -197.2903
expansions: []
discards: [ 1 55 66]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.4663 - loglik: -2.1646e+02 - logprior: -1.0090e+00
Epoch 2/2
29/29 - 3s - loss: 215.7491 - loglik: -2.1504e+02 - logprior: -7.0604e-01
Fitted a model with MAP estimate = -197.5701
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.1096 - loglik: -1.9644e+02 - logprior: -6.6466e-01
Epoch 2/10
42/42 - 4s - loss: 196.4295 - loglik: -1.9595e+02 - logprior: -4.7684e-01
Epoch 3/10
42/42 - 4s - loss: 195.8989 - loglik: -1.9543e+02 - logprior: -4.7244e-01
Epoch 4/10
42/42 - 4s - loss: 195.8353 - loglik: -1.9537e+02 - logprior: -4.6830e-01
Epoch 5/10
42/42 - 4s - loss: 194.9467 - loglik: -1.9448e+02 - logprior: -4.6404e-01
Epoch 6/10
42/42 - 4s - loss: 194.4201 - loglik: -1.9396e+02 - logprior: -4.6222e-01
Epoch 7/10
42/42 - 4s - loss: 194.3600 - loglik: -1.9390e+02 - logprior: -4.5928e-01
Epoch 8/10
42/42 - 4s - loss: 193.2221 - loglik: -1.9277e+02 - logprior: -4.5698e-01
Epoch 9/10
42/42 - 4s - loss: 193.3918 - loglik: -1.9294e+02 - logprior: -4.5330e-01
Fitted a model with MAP estimate = -193.3565
Time for alignment: 119.6006
Computed alignments with likelihoods: ['-193.3641', '-193.3694', '-195.1074', '-195.9367', '-193.3565']
Best model has likelihood: -193.3565  (prior= -0.4544 )
time for generating output: 0.1620
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6173999011369253
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.5132 - loglik: -3.3025e+02 - logprior: -4.5260e+01
Epoch 2/10
10/10 - 1s - loss: 298.5886 - loglik: -2.8777e+02 - logprior: -1.0821e+01
Epoch 3/10
10/10 - 1s - loss: 251.0123 - loglik: -2.4607e+02 - logprior: -4.9384e+00
Epoch 4/10
10/10 - 1s - loss: 221.7957 - loglik: -2.1854e+02 - logprior: -3.2581e+00
Epoch 5/10
10/10 - 1s - loss: 210.2376 - loglik: -2.0774e+02 - logprior: -2.4981e+00
Epoch 6/10
10/10 - 1s - loss: 206.9348 - loglik: -2.0491e+02 - logprior: -2.0224e+00
Epoch 7/10
10/10 - 1s - loss: 204.0898 - loglik: -2.0243e+02 - logprior: -1.6581e+00
Epoch 8/10
10/10 - 1s - loss: 203.6646 - loglik: -2.0230e+02 - logprior: -1.3665e+00
Epoch 9/10
10/10 - 1s - loss: 203.0119 - loglik: -2.0178e+02 - logprior: -1.2290e+00
Epoch 10/10
10/10 - 1s - loss: 202.0948 - loglik: -2.0095e+02 - logprior: -1.1449e+00
Fitted a model with MAP estimate = -202.0585
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2), (96, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.3766 - loglik: -2.0020e+02 - logprior: -5.1174e+01
Epoch 2/2
10/10 - 1s - loss: 206.1462 - loglik: -1.8563e+02 - logprior: -2.0514e+01
Fitted a model with MAP estimate = -197.5537
expansions: [(0, 3)]
discards: [  0  18  75 103 104 120 121 122 123 124]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.0944 - loglik: -1.8661e+02 - logprior: -4.0486e+01
Epoch 2/2
10/10 - 1s - loss: 192.1474 - loglik: -1.8272e+02 - logprior: -9.4251e+00
Fitted a model with MAP estimate = -187.2897
expansions: [(118, 6)]
discards: [ 0  2 12]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.6364 - loglik: -1.8477e+02 - logprior: -4.9871e+01
Epoch 2/10
10/10 - 1s - loss: 197.8412 - loglik: -1.8057e+02 - logprior: -1.7267e+01
Epoch 3/10
10/10 - 1s - loss: 184.4214 - loglik: -1.7846e+02 - logprior: -5.9614e+00
Epoch 4/10
10/10 - 1s - loss: 178.1457 - loglik: -1.7725e+02 - logprior: -8.9344e-01
Epoch 5/10
10/10 - 1s - loss: 175.8146 - loglik: -1.7661e+02 - logprior: 0.7938
Epoch 6/10
10/10 - 1s - loss: 174.7932 - loglik: -1.7657e+02 - logprior: 1.7725
Epoch 7/10
10/10 - 1s - loss: 173.9989 - loglik: -1.7649e+02 - logprior: 2.4945
Epoch 8/10
10/10 - 1s - loss: 173.6398 - loglik: -1.7660e+02 - logprior: 2.9575
Epoch 9/10
10/10 - 1s - loss: 173.7049 - loglik: -1.7696e+02 - logprior: 3.2542
Fitted a model with MAP estimate = -173.3008
Time for alignment: 43.5568
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.4398 - loglik: -3.3018e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 297.7514 - loglik: -2.8694e+02 - logprior: -1.0813e+01
Epoch 3/10
10/10 - 1s - loss: 249.1044 - loglik: -2.4428e+02 - logprior: -4.8259e+00
Epoch 4/10
10/10 - 1s - loss: 223.5573 - loglik: -2.2063e+02 - logprior: -2.9275e+00
Epoch 5/10
10/10 - 1s - loss: 212.1355 - loglik: -2.1013e+02 - logprior: -2.0020e+00
Epoch 6/10
10/10 - 1s - loss: 206.0790 - loglik: -2.0465e+02 - logprior: -1.4316e+00
Epoch 7/10
10/10 - 1s - loss: 203.7860 - loglik: -2.0269e+02 - logprior: -1.0993e+00
Epoch 8/10
10/10 - 1s - loss: 202.7785 - loglik: -2.0187e+02 - logprior: -9.1234e-01
Epoch 9/10
10/10 - 1s - loss: 202.0612 - loglik: -2.0124e+02 - logprior: -8.2330e-01
Epoch 10/10
10/10 - 1s - loss: 201.8172 - loglik: -2.0106e+02 - logprior: -7.5726e-01
Fitted a model with MAP estimate = -201.2571
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2), (96, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.9869 - loglik: -1.9986e+02 - logprior: -5.1129e+01
Epoch 2/2
10/10 - 1s - loss: 205.8086 - loglik: -1.8543e+02 - logprior: -2.0380e+01
Fitted a model with MAP estimate = -197.4974
expansions: [(0, 3)]
discards: [  0   9  13  75 103 104 120 121 122 123 124]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.2394 - loglik: -1.8689e+02 - logprior: -4.0351e+01
Epoch 2/2
10/10 - 1s - loss: 192.8892 - loglik: -1.8356e+02 - logprior: -9.3252e+00
Fitted a model with MAP estimate = -187.4320
expansions: [(117, 6)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.9324 - loglik: -1.8506e+02 - logprior: -4.9872e+01
Epoch 2/10
10/10 - 1s - loss: 198.5438 - loglik: -1.8128e+02 - logprior: -1.7266e+01
Epoch 3/10
10/10 - 1s - loss: 184.3694 - loglik: -1.7843e+02 - logprior: -5.9386e+00
Epoch 4/10
10/10 - 1s - loss: 178.5133 - loglik: -1.7763e+02 - logprior: -8.8710e-01
Epoch 5/10
10/10 - 1s - loss: 176.0614 - loglik: -1.7685e+02 - logprior: 0.7858
Epoch 6/10
10/10 - 1s - loss: 174.9122 - loglik: -1.7668e+02 - logprior: 1.7652
Epoch 7/10
10/10 - 1s - loss: 174.4401 - loglik: -1.7693e+02 - logprior: 2.4917
Epoch 8/10
10/10 - 1s - loss: 173.5467 - loglik: -1.7651e+02 - logprior: 2.9637
Epoch 9/10
10/10 - 1s - loss: 173.7549 - loglik: -1.7702e+02 - logprior: 3.2629
Fitted a model with MAP estimate = -173.3486
Time for alignment: 42.0737
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.3456 - loglik: -3.3009e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 298.4727 - loglik: -2.8765e+02 - logprior: -1.0821e+01
Epoch 3/10
10/10 - 1s - loss: 249.7701 - loglik: -2.4482e+02 - logprior: -4.9473e+00
Epoch 4/10
10/10 - 1s - loss: 220.6539 - loglik: -2.1734e+02 - logprior: -3.3182e+00
Epoch 5/10
10/10 - 1s - loss: 209.3976 - loglik: -2.0685e+02 - logprior: -2.5505e+00
Epoch 6/10
10/10 - 1s - loss: 204.8492 - loglik: -2.0272e+02 - logprior: -2.1255e+00
Epoch 7/10
10/10 - 1s - loss: 202.9016 - loglik: -2.0094e+02 - logprior: -1.9576e+00
Epoch 8/10
10/10 - 1s - loss: 201.8046 - loglik: -2.0005e+02 - logprior: -1.7504e+00
Epoch 9/10
10/10 - 1s - loss: 200.9699 - loglik: -1.9939e+02 - logprior: -1.5756e+00
Epoch 10/10
10/10 - 1s - loss: 200.9141 - loglik: -1.9945e+02 - logprior: -1.4681e+00
Fitted a model with MAP estimate = -200.6867
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (50, 1), (51, 1), (56, 1), (58, 1), (79, 1), (81, 2), (82, 2), (96, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.3904 - loglik: -1.9827e+02 - logprior: -5.1125e+01
Epoch 2/2
10/10 - 1s - loss: 203.7586 - loglik: -1.8354e+02 - logprior: -2.0219e+01
Fitted a model with MAP estimate = -195.9707
expansions: [(0, 3)]
discards: [  0  10 101 102 118 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.8800 - loglik: -1.8557e+02 - logprior: -4.0314e+01
Epoch 2/2
10/10 - 1s - loss: 191.8009 - loglik: -1.8252e+02 - logprior: -9.2784e+00
Fitted a model with MAP estimate = -186.7281
expansions: [(117, 6)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.3780 - loglik: -1.8453e+02 - logprior: -4.9853e+01
Epoch 2/10
10/10 - 1s - loss: 197.5814 - loglik: -1.8033e+02 - logprior: -1.7252e+01
Epoch 3/10
10/10 - 1s - loss: 184.3545 - loglik: -1.7842e+02 - logprior: -5.9349e+00
Epoch 4/10
10/10 - 1s - loss: 178.4476 - loglik: -1.7756e+02 - logprior: -8.8812e-01
Epoch 5/10
10/10 - 1s - loss: 175.2977 - loglik: -1.7610e+02 - logprior: 0.7988
Epoch 6/10
10/10 - 1s - loss: 174.9112 - loglik: -1.7669e+02 - logprior: 1.7753
Epoch 7/10
10/10 - 1s - loss: 173.9590 - loglik: -1.7646e+02 - logprior: 2.5021
Epoch 8/10
10/10 - 1s - loss: 173.8266 - loglik: -1.7679e+02 - logprior: 2.9648
Epoch 9/10
10/10 - 1s - loss: 173.4820 - loglik: -1.7674e+02 - logprior: 3.2558
Epoch 10/10
10/10 - 1s - loss: 173.0930 - loglik: -1.7661e+02 - logprior: 3.5178
Fitted a model with MAP estimate = -173.0710
Time for alignment: 42.6499
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.5195 - loglik: -3.3026e+02 - logprior: -4.5258e+01
Epoch 2/10
10/10 - 1s - loss: 298.0308 - loglik: -2.8721e+02 - logprior: -1.0821e+01
Epoch 3/10
10/10 - 1s - loss: 251.1707 - loglik: -2.4619e+02 - logprior: -4.9800e+00
Epoch 4/10
10/10 - 1s - loss: 224.6469 - loglik: -2.2141e+02 - logprior: -3.2393e+00
Epoch 5/10
10/10 - 1s - loss: 212.1455 - loglik: -2.0984e+02 - logprior: -2.3017e+00
Epoch 6/10
10/10 - 1s - loss: 206.4556 - loglik: -2.0469e+02 - logprior: -1.7618e+00
Epoch 7/10
10/10 - 1s - loss: 204.9556 - loglik: -2.0347e+02 - logprior: -1.4892e+00
Epoch 8/10
10/10 - 1s - loss: 203.5420 - loglik: -2.0221e+02 - logprior: -1.3288e+00
Epoch 9/10
10/10 - 1s - loss: 203.2682 - loglik: -2.0205e+02 - logprior: -1.2222e+00
Epoch 10/10
10/10 - 1s - loss: 202.6337 - loglik: -2.0148e+02 - logprior: -1.1504e+00
Fitted a model with MAP estimate = -202.3289
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.4813 - loglik: -2.0027e+02 - logprior: -5.1213e+01
Epoch 2/2
10/10 - 1s - loss: 208.0417 - loglik: -1.8775e+02 - logprior: -2.0290e+01
Fitted a model with MAP estimate = -200.0776
expansions: [(0, 3)]
discards: [  0   9  18  21 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.7890 - loglik: -1.8633e+02 - logprior: -4.0456e+01
Epoch 2/2
10/10 - 1s - loss: 192.2532 - loglik: -1.8290e+02 - logprior: -9.3542e+00
Fitted a model with MAP estimate = -186.8475
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.3501 - loglik: -1.8509e+02 - logprior: -4.9263e+01
Epoch 2/10
10/10 - 1s - loss: 197.6184 - loglik: -1.8276e+02 - logprior: -1.4854e+01
Epoch 3/10
10/10 - 1s - loss: 186.4004 - loglik: -1.8220e+02 - logprior: -4.2020e+00
Epoch 4/10
10/10 - 1s - loss: 181.4244 - loglik: -1.8086e+02 - logprior: -5.6410e-01
Epoch 5/10
10/10 - 1s - loss: 179.4377 - loglik: -1.8032e+02 - logprior: 0.8780
Epoch 6/10
10/10 - 1s - loss: 178.6700 - loglik: -1.8057e+02 - logprior: 1.8990
Epoch 7/10
10/10 - 1s - loss: 177.8530 - loglik: -1.8050e+02 - logprior: 2.6446
Epoch 8/10
10/10 - 1s - loss: 177.4673 - loglik: -1.8054e+02 - logprior: 3.0700
Epoch 9/10
10/10 - 1s - loss: 177.0149 - loglik: -1.8042e+02 - logprior: 3.4039
Epoch 10/10
10/10 - 1s - loss: 176.6767 - loglik: -1.8041e+02 - logprior: 3.7338
Fitted a model with MAP estimate = -176.5424
Time for alignment: 42.1489
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.1867 - loglik: -3.2993e+02 - logprior: -4.5260e+01
Epoch 2/10
10/10 - 1s - loss: 298.4368 - loglik: -2.8762e+02 - logprior: -1.0816e+01
Epoch 3/10
10/10 - 1s - loss: 250.4916 - loglik: -2.4556e+02 - logprior: -4.9315e+00
Epoch 4/10
10/10 - 1s - loss: 220.8599 - loglik: -2.1758e+02 - logprior: -3.2821e+00
Epoch 5/10
10/10 - 1s - loss: 209.9252 - loglik: -2.0738e+02 - logprior: -2.5495e+00
Epoch 6/10
10/10 - 1s - loss: 206.3205 - loglik: -2.0431e+02 - logprior: -2.0057e+00
Epoch 7/10
10/10 - 1s - loss: 204.4838 - loglik: -2.0286e+02 - logprior: -1.6206e+00
Epoch 8/10
10/10 - 1s - loss: 203.8237 - loglik: -2.0245e+02 - logprior: -1.3778e+00
Epoch 9/10
10/10 - 1s - loss: 202.8800 - loglik: -2.0164e+02 - logprior: -1.2431e+00
Epoch 10/10
10/10 - 1s - loss: 203.0601 - loglik: -2.0191e+02 - logprior: -1.1514e+00
Fitted a model with MAP estimate = -202.5267
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.5649 - loglik: -2.0035e+02 - logprior: -5.1216e+01
Epoch 2/2
10/10 - 1s - loss: 207.2971 - loglik: -1.8707e+02 - logprior: -2.0224e+01
Fitted a model with MAP estimate = -200.0083
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.9288 - loglik: -1.8648e+02 - logprior: -4.0448e+01
Epoch 2/2
10/10 - 1s - loss: 192.2665 - loglik: -1.8291e+02 - logprior: -9.3545e+00
Fitted a model with MAP estimate = -187.0398
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.3870 - loglik: -1.8490e+02 - logprior: -4.9482e+01
Epoch 2/10
10/10 - 1s - loss: 199.0853 - loglik: -1.8354e+02 - logprior: -1.5544e+01
Epoch 3/10
10/10 - 1s - loss: 187.0656 - loglik: -1.8248e+02 - logprior: -4.5831e+00
Epoch 4/10
10/10 - 1s - loss: 181.8482 - loglik: -1.8121e+02 - logprior: -6.3666e-01
Epoch 5/10
10/10 - 1s - loss: 179.5754 - loglik: -1.8044e+02 - logprior: 0.8687
Epoch 6/10
10/10 - 1s - loss: 178.4036 - loglik: -1.8033e+02 - logprior: 1.9260
Epoch 7/10
10/10 - 1s - loss: 177.9311 - loglik: -1.8060e+02 - logprior: 2.6707
Epoch 8/10
10/10 - 1s - loss: 177.4019 - loglik: -1.8050e+02 - logprior: 3.0982
Epoch 9/10
10/10 - 1s - loss: 176.8172 - loglik: -1.8026e+02 - logprior: 3.4409
Epoch 10/10
10/10 - 1s - loss: 176.9152 - loglik: -1.8068e+02 - logprior: 3.7640
Fitted a model with MAP estimate = -176.5369
Time for alignment: 41.4659
Computed alignments with likelihoods: ['-173.3008', '-173.3486', '-173.0710', '-176.5424', '-176.5369']
Best model has likelihood: -173.0710  (prior= 3.6626 )
time for generating output: 0.1344
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9422573639957681
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 630.3934 - loglik: -6.0795e+02 - logprior: -2.2441e+01
Epoch 2/10
14/14 - 3s - loss: 538.7814 - loglik: -5.3684e+02 - logprior: -1.9444e+00
Epoch 3/10
14/14 - 3s - loss: 480.5070 - loglik: -4.7990e+02 - logprior: -6.0447e-01
Epoch 4/10
14/14 - 3s - loss: 460.6980 - loglik: -4.6029e+02 - logprior: -4.0840e-01
Epoch 5/10
14/14 - 3s - loss: 457.4198 - loglik: -4.5730e+02 - logprior: -1.1499e-01
Epoch 6/10
14/14 - 3s - loss: 455.3864 - loglik: -4.5543e+02 - logprior: 0.0464
Epoch 7/10
14/14 - 3s - loss: 455.5616 - loglik: -4.5573e+02 - logprior: 0.1654
Fitted a model with MAP estimate = -454.4943
expansions: [(14, 3), (16, 1), (28, 1), (29, 1), (30, 2), (31, 2), (40, 1), (41, 2), (42, 2), (50, 1), (51, 1), (54, 4), (63, 1), (90, 1), (110, 1), (112, 1), (113, 2), (114, 2), (115, 2), (118, 1), (119, 1), (120, 3), (124, 1), (153, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 476.6539 - loglik: -4.5019e+02 - logprior: -2.6468e+01
Epoch 2/2
14/14 - 4s - loss: 446.5235 - loglik: -4.3891e+02 - logprior: -7.6160e+00
Fitted a model with MAP estimate = -442.4892
expansions: [(139, 1)]
discards: [  0  14  49  54  75  76 211]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 468.0746 - loglik: -4.4243e+02 - logprior: -2.5640e+01
Epoch 2/2
14/14 - 4s - loss: 446.9973 - loglik: -4.4107e+02 - logprior: -5.9298e+00
Fitted a model with MAP estimate = -440.2148
expansions: [(0, 4), (67, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 459.4673 - loglik: -4.4113e+02 - logprior: -1.8335e+01
Epoch 2/10
14/14 - 4s - loss: 436.1938 - loglik: -4.3663e+02 - logprior: 0.4373
Epoch 3/10
14/14 - 4s - loss: 434.0331 - loglik: -4.3753e+02 - logprior: 3.4981
Epoch 4/10
14/14 - 4s - loss: 431.3590 - loglik: -4.3602e+02 - logprior: 4.6564
Epoch 5/10
14/14 - 4s - loss: 430.9203 - loglik: -4.3613e+02 - logprior: 5.2077
Epoch 6/10
14/14 - 4s - loss: 428.6997 - loglik: -4.3431e+02 - logprior: 5.6119
Epoch 7/10
14/14 - 4s - loss: 430.7581 - loglik: -4.3677e+02 - logprior: 6.0088
Fitted a model with MAP estimate = -429.0261
Time for alignment: 95.7022
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 630.0596 - loglik: -6.0763e+02 - logprior: -2.2432e+01
Epoch 2/10
14/14 - 3s - loss: 539.9326 - loglik: -5.3796e+02 - logprior: -1.9761e+00
Epoch 3/10
14/14 - 3s - loss: 480.8371 - loglik: -4.8018e+02 - logprior: -6.5970e-01
Epoch 4/10
14/14 - 3s - loss: 463.6611 - loglik: -4.6328e+02 - logprior: -3.8390e-01
Epoch 5/10
14/14 - 3s - loss: 458.9304 - loglik: -4.5879e+02 - logprior: -1.4080e-01
Epoch 6/10
14/14 - 3s - loss: 458.0966 - loglik: -4.5811e+02 - logprior: 0.0125
Epoch 7/10
14/14 - 3s - loss: 454.9019 - loglik: -4.5506e+02 - logprior: 0.1608
Epoch 8/10
14/14 - 3s - loss: 455.3134 - loglik: -4.5559e+02 - logprior: 0.2801
Fitted a model with MAP estimate = -455.3614
expansions: [(15, 1), (17, 1), (29, 1), (30, 1), (31, 2), (32, 2), (33, 1), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (55, 3), (62, 1), (63, 1), (81, 1), (91, 1), (92, 7), (113, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (154, 1), (164, 1), (166, 9), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 478.2100 - loglik: -4.5188e+02 - logprior: -2.6326e+01
Epoch 2/2
14/14 - 5s - loss: 445.5623 - loglik: -4.3795e+02 - logprior: -7.6096e+00
Fitted a model with MAP estimate = -440.9473
expansions: [(0, 4), (119, 1), (148, 1)]
discards: [  0  50 115 156 159 218]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 456.9391 - loglik: -4.3802e+02 - logprior: -1.8914e+01
Epoch 2/2
14/14 - 5s - loss: 433.8782 - loglik: -4.3366e+02 - logprior: -2.1610e-01
Fitted a model with MAP estimate = -430.3284
expansions: [(71, 2)]
discards: [  1   2   3  74  75  76 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 457.2614 - loglik: -4.3894e+02 - logprior: -1.8322e+01
Epoch 2/10
14/14 - 4s - loss: 436.3640 - loglik: -4.3664e+02 - logprior: 0.2793
Epoch 3/10
14/14 - 4s - loss: 432.7978 - loglik: -4.3597e+02 - logprior: 3.1767
Epoch 4/10
14/14 - 4s - loss: 429.9824 - loglik: -4.3439e+02 - logprior: 4.4075
Epoch 5/10
14/14 - 4s - loss: 428.8432 - loglik: -4.3408e+02 - logprior: 5.2407
Epoch 6/10
14/14 - 4s - loss: 427.3707 - loglik: -4.3307e+02 - logprior: 5.6977
Epoch 7/10
14/14 - 4s - loss: 428.4947 - loglik: -4.3458e+02 - logprior: 6.0888
Fitted a model with MAP estimate = -427.1178
Time for alignment: 99.6350
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 631.1195 - loglik: -6.0867e+02 - logprior: -2.2445e+01
Epoch 2/10
14/14 - 3s - loss: 538.6065 - loglik: -5.3663e+02 - logprior: -1.9764e+00
Epoch 3/10
14/14 - 3s - loss: 478.5818 - loglik: -4.7796e+02 - logprior: -6.1692e-01
Epoch 4/10
14/14 - 3s - loss: 466.8042 - loglik: -4.6674e+02 - logprior: -6.8520e-02
Epoch 5/10
14/14 - 3s - loss: 461.1261 - loglik: -4.6138e+02 - logprior: 0.2507
Epoch 6/10
14/14 - 3s - loss: 460.5449 - loglik: -4.6102e+02 - logprior: 0.4764
Epoch 7/10
14/14 - 3s - loss: 458.5893 - loglik: -4.5920e+02 - logprior: 0.6115
Epoch 8/10
14/14 - 3s - loss: 457.5911 - loglik: -4.5834e+02 - logprior: 0.7442
Epoch 9/10
14/14 - 3s - loss: 459.5311 - loglik: -4.6033e+02 - logprior: 0.7971
Fitted a model with MAP estimate = -457.9076
expansions: [(15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 2), (52, 1), (53, 1), (54, 3), (61, 2), (80, 1), (90, 1), (113, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (154, 1), (164, 2), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 479.9873 - loglik: -4.5359e+02 - logprior: -2.6393e+01
Epoch 2/2
14/14 - 4s - loss: 448.6757 - loglik: -4.4099e+02 - logprior: -7.6882e+00
Fitted a model with MAP estimate = -444.1816
expansions: [(0, 4), (141, 1)]
discards: [  0  52  90 149 152 205]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 461.0484 - loglik: -4.4219e+02 - logprior: -1.8856e+01
Epoch 2/2
14/14 - 4s - loss: 437.7267 - loglik: -4.3774e+02 - logprior: 0.0137
Fitted a model with MAP estimate = -435.2658
expansions: [(19, 2)]
discards: [ 1  2  3 80]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 457.4233 - loglik: -4.3920e+02 - logprior: -1.8219e+01
Epoch 2/10
14/14 - 4s - loss: 433.7304 - loglik: -4.3417e+02 - logprior: 0.4439
Epoch 3/10
14/14 - 4s - loss: 432.7846 - loglik: -4.3611e+02 - logprior: 3.3243
Epoch 4/10
14/14 - 4s - loss: 429.6794 - loglik: -4.3429e+02 - logprior: 4.6063
Epoch 5/10
14/14 - 4s - loss: 430.2387 - loglik: -4.3568e+02 - logprior: 5.4427
Fitted a model with MAP estimate = -428.8468
Time for alignment: 92.4528
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 630.1616 - loglik: -6.0774e+02 - logprior: -2.2425e+01
Epoch 2/10
14/14 - 3s - loss: 539.7120 - loglik: -5.3777e+02 - logprior: -1.9452e+00
Epoch 3/10
14/14 - 3s - loss: 480.7931 - loglik: -4.8032e+02 - logprior: -4.7736e-01
Epoch 4/10
14/14 - 3s - loss: 462.6893 - loglik: -4.6267e+02 - logprior: -1.7653e-02
Epoch 5/10
14/14 - 3s - loss: 458.0074 - loglik: -4.5831e+02 - logprior: 0.3070
Epoch 6/10
14/14 - 3s - loss: 457.6646 - loglik: -4.5819e+02 - logprior: 0.5214
Epoch 7/10
14/14 - 3s - loss: 454.4418 - loglik: -4.5510e+02 - logprior: 0.6544
Epoch 8/10
14/14 - 3s - loss: 454.9909 - loglik: -4.5580e+02 - logprior: 0.8103
Fitted a model with MAP estimate = -454.2951
expansions: [(15, 2), (16, 2), (28, 1), (29, 2), (30, 2), (31, 2), (39, 2), (40, 2), (41, 2), (49, 1), (50, 1), (51, 1), (52, 3), (89, 1), (90, 7), (113, 1), (114, 1), (115, 4), (117, 2), (118, 2), (119, 3), (153, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 477.0630 - loglik: -4.5068e+02 - logprior: -2.6386e+01
Epoch 2/2
14/14 - 5s - loss: 447.5161 - loglik: -4.3980e+02 - logprior: -7.7178e+00
Fitted a model with MAP estimate = -441.3971
expansions: [(0, 4), (119, 1), (148, 1)]
discards: [  0  18  19  50 114 115 156 159 218]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 459.1974 - loglik: -4.4018e+02 - logprior: -1.9019e+01
Epoch 2/2
14/14 - 4s - loss: 434.6604 - loglik: -4.3422e+02 - logprior: -4.4399e-01
Fitted a model with MAP estimate = -431.8038
expansions: [(16, 1)]
discards: [  1   2   3  17 114]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 456.1567 - loglik: -4.3770e+02 - logprior: -1.8452e+01
Epoch 2/10
14/14 - 4s - loss: 432.7852 - loglik: -4.3299e+02 - logprior: 0.2077
Epoch 3/10
14/14 - 4s - loss: 431.5483 - loglik: -4.3456e+02 - logprior: 3.0101
Epoch 4/10
14/14 - 4s - loss: 428.3647 - loglik: -4.3261e+02 - logprior: 4.2474
Epoch 5/10
14/14 - 4s - loss: 427.4606 - loglik: -4.3257e+02 - logprior: 5.1065
Epoch 6/10
14/14 - 4s - loss: 426.1427 - loglik: -4.3170e+02 - logprior: 5.5585
Epoch 7/10
14/14 - 4s - loss: 427.1284 - loglik: -4.3308e+02 - logprior: 5.9549
Fitted a model with MAP estimate = -426.1616
Time for alignment: 98.6587
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 630.2666 - loglik: -6.0782e+02 - logprior: -2.2442e+01
Epoch 2/10
14/14 - 3s - loss: 540.1292 - loglik: -5.3819e+02 - logprior: -1.9403e+00
Epoch 3/10
14/14 - 3s - loss: 477.8403 - loglik: -4.7714e+02 - logprior: -6.9899e-01
Epoch 4/10
14/14 - 3s - loss: 463.3031 - loglik: -4.6301e+02 - logprior: -2.9752e-01
Epoch 5/10
14/14 - 3s - loss: 456.2005 - loglik: -4.5620e+02 - logprior: -5.1795e-03
Epoch 6/10
14/14 - 3s - loss: 455.6027 - loglik: -4.5580e+02 - logprior: 0.1953
Epoch 7/10
14/14 - 3s - loss: 455.6301 - loglik: -4.5594e+02 - logprior: 0.3118
Fitted a model with MAP estimate = -454.6052
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 2), (50, 1), (51, 1), (54, 4), (80, 1), (89, 1), (112, 1), (113, 2), (114, 2), (115, 3), (118, 1), (119, 1), (120, 3), (130, 2), (160, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 477.0773 - loglik: -4.5056e+02 - logprior: -2.6522e+01
Epoch 2/2
14/14 - 4s - loss: 447.6390 - loglik: -4.3986e+02 - logprior: -7.7831e+00
Fitted a model with MAP estimate = -443.1552
expansions: [(70, 2), (138, 1)]
discards: [  0  18  49  54  74  75  76 143 166 206]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 470.0866 - loglik: -4.4447e+02 - logprior: -2.5621e+01
Epoch 2/2
14/14 - 4s - loss: 446.5186 - loglik: -4.4052e+02 - logprior: -6.0025e+00
Fitted a model with MAP estimate = -441.0918
expansions: [(0, 4), (75, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 457.6861 - loglik: -4.3939e+02 - logprior: -1.8295e+01
Epoch 2/10
14/14 - 4s - loss: 436.6782 - loglik: -4.3713e+02 - logprior: 0.4557
Epoch 3/10
14/14 - 4s - loss: 430.5317 - loglik: -4.3405e+02 - logprior: 3.5213
Epoch 4/10
14/14 - 4s - loss: 432.1918 - loglik: -4.3692e+02 - logprior: 4.7237
Fitted a model with MAP estimate = -429.3015
Time for alignment: 80.6018
Computed alignments with likelihoods: ['-429.0261', '-427.1178', '-428.8468', '-426.1616', '-429.3015']
Best model has likelihood: -426.1616  (prior= 6.1514 )
time for generating output: 0.2194
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8683074265975821
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.5211 - loglik: -6.9695e+02 - logprior: -4.0574e+01
Epoch 2/10
11/11 - 8s - loss: 629.2103 - loglik: -6.2633e+02 - logprior: -2.8817e+00
Epoch 3/10
11/11 - 7s - loss: 545.4318 - loglik: -5.4664e+02 - logprior: 1.2095
Epoch 4/10
11/11 - 8s - loss: 496.8658 - loglik: -4.9872e+02 - logprior: 1.8531
Epoch 5/10
11/11 - 9s - loss: 476.5229 - loglik: -4.7828e+02 - logprior: 1.7581
Epoch 6/10
11/11 - 8s - loss: 470.0157 - loglik: -4.7186e+02 - logprior: 1.8487
Epoch 7/10
11/11 - 8s - loss: 465.4775 - loglik: -4.6774e+02 - logprior: 2.2618
Epoch 8/10
11/11 - 8s - loss: 461.7449 - loglik: -4.6427e+02 - logprior: 2.5208
Epoch 9/10
11/11 - 8s - loss: 463.9900 - loglik: -4.6683e+02 - logprior: 2.8435
Fitted a model with MAP estimate = -462.2893
expansions: [(20, 1), (21, 3), (26, 1), (43, 1), (47, 1), (49, 1), (61, 1), (63, 1), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (93, 1), (103, 5), (104, 1), (106, 3), (107, 1), (136, 1), (137, 1), (159, 2), (161, 5), (162, 1), (177, 1), (180, 1), (181, 2), (197, 5), (198, 2), (200, 2), (201, 3), (223, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 508.8258 - loglik: -4.6165e+02 - logprior: -4.7174e+01
Epoch 2/2
11/11 - 11s - loss: 454.6523 - loglik: -4.4230e+02 - logprior: -1.2353e+01
Fitted a model with MAP estimate = -444.2172
expansions: [(0, 2), (253, 2)]
discards: [  0 121 122 127 128 293 295 296]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 476.4691 - loglik: -4.4172e+02 - logprior: -3.4750e+01
Epoch 2/2
11/11 - 10s - loss: 437.5101 - loglik: -4.3756e+02 - logprior: 0.0457
Fitted a model with MAP estimate = -430.4792
expansions: [(24, 1), (292, 1), (293, 1)]
discards: [  0 247 248 275]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 484.8269 - loglik: -4.4020e+02 - logprior: -4.4631e+01
Epoch 2/10
11/11 - 11s - loss: 450.9821 - loglik: -4.4108e+02 - logprior: -9.9020e+00
Epoch 3/10
11/11 - 11s - loss: 430.1705 - loglik: -4.3251e+02 - logprior: 2.3435
Epoch 4/10
11/11 - 10s - loss: 421.9985 - loglik: -4.3184e+02 - logprior: 9.8450
Epoch 5/10
11/11 - 10s - loss: 421.9718 - loglik: -4.3407e+02 - logprior: 12.0991
Epoch 6/10
11/11 - 10s - loss: 417.2624 - loglik: -4.3067e+02 - logprior: 13.4043
Epoch 7/10
11/11 - 11s - loss: 416.6512 - loglik: -4.3099e+02 - logprior: 14.3375
Epoch 8/10
11/11 - 10s - loss: 415.0371 - loglik: -4.3017e+02 - logprior: 15.1300
Epoch 9/10
11/11 - 11s - loss: 414.9808 - loglik: -4.3059e+02 - logprior: 15.6063
Epoch 10/10
11/11 - 10s - loss: 412.5117 - loglik: -4.2878e+02 - logprior: 16.2722
Fitted a model with MAP estimate = -414.0016
Time for alignment: 252.8885
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.4033 - loglik: -6.9683e+02 - logprior: -4.0575e+01
Epoch 2/10
11/11 - 8s - loss: 629.1663 - loglik: -6.2630e+02 - logprior: -2.8660e+00
Epoch 3/10
11/11 - 8s - loss: 541.5621 - loglik: -5.4285e+02 - logprior: 1.2841
Epoch 4/10
11/11 - 9s - loss: 492.7485 - loglik: -4.9443e+02 - logprior: 1.6776
Epoch 5/10
11/11 - 9s - loss: 476.5776 - loglik: -4.7844e+02 - logprior: 1.8582
Epoch 6/10
11/11 - 8s - loss: 471.8778 - loglik: -4.7408e+02 - logprior: 2.1975
Epoch 7/10
11/11 - 8s - loss: 465.9775 - loglik: -4.6847e+02 - logprior: 2.4894
Epoch 8/10
11/11 - 8s - loss: 469.3014 - loglik: -4.7206e+02 - logprior: 2.7564
Fitted a model with MAP estimate = -466.8312
expansions: [(19, 4), (21, 1), (22, 1), (26, 1), (36, 1), (47, 1), (49, 1), (52, 2), (60, 1), (62, 1), (64, 1), (77, 1), (80, 1), (91, 2), (92, 1), (102, 4), (103, 1), (104, 1), (106, 1), (161, 2), (163, 5), (180, 1), (181, 1), (182, 1), (183, 2), (197, 1), (198, 2), (199, 4), (201, 2), (202, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 512.3376 - loglik: -4.6538e+02 - logprior: -4.6960e+01
Epoch 2/2
11/11 - 11s - loss: 455.8535 - loglik: -4.4356e+02 - logprior: -1.2296e+01
Fitted a model with MAP estimate = -447.1022
expansions: [(0, 2), (246, 1)]
discards: [  0  21  61 122 123 124 195 297 298]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 478.4890 - loglik: -4.4360e+02 - logprior: -3.4892e+01
Epoch 2/2
11/11 - 10s - loss: 439.2282 - loglik: -4.3931e+02 - logprior: 0.0822
Fitted a model with MAP estimate = -432.8851
expansions: [(232, 1), (293, 1)]
discards: [  0 274]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 488.6792 - loglik: -4.4397e+02 - logprior: -4.4710e+01
Epoch 2/10
11/11 - 10s - loss: 447.2349 - loglik: -4.3708e+02 - logprior: -1.0152e+01
Epoch 3/10
11/11 - 11s - loss: 435.0234 - loglik: -4.3603e+02 - logprior: 1.0031
Epoch 4/10
11/11 - 9s - loss: 424.4669 - loglik: -4.3389e+02 - logprior: 9.4258
Epoch 5/10
11/11 - 12s - loss: 423.5830 - loglik: -4.3565e+02 - logprior: 12.0640
Epoch 6/10
11/11 - 11s - loss: 418.5676 - loglik: -4.3189e+02 - logprior: 13.3269
Epoch 7/10
11/11 - 10s - loss: 419.9936 - loglik: -4.3423e+02 - logprior: 14.2396
Fitted a model with MAP estimate = -418.3455
Time for alignment: 214.0578
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.8145 - loglik: -6.9725e+02 - logprior: -4.0566e+01
Epoch 2/10
11/11 - 8s - loss: 625.3708 - loglik: -6.2244e+02 - logprior: -2.9355e+00
Epoch 3/10
11/11 - 8s - loss: 542.0709 - loglik: -5.4330e+02 - logprior: 1.2307
Epoch 4/10
11/11 - 9s - loss: 491.4032 - loglik: -4.9321e+02 - logprior: 1.8047
Epoch 5/10
11/11 - 8s - loss: 478.8044 - loglik: -4.8105e+02 - logprior: 2.2427
Epoch 6/10
11/11 - 8s - loss: 473.3176 - loglik: -4.7578e+02 - logprior: 2.4644
Epoch 7/10
11/11 - 8s - loss: 468.9715 - loglik: -4.7173e+02 - logprior: 2.7535
Epoch 8/10
11/11 - 9s - loss: 468.5038 - loglik: -4.7153e+02 - logprior: 3.0311
Epoch 9/10
11/11 - 7s - loss: 467.4572 - loglik: -4.7069e+02 - logprior: 3.2353
Epoch 10/10
11/11 - 9s - loss: 466.4559 - loglik: -4.6998e+02 - logprior: 3.5254
Fitted a model with MAP estimate = -466.0409
expansions: [(22, 4), (29, 1), (36, 1), (47, 1), (52, 2), (64, 1), (66, 1), (79, 1), (80, 2), (81, 2), (91, 1), (93, 1), (103, 3), (104, 1), (105, 2), (106, 1), (108, 4), (129, 1), (134, 1), (137, 1), (161, 2), (162, 5), (179, 1), (181, 2), (182, 1), (197, 1), (198, 2), (199, 4), (201, 2), (202, 5), (223, 2), (224, 1), (225, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 513.6035 - loglik: -4.6660e+02 - logprior: -4.7002e+01
Epoch 2/2
11/11 - 11s - loss: 456.6523 - loglik: -4.4409e+02 - logprior: -1.2566e+01
Fitted a model with MAP estimate = -446.5093
expansions: [(0, 3), (201, 1)]
discards: [  0  58  90 120 121 131 132 133 134 160 297 299 300]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 479.8566 - loglik: -4.4495e+02 - logprior: -3.4904e+01
Epoch 2/2
11/11 - 11s - loss: 443.2263 - loglik: -4.4303e+02 - logprior: -1.9987e-01
Fitted a model with MAP estimate = -433.3525
expansions: [(22, 3), (234, 1), (243, 1), (291, 1), (292, 1)]
discards: [  0 274]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 486.8898 - loglik: -4.4230e+02 - logprior: -4.4586e+01
Epoch 2/10
11/11 - 11s - loss: 448.0661 - loglik: -4.3801e+02 - logprior: -1.0061e+01
Epoch 3/10
11/11 - 11s - loss: 431.3926 - loglik: -4.3238e+02 - logprior: 0.9889
Epoch 4/10
11/11 - 10s - loss: 423.4916 - loglik: -4.3315e+02 - logprior: 9.6548
Epoch 5/10
11/11 - 11s - loss: 418.1936 - loglik: -4.3046e+02 - logprior: 12.2648
Epoch 6/10
11/11 - 11s - loss: 415.0174 - loglik: -4.2857e+02 - logprior: 13.5564
Epoch 7/10
11/11 - 11s - loss: 416.2693 - loglik: -4.3079e+02 - logprior: 14.5248
Fitted a model with MAP estimate = -415.4397
Time for alignment: 230.8535
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 735.7876 - loglik: -6.9521e+02 - logprior: -4.0579e+01
Epoch 2/10
11/11 - 8s - loss: 631.3324 - loglik: -6.2846e+02 - logprior: -2.8696e+00
Epoch 3/10
11/11 - 8s - loss: 542.5768 - loglik: -5.4414e+02 - logprior: 1.5599
Epoch 4/10
11/11 - 8s - loss: 493.6116 - loglik: -4.9569e+02 - logprior: 2.0755
Epoch 5/10
11/11 - 8s - loss: 480.9411 - loglik: -4.8336e+02 - logprior: 2.4181
Epoch 6/10
11/11 - 7s - loss: 468.0871 - loglik: -4.7091e+02 - logprior: 2.8181
Epoch 7/10
11/11 - 8s - loss: 469.2037 - loglik: -4.7231e+02 - logprior: 3.1013
Fitted a model with MAP estimate = -467.4046
expansions: [(19, 4), (21, 1), (22, 1), (26, 1), (35, 1), (36, 1), (52, 2), (64, 1), (66, 1), (79, 1), (80, 1), (81, 1), (92, 2), (93, 1), (103, 3), (104, 1), (105, 2), (106, 1), (121, 2), (150, 1), (163, 5), (181, 1), (182, 5), (196, 1), (197, 4), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0 208]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 513.2010 - loglik: -4.6596e+02 - logprior: -4.7245e+01
Epoch 2/2
11/11 - 11s - loss: 459.5056 - loglik: -4.4679e+02 - logprior: -1.2711e+01
Fitted a model with MAP estimate = -447.5683
expansions: [(0, 2), (218, 1), (242, 2)]
discards: [  0  21  61 122 123 146]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 475.3477 - loglik: -4.4057e+02 - logprior: -3.4778e+01
Epoch 2/2
11/11 - 11s - loss: 435.8622 - loglik: -4.3607e+02 - logprior: 0.2063
Fitted a model with MAP estimate = -429.1681
expansions: [(243, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 482.4276 - loglik: -4.3790e+02 - logprior: -4.4530e+01
Epoch 2/10
11/11 - 10s - loss: 443.1140 - loglik: -4.3359e+02 - logprior: -9.5242e+00
Epoch 3/10
11/11 - 11s - loss: 429.9989 - loglik: -4.3234e+02 - logprior: 2.3372
Epoch 4/10
11/11 - 12s - loss: 420.7292 - loglik: -4.3088e+02 - logprior: 10.1466
Epoch 5/10
11/11 - 11s - loss: 418.4179 - loglik: -4.3096e+02 - logprior: 12.5408
Epoch 6/10
11/11 - 11s - loss: 414.3913 - loglik: -4.2813e+02 - logprior: 13.7410
Epoch 7/10
11/11 - 11s - loss: 413.5703 - loglik: -4.2825e+02 - logprior: 14.6836
Epoch 8/10
11/11 - 11s - loss: 416.6419 - loglik: -4.3203e+02 - logprior: 15.3911
Fitted a model with MAP estimate = -413.6845
Time for alignment: 218.3593
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.5803 - loglik: -6.9702e+02 - logprior: -4.0565e+01
Epoch 2/10
11/11 - 8s - loss: 627.9007 - loglik: -6.2504e+02 - logprior: -2.8599e+00
Epoch 3/10
11/11 - 8s - loss: 541.7802 - loglik: -5.4314e+02 - logprior: 1.3552
Epoch 4/10
11/11 - 8s - loss: 489.2958 - loglik: -4.9110e+02 - logprior: 1.8065
Epoch 5/10
11/11 - 8s - loss: 478.7028 - loglik: -4.8057e+02 - logprior: 1.8715
Epoch 6/10
11/11 - 8s - loss: 472.1017 - loglik: -4.7419e+02 - logprior: 2.0849
Epoch 7/10
11/11 - 9s - loss: 469.8333 - loglik: -4.7238e+02 - logprior: 2.5486
Epoch 8/10
11/11 - 8s - loss: 466.3422 - loglik: -4.6913e+02 - logprior: 2.7841
Epoch 9/10
11/11 - 8s - loss: 466.2539 - loglik: -4.6927e+02 - logprior: 3.0171
Epoch 10/10
11/11 - 8s - loss: 463.4957 - loglik: -4.6680e+02 - logprior: 3.3045
Fitted a model with MAP estimate = -465.0914
expansions: [(22, 4), (33, 1), (36, 1), (42, 1), (52, 2), (64, 1), (66, 1), (79, 1), (80, 1), (81, 1), (92, 2), (93, 1), (103, 3), (104, 1), (105, 1), (107, 1), (108, 4), (129, 1), (134, 1), (137, 1), (161, 2), (163, 4), (180, 1), (181, 1), (182, 3), (198, 6), (200, 2), (201, 5), (220, 1), (222, 1), (224, 1), (225, 1)]
discards: [  0   1 209]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 512.6111 - loglik: -4.6540e+02 - logprior: -4.7210e+01
Epoch 2/2
11/11 - 11s - loss: 455.1682 - loglik: -4.4232e+02 - logprior: -1.2848e+01
Fitted a model with MAP estimate = -445.2464
expansions: [(0, 3), (218, 1), (245, 1)]
discards: [  0  58 119 120 129 130 131 158]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 473.3372 - loglik: -4.3843e+02 - logprior: -3.4912e+01
Epoch 2/2
11/11 - 11s - loss: 436.0921 - loglik: -4.3593e+02 - logprior: -1.6090e-01
Fitted a model with MAP estimate = -428.1717
expansions: [(22, 2), (194, 1), (236, 1), (273, 1)]
discards: [  0 192 247 292 293]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 486.6480 - loglik: -4.4218e+02 - logprior: -4.4468e+01
Epoch 2/10
11/11 - 11s - loss: 445.8810 - loglik: -4.3595e+02 - logprior: -9.9340e+00
Epoch 3/10
11/11 - 11s - loss: 433.8755 - loglik: -4.3478e+02 - logprior: 0.9046
Epoch 4/10
11/11 - 12s - loss: 422.7710 - loglik: -4.3236e+02 - logprior: 9.5887
Epoch 5/10
11/11 - 11s - loss: 420.5392 - loglik: -4.3271e+02 - logprior: 12.1735
Epoch 6/10
11/11 - 11s - loss: 416.8880 - loglik: -4.3014e+02 - logprior: 13.2542
Epoch 7/10
11/11 - 11s - loss: 413.4417 - loglik: -4.2761e+02 - logprior: 14.1694
Epoch 8/10
11/11 - 11s - loss: 417.4023 - loglik: -4.3223e+02 - logprior: 14.8232
Fitted a model with MAP estimate = -414.4787
Time for alignment: 242.0639
Computed alignments with likelihoods: ['-414.0016', '-418.3455', '-415.4397', '-413.6845', '-414.4787']
Best model has likelihood: -413.6845  (prior= 15.6045 )
time for generating output: 0.4913
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9123233727125318
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 816.5231 - loglik: -8.1058e+02 - logprior: -5.9394e+00
Epoch 2/10
21/21 - 15s - loss: 697.4329 - loglik: -6.9667e+02 - logprior: -7.6420e-01
Epoch 3/10
21/21 - 15s - loss: 645.2877 - loglik: -6.4246e+02 - logprior: -2.8296e+00
Epoch 4/10
21/21 - 15s - loss: 634.7258 - loglik: -6.3192e+02 - logprior: -2.8015e+00
Epoch 5/10
21/21 - 15s - loss: 631.3892 - loglik: -6.2879e+02 - logprior: -2.5976e+00
Epoch 6/10
21/21 - 15s - loss: 633.4688 - loglik: -6.3076e+02 - logprior: -2.7066e+00
Fitted a model with MAP estimate = -631.2948
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (54, 2), (55, 1), (58, 1), (60, 2), (61, 1), (62, 1), (64, 2), (66, 1), (75, 1), (76, 1), (77, 1), (78, 1), (80, 1), (81, 1), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (95, 1), (102, 1), (104, 1), (111, 1), (113, 1), (115, 1), (117, 1), (130, 1), (136, 1), (139, 1), (142, 1), (153, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 1), (181, 1), (182, 1), (184, 1), (188, 1), (190, 1), (191, 1), (192, 1), (194, 2), (195, 1), (196, 1), (197, 1), (201, 1), (210, 1), (212, 1), (213, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 629.4280 - loglik: -6.2397e+02 - logprior: -5.4600e+00
Epoch 2/2
21/21 - 23s - loss: 604.8013 - loglik: -6.0615e+02 - logprior: 1.3498
Fitted a model with MAP estimate = -602.2611
expansions: []
discards: [250 299 336]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 612.4960 - loglik: -6.0781e+02 - logprior: -4.6819e+00
Epoch 2/2
21/21 - 22s - loss: 601.8235 - loglik: -6.0377e+02 - logprior: 1.9470
Fitted a model with MAP estimate = -600.9613
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 610.6885 - loglik: -6.0640e+02 - logprior: -4.2932e+00
Epoch 2/10
21/21 - 22s - loss: 602.3451 - loglik: -6.0463e+02 - logprior: 2.2834
Epoch 3/10
21/21 - 23s - loss: 602.3317 - loglik: -6.0528e+02 - logprior: 2.9507
Epoch 4/10
21/21 - 22s - loss: 596.8813 - loglik: -6.0037e+02 - logprior: 3.4849
Epoch 5/10
21/21 - 22s - loss: 595.9727 - loglik: -5.9966e+02 - logprior: 3.6874
Epoch 6/10
21/21 - 22s - loss: 594.3256 - loglik: -5.9827e+02 - logprior: 3.9447
Epoch 7/10
21/21 - 22s - loss: 595.8345 - loglik: -6.0000e+02 - logprior: 4.1660
Fitted a model with MAP estimate = -594.2099
Time for alignment: 410.8457
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 816.4746 - loglik: -8.1052e+02 - logprior: -5.9511e+00
Epoch 2/10
21/21 - 15s - loss: 697.7530 - loglik: -6.9712e+02 - logprior: -6.3378e-01
Epoch 3/10
21/21 - 15s - loss: 647.6511 - loglik: -6.4532e+02 - logprior: -2.3336e+00
Epoch 4/10
21/21 - 15s - loss: 635.5933 - loglik: -6.3334e+02 - logprior: -2.2508e+00
Epoch 5/10
21/21 - 15s - loss: 634.3093 - loglik: -6.3215e+02 - logprior: -2.1635e+00
Epoch 6/10
21/21 - 15s - loss: 634.5906 - loglik: -6.3239e+02 - logprior: -2.2005e+00
Fitted a model with MAP estimate = -633.0190
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 2), (55, 1), (61, 2), (62, 2), (64, 2), (74, 1), (75, 3), (76, 1), (77, 1), (82, 1), (83, 1), (86, 1), (87, 1), (91, 1), (92, 1), (93, 1), (95, 1), (98, 1), (102, 1), (107, 1), (110, 1), (115, 1), (117, 1), (130, 1), (136, 1), (139, 1), (142, 1), (153, 1), (154, 1), (155, 1), (156, 1), (158, 1), (160, 1), (161, 3), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (211, 1), (212, 1), (218, 1), (221, 1), (222, 2), (228, 2), (229, 2), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 628.8087 - loglik: -6.2326e+02 - logprior: -5.5437e+00
Epoch 2/2
21/21 - 22s - loss: 608.1978 - loglik: -6.0897e+02 - logprior: 0.7743
Fitted a model with MAP estimate = -603.0717
expansions: [(19, 1), (75, 1)]
discards: [285 295 334]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 611.1193 - loglik: -6.0651e+02 - logprior: -4.6106e+00
Epoch 2/2
21/21 - 22s - loss: 605.4319 - loglik: -6.0741e+02 - logprior: 1.9803
Fitted a model with MAP estimate = -600.9366
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 612.8577 - loglik: -6.0876e+02 - logprior: -4.0996e+00
Epoch 2/10
21/21 - 22s - loss: 599.5244 - loglik: -6.0201e+02 - logprior: 2.4828
Epoch 3/10
21/21 - 22s - loss: 602.3181 - loglik: -6.0549e+02 - logprior: 3.1762
Fitted a model with MAP estimate = -597.8894
Time for alignment: 320.7098
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 817.7609 - loglik: -8.1182e+02 - logprior: -5.9382e+00
Epoch 2/10
21/21 - 15s - loss: 692.3151 - loglik: -6.9167e+02 - logprior: -6.4472e-01
Epoch 3/10
21/21 - 15s - loss: 644.2220 - loglik: -6.4170e+02 - logprior: -2.5218e+00
Epoch 4/10
21/21 - 15s - loss: 636.8981 - loglik: -6.3430e+02 - logprior: -2.5941e+00
Epoch 5/10
21/21 - 15s - loss: 631.9382 - loglik: -6.2944e+02 - logprior: -2.4968e+00
Epoch 6/10
21/21 - 15s - loss: 635.3223 - loglik: -6.3281e+02 - logprior: -2.5116e+00
Fitted a model with MAP estimate = -631.9801
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 3), (55, 1), (58, 1), (60, 2), (61, 2), (62, 1), (64, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (79, 1), (80, 1), (81, 1), (83, 1), (85, 1), (86, 1), (91, 1), (92, 1), (94, 1), (97, 1), (101, 1), (102, 1), (109, 1), (116, 1), (129, 1), (135, 1), (138, 1), (141, 1), (145, 1), (153, 1), (154, 1), (157, 1), (158, 1), (161, 1), (162, 1), (171, 1), (181, 1), (185, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (210, 1), (211, 1), (213, 1), (214, 1), (215, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 629.7131 - loglik: -6.2410e+02 - logprior: -5.6144e+00
Epoch 2/2
21/21 - 22s - loss: 602.6073 - loglik: -6.0367e+02 - logprior: 1.0642
Fitted a model with MAP estimate = -603.2205
expansions: [(19, 1), (76, 1), (145, 1)]
discards: [294 333]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 612.5544 - loglik: -6.0795e+02 - logprior: -4.6015e+00
Epoch 2/2
21/21 - 22s - loss: 602.2708 - loglik: -6.0420e+02 - logprior: 1.9262
Fitted a model with MAP estimate = -600.8290
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 612.0575 - loglik: -6.0783e+02 - logprior: -4.2228e+00
Epoch 2/10
21/21 - 22s - loss: 600.5025 - loglik: -6.0289e+02 - logprior: 2.3869
Epoch 3/10
21/21 - 23s - loss: 600.3204 - loglik: -6.0337e+02 - logprior: 3.0515
Epoch 4/10
21/21 - 22s - loss: 597.6288 - loglik: -6.0114e+02 - logprior: 3.5070
Epoch 5/10
21/21 - 22s - loss: 595.8950 - loglik: -5.9965e+02 - logprior: 3.7529
Epoch 6/10
21/21 - 22s - loss: 595.3351 - loglik: -5.9927e+02 - logprior: 3.9321
Epoch 7/10
21/21 - 23s - loss: 595.8568 - loglik: -6.0007e+02 - logprior: 4.2160
Fitted a model with MAP estimate = -594.1402
Time for alignment: 410.5823
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 815.9622 - loglik: -8.1003e+02 - logprior: -5.9350e+00
Epoch 2/10
21/21 - 15s - loss: 695.9997 - loglik: -6.9526e+02 - logprior: -7.4252e-01
Epoch 3/10
21/21 - 15s - loss: 645.2256 - loglik: -6.4256e+02 - logprior: -2.6661e+00
Epoch 4/10
21/21 - 15s - loss: 637.3561 - loglik: -6.3481e+02 - logprior: -2.5501e+00
Epoch 5/10
21/21 - 15s - loss: 632.3491 - loglik: -6.2990e+02 - logprior: -2.4464e+00
Epoch 6/10
21/21 - 15s - loss: 633.9343 - loglik: -6.3146e+02 - logprior: -2.4779e+00
Fitted a model with MAP estimate = -632.6176
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (54, 3), (55, 2), (59, 1), (60, 1), (61, 1), (62, 2), (64, 1), (66, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (84, 1), (85, 1), (86, 1), (93, 1), (94, 1), (95, 1), (102, 1), (103, 1), (110, 1), (112, 1), (114, 1), (116, 1), (136, 1), (138, 1), (139, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (208, 1), (212, 1), (213, 1), (215, 1), (219, 1), (223, 1), (227, 1), (228, 2), (229, 2), (231, 1), (239, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 630.1315 - loglik: -6.2453e+02 - logprior: -5.6063e+00
Epoch 2/2
21/21 - 23s - loss: 605.6976 - loglik: -6.0657e+02 - logprior: 0.8746
Fitted a model with MAP estimate = -602.8344
expansions: []
discards: [251 297 336]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 612.9223 - loglik: -6.0827e+02 - logprior: -4.6505e+00
Epoch 2/2
21/21 - 22s - loss: 604.1122 - loglik: -6.0602e+02 - logprior: 1.9028
Fitted a model with MAP estimate = -601.4337
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 612.2843 - loglik: -6.0812e+02 - logprior: -4.1625e+00
Epoch 2/10
21/21 - 22s - loss: 601.1932 - loglik: -6.0360e+02 - logprior: 2.4082
Epoch 3/10
21/21 - 22s - loss: 600.8001 - loglik: -6.0379e+02 - logprior: 2.9935
Epoch 4/10
21/21 - 22s - loss: 598.5435 - loglik: -6.0196e+02 - logprior: 3.4133
Epoch 5/10
21/21 - 22s - loss: 597.5944 - loglik: -6.0127e+02 - logprior: 3.6754
Epoch 6/10
21/21 - 22s - loss: 595.0472 - loglik: -5.9892e+02 - logprior: 3.8770
Epoch 7/10
21/21 - 22s - loss: 596.0705 - loglik: -6.0027e+02 - logprior: 4.2027
Fitted a model with MAP estimate = -594.6850
Time for alignment: 408.6784
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 816.9144 - loglik: -8.1099e+02 - logprior: -5.9200e+00
Epoch 2/10
21/21 - 15s - loss: 692.6053 - loglik: -6.9197e+02 - logprior: -6.3147e-01
Epoch 3/10
21/21 - 15s - loss: 646.7349 - loglik: -6.4414e+02 - logprior: -2.5998e+00
Epoch 4/10
21/21 - 15s - loss: 636.1909 - loglik: -6.3360e+02 - logprior: -2.5924e+00
Epoch 5/10
21/21 - 15s - loss: 631.1465 - loglik: -6.2858e+02 - logprior: -2.5693e+00
Epoch 6/10
21/21 - 15s - loss: 634.5223 - loglik: -6.3193e+02 - logprior: -2.5940e+00
Fitted a model with MAP estimate = -631.8125
expansions: [(13, 1), (14, 2), (15, 1), (49, 1), (51, 2), (53, 2), (54, 1), (60, 2), (61, 2), (62, 1), (63, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (79, 1), (80, 1), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (99, 1), (100, 1), (109, 1), (116, 1), (130, 1), (135, 1), (138, 1), (152, 1), (153, 1), (154, 1), (155, 1), (158, 1), (161, 1), (162, 1), (171, 1), (181, 1), (184, 1), (187, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (210, 1), (212, 1), (213, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (234, 1), (239, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 630.3757 - loglik: -6.2467e+02 - logprior: -5.7009e+00
Epoch 2/2
21/21 - 23s - loss: 606.5895 - loglik: -6.0730e+02 - logprior: 0.7137
Fitted a model with MAP estimate = -604.0794
expansions: [(19, 1), (145, 1)]
discards: [248 294 333]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 613.7646 - loglik: -6.0912e+02 - logprior: -4.6455e+00
Epoch 2/2
21/21 - 22s - loss: 603.8926 - loglik: -6.0573e+02 - logprior: 1.8388
Fitted a model with MAP estimate = -601.5433
expansions: []
discards: [60]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 611.3793 - loglik: -6.0720e+02 - logprior: -4.1785e+00
Epoch 2/10
21/21 - 22s - loss: 603.9183 - loglik: -6.0618e+02 - logprior: 2.2651
Epoch 3/10
21/21 - 22s - loss: 599.9088 - loglik: -6.0287e+02 - logprior: 2.9644
Epoch 4/10
21/21 - 22s - loss: 597.9363 - loglik: -6.0131e+02 - logprior: 3.3705
Epoch 5/10
21/21 - 22s - loss: 596.1053 - loglik: -5.9981e+02 - logprior: 3.7083
Epoch 6/10
21/21 - 23s - loss: 599.9526 - loglik: -6.0388e+02 - logprior: 3.9298
Fitted a model with MAP estimate = -595.1975
Time for alignment: 386.5882
Computed alignments with likelihoods: ['-594.2099', '-597.8894', '-594.1402', '-594.6850', '-595.1975']
Best model has likelihood: -594.1402  (prior= 4.2875 )
time for generating output: 0.3439
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9614775203010497
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 839.1545 - loglik: -8.3746e+02 - logprior: -1.6955e+00
Epoch 2/10
39/39 - 20s - loss: 740.3989 - loglik: -7.3911e+02 - logprior: -1.2883e+00
Epoch 3/10
39/39 - 20s - loss: 729.5475 - loglik: -7.2819e+02 - logprior: -1.3572e+00
Epoch 4/10
39/39 - 20s - loss: 726.1266 - loglik: -7.2473e+02 - logprior: -1.3998e+00
Epoch 5/10
39/39 - 20s - loss: 724.4479 - loglik: -7.2300e+02 - logprior: -1.4480e+00
Epoch 6/10
39/39 - 20s - loss: 722.7408 - loglik: -7.2129e+02 - logprior: -1.4541e+00
Epoch 7/10
39/39 - 20s - loss: 722.6881 - loglik: -7.2124e+02 - logprior: -1.4475e+00
Epoch 8/10
39/39 - 20s - loss: 722.4529 - loglik: -7.2101e+02 - logprior: -1.4395e+00
Epoch 9/10
39/39 - 20s - loss: 721.9249 - loglik: -7.2049e+02 - logprior: -1.4389e+00
Epoch 10/10
39/39 - 20s - loss: 721.5972 - loglik: -7.2017e+02 - logprior: -1.4305e+00
Fitted a model with MAP estimate = -622.4684
expansions: [(14, 1), (30, 1), (41, 1), (53, 1), (76, 1), (80, 5), (82, 1), (88, 1), (91, 2), (93, 1), (98, 1), (99, 1), (100, 1), (104, 2), (105, 1), (109, 1), (118, 1), (119, 1), (120, 1), (121, 3), (146, 17), (158, 1), (159, 1), (160, 2), (161, 1), (168, 1), (178, 3), (179, 1), (180, 1), (185, 5), (186, 1), (188, 1), (192, 2), (193, 1), (208, 2), (209, 1), (213, 3), (215, 1), (225, 1), (244, 3)]
discards: [  0 166]
Re-initialized the encoder parameters.
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 714.0953 - loglik: -7.1127e+02 - logprior: -2.8233e+00
Epoch 2/2
39/39 - 29s - loss: 698.5025 - loglik: -6.9672e+02 - logprior: -1.7841e+00
Fitted a model with MAP estimate = -600.5714
expansions: [(0, 2), (88, 1), (319, 2)]
discards: [  0 124 125 126 127 128 129 240 241 242 282 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 703.4343 - loglik: -7.0147e+02 - logprior: -1.9674e+00
Epoch 2/2
39/39 - 28s - loss: 698.8928 - loglik: -6.9816e+02 - logprior: -7.3076e-01
Fitted a model with MAP estimate = -601.4652
expansions: [(123, 1), (124, 1), (126, 1), (127, 5), (224, 1), (226, 1)]
discards: [  0 104 236 275 308 309]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 603.2470 - loglik: -6.0122e+02 - logprior: -2.0244e+00
Epoch 2/10
43/43 - 31s - loss: 594.6390 - loglik: -5.9415e+02 - logprior: -4.8707e-01
Epoch 3/10
43/43 - 32s - loss: 596.0239 - loglik: -5.9573e+02 - logprior: -2.9404e-01
Fitted a model with MAP estimate = -592.0950
Time for alignment: 573.0173
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 837.9564 - loglik: -8.3629e+02 - logprior: -1.6708e+00
Epoch 2/10
39/39 - 20s - loss: 741.5887 - loglik: -7.4039e+02 - logprior: -1.2023e+00
Epoch 3/10
39/39 - 20s - loss: 729.8375 - loglik: -7.2849e+02 - logprior: -1.3427e+00
Epoch 4/10
39/39 - 20s - loss: 726.5118 - loglik: -7.2515e+02 - logprior: -1.3636e+00
Epoch 5/10
39/39 - 20s - loss: 725.0649 - loglik: -7.2367e+02 - logprior: -1.3928e+00
Epoch 6/10
39/39 - 20s - loss: 724.6537 - loglik: -7.2326e+02 - logprior: -1.3925e+00
Epoch 7/10
39/39 - 20s - loss: 723.8210 - loglik: -7.2243e+02 - logprior: -1.3916e+00
Epoch 8/10
39/39 - 20s - loss: 724.5157 - loglik: -7.2313e+02 - logprior: -1.3891e+00
Fitted a model with MAP estimate = -624.8963
expansions: [(14, 1), (41, 1), (61, 1), (82, 3), (83, 2), (95, 2), (97, 1), (102, 2), (103, 1), (107, 4), (109, 1), (110, 1), (111, 3), (119, 1), (120, 1), (121, 1), (122, 4), (123, 2), (146, 14), (164, 2), (168, 5), (169, 4), (171, 1), (172, 1), (173, 1), (178, 1), (179, 2), (180, 1), (182, 3), (185, 2), (186, 1), (188, 2), (190, 4), (206, 1), (207, 1), (224, 1), (244, 3)]
discards: [  0 158 159 160 161 162]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 719.4222 - loglik: -7.1646e+02 - logprior: -2.9634e+00
Epoch 2/2
39/39 - 30s - loss: 700.9244 - loglik: -6.9917e+02 - logprior: -1.7508e+00
Fitted a model with MAP estimate = -602.3785
expansions: [(0, 2), (121, 1), (241, 1), (320, 2)]
discards: [  0  34 113 130 131 149 152 180 181 202 206 207 208 246 317 318 319]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 703.4024 - loglik: -7.0146e+02 - logprior: -1.9435e+00
Epoch 2/2
39/39 - 28s - loss: 697.3774 - loglik: -6.9665e+02 - logprior: -7.2321e-01
Fitted a model with MAP estimate = -600.1113
expansions: [(170, 12), (244, 1)]
discards: [  0 122 123 124 125 126 127 176 177 227 228 229 230 246 273 307 308]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 605.5631 - loglik: -6.0354e+02 - logprior: -2.0243e+00
Epoch 2/10
43/43 - 30s - loss: 601.1880 - loglik: -6.0054e+02 - logprior: -6.4490e-01
Epoch 3/10
43/43 - 29s - loss: 597.5165 - loglik: -5.9696e+02 - logprior: -5.5477e-01
Epoch 4/10
43/43 - 31s - loss: 596.4673 - loglik: -5.9611e+02 - logprior: -3.5888e-01
Epoch 5/10
43/43 - 30s - loss: 590.7720 - loglik: -5.9051e+02 - logprior: -2.6514e-01
Epoch 6/10
43/43 - 30s - loss: 595.2993 - loglik: -5.9510e+02 - logprior: -1.9755e-01
Fitted a model with MAP estimate = -592.8921
Time for alignment: 620.7852
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 835.9231 - loglik: -8.3425e+02 - logprior: -1.6697e+00
Epoch 2/10
39/39 - 20s - loss: 739.7879 - loglik: -7.3862e+02 - logprior: -1.1724e+00
Epoch 3/10
39/39 - 20s - loss: 728.4203 - loglik: -7.2719e+02 - logprior: -1.2288e+00
Epoch 4/10
39/39 - 20s - loss: 725.1851 - loglik: -7.2394e+02 - logprior: -1.2472e+00
Epoch 5/10
39/39 - 20s - loss: 723.9781 - loglik: -7.2273e+02 - logprior: -1.2484e+00
Epoch 6/10
39/39 - 20s - loss: 722.7299 - loglik: -7.2147e+02 - logprior: -1.2645e+00
Epoch 7/10
39/39 - 20s - loss: 722.6895 - loglik: -7.2141e+02 - logprior: -1.2825e+00
Epoch 8/10
39/39 - 20s - loss: 722.5319 - loglik: -7.2124e+02 - logprior: -1.2964e+00
Epoch 9/10
39/39 - 20s - loss: 722.9631 - loglik: -7.2167e+02 - logprior: -1.2907e+00
Fitted a model with MAP estimate = -623.4502
expansions: [(14, 1), (41, 1), (45, 1), (82, 3), (83, 1), (84, 1), (92, 1), (95, 3), (97, 3), (107, 3), (111, 1), (120, 1), (121, 4), (122, 3), (123, 1), (147, 1), (150, 4), (151, 1), (160, 2), (161, 2), (162, 1), (168, 6), (172, 1), (177, 1), (178, 1), (180, 2), (181, 1), (183, 1), (184, 2), (186, 2), (187, 2), (189, 4), (204, 1), (211, 4), (244, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 718.4798 - loglik: -7.1561e+02 - logprior: -2.8742e+00
Epoch 2/2
39/39 - 29s - loss: 704.7041 - loglik: -7.0310e+02 - logprior: -1.6022e+00
Fitted a model with MAP estimate = -606.0145
expansions: [(0, 2), (120, 1), (121, 1), (122, 1), (180, 1), (181, 1), (313, 2)]
discards: [  0  34 105 109 123 124 125 126 127 128 144 194 207 208 235 274 275 276
 310 311 312]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 710.5151 - loglik: -7.0874e+02 - logprior: -1.7705e+00
Epoch 2/2
39/39 - 27s - loss: 703.7404 - loglik: -7.0318e+02 - logprior: -5.6254e-01
Fitted a model with MAP estimate = -604.9388
expansions: [(122, 1), (124, 2), (125, 6)]
discards: [  0 117 299 300]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 606.7946 - loglik: -6.0479e+02 - logprior: -2.0050e+00
Epoch 2/10
43/43 - 31s - loss: 599.6409 - loglik: -5.9912e+02 - logprior: -5.2090e-01
Epoch 3/10
43/43 - 30s - loss: 596.0892 - loglik: -5.9581e+02 - logprior: -2.8043e-01
Epoch 4/10
43/43 - 30s - loss: 596.3449 - loglik: -5.9614e+02 - logprior: -2.0981e-01
Fitted a model with MAP estimate = -594.8305
Time for alignment: 573.1600
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 838.8472 - loglik: -8.3716e+02 - logprior: -1.6872e+00
Epoch 2/10
39/39 - 20s - loss: 740.4750 - loglik: -7.3916e+02 - logprior: -1.3186e+00
Epoch 3/10
39/39 - 20s - loss: 728.7067 - loglik: -7.2729e+02 - logprior: -1.4136e+00
Epoch 4/10
39/39 - 20s - loss: 725.0570 - loglik: -7.2362e+02 - logprior: -1.4418e+00
Epoch 5/10
39/39 - 20s - loss: 723.6682 - loglik: -7.2218e+02 - logprior: -1.4885e+00
Epoch 6/10
39/39 - 20s - loss: 723.0797 - loglik: -7.2159e+02 - logprior: -1.4896e+00
Epoch 7/10
39/39 - 20s - loss: 723.0540 - loglik: -7.2154e+02 - logprior: -1.5143e+00
Epoch 8/10
39/39 - 20s - loss: 722.5465 - loglik: -7.2103e+02 - logprior: -1.5177e+00
Epoch 9/10
39/39 - 20s - loss: 723.1948 - loglik: -7.2167e+02 - logprior: -1.5268e+00
Fitted a model with MAP estimate = -623.5109
expansions: [(14, 1), (45, 1), (56, 1), (81, 3), (82, 2), (83, 2), (89, 1), (94, 1), (99, 3), (109, 1), (110, 3), (118, 1), (119, 1), (120, 1), (121, 3), (122, 2), (145, 10), (159, 5), (161, 1), (167, 2), (168, 2), (169, 3), (170, 4), (171, 2), (172, 1), (173, 1), (178, 1), (179, 1), (184, 2), (185, 1), (186, 1), (187, 2), (188, 4), (190, 2), (191, 2), (207, 1), (208, 1), (212, 4), (224, 1), (244, 3)]
discards: [  0 103 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 720.3297 - loglik: -7.1745e+02 - logprior: -2.8764e+00
Epoch 2/2
39/39 - 30s - loss: 704.2402 - loglik: -7.0251e+02 - logprior: -1.7273e+00
Fitted a model with MAP estimate = -604.5864
expansions: [(0, 2), (117, 6), (212, 1), (250, 1), (251, 1), (322, 2)]
discards: [  0  90 141 188 189 190 191 192 193 194 195 196 197 198 239 246 252 257
 283 284 285 319 320 321]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 707.4772 - loglik: -7.0558e+02 - logprior: -1.8954e+00
Epoch 2/2
39/39 - 29s - loss: 699.9139 - loglik: -6.9902e+02 - logprior: -8.9347e-01
Fitted a model with MAP estimate = -601.7494
expansions: [(128, 1)]
discards: [  0 117 118 119 120 121 122 123 124 192 193 194 195 196 197 198 309 310]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 611.5178 - loglik: -6.0953e+02 - logprior: -1.9867e+00
Epoch 2/10
43/43 - 28s - loss: 603.1605 - loglik: -6.0271e+02 - logprior: -4.5344e-01
Epoch 3/10
43/43 - 28s - loss: 601.2493 - loglik: -6.0102e+02 - logprior: -2.2638e-01
Epoch 4/10
43/43 - 29s - loss: 602.9575 - loglik: -6.0281e+02 - logprior: -1.4893e-01
Fitted a model with MAP estimate = -599.0822
Time for alignment: 573.2172
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 838.2341 - loglik: -8.3656e+02 - logprior: -1.6739e+00
Epoch 2/10
39/39 - 20s - loss: 741.7064 - loglik: -7.4044e+02 - logprior: -1.2704e+00
Epoch 3/10
39/39 - 20s - loss: 730.7984 - loglik: -7.2941e+02 - logprior: -1.3899e+00
Epoch 4/10
39/39 - 20s - loss: 726.5914 - loglik: -7.2516e+02 - logprior: -1.4276e+00
Epoch 5/10
39/39 - 20s - loss: 724.2596 - loglik: -7.2276e+02 - logprior: -1.4991e+00
Epoch 6/10
39/39 - 20s - loss: 723.8190 - loglik: -7.2232e+02 - logprior: -1.4947e+00
Epoch 7/10
39/39 - 20s - loss: 723.6719 - loglik: -7.2215e+02 - logprior: -1.5219e+00
Epoch 8/10
39/39 - 20s - loss: 723.1223 - loglik: -7.2160e+02 - logprior: -1.5212e+00
Epoch 9/10
39/39 - 20s - loss: 723.4756 - loglik: -7.2195e+02 - logprior: -1.5225e+00
Fitted a model with MAP estimate = -623.5054
expansions: [(28, 1), (31, 1), (43, 1), (56, 1), (81, 3), (82, 2), (83, 1), (85, 1), (90, 1), (93, 3), (95, 3), (103, 1), (105, 2), (106, 1), (109, 1), (118, 1), (119, 1), (120, 1), (121, 3), (122, 2), (147, 1), (148, 1), (149, 5), (151, 1), (159, 3), (160, 2), (161, 1), (163, 1), (168, 1), (169, 1), (170, 1), (176, 1), (177, 1), (179, 2), (184, 2), (185, 1), (187, 2), (189, 2), (190, 2), (191, 2), (204, 1), (206, 1), (207, 1), (211, 2), (224, 1), (244, 3)]
discards: [  0 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 716.4496 - loglik: -7.1369e+02 - logprior: -2.7625e+00
Epoch 2/2
39/39 - 29s - loss: 702.7714 - loglik: -7.0100e+02 - logprior: -1.7717e+00
Fitted a model with MAP estimate = -604.2275
expansions: [(0, 2), (122, 1), (132, 3), (314, 2)]
discards: [  0  86 106 110 124 125 126 127 128 129 130 150 227 234 242 248 250 311
 312 313]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 707.3961 - loglik: -7.0561e+02 - logprior: -1.7889e+00
Epoch 2/2
39/39 - 27s - loss: 702.1714 - loglik: -7.0163e+02 - logprior: -5.4352e-01
Fitted a model with MAP estimate = -603.8629
expansions: [(123, 4)]
discards: [  0 266 267 300 301]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 606.4936 - loglik: -6.0456e+02 - logprior: -1.9348e+00
Epoch 2/10
43/43 - 29s - loss: 601.8504 - loglik: -6.0147e+02 - logprior: -3.8389e-01
Epoch 3/10
43/43 - 30s - loss: 595.9384 - loglik: -5.9577e+02 - logprior: -1.7193e-01
Epoch 4/10
43/43 - 29s - loss: 597.3834 - loglik: -5.9728e+02 - logprior: -1.0310e-01
Fitted a model with MAP estimate = -595.7803
Time for alignment: 568.7462
Computed alignments with likelihoods: ['-592.0950', '-592.8921', '-594.8305', '-599.0822', '-595.7803']
Best model has likelihood: -592.0950  (prior= -0.2929 )
time for generating output: 0.3872
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5257507221101967
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 274.5197 - loglik: -2.3638e+02 - logprior: -3.8139e+01
Epoch 2/10
10/10 - 1s - loss: 223.4920 - loglik: -2.1343e+02 - logprior: -1.0065e+01
Epoch 3/10
10/10 - 1s - loss: 197.4714 - loglik: -1.9238e+02 - logprior: -5.0870e+00
Epoch 4/10
10/10 - 1s - loss: 181.5934 - loglik: -1.7816e+02 - logprior: -3.4360e+00
Epoch 5/10
10/10 - 1s - loss: 174.9400 - loglik: -1.7237e+02 - logprior: -2.5676e+00
Epoch 6/10
10/10 - 1s - loss: 172.2209 - loglik: -1.7011e+02 - logprior: -2.1111e+00
Epoch 7/10
10/10 - 1s - loss: 171.5229 - loglik: -1.6968e+02 - logprior: -1.8439e+00
Epoch 8/10
10/10 - 1s - loss: 170.3424 - loglik: -1.6865e+02 - logprior: -1.6964e+00
Epoch 9/10
10/10 - 1s - loss: 169.7587 - loglik: -1.6809e+02 - logprior: -1.6705e+00
Epoch 10/10
10/10 - 1s - loss: 169.1761 - loglik: -1.6752e+02 - logprior: -1.6601e+00
Fitted a model with MAP estimate = -168.8044
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (32, 1), (43, 1), (55, 3), (56, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.9386 - loglik: -1.6617e+02 - logprior: -4.2765e+01
Epoch 2/2
10/10 - 1s - loss: 174.5368 - loglik: -1.5719e+02 - logprior: -1.7348e+01
Fitted a model with MAP estimate = -168.3344
expansions: [(0, 2)]
discards: [ 0 18 26 68]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.2346 - loglik: -1.5345e+02 - logprior: -3.3787e+01
Epoch 2/2
10/10 - 1s - loss: 159.5149 - loglik: -1.5102e+02 - logprior: -8.4952e+00
Fitted a model with MAP estimate = -155.5235
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.5578 - loglik: -1.5323e+02 - logprior: -3.9328e+01
Epoch 2/10
10/10 - 1s - loss: 162.9075 - loglik: -1.5228e+02 - logprior: -1.0627e+01
Epoch 3/10
10/10 - 1s - loss: 155.5343 - loglik: -1.5153e+02 - logprior: -4.0090e+00
Epoch 4/10
10/10 - 1s - loss: 152.1978 - loglik: -1.5024e+02 - logprior: -1.9534e+00
Epoch 5/10
10/10 - 1s - loss: 151.2989 - loglik: -1.5039e+02 - logprior: -9.0613e-01
Epoch 6/10
10/10 - 1s - loss: 150.6069 - loglik: -1.5053e+02 - logprior: -7.7753e-02
Epoch 7/10
10/10 - 1s - loss: 149.9599 - loglik: -1.5037e+02 - logprior: 0.4080
Epoch 8/10
10/10 - 1s - loss: 150.1759 - loglik: -1.5082e+02 - logprior: 0.6451
Fitted a model with MAP estimate = -149.8296
Time for alignment: 41.8268
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.4979 - loglik: -2.3636e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 223.2375 - loglik: -2.1317e+02 - logprior: -1.0067e+01
Epoch 3/10
10/10 - 1s - loss: 196.0452 - loglik: -1.9095e+02 - logprior: -5.0917e+00
Epoch 4/10
10/10 - 1s - loss: 179.7062 - loglik: -1.7629e+02 - logprior: -3.4183e+00
Epoch 5/10
10/10 - 1s - loss: 172.1490 - loglik: -1.6936e+02 - logprior: -2.7866e+00
Epoch 6/10
10/10 - 1s - loss: 169.4468 - loglik: -1.6688e+02 - logprior: -2.5647e+00
Epoch 7/10
10/10 - 1s - loss: 168.5430 - loglik: -1.6646e+02 - logprior: -2.0875e+00
Epoch 8/10
10/10 - 1s - loss: 167.3411 - loglik: -1.6562e+02 - logprior: -1.7182e+00
Epoch 9/10
10/10 - 1s - loss: 166.8105 - loglik: -1.6518e+02 - logprior: -1.6308e+00
Epoch 10/10
10/10 - 1s - loss: 166.4011 - loglik: -1.6478e+02 - logprior: -1.6203e+00
Fitted a model with MAP estimate = -166.2116
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.6701 - loglik: -1.6153e+02 - logprior: -5.0143e+01
Epoch 2/2
10/10 - 1s - loss: 168.6123 - loglik: -1.5364e+02 - logprior: -1.4975e+01
Fitted a model with MAP estimate = -160.8015
expansions: []
discards: [ 0 18]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.9593 - loglik: -1.5457e+02 - logprior: -4.3387e+01
Epoch 2/2
10/10 - 1s - loss: 170.8208 - loglik: -1.5390e+02 - logprior: -1.6926e+01
Fitted a model with MAP estimate = -166.0525
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 193.0627 - loglik: -1.5344e+02 - logprior: -3.9624e+01
Epoch 2/10
10/10 - 1s - loss: 163.2566 - loglik: -1.5242e+02 - logprior: -1.0837e+01
Epoch 3/10
10/10 - 1s - loss: 155.6931 - loglik: -1.5157e+02 - logprior: -4.1242e+00
Epoch 4/10
10/10 - 1s - loss: 153.4125 - loglik: -1.5137e+02 - logprior: -2.0403e+00
Epoch 5/10
10/10 - 1s - loss: 151.5979 - loglik: -1.5063e+02 - logprior: -9.7058e-01
Epoch 6/10
10/10 - 1s - loss: 150.8976 - loglik: -1.5077e+02 - logprior: -1.2819e-01
Epoch 7/10
10/10 - 1s - loss: 151.0516 - loglik: -1.5139e+02 - logprior: 0.3397
Fitted a model with MAP estimate = -150.6821
Time for alignment: 39.4410
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.4642 - loglik: -2.3632e+02 - logprior: -3.8140e+01
Epoch 2/10
10/10 - 1s - loss: 222.9444 - loglik: -2.1288e+02 - logprior: -1.0065e+01
Epoch 3/10
10/10 - 1s - loss: 196.7270 - loglik: -1.9167e+02 - logprior: -5.0561e+00
Epoch 4/10
10/10 - 1s - loss: 180.7852 - loglik: -1.7736e+02 - logprior: -3.4224e+00
Epoch 5/10
10/10 - 1s - loss: 174.5355 - loglik: -1.7196e+02 - logprior: -2.5723e+00
Epoch 6/10
10/10 - 1s - loss: 172.1362 - loglik: -1.7001e+02 - logprior: -2.1231e+00
Epoch 7/10
10/10 - 1s - loss: 170.8050 - loglik: -1.6892e+02 - logprior: -1.8853e+00
Epoch 8/10
10/10 - 1s - loss: 170.1426 - loglik: -1.6836e+02 - logprior: -1.7869e+00
Epoch 9/10
10/10 - 1s - loss: 169.3711 - loglik: -1.6762e+02 - logprior: -1.7472e+00
Epoch 10/10
10/10 - 1s - loss: 168.8139 - loglik: -1.6709e+02 - logprior: -1.7287e+00
Fitted a model with MAP estimate = -168.5171
expansions: [(7, 2), (13, 1), (15, 2), (20, 1), (21, 2), (24, 3), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 207.8425 - loglik: -1.6510e+02 - logprior: -4.2738e+01
Epoch 2/2
10/10 - 1s - loss: 173.6043 - loglik: -1.5636e+02 - logprior: -1.7242e+01
Fitted a model with MAP estimate = -167.2544
expansions: [(0, 2)]
discards: [ 0 18 26]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 186.0662 - loglik: -1.5233e+02 - logprior: -3.3738e+01
Epoch 2/2
10/10 - 1s - loss: 159.3314 - loglik: -1.5083e+02 - logprior: -8.5046e+00
Fitted a model with MAP estimate = -155.3675
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.4555 - loglik: -1.5314e+02 - logprior: -3.9318e+01
Epoch 2/10
10/10 - 1s - loss: 162.8899 - loglik: -1.5228e+02 - logprior: -1.0613e+01
Epoch 3/10
10/10 - 1s - loss: 155.2915 - loglik: -1.5128e+02 - logprior: -4.0128e+00
Epoch 4/10
10/10 - 1s - loss: 152.4263 - loglik: -1.5047e+02 - logprior: -1.9521e+00
Epoch 5/10
10/10 - 1s - loss: 151.3688 - loglik: -1.5046e+02 - logprior: -9.0724e-01
Epoch 6/10
10/10 - 1s - loss: 150.3786 - loglik: -1.5030e+02 - logprior: -7.9935e-02
Epoch 7/10
10/10 - 1s - loss: 150.2529 - loglik: -1.5067e+02 - logprior: 0.4132
Epoch 8/10
10/10 - 1s - loss: 150.0291 - loglik: -1.5068e+02 - logprior: 0.6512
Epoch 9/10
10/10 - 1s - loss: 149.8345 - loglik: -1.5068e+02 - logprior: 0.8503
Epoch 10/10
10/10 - 1s - loss: 149.8402 - loglik: -1.5088e+02 - logprior: 1.0377
Fitted a model with MAP estimate = -149.5919
Time for alignment: 43.7183
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.5657 - loglik: -2.3643e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 223.0228 - loglik: -2.1296e+02 - logprior: -1.0066e+01
Epoch 3/10
10/10 - 1s - loss: 196.9747 - loglik: -1.9190e+02 - logprior: -5.0721e+00
Epoch 4/10
10/10 - 1s - loss: 181.2456 - loglik: -1.7784e+02 - logprior: -3.4104e+00
Epoch 5/10
10/10 - 1s - loss: 175.2959 - loglik: -1.7281e+02 - logprior: -2.4845e+00
Epoch 6/10
10/10 - 1s - loss: 172.5489 - loglik: -1.7052e+02 - logprior: -2.0256e+00
Epoch 7/10
10/10 - 1s - loss: 171.1907 - loglik: -1.6934e+02 - logprior: -1.8532e+00
Epoch 8/10
10/10 - 1s - loss: 170.2953 - loglik: -1.6854e+02 - logprior: -1.7526e+00
Epoch 9/10
10/10 - 1s - loss: 169.7649 - loglik: -1.6811e+02 - logprior: -1.6506e+00
Epoch 10/10
10/10 - 1s - loss: 169.4209 - loglik: -1.6783e+02 - logprior: -1.5923e+00
Fitted a model with MAP estimate = -169.1282
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (21, 1), (24, 3), (30, 1), (35, 2), (43, 1), (55, 2), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.4486 - loglik: -1.6671e+02 - logprior: -4.2737e+01
Epoch 2/2
10/10 - 1s - loss: 174.3734 - loglik: -1.5703e+02 - logprior: -1.7339e+01
Fitted a model with MAP estimate = -168.7476
expansions: [(0, 2)]
discards: [ 0 18 46]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.2035 - loglik: -1.5342e+02 - logprior: -3.3780e+01
Epoch 2/2
10/10 - 1s - loss: 160.7379 - loglik: -1.5221e+02 - logprior: -8.5311e+00
Fitted a model with MAP estimate = -156.2922
expansions: []
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.9097 - loglik: -1.5357e+02 - logprior: -3.9340e+01
Epoch 2/10
10/10 - 1s - loss: 163.3894 - loglik: -1.5274e+02 - logprior: -1.0650e+01
Epoch 3/10
10/10 - 1s - loss: 155.3454 - loglik: -1.5132e+02 - logprior: -4.0227e+00
Epoch 4/10
10/10 - 1s - loss: 152.5930 - loglik: -1.5063e+02 - logprior: -1.9670e+00
Epoch 5/10
10/10 - 1s - loss: 151.2470 - loglik: -1.5033e+02 - logprior: -9.1268e-01
Epoch 6/10
10/10 - 1s - loss: 150.5449 - loglik: -1.5046e+02 - logprior: -8.9398e-02
Epoch 7/10
10/10 - 1s - loss: 150.4259 - loglik: -1.5083e+02 - logprior: 0.4059
Epoch 8/10
10/10 - 1s - loss: 150.0515 - loglik: -1.5069e+02 - logprior: 0.6428
Epoch 9/10
10/10 - 1s - loss: 149.8886 - loglik: -1.5073e+02 - logprior: 0.8442
Epoch 10/10
10/10 - 1s - loss: 149.7390 - loglik: -1.5077e+02 - logprior: 1.0263
Fitted a model with MAP estimate = -149.6264
Time for alignment: 43.7723
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.7360 - loglik: -2.3660e+02 - logprior: -3.8139e+01
Epoch 2/10
10/10 - 1s - loss: 223.2497 - loglik: -2.1319e+02 - logprior: -1.0064e+01
Epoch 3/10
10/10 - 1s - loss: 197.3190 - loglik: -1.9224e+02 - logprior: -5.0791e+00
Epoch 4/10
10/10 - 1s - loss: 182.0219 - loglik: -1.7860e+02 - logprior: -3.4173e+00
Epoch 5/10
10/10 - 1s - loss: 174.9406 - loglik: -1.7232e+02 - logprior: -2.6195e+00
Epoch 6/10
10/10 - 1s - loss: 171.6229 - loglik: -1.6933e+02 - logprior: -2.2899e+00
Epoch 7/10
10/10 - 1s - loss: 169.2634 - loglik: -1.6714e+02 - logprior: -2.1232e+00
Epoch 8/10
10/10 - 1s - loss: 168.3810 - loglik: -1.6657e+02 - logprior: -1.8097e+00
Epoch 9/10
10/10 - 1s - loss: 167.4857 - loglik: -1.6590e+02 - logprior: -1.5818e+00
Epoch 10/10
10/10 - 1s - loss: 167.2686 - loglik: -1.6572e+02 - logprior: -1.5442e+00
Fitted a model with MAP estimate = -167.1174
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (38, 1), (46, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 212.4596 - loglik: -1.6254e+02 - logprior: -4.9918e+01
Epoch 2/2
10/10 - 1s - loss: 168.4921 - loglik: -1.5353e+02 - logprior: -1.4963e+01
Fitted a model with MAP estimate = -160.7919
expansions: []
discards: [ 0 18 26]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.9137 - loglik: -1.5468e+02 - logprior: -4.3234e+01
Epoch 2/2
10/10 - 1s - loss: 170.3157 - loglik: -1.5349e+02 - logprior: -1.6824e+01
Fitted a model with MAP estimate = -165.5337
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.3167 - loglik: -1.5276e+02 - logprior: -3.9553e+01
Epoch 2/10
10/10 - 1s - loss: 162.9746 - loglik: -1.5223e+02 - logprior: -1.0749e+01
Epoch 3/10
10/10 - 1s - loss: 155.1884 - loglik: -1.5118e+02 - logprior: -4.0078e+00
Epoch 4/10
10/10 - 1s - loss: 152.4611 - loglik: -1.5054e+02 - logprior: -1.9193e+00
Epoch 5/10
10/10 - 1s - loss: 150.9833 - loglik: -1.5014e+02 - logprior: -8.4458e-01
Epoch 6/10
10/10 - 1s - loss: 150.4010 - loglik: -1.5039e+02 - logprior: -1.0085e-02
Epoch 7/10
10/10 - 1s - loss: 150.3435 - loglik: -1.5080e+02 - logprior: 0.4556
Epoch 8/10
10/10 - 1s - loss: 150.0297 - loglik: -1.5072e+02 - logprior: 0.6935
Epoch 9/10
10/10 - 1s - loss: 149.4606 - loglik: -1.5036e+02 - logprior: 0.9004
Epoch 10/10
10/10 - 1s - loss: 149.8023 - loglik: -1.5089e+02 - logprior: 1.0845
Fitted a model with MAP estimate = -149.5315
Time for alignment: 42.2139
Computed alignments with likelihoods: ['-149.8296', '-150.6821', '-149.5919', '-149.6264', '-149.5315']
Best model has likelihood: -149.5315  (prior= 1.1625 )
time for generating output: 0.1574
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9222944334557784
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 303.5191 - loglik: -2.9256e+02 - logprior: -1.0963e+01
Epoch 2/10
19/19 - 5s - loss: 239.9308 - loglik: -2.3775e+02 - logprior: -2.1837e+00
Epoch 3/10
19/19 - 5s - loss: 222.0635 - loglik: -2.2027e+02 - logprior: -1.7896e+00
Epoch 4/10
19/19 - 5s - loss: 217.9568 - loglik: -2.1636e+02 - logprior: -1.5957e+00
Epoch 5/10
19/19 - 4s - loss: 216.2566 - loglik: -2.1482e+02 - logprior: -1.4339e+00
Epoch 6/10
19/19 - 6s - loss: 216.3609 - loglik: -2.1494e+02 - logprior: -1.4210e+00
Fitted a model with MAP estimate = -216.0064
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 2), (28, 1), (47, 3), (48, 5), (49, 1), (50, 2), (60, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 223.1192 - loglik: -2.0948e+02 - logprior: -1.3635e+01
Epoch 2/2
19/19 - 5s - loss: 204.5584 - loglik: -2.0020e+02 - logprior: -4.3594e+00
Fitted a model with MAP estimate = -200.6902
expansions: [(0, 2)]
discards: [ 0 12 68]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 208.0302 - loglik: -1.9860e+02 - logprior: -9.4286e+00
Epoch 2/2
19/19 - 4s - loss: 198.9429 - loglik: -1.9768e+02 - logprior: -1.2678e+00
Fitted a model with MAP estimate = -197.8171
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 211.1828 - loglik: -1.9979e+02 - logprior: -1.1398e+01
Epoch 2/10
19/19 - 5s - loss: 199.6822 - loglik: -1.9808e+02 - logprior: -1.6047e+00
Epoch 3/10
19/19 - 5s - loss: 199.4989 - loglik: -1.9919e+02 - logprior: -3.1107e-01
Epoch 4/10
19/19 - 4s - loss: 196.3337 - loglik: -1.9643e+02 - logprior: 0.0972
Epoch 5/10
19/19 - 6s - loss: 198.1099 - loglik: -1.9838e+02 - logprior: 0.2707
Fitted a model with MAP estimate = -196.9842
Time for alignment: 101.1335
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 302.7916 - loglik: -2.9182e+02 - logprior: -1.0971e+01
Epoch 2/10
19/19 - 5s - loss: 242.0149 - loglik: -2.3976e+02 - logprior: -2.2541e+00
Epoch 3/10
19/19 - 5s - loss: 221.3376 - loglik: -2.1933e+02 - logprior: -2.0109e+00
Epoch 4/10
19/19 - 5s - loss: 217.6937 - loglik: -2.1597e+02 - logprior: -1.7243e+00
Epoch 5/10
19/19 - 5s - loss: 217.7853 - loglik: -2.1630e+02 - logprior: -1.4891e+00
Fitted a model with MAP estimate = -216.1392
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (27, 1), (29, 1), (34, 1), (43, 1), (47, 3), (48, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 223.1740 - loglik: -2.0952e+02 - logprior: -1.3654e+01
Epoch 2/2
19/19 - 5s - loss: 205.4810 - loglik: -2.0127e+02 - logprior: -4.2098e+00
Fitted a model with MAP estimate = -201.5499
expansions: [(0, 2), (58, 1)]
discards: [ 0 14 16]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 209.7730 - loglik: -2.0033e+02 - logprior: -9.4407e+00
Epoch 2/2
19/19 - 4s - loss: 198.9371 - loglik: -1.9767e+02 - logprior: -1.2652e+00
Fitted a model with MAP estimate = -198.3077
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 212.2050 - loglik: -2.0086e+02 - logprior: -1.1348e+01
Epoch 2/10
19/19 - 4s - loss: 199.7508 - loglik: -1.9815e+02 - logprior: -1.6046e+00
Epoch 3/10
19/19 - 5s - loss: 198.6888 - loglik: -1.9842e+02 - logprior: -2.7230e-01
Epoch 4/10
19/19 - 5s - loss: 198.2306 - loglik: -1.9831e+02 - logprior: 0.0755
Epoch 5/10
19/19 - 4s - loss: 196.8898 - loglik: -1.9714e+02 - logprior: 0.2524
Epoch 6/10
19/19 - 6s - loss: 198.1044 - loglik: -1.9846e+02 - logprior: 0.3605
Fitted a model with MAP estimate = -197.2229
Time for alignment: 99.2007
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 303.3092 - loglik: -2.9234e+02 - logprior: -1.0966e+01
Epoch 2/10
19/19 - 5s - loss: 240.4001 - loglik: -2.3831e+02 - logprior: -2.0929e+00
Epoch 3/10
19/19 - 5s - loss: 222.6523 - loglik: -2.2100e+02 - logprior: -1.6515e+00
Epoch 4/10
19/19 - 4s - loss: 219.6503 - loglik: -2.1826e+02 - logprior: -1.3915e+00
Epoch 5/10
19/19 - 5s - loss: 217.6712 - loglik: -2.1644e+02 - logprior: -1.2337e+00
Epoch 6/10
19/19 - 5s - loss: 218.4244 - loglik: -2.1713e+02 - logprior: -1.2911e+00
Fitted a model with MAP estimate = -217.5120
expansions: [(10, 5), (11, 2), (26, 2), (43, 1), (48, 6), (49, 2), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 222.5767 - loglik: -2.0895e+02 - logprior: -1.3626e+01
Epoch 2/2
19/19 - 5s - loss: 204.6921 - loglik: -2.0038e+02 - logprior: -4.3155e+00
Fitted a model with MAP estimate = -201.0693
expansions: [(0, 2)]
discards: [ 0 15 60 64]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 208.7463 - loglik: -1.9935e+02 - logprior: -9.3973e+00
Epoch 2/2
19/19 - 5s - loss: 199.6856 - loglik: -1.9845e+02 - logprior: -1.2317e+00
Fitted a model with MAP estimate = -198.2348
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 212.6316 - loglik: -2.0128e+02 - logprior: -1.1347e+01
Epoch 2/10
19/19 - 6s - loss: 202.1016 - loglik: -2.0053e+02 - logprior: -1.5748e+00
Epoch 3/10
19/19 - 5s - loss: 198.8632 - loglik: -1.9858e+02 - logprior: -2.8017e-01
Epoch 4/10
19/19 - 4s - loss: 198.0924 - loglik: -1.9820e+02 - logprior: 0.1095
Epoch 5/10
19/19 - 5s - loss: 198.4531 - loglik: -1.9871e+02 - logprior: 0.2606
Fitted a model with MAP estimate = -198.1875
Time for alignment: 98.2083
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 303.5064 - loglik: -2.9254e+02 - logprior: -1.0967e+01
Epoch 2/10
19/19 - 4s - loss: 240.5664 - loglik: -2.3838e+02 - logprior: -2.1909e+00
Epoch 3/10
19/19 - 5s - loss: 223.4471 - loglik: -2.2157e+02 - logprior: -1.8748e+00
Epoch 4/10
19/19 - 5s - loss: 218.1529 - loglik: -2.1647e+02 - logprior: -1.6792e+00
Epoch 5/10
19/19 - 4s - loss: 216.2402 - loglik: -2.1476e+02 - logprior: -1.4817e+00
Epoch 6/10
19/19 - 6s - loss: 217.4213 - loglik: -2.1595e+02 - logprior: -1.4738e+00
Fitted a model with MAP estimate = -216.3585
expansions: [(10, 5), (11, 2), (26, 1), (29, 1), (34, 1), (42, 1), (47, 6), (48, 1), (62, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 222.2213 - loglik: -2.0866e+02 - logprior: -1.3561e+01
Epoch 2/2
19/19 - 5s - loss: 205.3460 - loglik: -2.0105e+02 - logprior: -4.2937e+00
Fitted a model with MAP estimate = -200.7429
expansions: [(0, 2)]
discards: [ 0 15]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 208.5667 - loglik: -1.9919e+02 - logprior: -9.3723e+00
Epoch 2/2
19/19 - 6s - loss: 199.0640 - loglik: -1.9787e+02 - logprior: -1.1983e+00
Fitted a model with MAP estimate = -197.7445
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 212.0691 - loglik: -2.0088e+02 - logprior: -1.1191e+01
Epoch 2/10
19/19 - 5s - loss: 200.2842 - loglik: -1.9875e+02 - logprior: -1.5376e+00
Epoch 3/10
19/19 - 4s - loss: 199.6570 - loglik: -1.9942e+02 - logprior: -2.3714e-01
Epoch 4/10
19/19 - 5s - loss: 197.9801 - loglik: -1.9812e+02 - logprior: 0.1367
Epoch 5/10
19/19 - 4s - loss: 197.5531 - loglik: -1.9786e+02 - logprior: 0.3039
Epoch 6/10
19/19 - 5s - loss: 197.1553 - loglik: -1.9758e+02 - logprior: 0.4215
Epoch 7/10
19/19 - 5s - loss: 197.6818 - loglik: -1.9820e+02 - logprior: 0.5151
Fitted a model with MAP estimate = -197.3240
Time for alignment: 106.9255
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 303.0003 - loglik: -2.9203e+02 - logprior: -1.0973e+01
Epoch 2/10
19/19 - 4s - loss: 241.2266 - loglik: -2.3905e+02 - logprior: -2.1794e+00
Epoch 3/10
19/19 - 5s - loss: 222.5511 - loglik: -2.2075e+02 - logprior: -1.8043e+00
Epoch 4/10
19/19 - 4s - loss: 217.5899 - loglik: -2.1605e+02 - logprior: -1.5420e+00
Epoch 5/10
19/19 - 4s - loss: 218.3024 - loglik: -2.1691e+02 - logprior: -1.3881e+00
Fitted a model with MAP estimate = -217.2785
expansions: [(10, 3), (11, 2), (12, 1), (27, 2), (29, 1), (34, 1), (42, 1), (47, 4), (48, 1), (72, 5), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 222.7780 - loglik: -2.0924e+02 - logprior: -1.3534e+01
Epoch 2/2
19/19 - 4s - loss: 205.0680 - loglik: -2.0096e+02 - logprior: -4.1078e+00
Fitted a model with MAP estimate = -201.6639
expansions: [(0, 2)]
discards: [ 0 12 13]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 209.3233 - loglik: -1.9991e+02 - logprior: -9.4130e+00
Epoch 2/2
19/19 - 4s - loss: 200.3892 - loglik: -1.9913e+02 - logprior: -1.2564e+00
Fitted a model with MAP estimate = -198.9443
expansions: [(56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 214.2571 - loglik: -2.0162e+02 - logprior: -1.2637e+01
Epoch 2/10
19/19 - 5s - loss: 201.1630 - loglik: -1.9903e+02 - logprior: -2.1311e+00
Epoch 3/10
19/19 - 4s - loss: 197.9329 - loglik: -1.9762e+02 - logprior: -3.0943e-01
Epoch 4/10
19/19 - 5s - loss: 198.9634 - loglik: -1.9905e+02 - logprior: 0.0903
Fitted a model with MAP estimate = -197.9077
Time for alignment: 83.0996
Computed alignments with likelihoods: ['-196.9842', '-197.2229', '-198.1875', '-197.3240', '-197.9077']
Best model has likelihood: -196.9842  (prior= 0.3247 )
time for generating output: 0.6050
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.2101952684941795
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1101.2716 - loglik: -1.1001e+03 - logprior: -1.2031e+00
Epoch 2/10
49/49 - 41s - loss: 954.8023 - loglik: -9.5513e+02 - logprior: 0.3320
Epoch 3/10
49/49 - 41s - loss: 948.4390 - loglik: -9.4872e+02 - logprior: 0.2818
Epoch 4/10
49/49 - 41s - loss: 949.0930 - loglik: -9.4944e+02 - logprior: 0.3450
Fitted a model with MAP estimate = -948.0794
expansions: [(0, 5), (82, 1), (139, 1), (206, 1), (214, 1), (232, 3), (233, 2), (234, 2), (235, 1), (245, 1), (247, 1), (248, 1), (261, 1), (262, 5), (263, 3), (265, 1), (281, 2), (282, 3), (283, 2), (284, 3), (300, 1), (301, 2), (302, 1), (303, 2), (304, 2), (306, 4), (308, 5), (309, 2), (317, 2), (318, 1), (319, 6), (320, 2), (325, 1), (327, 1), (328, 1), (329, 2), (330, 1), (333, 1), (347, 2), (348, 1), (350, 1), (370, 3), (372, 1), (375, 1), (376, 1), (377, 1), (393, 4), (394, 1), (399, 11), (400, 2), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 940.9770 - loglik: -9.3852e+02 - logprior: -2.4599e+00
Epoch 2/2
49/49 - 58s - loss: 925.5904 - loglik: -9.2760e+02 - logprior: 2.0126
Fitted a model with MAP estimate = -919.6224
expansions: [(0, 6), (486, 1), (487, 2), (488, 1)]
discards: [  1   2   3   4   5   6   7  24 230 303 362 464 490 491 492 493 494 495
 496 497 498]
Re-initialized the encoder parameters.
Fitting a model of length 488 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 58s - loss: 933.5240 - loglik: -9.3222e+02 - logprior: -1.3086e+00
Epoch 2/2
49/49 - 55s - loss: 924.4902 - loglik: -9.2798e+02 - logprior: 3.4858
Fitted a model with MAP estimate = -920.5474
expansions: [(0, 6), (23, 3), (485, 1), (488, 7)]
discards: [  1   2   3   4   5   6 267 463]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 929.4330 - loglik: -9.2817e+02 - logprior: -1.2612e+00
Epoch 2/10
49/49 - 57s - loss: 917.7684 - loglik: -9.2176e+02 - logprior: 3.9958
Epoch 3/10
49/49 - 57s - loss: 917.8041 - loglik: -9.2243e+02 - logprior: 4.6277
Fitted a model with MAP estimate = -913.7091
Time for alignment: 739.1102
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1097.9717 - loglik: -1.0967e+03 - logprior: -1.2846e+00
Epoch 2/10
49/49 - 41s - loss: 959.0292 - loglik: -9.5922e+02 - logprior: 0.1869
Epoch 3/10
49/49 - 41s - loss: 946.7917 - loglik: -9.4715e+02 - logprior: 0.3581
Epoch 4/10
49/49 - 41s - loss: 948.3084 - loglik: -9.4859e+02 - logprior: 0.2846
Fitted a model with MAP estimate = -946.3213
expansions: [(0, 5), (141, 1), (163, 2), (193, 1), (213, 1), (214, 1), (226, 1), (227, 1), (229, 5), (230, 3), (231, 1), (234, 2), (238, 2), (240, 1), (247, 1), (250, 1), (252, 2), (253, 5), (254, 4), (255, 2), (275, 1), (279, 1), (297, 2), (298, 4), (299, 3), (300, 5), (302, 6), (303, 2), (305, 1), (310, 2), (311, 1), (312, 4), (313, 1), (314, 1), (322, 1), (324, 2), (325, 2), (326, 1), (330, 1), (343, 2), (344, 4), (345, 1), (370, 1), (372, 1), (391, 2), (392, 4), (398, 7), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 941.3443 - loglik: -9.3883e+02 - logprior: -2.5114e+00
Epoch 2/2
49/49 - 57s - loss: 923.9688 - loglik: -9.2595e+02 - logprior: 1.9777
Fitted a model with MAP estimate = -920.3197
expansions: [(0, 6), (326, 2), (332, 2), (338, 1), (479, 1), (487, 1), (488, 2)]
discards: [  1   2   3   4   5   6   7   8 152 272 273 362 387 412 413 469 470 492
 493 494 495 496 497]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 934.9789 - loglik: -9.3366e+02 - logprior: -1.3147e+00
Epoch 2/2
49/49 - 56s - loss: 922.0386 - loglik: -9.2527e+02 - logprior: 3.2302
Fitted a model with MAP estimate = -919.6895
expansions: [(0, 6), (341, 1), (485, 3)]
discards: [  0   1   2   3   4   5   6 486 487 488 489]
Re-initialized the encoder parameters.
Fitting a model of length 489 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 932.1000 - loglik: -9.3282e+02 - logprior: 0.7242
Epoch 2/10
49/49 - 55s - loss: 918.8727 - loglik: -9.2330e+02 - logprior: 4.4263
Epoch 3/10
49/49 - 55s - loss: 917.6145 - loglik: -9.2240e+02 - logprior: 4.7875
Epoch 4/10
49/49 - 56s - loss: 923.0330 - loglik: -9.2835e+02 - logprior: 5.3184
Fitted a model with MAP estimate = -916.3205
Time for alignment: 790.5650
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1091.7987 - loglik: -1.0905e+03 - logprior: -1.2923e+00
Epoch 2/10
49/49 - 41s - loss: 958.6341 - loglik: -9.5884e+02 - logprior: 0.2099
Epoch 3/10
49/49 - 41s - loss: 947.6068 - loglik: -9.4785e+02 - logprior: 0.2396
Epoch 4/10
49/49 - 41s - loss: 947.8281 - loglik: -9.4807e+02 - logprior: 0.2373
Fitted a model with MAP estimate = -947.0597
expansions: [(0, 5), (36, 3), (131, 1), (139, 1), (181, 1), (191, 1), (211, 1), (212, 1), (229, 3), (230, 2), (231, 3), (232, 1), (235, 2), (239, 2), (241, 1), (242, 1), (251, 1), (254, 8), (255, 2), (271, 1), (272, 1), (273, 1), (275, 1), (276, 2), (293, 3), (294, 1), (295, 5), (296, 2), (297, 4), (299, 5), (300, 2), (308, 1), (309, 1), (310, 1), (312, 2), (313, 2), (314, 1), (316, 1), (322, 1), (323, 1), (324, 1), (346, 5), (347, 1), (348, 1), (366, 2), (394, 2), (395, 3), (399, 11), (400, 2), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 939.7740 - loglik: -9.3741e+02 - logprior: -2.3603e+00
Epoch 2/2
49/49 - 57s - loss: 925.6729 - loglik: -9.2776e+02 - logprior: 2.0851
Fitted a model with MAP estimate = -919.3552
expansions: [(0, 7), (330, 1), (486, 1), (487, 2), (488, 1)]
discards: [  1   2   3   4   5   6  23  24 228 367 368 409 464 489 490 491 493 494
 495 496 497]
Re-initialized the encoder parameters.
Fitting a model of length 489 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 931.5850 - loglik: -9.3010e+02 - logprior: -1.4899e+00
Epoch 2/2
49/49 - 56s - loss: 925.5378 - loglik: -9.2854e+02 - logprior: 2.9981
Fitted a model with MAP estimate = -920.8987
expansions: [(0, 6), (322, 1), (333, 1), (345, 1), (486, 1), (487, 1)]
discards: [  1   2   3   4   5   6   7  24 488]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 931.9627 - loglik: -9.3103e+02 - logprior: -9.3734e-01
Epoch 2/10
49/49 - 56s - loss: 922.9277 - loglik: -9.2751e+02 - logprior: 4.5819
Epoch 3/10
49/49 - 56s - loss: 916.4063 - loglik: -9.2158e+02 - logprior: 5.1769
Epoch 4/10
49/49 - 56s - loss: 918.8090 - loglik: -9.2458e+02 - logprior: 5.7721
Fitted a model with MAP estimate = -915.5896
Time for alignment: 790.5811
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1095.2163 - loglik: -1.0940e+03 - logprior: -1.2606e+00
Epoch 2/10
49/49 - 41s - loss: 959.8690 - loglik: -9.6033e+02 - logprior: 0.4563
Epoch 3/10
49/49 - 41s - loss: 957.4852 - loglik: -9.5799e+02 - logprior: 0.5063
Epoch 4/10
49/49 - 41s - loss: 949.1856 - loglik: -9.4954e+02 - logprior: 0.3542
Epoch 5/10
49/49 - 41s - loss: 955.6517 - loglik: -9.5612e+02 - logprior: 0.4644
Fitted a model with MAP estimate = -951.0280
expansions: [(0, 5), (66, 1), (130, 1), (140, 1), (185, 1), (213, 3), (227, 1), (229, 5), (230, 3), (231, 1), (232, 1), (233, 1), (234, 1), (237, 2), (239, 1), (240, 1), (245, 1), (248, 1), (250, 2), (251, 5), (252, 3), (253, 2), (267, 1), (268, 2), (269, 8), (287, 2), (288, 2), (289, 4), (290, 1), (293, 3), (295, 4), (296, 2), (306, 1), (307, 1), (309, 4), (310, 1), (323, 1), (324, 1), (326, 1), (344, 5), (345, 1), (393, 6), (395, 1), (398, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 35 36 37]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 938.5157 - loglik: -9.3656e+02 - logprior: -1.9566e+00
Epoch 2/2
49/49 - 55s - loss: 925.3735 - loglik: -9.2769e+02 - logprior: 2.3132
Fitted a model with MAP estimate = -921.0775
expansions: [(0, 6), (301, 1), (333, 1), (477, 1), (485, 6)]
discards: [  1   2   3   4   5   6   7   8 273 329]
Re-initialized the encoder parameters.
Fitting a model of length 495 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 930.7722 - loglik: -9.2904e+02 - logprior: -1.7342e+00
Epoch 2/2
49/49 - 57s - loss: 920.5269 - loglik: -9.2368e+02 - logprior: 3.1579
Fitted a model with MAP estimate = -917.7962
expansions: [(0, 6), (327, 1), (487, 1), (488, 1)]
discards: [  1   2   3   4   5   6   7 491 492 493 494]
Re-initialized the encoder parameters.
Fitting a model of length 493 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 931.2804 - loglik: -9.3050e+02 - logprior: -7.7960e-01
Epoch 2/10
49/49 - 56s - loss: 917.7593 - loglik: -9.2250e+02 - logprior: 4.7390
Epoch 3/10
49/49 - 56s - loss: 922.2425 - loglik: -9.2751e+02 - logprior: 5.2658
Fitted a model with MAP estimate = -915.8751
Time for alignment: 774.8851
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1102.5095 - loglik: -1.1014e+03 - logprior: -1.0915e+00
Epoch 2/10
49/49 - 41s - loss: 960.4818 - loglik: -9.6092e+02 - logprior: 0.4396
Epoch 3/10
49/49 - 41s - loss: 952.1885 - loglik: -9.5265e+02 - logprior: 0.4608
Epoch 4/10
49/49 - 41s - loss: 953.6894 - loglik: -9.5398e+02 - logprior: 0.2868
Fitted a model with MAP estimate = -952.5111
expansions: [(0, 5), (141, 1), (215, 1), (235, 3), (236, 1), (238, 1), (240, 1), (264, 1), (265, 1), (267, 1), (268, 6), (269, 2), (285, 1), (286, 2), (287, 3), (288, 4), (295, 1), (303, 2), (304, 4), (305, 3), (306, 3), (307, 2), (309, 3), (310, 3), (320, 2), (321, 9), (322, 2), (323, 2), (324, 2), (325, 2), (326, 2), (327, 1), (345, 6), (346, 2), (379, 3), (392, 3), (393, 1), (395, 11)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 33 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 944.9810 - loglik: -9.4275e+02 - logprior: -2.2272e+00
Epoch 2/2
49/49 - 56s - loss: 921.1320 - loglik: -9.2318e+02 - logprior: 2.0471
Fitted a model with MAP estimate = -921.6679
expansions: [(0, 6), (303, 1), (339, 1), (358, 1), (412, 1), (477, 1), (479, 1), (481, 5)]
discards: [  1   2   3   4   5   6   7   8 226 381 410 463 485 486 487 488 489]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 932.6247 - loglik: -9.3120e+02 - logprior: -1.4273e+00
Epoch 2/2
49/49 - 56s - loss: 922.3438 - loglik: -9.2578e+02 - logprior: 3.4328
Fitted a model with MAP estimate = -920.1001
expansions: [(0, 6), (321, 2), (483, 1), (484, 1), (485, 1), (490, 6)]
discards: [1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 62s - loss: 930.1970 - loglik: -9.2864e+02 - logprior: -1.5559e+00
Epoch 2/10
49/49 - 58s - loss: 920.7282 - loglik: -9.2433e+02 - logprior: 3.6016
Epoch 3/10
49/49 - 58s - loss: 912.7557 - loglik: -9.1734e+02 - logprior: 4.5842
Epoch 4/10
49/49 - 58s - loss: 918.1370 - loglik: -9.2276e+02 - logprior: 4.6185
Fitted a model with MAP estimate = -914.7685
Time for alignment: 795.9597
Computed alignments with likelihoods: ['-913.7091', '-916.3205', '-915.5896', '-915.8751', '-914.7685']
Best model has likelihood: -913.7091  (prior= 5.0671 )
time for generating output: 0.4668
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8108829021072286
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 475.8760 - loglik: -4.7392e+02 - logprior: -1.9550e+00
Epoch 2/10
39/39 - 8s - loss: 409.0417 - loglik: -4.0746e+02 - logprior: -1.5837e+00
Epoch 3/10
39/39 - 8s - loss: 403.3290 - loglik: -4.0176e+02 - logprior: -1.5729e+00
Epoch 4/10
39/39 - 8s - loss: 401.2531 - loglik: -3.9971e+02 - logprior: -1.5399e+00
Epoch 5/10
39/39 - 8s - loss: 400.8548 - loglik: -3.9933e+02 - logprior: -1.5294e+00
Epoch 6/10
39/39 - 8s - loss: 400.5069 - loglik: -3.9895e+02 - logprior: -1.5596e+00
Epoch 7/10
39/39 - 8s - loss: 399.9544 - loglik: -3.9841e+02 - logprior: -1.5461e+00
Epoch 8/10
39/39 - 8s - loss: 399.6583 - loglik: -3.9811e+02 - logprior: -1.5497e+00
Epoch 9/10
39/39 - 8s - loss: 399.8799 - loglik: -3.9833e+02 - logprior: -1.5449e+00
Fitted a model with MAP estimate = -400.8124
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 1), (26, 1), (32, 1), (39, 3), (40, 1), (43, 1), (44, 1), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 396.0382 - loglik: -3.9318e+02 - logprior: -2.8562e+00
Epoch 2/2
39/39 - 11s - loss: 383.0478 - loglik: -3.8209e+02 - logprior: -9.5680e-01
Fitted a model with MAP estimate = -382.2922
expansions: []
discards: [ 13  28  50  75  92 113 134 141 167]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 385.5023 - loglik: -3.8359e+02 - logprior: -1.9154e+00
Epoch 2/2
39/39 - 10s - loss: 382.2687 - loglik: -3.8164e+02 - logprior: -6.2517e-01
Fitted a model with MAP estimate = -382.4706
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 384.6147 - loglik: -3.8303e+02 - logprior: -1.5866e+00
Epoch 2/10
41/41 - 10s - loss: 381.0538 - loglik: -3.8059e+02 - logprior: -4.6463e-01
Epoch 3/10
41/41 - 10s - loss: 380.3363 - loglik: -3.7996e+02 - logprior: -3.7535e-01
Epoch 4/10
41/41 - 10s - loss: 378.7146 - loglik: -3.7840e+02 - logprior: -3.1150e-01
Epoch 5/10
41/41 - 10s - loss: 376.4112 - loglik: -3.7616e+02 - logprior: -2.5618e-01
Epoch 6/10
41/41 - 10s - loss: 377.1121 - loglik: -3.7691e+02 - logprior: -2.0479e-01
Fitted a model with MAP estimate = -376.9432
Time for alignment: 234.6035
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 480.1137 - loglik: -4.7817e+02 - logprior: -1.9454e+00
Epoch 2/10
39/39 - 8s - loss: 416.8677 - loglik: -4.1537e+02 - logprior: -1.5012e+00
Epoch 3/10
39/39 - 8s - loss: 405.5246 - loglik: -4.0398e+02 - logprior: -1.5410e+00
Epoch 4/10
39/39 - 8s - loss: 403.3787 - loglik: -4.0186e+02 - logprior: -1.5212e+00
Epoch 5/10
39/39 - 8s - loss: 402.2145 - loglik: -4.0070e+02 - logprior: -1.5152e+00
Epoch 6/10
39/39 - 8s - loss: 401.7128 - loglik: -4.0022e+02 - logprior: -1.4969e+00
Epoch 7/10
39/39 - 8s - loss: 401.9076 - loglik: -4.0041e+02 - logprior: -1.4937e+00
Fitted a model with MAP estimate = -402.4646
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (17, 1), (23, 1), (27, 1), (39, 4), (40, 2), (43, 1), (54, 2), (56, 1), (57, 2), (59, 1), (66, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (122, 2), (126, 2), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 397.2263 - loglik: -3.9435e+02 - logprior: -2.8805e+00
Epoch 2/2
39/39 - 10s - loss: 384.4136 - loglik: -3.8338e+02 - logprior: -1.0352e+00
Fitted a model with MAP estimate = -383.4139
expansions: []
discards: [ 13  49  52  70  75  93 114 135 142 168]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 386.6124 - loglik: -3.8462e+02 - logprior: -1.9954e+00
Epoch 2/2
39/39 - 10s - loss: 383.5169 - loglik: -3.8279e+02 - logprior: -7.2429e-01
Fitted a model with MAP estimate = -384.0324
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 385.0332 - loglik: -3.8339e+02 - logprior: -1.6405e+00
Epoch 2/10
41/41 - 10s - loss: 381.7896 - loglik: -3.8127e+02 - logprior: -5.1939e-01
Epoch 3/10
41/41 - 10s - loss: 381.4627 - loglik: -3.8101e+02 - logprior: -4.4904e-01
Epoch 4/10
41/41 - 10s - loss: 379.4744 - loglik: -3.7907e+02 - logprior: -3.9974e-01
Epoch 5/10
41/41 - 10s - loss: 378.3781 - loglik: -3.7803e+02 - logprior: -3.4826e-01
Epoch 6/10
41/41 - 10s - loss: 377.7837 - loglik: -3.7748e+02 - logprior: -3.0399e-01
Epoch 7/10
41/41 - 10s - loss: 378.4682 - loglik: -3.7822e+02 - logprior: -2.4968e-01
Fitted a model with MAP estimate = -377.6741
Time for alignment: 229.5396
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 478.0847 - loglik: -4.7614e+02 - logprior: -1.9481e+00
Epoch 2/10
39/39 - 8s - loss: 411.1290 - loglik: -4.0952e+02 - logprior: -1.6093e+00
Epoch 3/10
39/39 - 8s - loss: 403.1479 - loglik: -4.0153e+02 - logprior: -1.6146e+00
Epoch 4/10
39/39 - 8s - loss: 400.7471 - loglik: -3.9914e+02 - logprior: -1.6034e+00
Epoch 5/10
39/39 - 8s - loss: 400.0644 - loglik: -3.9845e+02 - logprior: -1.6103e+00
Epoch 6/10
39/39 - 8s - loss: 399.6099 - loglik: -3.9800e+02 - logprior: -1.6129e+00
Epoch 7/10
39/39 - 8s - loss: 399.3949 - loglik: -3.9779e+02 - logprior: -1.6034e+00
Epoch 8/10
39/39 - 8s - loss: 399.3283 - loglik: -3.9772e+02 - logprior: -1.6065e+00
Epoch 9/10
39/39 - 8s - loss: 399.3649 - loglik: -3.9777e+02 - logprior: -1.5997e+00
Fitted a model with MAP estimate = -399.8733
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 1), (26, 1), (40, 3), (41, 1), (44, 1), (45, 1), (55, 2), (57, 1), (58, 1), (68, 1), (69, 1), (71, 2), (80, 1), (90, 2), (94, 2), (96, 1), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 395.1018 - loglik: -3.9226e+02 - logprior: -2.8451e+00
Epoch 2/2
39/39 - 11s - loss: 381.4555 - loglik: -3.8043e+02 - logprior: -1.0283e+00
Fitted a model with MAP estimate = -380.0107
expansions: []
discards: [ 13  28  50  71  92 115 120 129 158 176]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 384.1211 - loglik: -3.8222e+02 - logprior: -1.9017e+00
Epoch 2/2
39/39 - 10s - loss: 380.9383 - loglik: -3.8031e+02 - logprior: -6.2390e-01
Fitted a model with MAP estimate = -380.1746
expansions: [(150, 1)]
discards: [127]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 381.6275 - loglik: -3.8009e+02 - logprior: -1.5357e+00
Epoch 2/10
41/41 - 10s - loss: 378.7710 - loglik: -3.7832e+02 - logprior: -4.4606e-01
Epoch 3/10
41/41 - 10s - loss: 378.5140 - loglik: -3.7813e+02 - logprior: -3.8484e-01
Epoch 4/10
41/41 - 10s - loss: 375.2640 - loglik: -3.7495e+02 - logprior: -3.1325e-01
Epoch 5/10
41/41 - 11s - loss: 375.6829 - loglik: -3.7542e+02 - logprior: -2.6536e-01
Fitted a model with MAP estimate = -375.2699
Time for alignment: 226.0750
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 478.3823 - loglik: -4.7647e+02 - logprior: -1.9109e+00
Epoch 2/10
39/39 - 8s - loss: 411.4514 - loglik: -4.0990e+02 - logprior: -1.5492e+00
Epoch 3/10
39/39 - 8s - loss: 404.0811 - loglik: -4.0252e+02 - logprior: -1.5628e+00
Epoch 4/10
39/39 - 8s - loss: 401.7571 - loglik: -4.0017e+02 - logprior: -1.5877e+00
Epoch 5/10
39/39 - 8s - loss: 400.6403 - loglik: -3.9906e+02 - logprior: -1.5840e+00
Epoch 6/10
39/39 - 8s - loss: 400.6312 - loglik: -3.9905e+02 - logprior: -1.5830e+00
Epoch 7/10
39/39 - 8s - loss: 400.4926 - loglik: -3.9891e+02 - logprior: -1.5788e+00
Epoch 8/10
39/39 - 8s - loss: 399.9550 - loglik: -3.9838e+02 - logprior: -1.5788e+00
Epoch 9/10
39/39 - 8s - loss: 399.9287 - loglik: -3.9835e+02 - logprior: -1.5746e+00
Epoch 10/10
39/39 - 8s - loss: 400.2922 - loglik: -3.9872e+02 - logprior: -1.5759e+00
Fitted a model with MAP estimate = -401.1029
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 1), (26, 2), (32, 1), (39, 3), (41, 1), (43, 1), (44, 1), (54, 1), (55, 1), (56, 2), (57, 2), (68, 1), (69, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (116, 1), (122, 2), (126, 1), (131, 1)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 396.8113 - loglik: -3.9391e+02 - logprior: -2.8969e+00
Epoch 2/2
39/39 - 10s - loss: 383.8416 - loglik: -3.8282e+02 - logprior: -1.0260e+00
Fitted a model with MAP estimate = -382.8628
expansions: []
discards: [ 13  19  35  74  77  92 114 135 142]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 385.8578 - loglik: -3.8390e+02 - logprior: -1.9575e+00
Epoch 2/2
39/39 - 10s - loss: 382.5820 - loglik: -3.8192e+02 - logprior: -6.6107e-01
Fitted a model with MAP estimate = -382.9480
expansions: []
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 384.6300 - loglik: -3.8302e+02 - logprior: -1.6145e+00
Epoch 2/10
41/41 - 10s - loss: 382.5913 - loglik: -3.8210e+02 - logprior: -4.8691e-01
Epoch 3/10
41/41 - 10s - loss: 379.3476 - loglik: -3.7896e+02 - logprior: -3.8298e-01
Epoch 4/10
41/41 - 10s - loss: 380.4044 - loglik: -3.8008e+02 - logprior: -3.1943e-01
Fitted a model with MAP estimate = -377.8612
Time for alignment: 222.8610
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 478.7524 - loglik: -4.7680e+02 - logprior: -1.9507e+00
Epoch 2/10
39/39 - 8s - loss: 412.3685 - loglik: -4.1074e+02 - logprior: -1.6259e+00
Epoch 3/10
39/39 - 8s - loss: 405.5694 - loglik: -4.0394e+02 - logprior: -1.6320e+00
Epoch 4/10
39/39 - 8s - loss: 403.3294 - loglik: -4.0172e+02 - logprior: -1.6108e+00
Epoch 5/10
39/39 - 8s - loss: 402.2039 - loglik: -4.0058e+02 - logprior: -1.6271e+00
Epoch 6/10
39/39 - 8s - loss: 401.3042 - loglik: -3.9968e+02 - logprior: -1.6236e+00
Epoch 7/10
39/39 - 8s - loss: 401.2999 - loglik: -3.9968e+02 - logprior: -1.6156e+00
Epoch 8/10
39/39 - 8s - loss: 401.5527 - loglik: -3.9994e+02 - logprior: -1.6138e+00
Fitted a model with MAP estimate = -402.2092
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (32, 1), (40, 3), (41, 1), (44, 1), (45, 1), (55, 1), (56, 1), (57, 2), (58, 2), (67, 2), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (122, 1), (126, 2), (133, 2)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 398.2730 - loglik: -3.9537e+02 - logprior: -2.9052e+00
Epoch 2/2
39/39 - 11s - loss: 383.9303 - loglik: -3.8281e+02 - logprior: -1.1229e+00
Fitted a model with MAP estimate = -382.4232
expansions: [(60, 1)]
discards: [ 13  72  75  86  93 114 121 130 167 176]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 384.5960 - loglik: -3.8264e+02 - logprior: -1.9602e+00
Epoch 2/2
39/39 - 10s - loss: 381.0076 - loglik: -3.8030e+02 - logprior: -7.0601e-01
Fitted a model with MAP estimate = -381.1594
expansions: []
discards: [ 59 129]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 14s - loss: 384.2044 - loglik: -3.8259e+02 - logprior: -1.6191e+00
Epoch 2/10
41/41 - 10s - loss: 381.4147 - loglik: -3.8087e+02 - logprior: -5.4516e-01
Epoch 3/10
41/41 - 10s - loss: 379.1821 - loglik: -3.7873e+02 - logprior: -4.5320e-01
Epoch 4/10
41/41 - 10s - loss: 378.8695 - loglik: -3.7847e+02 - logprior: -3.9860e-01
Epoch 5/10
41/41 - 10s - loss: 379.0180 - loglik: -3.7867e+02 - logprior: -3.5217e-01
Fitted a model with MAP estimate = -377.2950
Time for alignment: 219.1593
Computed alignments with likelihoods: ['-376.9432', '-377.6741', '-375.2699', '-377.8612', '-377.2950']
Best model has likelihood: -375.2699  (prior= -0.1983 )
time for generating output: 0.2812
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.31367731367731366
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5221 - loglik: -1.1162e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 105.6015 - loglik: -8.9024e+01 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 80.4695 - loglik: -7.2207e+01 - logprior: -8.2624e+00
Epoch 4/10
10/10 - 0s - loss: 69.8762 - loglik: -6.4658e+01 - logprior: -5.2177e+00
Epoch 5/10
10/10 - 0s - loss: 65.5670 - loglik: -6.1864e+01 - logprior: -3.7029e+00
Epoch 6/10
10/10 - 0s - loss: 63.8289 - loglik: -6.0894e+01 - logprior: -2.9349e+00
Epoch 7/10
10/10 - 0s - loss: 63.2586 - loglik: -6.0786e+01 - logprior: -2.4731e+00
Epoch 8/10
10/10 - 0s - loss: 62.7507 - loglik: -6.0648e+01 - logprior: -2.1025e+00
Epoch 9/10
10/10 - 0s - loss: 62.6478 - loglik: -6.0816e+01 - logprior: -1.8319e+00
Epoch 10/10
10/10 - 0s - loss: 62.2031 - loglik: -6.0508e+01 - logprior: -1.6951e+00
Fitted a model with MAP estimate = -62.2153
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.8092 - loglik: -5.8793e+01 - logprior: -7.9016e+01
Epoch 2/2
10/10 - 0s - loss: 79.6025 - loglik: -5.4017e+01 - logprior: -2.5585e+01
Fitted a model with MAP estimate = -68.5894
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 117.4331 - loglik: -5.1744e+01 - logprior: -6.5689e+01
Epoch 2/2
10/10 - 0s - loss: 74.0425 - loglik: -5.0989e+01 - logprior: -2.3054e+01
Fitted a model with MAP estimate = -65.0375
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.4810 - loglik: -5.1761e+01 - logprior: -6.5720e+01
Epoch 2/10
10/10 - 0s - loss: 79.2024 - loglik: -5.1939e+01 - logprior: -2.7264e+01
Epoch 3/10
10/10 - 0s - loss: 68.0526 - loglik: -5.2065e+01 - logprior: -1.5988e+01
Epoch 4/10
10/10 - 0s - loss: 58.5781 - loglik: -5.2139e+01 - logprior: -6.4390e+00
Epoch 5/10
10/10 - 0s - loss: 55.2362 - loglik: -5.2420e+01 - logprior: -2.8162e+00
Epoch 6/10
10/10 - 0s - loss: 53.9867 - loglik: -5.2302e+01 - logprior: -1.6846e+00
Epoch 7/10
10/10 - 0s - loss: 53.2870 - loglik: -5.2176e+01 - logprior: -1.1111e+00
Epoch 8/10
10/10 - 0s - loss: 53.1899 - loglik: -5.2434e+01 - logprior: -7.5551e-01
Epoch 9/10
10/10 - 0s - loss: 52.9214 - loglik: -5.2414e+01 - logprior: -5.0731e-01
Epoch 10/10
10/10 - 0s - loss: 52.8281 - loglik: -5.2524e+01 - logprior: -3.0366e-01
Fitted a model with MAP estimate = -52.6417
Time for alignment: 24.2634
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.5989 - loglik: -1.1170e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 105.4680 - loglik: -8.8892e+01 - logprior: -1.6576e+01
Epoch 3/10
10/10 - 0s - loss: 80.6905 - loglik: -7.2429e+01 - logprior: -8.2615e+00
Epoch 4/10
10/10 - 0s - loss: 69.9042 - loglik: -6.4684e+01 - logprior: -5.2197e+00
Epoch 5/10
10/10 - 0s - loss: 65.5784 - loglik: -6.1876e+01 - logprior: -3.7022e+00
Epoch 6/10
10/10 - 0s - loss: 63.7857 - loglik: -6.0850e+01 - logprior: -2.9355e+00
Epoch 7/10
10/10 - 0s - loss: 63.2629 - loglik: -6.0789e+01 - logprior: -2.4742e+00
Epoch 8/10
10/10 - 0s - loss: 62.7303 - loglik: -6.0627e+01 - logprior: -2.1035e+00
Epoch 9/10
10/10 - 0s - loss: 62.6954 - loglik: -6.0863e+01 - logprior: -1.8327e+00
Epoch 10/10
10/10 - 0s - loss: 62.3065 - loglik: -6.0613e+01 - logprior: -1.6935e+00
Fitted a model with MAP estimate = -62.2211
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.8787 - loglik: -5.8865e+01 - logprior: -7.9014e+01
Epoch 2/2
10/10 - 0s - loss: 79.3813 - loglik: -5.3798e+01 - logprior: -2.5583e+01
Fitted a model with MAP estimate = -68.5735
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.2883 - loglik: -5.1601e+01 - logprior: -6.5687e+01
Epoch 2/2
10/10 - 0s - loss: 73.9756 - loglik: -5.0919e+01 - logprior: -2.3057e+01
Fitted a model with MAP estimate = -65.0327
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.3515 - loglik: -5.1634e+01 - logprior: -6.5718e+01
Epoch 2/10
10/10 - 0s - loss: 79.3033 - loglik: -5.2037e+01 - logprior: -2.7267e+01
Epoch 3/10
10/10 - 0s - loss: 68.0258 - loglik: -5.2029e+01 - logprior: -1.5997e+01
Epoch 4/10
10/10 - 0s - loss: 58.7344 - loglik: -5.2294e+01 - logprior: -6.4402e+00
Epoch 5/10
10/10 - 0s - loss: 55.1466 - loglik: -5.2328e+01 - logprior: -2.8183e+00
Epoch 6/10
10/10 - 0s - loss: 53.8264 - loglik: -5.2146e+01 - logprior: -1.6806e+00
Epoch 7/10
10/10 - 0s - loss: 53.5353 - loglik: -5.2430e+01 - logprior: -1.1057e+00
Epoch 8/10
10/10 - 0s - loss: 53.1364 - loglik: -5.2383e+01 - logprior: -7.5386e-01
Epoch 9/10
10/10 - 0s - loss: 52.9169 - loglik: -5.2414e+01 - logprior: -5.0251e-01
Epoch 10/10
10/10 - 0s - loss: 52.7287 - loglik: -5.2430e+01 - logprior: -2.9858e-01
Fitted a model with MAP estimate = -52.6370
Time for alignment: 24.3925
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5091 - loglik: -1.1161e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.5456 - loglik: -8.8969e+01 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 80.5375 - loglik: -7.2274e+01 - logprior: -8.2634e+00
Epoch 4/10
10/10 - 0s - loss: 69.5679 - loglik: -6.4338e+01 - logprior: -5.2295e+00
Epoch 5/10
10/10 - 0s - loss: 65.5654 - loglik: -6.1856e+01 - logprior: -3.7090e+00
Epoch 6/10
10/10 - 0s - loss: 63.9383 - loglik: -6.0997e+01 - logprior: -2.9412e+00
Epoch 7/10
10/10 - 0s - loss: 62.9593 - loglik: -6.0487e+01 - logprior: -2.4721e+00
Epoch 8/10
10/10 - 0s - loss: 62.9599 - loglik: -6.0860e+01 - logprior: -2.1002e+00
Fitted a model with MAP estimate = -62.6676
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 136.4324 - loglik: -5.8459e+01 - logprior: -7.7974e+01
Epoch 2/2
10/10 - 0s - loss: 78.9066 - loglik: -5.4023e+01 - logprior: -2.4883e+01
Fitted a model with MAP estimate = -68.1858
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.3623 - loglik: -5.1713e+01 - logprior: -6.5650e+01
Epoch 2/2
10/10 - 0s - loss: 73.9056 - loglik: -5.0831e+01 - logprior: -2.3074e+01
Fitted a model with MAP estimate = -65.1134
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.4902 - loglik: -5.1767e+01 - logprior: -6.5723e+01
Epoch 2/10
10/10 - 0s - loss: 79.1548 - loglik: -5.1889e+01 - logprior: -2.7266e+01
Epoch 3/10
10/10 - 0s - loss: 68.1742 - loglik: -5.2157e+01 - logprior: -1.6017e+01
Epoch 4/10
10/10 - 0s - loss: 58.8131 - loglik: -5.2335e+01 - logprior: -6.4786e+00
Epoch 5/10
10/10 - 0s - loss: 55.0788 - loglik: -5.2240e+01 - logprior: -2.8390e+00
Epoch 6/10
10/10 - 0s - loss: 53.9700 - loglik: -5.2270e+01 - logprior: -1.6995e+00
Epoch 7/10
10/10 - 0s - loss: 53.4492 - loglik: -5.2322e+01 - logprior: -1.1275e+00
Epoch 8/10
10/10 - 0s - loss: 53.1509 - loglik: -5.2379e+01 - logprior: -7.7183e-01
Epoch 9/10
10/10 - 0s - loss: 52.9732 - loglik: -5.2453e+01 - logprior: -5.1996e-01
Epoch 10/10
10/10 - 0s - loss: 52.8227 - loglik: -5.2500e+01 - logprior: -3.2316e-01
Fitted a model with MAP estimate = -52.6758
Time for alignment: 24.3312
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5647 - loglik: -1.1166e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 105.5487 - loglik: -8.8971e+01 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 80.5427 - loglik: -7.2277e+01 - logprior: -8.2657e+00
Epoch 4/10
10/10 - 0s - loss: 69.4834 - loglik: -6.4242e+01 - logprior: -5.2415e+00
Epoch 5/10
10/10 - 0s - loss: 65.3337 - loglik: -6.1620e+01 - logprior: -3.7142e+00
Epoch 6/10
10/10 - 0s - loss: 63.8567 - loglik: -6.0915e+01 - logprior: -2.9418e+00
Epoch 7/10
10/10 - 0s - loss: 63.1086 - loglik: -6.0642e+01 - logprior: -2.4666e+00
Epoch 8/10
10/10 - 0s - loss: 62.8418 - loglik: -6.0747e+01 - logprior: -2.0951e+00
Epoch 9/10
10/10 - 0s - loss: 62.5770 - loglik: -6.0744e+01 - logprior: -1.8329e+00
Epoch 10/10
10/10 - 0s - loss: 62.3105 - loglik: -6.0607e+01 - logprior: -1.7037e+00
Fitted a model with MAP estimate = -62.2059
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.8063 - loglik: -5.8794e+01 - logprior: -7.9013e+01
Epoch 2/2
10/10 - 0s - loss: 79.6234 - loglik: -5.4044e+01 - logprior: -2.5579e+01
Fitted a model with MAP estimate = -68.5699
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.4280 - loglik: -5.1742e+01 - logprior: -6.5686e+01
Epoch 2/2
10/10 - 0s - loss: 73.9453 - loglik: -5.0889e+01 - logprior: -2.3057e+01
Fitted a model with MAP estimate = -65.0615
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.4239 - loglik: -5.1701e+01 - logprior: -6.5723e+01
Epoch 2/10
10/10 - 0s - loss: 79.2124 - loglik: -5.1944e+01 - logprior: -2.7268e+01
Epoch 3/10
10/10 - 0s - loss: 68.1246 - loglik: -5.2130e+01 - logprior: -1.5994e+01
Epoch 4/10
10/10 - 0s - loss: 58.6291 - loglik: -5.2183e+01 - logprior: -6.4465e+00
Epoch 5/10
10/10 - 0s - loss: 55.1700 - loglik: -5.2349e+01 - logprior: -2.8211e+00
Epoch 6/10
10/10 - 0s - loss: 53.9679 - loglik: -5.2280e+01 - logprior: -1.6884e+00
Epoch 7/10
10/10 - 0s - loss: 53.3612 - loglik: -5.2249e+01 - logprior: -1.1119e+00
Epoch 8/10
10/10 - 0s - loss: 53.1459 - loglik: -5.2389e+01 - logprior: -7.5678e-01
Epoch 9/10
10/10 - 0s - loss: 52.7831 - loglik: -5.2274e+01 - logprior: -5.0951e-01
Epoch 10/10
10/10 - 0s - loss: 52.9226 - loglik: -5.2614e+01 - logprior: -3.0879e-01
Fitted a model with MAP estimate = -52.6524
Time for alignment: 23.3373
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.5367 - loglik: -1.1164e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 105.5418 - loglik: -8.8963e+01 - logprior: -1.6578e+01
Epoch 3/10
10/10 - 0s - loss: 80.5473 - loglik: -7.2284e+01 - logprior: -8.2635e+00
Epoch 4/10
10/10 - 0s - loss: 69.8247 - loglik: -6.4602e+01 - logprior: -5.2229e+00
Epoch 5/10
10/10 - 0s - loss: 65.4851 - loglik: -6.1780e+01 - logprior: -3.7056e+00
Epoch 6/10
10/10 - 0s - loss: 63.8549 - loglik: -6.0920e+01 - logprior: -2.9347e+00
Epoch 7/10
10/10 - 0s - loss: 63.1333 - loglik: -6.0664e+01 - logprior: -2.4698e+00
Epoch 8/10
10/10 - 0s - loss: 62.8658 - loglik: -6.0767e+01 - logprior: -2.0992e+00
Epoch 9/10
10/10 - 0s - loss: 62.4900 - loglik: -6.0659e+01 - logprior: -1.8313e+00
Epoch 10/10
10/10 - 0s - loss: 62.4558 - loglik: -6.0760e+01 - logprior: -1.6956e+00
Fitted a model with MAP estimate = -62.2120
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.7953 - loglik: -5.8783e+01 - logprior: -7.9013e+01
Epoch 2/2
10/10 - 0s - loss: 79.6186 - loglik: -5.4037e+01 - logprior: -2.5581e+01
Fitted a model with MAP estimate = -68.5781
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.2830 - loglik: -5.1596e+01 - logprior: -6.5687e+01
Epoch 2/2
10/10 - 0s - loss: 73.9878 - loglik: -5.0929e+01 - logprior: -2.3059e+01
Fitted a model with MAP estimate = -65.0589
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.4647 - loglik: -5.1745e+01 - logprior: -6.5719e+01
Epoch 2/10
10/10 - 0s - loss: 79.2681 - loglik: -5.2002e+01 - logprior: -2.7266e+01
Epoch 3/10
10/10 - 0s - loss: 67.9521 - loglik: -5.1964e+01 - logprior: -1.5988e+01
Epoch 4/10
10/10 - 0s - loss: 58.8183 - loglik: -5.2381e+01 - logprior: -6.4372e+00
Epoch 5/10
10/10 - 0s - loss: 55.0122 - loglik: -5.2198e+01 - logprior: -2.8142e+00
Epoch 6/10
10/10 - 0s - loss: 54.0230 - loglik: -5.2338e+01 - logprior: -1.6851e+00
Epoch 7/10
10/10 - 0s - loss: 53.3386 - loglik: -5.2230e+01 - logprior: -1.1088e+00
Epoch 8/10
10/10 - 0s - loss: 53.2016 - loglik: -5.2447e+01 - logprior: -7.5496e-01
Epoch 9/10
10/10 - 0s - loss: 52.9078 - loglik: -5.2401e+01 - logprior: -5.0672e-01
Epoch 10/10
10/10 - 0s - loss: 52.6516 - loglik: -5.2350e+01 - logprior: -3.0151e-01
Fitted a model with MAP estimate = -52.6417
Time for alignment: 24.6313
Computed alignments with likelihoods: ['-52.6417', '-52.6370', '-52.6758', '-52.6524', '-52.6417']
Best model has likelihood: -52.6370  (prior= -0.1996 )
time for generating output: 0.0877
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9202898550724637
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 372.8091 - loglik: -3.7002e+02 - logprior: -2.7913e+00
Epoch 2/10
20/20 - 3s - loss: 329.6094 - loglik: -3.2837e+02 - logprior: -1.2403e+00
Epoch 3/10
20/20 - 3s - loss: 310.0562 - loglik: -3.0858e+02 - logprior: -1.4772e+00
Epoch 4/10
20/20 - 3s - loss: 305.0711 - loglik: -3.0367e+02 - logprior: -1.4008e+00
Epoch 5/10
20/20 - 3s - loss: 303.4002 - loglik: -3.0198e+02 - logprior: -1.4250e+00
Epoch 6/10
20/20 - 3s - loss: 302.6857 - loglik: -3.0128e+02 - logprior: -1.4026e+00
Epoch 7/10
20/20 - 3s - loss: 302.7838 - loglik: -3.0138e+02 - logprior: -1.4066e+00
Fitted a model with MAP estimate = -290.0814
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (22, 1), (31, 1), (37, 2), (38, 2), (40, 1), (47, 1), (51, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (93, 1), (94, 1), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.2213 - loglik: -2.9963e+02 - logprior: -2.5922e+00
Epoch 2/2
40/40 - 5s - loss: 291.2745 - loglik: -2.9045e+02 - logprior: -8.2172e-01
Fitted a model with MAP estimate = -277.8821
expansions: [(140, 2)]
discards: [  8  46  48  74  82  87 104 107 110 126]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 293.7779 - loglik: -2.9185e+02 - logprior: -1.9266e+00
Epoch 2/2
40/40 - 5s - loss: 290.3278 - loglik: -2.8961e+02 - logprior: -7.1526e-01
Fitted a model with MAP estimate = -277.3386
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 274.6077 - loglik: -2.7361e+02 - logprior: -9.9299e-01
Epoch 2/10
57/57 - 6s - loss: 272.4769 - loglik: -2.7186e+02 - logprior: -6.1249e-01
Epoch 3/10
57/57 - 6s - loss: 271.8140 - loglik: -2.7121e+02 - logprior: -6.0535e-01
Epoch 4/10
57/57 - 6s - loss: 269.6341 - loglik: -2.6905e+02 - logprior: -5.8401e-01
Epoch 5/10
57/57 - 6s - loss: 270.5938 - loglik: -2.7001e+02 - logprior: -5.7954e-01
Fitted a model with MAP estimate = -268.8202
Time for alignment: 123.4119
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.7182 - loglik: -3.6993e+02 - logprior: -2.7925e+00
Epoch 2/10
20/20 - 3s - loss: 326.7792 - loglik: -3.2555e+02 - logprior: -1.2284e+00
Epoch 3/10
20/20 - 3s - loss: 307.6939 - loglik: -3.0624e+02 - logprior: -1.4528e+00
Epoch 4/10
20/20 - 3s - loss: 304.2543 - loglik: -3.0289e+02 - logprior: -1.3615e+00
Epoch 5/10
20/20 - 3s - loss: 302.4611 - loglik: -3.0108e+02 - logprior: -1.3767e+00
Epoch 6/10
20/20 - 3s - loss: 302.0778 - loglik: -3.0073e+02 - logprior: -1.3491e+00
Epoch 7/10
20/20 - 3s - loss: 301.2567 - loglik: -2.9991e+02 - logprior: -1.3506e+00
Epoch 8/10
20/20 - 3s - loss: 301.5223 - loglik: -3.0017e+02 - logprior: -1.3490e+00
Fitted a model with MAP estimate = -289.3414
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 301.8715 - loglik: -2.9928e+02 - logprior: -2.5932e+00
Epoch 2/2
40/40 - 5s - loss: 291.5682 - loglik: -2.9076e+02 - logprior: -8.0494e-01
Fitted a model with MAP estimate = -277.5188
expansions: [(133, 1)]
discards: [  8  45  73  81  86 103 109 126]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 293.3318 - loglik: -2.9153e+02 - logprior: -1.8044e+00
Epoch 2/2
40/40 - 5s - loss: 290.4551 - loglik: -2.8982e+02 - logprior: -6.3587e-01
Fitted a model with MAP estimate = -277.7940
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 274.7480 - loglik: -2.7378e+02 - logprior: -9.6734e-01
Epoch 2/10
57/57 - 6s - loss: 272.6007 - loglik: -2.7194e+02 - logprior: -6.6320e-01
Epoch 3/10
57/57 - 6s - loss: 271.0424 - loglik: -2.7038e+02 - logprior: -6.6418e-01
Epoch 4/10
57/57 - 6s - loss: 270.2441 - loglik: -2.6959e+02 - logprior: -6.4928e-01
Epoch 5/10
57/57 - 6s - loss: 269.2272 - loglik: -2.6859e+02 - logprior: -6.3453e-01
Epoch 6/10
57/57 - 6s - loss: 268.4919 - loglik: -2.6788e+02 - logprior: -6.1366e-01
Epoch 7/10
57/57 - 6s - loss: 269.2744 - loglik: -2.6868e+02 - logprior: -5.9105e-01
Fitted a model with MAP estimate = -268.7058
Time for alignment: 139.6393
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.7137 - loglik: -3.6992e+02 - logprior: -2.7938e+00
Epoch 2/10
20/20 - 3s - loss: 329.9750 - loglik: -3.2873e+02 - logprior: -1.2435e+00
Epoch 3/10
20/20 - 3s - loss: 310.8754 - loglik: -3.0939e+02 - logprior: -1.4898e+00
Epoch 4/10
20/20 - 3s - loss: 305.5714 - loglik: -3.0415e+02 - logprior: -1.4229e+00
Epoch 5/10
20/20 - 3s - loss: 303.8757 - loglik: -3.0247e+02 - logprior: -1.4069e+00
Epoch 6/10
20/20 - 3s - loss: 302.9820 - loglik: -3.0163e+02 - logprior: -1.3479e+00
Epoch 7/10
20/20 - 3s - loss: 302.8026 - loglik: -3.0147e+02 - logprior: -1.3339e+00
Epoch 8/10
20/20 - 3s - loss: 301.9993 - loglik: -3.0068e+02 - logprior: -1.3180e+00
Epoch 9/10
20/20 - 3s - loss: 302.5207 - loglik: -3.0120e+02 - logprior: -1.3235e+00
Fitted a model with MAP estimate = -290.8372
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (26, 2), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 2), (76, 2), (81, 2), (87, 1), (94, 3), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.7399 - loglik: -3.0011e+02 - logprior: -2.6303e+00
Epoch 2/2
40/40 - 5s - loss: 292.0408 - loglik: -2.9118e+02 - logprior: -8.5924e-01
Fitted a model with MAP estimate = -277.9700
expansions: [(139, 2)]
discards: [  8  34  46  81  85 101 107 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.1635 - loglik: -2.9232e+02 - logprior: -1.8426e+00
Epoch 2/2
40/40 - 5s - loss: 291.0511 - loglik: -2.9041e+02 - logprior: -6.3941e-01
Fitted a model with MAP estimate = -278.2426
expansions: []
discards: [129]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 274.1295 - loglik: -2.7311e+02 - logprior: -1.0164e+00
Epoch 2/10
57/57 - 6s - loss: 272.8752 - loglik: -2.7225e+02 - logprior: -6.3007e-01
Epoch 3/10
57/57 - 6s - loss: 271.6533 - loglik: -2.7105e+02 - logprior: -6.0259e-01
Epoch 4/10
57/57 - 6s - loss: 270.4729 - loglik: -2.6989e+02 - logprior: -5.8354e-01
Epoch 5/10
57/57 - 6s - loss: 269.3188 - loglik: -2.6874e+02 - logprior: -5.7551e-01
Epoch 6/10
57/57 - 6s - loss: 269.0367 - loglik: -2.6849e+02 - logprior: -5.5115e-01
Epoch 7/10
57/57 - 6s - loss: 268.6591 - loglik: -2.6813e+02 - logprior: -5.2553e-01
Epoch 8/10
57/57 - 6s - loss: 269.1774 - loglik: -2.6867e+02 - logprior: -5.0773e-01
Fitted a model with MAP estimate = -268.7253
Time for alignment: 148.7772
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.8638 - loglik: -3.7007e+02 - logprior: -2.7901e+00
Epoch 2/10
20/20 - 3s - loss: 329.0132 - loglik: -3.2779e+02 - logprior: -1.2235e+00
Epoch 3/10
20/20 - 3s - loss: 310.1972 - loglik: -3.0876e+02 - logprior: -1.4357e+00
Epoch 4/10
20/20 - 3s - loss: 304.8872 - loglik: -3.0352e+02 - logprior: -1.3646e+00
Epoch 5/10
20/20 - 3s - loss: 303.4012 - loglik: -3.0201e+02 - logprior: -1.3960e+00
Epoch 6/10
20/20 - 3s - loss: 302.3597 - loglik: -3.0101e+02 - logprior: -1.3506e+00
Epoch 7/10
20/20 - 3s - loss: 302.3030 - loglik: -3.0096e+02 - logprior: -1.3466e+00
Epoch 8/10
20/20 - 3s - loss: 302.1124 - loglik: -3.0076e+02 - logprior: -1.3544e+00
Epoch 9/10
20/20 - 3s - loss: 301.8575 - loglik: -3.0049e+02 - logprior: -1.3628e+00
Epoch 10/10
20/20 - 3s - loss: 301.6167 - loglik: -3.0025e+02 - logprior: -1.3624e+00
Fitted a model with MAP estimate = -290.9938
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 301.9504 - loglik: -2.9937e+02 - logprior: -2.5824e+00
Epoch 2/2
40/40 - 5s - loss: 291.4271 - loglik: -2.9067e+02 - logprior: -7.5840e-01
Fitted a model with MAP estimate = -277.9005
expansions: [(138, 2)]
discards: [  8  45  73  81 102 108 125]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 293.3906 - loglik: -2.9150e+02 - logprior: -1.8939e+00
Epoch 2/2
40/40 - 5s - loss: 290.0933 - loglik: -2.8940e+02 - logprior: -6.8859e-01
Fitted a model with MAP estimate = -278.2224
expansions: [(133, 2)]
discards: [100 131 132]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 274.7773 - loglik: -2.7379e+02 - logprior: -9.9042e-01
Epoch 2/10
57/57 - 6s - loss: 272.8927 - loglik: -2.7232e+02 - logprior: -5.7438e-01
Epoch 3/10
57/57 - 6s - loss: 271.4296 - loglik: -2.7086e+02 - logprior: -5.7112e-01
Epoch 4/10
57/57 - 6s - loss: 270.5174 - loglik: -2.6995e+02 - logprior: -5.6981e-01
Epoch 5/10
57/57 - 6s - loss: 268.8132 - loglik: -2.6826e+02 - logprior: -5.5586e-01
Epoch 6/10
57/57 - 6s - loss: 269.4933 - loglik: -2.6897e+02 - logprior: -5.2805e-01
Fitted a model with MAP estimate = -268.9681
Time for alignment: 140.0338
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 372.7079 - loglik: -3.6992e+02 - logprior: -2.7884e+00
Epoch 2/10
20/20 - 3s - loss: 328.7902 - loglik: -3.2757e+02 - logprior: -1.2213e+00
Epoch 3/10
20/20 - 3s - loss: 309.1298 - loglik: -3.0766e+02 - logprior: -1.4666e+00
Epoch 4/10
20/20 - 3s - loss: 304.8580 - loglik: -3.0345e+02 - logprior: -1.4068e+00
Epoch 5/10
20/20 - 3s - loss: 303.6377 - loglik: -3.0222e+02 - logprior: -1.4194e+00
Epoch 6/10
20/20 - 3s - loss: 302.3873 - loglik: -3.0099e+02 - logprior: -1.3926e+00
Epoch 7/10
20/20 - 3s - loss: 301.6373 - loglik: -3.0026e+02 - logprior: -1.3757e+00
Epoch 8/10
20/20 - 3s - loss: 302.1215 - loglik: -3.0075e+02 - logprior: -1.3713e+00
Fitted a model with MAP estimate = -290.3895
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (63, 2), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 302.3092 - loglik: -2.9972e+02 - logprior: -2.5916e+00
Epoch 2/2
40/40 - 5s - loss: 292.0986 - loglik: -2.9130e+02 - logprior: -7.9509e-01
Fitted a model with MAP estimate = -277.9426
expansions: [(137, 2)]
discards: [  8  45  73  81  85 101 107 124]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 293.8948 - loglik: -2.9201e+02 - logprior: -1.8870e+00
Epoch 2/2
40/40 - 5s - loss: 290.6328 - loglik: -2.8996e+02 - logprior: -6.7338e-01
Fitted a model with MAP estimate = -278.0188
expansions: [(131, 2)]
discards: [129 130]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 274.9091 - loglik: -2.7391e+02 - logprior: -9.9643e-01
Epoch 2/10
57/57 - 6s - loss: 272.6078 - loglik: -2.7199e+02 - logprior: -6.2031e-01
Epoch 3/10
57/57 - 6s - loss: 271.4951 - loglik: -2.7088e+02 - logprior: -6.1278e-01
Epoch 4/10
57/57 - 6s - loss: 270.2733 - loglik: -2.6968e+02 - logprior: -5.9256e-01
Epoch 5/10
57/57 - 6s - loss: 269.1858 - loglik: -2.6861e+02 - logprior: -5.8069e-01
Epoch 6/10
57/57 - 6s - loss: 269.0630 - loglik: -2.6850e+02 - logprior: -5.6003e-01
Epoch 7/10
57/57 - 6s - loss: 269.2427 - loglik: -2.6871e+02 - logprior: -5.3735e-01
Fitted a model with MAP estimate = -268.8553
Time for alignment: 137.7275
Computed alignments with likelihoods: ['-268.8202', '-268.7058', '-268.7253', '-268.9681', '-268.8553']
Best model has likelihood: -268.7058  (prior= -0.5957 )
time for generating output: 0.2809
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.47114093959731546
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 687.8925 - loglik: -6.7835e+02 - logprior: -9.5472e+00
Epoch 2/10
19/19 - 6s - loss: 622.6696 - loglik: -6.2209e+02 - logprior: -5.8254e-01
Epoch 3/10
19/19 - 6s - loss: 593.4442 - loglik: -5.9275e+02 - logprior: -6.9194e-01
Epoch 4/10
19/19 - 6s - loss: 583.7361 - loglik: -5.8330e+02 - logprior: -4.3406e-01
Epoch 5/10
19/19 - 6s - loss: 583.3167 - loglik: -5.8309e+02 - logprior: -2.2927e-01
Epoch 6/10
19/19 - 6s - loss: 582.6354 - loglik: -5.8245e+02 - logprior: -1.8615e-01
Epoch 7/10
19/19 - 6s - loss: 578.8133 - loglik: -5.7862e+02 - logprior: -1.9141e-01
Epoch 8/10
19/19 - 6s - loss: 579.6828 - loglik: -5.7950e+02 - logprior: -1.8730e-01
Fitted a model with MAP estimate = -579.9920
expansions: [(18, 1), (25, 1), (26, 1), (28, 2), (30, 2), (44, 4), (45, 1), (65, 4), (90, 4), (103, 1), (104, 1), (117, 1), (119, 1), (134, 1), (135, 1), (146, 2), (149, 2), (150, 1), (151, 1), (152, 1), (165, 5), (183, 1), (184, 2)]
discards: [3]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 600.3618 - loglik: -5.9134e+02 - logprior: -9.0212e+00
Epoch 2/2
19/19 - 8s - loss: 579.9907 - loglik: -5.7986e+02 - logprior: -1.3288e-01
Fitted a model with MAP estimate = -576.7043
expansions: [(121, 5), (224, 1), (225, 1)]
discards: [ 31  34  35  77 108 172 198 199 200 206 207]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 589.7258 - loglik: -5.8131e+02 - logprior: -8.4180e+00
Epoch 2/2
19/19 - 8s - loss: 576.9842 - loglik: -5.7737e+02 - logprior: 0.3846
Fitted a model with MAP estimate = -574.2114
expansions: []
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 586.2443 - loglik: -5.7814e+02 - logprior: -8.1068e+00
Epoch 2/10
19/19 - 8s - loss: 576.4137 - loglik: -5.7720e+02 - logprior: 0.7842
Epoch 3/10
19/19 - 8s - loss: 571.3196 - loglik: -5.7316e+02 - logprior: 1.8373
Epoch 4/10
19/19 - 8s - loss: 569.0913 - loglik: -5.7153e+02 - logprior: 2.4425
Epoch 5/10
19/19 - 8s - loss: 568.9751 - loglik: -5.7165e+02 - logprior: 2.6742
Epoch 6/10
19/19 - 8s - loss: 565.6467 - loglik: -5.6850e+02 - logprior: 2.8504
Epoch 7/10
19/19 - 8s - loss: 564.5912 - loglik: -5.6759e+02 - logprior: 3.0034
Epoch 8/10
19/19 - 8s - loss: 565.7289 - loglik: -5.6889e+02 - logprior: 3.1622
Fitted a model with MAP estimate = -564.3803
Time for alignment: 179.2437
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 687.5753 - loglik: -6.7803e+02 - logprior: -9.5406e+00
Epoch 2/10
19/19 - 6s - loss: 622.8173 - loglik: -6.2218e+02 - logprior: -6.3255e-01
Epoch 3/10
19/19 - 6s - loss: 592.8601 - loglik: -5.9221e+02 - logprior: -6.5475e-01
Epoch 4/10
19/19 - 6s - loss: 588.0657 - loglik: -5.8764e+02 - logprior: -4.2747e-01
Epoch 5/10
19/19 - 6s - loss: 584.2088 - loglik: -5.8393e+02 - logprior: -2.7498e-01
Epoch 6/10
19/19 - 6s - loss: 582.1598 - loglik: -5.8186e+02 - logprior: -2.9810e-01
Epoch 7/10
19/19 - 6s - loss: 582.6351 - loglik: -5.8234e+02 - logprior: -2.9153e-01
Fitted a model with MAP estimate = -582.0231
expansions: [(18, 1), (25, 1), (26, 1), (28, 2), (49, 16), (90, 3), (99, 4), (103, 1), (118, 1), (120, 1), (135, 3), (136, 1), (149, 1), (150, 2), (162, 1), (163, 1), (166, 2), (182, 1), (183, 1), (184, 2), (185, 2)]
discards: [  2 171]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 600.3041 - loglik: -5.9120e+02 - logprior: -9.1069e+00
Epoch 2/2
19/19 - 8s - loss: 579.5083 - loglik: -5.7925e+02 - logprior: -2.5586e-01
Fitted a model with MAP estimate = -575.8212
expansions: [(112, 1), (230, 1)]
discards: [ 27  55  56  57  58  59  60 124 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 588.3873 - loglik: -5.7988e+02 - logprior: -8.5066e+00
Epoch 2/2
19/19 - 8s - loss: 575.9836 - loglik: -5.7636e+02 - logprior: 0.3792
Fitted a model with MAP estimate = -573.9428
expansions: [(220, 1), (223, 1)]
discards: [44 45 53]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 586.4509 - loglik: -5.7844e+02 - logprior: -8.0107e+00
Epoch 2/10
19/19 - 8s - loss: 575.0561 - loglik: -5.7593e+02 - logprior: 0.8720
Epoch 3/10
19/19 - 8s - loss: 572.0923 - loglik: -5.7399e+02 - logprior: 1.8983
Epoch 4/10
19/19 - 8s - loss: 569.3657 - loglik: -5.7177e+02 - logprior: 2.4079
Epoch 5/10
19/19 - 8s - loss: 567.7688 - loglik: -5.7040e+02 - logprior: 2.6265
Epoch 6/10
19/19 - 8s - loss: 566.5884 - loglik: -5.6939e+02 - logprior: 2.7980
Epoch 7/10
19/19 - 8s - loss: 565.6440 - loglik: -5.6859e+02 - logprior: 2.9451
Epoch 8/10
19/19 - 8s - loss: 565.8644 - loglik: -5.6896e+02 - logprior: 3.0962
Fitted a model with MAP estimate = -564.7353
Time for alignment: 171.9006
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 687.5402 - loglik: -6.7800e+02 - logprior: -9.5422e+00
Epoch 2/10
19/19 - 6s - loss: 621.4820 - loglik: -6.2095e+02 - logprior: -5.3554e-01
Epoch 3/10
19/19 - 6s - loss: 592.7068 - loglik: -5.9238e+02 - logprior: -3.2635e-01
Epoch 4/10
19/19 - 6s - loss: 588.4949 - loglik: -5.8858e+02 - logprior: 0.0892
Epoch 5/10
19/19 - 6s - loss: 584.0514 - loglik: -5.8432e+02 - logprior: 0.2690
Epoch 6/10
19/19 - 6s - loss: 585.4233 - loglik: -5.8571e+02 - logprior: 0.2913
Fitted a model with MAP estimate = -583.5521
expansions: [(25, 1), (28, 2), (30, 2), (40, 6), (63, 3), (64, 2), (90, 2), (104, 1), (117, 1), (119, 1), (135, 1), (148, 1), (150, 4), (162, 1), (163, 1), (183, 1), (184, 2), (198, 4)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 602.3751 - loglik: -5.9318e+02 - logprior: -9.1901e+00
Epoch 2/2
19/19 - 8s - loss: 583.9156 - loglik: -5.8335e+02 - logprior: -5.6992e-01
Fitted a model with MAP estimate = -580.4848
expansions: [(0, 2), (118, 4), (175, 1), (215, 1), (216, 1)]
discards: [ 32  33  39 181 189 197 198 229 230 231 232]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 598.7142 - loglik: -5.8554e+02 - logprior: -1.3171e+01
Epoch 2/2
19/19 - 8s - loss: 582.4406 - loglik: -5.8177e+02 - logprior: -6.7420e-01
Fitted a model with MAP estimate = -578.2007
expansions: [(39, 1), (185, 1), (197, 4)]
discards: [  1 190 191 199 200 201 202 203 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 594.4988 - loglik: -5.8618e+02 - logprior: -8.3181e+00
Epoch 2/10
19/19 - 7s - loss: 581.0257 - loglik: -5.8145e+02 - logprior: 0.4288
Epoch 3/10
19/19 - 7s - loss: 578.1760 - loglik: -5.7969e+02 - logprior: 1.5114
Epoch 4/10
19/19 - 7s - loss: 575.9246 - loglik: -5.7796e+02 - logprior: 2.0323
Epoch 5/10
19/19 - 7s - loss: 573.3260 - loglik: -5.7560e+02 - logprior: 2.2784
Epoch 6/10
19/19 - 7s - loss: 570.9681 - loglik: -5.7342e+02 - logprior: 2.4504
Epoch 7/10
19/19 - 7s - loss: 571.0386 - loglik: -5.7361e+02 - logprior: 2.5736
Fitted a model with MAP estimate = -570.5382
Time for alignment: 154.1503
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 687.7519 - loglik: -6.7821e+02 - logprior: -9.5375e+00
Epoch 2/10
19/19 - 6s - loss: 623.8384 - loglik: -6.2327e+02 - logprior: -5.7288e-01
Epoch 3/10
19/19 - 6s - loss: 593.5482 - loglik: -5.9295e+02 - logprior: -6.0281e-01
Epoch 4/10
19/19 - 6s - loss: 587.0504 - loglik: -5.8662e+02 - logprior: -4.3086e-01
Epoch 5/10
19/19 - 6s - loss: 583.4475 - loglik: -5.8322e+02 - logprior: -2.2251e-01
Epoch 6/10
19/19 - 6s - loss: 581.3983 - loglik: -5.8122e+02 - logprior: -1.7502e-01
Epoch 7/10
19/19 - 6s - loss: 582.3497 - loglik: -5.8218e+02 - logprior: -1.7038e-01
Fitted a model with MAP estimate = -581.3625
expansions: [(18, 1), (25, 1), (27, 1), (30, 2), (44, 5), (45, 1), (65, 2), (66, 3), (91, 3), (102, 1), (103, 2), (104, 1), (105, 1), (118, 1), (120, 1), (135, 3), (136, 1), (152, 3), (167, 4), (183, 1), (184, 2), (185, 2)]
discards: [  2 171]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 599.8264 - loglik: -5.9085e+02 - logprior: -8.9753e+00
Epoch 2/2
19/19 - 8s - loss: 578.8444 - loglik: -5.7868e+02 - logprior: -1.6292e-01
Fitted a model with MAP estimate = -574.6831
expansions: [(108, 1), (183, 1), (224, 1), (225, 1)]
discards: [186 201]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 586.3677 - loglik: -5.7795e+02 - logprior: -8.4175e+00
Epoch 2/2
19/19 - 8s - loss: 573.9807 - loglik: -5.7438e+02 - logprior: 0.3968
Fitted a model with MAP estimate = -572.4395
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 584.7261 - loglik: -5.7656e+02 - logprior: -8.1669e+00
Epoch 2/10
19/19 - 8s - loss: 573.9892 - loglik: -5.7469e+02 - logprior: 0.7012
Epoch 3/10
19/19 - 8s - loss: 571.0519 - loglik: -5.7280e+02 - logprior: 1.7528
Epoch 4/10
19/19 - 8s - loss: 567.6841 - loglik: -5.6996e+02 - logprior: 2.2741
Epoch 5/10
19/19 - 8s - loss: 567.0834 - loglik: -5.6962e+02 - logprior: 2.5335
Epoch 6/10
19/19 - 8s - loss: 565.7348 - loglik: -5.6849e+02 - logprior: 2.7541
Epoch 7/10
19/19 - 8s - loss: 563.3710 - loglik: -5.6630e+02 - logprior: 2.9245
Epoch 8/10
19/19 - 8s - loss: 564.4617 - loglik: -5.6755e+02 - logprior: 3.0862
Fitted a model with MAP estimate = -563.4726
Time for alignment: 174.5273
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 687.4135 - loglik: -6.7788e+02 - logprior: -9.5286e+00
Epoch 2/10
19/19 - 6s - loss: 624.7679 - loglik: -6.2415e+02 - logprior: -6.1986e-01
Epoch 3/10
19/19 - 6s - loss: 596.2739 - loglik: -5.9562e+02 - logprior: -6.5619e-01
Epoch 4/10
19/19 - 6s - loss: 587.4205 - loglik: -5.8697e+02 - logprior: -4.5139e-01
Epoch 5/10
19/19 - 6s - loss: 584.4676 - loglik: -5.8418e+02 - logprior: -2.9143e-01
Epoch 6/10
19/19 - 6s - loss: 584.7588 - loglik: -5.8452e+02 - logprior: -2.4001e-01
Fitted a model with MAP estimate = -582.9977
expansions: [(18, 1), (25, 1), (27, 1), (33, 2), (63, 2), (64, 2), (89, 4), (102, 1), (103, 1), (119, 1), (134, 2), (135, 2), (146, 2), (151, 1), (162, 1), (182, 1), (183, 1), (184, 2), (185, 2)]
discards: [  2 171]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 602.6808 - loglik: -5.9347e+02 - logprior: -9.2141e+00
Epoch 2/2
19/19 - 7s - loss: 582.1115 - loglik: -5.8172e+02 - logprior: -3.8687e-01
Fitted a model with MAP estimate = -578.8348
expansions: [(30, 1), (100, 1), (212, 1)]
discards: [ 70  71 152 166 172 173 174 175 176 209]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 591.4198 - loglik: -5.8277e+02 - logprior: -8.6473e+00
Epoch 2/2
19/19 - 7s - loss: 581.2317 - loglik: -5.8138e+02 - logprior: 0.1530
Fitted a model with MAP estimate = -577.7079
expansions: [(98, 2), (170, 5), (182, 2), (203, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 588.3978 - loglik: -5.8026e+02 - logprior: -8.1347e+00
Epoch 2/10
19/19 - 7s - loss: 577.1167 - loglik: -5.7769e+02 - logprior: 0.5730
Epoch 3/10
19/19 - 7s - loss: 572.1490 - loglik: -5.7378e+02 - logprior: 1.6260
Epoch 4/10
19/19 - 7s - loss: 571.8240 - loglik: -5.7395e+02 - logprior: 2.1220
Epoch 5/10
19/19 - 7s - loss: 567.6512 - loglik: -5.6998e+02 - logprior: 2.3335
Epoch 6/10
19/19 - 7s - loss: 568.6978 - loglik: -5.7120e+02 - logprior: 2.5034
Fitted a model with MAP estimate = -566.7263
Time for alignment: 143.9173
Computed alignments with likelihoods: ['-564.3803', '-564.7353', '-570.5382', '-563.4726', '-566.7263']
Best model has likelihood: -563.4726  (prior= 3.1743 )
time for generating output: 0.2685
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7686847599164927
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.1830 - loglik: -1.5095e+02 - logprior: -3.2310e+00
Epoch 2/10
19/19 - 1s - loss: 120.9863 - loglik: -1.1943e+02 - logprior: -1.5543e+00
Epoch 3/10
19/19 - 1s - loss: 106.9043 - loglik: -1.0530e+02 - logprior: -1.6074e+00
Epoch 4/10
19/19 - 1s - loss: 104.1390 - loglik: -1.0251e+02 - logprior: -1.6281e+00
Epoch 5/10
19/19 - 1s - loss: 102.9937 - loglik: -1.0141e+02 - logprior: -1.5791e+00
Epoch 6/10
19/19 - 1s - loss: 102.5863 - loglik: -1.0102e+02 - logprior: -1.5644e+00
Epoch 7/10
19/19 - 1s - loss: 102.6371 - loglik: -1.0109e+02 - logprior: -1.5499e+00
Fitted a model with MAP estimate = -97.9822
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.3615 - loglik: -1.0104e+02 - logprior: -3.3173e+00
Epoch 2/2
19/19 - 1s - loss: 95.6159 - loglik: -9.4172e+01 - logprior: -1.4441e+00
Fitted a model with MAP estimate = -90.6042
expansions: []
discards: [12 38 42 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.7409 - loglik: -9.4573e+01 - logprior: -3.1674e+00
Epoch 2/2
19/19 - 1s - loss: 94.7521 - loglik: -9.3472e+01 - logprior: -1.2802e+00
Fitted a model with MAP estimate = -90.3592
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.1075 - loglik: -8.9526e+01 - logprior: -2.5814e+00
Epoch 2/10
21/21 - 1s - loss: 89.7536 - loglik: -8.8495e+01 - logprior: -1.2584e+00
Epoch 3/10
21/21 - 1s - loss: 88.8808 - loglik: -8.7709e+01 - logprior: -1.1718e+00
Epoch 4/10
21/21 - 1s - loss: 88.6091 - loglik: -8.7474e+01 - logprior: -1.1347e+00
Epoch 5/10
21/21 - 1s - loss: 88.1923 - loglik: -8.7073e+01 - logprior: -1.1196e+00
Epoch 6/10
21/21 - 1s - loss: 87.7962 - loglik: -8.6682e+01 - logprior: -1.1144e+00
Epoch 7/10
21/21 - 1s - loss: 87.6179 - loglik: -8.6505e+01 - logprior: -1.1127e+00
Epoch 8/10
21/21 - 1s - loss: 87.7593 - loglik: -8.6656e+01 - logprior: -1.1036e+00
Fitted a model with MAP estimate = -87.5544
Time for alignment: 43.9009
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.1259 - loglik: -1.5089e+02 - logprior: -3.2343e+00
Epoch 2/10
19/19 - 1s - loss: 121.6937 - loglik: -1.2016e+02 - logprior: -1.5345e+00
Epoch 3/10
19/19 - 1s - loss: 108.0896 - loglik: -1.0650e+02 - logprior: -1.5927e+00
Epoch 4/10
19/19 - 1s - loss: 105.3663 - loglik: -1.0381e+02 - logprior: -1.5530e+00
Epoch 5/10
19/19 - 1s - loss: 104.2528 - loglik: -1.0274e+02 - logprior: -1.5102e+00
Epoch 6/10
19/19 - 1s - loss: 103.8635 - loglik: -1.0237e+02 - logprior: -1.4936e+00
Epoch 7/10
19/19 - 1s - loss: 103.4608 - loglik: -1.0198e+02 - logprior: -1.4779e+00
Epoch 8/10
19/19 - 1s - loss: 103.3292 - loglik: -1.0186e+02 - logprior: -1.4725e+00
Epoch 9/10
19/19 - 1s - loss: 103.4023 - loglik: -1.0193e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -98.9776
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.6729 - loglik: -1.0038e+02 - logprior: -3.2961e+00
Epoch 2/2
19/19 - 1s - loss: 95.4566 - loglik: -9.4044e+01 - logprior: -1.4121e+00
Fitted a model with MAP estimate = -90.4889
expansions: []
discards: [11 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.0830 - loglik: -9.4916e+01 - logprior: -3.1667e+00
Epoch 2/2
19/19 - 1s - loss: 94.4677 - loglik: -9.3187e+01 - logprior: -1.2803e+00
Fitted a model with MAP estimate = -90.2725
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.2780 - loglik: -8.9715e+01 - logprior: -2.5630e+00
Epoch 2/10
21/21 - 1s - loss: 89.5065 - loglik: -8.8238e+01 - logprior: -1.2683e+00
Epoch 3/10
21/21 - 1s - loss: 88.9911 - loglik: -8.7814e+01 - logprior: -1.1771e+00
Epoch 4/10
21/21 - 1s - loss: 88.1936 - loglik: -8.7069e+01 - logprior: -1.1241e+00
Epoch 5/10
21/21 - 1s - loss: 88.4983 - loglik: -8.7379e+01 - logprior: -1.1193e+00
Fitted a model with MAP estimate = -87.8999
Time for alignment: 41.0290
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.0909 - loglik: -1.5086e+02 - logprior: -3.2260e+00
Epoch 2/10
19/19 - 1s - loss: 118.9857 - loglik: -1.1743e+02 - logprior: -1.5547e+00
Epoch 3/10
19/19 - 1s - loss: 106.2188 - loglik: -1.0459e+02 - logprior: -1.6266e+00
Epoch 4/10
19/19 - 1s - loss: 103.2930 - loglik: -1.0169e+02 - logprior: -1.5987e+00
Epoch 5/10
19/19 - 1s - loss: 102.6290 - loglik: -1.0109e+02 - logprior: -1.5424e+00
Epoch 6/10
19/19 - 1s - loss: 102.2168 - loglik: -1.0070e+02 - logprior: -1.5156e+00
Epoch 7/10
19/19 - 1s - loss: 101.9656 - loglik: -1.0045e+02 - logprior: -1.5129e+00
Epoch 8/10
19/19 - 1s - loss: 102.0880 - loglik: -1.0059e+02 - logprior: -1.4944e+00
Fitted a model with MAP estimate = -97.7349
expansions: [(5, 1), (6, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.9768 - loglik: -1.0067e+02 - logprior: -3.3097e+00
Epoch 2/2
19/19 - 1s - loss: 95.4804 - loglik: -9.4048e+01 - logprior: -1.4328e+00
Fitted a model with MAP estimate = -90.5287
expansions: []
discards: [ 9 21 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.6474 - loglik: -9.4479e+01 - logprior: -3.1689e+00
Epoch 2/2
19/19 - 1s - loss: 94.6744 - loglik: -9.3401e+01 - logprior: -1.2738e+00
Fitted a model with MAP estimate = -90.3274
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.1514 - loglik: -8.9576e+01 - logprior: -2.5754e+00
Epoch 2/10
21/21 - 1s - loss: 89.5931 - loglik: -8.8333e+01 - logprior: -1.2606e+00
Epoch 3/10
21/21 - 1s - loss: 88.9182 - loglik: -8.7756e+01 - logprior: -1.1625e+00
Epoch 4/10
21/21 - 1s - loss: 88.5698 - loglik: -8.7443e+01 - logprior: -1.1268e+00
Epoch 5/10
21/21 - 1s - loss: 88.0769 - loglik: -8.6963e+01 - logprior: -1.1141e+00
Epoch 6/10
21/21 - 1s - loss: 88.0888 - loglik: -8.6983e+01 - logprior: -1.1057e+00
Fitted a model with MAP estimate = -87.7277
Time for alignment: 41.6040
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0240 - loglik: -1.5079e+02 - logprior: -3.2296e+00
Epoch 2/10
19/19 - 1s - loss: 119.7157 - loglik: -1.1818e+02 - logprior: -1.5308e+00
Epoch 3/10
19/19 - 1s - loss: 107.2034 - loglik: -1.0560e+02 - logprior: -1.6043e+00
Epoch 4/10
19/19 - 1s - loss: 104.5656 - loglik: -1.0299e+02 - logprior: -1.5754e+00
Epoch 5/10
19/19 - 1s - loss: 103.5942 - loglik: -1.0206e+02 - logprior: -1.5342e+00
Epoch 6/10
19/19 - 1s - loss: 103.3484 - loglik: -1.0183e+02 - logprior: -1.5187e+00
Epoch 7/10
19/19 - 1s - loss: 103.0040 - loglik: -1.0150e+02 - logprior: -1.5054e+00
Epoch 8/10
19/19 - 1s - loss: 102.9213 - loglik: -1.0143e+02 - logprior: -1.4949e+00
Epoch 9/10
19/19 - 1s - loss: 103.0659 - loglik: -1.0157e+02 - logprior: -1.4928e+00
Fitted a model with MAP estimate = -98.6056
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.7920 - loglik: -1.0049e+02 - logprior: -3.2981e+00
Epoch 2/2
19/19 - 1s - loss: 95.4017 - loglik: -9.3989e+01 - logprior: -1.4123e+00
Fitted a model with MAP estimate = -90.5591
expansions: []
discards: [12 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.6676 - loglik: -9.4502e+01 - logprior: -3.1661e+00
Epoch 2/2
19/19 - 1s - loss: 94.4585 - loglik: -9.3184e+01 - logprior: -1.2745e+00
Fitted a model with MAP estimate = -90.3603
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.2797 - loglik: -8.9707e+01 - logprior: -2.5728e+00
Epoch 2/10
21/21 - 1s - loss: 89.3973 - loglik: -8.8146e+01 - logprior: -1.2511e+00
Epoch 3/10
21/21 - 1s - loss: 89.2544 - loglik: -8.8085e+01 - logprior: -1.1691e+00
Epoch 4/10
21/21 - 1s - loss: 88.7154 - loglik: -8.7585e+01 - logprior: -1.1306e+00
Epoch 5/10
21/21 - 1s - loss: 87.8399 - loglik: -8.6728e+01 - logprior: -1.1121e+00
Epoch 6/10
21/21 - 1s - loss: 87.9939 - loglik: -8.6883e+01 - logprior: -1.1110e+00
Fitted a model with MAP estimate = -87.7609
Time for alignment: 40.7824
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.1307 - loglik: -1.5090e+02 - logprior: -3.2268e+00
Epoch 2/10
19/19 - 1s - loss: 119.9684 - loglik: -1.1842e+02 - logprior: -1.5443e+00
Epoch 3/10
19/19 - 1s - loss: 106.5882 - loglik: -1.0496e+02 - logprior: -1.6245e+00
Epoch 4/10
19/19 - 1s - loss: 103.4551 - loglik: -1.0187e+02 - logprior: -1.5873e+00
Epoch 5/10
19/19 - 1s - loss: 102.4291 - loglik: -1.0088e+02 - logprior: -1.5488e+00
Epoch 6/10
19/19 - 1s - loss: 102.1825 - loglik: -1.0066e+02 - logprior: -1.5189e+00
Epoch 7/10
19/19 - 1s - loss: 101.8943 - loglik: -1.0039e+02 - logprior: -1.5074e+00
Epoch 8/10
19/19 - 1s - loss: 102.0758 - loglik: -1.0058e+02 - logprior: -1.4964e+00
Fitted a model with MAP estimate = -97.7302
expansions: [(5, 1), (6, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.9086 - loglik: -1.0060e+02 - logprior: -3.3099e+00
Epoch 2/2
19/19 - 1s - loss: 95.3554 - loglik: -9.3923e+01 - logprior: -1.4328e+00
Fitted a model with MAP estimate = -90.5218
expansions: []
discards: [ 9 21 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.7054 - loglik: -9.4536e+01 - logprior: -3.1697e+00
Epoch 2/2
19/19 - 1s - loss: 94.6419 - loglik: -9.3362e+01 - logprior: -1.2797e+00
Fitted a model with MAP estimate = -90.3272
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.1840 - loglik: -8.9609e+01 - logprior: -2.5751e+00
Epoch 2/10
21/21 - 1s - loss: 89.6169 - loglik: -8.8360e+01 - logprior: -1.2569e+00
Epoch 3/10
21/21 - 1s - loss: 88.7311 - loglik: -8.7569e+01 - logprior: -1.1623e+00
Epoch 4/10
21/21 - 1s - loss: 88.7063 - loglik: -8.7576e+01 - logprior: -1.1305e+00
Epoch 5/10
21/21 - 1s - loss: 88.2351 - loglik: -8.7128e+01 - logprior: -1.1071e+00
Epoch 6/10
21/21 - 1s - loss: 88.1204 - loglik: -8.7006e+01 - logprior: -1.1140e+00
Epoch 7/10
21/21 - 1s - loss: 87.6957 - loglik: -8.6592e+01 - logprior: -1.1037e+00
Epoch 8/10
21/21 - 1s - loss: 87.6114 - loglik: -8.6528e+01 - logprior: -1.0834e+00
Epoch 9/10
21/21 - 1s - loss: 87.6303 - loglik: -8.6544e+01 - logprior: -1.0859e+00
Fitted a model with MAP estimate = -87.5644
Time for alignment: 44.2180
Computed alignments with likelihoods: ['-87.5544', '-87.8999', '-87.7277', '-87.7609', '-87.5644']
Best model has likelihood: -87.5544  (prior= -1.0740 )
time for generating output: 0.1112
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9478428847392144
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 417.0815 - loglik: -4.1239e+02 - logprior: -4.6882e+00
Epoch 2/10
27/27 - 4s - loss: 334.0143 - loglik: -3.3202e+02 - logprior: -1.9966e+00
Epoch 3/10
27/27 - 3s - loss: 318.6459 - loglik: -3.1668e+02 - logprior: -1.9619e+00
Epoch 4/10
27/27 - 4s - loss: 315.4024 - loglik: -3.1348e+02 - logprior: -1.9252e+00
Epoch 5/10
27/27 - 4s - loss: 314.5890 - loglik: -3.1271e+02 - logprior: -1.8823e+00
Epoch 6/10
27/27 - 4s - loss: 313.7016 - loglik: -3.1185e+02 - logprior: -1.8552e+00
Epoch 7/10
27/27 - 4s - loss: 313.9474 - loglik: -3.1210e+02 - logprior: -1.8477e+00
Fitted a model with MAP estimate = -313.6058
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 1), (69, 2), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (85, 1), (100, 1), (104, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 309.7024 - loglik: -3.0331e+02 - logprior: -6.3894e+00
Epoch 2/2
27/27 - 4s - loss: 293.0362 - loglik: -2.9171e+02 - logprior: -1.3237e+00
Fitted a model with MAP estimate = -291.6068
expansions: [(45, 1), (114, 1)]
discards: [  0  12  30  48  49  54  89 135]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 302.6511 - loglik: -2.9637e+02 - logprior: -6.2821e+00
Epoch 2/2
27/27 - 4s - loss: 293.6733 - loglik: -2.9243e+02 - logprior: -1.2480e+00
Fitted a model with MAP estimate = -292.3536
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 297.5050 - loglik: -2.9336e+02 - logprior: -4.1435e+00
Epoch 2/10
27/27 - 4s - loss: 292.4792 - loglik: -2.9176e+02 - logprior: -7.2327e-01
Epoch 3/10
27/27 - 4s - loss: 291.8529 - loglik: -2.9143e+02 - logprior: -4.1921e-01
Epoch 4/10
27/27 - 4s - loss: 291.2303 - loglik: -2.9092e+02 - logprior: -3.1228e-01
Epoch 5/10
27/27 - 4s - loss: 290.9551 - loglik: -2.9076e+02 - logprior: -1.9475e-01
Epoch 6/10
27/27 - 4s - loss: 290.8947 - loglik: -2.9080e+02 - logprior: -9.3999e-02
Epoch 7/10
27/27 - 4s - loss: 290.4933 - loglik: -2.9051e+02 - logprior: 0.0146
Epoch 8/10
27/27 - 4s - loss: 289.7391 - loglik: -2.8986e+02 - logprior: 0.1198
Epoch 9/10
27/27 - 4s - loss: 290.7158 - loglik: -2.9094e+02 - logprior: 0.2218
Fitted a model with MAP estimate = -289.9902
Time for alignment: 112.0206
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 416.2865 - loglik: -4.1158e+02 - logprior: -4.7032e+00
Epoch 2/10
27/27 - 3s - loss: 332.5578 - loglik: -3.3064e+02 - logprior: -1.9208e+00
Epoch 3/10
27/27 - 3s - loss: 317.8899 - loglik: -3.1600e+02 - logprior: -1.8921e+00
Epoch 4/10
27/27 - 4s - loss: 313.7648 - loglik: -3.1181e+02 - logprior: -1.9569e+00
Epoch 5/10
27/27 - 3s - loss: 312.9391 - loglik: -3.1097e+02 - logprior: -1.9671e+00
Epoch 6/10
27/27 - 4s - loss: 312.2951 - loglik: -3.1034e+02 - logprior: -1.9533e+00
Epoch 7/10
27/27 - 4s - loss: 312.3766 - loglik: -3.1044e+02 - logprior: -1.9412e+00
Fitted a model with MAP estimate = -312.1673
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (38, 4), (39, 1), (40, 2), (42, 1), (43, 1), (48, 1), (69, 1), (72, 1), (73, 1), (74, 1), (77, 1), (78, 1), (82, 2), (101, 1), (102, 1), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 307.3550 - loglik: -3.0094e+02 - logprior: -6.4124e+00
Epoch 2/2
27/27 - 4s - loss: 290.1873 - loglik: -2.8883e+02 - logprior: -1.3542e+00
Fitted a model with MAP estimate = -288.2730
expansions: []
discards: [  0  12  30  47  53 107 134 136]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 298.7844 - loglik: -2.9243e+02 - logprior: -6.3531e+00
Epoch 2/2
27/27 - 4s - loss: 291.1470 - loglik: -2.8985e+02 - logprior: -1.2998e+00
Fitted a model with MAP estimate = -289.4319
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 294.2677 - loglik: -2.9005e+02 - logprior: -4.2128e+00
Epoch 2/10
27/27 - 4s - loss: 289.9204 - loglik: -2.8912e+02 - logprior: -7.9995e-01
Epoch 3/10
27/27 - 4s - loss: 289.1181 - loglik: -2.8861e+02 - logprior: -5.0458e-01
Epoch 4/10
27/27 - 4s - loss: 287.8895 - loglik: -2.8749e+02 - logprior: -4.0207e-01
Epoch 5/10
27/27 - 4s - loss: 288.1797 - loglik: -2.8789e+02 - logprior: -2.9150e-01
Fitted a model with MAP estimate = -287.6169
Time for alignment: 93.4567
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 417.0982 - loglik: -4.1241e+02 - logprior: -4.6925e+00
Epoch 2/10
27/27 - 4s - loss: 330.3389 - loglik: -3.2838e+02 - logprior: -1.9543e+00
Epoch 3/10
27/27 - 3s - loss: 316.5307 - loglik: -3.1460e+02 - logprior: -1.9355e+00
Epoch 4/10
27/27 - 3s - loss: 313.7549 - loglik: -3.1189e+02 - logprior: -1.8699e+00
Epoch 5/10
27/27 - 3s - loss: 312.4684 - loglik: -3.1063e+02 - logprior: -1.8422e+00
Epoch 6/10
27/27 - 3s - loss: 312.8528 - loglik: -3.1103e+02 - logprior: -1.8259e+00
Fitted a model with MAP estimate = -312.1823
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (94, 2), (100, 1), (102, 1), (105, 1), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 306.3964 - loglik: -3.0015e+02 - logprior: -6.2474e+00
Epoch 2/2
27/27 - 4s - loss: 289.0218 - loglik: -2.8785e+02 - logprior: -1.1760e+00
Fitted a model with MAP estimate = -287.6543
expansions: [(43, 1)]
discards: [  0  30  47  48  67 111]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 298.5156 - loglik: -2.9237e+02 - logprior: -6.1473e+00
Epoch 2/2
27/27 - 4s - loss: 290.7332 - loglik: -2.8967e+02 - logprior: -1.0614e+00
Fitted a model with MAP estimate = -289.1212
expansions: []
discards: [45]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 294.8973 - loglik: -2.9087e+02 - logprior: -4.0242e+00
Epoch 2/10
27/27 - 4s - loss: 289.8806 - loglik: -2.8927e+02 - logprior: -6.1467e-01
Epoch 3/10
27/27 - 4s - loss: 288.9270 - loglik: -2.8860e+02 - logprior: -3.3030e-01
Epoch 4/10
27/27 - 4s - loss: 287.4099 - loglik: -2.8717e+02 - logprior: -2.3530e-01
Epoch 5/10
27/27 - 4s - loss: 288.1471 - loglik: -2.8803e+02 - logprior: -1.1604e-01
Fitted a model with MAP estimate = -287.3875
Time for alignment: 92.5496
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 416.9045 - loglik: -4.1223e+02 - logprior: -4.6763e+00
Epoch 2/10
27/27 - 3s - loss: 330.9939 - loglik: -3.2913e+02 - logprior: -1.8650e+00
Epoch 3/10
27/27 - 4s - loss: 316.0279 - loglik: -3.1412e+02 - logprior: -1.9116e+00
Epoch 4/10
27/27 - 3s - loss: 312.8958 - loglik: -3.1097e+02 - logprior: -1.9287e+00
Epoch 5/10
27/27 - 4s - loss: 311.8760 - loglik: -3.0994e+02 - logprior: -1.9403e+00
Epoch 6/10
27/27 - 4s - loss: 311.9703 - loglik: -3.1003e+02 - logprior: -1.9445e+00
Fitted a model with MAP estimate = -311.1311
expansions: [(0, 2), (19, 1), (22, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 2), (100, 1), (102, 1), (103, 1), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 307.0659 - loglik: -3.0072e+02 - logprior: -6.3493e+00
Epoch 2/2
27/27 - 4s - loss: 289.7617 - loglik: -2.8839e+02 - logprior: -1.3735e+00
Fitted a model with MAP estimate = -287.5405
expansions: [(42, 1), (131, 1)]
discards: [  0  29  46  47  87 110 137]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 297.6520 - loglik: -2.9141e+02 - logprior: -6.2426e+00
Epoch 2/2
27/27 - 4s - loss: 290.1436 - loglik: -2.8903e+02 - logprior: -1.1149e+00
Fitted a model with MAP estimate = -288.1613
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 292.9507 - loglik: -2.8884e+02 - logprior: -4.1151e+00
Epoch 2/10
27/27 - 4s - loss: 288.9627 - loglik: -2.8826e+02 - logprior: -7.0435e-01
Epoch 3/10
27/27 - 4s - loss: 286.9674 - loglik: -2.8654e+02 - logprior: -4.2996e-01
Epoch 4/10
27/27 - 4s - loss: 287.1463 - loglik: -2.8679e+02 - logprior: -3.5946e-01
Fitted a model with MAP estimate = -286.1847
Time for alignment: 87.7100
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 416.8372 - loglik: -4.1214e+02 - logprior: -4.6996e+00
Epoch 2/10
27/27 - 4s - loss: 332.5287 - loglik: -3.3058e+02 - logprior: -1.9531e+00
Epoch 3/10
27/27 - 4s - loss: 316.5234 - loglik: -3.1457e+02 - logprior: -1.9493e+00
Epoch 4/10
27/27 - 4s - loss: 312.3689 - loglik: -3.1044e+02 - logprior: -1.9257e+00
Epoch 5/10
27/27 - 4s - loss: 311.0450 - loglik: -3.0917e+02 - logprior: -1.8791e+00
Epoch 6/10
27/27 - 3s - loss: 310.5850 - loglik: -3.0873e+02 - logprior: -1.8518e+00
Epoch 7/10
27/27 - 3s - loss: 310.9318 - loglik: -3.0909e+02 - logprior: -1.8431e+00
Fitted a model with MAP estimate = -310.4856
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (68, 1), (69, 2), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (94, 2), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 305.4447 - loglik: -2.9907e+02 - logprior: -6.3747e+00
Epoch 2/2
27/27 - 4s - loss: 287.9419 - loglik: -2.8666e+02 - logprior: -1.2849e+00
Fitted a model with MAP estimate = -286.3721
expansions: [(43, 1)]
discards: [  0  12  29  46  47  48 135 137]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 298.0699 - loglik: -2.9175e+02 - logprior: -6.3160e+00
Epoch 2/2
27/27 - 4s - loss: 289.6763 - loglik: -2.8840e+02 - logprior: -1.2714e+00
Fitted a model with MAP estimate = -288.2454
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 293.5760 - loglik: -2.8938e+02 - logprior: -4.1986e+00
Epoch 2/10
27/27 - 4s - loss: 288.3056 - loglik: -2.8754e+02 - logprior: -7.6762e-01
Epoch 3/10
27/27 - 4s - loss: 287.6471 - loglik: -2.8717e+02 - logprior: -4.7222e-01
Epoch 4/10
27/27 - 4s - loss: 286.7924 - loglik: -2.8643e+02 - logprior: -3.6425e-01
Epoch 5/10
27/27 - 4s - loss: 286.4852 - loglik: -2.8624e+02 - logprior: -2.4182e-01
Epoch 6/10
27/27 - 4s - loss: 286.5602 - loglik: -2.8643e+02 - logprior: -1.2820e-01
Fitted a model with MAP estimate = -286.0684
Time for alignment: 98.7785
Computed alignments with likelihoods: ['-289.9902', '-287.6169', '-287.3875', '-286.1847', '-286.0684']
Best model has likelihood: -286.0684  (prior= -0.1021 )
time for generating output: 0.2832
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5053156146179402
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 764.8007 - loglik: -7.6268e+02 - logprior: -2.1169e+00
Epoch 2/10
34/34 - 14s - loss: 639.5338 - loglik: -6.3786e+02 - logprior: -1.6701e+00
Epoch 3/10
34/34 - 14s - loss: 620.4012 - loglik: -6.1862e+02 - logprior: -1.7842e+00
Epoch 4/10
34/34 - 14s - loss: 618.4410 - loglik: -6.1671e+02 - logprior: -1.7269e+00
Epoch 5/10
34/34 - 14s - loss: 616.3767 - loglik: -6.1467e+02 - logprior: -1.7114e+00
Epoch 6/10
34/34 - 14s - loss: 615.4991 - loglik: -6.1378e+02 - logprior: -1.7216e+00
Epoch 7/10
34/34 - 14s - loss: 615.2950 - loglik: -6.1355e+02 - logprior: -1.7433e+00
Epoch 8/10
34/34 - 14s - loss: 614.7537 - loglik: -6.1302e+02 - logprior: -1.7372e+00
Epoch 9/10
34/34 - 14s - loss: 614.4579 - loglik: -6.1272e+02 - logprior: -1.7425e+00
Epoch 10/10
34/34 - 14s - loss: 615.7431 - loglik: -6.1400e+02 - logprior: -1.7412e+00
Fitted a model with MAP estimate = -614.3475
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 2), (41, 1), (50, 1), (51, 1), (55, 5), (65, 1), (68, 1), (88, 2), (93, 1), (94, 1), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (163, 1), (164, 1), (167, 1), (171, 1), (172, 1), (175, 1), (178, 1), (188, 1), (193, 1), (195, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 601.6938 - loglik: -5.9837e+02 - logprior: -3.3233e+00
Epoch 2/2
34/34 - 19s - loss: 583.1448 - loglik: -5.8173e+02 - logprior: -1.4189e+00
Fitted a model with MAP estimate = -577.5238
expansions: [(18, 1), (77, 2)]
discards: [ 19  20  21  22  23  73  74  75 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 589.9501 - loglik: -5.8797e+02 - logprior: -1.9850e+00
Epoch 2/2
34/34 - 19s - loss: 583.7320 - loglik: -5.8358e+02 - logprior: -1.4829e-01
Fitted a model with MAP estimate = -582.3096
expansions: [(179, 1)]
discards: [70 71]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 588.1772 - loglik: -5.8649e+02 - logprior: -1.6859e+00
Epoch 2/10
34/34 - 19s - loss: 585.7460 - loglik: -5.8591e+02 - logprior: 0.1611
Epoch 3/10
34/34 - 19s - loss: 581.3030 - loglik: -5.8162e+02 - logprior: 0.3216
Epoch 4/10
34/34 - 19s - loss: 580.9243 - loglik: -5.8138e+02 - logprior: 0.4553
Epoch 5/10
34/34 - 18s - loss: 580.5271 - loglik: -5.8118e+02 - logprior: 0.6561
Epoch 6/10
34/34 - 18s - loss: 579.6030 - loglik: -5.8037e+02 - logprior: 0.7678
Epoch 7/10
34/34 - 19s - loss: 579.9978 - loglik: -5.8094e+02 - logprior: 0.9434
Fitted a model with MAP estimate = -579.3214
Time for alignment: 455.9880
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 768.0367 - loglik: -7.6592e+02 - logprior: -2.1162e+00
Epoch 2/10
34/34 - 14s - loss: 644.5257 - loglik: -6.4289e+02 - logprior: -1.6330e+00
Epoch 3/10
34/34 - 14s - loss: 624.1437 - loglik: -6.2227e+02 - logprior: -1.8747e+00
Epoch 4/10
34/34 - 14s - loss: 620.2375 - loglik: -6.1836e+02 - logprior: -1.8746e+00
Epoch 5/10
34/34 - 14s - loss: 619.4949 - loglik: -6.1762e+02 - logprior: -1.8761e+00
Epoch 6/10
34/34 - 14s - loss: 616.2522 - loglik: -6.1436e+02 - logprior: -1.8963e+00
Epoch 7/10
34/34 - 14s - loss: 618.1587 - loglik: -6.1626e+02 - logprior: -1.8983e+00
Fitted a model with MAP estimate = -617.1658
expansions: [(13, 1), (14, 1), (16, 3), (18, 6), (23, 2), (30, 2), (54, 1), (55, 1), (56, 1), (61, 1), (66, 1), (67, 2), (68, 1), (70, 1), (93, 1), (94, 1), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (163, 1), (164, 1), (167, 1), (172, 1), (175, 1), (183, 1), (184, 2), (185, 1), (186, 2), (188, 1), (190, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [ 0 62]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 602.4692 - loglik: -5.9912e+02 - logprior: -3.3526e+00
Epoch 2/2
34/34 - 19s - loss: 581.9943 - loglik: -5.8069e+02 - logprior: -1.3034e+00
Fitted a model with MAP estimate = -576.7642
expansions: [(183, 1), (232, 1)]
discards: [ 17  41  85 254 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 587.0673 - loglik: -5.8501e+02 - logprior: -2.0575e+00
Epoch 2/2
34/34 - 19s - loss: 580.5643 - loglik: -5.8035e+02 - logprior: -2.1908e-01
Fitted a model with MAP estimate = -578.8849
expansions: [(93, 1)]
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 584.5839 - loglik: -5.8285e+02 - logprior: -1.7308e+00
Epoch 2/10
34/34 - 19s - loss: 580.2053 - loglik: -5.8031e+02 - logprior: 0.1042
Epoch 3/10
34/34 - 19s - loss: 578.0065 - loglik: -5.7831e+02 - logprior: 0.3072
Epoch 4/10
34/34 - 19s - loss: 577.5776 - loglik: -5.7800e+02 - logprior: 0.4271
Epoch 5/10
34/34 - 19s - loss: 577.3849 - loglik: -5.7797e+02 - logprior: 0.5828
Epoch 6/10
34/34 - 19s - loss: 575.8768 - loglik: -5.7662e+02 - logprior: 0.7406
Epoch 7/10
34/34 - 19s - loss: 575.2664 - loglik: -5.7617e+02 - logprior: 0.9071
Epoch 8/10
34/34 - 19s - loss: 575.2587 - loglik: -5.7630e+02 - logprior: 1.0386
Epoch 9/10
34/34 - 19s - loss: 576.4348 - loglik: -5.7764e+02 - logprior: 1.2091
Fitted a model with MAP estimate = -575.0615
Time for alignment: 454.9400
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 768.3076 - loglik: -7.6619e+02 - logprior: -2.1178e+00
Epoch 2/10
34/34 - 14s - loss: 643.8651 - loglik: -6.4228e+02 - logprior: -1.5891e+00
Epoch 3/10
34/34 - 14s - loss: 625.9861 - loglik: -6.2424e+02 - logprior: -1.7482e+00
Epoch 4/10
34/34 - 14s - loss: 623.3705 - loglik: -6.2166e+02 - logprior: -1.7154e+00
Epoch 5/10
34/34 - 14s - loss: 622.4736 - loglik: -6.2076e+02 - logprior: -1.7164e+00
Epoch 6/10
34/34 - 14s - loss: 621.0859 - loglik: -6.1936e+02 - logprior: -1.7306e+00
Epoch 7/10
34/34 - 14s - loss: 620.8608 - loglik: -6.1910e+02 - logprior: -1.7628e+00
Epoch 8/10
34/34 - 14s - loss: 620.6805 - loglik: -6.1891e+02 - logprior: -1.7657e+00
Epoch 9/10
34/34 - 14s - loss: 620.8111 - loglik: -6.1905e+02 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -620.0947
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (16, 2), (17, 5), (18, 1), (23, 2), (29, 1), (30, 2), (52, 2), (56, 1), (57, 5), (65, 1), (68, 1), (70, 1), (73, 2), (93, 1), (94, 1), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (130, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 3), (168, 1), (172, 2), (173, 2), (175, 1), (178, 1), (184, 1), (185, 1), (186, 1), (187, 3), (189, 1), (193, 1), (202, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 601.3003 - loglik: -5.9799e+02 - logprior: -3.3105e+00
Epoch 2/2
34/34 - 21s - loss: 578.3474 - loglik: -5.7688e+02 - logprior: -1.4672e+00
Fitted a model with MAP estimate = -571.5679
expansions: [(80, 2), (190, 1), (215, 1)]
discards: [ 20  43  76  77  78 227]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 580.9904 - loglik: -5.7887e+02 - logprior: -2.1197e+00
Epoch 2/2
34/34 - 20s - loss: 575.2559 - loglik: -5.7497e+02 - logprior: -2.8347e-01
Fitted a model with MAP estimate = -573.2269
expansions: []
discards: [93]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 579.1754 - loglik: -5.7734e+02 - logprior: -1.8375e+00
Epoch 2/10
34/34 - 20s - loss: 574.9445 - loglik: -5.7490e+02 - logprior: -3.9429e-02
Epoch 3/10
34/34 - 21s - loss: 572.7832 - loglik: -5.7294e+02 - logprior: 0.1601
Epoch 4/10
34/34 - 20s - loss: 571.0775 - loglik: -5.7140e+02 - logprior: 0.3183
Epoch 5/10
34/34 - 20s - loss: 571.0214 - loglik: -5.7150e+02 - logprior: 0.4816
Epoch 6/10
34/34 - 20s - loss: 569.6484 - loglik: -5.7027e+02 - logprior: 0.6245
Epoch 7/10
34/34 - 20s - loss: 570.2428 - loglik: -5.7099e+02 - logprior: 0.7431
Fitted a model with MAP estimate = -569.7036
Time for alignment: 463.1715
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 767.3960 - loglik: -7.6528e+02 - logprior: -2.1125e+00
Epoch 2/10
34/34 - 14s - loss: 643.3870 - loglik: -6.4169e+02 - logprior: -1.6997e+00
Epoch 3/10
34/34 - 14s - loss: 624.1719 - loglik: -6.2229e+02 - logprior: -1.8827e+00
Epoch 4/10
34/34 - 14s - loss: 620.8236 - loglik: -6.1904e+02 - logprior: -1.7803e+00
Epoch 5/10
34/34 - 14s - loss: 619.6868 - loglik: -6.1791e+02 - logprior: -1.7761e+00
Epoch 6/10
34/34 - 14s - loss: 619.9242 - loglik: -6.1815e+02 - logprior: -1.7750e+00
Fitted a model with MAP estimate = -618.2277
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (23, 1), (30, 2), (34, 1), (51, 1), (54, 1), (55, 2), (64, 1), (68, 1), (89, 1), (94, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (104, 1), (107, 1), (109, 1), (129, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (172, 2), (173, 1), (175, 1), (178, 1), (186, 3), (190, 1), (193, 1), (203, 1), (208, 1), (225, 1), (226, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 605.2917 - loglik: -6.0191e+02 - logprior: -3.3797e+00
Epoch 2/2
34/34 - 20s - loss: 586.8665 - loglik: -5.8549e+02 - logprior: -1.3730e+00
Fitted a model with MAP estimate = -581.9234
expansions: [(67, 1), (73, 1), (183, 1)]
discards: [ 19  20  21  22  23  42 116]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 590.6660 - loglik: -5.8857e+02 - logprior: -2.0910e+00
Epoch 2/2
34/34 - 19s - loss: 585.5100 - loglik: -5.8517e+02 - logprior: -3.4094e-01
Fitted a model with MAP estimate = -582.7184
expansions: [(18, 1), (54, 2), (232, 1), (233, 2), (236, 1)]
discards: [60 61 67 68]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 586.6912 - loglik: -5.8492e+02 - logprior: -1.7671e+00
Epoch 2/10
34/34 - 19s - loss: 576.6716 - loglik: -5.7672e+02 - logprior: 0.0500
Epoch 3/10
34/34 - 19s - loss: 576.5598 - loglik: -5.7674e+02 - logprior: 0.1792
Epoch 4/10
34/34 - 19s - loss: 574.2407 - loglik: -5.7459e+02 - logprior: 0.3500
Epoch 5/10
34/34 - 19s - loss: 574.9195 - loglik: -5.7537e+02 - logprior: 0.4480
Fitted a model with MAP estimate = -572.7477
Time for alignment: 365.1743
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 766.4927 - loglik: -7.6439e+02 - logprior: -2.1016e+00
Epoch 2/10
34/34 - 14s - loss: 640.3417 - loglik: -6.3877e+02 - logprior: -1.5740e+00
Epoch 3/10
34/34 - 14s - loss: 621.5439 - loglik: -6.1985e+02 - logprior: -1.6985e+00
Epoch 4/10
34/34 - 14s - loss: 619.0628 - loglik: -6.1738e+02 - logprior: -1.6821e+00
Epoch 5/10
34/34 - 14s - loss: 615.7885 - loglik: -6.1410e+02 - logprior: -1.6896e+00
Epoch 6/10
34/34 - 14s - loss: 616.2144 - loglik: -6.1450e+02 - logprior: -1.7128e+00
Fitted a model with MAP estimate = -615.1468
expansions: [(13, 1), (14, 1), (16, 2), (18, 5), (23, 2), (29, 1), (30, 2), (52, 2), (56, 2), (65, 1), (66, 1), (67, 1), (94, 2), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (131, 1), (135, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (167, 1), (172, 1), (173, 2), (175, 1), (178, 1), (185, 4), (186, 2), (188, 2), (189, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 600.7111 - loglik: -5.9737e+02 - logprior: -3.3422e+00
Epoch 2/2
34/34 - 19s - loss: 577.6443 - loglik: -5.7639e+02 - logprior: -1.2541e+00
Fitted a model with MAP estimate = -573.4016
expansions: [(183, 1)]
discards: [ 21  22  40  71 115 116 120 217 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 587.3403 - loglik: -5.8537e+02 - logprior: -1.9735e+00
Epoch 2/2
34/34 - 18s - loss: 580.2944 - loglik: -5.8014e+02 - logprior: -1.5179e-01
Fitted a model with MAP estimate = -578.7181
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 21s - loss: 585.0425 - loglik: -5.8335e+02 - logprior: -1.6884e+00
Epoch 2/10
34/34 - 18s - loss: 579.3452 - loglik: -5.7953e+02 - logprior: 0.1826
Epoch 3/10
34/34 - 19s - loss: 580.0990 - loglik: -5.8047e+02 - logprior: 0.3746
Fitted a model with MAP estimate = -577.6940
Time for alignment: 322.1084
Computed alignments with likelihoods: ['-577.5238', '-575.0615', '-569.7036', '-572.7477', '-573.4016']
Best model has likelihood: -569.7036  (prior= 0.8179 )
time for generating output: 0.4010
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.711604938271605
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.2244 - loglik: -1.8299e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 183.5530 - loglik: -1.5983e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 152.8610 - loglik: -1.4167e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 138.6685 - loglik: -1.3209e+02 - logprior: -6.5817e+00
Epoch 5/10
10/10 - 1s - loss: 132.2069 - loglik: -1.2805e+02 - logprior: -4.1604e+00
Epoch 6/10
10/10 - 1s - loss: 128.9109 - loglik: -1.2610e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 127.2593 - loglik: -1.2534e+02 - logprior: -1.9170e+00
Epoch 8/10
10/10 - 1s - loss: 126.2743 - loglik: -1.2493e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 125.6032 - loglik: -1.2461e+02 - logprior: -9.9278e-01
Epoch 10/10
10/10 - 1s - loss: 125.0603 - loglik: -1.2431e+02 - logprior: -7.5081e-01
Fitted a model with MAP estimate = -124.7944
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.4529 - loglik: -1.2283e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 156.8225 - loglik: -1.1658e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -145.2494
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3253 - loglik: -1.1256e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 130.9606 - loglik: -1.1023e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -122.0199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.9416 - loglik: -1.0964e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 129.5714 - loglik: -1.0950e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 117.8618 - loglik: -1.0965e+02 - logprior: -8.2155e+00
Epoch 4/10
10/10 - 1s - loss: 113.3184 - loglik: -1.1015e+02 - logprior: -3.1685e+00
Epoch 5/10
10/10 - 1s - loss: 111.0048 - loglik: -1.1065e+02 - logprior: -3.5204e-01
Epoch 6/10
10/10 - 1s - loss: 109.7048 - loglik: -1.1101e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 108.9064 - loglik: -1.1122e+02 - logprior: 2.3092
Epoch 8/10
10/10 - 1s - loss: 108.3505 - loglik: -1.1131e+02 - logprior: 2.9559
Epoch 9/10
10/10 - 1s - loss: 107.8991 - loglik: -1.1135e+02 - logprior: 3.4491
Epoch 10/10
10/10 - 1s - loss: 107.4633 - loglik: -1.1126e+02 - logprior: 3.7991
Fitted a model with MAP estimate = -107.2003
Time for alignment: 30.5485
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2244 - loglik: -1.8299e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 183.5530 - loglik: -1.5983e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 152.8610 - loglik: -1.4167e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 138.6685 - loglik: -1.3209e+02 - logprior: -6.5817e+00
Epoch 5/10
10/10 - 1s - loss: 132.2070 - loglik: -1.2805e+02 - logprior: -4.1604e+00
Epoch 6/10
10/10 - 1s - loss: 128.9108 - loglik: -1.2610e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 127.2593 - loglik: -1.2534e+02 - logprior: -1.9170e+00
Epoch 8/10
10/10 - 1s - loss: 126.2743 - loglik: -1.2493e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 125.6033 - loglik: -1.2461e+02 - logprior: -9.9278e-01
Epoch 10/10
10/10 - 1s - loss: 125.0603 - loglik: -1.2431e+02 - logprior: -7.5081e-01
Fitted a model with MAP estimate = -124.7944
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 221.4529 - loglik: -1.2283e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 156.8225 - loglik: -1.1658e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -145.2494
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3253 - loglik: -1.1256e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 130.9606 - loglik: -1.1023e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -122.0199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.9416 - loglik: -1.0964e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 129.5714 - loglik: -1.0950e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 117.8618 - loglik: -1.0965e+02 - logprior: -8.2155e+00
Epoch 4/10
10/10 - 1s - loss: 113.3184 - loglik: -1.1015e+02 - logprior: -3.1685e+00
Epoch 5/10
10/10 - 1s - loss: 111.0048 - loglik: -1.1065e+02 - logprior: -3.5204e-01
Epoch 6/10
10/10 - 1s - loss: 109.7048 - loglik: -1.1101e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 108.9063 - loglik: -1.1122e+02 - logprior: 2.3092
Epoch 8/10
10/10 - 1s - loss: 108.3505 - loglik: -1.1131e+02 - logprior: 2.9559
Epoch 9/10
10/10 - 1s - loss: 107.8991 - loglik: -1.1135e+02 - logprior: 3.4491
Epoch 10/10
10/10 - 1s - loss: 107.4632 - loglik: -1.1126e+02 - logprior: 3.7991
Fitted a model with MAP estimate = -107.2003
Time for alignment: 28.5927
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2244 - loglik: -1.8299e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 183.5530 - loglik: -1.5983e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 152.8610 - loglik: -1.4167e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 138.6685 - loglik: -1.3209e+02 - logprior: -6.5817e+00
Epoch 5/10
10/10 - 1s - loss: 132.2070 - loglik: -1.2805e+02 - logprior: -4.1604e+00
Epoch 6/10
10/10 - 1s - loss: 128.9108 - loglik: -1.2610e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 127.2593 - loglik: -1.2534e+02 - logprior: -1.9170e+00
Epoch 8/10
10/10 - 1s - loss: 126.2743 - loglik: -1.2493e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 125.6033 - loglik: -1.2461e+02 - logprior: -9.9278e-01
Epoch 10/10
10/10 - 1s - loss: 125.0603 - loglik: -1.2431e+02 - logprior: -7.5081e-01
Fitted a model with MAP estimate = -124.7944
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.4529 - loglik: -1.2283e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 156.8225 - loglik: -1.1658e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -145.2494
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3253 - loglik: -1.1256e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 130.9606 - loglik: -1.1023e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -122.0199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.9416 - loglik: -1.0964e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 129.5714 - loglik: -1.0950e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 117.8618 - loglik: -1.0965e+02 - logprior: -8.2155e+00
Epoch 4/10
10/10 - 1s - loss: 113.3184 - loglik: -1.1015e+02 - logprior: -3.1685e+00
Epoch 5/10
10/10 - 1s - loss: 111.0048 - loglik: -1.1065e+02 - logprior: -3.5204e-01
Epoch 6/10
10/10 - 1s - loss: 109.7048 - loglik: -1.1101e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 108.9063 - loglik: -1.1122e+02 - logprior: 2.3092
Epoch 8/10
10/10 - 1s - loss: 108.3505 - loglik: -1.1131e+02 - logprior: 2.9559
Epoch 9/10
10/10 - 1s - loss: 107.8991 - loglik: -1.1135e+02 - logprior: 3.4491
Epoch 10/10
10/10 - 1s - loss: 107.4632 - loglik: -1.1126e+02 - logprior: 3.7991
Fitted a model with MAP estimate = -107.2004
Time for alignment: 27.2449
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.2244 - loglik: -1.8299e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 183.5530 - loglik: -1.5983e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 152.8610 - loglik: -1.4167e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 138.6685 - loglik: -1.3209e+02 - logprior: -6.5817e+00
Epoch 5/10
10/10 - 1s - loss: 132.2069 - loglik: -1.2805e+02 - logprior: -4.1604e+00
Epoch 6/10
10/10 - 1s - loss: 128.9109 - loglik: -1.2610e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 127.2593 - loglik: -1.2534e+02 - logprior: -1.9170e+00
Epoch 8/10
10/10 - 1s - loss: 126.2743 - loglik: -1.2493e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 125.6033 - loglik: -1.2461e+02 - logprior: -9.9278e-01
Epoch 10/10
10/10 - 1s - loss: 125.0603 - loglik: -1.2431e+02 - logprior: -7.5081e-01
Fitted a model with MAP estimate = -124.7943
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 221.4529 - loglik: -1.2283e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 156.8225 - loglik: -1.1658e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -145.2494
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3253 - loglik: -1.1256e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 130.9606 - loglik: -1.1023e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -122.0199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.9415 - loglik: -1.0964e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 129.5714 - loglik: -1.0950e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 117.8618 - loglik: -1.0965e+02 - logprior: -8.2155e+00
Epoch 4/10
10/10 - 1s - loss: 113.3184 - loglik: -1.1015e+02 - logprior: -3.1685e+00
Epoch 5/10
10/10 - 1s - loss: 111.0048 - loglik: -1.1065e+02 - logprior: -3.5204e-01
Epoch 6/10
10/10 - 1s - loss: 109.7048 - loglik: -1.1101e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 108.9063 - loglik: -1.1122e+02 - logprior: 2.3092
Epoch 8/10
10/10 - 1s - loss: 108.3505 - loglik: -1.1131e+02 - logprior: 2.9559
Epoch 9/10
10/10 - 1s - loss: 107.8991 - loglik: -1.1135e+02 - logprior: 3.4491
Epoch 10/10
10/10 - 1s - loss: 107.4632 - loglik: -1.1126e+02 - logprior: 3.7991
Fitted a model with MAP estimate = -107.2004
Time for alignment: 28.5145
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2244 - loglik: -1.8299e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 183.5530 - loglik: -1.5983e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 152.8610 - loglik: -1.4167e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 138.6685 - loglik: -1.3209e+02 - logprior: -6.5817e+00
Epoch 5/10
10/10 - 1s - loss: 132.2070 - loglik: -1.2805e+02 - logprior: -4.1604e+00
Epoch 6/10
10/10 - 1s - loss: 128.9109 - loglik: -1.2610e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 127.2593 - loglik: -1.2534e+02 - logprior: -1.9170e+00
Epoch 8/10
10/10 - 1s - loss: 126.2743 - loglik: -1.2493e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 125.6032 - loglik: -1.2461e+02 - logprior: -9.9278e-01
Epoch 10/10
10/10 - 1s - loss: 125.0603 - loglik: -1.2431e+02 - logprior: -7.5081e-01
Fitted a model with MAP estimate = -124.7944
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.4529 - loglik: -1.2283e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 156.8225 - loglik: -1.1658e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -145.2494
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3253 - loglik: -1.1256e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 130.9606 - loglik: -1.1023e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -122.0199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.9415 - loglik: -1.0964e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 129.5714 - loglik: -1.0950e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 0s - loss: 117.8618 - loglik: -1.0965e+02 - logprior: -8.2155e+00
Epoch 4/10
10/10 - 1s - loss: 113.3184 - loglik: -1.1015e+02 - logprior: -3.1685e+00
Epoch 5/10
10/10 - 1s - loss: 111.0048 - loglik: -1.1065e+02 - logprior: -3.5205e-01
Epoch 6/10
10/10 - 1s - loss: 109.7048 - loglik: -1.1101e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 108.9063 - loglik: -1.1122e+02 - logprior: 2.3092
Epoch 8/10
10/10 - 1s - loss: 108.3505 - loglik: -1.1131e+02 - logprior: 2.9559
Epoch 9/10
10/10 - 1s - loss: 107.8991 - loglik: -1.1135e+02 - logprior: 3.4491
Epoch 10/10
10/10 - 1s - loss: 107.4632 - loglik: -1.1126e+02 - logprior: 3.7991
Fitted a model with MAP estimate = -107.2004
Time for alignment: 26.7144
Computed alignments with likelihoods: ['-107.2003', '-107.2003', '-107.2004', '-107.2004', '-107.2004']
Best model has likelihood: -107.2003  (prior= 3.9378 )
time for generating output: 0.1205
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8518282699654357
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.4847 - loglik: -3.0879e+02 - logprior: -7.6925e+00
Epoch 2/10
13/13 - 1s - loss: 284.0636 - loglik: -2.8220e+02 - logprior: -1.8651e+00
Epoch 3/10
13/13 - 1s - loss: 259.5145 - loglik: -2.5789e+02 - logprior: -1.6219e+00
Epoch 4/10
13/13 - 1s - loss: 252.0374 - loglik: -2.5021e+02 - logprior: -1.8239e+00
Epoch 5/10
13/13 - 1s - loss: 249.0888 - loglik: -2.4736e+02 - logprior: -1.7333e+00
Epoch 6/10
13/13 - 1s - loss: 248.9162 - loglik: -2.4728e+02 - logprior: -1.6363e+00
Epoch 7/10
13/13 - 1s - loss: 249.0360 - loglik: -2.4739e+02 - logprior: -1.6424e+00
Fitted a model with MAP estimate = -248.5495
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 3), (29, 2), (30, 2), (31, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 258.2315 - loglik: -2.4919e+02 - logprior: -9.0445e+00
Epoch 2/2
13/13 - 2s - loss: 247.9241 - loglik: -2.4393e+02 - logprior: -3.9980e+00
Fitted a model with MAP estimate = -246.1518
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 249.8971 - loglik: -2.4308e+02 - logprior: -6.8131e+00
Epoch 2/2
13/13 - 1s - loss: 243.7246 - loglik: -2.4193e+02 - logprior: -1.7957e+00
Fitted a model with MAP estimate = -242.9575
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.0491 - loglik: -2.4345e+02 - logprior: -8.5962e+00
Epoch 2/10
13/13 - 1s - loss: 245.5453 - loglik: -2.4298e+02 - logprior: -2.5651e+00
Epoch 3/10
13/13 - 1s - loss: 243.1838 - loglik: -2.4198e+02 - logprior: -1.2072e+00
Epoch 4/10
13/13 - 1s - loss: 242.5114 - loglik: -2.4161e+02 - logprior: -8.9972e-01
Epoch 5/10
13/13 - 1s - loss: 242.0526 - loglik: -2.4126e+02 - logprior: -7.9585e-01
Epoch 6/10
13/13 - 2s - loss: 242.0291 - loglik: -2.4128e+02 - logprior: -7.5002e-01
Epoch 7/10
13/13 - 2s - loss: 241.5283 - loglik: -2.4080e+02 - logprior: -7.2994e-01
Epoch 8/10
13/13 - 1s - loss: 241.9958 - loglik: -2.4129e+02 - logprior: -7.1055e-01
Fitted a model with MAP estimate = -241.5073
Time for alignment: 50.8922
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.5017 - loglik: -3.0881e+02 - logprior: -7.6904e+00
Epoch 2/10
13/13 - 1s - loss: 285.6228 - loglik: -2.8376e+02 - logprior: -1.8665e+00
Epoch 3/10
13/13 - 1s - loss: 262.8754 - loglik: -2.6124e+02 - logprior: -1.6383e+00
Epoch 4/10
13/13 - 1s - loss: 253.2677 - loglik: -2.5140e+02 - logprior: -1.8725e+00
Epoch 5/10
13/13 - 1s - loss: 249.9064 - loglik: -2.4806e+02 - logprior: -1.8500e+00
Epoch 6/10
13/13 - 1s - loss: 249.6301 - loglik: -2.4787e+02 - logprior: -1.7649e+00
Epoch 7/10
13/13 - 1s - loss: 248.5774 - loglik: -2.4682e+02 - logprior: -1.7617e+00
Epoch 8/10
13/13 - 1s - loss: 249.0923 - loglik: -2.4734e+02 - logprior: -1.7553e+00
Fitted a model with MAP estimate = -248.6892
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (21, 1), (27, 1), (30, 3), (31, 2), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 257.7947 - loglik: -2.4875e+02 - logprior: -9.0492e+00
Epoch 2/2
13/13 - 2s - loss: 248.0822 - loglik: -2.4406e+02 - logprior: -4.0268e+00
Fitted a model with MAP estimate = -245.9701
expansions: [(0, 2)]
discards: [ 0 14 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 249.5972 - loglik: -2.4279e+02 - logprior: -6.8055e+00
Epoch 2/2
13/13 - 2s - loss: 244.3408 - loglik: -2.4254e+02 - logprior: -1.8023e+00
Fitted a model with MAP estimate = -242.8170
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.3583 - loglik: -2.4374e+02 - logprior: -8.6148e+00
Epoch 2/10
13/13 - 1s - loss: 245.0740 - loglik: -2.4250e+02 - logprior: -2.5741e+00
Epoch 3/10
13/13 - 2s - loss: 243.2472 - loglik: -2.4202e+02 - logprior: -1.2240e+00
Epoch 4/10
13/13 - 2s - loss: 243.1085 - loglik: -2.4221e+02 - logprior: -9.0311e-01
Epoch 5/10
13/13 - 1s - loss: 241.9924 - loglik: -2.4120e+02 - logprior: -7.9129e-01
Epoch 6/10
13/13 - 2s - loss: 241.7346 - loglik: -2.4097e+02 - logprior: -7.6568e-01
Epoch 7/10
13/13 - 1s - loss: 241.9180 - loglik: -2.4117e+02 - logprior: -7.4322e-01
Fitted a model with MAP estimate = -241.6616
Time for alignment: 49.1730
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.5182 - loglik: -3.0883e+02 - logprior: -7.6894e+00
Epoch 2/10
13/13 - 1s - loss: 285.3864 - loglik: -2.8352e+02 - logprior: -1.8671e+00
Epoch 3/10
13/13 - 1s - loss: 262.4915 - loglik: -2.6085e+02 - logprior: -1.6374e+00
Epoch 4/10
13/13 - 1s - loss: 251.9250 - loglik: -2.5004e+02 - logprior: -1.8834e+00
Epoch 5/10
13/13 - 1s - loss: 249.9216 - loglik: -2.4810e+02 - logprior: -1.8254e+00
Epoch 6/10
13/13 - 1s - loss: 248.4790 - loglik: -2.4674e+02 - logprior: -1.7395e+00
Epoch 7/10
13/13 - 1s - loss: 248.6484 - loglik: -2.4692e+02 - logprior: -1.7276e+00
Fitted a model with MAP estimate = -248.3701
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 2), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 258.1353 - loglik: -2.4910e+02 - logprior: -9.0346e+00
Epoch 2/2
13/13 - 2s - loss: 247.7865 - loglik: -2.4377e+02 - logprior: -4.0147e+00
Fitted a model with MAP estimate = -245.9592
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 249.7843 - loglik: -2.4299e+02 - logprior: -6.7988e+00
Epoch 2/2
13/13 - 1s - loss: 243.2902 - loglik: -2.4150e+02 - logprior: -1.7896e+00
Fitted a model with MAP estimate = -242.8249
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.1760 - loglik: -2.4356e+02 - logprior: -8.6146e+00
Epoch 2/10
13/13 - 1s - loss: 244.8682 - loglik: -2.4230e+02 - logprior: -2.5722e+00
Epoch 3/10
13/13 - 1s - loss: 243.4615 - loglik: -2.4225e+02 - logprior: -1.2089e+00
Epoch 4/10
13/13 - 1s - loss: 242.4363 - loglik: -2.4156e+02 - logprior: -8.7423e-01
Epoch 5/10
13/13 - 2s - loss: 242.1372 - loglik: -2.4136e+02 - logprior: -7.7993e-01
Epoch 6/10
13/13 - 2s - loss: 242.1044 - loglik: -2.4135e+02 - logprior: -7.5323e-01
Epoch 7/10
13/13 - 1s - loss: 241.6747 - loglik: -2.4095e+02 - logprior: -7.2597e-01
Epoch 8/10
13/13 - 2s - loss: 241.4097 - loglik: -2.4070e+02 - logprior: -7.0539e-01
Epoch 9/10
13/13 - 2s - loss: 241.7803 - loglik: -2.4111e+02 - logprior: -6.7011e-01
Fitted a model with MAP estimate = -241.3950
Time for alignment: 49.4770
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 315.9258 - loglik: -3.0824e+02 - logprior: -7.6897e+00
Epoch 2/10
13/13 - 1s - loss: 285.1838 - loglik: -2.8332e+02 - logprior: -1.8624e+00
Epoch 3/10
13/13 - 1s - loss: 260.5260 - loglik: -2.5889e+02 - logprior: -1.6325e+00
Epoch 4/10
13/13 - 1s - loss: 251.8957 - loglik: -2.5007e+02 - logprior: -1.8209e+00
Epoch 5/10
13/13 - 1s - loss: 249.3455 - loglik: -2.4761e+02 - logprior: -1.7330e+00
Epoch 6/10
13/13 - 1s - loss: 249.2777 - loglik: -2.4764e+02 - logprior: -1.6419e+00
Epoch 7/10
13/13 - 1s - loss: 248.9097 - loglik: -2.4727e+02 - logprior: -1.6419e+00
Epoch 8/10
13/13 - 1s - loss: 248.0510 - loglik: -2.4641e+02 - logprior: -1.6449e+00
Epoch 9/10
13/13 - 1s - loss: 248.6216 - loglik: -2.4700e+02 - logprior: -1.6204e+00
Fitted a model with MAP estimate = -248.3859
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 3), (30, 1), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 258.2420 - loglik: -2.4920e+02 - logprior: -9.0415e+00
Epoch 2/2
13/13 - 2s - loss: 247.8515 - loglik: -2.4382e+02 - logprior: -4.0354e+00
Fitted a model with MAP estimate = -246.0647
expansions: [(0, 2)]
discards: [ 0 14 36 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.2201 - loglik: -2.4341e+02 - logprior: -6.8149e+00
Epoch 2/2
13/13 - 1s - loss: 243.2776 - loglik: -2.4147e+02 - logprior: -1.8103e+00
Fitted a model with MAP estimate = -242.9159
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.2288 - loglik: -2.4362e+02 - logprior: -8.6079e+00
Epoch 2/10
13/13 - 1s - loss: 245.1365 - loglik: -2.4259e+02 - logprior: -2.5512e+00
Epoch 3/10
13/13 - 1s - loss: 243.3088 - loglik: -2.4209e+02 - logprior: -1.2162e+00
Epoch 4/10
13/13 - 1s - loss: 242.3485 - loglik: -2.4144e+02 - logprior: -9.0729e-01
Epoch 5/10
13/13 - 1s - loss: 242.5519 - loglik: -2.4174e+02 - logprior: -8.1025e-01
Fitted a model with MAP estimate = -241.8961
Time for alignment: 46.9627
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.3544 - loglik: -3.0866e+02 - logprior: -7.6921e+00
Epoch 2/10
13/13 - 1s - loss: 284.8290 - loglik: -2.8296e+02 - logprior: -1.8700e+00
Epoch 3/10
13/13 - 1s - loss: 260.1389 - loglik: -2.5848e+02 - logprior: -1.6541e+00
Epoch 4/10
13/13 - 1s - loss: 251.7578 - loglik: -2.4991e+02 - logprior: -1.8441e+00
Epoch 5/10
13/13 - 1s - loss: 249.4289 - loglik: -2.4769e+02 - logprior: -1.7376e+00
Epoch 6/10
13/13 - 1s - loss: 248.3208 - loglik: -2.4669e+02 - logprior: -1.6313e+00
Epoch 7/10
13/13 - 1s - loss: 248.8466 - loglik: -2.4721e+02 - logprior: -1.6336e+00
Fitted a model with MAP estimate = -248.2496
expansions: [(9, 1), (10, 1), (13, 2), (20, 1), (28, 4), (29, 3), (30, 1), (31, 1), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 258.2875 - loglik: -2.4926e+02 - logprior: -9.0232e+00
Epoch 2/2
13/13 - 2s - loss: 247.3599 - loglik: -2.4337e+02 - logprior: -3.9892e+00
Fitted a model with MAP estimate = -245.8823
expansions: [(0, 2)]
discards: [ 0 14 36 67 93 96]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 249.8376 - loglik: -2.4301e+02 - logprior: -6.8256e+00
Epoch 2/2
13/13 - 2s - loss: 243.5734 - loglik: -2.4175e+02 - logprior: -1.8207e+00
Fitted a model with MAP estimate = -242.9078
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.1863 - loglik: -2.4356e+02 - logprior: -8.6258e+00
Epoch 2/10
13/13 - 1s - loss: 245.0844 - loglik: -2.4251e+02 - logprior: -2.5790e+00
Epoch 3/10
13/13 - 1s - loss: 243.4314 - loglik: -2.4219e+02 - logprior: -1.2379e+00
Epoch 4/10
13/13 - 1s - loss: 242.6680 - loglik: -2.4176e+02 - logprior: -9.1168e-01
Epoch 5/10
13/13 - 1s - loss: 242.1431 - loglik: -2.4133e+02 - logprior: -8.1210e-01
Epoch 6/10
13/13 - 1s - loss: 242.2666 - loglik: -2.4149e+02 - logprior: -7.7328e-01
Fitted a model with MAP estimate = -241.7203
Time for alignment: 44.4665
Computed alignments with likelihoods: ['-241.5073', '-241.6616', '-241.3950', '-241.8961', '-241.7203']
Best model has likelihood: -241.3950  (prior= -0.6562 )
time for generating output: 0.1642
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9347964376590331
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 432.4066 - loglik: -3.4684e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 327.9088 - loglik: -3.0808e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 276.8516 - loglik: -2.6886e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 245.3766 - loglik: -2.4087e+02 - logprior: -4.5111e+00
Epoch 5/10
10/10 - 1s - loss: 231.6155 - loglik: -2.2919e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 226.1261 - loglik: -2.2498e+02 - logprior: -1.1500e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2304e+02 - logprior: -2.3722e-01
Epoch 8/10
10/10 - 1s - loss: 221.6673 - loglik: -2.2203e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 220.6929 - loglik: -2.2152e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 220.0384 - loglik: -2.2128e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -219.7978
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3567 - loglik: -2.1681e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 236.8901 - loglik: -2.0075e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -223.8493
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 272.7868 - loglik: -1.9773e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 209.7395 - loglik: -1.9371e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -200.0824
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 286.1926 - loglik: -1.9638e+02 - logprior: -8.9810e+01
Epoch 2/10
10/10 - 1s - loss: 217.5236 - loglik: -1.9494e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 198.3763 - loglik: -1.9409e+02 - logprior: -4.2907e+00
Epoch 4/10
10/10 - 1s - loss: 191.4062 - loglik: -1.9360e+02 - logprior: 2.1900
Epoch 5/10
10/10 - 1s - loss: 188.0892 - loglik: -1.9339e+02 - logprior: 5.2989
Epoch 6/10
10/10 - 1s - loss: 186.2533 - loglik: -1.9332e+02 - logprior: 7.0716
Epoch 7/10
10/10 - 1s - loss: 185.1154 - loglik: -1.9338e+02 - logprior: 8.2601
Epoch 8/10
10/10 - 1s - loss: 184.3044 - loglik: -1.9351e+02 - logprior: 9.2069
Epoch 9/10
10/10 - 1s - loss: 183.6523 - loglik: -1.9367e+02 - logprior: 10.0220
Epoch 10/10
10/10 - 1s - loss: 183.0842 - loglik: -1.9379e+02 - logprior: 10.7106
Fitted a model with MAP estimate = -182.7934
Time for alignment: 45.2852
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4066 - loglik: -3.4684e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 327.9088 - loglik: -3.0808e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 276.8516 - loglik: -2.6886e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 245.3766 - loglik: -2.4087e+02 - logprior: -4.5111e+00
Epoch 5/10
10/10 - 1s - loss: 231.6156 - loglik: -2.2919e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 226.1261 - loglik: -2.2498e+02 - logprior: -1.1500e+00
Epoch 7/10
10/10 - 1s - loss: 223.2770 - loglik: -2.2304e+02 - logprior: -2.3722e-01
Epoch 8/10
10/10 - 1s - loss: 221.6673 - loglik: -2.2203e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 220.6929 - loglik: -2.2152e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 220.0384 - loglik: -2.2128e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -219.7976
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 312.3566 - loglik: -2.1681e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 236.8901 - loglik: -2.0075e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -223.8493
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.7869 - loglik: -1.9773e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 209.7395 - loglik: -1.9371e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -200.0824
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 286.1927 - loglik: -1.9638e+02 - logprior: -8.9810e+01
Epoch 2/10
10/10 - 1s - loss: 217.5237 - loglik: -1.9494e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 198.3763 - loglik: -1.9409e+02 - logprior: -4.2907e+00
Epoch 4/10
10/10 - 1s - loss: 191.4062 - loglik: -1.9360e+02 - logprior: 2.1900
Epoch 5/10
10/10 - 1s - loss: 188.0892 - loglik: -1.9339e+02 - logprior: 5.2989
Epoch 6/10
10/10 - 1s - loss: 186.2533 - loglik: -1.9332e+02 - logprior: 7.0716
Epoch 7/10
10/10 - 1s - loss: 185.1154 - loglik: -1.9338e+02 - logprior: 8.2601
Epoch 8/10
10/10 - 1s - loss: 184.3044 - loglik: -1.9351e+02 - logprior: 9.2069
Epoch 9/10
10/10 - 1s - loss: 183.6522 - loglik: -1.9367e+02 - logprior: 10.0220
Epoch 10/10
10/10 - 1s - loss: 183.0842 - loglik: -1.9379e+02 - logprior: 10.7106
Fitted a model with MAP estimate = -182.7934
Time for alignment: 45.3126
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4066 - loglik: -3.4684e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 327.9088 - loglik: -3.0808e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 276.8516 - loglik: -2.6886e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 245.3767 - loglik: -2.4087e+02 - logprior: -4.5111e+00
Epoch 5/10
10/10 - 1s - loss: 231.6156 - loglik: -2.2919e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 226.1260 - loglik: -2.2498e+02 - logprior: -1.1500e+00
Epoch 7/10
10/10 - 1s - loss: 223.2770 - loglik: -2.2304e+02 - logprior: -2.3722e-01
Epoch 8/10
10/10 - 1s - loss: 221.6673 - loglik: -2.2203e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 220.6929 - loglik: -2.2152e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 220.0383 - loglik: -2.2128e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -219.7978
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3566 - loglik: -2.1681e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 236.8901 - loglik: -2.0075e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -223.8494
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 272.7869 - loglik: -1.9773e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 209.7395 - loglik: -1.9371e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -200.0824
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 286.1926 - loglik: -1.9638e+02 - logprior: -8.9810e+01
Epoch 2/10
10/10 - 1s - loss: 217.5237 - loglik: -1.9494e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 198.3763 - loglik: -1.9409e+02 - logprior: -4.2908e+00
Epoch 4/10
10/10 - 1s - loss: 191.4062 - loglik: -1.9360e+02 - logprior: 2.1899
Epoch 5/10
10/10 - 1s - loss: 188.0893 - loglik: -1.9339e+02 - logprior: 5.2989
Epoch 6/10
10/10 - 1s - loss: 186.2534 - loglik: -1.9332e+02 - logprior: 7.0715
Epoch 7/10
10/10 - 1s - loss: 185.1153 - loglik: -1.9338e+02 - logprior: 8.2602
Epoch 8/10
10/10 - 1s - loss: 184.3044 - loglik: -1.9351e+02 - logprior: 9.2068
Epoch 9/10
10/10 - 1s - loss: 183.6522 - loglik: -1.9367e+02 - logprior: 10.0220
Epoch 10/10
10/10 - 1s - loss: 183.0842 - loglik: -1.9379e+02 - logprior: 10.7107
Fitted a model with MAP estimate = -182.7935
Time for alignment: 42.9641
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4066 - loglik: -3.4684e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 327.9088 - loglik: -3.0808e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 276.8516 - loglik: -2.6886e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 245.3766 - loglik: -2.4087e+02 - logprior: -4.5111e+00
Epoch 5/10
10/10 - 1s - loss: 231.6156 - loglik: -2.2919e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 226.1260 - loglik: -2.2498e+02 - logprior: -1.1500e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2304e+02 - logprior: -2.3722e-01
Epoch 8/10
10/10 - 1s - loss: 221.6674 - loglik: -2.2203e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 220.6929 - loglik: -2.2152e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 220.0384 - loglik: -2.2128e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -219.7976
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3568 - loglik: -2.1681e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 236.8901 - loglik: -2.0075e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -223.8493
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.7868 - loglik: -1.9773e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 209.7395 - loglik: -1.9371e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -200.0823
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 286.1926 - loglik: -1.9638e+02 - logprior: -8.9810e+01
Epoch 2/10
10/10 - 1s - loss: 217.5236 - loglik: -1.9494e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 198.3763 - loglik: -1.9409e+02 - logprior: -4.2907e+00
Epoch 4/10
10/10 - 1s - loss: 191.4062 - loglik: -1.9360e+02 - logprior: 2.1900
Epoch 5/10
10/10 - 1s - loss: 188.0892 - loglik: -1.9339e+02 - logprior: 5.2989
Epoch 6/10
10/10 - 1s - loss: 186.2533 - loglik: -1.9332e+02 - logprior: 7.0716
Epoch 7/10
10/10 - 1s - loss: 185.1153 - loglik: -1.9338e+02 - logprior: 8.2601
Epoch 8/10
10/10 - 1s - loss: 184.3044 - loglik: -1.9351e+02 - logprior: 9.2069
Epoch 9/10
10/10 - 1s - loss: 183.6523 - loglik: -1.9367e+02 - logprior: 10.0220
Epoch 10/10
10/10 - 1s - loss: 183.0842 - loglik: -1.9379e+02 - logprior: 10.7106
Fitted a model with MAP estimate = -182.7932
Time for alignment: 44.7773
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4066 - loglik: -3.4684e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 327.9088 - loglik: -3.0808e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 276.8516 - loglik: -2.6886e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 245.3766 - loglik: -2.4087e+02 - logprior: -4.5111e+00
Epoch 5/10
10/10 - 1s - loss: 231.6155 - loglik: -2.2919e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 226.1260 - loglik: -2.2498e+02 - logprior: -1.1500e+00
Epoch 7/10
10/10 - 1s - loss: 223.2770 - loglik: -2.2304e+02 - logprior: -2.3722e-01
Epoch 8/10
10/10 - 1s - loss: 221.6674 - loglik: -2.2203e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 220.6928 - loglik: -2.2152e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 220.0384 - loglik: -2.2128e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -219.7978
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3567 - loglik: -2.1681e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 236.8901 - loglik: -2.0075e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -223.8493
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 272.7869 - loglik: -1.9773e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 209.7395 - loglik: -1.9371e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -200.0824
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 286.1927 - loglik: -1.9638e+02 - logprior: -8.9810e+01
Epoch 2/10
10/10 - 1s - loss: 217.5236 - loglik: -1.9494e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 198.3763 - loglik: -1.9409e+02 - logprior: -4.2907e+00
Epoch 4/10
10/10 - 1s - loss: 191.4062 - loglik: -1.9360e+02 - logprior: 2.1900
Epoch 5/10
10/10 - 1s - loss: 188.0893 - loglik: -1.9339e+02 - logprior: 5.2989
Epoch 6/10
10/10 - 1s - loss: 186.2533 - loglik: -1.9332e+02 - logprior: 7.0716
Epoch 7/10
10/10 - 1s - loss: 185.1153 - loglik: -1.9338e+02 - logprior: 8.2602
Epoch 8/10
10/10 - 1s - loss: 184.3043 - loglik: -1.9351e+02 - logprior: 9.2069
Epoch 9/10
10/10 - 1s - loss: 183.6522 - loglik: -1.9367e+02 - logprior: 10.0220
Epoch 10/10
10/10 - 1s - loss: 183.0843 - loglik: -1.9379e+02 - logprior: 10.7106
Fitted a model with MAP estimate = -182.7932
Time for alignment: 42.6082
Computed alignments with likelihoods: ['-182.7934', '-182.7934', '-182.7935', '-182.7932', '-182.7932']
Best model has likelihood: -182.7932  (prior= 11.0461 )
time for generating output: 0.1385
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.9116417910447762
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.2120 - loglik: -1.3554e+02 - logprior: -3.1675e+01
Epoch 2/10
10/10 - 0s - loss: 124.0273 - loglik: -1.1506e+02 - logprior: -8.9685e+00
Epoch 3/10
10/10 - 0s - loss: 98.9691 - loglik: -9.4054e+01 - logprior: -4.9151e+00
Epoch 4/10
10/10 - 0s - loss: 83.7609 - loglik: -7.9847e+01 - logprior: -3.9138e+00
Epoch 5/10
10/10 - 0s - loss: 78.6486 - loglik: -7.5077e+01 - logprior: -3.5718e+00
Epoch 6/10
10/10 - 0s - loss: 76.7473 - loglik: -7.3733e+01 - logprior: -3.0140e+00
Epoch 7/10
10/10 - 0s - loss: 76.0453 - loglik: -7.3451e+01 - logprior: -2.5944e+00
Epoch 8/10
10/10 - 0s - loss: 75.6132 - loglik: -7.3182e+01 - logprior: -2.4316e+00
Epoch 9/10
10/10 - 0s - loss: 75.2472 - loglik: -7.2917e+01 - logprior: -2.3299e+00
Epoch 10/10
10/10 - 0s - loss: 75.0744 - loglik: -7.2854e+01 - logprior: -2.2206e+00
Fitted a model with MAP estimate = -75.0073
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 1), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 113.5118 - loglik: -7.1243e+01 - logprior: -4.2269e+01
Epoch 2/2
10/10 - 0s - loss: 77.7333 - loglik: -6.4095e+01 - logprior: -1.3638e+01
Fitted a model with MAP estimate = -70.7174
expansions: []
discards: [30]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 90.9803 - loglik: -6.1173e+01 - logprior: -2.9807e+01
Epoch 2/2
10/10 - 0s - loss: 69.1420 - loglik: -6.0710e+01 - logprior: -8.4321e+00
Fitted a model with MAP estimate = -66.2633
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.5365 - loglik: -6.0323e+01 - logprior: -2.8213e+01
Epoch 2/10
10/10 - 0s - loss: 68.7497 - loglik: -6.0769e+01 - logprior: -7.9809e+00
Epoch 3/10
10/10 - 0s - loss: 65.1261 - loglik: -6.1150e+01 - logprior: -3.9759e+00
Epoch 4/10
10/10 - 0s - loss: 63.6895 - loglik: -6.1232e+01 - logprior: -2.4577e+00
Epoch 5/10
10/10 - 0s - loss: 63.0579 - loglik: -6.1325e+01 - logprior: -1.7327e+00
Epoch 6/10
10/10 - 0s - loss: 62.5935 - loglik: -6.1220e+01 - logprior: -1.3738e+00
Epoch 7/10
10/10 - 0s - loss: 62.5283 - loglik: -6.1389e+01 - logprior: -1.1389e+00
Epoch 8/10
10/10 - 0s - loss: 62.3544 - loglik: -6.1427e+01 - logprior: -9.2741e-01
Epoch 9/10
10/10 - 0s - loss: 62.4649 - loglik: -6.1689e+01 - logprior: -7.7556e-01
Fitted a model with MAP estimate = -62.2534
Time for alignment: 25.1579
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.2253 - loglik: -1.3555e+02 - logprior: -3.1676e+01
Epoch 2/10
10/10 - 0s - loss: 123.7679 - loglik: -1.1480e+02 - logprior: -8.9657e+00
Epoch 3/10
10/10 - 0s - loss: 97.8969 - loglik: -9.2966e+01 - logprior: -4.9309e+00
Epoch 4/10
10/10 - 0s - loss: 82.9183 - loglik: -7.8966e+01 - logprior: -3.9527e+00
Epoch 5/10
10/10 - 0s - loss: 78.1731 - loglik: -7.4601e+01 - logprior: -3.5718e+00
Epoch 6/10
10/10 - 0s - loss: 76.5090 - loglik: -7.3515e+01 - logprior: -2.9942e+00
Epoch 7/10
10/10 - 0s - loss: 75.9559 - loglik: -7.3376e+01 - logprior: -2.5798e+00
Epoch 8/10
10/10 - 0s - loss: 75.5810 - loglik: -7.3153e+01 - logprior: -2.4284e+00
Epoch 9/10
10/10 - 0s - loss: 75.1116 - loglik: -7.2788e+01 - logprior: -2.3239e+00
Epoch 10/10
10/10 - 0s - loss: 75.0882 - loglik: -7.2872e+01 - logprior: -2.2164e+00
Fitted a model with MAP estimate = -74.9834
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 113.5250 - loglik: -7.1274e+01 - logprior: -4.2251e+01
Epoch 2/2
10/10 - 0s - loss: 77.6764 - loglik: -6.4054e+01 - logprior: -1.3623e+01
Fitted a model with MAP estimate = -70.6954
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9811 - loglik: -6.1165e+01 - logprior: -2.9816e+01
Epoch 2/2
10/10 - 0s - loss: 69.1170 - loglik: -6.0693e+01 - logprior: -8.4242e+00
Fitted a model with MAP estimate = -66.2505
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.5660 - loglik: -6.0359e+01 - logprior: -2.8207e+01
Epoch 2/10
10/10 - 0s - loss: 68.7700 - loglik: -6.0786e+01 - logprior: -7.9838e+00
Epoch 3/10
10/10 - 0s - loss: 65.0155 - loglik: -6.1045e+01 - logprior: -3.9709e+00
Epoch 4/10
10/10 - 0s - loss: 63.7548 - loglik: -6.1293e+01 - logprior: -2.4617e+00
Epoch 5/10
10/10 - 0s - loss: 62.9774 - loglik: -6.1249e+01 - logprior: -1.7286e+00
Epoch 6/10
10/10 - 0s - loss: 62.5831 - loglik: -6.1213e+01 - logprior: -1.3701e+00
Epoch 7/10
10/10 - 0s - loss: 62.5825 - loglik: -6.1448e+01 - logprior: -1.1343e+00
Epoch 8/10
10/10 - 0s - loss: 62.4205 - loglik: -6.1493e+01 - logprior: -9.2724e-01
Epoch 9/10
10/10 - 0s - loss: 62.3085 - loglik: -6.1538e+01 - logprior: -7.7025e-01
Epoch 10/10
10/10 - 0s - loss: 62.2013 - loglik: -6.1527e+01 - logprior: -6.7430e-01
Fitted a model with MAP estimate = -62.1742
Time for alignment: 26.2825
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.2277 - loglik: -1.3555e+02 - logprior: -3.1675e+01
Epoch 2/10
10/10 - 0s - loss: 124.1305 - loglik: -1.1516e+02 - logprior: -8.9754e+00
Epoch 3/10
10/10 - 0s - loss: 98.9815 - loglik: -9.4043e+01 - logprior: -4.9381e+00
Epoch 4/10
10/10 - 0s - loss: 83.8248 - loglik: -7.9863e+01 - logprior: -3.9617e+00
Epoch 5/10
10/10 - 0s - loss: 78.7474 - loglik: -7.5057e+01 - logprior: -3.6903e+00
Epoch 6/10
10/10 - 0s - loss: 76.9539 - loglik: -7.3742e+01 - logprior: -3.2118e+00
Epoch 7/10
10/10 - 0s - loss: 75.9745 - loglik: -7.3154e+01 - logprior: -2.8206e+00
Epoch 8/10
10/10 - 0s - loss: 75.3296 - loglik: -7.2646e+01 - logprior: -2.6839e+00
Epoch 9/10
10/10 - 0s - loss: 74.9544 - loglik: -7.2370e+01 - logprior: -2.5839e+00
Epoch 10/10
10/10 - 0s - loss: 74.7411 - loglik: -7.2258e+01 - logprior: -2.4828e+00
Fitted a model with MAP estimate = -74.6577
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 1), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 111.1246 - loglik: -6.9009e+01 - logprior: -4.2115e+01
Epoch 2/2
10/10 - 0s - loss: 75.8849 - loglik: -6.2397e+01 - logprior: -1.3488e+01
Fitted a model with MAP estimate = -69.4512
expansions: []
discards: [ 0 31]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 98.7570 - loglik: -6.2718e+01 - logprior: -3.6039e+01
Epoch 2/2
10/10 - 0s - loss: 77.0819 - loglik: -6.2632e+01 - logprior: -1.4450e+01
Fitted a model with MAP estimate = -72.7419
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 93.5827 - loglik: -6.1641e+01 - logprior: -3.1942e+01
Epoch 2/10
10/10 - 0s - loss: 70.6537 - loglik: -6.1475e+01 - logprior: -9.1783e+00
Epoch 3/10
10/10 - 0s - loss: 65.4041 - loglik: -6.0817e+01 - logprior: -4.5868e+00
Epoch 4/10
10/10 - 0s - loss: 63.7027 - loglik: -6.0602e+01 - logprior: -3.1006e+00
Epoch 5/10
10/10 - 0s - loss: 62.9368 - loglik: -6.0910e+01 - logprior: -2.0267e+00
Epoch 6/10
10/10 - 0s - loss: 62.5942 - loglik: -6.1197e+01 - logprior: -1.3968e+00
Epoch 7/10
10/10 - 0s - loss: 62.1724 - loglik: -6.1033e+01 - logprior: -1.1394e+00
Epoch 8/10
10/10 - 0s - loss: 62.1951 - loglik: -6.1268e+01 - logprior: -9.2684e-01
Fitted a model with MAP estimate = -62.0498
Time for alignment: 24.6887
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.1593 - loglik: -1.3548e+02 - logprior: -3.1675e+01
Epoch 2/10
10/10 - 0s - loss: 124.0185 - loglik: -1.1505e+02 - logprior: -8.9687e+00
Epoch 3/10
10/10 - 0s - loss: 98.6134 - loglik: -9.3693e+01 - logprior: -4.9204e+00
Epoch 4/10
10/10 - 0s - loss: 83.5012 - loglik: -7.9566e+01 - logprior: -3.9351e+00
Epoch 5/10
10/10 - 0s - loss: 78.5988 - loglik: -7.5020e+01 - logprior: -3.5787e+00
Epoch 6/10
10/10 - 0s - loss: 76.7678 - loglik: -7.3755e+01 - logprior: -3.0130e+00
Epoch 7/10
10/10 - 0s - loss: 75.9955 - loglik: -7.3409e+01 - logprior: -2.5860e+00
Epoch 8/10
10/10 - 0s - loss: 75.5356 - loglik: -7.3105e+01 - logprior: -2.4305e+00
Epoch 9/10
10/10 - 0s - loss: 75.2067 - loglik: -7.2878e+01 - logprior: -2.3286e+00
Epoch 10/10
10/10 - 0s - loss: 75.0289 - loglik: -7.2811e+01 - logprior: -2.2184e+00
Fitted a model with MAP estimate = -74.9974
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.3504 - loglik: -7.2113e+01 - logprior: -4.2238e+01
Epoch 2/2
10/10 - 0s - loss: 78.0713 - loglik: -6.4303e+01 - logprior: -1.3769e+01
Fitted a model with MAP estimate = -70.9735
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.0930 - loglik: -6.1264e+01 - logprior: -2.9829e+01
Epoch 2/2
10/10 - 0s - loss: 69.2157 - loglik: -6.0790e+01 - logprior: -8.4255e+00
Fitted a model with MAP estimate = -66.2665
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6525 - loglik: -6.0439e+01 - logprior: -2.8213e+01
Epoch 2/10
10/10 - 0s - loss: 68.7221 - loglik: -6.0744e+01 - logprior: -7.9782e+00
Epoch 3/10
10/10 - 0s - loss: 65.0134 - loglik: -6.1037e+01 - logprior: -3.9766e+00
Epoch 4/10
10/10 - 0s - loss: 63.7050 - loglik: -6.1245e+01 - logprior: -2.4599e+00
Epoch 5/10
10/10 - 0s - loss: 63.0055 - loglik: -6.1274e+01 - logprior: -1.7310e+00
Epoch 6/10
10/10 - 0s - loss: 62.5568 - loglik: -6.1179e+01 - logprior: -1.3778e+00
Epoch 7/10
10/10 - 0s - loss: 62.5881 - loglik: -6.1453e+01 - logprior: -1.1346e+00
Fitted a model with MAP estimate = -62.4370
Time for alignment: 23.8872
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.2364 - loglik: -1.3556e+02 - logprior: -3.1675e+01
Epoch 2/10
10/10 - 0s - loss: 123.7938 - loglik: -1.1482e+02 - logprior: -8.9696e+00
Epoch 3/10
10/10 - 0s - loss: 98.1439 - loglik: -9.3211e+01 - logprior: -4.9332e+00
Epoch 4/10
10/10 - 0s - loss: 83.1966 - loglik: -7.9238e+01 - logprior: -3.9582e+00
Epoch 5/10
10/10 - 0s - loss: 78.6423 - loglik: -7.5039e+01 - logprior: -3.6037e+00
Epoch 6/10
10/10 - 0s - loss: 76.7246 - loglik: -7.3714e+01 - logprior: -3.0108e+00
Epoch 7/10
10/10 - 0s - loss: 76.0795 - loglik: -7.3492e+01 - logprior: -2.5871e+00
Epoch 8/10
10/10 - 0s - loss: 75.4627 - loglik: -7.3033e+01 - logprior: -2.4297e+00
Epoch 9/10
10/10 - 0s - loss: 75.2378 - loglik: -7.2907e+01 - logprior: -2.3313e+00
Epoch 10/10
10/10 - 0s - loss: 75.0827 - loglik: -7.2861e+01 - logprior: -2.2221e+00
Fitted a model with MAP estimate = -74.9924
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 113.3960 - loglik: -7.1167e+01 - logprior: -4.2229e+01
Epoch 2/2
10/10 - 0s - loss: 77.7618 - loglik: -6.4149e+01 - logprior: -1.3613e+01
Fitted a model with MAP estimate = -70.6928
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.9214 - loglik: -6.1118e+01 - logprior: -2.9804e+01
Epoch 2/2
10/10 - 0s - loss: 69.2530 - loglik: -6.0822e+01 - logprior: -8.4306e+00
Fitted a model with MAP estimate = -66.2519
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6606 - loglik: -6.0451e+01 - logprior: -2.8210e+01
Epoch 2/10
10/10 - 0s - loss: 68.6632 - loglik: -6.0684e+01 - logprior: -7.9793e+00
Epoch 3/10
10/10 - 0s - loss: 65.0112 - loglik: -6.1034e+01 - logprior: -3.9769e+00
Epoch 4/10
10/10 - 0s - loss: 63.6357 - loglik: -6.1178e+01 - logprior: -2.4580e+00
Epoch 5/10
10/10 - 0s - loss: 62.9823 - loglik: -6.1254e+01 - logprior: -1.7284e+00
Epoch 6/10
10/10 - 0s - loss: 62.8256 - loglik: -6.1456e+01 - logprior: -1.3699e+00
Epoch 7/10
10/10 - 0s - loss: 62.4609 - loglik: -6.1329e+01 - logprior: -1.1314e+00
Epoch 8/10
10/10 - 0s - loss: 62.3965 - loglik: -6.1467e+01 - logprior: -9.2932e-01
Epoch 9/10
10/10 - 0s - loss: 62.3016 - loglik: -6.1534e+01 - logprior: -7.6746e-01
Epoch 10/10
10/10 - 0s - loss: 62.2402 - loglik: -6.1568e+01 - logprior: -6.7250e-01
Fitted a model with MAP estimate = -62.1748
Time for alignment: 24.5965
Computed alignments with likelihoods: ['-62.2534', '-62.1742', '-62.0498', '-62.4370', '-62.1748']
Best model has likelihood: -62.0498  (prior= -0.8084 )
time for generating output: 0.0921
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.3544 - loglik: -4.4004e+02 - logprior: -5.3162e+00
Epoch 2/10
15/15 - 3s - loss: 365.2937 - loglik: -3.6402e+02 - logprior: -1.2703e+00
Epoch 3/10
15/15 - 3s - loss: 322.5317 - loglik: -3.2106e+02 - logprior: -1.4759e+00
Epoch 4/10
15/15 - 3s - loss: 310.2586 - loglik: -3.0866e+02 - logprior: -1.5985e+00
Epoch 5/10
15/15 - 3s - loss: 305.3927 - loglik: -3.0397e+02 - logprior: -1.4267e+00
Epoch 6/10
15/15 - 3s - loss: 303.7905 - loglik: -3.0236e+02 - logprior: -1.4286e+00
Epoch 7/10
15/15 - 3s - loss: 302.3163 - loglik: -3.0088e+02 - logprior: -1.4350e+00
Epoch 8/10
15/15 - 3s - loss: 302.4406 - loglik: -3.0102e+02 - logprior: -1.4234e+00
Fitted a model with MAP estimate = -301.6576
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 2), (67, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 2), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 303.2609 - loglik: -2.9672e+02 - logprior: -6.5453e+00
Epoch 2/2
15/15 - 4s - loss: 287.1960 - loglik: -2.8421e+02 - logprior: -2.9846e+00
Fitted a model with MAP estimate = -284.5217
expansions: [(0, 2)]
discards: [  0   7  57  83 107 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 289.2851 - loglik: -2.8439e+02 - logprior: -4.8918e+00
Epoch 2/2
15/15 - 4s - loss: 283.6797 - loglik: -2.8229e+02 - logprior: -1.3932e+00
Fitted a model with MAP estimate = -282.2555
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 291.0190 - loglik: -2.8463e+02 - logprior: -6.3940e+00
Epoch 2/10
15/15 - 4s - loss: 284.4202 - loglik: -2.8242e+02 - logprior: -1.9976e+00
Epoch 3/10
15/15 - 4s - loss: 283.0475 - loglik: -2.8214e+02 - logprior: -9.0888e-01
Epoch 4/10
15/15 - 4s - loss: 281.3610 - loglik: -2.8061e+02 - logprior: -7.4820e-01
Epoch 5/10
15/15 - 4s - loss: 280.5260 - loglik: -2.7980e+02 - logprior: -7.2135e-01
Epoch 6/10
15/15 - 4s - loss: 280.1526 - loglik: -2.7947e+02 - logprior: -6.8667e-01
Epoch 7/10
15/15 - 4s - loss: 279.0998 - loglik: -2.7846e+02 - logprior: -6.4181e-01
Epoch 8/10
15/15 - 4s - loss: 279.7456 - loglik: -2.7915e+02 - logprior: -5.9480e-01
Fitted a model with MAP estimate = -279.1716
Time for alignment: 111.0160
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.2603 - loglik: -4.3995e+02 - logprior: -5.3121e+00
Epoch 2/10
15/15 - 3s - loss: 365.7977 - loglik: -3.6453e+02 - logprior: -1.2653e+00
Epoch 3/10
15/15 - 3s - loss: 320.3951 - loglik: -3.1887e+02 - logprior: -1.5299e+00
Epoch 4/10
15/15 - 3s - loss: 309.1133 - loglik: -3.0744e+02 - logprior: -1.6696e+00
Epoch 5/10
15/15 - 3s - loss: 304.3538 - loglik: -3.0284e+02 - logprior: -1.5109e+00
Epoch 6/10
15/15 - 3s - loss: 303.8220 - loglik: -3.0230e+02 - logprior: -1.5184e+00
Epoch 7/10
15/15 - 3s - loss: 301.2780 - loglik: -2.9978e+02 - logprior: -1.4993e+00
Epoch 8/10
15/15 - 3s - loss: 302.4262 - loglik: -3.0095e+02 - logprior: -1.4786e+00
Fitted a model with MAP estimate = -301.5253
expansions: [(5, 1), (7, 2), (10, 1), (14, 1), (24, 2), (25, 1), (26, 1), (60, 1), (65, 3), (91, 2), (92, 2), (106, 2), (112, 1), (114, 1), (115, 1), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 304.9981 - loglik: -2.9844e+02 - logprior: -6.5596e+00
Epoch 2/2
15/15 - 4s - loss: 288.2710 - loglik: -2.8525e+02 - logprior: -3.0195e+00
Fitted a model with MAP estimate = -286.4456
expansions: [(0, 2)]
discards: [  0   7 106 122]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 289.6726 - loglik: -2.8477e+02 - logprior: -4.9062e+00
Epoch 2/2
15/15 - 4s - loss: 283.9060 - loglik: -2.8253e+02 - logprior: -1.3795e+00
Fitted a model with MAP estimate = -282.6308
expansions: [(75, 1)]
discards: [ 0 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 294.0057 - loglik: -2.8763e+02 - logprior: -6.3779e+00
Epoch 2/10
15/15 - 4s - loss: 287.4089 - loglik: -2.8541e+02 - logprior: -2.0036e+00
Epoch 3/10
15/15 - 4s - loss: 285.1996 - loglik: -2.8429e+02 - logprior: -9.1357e-01
Epoch 4/10
15/15 - 4s - loss: 283.0211 - loglik: -2.8227e+02 - logprior: -7.5066e-01
Epoch 5/10
15/15 - 4s - loss: 282.4164 - loglik: -2.8170e+02 - logprior: -7.2006e-01
Epoch 6/10
15/15 - 4s - loss: 281.4721 - loglik: -2.8080e+02 - logprior: -6.6977e-01
Epoch 7/10
15/15 - 4s - loss: 281.8764 - loglik: -2.8125e+02 - logprior: -6.2389e-01
Fitted a model with MAP estimate = -281.1992
Time for alignment: 108.2804
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.1456 - loglik: -4.3982e+02 - logprior: -5.3257e+00
Epoch 2/10
15/15 - 3s - loss: 366.2047 - loglik: -3.6493e+02 - logprior: -1.2714e+00
Epoch 3/10
15/15 - 3s - loss: 321.8904 - loglik: -3.2039e+02 - logprior: -1.5001e+00
Epoch 4/10
15/15 - 3s - loss: 307.4640 - loglik: -3.0585e+02 - logprior: -1.6176e+00
Epoch 5/10
15/15 - 3s - loss: 302.2604 - loglik: -3.0075e+02 - logprior: -1.5132e+00
Epoch 6/10
15/15 - 3s - loss: 301.3468 - loglik: -2.9984e+02 - logprior: -1.5064e+00
Epoch 7/10
15/15 - 3s - loss: 301.1450 - loglik: -2.9968e+02 - logprior: -1.4652e+00
Epoch 8/10
15/15 - 3s - loss: 299.7131 - loglik: -2.9829e+02 - logprior: -1.4227e+00
Epoch 9/10
15/15 - 3s - loss: 299.3748 - loglik: -2.9796e+02 - logprior: -1.4166e+00
Epoch 10/10
15/15 - 3s - loss: 299.4128 - loglik: -2.9801e+02 - logprior: -1.4066e+00
Fitted a model with MAP estimate = -299.2994
expansions: [(7, 3), (12, 1), (16, 1), (24, 2), (25, 1), (49, 2), (55, 1), (59, 1), (66, 3), (69, 1), (91, 1), (92, 2), (105, 2), (111, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 303.2520 - loglik: -2.9670e+02 - logprior: -6.5516e+00
Epoch 2/2
15/15 - 4s - loss: 287.0436 - loglik: -2.8402e+02 - logprior: -3.0221e+00
Fitted a model with MAP estimate = -284.2550
expansions: [(0, 2)]
discards: [  0   7  57  83 108 124]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 288.5284 - loglik: -2.8362e+02 - logprior: -4.9047e+00
Epoch 2/2
15/15 - 4s - loss: 282.0781 - loglik: -2.8069e+02 - logprior: -1.3897e+00
Fitted a model with MAP estimate = -281.2150
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.1004 - loglik: -2.8371e+02 - logprior: -6.3929e+00
Epoch 2/10
15/15 - 4s - loss: 283.2263 - loglik: -2.8125e+02 - logprior: -1.9778e+00
Epoch 3/10
15/15 - 4s - loss: 282.1097 - loglik: -2.8119e+02 - logprior: -9.1637e-01
Epoch 4/10
15/15 - 4s - loss: 279.6696 - loglik: -2.7891e+02 - logprior: -7.5714e-01
Epoch 5/10
15/15 - 4s - loss: 279.7672 - loglik: -2.7905e+02 - logprior: -7.1763e-01
Fitted a model with MAP estimate = -278.9953
Time for alignment: 105.1886
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.4414 - loglik: -4.4013e+02 - logprior: -5.3132e+00
Epoch 2/10
15/15 - 3s - loss: 363.2255 - loglik: -3.6197e+02 - logprior: -1.2546e+00
Epoch 3/10
15/15 - 3s - loss: 321.0766 - loglik: -3.1960e+02 - logprior: -1.4721e+00
Epoch 4/10
15/15 - 3s - loss: 309.6051 - loglik: -3.0801e+02 - logprior: -1.5904e+00
Epoch 5/10
15/15 - 3s - loss: 303.7752 - loglik: -3.0231e+02 - logprior: -1.4632e+00
Epoch 6/10
15/15 - 3s - loss: 301.0909 - loglik: -2.9961e+02 - logprior: -1.4826e+00
Epoch 7/10
15/15 - 3s - loss: 298.7422 - loglik: -2.9725e+02 - logprior: -1.4878e+00
Epoch 8/10
15/15 - 3s - loss: 298.0957 - loglik: -2.9664e+02 - logprior: -1.4600e+00
Epoch 9/10
15/15 - 3s - loss: 297.6770 - loglik: -2.9623e+02 - logprior: -1.4515e+00
Epoch 10/10
15/15 - 3s - loss: 297.2430 - loglik: -2.9580e+02 - logprior: -1.4477e+00
Fitted a model with MAP estimate = -297.5271
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (25, 1), (26, 2), (27, 1), (56, 2), (60, 1), (66, 2), (69, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 2), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 302.0757 - loglik: -2.9549e+02 - logprior: -6.5839e+00
Epoch 2/2
15/15 - 4s - loss: 286.9622 - loglik: -2.8388e+02 - logprior: -3.0865e+00
Fitted a model with MAP estimate = -284.4658
expansions: [(0, 2)]
discards: [  0   7  32  65 108 139]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 288.6739 - loglik: -2.8375e+02 - logprior: -4.9262e+00
Epoch 2/2
15/15 - 4s - loss: 283.0633 - loglik: -2.8168e+02 - logprior: -1.3808e+00
Fitted a model with MAP estimate = -282.2723
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 289.9096 - loglik: -2.8352e+02 - logprior: -6.3896e+00
Epoch 2/10
15/15 - 4s - loss: 284.5535 - loglik: -2.8258e+02 - logprior: -1.9704e+00
Epoch 3/10
15/15 - 4s - loss: 281.7960 - loglik: -2.8091e+02 - logprior: -8.8897e-01
Epoch 4/10
15/15 - 4s - loss: 280.4174 - loglik: -2.7968e+02 - logprior: -7.3268e-01
Epoch 5/10
15/15 - 4s - loss: 279.0975 - loglik: -2.7839e+02 - logprior: -7.0252e-01
Epoch 6/10
15/15 - 4s - loss: 279.1484 - loglik: -2.7848e+02 - logprior: -6.7196e-01
Fitted a model with MAP estimate = -278.2365
Time for alignment: 111.5038
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 444.8132 - loglik: -4.3950e+02 - logprior: -5.3144e+00
Epoch 2/10
15/15 - 3s - loss: 364.8983 - loglik: -3.6364e+02 - logprior: -1.2604e+00
Epoch 3/10
15/15 - 3s - loss: 319.4860 - loglik: -3.1801e+02 - logprior: -1.4725e+00
Epoch 4/10
15/15 - 3s - loss: 307.1101 - loglik: -3.0550e+02 - logprior: -1.6135e+00
Epoch 5/10
15/15 - 3s - loss: 303.3197 - loglik: -3.0186e+02 - logprior: -1.4580e+00
Epoch 6/10
15/15 - 3s - loss: 300.9998 - loglik: -2.9954e+02 - logprior: -1.4642e+00
Epoch 7/10
15/15 - 3s - loss: 298.7322 - loglik: -2.9726e+02 - logprior: -1.4698e+00
Epoch 8/10
15/15 - 3s - loss: 299.6490 - loglik: -2.9820e+02 - logprior: -1.4513e+00
Fitted a model with MAP estimate = -297.9158
expansions: [(7, 3), (10, 1), (16, 1), (25, 1), (26, 2), (27, 1), (56, 1), (60, 1), (65, 2), (66, 2), (69, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 2), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 301.1595 - loglik: -2.9455e+02 - logprior: -6.6088e+00
Epoch 2/2
15/15 - 4s - loss: 285.3684 - loglik: -2.8226e+02 - logprior: -3.1118e+00
Fitted a model with MAP estimate = -282.7182
expansions: [(0, 2)]
discards: [  0   7  32  66  67  77 109 140]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 289.8429 - loglik: -2.8492e+02 - logprior: -4.9267e+00
Epoch 2/2
15/15 - 4s - loss: 284.4239 - loglik: -2.8303e+02 - logprior: -1.3930e+00
Fitted a model with MAP estimate = -283.3169
expansions: [(65, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 291.0821 - loglik: -2.8468e+02 - logprior: -6.4029e+00
Epoch 2/10
15/15 - 4s - loss: 282.7849 - loglik: -2.8070e+02 - logprior: -2.0823e+00
Epoch 3/10
15/15 - 4s - loss: 281.1684 - loglik: -2.8027e+02 - logprior: -9.0088e-01
Epoch 4/10
15/15 - 4s - loss: 279.6235 - loglik: -2.7889e+02 - logprior: -7.3573e-01
Epoch 5/10
15/15 - 4s - loss: 278.8217 - loglik: -2.7813e+02 - logprior: -6.9261e-01
Epoch 6/10
15/15 - 4s - loss: 278.3667 - loglik: -2.7771e+02 - logprior: -6.6131e-01
Epoch 7/10
15/15 - 4s - loss: 277.0910 - loglik: -2.7647e+02 - logprior: -6.1900e-01
Epoch 8/10
15/15 - 4s - loss: 277.7265 - loglik: -2.7714e+02 - logprior: -5.8530e-01
Fitted a model with MAP estimate = -277.1606
Time for alignment: 111.5975
Computed alignments with likelihoods: ['-279.1716', '-281.1992', '-278.9953', '-278.2365', '-277.1606']
Best model has likelihood: -277.1606  (prior= -0.5822 )
time for generating output: 0.2147
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9055183946488294
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 300.2090 - loglik: -2.9737e+02 - logprior: -2.8419e+00
Epoch 2/10
34/34 - 4s - loss: 210.3868 - loglik: -2.0853e+02 - logprior: -1.8587e+00
Epoch 3/10
34/34 - 4s - loss: 201.6192 - loglik: -1.9980e+02 - logprior: -1.8144e+00
Epoch 4/10
34/34 - 4s - loss: 201.6006 - loglik: -1.9983e+02 - logprior: -1.7660e+00
Epoch 5/10
34/34 - 4s - loss: 200.2731 - loglik: -1.9852e+02 - logprior: -1.7575e+00
Epoch 6/10
34/34 - 4s - loss: 201.2920 - loglik: -1.9953e+02 - logprior: -1.7592e+00
Fitted a model with MAP estimate = -200.3547
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (60, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 188.7959 - loglik: -1.8583e+02 - logprior: -2.9631e+00
Epoch 2/2
34/34 - 5s - loss: 178.8239 - loglik: -1.7761e+02 - logprior: -1.2093e+00
Fitted a model with MAP estimate = -177.8535
expansions: []
discards: [ 34 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 182.0332 - loglik: -1.7939e+02 - logprior: -2.6424e+00
Epoch 2/2
34/34 - 5s - loss: 179.0458 - loglik: -1.7796e+02 - logprior: -1.0871e+00
Fitted a model with MAP estimate = -178.1183
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 181.3579 - loglik: -1.7885e+02 - logprior: -2.5088e+00
Epoch 2/10
34/34 - 5s - loss: 178.5902 - loglik: -1.7765e+02 - logprior: -9.4445e-01
Epoch 3/10
34/34 - 5s - loss: 178.2002 - loglik: -1.7737e+02 - logprior: -8.3212e-01
Epoch 4/10
34/34 - 5s - loss: 177.4286 - loglik: -1.7668e+02 - logprior: -7.4491e-01
Epoch 5/10
34/34 - 5s - loss: 176.7651 - loglik: -1.7611e+02 - logprior: -6.5247e-01
Epoch 6/10
34/34 - 5s - loss: 177.0150 - loglik: -1.7643e+02 - logprior: -5.8539e-01
Fitted a model with MAP estimate = -176.6662
Time for alignment: 114.9371
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 300.0671 - loglik: -2.9721e+02 - logprior: -2.8572e+00
Epoch 2/10
34/34 - 5s - loss: 210.8830 - loglik: -2.0903e+02 - logprior: -1.8562e+00
Epoch 3/10
34/34 - 5s - loss: 204.0114 - loglik: -2.0221e+02 - logprior: -1.8009e+00
Epoch 4/10
34/34 - 4s - loss: 200.9764 - loglik: -1.9921e+02 - logprior: -1.7675e+00
Epoch 5/10
34/34 - 4s - loss: 200.2204 - loglik: -1.9846e+02 - logprior: -1.7575e+00
Epoch 6/10
34/34 - 4s - loss: 200.2870 - loglik: -1.9853e+02 - logprior: -1.7549e+00
Fitted a model with MAP estimate = -200.2937
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (29, 1), (40, 1), (43, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 188.5779 - loglik: -1.8558e+02 - logprior: -2.9952e+00
Epoch 2/2
34/34 - 5s - loss: 178.7597 - loglik: -1.7764e+02 - logprior: -1.1212e+00
Fitted a model with MAP estimate = -177.2360
expansions: []
discards: [ 20 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 181.3057 - loglik: -1.7868e+02 - logprior: -2.6271e+00
Epoch 2/2
34/34 - 5s - loss: 177.3099 - loglik: -1.7622e+02 - logprior: -1.0852e+00
Fitted a model with MAP estimate = -176.9652
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 180.4706 - loglik: -1.7795e+02 - logprior: -2.5168e+00
Epoch 2/10
34/34 - 5s - loss: 177.5783 - loglik: -1.7664e+02 - logprior: -9.3621e-01
Epoch 3/10
34/34 - 5s - loss: 176.4081 - loglik: -1.7558e+02 - logprior: -8.2652e-01
Epoch 4/10
34/34 - 5s - loss: 176.6515 - loglik: -1.7591e+02 - logprior: -7.3805e-01
Fitted a model with MAP estimate = -175.7852
Time for alignment: 105.6134
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 300.2250 - loglik: -2.9736e+02 - logprior: -2.8612e+00
Epoch 2/10
34/34 - 4s - loss: 208.9356 - loglik: -2.0711e+02 - logprior: -1.8228e+00
Epoch 3/10
34/34 - 5s - loss: 202.2571 - loglik: -2.0050e+02 - logprior: -1.7592e+00
Epoch 4/10
34/34 - 5s - loss: 201.0218 - loglik: -1.9929e+02 - logprior: -1.7302e+00
Epoch 5/10
34/34 - 4s - loss: 200.6065 - loglik: -1.9889e+02 - logprior: -1.7197e+00
Epoch 6/10
34/34 - 4s - loss: 201.0133 - loglik: -1.9930e+02 - logprior: -1.7167e+00
Fitted a model with MAP estimate = -200.3808
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (78, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 188.0901 - loglik: -1.8507e+02 - logprior: -3.0153e+00
Epoch 2/2
34/34 - 5s - loss: 178.1630 - loglik: -1.7689e+02 - logprior: -1.2721e+00
Fitted a model with MAP estimate = -176.6064
expansions: []
discards: [ 20  35 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 180.5613 - loglik: -1.7793e+02 - logprior: -2.6333e+00
Epoch 2/2
34/34 - 5s - loss: 178.0631 - loglik: -1.7697e+02 - logprior: -1.0918e+00
Fitted a model with MAP estimate = -176.8419
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 180.4952 - loglik: -1.7798e+02 - logprior: -2.5148e+00
Epoch 2/10
34/34 - 5s - loss: 177.4250 - loglik: -1.7648e+02 - logprior: -9.4311e-01
Epoch 3/10
34/34 - 5s - loss: 176.7728 - loglik: -1.7595e+02 - logprior: -8.2513e-01
Epoch 4/10
34/34 - 5s - loss: 175.8643 - loglik: -1.7513e+02 - logprior: -7.3701e-01
Epoch 5/10
34/34 - 5s - loss: 175.0160 - loglik: -1.7435e+02 - logprior: -6.6836e-01
Epoch 6/10
34/34 - 5s - loss: 176.4736 - loglik: -1.7590e+02 - logprior: -5.7246e-01
Fitted a model with MAP estimate = -175.4497
Time for alignment: 117.6257
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 302.0610 - loglik: -2.9921e+02 - logprior: -2.8500e+00
Epoch 2/10
34/34 - 4s - loss: 212.2206 - loglik: -2.1036e+02 - logprior: -1.8558e+00
Epoch 3/10
34/34 - 5s - loss: 203.3308 - loglik: -2.0145e+02 - logprior: -1.8769e+00
Epoch 4/10
34/34 - 4s - loss: 202.4279 - loglik: -2.0060e+02 - logprior: -1.8309e+00
Epoch 5/10
34/34 - 4s - loss: 200.9358 - loglik: -1.9910e+02 - logprior: -1.8315e+00
Epoch 6/10
34/34 - 4s - loss: 201.1080 - loglik: -1.9929e+02 - logprior: -1.8229e+00
Fitted a model with MAP estimate = -200.7416
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 188.6078 - loglik: -1.8558e+02 - logprior: -3.0252e+00
Epoch 2/2
34/34 - 5s - loss: 177.8028 - loglik: -1.7653e+02 - logprior: -1.2704e+00
Fitted a model with MAP estimate = -176.6535
expansions: []
discards: [ 20 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 180.0606 - loglik: -1.7743e+02 - logprior: -2.6313e+00
Epoch 2/2
34/34 - 5s - loss: 177.3827 - loglik: -1.7630e+02 - logprior: -1.0823e+00
Fitted a model with MAP estimate = -176.3729
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 179.3242 - loglik: -1.7682e+02 - logprior: -2.5039e+00
Epoch 2/10
34/34 - 5s - loss: 178.0591 - loglik: -1.7712e+02 - logprior: -9.4014e-01
Epoch 3/10
34/34 - 5s - loss: 176.5017 - loglik: -1.7568e+02 - logprior: -8.1789e-01
Epoch 4/10
34/34 - 5s - loss: 174.9843 - loglik: -1.7425e+02 - logprior: -7.3300e-01
Epoch 5/10
34/34 - 5s - loss: 175.0118 - loglik: -1.7436e+02 - logprior: -6.4917e-01
Fitted a model with MAP estimate = -175.1464
Time for alignment: 109.9362
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 301.0290 - loglik: -2.9818e+02 - logprior: -2.8456e+00
Epoch 2/10
34/34 - 5s - loss: 209.3442 - loglik: -2.0747e+02 - logprior: -1.8710e+00
Epoch 3/10
34/34 - 4s - loss: 201.3517 - loglik: -1.9954e+02 - logprior: -1.8130e+00
Epoch 4/10
34/34 - 5s - loss: 199.3055 - loglik: -1.9753e+02 - logprior: -1.7773e+00
Epoch 5/10
34/34 - 5s - loss: 200.9088 - loglik: -1.9914e+02 - logprior: -1.7715e+00
Fitted a model with MAP estimate = -199.6873
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (70, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 187.5260 - loglik: -1.8455e+02 - logprior: -2.9762e+00
Epoch 2/2
34/34 - 5s - loss: 176.3535 - loglik: -1.7508e+02 - logprior: -1.2718e+00
Fitted a model with MAP estimate = -176.0358
expansions: []
discards: [ 34 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 7s - loss: 180.0266 - loglik: -1.7739e+02 - logprior: -2.6333e+00
Epoch 2/2
34/34 - 5s - loss: 177.4957 - loglik: -1.7640e+02 - logprior: -1.0959e+00
Fitted a model with MAP estimate = -176.2416
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 179.7820 - loglik: -1.7727e+02 - logprior: -2.5169e+00
Epoch 2/10
34/34 - 5s - loss: 176.3801 - loglik: -1.7543e+02 - logprior: -9.5146e-01
Epoch 3/10
34/34 - 5s - loss: 176.3965 - loglik: -1.7556e+02 - logprior: -8.3445e-01
Fitted a model with MAP estimate = -175.5065
Time for alignment: 95.8600
Computed alignments with likelihoods: ['-176.6662', '-175.7852', '-175.4497', '-175.1464', '-175.5065']
Best model has likelihood: -175.1464  (prior= -0.6174 )
time for generating output: 0.2623
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6261742792355037
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.9993 - loglik: -3.9930e+02 - logprior: -2.0701e+01
Epoch 2/10
10/10 - 2s - loss: 363.8220 - loglik: -3.5909e+02 - logprior: -4.7367e+00
Epoch 3/10
10/10 - 2s - loss: 323.4240 - loglik: -3.2087e+02 - logprior: -2.5561e+00
Epoch 4/10
10/10 - 2s - loss: 294.1031 - loglik: -2.9185e+02 - logprior: -2.2487e+00
Epoch 5/10
10/10 - 2s - loss: 282.3903 - loglik: -2.8019e+02 - logprior: -2.2027e+00
Epoch 6/10
10/10 - 2s - loss: 276.4562 - loglik: -2.7434e+02 - logprior: -2.1148e+00
Epoch 7/10
10/10 - 2s - loss: 274.4623 - loglik: -2.7243e+02 - logprior: -2.0351e+00
Epoch 8/10
10/10 - 2s - loss: 273.8636 - loglik: -2.7194e+02 - logprior: -1.9196e+00
Epoch 9/10
10/10 - 2s - loss: 272.9310 - loglik: -2.7112e+02 - logprior: -1.8148e+00
Epoch 10/10
10/10 - 2s - loss: 273.2727 - loglik: -2.7149e+02 - logprior: -1.7783e+00
Fitted a model with MAP estimate = -272.7508
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (71, 1), (78, 1), (79, 1), (80, 2), (81, 2), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.1213 - loglik: -2.7017e+02 - logprior: -1.8948e+01
Epoch 2/2
10/10 - 2s - loss: 263.5400 - loglik: -2.5891e+02 - logprior: -4.6329e+00
Fitted a model with MAP estimate = -259.1154
expansions: []
discards: [ 0 96]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 282.5972 - loglik: -2.5912e+02 - logprior: -2.3480e+01
Epoch 2/2
10/10 - 2s - loss: 265.7538 - loglik: -2.5648e+02 - logprior: -9.2783e+00
Fitted a model with MAP estimate = -262.6979
expansions: [(0, 5), (117, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.8321 - loglik: -2.5627e+02 - logprior: -1.8566e+01
Epoch 2/10
10/10 - 2s - loss: 256.2151 - loglik: -2.5201e+02 - logprior: -4.2097e+00
Epoch 3/10
10/10 - 2s - loss: 252.2701 - loglik: -2.5088e+02 - logprior: -1.3944e+00
Epoch 4/10
10/10 - 2s - loss: 251.1928 - loglik: -2.5081e+02 - logprior: -3.8501e-01
Epoch 5/10
10/10 - 2s - loss: 250.0530 - loglik: -2.5012e+02 - logprior: 0.0704
Epoch 6/10
10/10 - 2s - loss: 248.4386 - loglik: -2.4880e+02 - logprior: 0.3577
Epoch 7/10
10/10 - 2s - loss: 248.6004 - loglik: -2.4920e+02 - logprior: 0.5952
Fitted a model with MAP estimate = -248.1980
Time for alignment: 58.7789
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.8167 - loglik: -3.9912e+02 - logprior: -2.0701e+01
Epoch 2/10
10/10 - 2s - loss: 363.8802 - loglik: -3.5915e+02 - logprior: -4.7343e+00
Epoch 3/10
10/10 - 2s - loss: 319.3627 - loglik: -3.1685e+02 - logprior: -2.5129e+00
Epoch 4/10
10/10 - 2s - loss: 291.1741 - loglik: -2.8900e+02 - logprior: -2.1702e+00
Epoch 5/10
10/10 - 2s - loss: 280.3664 - loglik: -2.7819e+02 - logprior: -2.1798e+00
Epoch 6/10
10/10 - 2s - loss: 277.2307 - loglik: -2.7517e+02 - logprior: -2.0577e+00
Epoch 7/10
10/10 - 2s - loss: 274.8337 - loglik: -2.7291e+02 - logprior: -1.9187e+00
Epoch 8/10
10/10 - 2s - loss: 274.8094 - loglik: -2.7300e+02 - logprior: -1.8051e+00
Epoch 9/10
10/10 - 2s - loss: 274.1299 - loglik: -2.7239e+02 - logprior: -1.7370e+00
Epoch 10/10
10/10 - 2s - loss: 272.8685 - loglik: -2.7113e+02 - logprior: -1.7375e+00
Fitted a model with MAP estimate = -273.0098
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.3293 - loglik: -2.7045e+02 - logprior: -1.8883e+01
Epoch 2/2
10/10 - 2s - loss: 263.8313 - loglik: -2.5923e+02 - logprior: -4.5995e+00
Fitted a model with MAP estimate = -259.3477
expansions: [(130, 1)]
discards: [  0   7   8 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 285.6083 - loglik: -2.6226e+02 - logprior: -2.3353e+01
Epoch 2/2
10/10 - 2s - loss: 268.3185 - loglik: -2.5911e+02 - logprior: -9.2102e+00
Fitted a model with MAP estimate = -265.0160
expansions: [(0, 5)]
discards: [  0 116]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 278.0294 - loglik: -2.5959e+02 - logprior: -1.8440e+01
Epoch 2/10
10/10 - 2s - loss: 262.8493 - loglik: -2.5878e+02 - logprior: -4.0706e+00
Epoch 3/10
10/10 - 2s - loss: 257.9674 - loglik: -2.5672e+02 - logprior: -1.2488e+00
Epoch 4/10
10/10 - 2s - loss: 256.9035 - loglik: -2.5667e+02 - logprior: -2.2900e-01
Epoch 5/10
10/10 - 2s - loss: 254.7202 - loglik: -2.5494e+02 - logprior: 0.2240
Epoch 6/10
10/10 - 2s - loss: 254.7840 - loglik: -2.5528e+02 - logprior: 0.4916
Fitted a model with MAP estimate = -254.1583
Time for alignment: 55.8381
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.9080 - loglik: -3.9920e+02 - logprior: -2.0704e+01
Epoch 2/10
10/10 - 2s - loss: 364.2659 - loglik: -3.5952e+02 - logprior: -4.7453e+00
Epoch 3/10
10/10 - 2s - loss: 322.8257 - loglik: -3.2026e+02 - logprior: -2.5666e+00
Epoch 4/10
10/10 - 2s - loss: 294.0927 - loglik: -2.9197e+02 - logprior: -2.1277e+00
Epoch 5/10
10/10 - 2s - loss: 284.0605 - loglik: -2.8208e+02 - logprior: -1.9838e+00
Epoch 6/10
10/10 - 2s - loss: 279.8763 - loglik: -2.7814e+02 - logprior: -1.7389e+00
Epoch 7/10
10/10 - 2s - loss: 278.2262 - loglik: -2.7666e+02 - logprior: -1.5680e+00
Epoch 8/10
10/10 - 2s - loss: 277.4090 - loglik: -2.7592e+02 - logprior: -1.4878e+00
Epoch 9/10
10/10 - 2s - loss: 275.9995 - loglik: -2.7452e+02 - logprior: -1.4826e+00
Epoch 10/10
10/10 - 2s - loss: 276.2608 - loglik: -2.7478e+02 - logprior: -1.4765e+00
Fitted a model with MAP estimate = -276.0269
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 3), (37, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 2), (87, 1), (102, 1), (104, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 291.6807 - loglik: -2.7280e+02 - logprior: -1.8879e+01
Epoch 2/2
10/10 - 2s - loss: 267.5937 - loglik: -2.6308e+02 - logprior: -4.5116e+00
Fitted a model with MAP estimate = -263.2352
expansions: []
discards: [  0 103]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 286.4279 - loglik: -2.6304e+02 - logprior: -2.3391e+01
Epoch 2/2
10/10 - 2s - loss: 270.9348 - loglik: -2.6177e+02 - logprior: -9.1670e+00
Fitted a model with MAP estimate = -267.4228
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 279.0611 - loglik: -2.6061e+02 - logprior: -1.8454e+01
Epoch 2/10
10/10 - 2s - loss: 264.0706 - loglik: -2.6000e+02 - logprior: -4.0661e+00
Epoch 3/10
10/10 - 2s - loss: 259.8636 - loglik: -2.5862e+02 - logprior: -1.2414e+00
Epoch 4/10
10/10 - 2s - loss: 257.8763 - loglik: -2.5763e+02 - logprior: -2.4683e-01
Epoch 5/10
10/10 - 2s - loss: 256.3227 - loglik: -2.5652e+02 - logprior: 0.1932
Epoch 6/10
10/10 - 2s - loss: 255.7411 - loglik: -2.5622e+02 - logprior: 0.4828
Epoch 7/10
10/10 - 2s - loss: 255.7872 - loglik: -2.5651e+02 - logprior: 0.7205
Fitted a model with MAP estimate = -255.2959
Time for alignment: 56.7026
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 419.8734 - loglik: -3.9917e+02 - logprior: -2.0702e+01
Epoch 2/10
10/10 - 2s - loss: 364.4073 - loglik: -3.5966e+02 - logprior: -4.7504e+00
Epoch 3/10
10/10 - 2s - loss: 323.0833 - loglik: -3.2054e+02 - logprior: -2.5411e+00
Epoch 4/10
10/10 - 2s - loss: 295.3587 - loglik: -2.9323e+02 - logprior: -2.1322e+00
Epoch 5/10
10/10 - 2s - loss: 285.5035 - loglik: -2.8352e+02 - logprior: -1.9797e+00
Epoch 6/10
10/10 - 2s - loss: 282.0413 - loglik: -2.8033e+02 - logprior: -1.7114e+00
Epoch 7/10
10/10 - 2s - loss: 280.4666 - loglik: -2.7893e+02 - logprior: -1.5405e+00
Epoch 8/10
10/10 - 2s - loss: 278.7581 - loglik: -2.7731e+02 - logprior: -1.4432e+00
Epoch 9/10
10/10 - 2s - loss: 279.1941 - loglik: -2.7780e+02 - logprior: -1.3905e+00
Fitted a model with MAP estimate = -278.6246
expansions: [(8, 1), (9, 1), (14, 1), (17, 2), (23, 2), (24, 3), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (87, 1), (93, 1), (102, 1), (104, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 294.3561 - loglik: -2.7539e+02 - logprior: -1.8961e+01
Epoch 2/2
10/10 - 2s - loss: 270.0721 - loglik: -2.6543e+02 - logprior: -4.6406e+00
Fitted a model with MAP estimate = -265.4949
expansions: []
discards: [  0  29 104]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.3946 - loglik: -2.6592e+02 - logprior: -2.3472e+01
Epoch 2/2
10/10 - 2s - loss: 272.0838 - loglik: -2.6284e+02 - logprior: -9.2468e+00
Fitted a model with MAP estimate = -269.2150
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 281.4792 - loglik: -2.6298e+02 - logprior: -1.8502e+01
Epoch 2/10
10/10 - 2s - loss: 264.3210 - loglik: -2.6020e+02 - logprior: -4.1162e+00
Epoch 3/10
10/10 - 2s - loss: 261.1502 - loglik: -2.5986e+02 - logprior: -1.2905e+00
Epoch 4/10
10/10 - 2s - loss: 258.7907 - loglik: -2.5848e+02 - logprior: -3.0576e-01
Epoch 5/10
10/10 - 2s - loss: 257.9942 - loglik: -2.5814e+02 - logprior: 0.1481
Epoch 6/10
10/10 - 2s - loss: 256.9951 - loglik: -2.5742e+02 - logprior: 0.4294
Epoch 7/10
10/10 - 2s - loss: 257.2562 - loglik: -2.5791e+02 - logprior: 0.6562
Fitted a model with MAP estimate = -256.3219
Time for alignment: 56.0981
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.9716 - loglik: -3.9927e+02 - logprior: -2.0703e+01
Epoch 2/10
10/10 - 2s - loss: 364.4123 - loglik: -3.5967e+02 - logprior: -4.7425e+00
Epoch 3/10
10/10 - 2s - loss: 324.1865 - loglik: -3.2169e+02 - logprior: -2.4958e+00
Epoch 4/10
10/10 - 2s - loss: 298.4077 - loglik: -2.9652e+02 - logprior: -1.8834e+00
Epoch 5/10
10/10 - 2s - loss: 285.3392 - loglik: -2.8362e+02 - logprior: -1.7178e+00
Epoch 6/10
10/10 - 2s - loss: 281.3257 - loglik: -2.7966e+02 - logprior: -1.6615e+00
Epoch 7/10
10/10 - 2s - loss: 279.4427 - loglik: -2.7789e+02 - logprior: -1.5538e+00
Epoch 8/10
10/10 - 2s - loss: 277.8511 - loglik: -2.7641e+02 - logprior: -1.4395e+00
Epoch 9/10
10/10 - 2s - loss: 277.2440 - loglik: -2.7587e+02 - logprior: -1.3702e+00
Epoch 10/10
10/10 - 2s - loss: 277.1292 - loglik: -2.7573e+02 - logprior: -1.3985e+00
Fitted a model with MAP estimate = -276.6336
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 3), (29, 2), (53, 1), (55, 2), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 1), (94, 1), (97, 4), (98, 1), (102, 1), (104, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 291.5683 - loglik: -2.7276e+02 - logprior: -1.8812e+01
Epoch 2/2
10/10 - 2s - loss: 265.8034 - loglik: -2.6129e+02 - logprior: -4.5139e+00
Fitted a model with MAP estimate = -260.2653
expansions: []
discards: [  0  38  67 121 122 123]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 286.2530 - loglik: -2.6288e+02 - logprior: -2.3369e+01
Epoch 2/2
10/10 - 2s - loss: 270.2828 - loglik: -2.6104e+02 - logprior: -9.2412e+00
Fitted a model with MAP estimate = -266.5717
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.2556 - loglik: -2.5975e+02 - logprior: -1.8507e+01
Epoch 2/10
10/10 - 2s - loss: 263.1626 - loglik: -2.5905e+02 - logprior: -4.1142e+00
Epoch 3/10
10/10 - 2s - loss: 258.6425 - loglik: -2.5738e+02 - logprior: -1.2629e+00
Epoch 4/10
10/10 - 2s - loss: 257.1663 - loglik: -2.5691e+02 - logprior: -2.5912e-01
Epoch 5/10
10/10 - 2s - loss: 255.3455 - loglik: -2.5552e+02 - logprior: 0.1743
Epoch 6/10
10/10 - 2s - loss: 255.1633 - loglik: -2.5563e+02 - logprior: 0.4626
Epoch 7/10
10/10 - 2s - loss: 254.7881 - loglik: -2.5549e+02 - logprior: 0.7016
Epoch 8/10
10/10 - 2s - loss: 254.3825 - loglik: -2.5524e+02 - logprior: 0.8612
Epoch 9/10
10/10 - 2s - loss: 253.8576 - loglik: -2.5484e+02 - logprior: 0.9775
Epoch 10/10
10/10 - 2s - loss: 254.6491 - loglik: -2.5571e+02 - logprior: 1.0658
Fitted a model with MAP estimate = -253.9747
Time for alignment: 62.5341
Computed alignments with likelihoods: ['-248.1980', '-254.1583', '-255.2959', '-256.3219', '-253.9747']
Best model has likelihood: -248.1980  (prior= 0.7097 )
time for generating output: 0.1671
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8369565217391305
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.1059 - loglik: -2.3280e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 231.3943 - loglik: -2.2185e+02 - logprior: -9.5469e+00
Epoch 3/10
10/10 - 1s - loss: 214.3894 - loglik: -2.0999e+02 - logprior: -4.4009e+00
Epoch 4/10
10/10 - 1s - loss: 202.6718 - loglik: -1.9987e+02 - logprior: -2.7996e+00
Epoch 5/10
10/10 - 1s - loss: 196.8355 - loglik: -1.9448e+02 - logprior: -2.3538e+00
Epoch 6/10
10/10 - 1s - loss: 193.9381 - loglik: -1.9214e+02 - logprior: -1.7947e+00
Epoch 7/10
10/10 - 1s - loss: 192.6086 - loglik: -1.9145e+02 - logprior: -1.1599e+00
Epoch 8/10
10/10 - 1s - loss: 191.8963 - loglik: -1.9092e+02 - logprior: -9.7317e-01
Epoch 9/10
10/10 - 1s - loss: 191.2002 - loglik: -1.9031e+02 - logprior: -8.8597e-01
Epoch 10/10
10/10 - 1s - loss: 190.8036 - loglik: -1.9003e+02 - logprior: -7.7311e-01
Fitted a model with MAP estimate = -190.5759
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 1), (31, 2), (34, 1), (45, 2), (49, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.2524 - loglik: -1.9461e+02 - logprior: -4.8646e+01
Epoch 2/2
10/10 - 1s - loss: 201.2613 - loglik: -1.8720e+02 - logprior: -1.4058e+01
Fitted a model with MAP estimate = -194.0142
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.6022 - loglik: -1.8685e+02 - logprior: -4.1753e+01
Epoch 2/2
10/10 - 1s - loss: 202.1846 - loglik: -1.8629e+02 - logprior: -1.5891e+01
Fitted a model with MAP estimate = -197.5251
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.9595 - loglik: -1.8567e+02 - logprior: -3.7292e+01
Epoch 2/10
10/10 - 1s - loss: 194.3795 - loglik: -1.8507e+02 - logprior: -9.3119e+00
Epoch 3/10
10/10 - 1s - loss: 188.0543 - loglik: -1.8493e+02 - logprior: -3.1239e+00
Epoch 4/10
10/10 - 1s - loss: 185.5440 - loglik: -1.8464e+02 - logprior: -9.0309e-01
Epoch 5/10
10/10 - 1s - loss: 184.0154 - loglik: -1.8416e+02 - logprior: 0.1457
Epoch 6/10
10/10 - 1s - loss: 183.3033 - loglik: -1.8394e+02 - logprior: 0.6386
Epoch 7/10
10/10 - 1s - loss: 182.8230 - loglik: -1.8368e+02 - logprior: 0.8581
Epoch 8/10
10/10 - 1s - loss: 182.6357 - loglik: -1.8372e+02 - logprior: 1.0793
Epoch 9/10
10/10 - 1s - loss: 182.2916 - loglik: -1.8366e+02 - logprior: 1.3655
Epoch 10/10
10/10 - 1s - loss: 182.0907 - loglik: -1.8366e+02 - logprior: 1.5711
Fitted a model with MAP estimate = -181.9891
Time for alignment: 35.8026
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.2018 - loglik: -2.3290e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 230.6236 - loglik: -2.2108e+02 - logprior: -9.5426e+00
Epoch 3/10
10/10 - 1s - loss: 212.4977 - loglik: -2.0817e+02 - logprior: -4.3250e+00
Epoch 4/10
10/10 - 1s - loss: 200.3092 - loglik: -1.9761e+02 - logprior: -2.6960e+00
Epoch 5/10
10/10 - 1s - loss: 195.2425 - loglik: -1.9300e+02 - logprior: -2.2378e+00
Epoch 6/10
10/10 - 1s - loss: 192.9597 - loglik: -1.9147e+02 - logprior: -1.4880e+00
Epoch 7/10
10/10 - 1s - loss: 191.7273 - loglik: -1.9086e+02 - logprior: -8.7127e-01
Epoch 8/10
10/10 - 1s - loss: 191.1176 - loglik: -1.9042e+02 - logprior: -6.9880e-01
Epoch 9/10
10/10 - 1s - loss: 190.7840 - loglik: -1.9026e+02 - logprior: -5.2523e-01
Epoch 10/10
10/10 - 1s - loss: 190.5456 - loglik: -1.9020e+02 - logprior: -3.4295e-01
Fitted a model with MAP estimate = -190.4318
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (40, 1), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 242.9422 - loglik: -1.9403e+02 - logprior: -4.8907e+01
Epoch 2/2
10/10 - 1s - loss: 202.5938 - loglik: -1.8823e+02 - logprior: -1.4365e+01
Fitted a model with MAP estimate = -195.1565
expansions: [(38, 1), (50, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.7321 - loglik: -1.8767e+02 - logprior: -4.2063e+01
Epoch 2/2
10/10 - 1s - loss: 202.2207 - loglik: -1.8622e+02 - logprior: -1.6002e+01
Fitted a model with MAP estimate = -197.6821
expansions: [(0, 2), (36, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.1608 - loglik: -1.8593e+02 - logprior: -3.7234e+01
Epoch 2/10
10/10 - 1s - loss: 194.2927 - loglik: -1.8498e+02 - logprior: -9.3124e+00
Epoch 3/10
10/10 - 1s - loss: 187.8124 - loglik: -1.8464e+02 - logprior: -3.1751e+00
Epoch 4/10
10/10 - 1s - loss: 185.3507 - loglik: -1.8440e+02 - logprior: -9.4733e-01
Epoch 5/10
10/10 - 1s - loss: 183.7896 - loglik: -1.8392e+02 - logprior: 0.1260
Epoch 6/10
10/10 - 1s - loss: 183.3087 - loglik: -1.8392e+02 - logprior: 0.6154
Epoch 7/10
10/10 - 1s - loss: 182.6516 - loglik: -1.8346e+02 - logprior: 0.8091
Epoch 8/10
10/10 - 1s - loss: 182.5282 - loglik: -1.8357e+02 - logprior: 1.0384
Epoch 9/10
10/10 - 1s - loss: 182.1339 - loglik: -1.8348e+02 - logprior: 1.3431
Epoch 10/10
10/10 - 1s - loss: 181.9243 - loglik: -1.8348e+02 - logprior: 1.5509
Fitted a model with MAP estimate = -181.8683
Time for alignment: 36.4584
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 270.1661 - loglik: -2.3286e+02 - logprior: -3.7307e+01
Epoch 2/10
10/10 - 1s - loss: 231.0608 - loglik: -2.2151e+02 - logprior: -9.5473e+00
Epoch 3/10
10/10 - 1s - loss: 213.6071 - loglik: -2.0927e+02 - logprior: -4.3405e+00
Epoch 4/10
10/10 - 1s - loss: 202.3761 - loglik: -1.9983e+02 - logprior: -2.5504e+00
Epoch 5/10
10/10 - 1s - loss: 197.5615 - loglik: -1.9563e+02 - logprior: -1.9355e+00
Epoch 6/10
10/10 - 1s - loss: 195.2809 - loglik: -1.9393e+02 - logprior: -1.3465e+00
Epoch 7/10
10/10 - 1s - loss: 193.4628 - loglik: -1.9272e+02 - logprior: -7.4655e-01
Epoch 8/10
10/10 - 1s - loss: 192.7176 - loglik: -1.9217e+02 - logprior: -5.5024e-01
Epoch 9/10
10/10 - 1s - loss: 192.1119 - loglik: -1.9171e+02 - logprior: -4.0082e-01
Epoch 10/10
10/10 - 1s - loss: 191.8306 - loglik: -1.9165e+02 - logprior: -1.8415e-01
Fitted a model with MAP estimate = -191.6938
expansions: [(0, 4), (6, 1), (25, 2), (27, 1), (30, 3), (40, 2), (43, 1), (55, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 242.9863 - loglik: -1.9438e+02 - logprior: -4.8606e+01
Epoch 2/2
10/10 - 1s - loss: 202.0675 - loglik: -1.8797e+02 - logprior: -1.4100e+01
Fitted a model with MAP estimate = -194.6620
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.3479 - loglik: -1.8743e+02 - logprior: -4.1917e+01
Epoch 2/2
10/10 - 1s - loss: 202.4560 - loglik: -1.8649e+02 - logprior: -1.5966e+01
Fitted a model with MAP estimate = -197.9187
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.5115 - loglik: -1.8618e+02 - logprior: -3.7334e+01
Epoch 2/10
10/10 - 1s - loss: 194.6600 - loglik: -1.8530e+02 - logprior: -9.3578e+00
Epoch 3/10
10/10 - 1s - loss: 188.6051 - loglik: -1.8542e+02 - logprior: -3.1899e+00
Epoch 4/10
10/10 - 1s - loss: 185.8841 - loglik: -1.8492e+02 - logprior: -9.6568e-01
Epoch 5/10
10/10 - 1s - loss: 184.6856 - loglik: -1.8478e+02 - logprior: 0.0950
Epoch 6/10
10/10 - 1s - loss: 183.7198 - loglik: -1.8431e+02 - logprior: 0.5859
Epoch 7/10
10/10 - 1s - loss: 183.2721 - loglik: -1.8406e+02 - logprior: 0.7882
Epoch 8/10
10/10 - 1s - loss: 183.0499 - loglik: -1.8407e+02 - logprior: 1.0235
Epoch 9/10
10/10 - 1s - loss: 182.8618 - loglik: -1.8419e+02 - logprior: 1.3310
Epoch 10/10
10/10 - 1s - loss: 182.5765 - loglik: -1.8412e+02 - logprior: 1.5401
Fitted a model with MAP estimate = -182.5041
Time for alignment: 36.6363
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 269.9848 - loglik: -2.3268e+02 - logprior: -3.7306e+01
Epoch 2/10
10/10 - 1s - loss: 230.8829 - loglik: -2.2134e+02 - logprior: -9.5474e+00
Epoch 3/10
10/10 - 1s - loss: 213.0648 - loglik: -2.0873e+02 - logprior: -4.3309e+00
Epoch 4/10
10/10 - 1s - loss: 201.0356 - loglik: -1.9834e+02 - logprior: -2.6977e+00
Epoch 5/10
10/10 - 1s - loss: 195.5649 - loglik: -1.9332e+02 - logprior: -2.2475e+00
Epoch 6/10
10/10 - 1s - loss: 193.0077 - loglik: -1.9147e+02 - logprior: -1.5338e+00
Epoch 7/10
10/10 - 1s - loss: 192.0689 - loglik: -1.9119e+02 - logprior: -8.8221e-01
Epoch 8/10
10/10 - 1s - loss: 191.2130 - loglik: -1.9054e+02 - logprior: -6.7622e-01
Epoch 9/10
10/10 - 1s - loss: 190.9868 - loglik: -1.9045e+02 - logprior: -5.3309e-01
Epoch 10/10
10/10 - 1s - loss: 190.8849 - loglik: -1.9053e+02 - logprior: -3.5667e-01
Fitted a model with MAP estimate = -190.6499
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 1), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 242.6809 - loglik: -1.9389e+02 - logprior: -4.8791e+01
Epoch 2/2
10/10 - 1s - loss: 202.1310 - loglik: -1.8783e+02 - logprior: -1.4302e+01
Fitted a model with MAP estimate = -194.5628
expansions: [(39, 1), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.9418 - loglik: -1.8700e+02 - logprior: -4.1947e+01
Epoch 2/2
10/10 - 1s - loss: 201.9131 - loglik: -1.8596e+02 - logprior: -1.5951e+01
Fitted a model with MAP estimate = -197.2977
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.7974 - loglik: -1.8552e+02 - logprior: -3.7276e+01
Epoch 2/10
10/10 - 1s - loss: 194.0700 - loglik: -1.8478e+02 - logprior: -9.2946e+00
Epoch 3/10
10/10 - 1s - loss: 187.8636 - loglik: -1.8473e+02 - logprior: -3.1363e+00
Epoch 4/10
10/10 - 1s - loss: 185.1810 - loglik: -1.8426e+02 - logprior: -9.1609e-01
Epoch 5/10
10/10 - 1s - loss: 184.0787 - loglik: -1.8422e+02 - logprior: 0.1431
Epoch 6/10
10/10 - 1s - loss: 183.0996 - loglik: -1.8374e+02 - logprior: 0.6421
Epoch 7/10
10/10 - 1s - loss: 182.6454 - loglik: -1.8349e+02 - logprior: 0.8419
Epoch 8/10
10/10 - 1s - loss: 182.5025 - loglik: -1.8357e+02 - logprior: 1.0657
Epoch 9/10
10/10 - 1s - loss: 182.1330 - loglik: -1.8350e+02 - logprior: 1.3629
Epoch 10/10
10/10 - 1s - loss: 182.0039 - loglik: -1.8358e+02 - logprior: 1.5738
Fitted a model with MAP estimate = -181.8661
Time for alignment: 36.8939
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.0157 - loglik: -2.3271e+02 - logprior: -3.7306e+01
Epoch 2/10
10/10 - 1s - loss: 231.2639 - loglik: -2.2171e+02 - logprior: -9.5528e+00
Epoch 3/10
10/10 - 1s - loss: 214.2777 - loglik: -2.0990e+02 - logprior: -4.3775e+00
Epoch 4/10
10/10 - 1s - loss: 202.8641 - loglik: -2.0019e+02 - logprior: -2.6765e+00
Epoch 5/10
10/10 - 1s - loss: 196.8950 - loglik: -1.9464e+02 - logprior: -2.2541e+00
Epoch 6/10
10/10 - 1s - loss: 193.9281 - loglik: -1.9220e+02 - logprior: -1.7253e+00
Epoch 7/10
10/10 - 1s - loss: 192.1825 - loglik: -1.9106e+02 - logprior: -1.1265e+00
Epoch 8/10
10/10 - 1s - loss: 191.4626 - loglik: -1.9052e+02 - logprior: -9.4568e-01
Epoch 9/10
10/10 - 1s - loss: 190.9322 - loglik: -1.9017e+02 - logprior: -7.5903e-01
Epoch 10/10
10/10 - 1s - loss: 190.8045 - loglik: -1.9023e+02 - logprior: -5.7073e-01
Fitted a model with MAP estimate = -190.5472
expansions: [(0, 4), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (43, 2), (45, 1), (48, 1), (55, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.1444 - loglik: -1.9539e+02 - logprior: -4.8754e+01
Epoch 2/2
10/10 - 1s - loss: 202.1038 - loglik: -1.8782e+02 - logprior: -1.4282e+01
Fitted a model with MAP estimate = -194.7588
expansions: [(39, 1)]
discards: [ 0  1 53]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 229.3003 - loglik: -1.8734e+02 - logprior: -4.1963e+01
Epoch 2/2
10/10 - 1s - loss: 202.2029 - loglik: -1.8624e+02 - logprior: -1.5964e+01
Fitted a model with MAP estimate = -197.7692
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.9966 - loglik: -1.8572e+02 - logprior: -3.7277e+01
Epoch 2/10
10/10 - 1s - loss: 194.3173 - loglik: -1.8506e+02 - logprior: -9.2585e+00
Epoch 3/10
10/10 - 1s - loss: 187.7025 - loglik: -1.8462e+02 - logprior: -3.0796e+00
Epoch 4/10
10/10 - 1s - loss: 185.1206 - loglik: -1.8427e+02 - logprior: -8.5353e-01
Epoch 5/10
10/10 - 1s - loss: 183.8226 - loglik: -1.8406e+02 - logprior: 0.2330
Epoch 6/10
10/10 - 1s - loss: 183.0754 - loglik: -1.8383e+02 - logprior: 0.7586
Epoch 7/10
10/10 - 1s - loss: 182.5199 - loglik: -1.8350e+02 - logprior: 0.9829
Epoch 8/10
10/10 - 1s - loss: 182.2076 - loglik: -1.8341e+02 - logprior: 1.2051
Epoch 9/10
10/10 - 1s - loss: 182.1458 - loglik: -1.8361e+02 - logprior: 1.4656
Epoch 10/10
10/10 - 1s - loss: 181.6953 - loglik: -1.8335e+02 - logprior: 1.6534
Fitted a model with MAP estimate = -181.7879
Time for alignment: 35.8350
Computed alignments with likelihoods: ['-181.9891', '-181.8683', '-182.5041', '-181.8661', '-181.7879']
Best model has likelihood: -181.7879  (prior= 1.7174 )
time for generating output: 0.1649
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.745691906005222
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 954.3761 - loglik: -9.5315e+02 - logprior: -1.2273e+00
Epoch 2/10
43/43 - 27s - loss: 838.7362 - loglik: -8.3704e+02 - logprior: -1.7011e+00
Epoch 3/10
43/43 - 27s - loss: 826.4976 - loglik: -8.2473e+02 - logprior: -1.7705e+00
Epoch 4/10
43/43 - 27s - loss: 822.0338 - loglik: -8.2026e+02 - logprior: -1.7732e+00
Epoch 5/10
43/43 - 27s - loss: 820.7502 - loglik: -8.1892e+02 - logprior: -1.8341e+00
Epoch 6/10
43/43 - 27s - loss: 817.4957 - loglik: -8.1565e+02 - logprior: -1.8465e+00
Epoch 7/10
43/43 - 27s - loss: 817.1933 - loglik: -8.1539e+02 - logprior: -1.8029e+00
Epoch 8/10
43/43 - 27s - loss: 817.7018 - loglik: -8.1590e+02 - logprior: -1.8012e+00
Fitted a model with MAP estimate = -828.0785
expansions: [(7, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 3), (25, 1), (29, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (50, 2), (56, 1), (59, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 2), (98, 1), (103, 2), (119, 1), (120, 1), (121, 1), (124, 1), (130, 1), (134, 1), (144, 1), (147, 1), (149, 1), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (183, 1), (184, 2), (186, 1), (187, 1), (188, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (240, 1), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (261, 1), (269, 2), (270, 1), (271, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 811.8997 - loglik: -8.0985e+02 - logprior: -2.0498e+00
Epoch 2/2
43/43 - 41s - loss: 795.8934 - loglik: -7.9501e+02 - logprior: -8.8754e-01
Fitted a model with MAP estimate = -793.7446
expansions: [(0, 2), (314, 2)]
discards: [  0  32  59  67 121 128 137 206 238 265 266 267 293 298]
Re-initialized the encoder parameters.
Fitting a model of length 356 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 798.5391 - loglik: -7.9746e+02 - logprior: -1.0817e+00
Epoch 2/2
43/43 - 40s - loss: 794.3046 - loglik: -7.9396e+02 - logprior: -3.4417e-01
Fitted a model with MAP estimate = -793.7102
expansions: []
discards: [  0   1 302 343]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 778.9370 - loglik: -7.7807e+02 - logprior: -8.7089e-01
Epoch 2/10
61/61 - 54s - loss: 775.0190 - loglik: -7.7470e+02 - logprior: -3.2153e-01
Epoch 3/10
61/61 - 54s - loss: 771.9753 - loglik: -7.7173e+02 - logprior: -2.4318e-01
Epoch 4/10
61/61 - 54s - loss: 767.7686 - loglik: -7.6759e+02 - logprior: -1.8170e-01
Epoch 5/10
61/61 - 54s - loss: 766.7304 - loglik: -7.6660e+02 - logprior: -1.3453e-01
Epoch 6/10
61/61 - 54s - loss: 764.0112 - loglik: -7.6390e+02 - logprior: -1.1550e-01
Epoch 7/10
61/61 - 55s - loss: 764.0921 - loglik: -7.6406e+02 - logprior: -2.9038e-02
Fitted a model with MAP estimate = -764.2729
Time for alignment: 1064.2574
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 952.3383 - loglik: -9.5107e+02 - logprior: -1.2682e+00
Epoch 2/10
43/43 - 27s - loss: 834.8063 - loglik: -8.3307e+02 - logprior: -1.7313e+00
Epoch 3/10
43/43 - 27s - loss: 825.4930 - loglik: -8.2375e+02 - logprior: -1.7407e+00
Epoch 4/10
43/43 - 27s - loss: 823.0455 - loglik: -8.2129e+02 - logprior: -1.7558e+00
Epoch 5/10
43/43 - 27s - loss: 819.1808 - loglik: -8.1737e+02 - logprior: -1.8149e+00
Epoch 6/10
43/43 - 27s - loss: 816.8363 - loglik: -8.1508e+02 - logprior: -1.7558e+00
Epoch 7/10
43/43 - 27s - loss: 818.4273 - loglik: -8.1664e+02 - logprior: -1.7919e+00
Fitted a model with MAP estimate = -826.0428
expansions: [(8, 1), (13, 1), (16, 1), (21, 3), (22, 2), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (78, 1), (80, 1), (81, 1), (82, 2), (86, 2), (88, 1), (91, 1), (96, 1), (98, 1), (99, 1), (104, 2), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (133, 2), (142, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 2), (188, 1), (205, 1), (206, 3), (207, 2), (208, 1), (209, 2), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 813.1710 - loglik: -8.1105e+02 - logprior: -2.1163e+00
Epoch 2/2
43/43 - 43s - loss: 795.6323 - loglik: -7.9467e+02 - logprior: -9.6546e-01
Fitted a model with MAP estimate = -795.2219
expansions: [(0, 2)]
discards: [  0  26  28 105 111 136 172 198 205 239 242 266 267 268 275 295 300 356
 358 361]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 799.2272 - loglik: -7.9813e+02 - logprior: -1.0932e+00
Epoch 2/2
43/43 - 40s - loss: 795.1718 - loglik: -7.9478e+02 - logprior: -3.9081e-01
Fitted a model with MAP estimate = -795.3010
expansions: [(282, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 779.4083 - loglik: -7.7854e+02 - logprior: -8.6384e-01
Epoch 2/10
61/61 - 54s - loss: 774.0682 - loglik: -7.7376e+02 - logprior: -3.0505e-01
Epoch 3/10
61/61 - 54s - loss: 771.2628 - loglik: -7.7100e+02 - logprior: -2.6149e-01
Epoch 4/10
61/61 - 54s - loss: 767.8395 - loglik: -7.6764e+02 - logprior: -2.0053e-01
Epoch 5/10
61/61 - 54s - loss: 766.1144 - loglik: -7.6597e+02 - logprior: -1.4415e-01
Epoch 6/10
61/61 - 54s - loss: 762.9164 - loglik: -7.6278e+02 - logprior: -1.3328e-01
Epoch 7/10
61/61 - 54s - loss: 764.5717 - loglik: -7.6449e+02 - logprior: -8.3821e-02
Fitted a model with MAP estimate = -764.0734
Time for alignment: 1036.4247
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 952.9984 - loglik: -9.5176e+02 - logprior: -1.2411e+00
Epoch 2/10
43/43 - 27s - loss: 839.0504 - loglik: -8.3736e+02 - logprior: -1.6861e+00
Epoch 3/10
43/43 - 27s - loss: 828.0829 - loglik: -8.2634e+02 - logprior: -1.7389e+00
Epoch 4/10
43/43 - 27s - loss: 823.5255 - loglik: -8.2176e+02 - logprior: -1.7667e+00
Epoch 5/10
43/43 - 27s - loss: 820.3294 - loglik: -8.1855e+02 - logprior: -1.7756e+00
Epoch 6/10
43/43 - 27s - loss: 819.5705 - loglik: -8.1781e+02 - logprior: -1.7559e+00
Epoch 7/10
43/43 - 27s - loss: 818.3391 - loglik: -8.1651e+02 - logprior: -1.8263e+00
Epoch 8/10
43/43 - 27s - loss: 819.6401 - loglik: -8.1782e+02 - logprior: -1.8243e+00
Fitted a model with MAP estimate = -828.7290
expansions: [(7, 2), (16, 1), (17, 1), (20, 1), (21, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (46, 1), (56, 1), (59, 2), (61, 1), (62, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (103, 2), (119, 1), (120, 1), (121, 1), (124, 1), (130, 1), (132, 2), (144, 1), (147, 1), (149, 1), (152, 2), (155, 2), (157, 2), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 2), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (239, 4), (240, 1), (242, 1), (245, 1), (248, 1), (249, 2), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 369 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 816.0919 - loglik: -8.1392e+02 - logprior: -2.1760e+00
Epoch 2/2
43/43 - 42s - loss: 794.8593 - loglik: -7.9379e+02 - logprior: -1.0734e+00
Fitted a model with MAP estimate = -796.9469
expansions: [(0, 2)]
discards: [  0  75 109 135 170 204 243 263 264 265 291 296 313 314 329 356 357 359]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 799.4957 - loglik: -7.9835e+02 - logprior: -1.1484e+00
Epoch 2/2
43/43 - 39s - loss: 796.1749 - loglik: -7.9587e+02 - logprior: -3.0040e-01
Fitted a model with MAP estimate = -799.0540
expansions: [(343, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 779.3542 - loglik: -7.7872e+02 - logprior: -6.3011e-01
Epoch 2/10
61/61 - 55s - loss: 773.8568 - loglik: -7.7346e+02 - logprior: -3.9790e-01
Epoch 3/10
61/61 - 54s - loss: 770.3422 - loglik: -7.7001e+02 - logprior: -3.2802e-01
Epoch 4/10
61/61 - 54s - loss: 768.0638 - loglik: -7.6775e+02 - logprior: -3.0927e-01
Epoch 5/10
61/61 - 55s - loss: 764.7830 - loglik: -7.6453e+02 - logprior: -2.5749e-01
Epoch 6/10
61/61 - 54s - loss: 762.6662 - loglik: -7.6244e+02 - logprior: -2.2401e-01
Epoch 7/10
61/61 - 54s - loss: 763.6971 - loglik: -7.6350e+02 - logprior: -1.9763e-01
Fitted a model with MAP estimate = -763.1646
Time for alignment: 1063.7367
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 952.8692 - loglik: -9.5164e+02 - logprior: -1.2299e+00
Epoch 2/10
43/43 - 27s - loss: 838.8960 - loglik: -8.3727e+02 - logprior: -1.6274e+00
Epoch 3/10
43/43 - 27s - loss: 825.4574 - loglik: -8.2365e+02 - logprior: -1.8118e+00
Epoch 4/10
43/43 - 27s - loss: 822.2063 - loglik: -8.2040e+02 - logprior: -1.8090e+00
Epoch 5/10
43/43 - 27s - loss: 818.5835 - loglik: -8.1676e+02 - logprior: -1.8239e+00
Epoch 6/10
43/43 - 27s - loss: 819.5993 - loglik: -8.1781e+02 - logprior: -1.7878e+00
Fitted a model with MAP estimate = -825.1325
expansions: [(8, 1), (13, 1), (16, 1), (17, 1), (20, 1), (21, 1), (23, 1), (24, 2), (29, 1), (33, 1), (39, 1), (41, 1), (42, 1), (45, 1), (46, 1), (56, 1), (59, 1), (60, 1), (62, 1), (80, 1), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (104, 2), (119, 1), (122, 1), (123, 1), (125, 1), (130, 1), (132, 2), (144, 1), (147, 1), (149, 1), (152, 1), (155, 2), (157, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (244, 1), (245, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 813.1143 - loglik: -8.1101e+02 - logprior: -2.1028e+00
Epoch 2/2
43/43 - 42s - loss: 794.1398 - loglik: -7.9329e+02 - logprior: -8.4861e-01
Fitted a model with MAP estimate = -794.1403
expansions: [(0, 2)]
discards: [  0 103 109 135 170 203 261 262 263 294 310 351 353 356]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 797.7836 - loglik: -7.9666e+02 - logprior: -1.1203e+00
Epoch 2/2
43/43 - 39s - loss: 794.4315 - loglik: -7.9407e+02 - logprior: -3.6355e-01
Fitted a model with MAP estimate = -793.4733
expansions: []
discards: [  0 282]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 778.6802 - loglik: -7.7789e+02 - logprior: -7.9436e-01
Epoch 2/10
61/61 - 55s - loss: 773.7410 - loglik: -7.7338e+02 - logprior: -3.6507e-01
Epoch 3/10
61/61 - 54s - loss: 770.9398 - loglik: -7.7059e+02 - logprior: -3.5019e-01
Epoch 4/10
61/61 - 54s - loss: 766.5895 - loglik: -7.6629e+02 - logprior: -3.0084e-01
Epoch 5/10
61/61 - 54s - loss: 766.9154 - loglik: -7.6666e+02 - logprior: -2.5185e-01
Fitted a model with MAP estimate = -763.9072
Time for alignment: 896.8480
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 953.1986 - loglik: -9.5197e+02 - logprior: -1.2307e+00
Epoch 2/10
43/43 - 27s - loss: 839.8177 - loglik: -8.3829e+02 - logprior: -1.5262e+00
Epoch 3/10
43/43 - 27s - loss: 827.5576 - loglik: -8.2597e+02 - logprior: -1.5918e+00
Epoch 4/10
43/43 - 27s - loss: 824.2777 - loglik: -8.2272e+02 - logprior: -1.5590e+00
Epoch 5/10
43/43 - 27s - loss: 820.6661 - loglik: -8.1909e+02 - logprior: -1.5718e+00
Epoch 6/10
43/43 - 27s - loss: 819.8483 - loglik: -8.1827e+02 - logprior: -1.5824e+00
Epoch 7/10
43/43 - 27s - loss: 819.5549 - loglik: -8.1793e+02 - logprior: -1.6227e+00
Epoch 8/10
43/43 - 27s - loss: 818.4680 - loglik: -8.1686e+02 - logprior: -1.6033e+00
Epoch 9/10
43/43 - 27s - loss: 819.3500 - loglik: -8.1772e+02 - logprior: -1.6310e+00
Fitted a model with MAP estimate = -832.3470
expansions: [(0, 2), (16, 1), (20, 2), (21, 2), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (46, 1), (57, 1), (59, 2), (61, 1), (62, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (102, 1), (108, 1), (121, 1), (122, 1), (125, 1), (128, 2), (130, 1), (133, 2), (148, 2), (153, 1), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 2), (188, 1), (197, 1), (204, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (221, 1), (222, 1), (223, 2), (225, 1), (226, 2), (236, 1), (240, 1), (242, 1), (245, 1), (250, 1), (257, 1), (260, 3), (269, 3), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 368 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 815.2087 - loglik: -8.1328e+02 - logprior: -1.9279e+00
Epoch 2/2
43/43 - 42s - loss: 796.0917 - loglik: -7.9523e+02 - logprior: -8.6624e-01
Fitted a model with MAP estimate = -795.7506
expansions: [(313, 2)]
discards: [  0   1  26  27  78 112 166 174 205 241 297 340 353 355 358]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 798.8365 - loglik: -7.9769e+02 - logprior: -1.1419e+00
Epoch 2/2
43/43 - 40s - loss: 795.6341 - loglik: -7.9541e+02 - logprior: -2.2462e-01
Fitted a model with MAP estimate = -794.7371
expansions: [(0, 2)]
discards: [230 282 302]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 779.5318 - loglik: -7.7878e+02 - logprior: -7.5325e-01
Epoch 2/10
61/61 - 55s - loss: 772.2791 - loglik: -7.7187e+02 - logprior: -4.0478e-01
Epoch 3/10
61/61 - 54s - loss: 769.9991 - loglik: -7.6963e+02 - logprior: -3.7033e-01
Epoch 4/10
61/61 - 55s - loss: 767.0369 - loglik: -7.6675e+02 - logprior: -2.9141e-01
Epoch 5/10
61/61 - 54s - loss: 767.1872 - loglik: -7.6692e+02 - logprior: -2.6768e-01
Fitted a model with MAP estimate = -763.4787
Time for alignment: 985.7438
Computed alignments with likelihoods: ['-764.2729', '-764.0734', '-763.1646', '-763.9072', '-763.4787']
Best model has likelihood: -763.1646  (prior= -0.2141 )
time for generating output: 0.3538
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8316296557396311
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.4220 - loglik: -1.9376e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 187.4137 - loglik: -1.7183e+02 - logprior: -1.5587e+01
Epoch 3/10
10/10 - 1s - loss: 162.2022 - loglik: -1.5462e+02 - logprior: -7.5847e+00
Epoch 4/10
10/10 - 1s - loss: 153.2155 - loglik: -1.4867e+02 - logprior: -4.5500e+00
Epoch 5/10
10/10 - 1s - loss: 149.6433 - loglik: -1.4647e+02 - logprior: -3.1775e+00
Epoch 6/10
10/10 - 1s - loss: 148.3052 - loglik: -1.4598e+02 - logprior: -2.3243e+00
Epoch 7/10
10/10 - 1s - loss: 147.8772 - loglik: -1.4612e+02 - logprior: -1.7531e+00
Epoch 8/10
10/10 - 1s - loss: 146.9638 - loglik: -1.4551e+02 - logprior: -1.4567e+00
Epoch 9/10
10/10 - 1s - loss: 147.1139 - loglik: -1.4582e+02 - logprior: -1.2941e+00
Fitted a model with MAP estimate = -146.9728
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.2354 - loglik: -1.4520e+02 - logprior: -6.4031e+01
Epoch 2/2
10/10 - 1s - loss: 169.5133 - loglik: -1.4261e+02 - logprior: -2.6906e+01
Fitted a model with MAP estimate = -163.7321
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 193.5360 - loglik: -1.4139e+02 - logprior: -5.2146e+01
Epoch 2/2
10/10 - 1s - loss: 155.3615 - loglik: -1.4096e+02 - logprior: -1.4402e+01
Fitted a model with MAP estimate = -150.0594
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.9159 - loglik: -1.4162e+02 - logprior: -6.2292e+01
Epoch 2/10
10/10 - 1s - loss: 163.1227 - loglik: -1.4169e+02 - logprior: -2.1437e+01
Epoch 3/10
10/10 - 1s - loss: 150.4396 - loglik: -1.4197e+02 - logprior: -8.4670e+00
Epoch 4/10
10/10 - 1s - loss: 145.6038 - loglik: -1.4171e+02 - logprior: -3.8963e+00
Epoch 5/10
10/10 - 1s - loss: 144.4161 - loglik: -1.4227e+02 - logprior: -2.1469e+00
Epoch 6/10
10/10 - 1s - loss: 143.0767 - loglik: -1.4182e+02 - logprior: -1.2529e+00
Epoch 7/10
10/10 - 1s - loss: 143.2924 - loglik: -1.4265e+02 - logprior: -6.3858e-01
Fitted a model with MAP estimate = -142.5809
Time for alignment: 37.6401
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 250.2126 - loglik: -1.9355e+02 - logprior: -5.6667e+01
Epoch 2/10
10/10 - 1s - loss: 187.2621 - loglik: -1.7167e+02 - logprior: -1.5595e+01
Epoch 3/10
10/10 - 1s - loss: 162.1101 - loglik: -1.5452e+02 - logprior: -7.5918e+00
Epoch 4/10
10/10 - 1s - loss: 152.6832 - loglik: -1.4812e+02 - logprior: -4.5606e+00
Epoch 5/10
10/10 - 1s - loss: 149.9328 - loglik: -1.4678e+02 - logprior: -3.1536e+00
Epoch 6/10
10/10 - 1s - loss: 148.5434 - loglik: -1.4629e+02 - logprior: -2.2502e+00
Epoch 7/10
10/10 - 1s - loss: 147.5737 - loglik: -1.4592e+02 - logprior: -1.6541e+00
Epoch 8/10
10/10 - 1s - loss: 147.1519 - loglik: -1.4577e+02 - logprior: -1.3837e+00
Epoch 9/10
10/10 - 1s - loss: 148.0705 - loglik: -1.4684e+02 - logprior: -1.2305e+00
Fitted a model with MAP estimate = -147.0510
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.5781 - loglik: -1.4462e+02 - logprior: -6.3954e+01
Epoch 2/2
10/10 - 1s - loss: 170.0699 - loglik: -1.4323e+02 - logprior: -2.6843e+01
Fitted a model with MAP estimate = -163.7742
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.6134 - loglik: -1.4161e+02 - logprior: -5.2007e+01
Epoch 2/2
10/10 - 1s - loss: 155.2040 - loglik: -1.4087e+02 - logprior: -1.4335e+01
Fitted a model with MAP estimate = -150.0411
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 203.5660 - loglik: -1.4136e+02 - logprior: -6.2203e+01
Epoch 2/10
10/10 - 1s - loss: 163.1469 - loglik: -1.4186e+02 - logprior: -2.1291e+01
Epoch 3/10
10/10 - 1s - loss: 150.9808 - loglik: -1.4261e+02 - logprior: -8.3708e+00
Epoch 4/10
10/10 - 1s - loss: 145.2168 - loglik: -1.4137e+02 - logprior: -3.8419e+00
Epoch 5/10
10/10 - 1s - loss: 144.6151 - loglik: -1.4251e+02 - logprior: -2.1086e+00
Epoch 6/10
10/10 - 1s - loss: 143.4766 - loglik: -1.4221e+02 - logprior: -1.2616e+00
Epoch 7/10
10/10 - 1s - loss: 143.0407 - loglik: -1.4236e+02 - logprior: -6.7693e-01
Epoch 8/10
10/10 - 1s - loss: 141.8007 - loglik: -1.4158e+02 - logprior: -2.2391e-01
Epoch 9/10
10/10 - 1s - loss: 142.7605 - loglik: -1.4279e+02 - logprior: 0.0307
Fitted a model with MAP estimate = -142.0869
Time for alignment: 40.4101
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 249.9156 - loglik: -1.9325e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 187.5728 - loglik: -1.7198e+02 - logprior: -1.5591e+01
Epoch 3/10
10/10 - 1s - loss: 162.5235 - loglik: -1.5494e+02 - logprior: -7.5843e+00
Epoch 4/10
10/10 - 1s - loss: 153.5730 - loglik: -1.4903e+02 - logprior: -4.5398e+00
Epoch 5/10
10/10 - 1s - loss: 149.8045 - loglik: -1.4670e+02 - logprior: -3.1049e+00
Epoch 6/10
10/10 - 1s - loss: 148.2918 - loglik: -1.4606e+02 - logprior: -2.2311e+00
Epoch 7/10
10/10 - 1s - loss: 147.6932 - loglik: -1.4603e+02 - logprior: -1.6596e+00
Epoch 8/10
10/10 - 1s - loss: 147.9733 - loglik: -1.4657e+02 - logprior: -1.4016e+00
Fitted a model with MAP estimate = -147.2392
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.3558 - loglik: -1.4440e+02 - logprior: -6.3959e+01
Epoch 2/2
10/10 - 1s - loss: 170.1189 - loglik: -1.4327e+02 - logprior: -2.6847e+01
Fitted a model with MAP estimate = -163.7761
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.2062 - loglik: -1.4124e+02 - logprior: -5.1961e+01
Epoch 2/2
10/10 - 1s - loss: 155.7683 - loglik: -1.4142e+02 - logprior: -1.4352e+01
Fitted a model with MAP estimate = -150.0600
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.5617 - loglik: -1.4128e+02 - logprior: -6.2285e+01
Epoch 2/10
10/10 - 1s - loss: 163.5686 - loglik: -1.4213e+02 - logprior: -2.1434e+01
Epoch 3/10
10/10 - 1s - loss: 150.3109 - loglik: -1.4184e+02 - logprior: -8.4672e+00
Epoch 4/10
10/10 - 1s - loss: 145.8284 - loglik: -1.4194e+02 - logprior: -3.8839e+00
Epoch 5/10
10/10 - 1s - loss: 144.8073 - loglik: -1.4266e+02 - logprior: -2.1464e+00
Epoch 6/10
10/10 - 1s - loss: 143.1322 - loglik: -1.4183e+02 - logprior: -1.2999e+00
Epoch 7/10
10/10 - 1s - loss: 142.7102 - loglik: -1.4200e+02 - logprior: -7.0614e-01
Epoch 8/10
10/10 - 1s - loss: 142.8976 - loglik: -1.4264e+02 - logprior: -2.5931e-01
Fitted a model with MAP estimate = -142.3435
Time for alignment: 37.1105
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.0022 - loglik: -1.9334e+02 - logprior: -5.6667e+01
Epoch 2/10
10/10 - 1s - loss: 187.9581 - loglik: -1.7236e+02 - logprior: -1.5594e+01
Epoch 3/10
10/10 - 1s - loss: 162.1142 - loglik: -1.5453e+02 - logprior: -7.5817e+00
Epoch 4/10
10/10 - 1s - loss: 152.4098 - loglik: -1.4785e+02 - logprior: -4.5624e+00
Epoch 5/10
10/10 - 1s - loss: 149.6121 - loglik: -1.4641e+02 - logprior: -3.2060e+00
Epoch 6/10
10/10 - 1s - loss: 148.2114 - loglik: -1.4589e+02 - logprior: -2.3193e+00
Epoch 7/10
10/10 - 1s - loss: 147.8896 - loglik: -1.4616e+02 - logprior: -1.7312e+00
Epoch 8/10
10/10 - 1s - loss: 147.2755 - loglik: -1.4584e+02 - logprior: -1.4306e+00
Epoch 9/10
10/10 - 1s - loss: 147.1982 - loglik: -1.4593e+02 - logprior: -1.2705e+00
Epoch 10/10
10/10 - 1s - loss: 146.4377 - loglik: -1.4531e+02 - logprior: -1.1306e+00
Fitted a model with MAP estimate = -146.8220
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 208.7636 - loglik: -1.4476e+02 - logprior: -6.4005e+01
Epoch 2/2
10/10 - 1s - loss: 169.7309 - loglik: -1.4284e+02 - logprior: -2.6892e+01
Fitted a model with MAP estimate = -163.7465
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.8130 - loglik: -1.4162e+02 - logprior: -5.2192e+01
Epoch 2/2
10/10 - 1s - loss: 155.3312 - loglik: -1.4095e+02 - logprior: -1.4380e+01
Fitted a model with MAP estimate = -150.0374
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 206.0558 - loglik: -1.4288e+02 - logprior: -6.3176e+01
Epoch 2/10
10/10 - 1s - loss: 167.8656 - loglik: -1.4251e+02 - logprior: -2.5357e+01
Epoch 3/10
10/10 - 1s - loss: 155.1873 - loglik: -1.4247e+02 - logprior: -1.2720e+01
Epoch 4/10
10/10 - 1s - loss: 146.8002 - loglik: -1.4208e+02 - logprior: -4.7207e+00
Epoch 5/10
10/10 - 1s - loss: 144.9025 - loglik: -1.4265e+02 - logprior: -2.2507e+00
Epoch 6/10
10/10 - 1s - loss: 144.8368 - loglik: -1.4355e+02 - logprior: -1.2860e+00
Epoch 7/10
10/10 - 1s - loss: 143.1129 - loglik: -1.4246e+02 - logprior: -6.4794e-01
Epoch 8/10
10/10 - 1s - loss: 142.0252 - loglik: -1.4179e+02 - logprior: -2.3262e-01
Epoch 9/10
10/10 - 1s - loss: 143.4110 - loglik: -1.4337e+02 - logprior: -3.7138e-02
Fitted a model with MAP estimate = -142.6212
Time for alignment: 39.5709
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 250.4796 - loglik: -1.9382e+02 - logprior: -5.6663e+01
Epoch 2/10
10/10 - 1s - loss: 186.8695 - loglik: -1.7128e+02 - logprior: -1.5587e+01
Epoch 3/10
10/10 - 1s - loss: 162.8053 - loglik: -1.5520e+02 - logprior: -7.6032e+00
Epoch 4/10
10/10 - 1s - loss: 153.2198 - loglik: -1.4872e+02 - logprior: -4.5041e+00
Epoch 5/10
10/10 - 1s - loss: 150.4801 - loglik: -1.4752e+02 - logprior: -2.9626e+00
Epoch 6/10
10/10 - 1s - loss: 149.8664 - loglik: -1.4785e+02 - logprior: -2.0160e+00
Epoch 7/10
10/10 - 1s - loss: 148.9476 - loglik: -1.4750e+02 - logprior: -1.4507e+00
Epoch 8/10
10/10 - 1s - loss: 147.7023 - loglik: -1.4652e+02 - logprior: -1.1842e+00
Epoch 9/10
10/10 - 1s - loss: 147.9934 - loglik: -1.4697e+02 - logprior: -1.0271e+00
Fitted a model with MAP estimate = -147.9996
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.5194 - loglik: -1.4732e+02 - logprior: -6.4202e+01
Epoch 2/2
10/10 - 1s - loss: 173.6995 - loglik: -1.4665e+02 - logprior: -2.7050e+01
Fitted a model with MAP estimate = -168.1608
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.3335 - loglik: -1.4490e+02 - logprior: -5.2428e+01
Epoch 2/2
10/10 - 1s - loss: 160.5367 - loglik: -1.4601e+02 - logprior: -1.4529e+01
Fitted a model with MAP estimate = -154.7236
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 45 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 210.6671 - loglik: -1.4741e+02 - logprior: -6.3260e+01
Epoch 2/10
10/10 - 1s - loss: 172.4184 - loglik: -1.4721e+02 - logprior: -2.5211e+01
Epoch 3/10
10/10 - 1s - loss: 159.5505 - loglik: -1.4733e+02 - logprior: -1.2217e+01
Epoch 4/10
10/10 - 1s - loss: 151.4589 - loglik: -1.4676e+02 - logprior: -4.6995e+00
Epoch 5/10
10/10 - 1s - loss: 149.5445 - loglik: -1.4714e+02 - logprior: -2.4059e+00
Epoch 6/10
10/10 - 1s - loss: 148.8636 - loglik: -1.4738e+02 - logprior: -1.4883e+00
Epoch 7/10
10/10 - 1s - loss: 148.3690 - loglik: -1.4749e+02 - logprior: -8.8094e-01
Epoch 8/10
10/10 - 1s - loss: 147.8195 - loglik: -1.4736e+02 - logprior: -4.6321e-01
Epoch 9/10
10/10 - 1s - loss: 148.3331 - loglik: -1.4810e+02 - logprior: -2.3444e-01
Fitted a model with MAP estimate = -147.5782
Time for alignment: 38.8231
Computed alignments with likelihoods: ['-142.5809', '-142.0869', '-142.3435', '-142.6212', '-147.5782']
Best model has likelihood: -142.0869  (prior= 0.1188 )
time for generating output: 0.1118
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9597780859916782
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.6527 - loglik: -4.7191e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 2s - loss: 439.2917 - loglik: -4.2602e+02 - logprior: -1.3276e+01
Epoch 3/10
10/10 - 2s - loss: 386.6455 - loglik: -3.8240e+02 - logprior: -4.2486e+00
Epoch 4/10
10/10 - 2s - loss: 357.7295 - loglik: -3.5643e+02 - logprior: -1.2999e+00
Epoch 5/10
10/10 - 2s - loss: 344.9557 - loglik: -3.4489e+02 - logprior: -6.5260e-02
Epoch 6/10
10/10 - 2s - loss: 338.8542 - loglik: -3.3973e+02 - logprior: 0.8730
Epoch 7/10
10/10 - 2s - loss: 336.3694 - loglik: -3.3794e+02 - logprior: 1.5656
Epoch 8/10
10/10 - 2s - loss: 334.1990 - loglik: -3.3612e+02 - logprior: 1.9254
Epoch 9/10
10/10 - 2s - loss: 332.4268 - loglik: -3.3454e+02 - logprior: 2.1111
Epoch 10/10
10/10 - 2s - loss: 331.4684 - loglik: -3.3379e+02 - logprior: 2.3259
Fitted a model with MAP estimate = -330.8438
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (65, 1), (68, 1), (78, 2), (79, 4), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 407.2881 - loglik: -3.3221e+02 - logprior: -7.5077e+01
Epoch 2/2
10/10 - 3s - loss: 342.5734 - loglik: -3.1520e+02 - logprior: -2.7374e+01
Fitted a model with MAP estimate = -330.4120
expansions: [(0, 2), (75, 1), (128, 1)]
discards: [  0  11  23  44  49  51 114 162]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 369.3912 - loglik: -3.1101e+02 - logprior: -5.8378e+01
Epoch 2/2
10/10 - 3s - loss: 316.1397 - loglik: -3.0579e+02 - logprior: -1.0354e+01
Fitted a model with MAP estimate = -307.1069
expansions: []
discards: [  0 138]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 379.9961 - loglik: -3.0883e+02 - logprior: -7.1171e+01
Epoch 2/10
10/10 - 2s - loss: 321.7747 - loglik: -3.0466e+02 - logprior: -1.7115e+01
Epoch 3/10
10/10 - 2s - loss: 304.0847 - loglik: -3.0258e+02 - logprior: -1.5076e+00
Epoch 4/10
10/10 - 2s - loss: 297.0677 - loglik: -3.0097e+02 - logprior: 3.9025
Epoch 5/10
10/10 - 2s - loss: 294.2424 - loglik: -3.0065e+02 - logprior: 6.4046
Epoch 6/10
10/10 - 2s - loss: 292.6010 - loglik: -3.0048e+02 - logprior: 7.8832
Epoch 7/10
10/10 - 3s - loss: 291.8649 - loglik: -3.0089e+02 - logprior: 9.0296
Epoch 8/10
10/10 - 3s - loss: 290.2286 - loglik: -3.0018e+02 - logprior: 9.9560
Epoch 9/10
10/10 - 3s - loss: 290.1701 - loglik: -3.0084e+02 - logprior: 10.6688
Epoch 10/10
10/10 - 3s - loss: 288.9475 - loglik: -3.0018e+02 - logprior: 11.2335
Fitted a model with MAP estimate = -289.2049
Time for alignment: 71.9541
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.6649 - loglik: -4.7192e+02 - logprior: -6.6741e+01
Epoch 2/10
10/10 - 2s - loss: 439.0272 - loglik: -4.2575e+02 - logprior: -1.3273e+01
Epoch 3/10
10/10 - 2s - loss: 387.1016 - loglik: -3.8277e+02 - logprior: -4.3330e+00
Epoch 4/10
10/10 - 2s - loss: 358.1637 - loglik: -3.5669e+02 - logprior: -1.4688e+00
Epoch 5/10
10/10 - 2s - loss: 344.7709 - loglik: -3.4477e+02 - logprior: 0.0034
Epoch 6/10
10/10 - 2s - loss: 337.5576 - loglik: -3.3825e+02 - logprior: 0.6945
Epoch 7/10
10/10 - 2s - loss: 333.2631 - loglik: -3.3448e+02 - logprior: 1.2198
Epoch 8/10
10/10 - 2s - loss: 331.8980 - loglik: -3.3355e+02 - logprior: 1.6514
Epoch 9/10
10/10 - 2s - loss: 329.7825 - loglik: -3.3178e+02 - logprior: 1.9979
Epoch 10/10
10/10 - 2s - loss: 328.6773 - loglik: -3.3091e+02 - logprior: 2.2290
Fitted a model with MAP estimate = -328.2939
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (39, 1), (40, 2), (51, 1), (59, 1), (65, 1), (68, 1), (78, 3), (79, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 402.3311 - loglik: -3.2742e+02 - logprior: -7.4915e+01
Epoch 2/2
10/10 - 3s - loss: 336.9913 - loglik: -3.1001e+02 - logprior: -2.6977e+01
Fitted a model with MAP estimate = -325.7835
expansions: [(0, 2), (60, 1), (128, 1)]
discards: [  0  22  49 114 162]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 364.9001 - loglik: -3.0680e+02 - logprior: -5.8100e+01
Epoch 2/2
10/10 - 3s - loss: 311.9886 - loglik: -3.0193e+02 - logprior: -1.0058e+01
Fitted a model with MAP estimate = -303.0034
expansions: []
discards: [  0  95 141]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 376.8589 - loglik: -3.0591e+02 - logprior: -7.0951e+01
Epoch 2/10
10/10 - 3s - loss: 319.4798 - loglik: -3.0246e+02 - logprior: -1.7022e+01
Epoch 3/10
10/10 - 3s - loss: 301.3447 - loglik: -2.9995e+02 - logprior: -1.3952e+00
Epoch 4/10
10/10 - 3s - loss: 294.5784 - loglik: -2.9866e+02 - logprior: 4.0823
Epoch 5/10
10/10 - 3s - loss: 292.1904 - loglik: -2.9879e+02 - logprior: 6.5985
Epoch 6/10
10/10 - 3s - loss: 289.9923 - loglik: -2.9807e+02 - logprior: 8.0745
Epoch 7/10
10/10 - 3s - loss: 288.8661 - loglik: -2.9810e+02 - logprior: 9.2300
Epoch 8/10
10/10 - 3s - loss: 288.5914 - loglik: -2.9876e+02 - logprior: 10.1669
Epoch 9/10
10/10 - 3s - loss: 287.7334 - loglik: -2.9863e+02 - logprior: 10.8938
Epoch 10/10
10/10 - 3s - loss: 287.1671 - loglik: -2.9862e+02 - logprior: 11.4545
Fitted a model with MAP estimate = -286.9422
Time for alignment: 72.5635
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.7421 - loglik: -4.7200e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 2s - loss: 439.1167 - loglik: -4.2585e+02 - logprior: -1.3269e+01
Epoch 3/10
10/10 - 2s - loss: 386.4827 - loglik: -3.8219e+02 - logprior: -4.2920e+00
Epoch 4/10
10/10 - 2s - loss: 357.9096 - loglik: -3.5653e+02 - logprior: -1.3795e+00
Epoch 5/10
10/10 - 2s - loss: 344.8133 - loglik: -3.4483e+02 - logprior: 0.0160
Epoch 6/10
10/10 - 2s - loss: 338.5995 - loglik: -3.3952e+02 - logprior: 0.9228
Epoch 7/10
10/10 - 2s - loss: 334.4190 - loglik: -3.3595e+02 - logprior: 1.5324
Epoch 8/10
10/10 - 2s - loss: 332.4785 - loglik: -3.3438e+02 - logprior: 1.9063
Epoch 9/10
10/10 - 2s - loss: 331.0532 - loglik: -3.3323e+02 - logprior: 2.1765
Epoch 10/10
10/10 - 2s - loss: 330.7963 - loglik: -3.3323e+02 - logprior: 2.4378
Fitted a model with MAP estimate = -330.0819
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 2), (39, 2), (66, 1), (69, 2), (78, 3), (79, 2), (80, 2), (89, 1), (90, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.9876 - loglik: -3.3186e+02 - logprior: -7.5128e+01
Epoch 2/2
10/10 - 3s - loss: 342.6823 - loglik: -3.1532e+02 - logprior: -2.7363e+01
Fitted a model with MAP estimate = -331.0802
expansions: [(0, 2), (127, 1)]
discards: [ 0 11 23 82 97]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 368.7406 - loglik: -3.1060e+02 - logprior: -5.8143e+01
Epoch 2/2
10/10 - 3s - loss: 315.7447 - loglik: -3.0565e+02 - logprior: -1.0099e+01
Fitted a model with MAP estimate = -307.2421
expansions: []
discards: [  0 138 160]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 380.5676 - loglik: -3.0968e+02 - logprior: -7.0891e+01
Epoch 2/10
10/10 - 2s - loss: 322.1393 - loglik: -3.0535e+02 - logprior: -1.6786e+01
Epoch 3/10
10/10 - 2s - loss: 304.2035 - loglik: -3.0287e+02 - logprior: -1.3316e+00
Epoch 4/10
10/10 - 2s - loss: 297.7174 - loglik: -3.0174e+02 - logprior: 4.0260
Epoch 5/10
10/10 - 2s - loss: 294.7256 - loglik: -3.0123e+02 - logprior: 6.5021
Epoch 6/10
10/10 - 2s - loss: 293.4340 - loglik: -3.0142e+02 - logprior: 7.9812
Epoch 7/10
10/10 - 2s - loss: 292.2190 - loglik: -3.0134e+02 - logprior: 9.1231
Epoch 8/10
10/10 - 2s - loss: 291.4483 - loglik: -3.0150e+02 - logprior: 10.0547
Epoch 9/10
10/10 - 2s - loss: 291.1321 - loglik: -3.0190e+02 - logprior: 10.7672
Epoch 10/10
10/10 - 2s - loss: 290.4688 - loglik: -3.0180e+02 - logprior: 11.3292
Fitted a model with MAP estimate = -290.0835
Time for alignment: 70.6221
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.6099 - loglik: -4.7187e+02 - logprior: -6.6743e+01
Epoch 2/10
10/10 - 2s - loss: 438.6552 - loglik: -4.2539e+02 - logprior: -1.3262e+01
Epoch 3/10
10/10 - 2s - loss: 384.7500 - loglik: -3.8055e+02 - logprior: -4.2015e+00
Epoch 4/10
10/10 - 2s - loss: 354.8874 - loglik: -3.5345e+02 - logprior: -1.4422e+00
Epoch 5/10
10/10 - 2s - loss: 343.0614 - loglik: -3.4294e+02 - logprior: -1.1971e-01
Epoch 6/10
10/10 - 2s - loss: 336.0874 - loglik: -3.3685e+02 - logprior: 0.7652
Epoch 7/10
10/10 - 2s - loss: 332.3188 - loglik: -3.3368e+02 - logprior: 1.3597
Epoch 8/10
10/10 - 2s - loss: 330.5970 - loglik: -3.3235e+02 - logprior: 1.7502
Epoch 9/10
10/10 - 2s - loss: 329.2143 - loglik: -3.3128e+02 - logprior: 2.0628
Epoch 10/10
10/10 - 2s - loss: 329.2599 - loglik: -3.3157e+02 - logprior: 2.3109
Fitted a model with MAP estimate = -328.4031
expansions: [(11, 2), (12, 1), (19, 2), (26, 2), (28, 1), (36, 1), (38, 2), (39, 2), (62, 1), (68, 1), (78, 2), (79, 3), (80, 2), (89, 1), (90, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 403.4085 - loglik: -3.2833e+02 - logprior: -7.5075e+01
Epoch 2/2
10/10 - 3s - loss: 340.1790 - loglik: -3.1317e+02 - logprior: -2.7008e+01
Fitted a model with MAP estimate = -328.5435
expansions: [(0, 2)]
discards: [ 0 22 47]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 367.2498 - loglik: -3.0899e+02 - logprior: -5.8263e+01
Epoch 2/2
10/10 - 3s - loss: 314.8163 - loglik: -3.0458e+02 - logprior: -1.0234e+01
Fitted a model with MAP estimate = -306.5524
expansions: [(62, 1), (70, 1), (124, 1)]
discards: [  0 139 160]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 381.4605 - loglik: -3.0905e+02 - logprior: -7.2407e+01
Epoch 2/10
10/10 - 3s - loss: 326.7413 - loglik: -3.0553e+02 - logprior: -2.1212e+01
Epoch 3/10
10/10 - 3s - loss: 306.8935 - loglik: -3.0329e+02 - logprior: -3.6017e+00
Epoch 4/10
10/10 - 3s - loss: 297.9150 - loglik: -3.0184e+02 - logprior: 3.9273
Epoch 5/10
10/10 - 3s - loss: 294.3293 - loglik: -3.0103e+02 - logprior: 6.7003
Epoch 6/10
10/10 - 3s - loss: 292.8648 - loglik: -3.0109e+02 - logprior: 8.2215
Epoch 7/10
10/10 - 3s - loss: 291.3821 - loglik: -3.0077e+02 - logprior: 9.3894
Epoch 8/10
10/10 - 3s - loss: 291.2214 - loglik: -3.0155e+02 - logprior: 10.3299
Epoch 9/10
10/10 - 3s - loss: 290.0356 - loglik: -3.0109e+02 - logprior: 11.0510
Epoch 10/10
10/10 - 3s - loss: 289.9819 - loglik: -3.0161e+02 - logprior: 11.6315
Fitted a model with MAP estimate = -289.4868
Time for alignment: 72.0080
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 538.5812 - loglik: -4.7184e+02 - logprior: -6.6745e+01
Epoch 2/10
10/10 - 2s - loss: 439.3031 - loglik: -4.2603e+02 - logprior: -1.3269e+01
Epoch 3/10
10/10 - 2s - loss: 386.7087 - loglik: -3.8237e+02 - logprior: -4.3419e+00
Epoch 4/10
10/10 - 2s - loss: 357.5661 - loglik: -3.5617e+02 - logprior: -1.3936e+00
Epoch 5/10
10/10 - 2s - loss: 346.0335 - loglik: -3.4614e+02 - logprior: 0.1071
Epoch 6/10
10/10 - 2s - loss: 337.9219 - loglik: -3.3891e+02 - logprior: 0.9846
Epoch 7/10
10/10 - 2s - loss: 334.7580 - loglik: -3.3642e+02 - logprior: 1.6656
Epoch 8/10
10/10 - 2s - loss: 333.1406 - loglik: -3.3522e+02 - logprior: 2.0795
Epoch 9/10
10/10 - 2s - loss: 331.6958 - loglik: -3.3410e+02 - logprior: 2.4037
Epoch 10/10
10/10 - 2s - loss: 330.6544 - loglik: -3.3332e+02 - logprior: 2.6696
Fitted a model with MAP estimate = -330.5573
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 2), (40, 2), (49, 1), (51, 1), (69, 2), (78, 3), (79, 2), (80, 2), (89, 1), (90, 3), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 407.7382 - loglik: -3.3271e+02 - logprior: -7.5032e+01
Epoch 2/2
10/10 - 3s - loss: 342.9515 - loglik: -3.1550e+02 - logprior: -2.7456e+01
Fitted a model with MAP estimate = -331.9847
expansions: [(0, 2), (79, 1), (129, 1)]
discards: [  0  22  46  49  82  97 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 371.6297 - loglik: -3.1323e+02 - logprior: -5.8403e+01
Epoch 2/2
10/10 - 3s - loss: 317.1830 - loglik: -3.0677e+02 - logprior: -1.0415e+01
Fitted a model with MAP estimate = -307.4364
expansions: []
discards: [  0  59 138 160]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 381.6397 - loglik: -3.1018e+02 - logprior: -7.1460e+01
Epoch 2/10
10/10 - 2s - loss: 323.5164 - loglik: -3.0590e+02 - logprior: -1.7621e+01
Epoch 3/10
10/10 - 2s - loss: 305.1931 - loglik: -3.0331e+02 - logprior: -1.8835e+00
Epoch 4/10
10/10 - 2s - loss: 297.5200 - loglik: -3.0113e+02 - logprior: 3.6128
Epoch 5/10
10/10 - 2s - loss: 294.6985 - loglik: -3.0081e+02 - logprior: 6.1096
Epoch 6/10
10/10 - 2s - loss: 293.0990 - loglik: -3.0069e+02 - logprior: 7.5879
Epoch 7/10
10/10 - 2s - loss: 292.2813 - loglik: -3.0103e+02 - logprior: 8.7452
Epoch 8/10
10/10 - 2s - loss: 291.1064 - loglik: -3.0077e+02 - logprior: 9.6679
Epoch 9/10
10/10 - 2s - loss: 290.7326 - loglik: -3.0111e+02 - logprior: 10.3821
Epoch 10/10
10/10 - 2s - loss: 289.9785 - loglik: -3.0093e+02 - logprior: 10.9532
Fitted a model with MAP estimate = -289.8057
Time for alignment: 72.0320
Computed alignments with likelihoods: ['-289.2049', '-286.9422', '-290.0835', '-289.4868', '-289.8057']
Best model has likelihood: -286.9422  (prior= 11.7478 )
time for generating output: 0.1759
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.8226906385616863
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.3493 - loglik: -2.8243e+02 - logprior: -9.9152e+00
Epoch 2/10
12/12 - 1s - loss: 253.7753 - loglik: -2.5137e+02 - logprior: -2.4031e+00
Epoch 3/10
12/12 - 1s - loss: 226.3184 - loglik: -2.2463e+02 - logprior: -1.6918e+00
Epoch 4/10
12/12 - 1s - loss: 216.0543 - loglik: -2.1433e+02 - logprior: -1.7234e+00
Epoch 5/10
12/12 - 1s - loss: 211.6799 - loglik: -2.0990e+02 - logprior: -1.7759e+00
Epoch 6/10
12/12 - 1s - loss: 207.6096 - loglik: -2.0589e+02 - logprior: -1.7169e+00
Epoch 7/10
12/12 - 1s - loss: 207.2760 - loglik: -2.0559e+02 - logprior: -1.6885e+00
Epoch 8/10
12/12 - 1s - loss: 206.3463 - loglik: -2.0464e+02 - logprior: -1.7065e+00
Epoch 9/10
12/12 - 1s - loss: 205.0314 - loglik: -2.0332e+02 - logprior: -1.7109e+00
Epoch 10/10
12/12 - 1s - loss: 205.3998 - loglik: -2.0369e+02 - logprior: -1.7068e+00
Fitted a model with MAP estimate = -205.2093
expansions: [(6, 3), (10, 4), (13, 1), (21, 1), (49, 1), (50, 3), (52, 1), (57, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 218.2442 - loglik: -2.0674e+02 - logprior: -1.1505e+01
Epoch 2/2
12/12 - 1s - loss: 200.7124 - loglik: -1.9596e+02 - logprior: -4.7529e+00
Fitted a model with MAP estimate = -197.6287
expansions: [(0, 4)]
discards: [ 0 72 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 204.0076 - loglik: -1.9489e+02 - logprior: -9.1203e+00
Epoch 2/2
12/12 - 1s - loss: 193.7719 - loglik: -1.9148e+02 - logprior: -2.2943e+00
Fitted a model with MAP estimate = -191.6889
expansions: [(47, 3)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 204.7425 - loglik: -1.9349e+02 - logprior: -1.1256e+01
Epoch 2/10
12/12 - 1s - loss: 194.3837 - loglik: -1.9033e+02 - logprior: -4.0548e+00
Epoch 3/10
12/12 - 1s - loss: 189.9061 - loglik: -1.8804e+02 - logprior: -1.8666e+00
Epoch 4/10
12/12 - 1s - loss: 188.1491 - loglik: -1.8704e+02 - logprior: -1.1098e+00
Epoch 5/10
12/12 - 1s - loss: 186.3363 - loglik: -1.8537e+02 - logprior: -9.6621e-01
Epoch 6/10
12/12 - 1s - loss: 184.2421 - loglik: -1.8336e+02 - logprior: -8.7770e-01
Epoch 7/10
12/12 - 1s - loss: 183.8363 - loglik: -1.8296e+02 - logprior: -8.7590e-01
Epoch 8/10
12/12 - 1s - loss: 183.3584 - loglik: -1.8249e+02 - logprior: -8.6905e-01
Epoch 9/10
12/12 - 1s - loss: 182.4813 - loglik: -1.8164e+02 - logprior: -8.4619e-01
Epoch 10/10
12/12 - 1s - loss: 182.1090 - loglik: -1.8129e+02 - logprior: -8.1461e-01
Fitted a model with MAP estimate = -182.3935
Time for alignment: 51.7889
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.6880 - loglik: -2.8277e+02 - logprior: -9.9156e+00
Epoch 2/10
12/12 - 1s - loss: 254.1185 - loglik: -2.5171e+02 - logprior: -2.4054e+00
Epoch 3/10
12/12 - 1s - loss: 227.0748 - loglik: -2.2537e+02 - logprior: -1.7086e+00
Epoch 4/10
12/12 - 1s - loss: 216.3800 - loglik: -2.1462e+02 - logprior: -1.7622e+00
Epoch 5/10
12/12 - 1s - loss: 211.4104 - loglik: -2.0953e+02 - logprior: -1.8782e+00
Epoch 6/10
12/12 - 1s - loss: 207.8276 - loglik: -2.0598e+02 - logprior: -1.8468e+00
Epoch 7/10
12/12 - 1s - loss: 206.4468 - loglik: -2.0466e+02 - logprior: -1.7846e+00
Epoch 8/10
12/12 - 1s - loss: 204.9806 - loglik: -2.0316e+02 - logprior: -1.8168e+00
Epoch 9/10
12/12 - 1s - loss: 205.4133 - loglik: -2.0358e+02 - logprior: -1.8323e+00
Fitted a model with MAP estimate = -204.7452
expansions: [(8, 1), (9, 1), (10, 3), (11, 1), (12, 2), (13, 1), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 219.9893 - loglik: -2.0850e+02 - logprior: -1.1487e+01
Epoch 2/2
12/12 - 1s - loss: 199.8709 - loglik: -1.9503e+02 - logprior: -4.8411e+00
Fitted a model with MAP estimate = -195.7889
expansions: [(0, 4)]
discards: [ 0 12 17 47 62 80]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 203.3101 - loglik: -1.9424e+02 - logprior: -9.0679e+00
Epoch 2/2
12/12 - 1s - loss: 192.8974 - loglik: -1.9054e+02 - logprior: -2.3599e+00
Fitted a model with MAP estimate = -190.8463
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 200.3351 - loglik: -1.9142e+02 - logprior: -8.9161e+00
Epoch 2/10
12/12 - 1s - loss: 191.7311 - loglik: -1.8922e+02 - logprior: -2.5159e+00
Epoch 3/10
12/12 - 1s - loss: 189.5437 - loglik: -1.8799e+02 - logprior: -1.5489e+00
Epoch 4/10
12/12 - 1s - loss: 186.4553 - loglik: -1.8531e+02 - logprior: -1.1449e+00
Epoch 5/10
12/12 - 1s - loss: 185.6618 - loglik: -1.8469e+02 - logprior: -9.6754e-01
Epoch 6/10
12/12 - 1s - loss: 183.2148 - loglik: -1.8230e+02 - logprior: -9.1157e-01
Epoch 7/10
12/12 - 1s - loss: 182.3704 - loglik: -1.8148e+02 - logprior: -8.9507e-01
Epoch 8/10
12/12 - 1s - loss: 182.0457 - loglik: -1.8116e+02 - logprior: -8.8294e-01
Epoch 9/10
12/12 - 1s - loss: 182.1858 - loglik: -1.8134e+02 - logprior: -8.4576e-01
Fitted a model with MAP estimate = -181.4736
Time for alignment: 49.3992
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.2964 - loglik: -2.8238e+02 - logprior: -9.9163e+00
Epoch 2/10
12/12 - 1s - loss: 253.8438 - loglik: -2.5145e+02 - logprior: -2.3967e+00
Epoch 3/10
12/12 - 1s - loss: 226.0635 - loglik: -2.2440e+02 - logprior: -1.6677e+00
Epoch 4/10
12/12 - 1s - loss: 216.0369 - loglik: -2.1437e+02 - logprior: -1.6673e+00
Epoch 5/10
12/12 - 1s - loss: 212.7904 - loglik: -2.1111e+02 - logprior: -1.6831e+00
Epoch 6/10
12/12 - 1s - loss: 208.9549 - loglik: -2.0735e+02 - logprior: -1.6019e+00
Epoch 7/10
12/12 - 1s - loss: 208.4475 - loglik: -2.0689e+02 - logprior: -1.5596e+00
Epoch 8/10
12/12 - 1s - loss: 207.4761 - loglik: -2.0590e+02 - logprior: -1.5790e+00
Epoch 9/10
12/12 - 1s - loss: 207.3046 - loglik: -2.0572e+02 - logprior: -1.5880e+00
Epoch 10/10
12/12 - 1s - loss: 207.6047 - loglik: -2.0603e+02 - logprior: -1.5721e+00
Fitted a model with MAP estimate = -207.0927
expansions: [(6, 3), (10, 3), (11, 2), (21, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 220.9114 - loglik: -2.0946e+02 - logprior: -1.1450e+01
Epoch 2/2
12/12 - 1s - loss: 200.6742 - loglik: -1.9585e+02 - logprior: -4.8246e+00
Fitted a model with MAP estimate = -197.1259
expansions: [(0, 5), (45, 1), (46, 1)]
discards: [ 0 16 61 80]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 202.7624 - loglik: -1.9355e+02 - logprior: -9.2086e+00
Epoch 2/2
12/12 - 2s - loss: 192.1368 - loglik: -1.8975e+02 - logprior: -2.3827e+00
Fitted a model with MAP estimate = -189.8859
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 199.3142 - loglik: -1.9039e+02 - logprior: -8.9278e+00
Epoch 2/10
12/12 - 1s - loss: 190.8363 - loglik: -1.8833e+02 - logprior: -2.5023e+00
Epoch 3/10
12/12 - 1s - loss: 189.1325 - loglik: -1.8755e+02 - logprior: -1.5811e+00
Epoch 4/10
12/12 - 1s - loss: 186.1085 - loglik: -1.8491e+02 - logprior: -1.2016e+00
Epoch 5/10
12/12 - 1s - loss: 185.1919 - loglik: -1.8417e+02 - logprior: -1.0209e+00
Epoch 6/10
12/12 - 1s - loss: 183.8504 - loglik: -1.8289e+02 - logprior: -9.5669e-01
Epoch 7/10
12/12 - 1s - loss: 182.4503 - loglik: -1.8152e+02 - logprior: -9.3299e-01
Epoch 8/10
12/12 - 1s - loss: 182.6271 - loglik: -1.8171e+02 - logprior: -9.1510e-01
Fitted a model with MAP estimate = -181.7636
Time for alignment: 49.1708
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.3131 - loglik: -2.8240e+02 - logprior: -9.9153e+00
Epoch 2/10
12/12 - 1s - loss: 253.7381 - loglik: -2.5133e+02 - logprior: -2.4058e+00
Epoch 3/10
12/12 - 1s - loss: 226.0427 - loglik: -2.2432e+02 - logprior: -1.7195e+00
Epoch 4/10
12/12 - 1s - loss: 215.2407 - loglik: -2.1350e+02 - logprior: -1.7377e+00
Epoch 5/10
12/12 - 1s - loss: 211.5220 - loglik: -2.0976e+02 - logprior: -1.7603e+00
Epoch 6/10
12/12 - 1s - loss: 208.0949 - loglik: -2.0638e+02 - logprior: -1.7143e+00
Epoch 7/10
12/12 - 1s - loss: 208.6566 - loglik: -2.0697e+02 - logprior: -1.6825e+00
Fitted a model with MAP estimate = -207.3344
expansions: [(8, 1), (10, 3), (11, 1), (12, 2), (13, 1), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 221.2032 - loglik: -2.0981e+02 - logprior: -1.1398e+01
Epoch 2/2
12/12 - 1s - loss: 202.5715 - loglik: -1.9781e+02 - logprior: -4.7571e+00
Fitted a model with MAP estimate = -197.9864
expansions: [(0, 3)]
discards: [ 0 16 44 61 79]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 204.3060 - loglik: -1.9532e+02 - logprior: -8.9870e+00
Epoch 2/2
12/12 - 1s - loss: 193.7541 - loglik: -1.9137e+02 - logprior: -2.3802e+00
Fitted a model with MAP estimate = -192.0620
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 205.5996 - loglik: -1.9434e+02 - logprior: -1.1261e+01
Epoch 2/10
12/12 - 1s - loss: 193.7311 - loglik: -1.8989e+02 - logprior: -3.8417e+00
Epoch 3/10
12/12 - 1s - loss: 189.9298 - loglik: -1.8819e+02 - logprior: -1.7349e+00
Epoch 4/10
12/12 - 1s - loss: 187.0626 - loglik: -1.8600e+02 - logprior: -1.0668e+00
Epoch 5/10
12/12 - 1s - loss: 185.4636 - loglik: -1.8453e+02 - logprior: -9.2908e-01
Epoch 6/10
12/12 - 1s - loss: 183.7701 - loglik: -1.8293e+02 - logprior: -8.4453e-01
Epoch 7/10
12/12 - 1s - loss: 183.0711 - loglik: -1.8223e+02 - logprior: -8.4565e-01
Epoch 8/10
12/12 - 1s - loss: 181.1183 - loglik: -1.8025e+02 - logprior: -8.6954e-01
Epoch 9/10
12/12 - 1s - loss: 182.2766 - loglik: -1.8141e+02 - logprior: -8.6782e-01
Fitted a model with MAP estimate = -181.3416
Time for alignment: 45.8548
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.4173 - loglik: -2.8250e+02 - logprior: -9.9137e+00
Epoch 2/10
12/12 - 1s - loss: 255.2978 - loglik: -2.5290e+02 - logprior: -2.4028e+00
Epoch 3/10
12/12 - 1s - loss: 227.7941 - loglik: -2.2611e+02 - logprior: -1.6859e+00
Epoch 4/10
12/12 - 1s - loss: 216.4683 - loglik: -2.1479e+02 - logprior: -1.6780e+00
Epoch 5/10
12/12 - 1s - loss: 211.9357 - loglik: -2.1025e+02 - logprior: -1.6893e+00
Epoch 6/10
12/12 - 1s - loss: 210.5757 - loglik: -2.0895e+02 - logprior: -1.6306e+00
Epoch 7/10
12/12 - 1s - loss: 207.7840 - loglik: -2.0619e+02 - logprior: -1.5934e+00
Epoch 8/10
12/12 - 1s - loss: 208.0910 - loglik: -2.0647e+02 - logprior: -1.6183e+00
Fitted a model with MAP estimate = -207.4276
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 219.8354 - loglik: -2.0842e+02 - logprior: -1.1414e+01
Epoch 2/2
12/12 - 1s - loss: 201.0460 - loglik: -1.9630e+02 - logprior: -4.7499e+00
Fitted a model with MAP estimate = -196.8814
expansions: [(0, 5), (46, 1)]
discards: [ 0 62 80]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 203.3128 - loglik: -1.9412e+02 - logprior: -9.1931e+00
Epoch 2/2
12/12 - 1s - loss: 191.4606 - loglik: -1.8906e+02 - logprior: -2.4032e+00
Fitted a model with MAP estimate = -190.1621
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 199.6415 - loglik: -1.9072e+02 - logprior: -8.9258e+00
Epoch 2/10
12/12 - 1s - loss: 191.3533 - loglik: -1.8888e+02 - logprior: -2.4772e+00
Epoch 3/10
12/12 - 1s - loss: 188.4050 - loglik: -1.8685e+02 - logprior: -1.5506e+00
Epoch 4/10
12/12 - 1s - loss: 187.8894 - loglik: -1.8671e+02 - logprior: -1.1771e+00
Epoch 5/10
12/12 - 1s - loss: 186.4616 - loglik: -1.8541e+02 - logprior: -1.0500e+00
Epoch 6/10
12/12 - 1s - loss: 183.3738 - loglik: -1.8238e+02 - logprior: -9.9001e-01
Epoch 7/10
12/12 - 1s - loss: 183.0769 - loglik: -1.8211e+02 - logprior: -9.6213e-01
Epoch 8/10
12/12 - 1s - loss: 182.7493 - loglik: -1.8181e+02 - logprior: -9.4090e-01
Epoch 9/10
12/12 - 1s - loss: 181.3303 - loglik: -1.8042e+02 - logprior: -9.0912e-01
Epoch 10/10
12/12 - 1s - loss: 182.4311 - loglik: -1.8157e+02 - logprior: -8.6008e-01
Fitted a model with MAP estimate = -181.6506
Time for alignment: 49.3593
Computed alignments with likelihoods: ['-182.3935', '-181.4736', '-181.7636', '-181.3416', '-181.6506']
Best model has likelihood: -181.3416  (prior= -0.8547 )
time for generating output: 0.1706
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.893360318941978
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.0456 - loglik: -2.8881e+02 - logprior: -4.0232e+01
Epoch 2/10
10/10 - 1s - loss: 276.1378 - loglik: -2.6607e+02 - logprior: -1.0069e+01
Epoch 3/10
10/10 - 1s - loss: 249.3843 - loglik: -2.4521e+02 - logprior: -4.1766e+00
Epoch 4/10
10/10 - 1s - loss: 236.1270 - loglik: -2.3405e+02 - logprior: -2.0723e+00
Epoch 5/10
10/10 - 1s - loss: 230.3447 - loglik: -2.2920e+02 - logprior: -1.1493e+00
Epoch 6/10
10/10 - 1s - loss: 227.7805 - loglik: -2.2709e+02 - logprior: -6.9146e-01
Epoch 7/10
10/10 - 1s - loss: 226.5427 - loglik: -2.2606e+02 - logprior: -4.8525e-01
Epoch 8/10
10/10 - 1s - loss: 225.1600 - loglik: -2.2489e+02 - logprior: -2.7405e-01
Epoch 9/10
10/10 - 1s - loss: 225.3098 - loglik: -2.2521e+02 - logprior: -1.0175e-01
Fitted a model with MAP estimate = -224.5116
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (43, 10), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 273.2193 - loglik: -2.2145e+02 - logprior: -5.1770e+01
Epoch 2/2
10/10 - 1s - loss: 231.6302 - loglik: -2.1651e+02 - logprior: -1.5121e+01
Fitted a model with MAP estimate = -223.5994
expansions: []
discards: [ 0  1 71]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 261.6640 - loglik: -2.1611e+02 - logprior: -4.5550e+01
Epoch 2/2
10/10 - 1s - loss: 232.6853 - loglik: -2.1523e+02 - logprior: -1.7458e+01
Fitted a model with MAP estimate = -228.0867
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 255.1297 - loglik: -2.1454e+02 - logprior: -4.0585e+01
Epoch 2/10
10/10 - 1s - loss: 224.4744 - loglik: -2.1444e+02 - logprior: -1.0036e+01
Epoch 3/10
10/10 - 1s - loss: 216.8504 - loglik: -2.1359e+02 - logprior: -3.2605e+00
Epoch 4/10
10/10 - 1s - loss: 214.8280 - loglik: -2.1407e+02 - logprior: -7.5588e-01
Epoch 5/10
10/10 - 1s - loss: 213.9576 - loglik: -2.1446e+02 - logprior: 0.5033
Epoch 6/10
10/10 - 1s - loss: 213.0181 - loglik: -2.1419e+02 - logprior: 1.1729
Epoch 7/10
10/10 - 1s - loss: 212.9866 - loglik: -2.1454e+02 - logprior: 1.5549
Epoch 8/10
10/10 - 1s - loss: 212.5869 - loglik: -2.1439e+02 - logprior: 1.8023
Epoch 9/10
10/10 - 1s - loss: 212.2186 - loglik: -2.1424e+02 - logprior: 2.0239
Epoch 10/10
10/10 - 1s - loss: 212.3614 - loglik: -2.1460e+02 - logprior: 2.2425
Fitted a model with MAP estimate = -212.0946
Time for alignment: 42.3422
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.2521 - loglik: -2.8902e+02 - logprior: -4.0229e+01
Epoch 2/10
10/10 - 1s - loss: 276.4220 - loglik: -2.6635e+02 - logprior: -1.0075e+01
Epoch 3/10
10/10 - 1s - loss: 250.5726 - loglik: -2.4636e+02 - logprior: -4.2081e+00
Epoch 4/10
10/10 - 1s - loss: 238.2589 - loglik: -2.3620e+02 - logprior: -2.0582e+00
Epoch 5/10
10/10 - 1s - loss: 232.3425 - loglik: -2.3121e+02 - logprior: -1.1337e+00
Epoch 6/10
10/10 - 1s - loss: 229.1085 - loglik: -2.2838e+02 - logprior: -7.2687e-01
Epoch 7/10
10/10 - 1s - loss: 227.1844 - loglik: -2.2669e+02 - logprior: -4.9642e-01
Epoch 8/10
10/10 - 1s - loss: 226.5669 - loglik: -2.2626e+02 - logprior: -3.0742e-01
Epoch 9/10
10/10 - 1s - loss: 224.9581 - loglik: -2.2478e+02 - logprior: -1.7609e-01
Epoch 10/10
10/10 - 1s - loss: 225.0592 - loglik: -2.2494e+02 - logprior: -1.1804e-01
Fitted a model with MAP estimate = -224.6417
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 1), (38, 2), (43, 10), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.2399 - loglik: -2.2239e+02 - logprior: -5.1845e+01
Epoch 2/2
10/10 - 1s - loss: 231.1664 - loglik: -2.1590e+02 - logprior: -1.5263e+01
Fitted a model with MAP estimate = -223.6792
expansions: []
discards: [ 0  1  9 75]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 261.7916 - loglik: -2.1643e+02 - logprior: -4.5365e+01
Epoch 2/2
10/10 - 1s - loss: 232.2953 - loglik: -2.1486e+02 - logprior: -1.7431e+01
Fitted a model with MAP estimate = -228.1235
expansions: [(0, 3)]
discards: [ 0 55 57]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 256.7852 - loglik: -2.1623e+02 - logprior: -4.0556e+01
Epoch 2/10
10/10 - 1s - loss: 225.1569 - loglik: -2.1511e+02 - logprior: -1.0048e+01
Epoch 3/10
10/10 - 1s - loss: 217.5811 - loglik: -2.1429e+02 - logprior: -3.2923e+00
Epoch 4/10
10/10 - 1s - loss: 215.7981 - loglik: -2.1501e+02 - logprior: -7.9193e-01
Epoch 5/10
10/10 - 1s - loss: 214.6379 - loglik: -2.1511e+02 - logprior: 0.4749
Epoch 6/10
10/10 - 1s - loss: 213.6989 - loglik: -2.1486e+02 - logprior: 1.1593
Epoch 7/10
10/10 - 1s - loss: 213.6360 - loglik: -2.1518e+02 - logprior: 1.5412
Epoch 8/10
10/10 - 1s - loss: 213.3036 - loglik: -2.1509e+02 - logprior: 1.7905
Epoch 9/10
10/10 - 1s - loss: 212.8497 - loglik: -2.1486e+02 - logprior: 2.0082
Epoch 10/10
10/10 - 1s - loss: 213.2144 - loglik: -2.1543e+02 - logprior: 2.2190
Fitted a model with MAP estimate = -212.7980
Time for alignment: 43.0409
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.3138 - loglik: -2.8908e+02 - logprior: -4.0232e+01
Epoch 2/10
10/10 - 1s - loss: 276.3718 - loglik: -2.6631e+02 - logprior: -1.0065e+01
Epoch 3/10
10/10 - 1s - loss: 250.6100 - loglik: -2.4643e+02 - logprior: -4.1755e+00
Epoch 4/10
10/10 - 1s - loss: 237.5046 - loglik: -2.3547e+02 - logprior: -2.0338e+00
Epoch 5/10
10/10 - 1s - loss: 231.7684 - loglik: -2.3073e+02 - logprior: -1.0388e+00
Epoch 6/10
10/10 - 1s - loss: 228.7144 - loglik: -2.2810e+02 - logprior: -6.1820e-01
Epoch 7/10
10/10 - 1s - loss: 227.1312 - loglik: -2.2670e+02 - logprior: -4.3579e-01
Epoch 8/10
10/10 - 1s - loss: 225.8726 - loglik: -2.2560e+02 - logprior: -2.6999e-01
Epoch 9/10
10/10 - 1s - loss: 225.4138 - loglik: -2.2529e+02 - logprior: -1.2352e-01
Epoch 10/10
10/10 - 1s - loss: 225.1501 - loglik: -2.2514e+02 - logprior: -5.7458e-03
Fitted a model with MAP estimate = -224.4186
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 1), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 274.1041 - loglik: -2.2209e+02 - logprior: -5.2014e+01
Epoch 2/2
10/10 - 1s - loss: 232.2320 - loglik: -2.1685e+02 - logprior: -1.5387e+01
Fitted a model with MAP estimate = -224.3500
expansions: []
discards: [ 0  1 55 73]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 262.4061 - loglik: -2.1685e+02 - logprior: -4.5556e+01
Epoch 2/2
10/10 - 1s - loss: 233.3368 - loglik: -2.1586e+02 - logprior: -1.7477e+01
Fitted a model with MAP estimate = -228.6842
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 255.6958 - loglik: -2.1502e+02 - logprior: -4.0678e+01
Epoch 2/10
10/10 - 1s - loss: 224.7636 - loglik: -2.1471e+02 - logprior: -1.0051e+01
Epoch 3/10
10/10 - 1s - loss: 217.6248 - loglik: -2.1441e+02 - logprior: -3.2191e+00
Epoch 4/10
10/10 - 1s - loss: 215.0740 - loglik: -2.1434e+02 - logprior: -7.3025e-01
Epoch 5/10
10/10 - 1s - loss: 213.8776 - loglik: -2.1439e+02 - logprior: 0.5128
Epoch 6/10
10/10 - 1s - loss: 213.8669 - loglik: -2.1503e+02 - logprior: 1.1604
Epoch 7/10
10/10 - 1s - loss: 213.0401 - loglik: -2.1457e+02 - logprior: 1.5328
Epoch 8/10
10/10 - 1s - loss: 212.8398 - loglik: -2.1467e+02 - logprior: 1.8342
Epoch 9/10
10/10 - 1s - loss: 212.6071 - loglik: -2.1471e+02 - logprior: 2.1065
Epoch 10/10
10/10 - 1s - loss: 212.6857 - loglik: -2.1502e+02 - logprior: 2.3302
Fitted a model with MAP estimate = -212.3582
Time for alignment: 41.0451
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 328.8918 - loglik: -2.8866e+02 - logprior: -4.0231e+01
Epoch 2/10
10/10 - 1s - loss: 276.6060 - loglik: -2.6653e+02 - logprior: -1.0075e+01
Epoch 3/10
10/10 - 1s - loss: 251.3810 - loglik: -2.4714e+02 - logprior: -4.2421e+00
Epoch 4/10
10/10 - 1s - loss: 239.1066 - loglik: -2.3696e+02 - logprior: -2.1439e+00
Epoch 5/10
10/10 - 1s - loss: 233.5963 - loglik: -2.3243e+02 - logprior: -1.1640e+00
Epoch 6/10
10/10 - 1s - loss: 230.4854 - loglik: -2.2974e+02 - logprior: -7.4719e-01
Epoch 7/10
10/10 - 1s - loss: 228.5878 - loglik: -2.2802e+02 - logprior: -5.7203e-01
Epoch 8/10
10/10 - 1s - loss: 227.0415 - loglik: -2.2663e+02 - logprior: -4.1277e-01
Epoch 9/10
10/10 - 1s - loss: 226.3049 - loglik: -2.2603e+02 - logprior: -2.7348e-01
Epoch 10/10
10/10 - 1s - loss: 225.5009 - loglik: -2.2529e+02 - logprior: -2.0744e-01
Fitted a model with MAP estimate = -225.0468
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 1), (38, 2), (43, 9), (52, 1), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.9921 - loglik: -2.2294e+02 - logprior: -5.2053e+01
Epoch 2/2
10/10 - 1s - loss: 232.2238 - loglik: -2.1680e+02 - logprior: -1.5426e+01
Fitted a model with MAP estimate = -224.4847
expansions: [(7, 1)]
discards: [ 0  1 43 73]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 261.8284 - loglik: -2.1620e+02 - logprior: -4.5631e+01
Epoch 2/2
10/10 - 1s - loss: 233.2750 - loglik: -2.1567e+02 - logprior: -1.7602e+01
Fitted a model with MAP estimate = -228.1434
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 255.4205 - loglik: -2.1463e+02 - logprior: -4.0793e+01
Epoch 2/10
10/10 - 1s - loss: 223.8106 - loglik: -2.1365e+02 - logprior: -1.0161e+01
Epoch 3/10
10/10 - 1s - loss: 216.7453 - loglik: -2.1345e+02 - logprior: -3.2983e+00
Epoch 4/10
10/10 - 1s - loss: 214.3329 - loglik: -2.1356e+02 - logprior: -7.7627e-01
Epoch 5/10
10/10 - 1s - loss: 213.6739 - loglik: -2.1416e+02 - logprior: 0.4878
Epoch 6/10
10/10 - 1s - loss: 212.8177 - loglik: -2.1399e+02 - logprior: 1.1683
Epoch 7/10
10/10 - 1s - loss: 212.4420 - loglik: -2.1399e+02 - logprior: 1.5471
Epoch 8/10
10/10 - 1s - loss: 212.1581 - loglik: -2.1396e+02 - logprior: 1.8046
Epoch 9/10
10/10 - 1s - loss: 212.2691 - loglik: -2.1429e+02 - logprior: 2.0189
Fitted a model with MAP estimate = -211.8890
Time for alignment: 41.2867
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2956 - loglik: -2.8906e+02 - logprior: -4.0231e+01
Epoch 2/10
10/10 - 1s - loss: 276.0614 - loglik: -2.6600e+02 - logprior: -1.0062e+01
Epoch 3/10
10/10 - 1s - loss: 249.9561 - loglik: -2.4579e+02 - logprior: -4.1663e+00
Epoch 4/10
10/10 - 1s - loss: 236.5726 - loglik: -2.3452e+02 - logprior: -2.0511e+00
Epoch 5/10
10/10 - 1s - loss: 231.3609 - loglik: -2.3025e+02 - logprior: -1.1089e+00
Epoch 6/10
10/10 - 1s - loss: 228.3427 - loglik: -2.2766e+02 - logprior: -6.8209e-01
Epoch 7/10
10/10 - 1s - loss: 226.9034 - loglik: -2.2646e+02 - logprior: -4.3899e-01
Epoch 8/10
10/10 - 1s - loss: 225.6340 - loglik: -2.2540e+02 - logprior: -2.2978e-01
Epoch 9/10
10/10 - 1s - loss: 225.6725 - loglik: -2.2560e+02 - logprior: -6.9209e-02
Fitted a model with MAP estimate = -224.9710
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.6736 - loglik: -2.2262e+02 - logprior: -5.2051e+01
Epoch 2/2
10/10 - 1s - loss: 233.8638 - loglik: -2.1865e+02 - logprior: -1.5212e+01
Fitted a model with MAP estimate = -226.3002
expansions: [(7, 1)]
discards: [ 0  1 54 67]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 264.7155 - loglik: -2.1862e+02 - logprior: -4.6100e+01
Epoch 2/2
10/10 - 1s - loss: 235.5046 - loglik: -2.1783e+02 - logprior: -1.7672e+01
Fitted a model with MAP estimate = -230.7139
expansions: [(0, 3)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 258.2260 - loglik: -2.1773e+02 - logprior: -4.0500e+01
Epoch 2/10
10/10 - 1s - loss: 226.6884 - loglik: -2.1666e+02 - logprior: -1.0026e+01
Epoch 3/10
10/10 - 1s - loss: 220.0209 - loglik: -2.1665e+02 - logprior: -3.3681e+00
Epoch 4/10
10/10 - 1s - loss: 217.7506 - loglik: -2.1687e+02 - logprior: -8.7937e-01
Epoch 5/10
10/10 - 1s - loss: 216.5349 - loglik: -2.1691e+02 - logprior: 0.3757
Epoch 6/10
10/10 - 1s - loss: 216.4397 - loglik: -2.1749e+02 - logprior: 1.0465
Epoch 7/10
10/10 - 1s - loss: 215.6174 - loglik: -2.1705e+02 - logprior: 1.4286
Epoch 8/10
10/10 - 1s - loss: 215.5180 - loglik: -2.1719e+02 - logprior: 1.6697
Epoch 9/10
10/10 - 1s - loss: 215.3408 - loglik: -2.1723e+02 - logprior: 1.8844
Epoch 10/10
10/10 - 1s - loss: 215.2271 - loglik: -2.1732e+02 - logprior: 2.0904
Fitted a model with MAP estimate = -215.0250
Time for alignment: 38.9266
Computed alignments with likelihoods: ['-212.0946', '-212.7980', '-212.3582', '-211.8890', '-215.0250']
Best model has likelihood: -211.8890  (prior= 2.1436 )
time for generating output: 0.1644
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7527917328266133
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.4305 - loglik: -4.7366e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 419.4184 - loglik: -4.0420e+02 - logprior: -1.5222e+01
Epoch 3/10
10/10 - 2s - loss: 341.1682 - loglik: -3.3584e+02 - logprior: -5.3293e+00
Epoch 4/10
10/10 - 2s - loss: 293.4040 - loglik: -2.9005e+02 - logprior: -3.3585e+00
Epoch 5/10
10/10 - 2s - loss: 276.3698 - loglik: -2.7370e+02 - logprior: -2.6741e+00
Epoch 6/10
10/10 - 2s - loss: 269.3676 - loglik: -2.6796e+02 - logprior: -1.4060e+00
Epoch 7/10
10/10 - 2s - loss: 266.5637 - loglik: -2.6620e+02 - logprior: -3.6147e-01
Epoch 8/10
10/10 - 2s - loss: 263.9385 - loglik: -2.6395e+02 - logprior: 0.0067
Epoch 9/10
10/10 - 2s - loss: 264.1281 - loglik: -2.6449e+02 - logprior: 0.3595
Fitted a model with MAP estimate = -263.1864
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.3989 - loglik: -2.5791e+02 - logprior: -8.8494e+01
Epoch 2/2
10/10 - 2s - loss: 269.1862 - loglik: -2.3707e+02 - logprior: -3.2112e+01
Fitted a model with MAP estimate = -256.7077
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (88, 1)]
discards: [  0  43  47  48  55 108 121]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 300.2230 - loglik: -2.3082e+02 - logprior: -6.9405e+01
Epoch 2/2
10/10 - 2s - loss: 234.3665 - loglik: -2.2253e+02 - logprior: -1.1840e+01
Fitted a model with MAP estimate = -224.4766
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 308.9573 - loglik: -2.2407e+02 - logprior: -8.4883e+01
Epoch 2/10
10/10 - 2s - loss: 247.7076 - loglik: -2.2209e+02 - logprior: -2.5613e+01
Epoch 3/10
10/10 - 2s - loss: 224.9851 - loglik: -2.2031e+02 - logprior: -4.6711e+00
Epoch 4/10
10/10 - 2s - loss: 214.0898 - loglik: -2.1993e+02 - logprior: 5.8444
Epoch 5/10
10/10 - 2s - loss: 210.2256 - loglik: -2.1979e+02 - logprior: 9.5623
Epoch 6/10
10/10 - 2s - loss: 208.7870 - loglik: -2.2027e+02 - logprior: 11.4792
Epoch 7/10
10/10 - 2s - loss: 207.0645 - loglik: -2.1980e+02 - logprior: 12.7391
Epoch 8/10
10/10 - 2s - loss: 206.1296 - loglik: -2.1984e+02 - logprior: 13.7093
Epoch 9/10
10/10 - 2s - loss: 206.0070 - loglik: -2.2056e+02 - logprior: 14.5570
Epoch 10/10
10/10 - 2s - loss: 205.2275 - loglik: -2.2051e+02 - logprior: 15.2863
Fitted a model with MAP estimate = -204.6251
Time for alignment: 66.3354
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.3810 - loglik: -4.7361e+02 - logprior: -7.7774e+01
Epoch 2/10
10/10 - 2s - loss: 419.4815 - loglik: -4.0426e+02 - logprior: -1.5222e+01
Epoch 3/10
10/10 - 2s - loss: 341.6536 - loglik: -3.3629e+02 - logprior: -5.3617e+00
Epoch 4/10
10/10 - 2s - loss: 293.5226 - loglik: -2.9002e+02 - logprior: -3.4984e+00
Epoch 5/10
10/10 - 2s - loss: 275.5375 - loglik: -2.7254e+02 - logprior: -3.0020e+00
Epoch 6/10
10/10 - 2s - loss: 268.4175 - loglik: -2.6661e+02 - logprior: -1.8058e+00
Epoch 7/10
10/10 - 2s - loss: 265.5934 - loglik: -2.6482e+02 - logprior: -7.7339e-01
Epoch 8/10
10/10 - 2s - loss: 263.8514 - loglik: -2.6351e+02 - logprior: -3.3838e-01
Epoch 9/10
10/10 - 2s - loss: 263.1035 - loglik: -2.6309e+02 - logprior: -8.9612e-03
Epoch 10/10
10/10 - 2s - loss: 262.2408 - loglik: -2.6269e+02 - logprior: 0.4491
Fitted a model with MAP estimate = -262.0171
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 344.9680 - loglik: -2.5650e+02 - logprior: -8.8468e+01
Epoch 2/2
10/10 - 2s - loss: 269.2618 - loglik: -2.3739e+02 - logprior: -3.1867e+01
Fitted a model with MAP estimate = -256.8072
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 300.0329 - loglik: -2.3060e+02 - logprior: -6.9429e+01
Epoch 2/2
10/10 - 2s - loss: 235.4383 - loglik: -2.2365e+02 - logprior: -1.1786e+01
Fitted a model with MAP estimate = -224.5927
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.0645 - loglik: -2.2429e+02 - logprior: -8.4775e+01
Epoch 2/10
10/10 - 2s - loss: 248.0127 - loglik: -2.2264e+02 - logprior: -2.5370e+01
Epoch 3/10
10/10 - 2s - loss: 223.9874 - loglik: -2.1964e+02 - logprior: -4.3519e+00
Epoch 4/10
10/10 - 2s - loss: 214.3543 - loglik: -2.2031e+02 - logprior: 5.9515
Epoch 5/10
10/10 - 2s - loss: 210.9119 - loglik: -2.2054e+02 - logprior: 9.6249
Epoch 6/10
10/10 - 2s - loss: 207.9812 - loglik: -2.1952e+02 - logprior: 11.5388
Epoch 7/10
10/10 - 2s - loss: 206.8633 - loglik: -2.1966e+02 - logprior: 12.8002
Epoch 8/10
10/10 - 2s - loss: 207.2385 - loglik: -2.2102e+02 - logprior: 13.7847
Fitted a model with MAP estimate = -205.9496
Time for alignment: 65.1130
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.6736 - loglik: -4.7390e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 419.2305 - loglik: -4.0401e+02 - logprior: -1.5221e+01
Epoch 3/10
10/10 - 2s - loss: 341.0962 - loglik: -3.3575e+02 - logprior: -5.3450e+00
Epoch 4/10
10/10 - 2s - loss: 293.9479 - loglik: -2.9059e+02 - logprior: -3.3615e+00
Epoch 5/10
10/10 - 2s - loss: 276.2944 - loglik: -2.7389e+02 - logprior: -2.4055e+00
Epoch 6/10
10/10 - 2s - loss: 270.3008 - loglik: -2.6918e+02 - logprior: -1.1209e+00
Epoch 7/10
10/10 - 2s - loss: 267.0691 - loglik: -2.6690e+02 - logprior: -1.6652e-01
Epoch 8/10
10/10 - 2s - loss: 265.3865 - loglik: -2.6549e+02 - logprior: 0.0999
Epoch 9/10
10/10 - 2s - loss: 263.6606 - loglik: -2.6402e+02 - logprior: 0.3579
Epoch 10/10
10/10 - 2s - loss: 262.6030 - loglik: -2.6343e+02 - logprior: 0.8286
Fitted a model with MAP estimate = -262.7448
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.0890 - loglik: -2.5764e+02 - logprior: -8.8445e+01
Epoch 2/2
10/10 - 2s - loss: 270.0486 - loglik: -2.3794e+02 - logprior: -3.2109e+01
Fitted a model with MAP estimate = -256.9468
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (87, 1)]
discards: [  0  43  47  54 107 120]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 300.4660 - loglik: -2.3109e+02 - logprior: -6.9380e+01
Epoch 2/2
10/10 - 2s - loss: 234.9829 - loglik: -2.2316e+02 - logprior: -1.1824e+01
Fitted a model with MAP estimate = -224.6078
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 308.8019 - loglik: -2.2399e+02 - logprior: -8.4813e+01
Epoch 2/10
10/10 - 2s - loss: 247.2671 - loglik: -2.2182e+02 - logprior: -2.5444e+01
Epoch 3/10
10/10 - 2s - loss: 225.7010 - loglik: -2.2120e+02 - logprior: -4.5024e+00
Epoch 4/10
10/10 - 2s - loss: 214.8996 - loglik: -2.2082e+02 - logprior: 5.9167
Epoch 5/10
10/10 - 2s - loss: 209.1656 - loglik: -2.1879e+02 - logprior: 9.6265
Epoch 6/10
10/10 - 2s - loss: 208.6051 - loglik: -2.2013e+02 - logprior: 11.5284
Epoch 7/10
10/10 - 2s - loss: 208.2732 - loglik: -2.2106e+02 - logprior: 12.7827
Epoch 8/10
10/10 - 2s - loss: 205.2341 - loglik: -2.1899e+02 - logprior: 13.7572
Epoch 9/10
10/10 - 2s - loss: 206.1713 - loglik: -2.2078e+02 - logprior: 14.6064
Fitted a model with MAP estimate = -205.3321
Time for alignment: 65.6499
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.2649 - loglik: -4.7349e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 2s - loss: 419.5160 - loglik: -4.0430e+02 - logprior: -1.5221e+01
Epoch 3/10
10/10 - 2s - loss: 341.3714 - loglik: -3.3603e+02 - logprior: -5.3367e+00
Epoch 4/10
10/10 - 2s - loss: 292.8661 - loglik: -2.8946e+02 - logprior: -3.4049e+00
Epoch 5/10
10/10 - 2s - loss: 275.6381 - loglik: -2.7275e+02 - logprior: -2.8896e+00
Epoch 6/10
10/10 - 2s - loss: 268.7598 - loglik: -2.6691e+02 - logprior: -1.8493e+00
Epoch 7/10
10/10 - 2s - loss: 265.5453 - loglik: -2.6475e+02 - logprior: -7.9803e-01
Epoch 8/10
10/10 - 2s - loss: 263.5706 - loglik: -2.6322e+02 - logprior: -3.5314e-01
Epoch 9/10
10/10 - 2s - loss: 263.1216 - loglik: -2.6312e+02 - logprior: 7.5510e-04
Epoch 10/10
10/10 - 2s - loss: 261.9548 - loglik: -2.6240e+02 - logprior: 0.4458
Fitted a model with MAP estimate = -261.8684
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 345.0002 - loglik: -2.5653e+02 - logprior: -8.8466e+01
Epoch 2/2
10/10 - 2s - loss: 269.3198 - loglik: -2.3744e+02 - logprior: -3.1877e+01
Fitted a model with MAP estimate = -256.8958
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 300.0681 - loglik: -2.3064e+02 - logprior: -6.9425e+01
Epoch 2/2
10/10 - 2s - loss: 235.2414 - loglik: -2.2342e+02 - logprior: -1.1817e+01
Fitted a model with MAP estimate = -224.6331
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 309.3820 - loglik: -2.2457e+02 - logprior: -8.4810e+01
Epoch 2/10
10/10 - 2s - loss: 246.8854 - loglik: -2.2147e+02 - logprior: -2.5416e+01
Epoch 3/10
10/10 - 2s - loss: 225.6113 - loglik: -2.2118e+02 - logprior: -4.4327e+00
Epoch 4/10
10/10 - 2s - loss: 213.4180 - loglik: -2.1935e+02 - logprior: 5.9353
Epoch 5/10
10/10 - 2s - loss: 210.2516 - loglik: -2.1987e+02 - logprior: 9.6173
Epoch 6/10
10/10 - 2s - loss: 209.6426 - loglik: -2.2118e+02 - logprior: 11.5358
Epoch 7/10
10/10 - 2s - loss: 206.2054 - loglik: -2.1901e+02 - logprior: 12.8026
Epoch 8/10
10/10 - 2s - loss: 206.5391 - loglik: -2.2031e+02 - logprior: 13.7677
Fitted a model with MAP estimate = -205.9673
Time for alignment: 63.6822
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.2461 - loglik: -4.7347e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 2s - loss: 419.5255 - loglik: -4.0431e+02 - logprior: -1.5217e+01
Epoch 3/10
10/10 - 2s - loss: 340.8913 - loglik: -3.3555e+02 - logprior: -5.3364e+00
Epoch 4/10
10/10 - 2s - loss: 293.2446 - loglik: -2.8986e+02 - logprior: -3.3808e+00
Epoch 5/10
10/10 - 2s - loss: 275.6793 - loglik: -2.7290e+02 - logprior: -2.7753e+00
Epoch 6/10
10/10 - 2s - loss: 269.1100 - loglik: -2.6728e+02 - logprior: -1.8346e+00
Epoch 7/10
10/10 - 2s - loss: 265.7051 - loglik: -2.6489e+02 - logprior: -8.1733e-01
Epoch 8/10
10/10 - 2s - loss: 263.7546 - loglik: -2.6340e+02 - logprior: -3.5753e-01
Epoch 9/10
10/10 - 2s - loss: 263.0555 - loglik: -2.6305e+02 - logprior: -1.1702e-03
Epoch 10/10
10/10 - 2s - loss: 262.3709 - loglik: -2.6283e+02 - logprior: 0.4552
Fitted a model with MAP estimate = -262.0293
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 344.8800 - loglik: -2.5642e+02 - logprior: -8.8460e+01
Epoch 2/2
10/10 - 2s - loss: 269.2831 - loglik: -2.3735e+02 - logprior: -3.1937e+01
Fitted a model with MAP estimate = -256.7264
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 300.3133 - loglik: -2.3091e+02 - logprior: -6.9403e+01
Epoch 2/2
10/10 - 2s - loss: 234.8165 - loglik: -2.2299e+02 - logprior: -1.1824e+01
Fitted a model with MAP estimate = -224.6560
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 308.7299 - loglik: -2.2393e+02 - logprior: -8.4805e+01
Epoch 2/10
10/10 - 2s - loss: 247.9286 - loglik: -2.2252e+02 - logprior: -2.5413e+01
Epoch 3/10
10/10 - 2s - loss: 225.4233 - loglik: -2.2101e+02 - logprior: -4.4180e+00
Epoch 4/10
10/10 - 2s - loss: 213.3012 - loglik: -2.1924e+02 - logprior: 5.9413
Epoch 5/10
10/10 - 2s - loss: 210.7070 - loglik: -2.2033e+02 - logprior: 9.6227
Epoch 6/10
10/10 - 2s - loss: 208.6672 - loglik: -2.2021e+02 - logprior: 11.5423
Epoch 7/10
10/10 - 2s - loss: 206.9626 - loglik: -2.1975e+02 - logprior: 12.7892
Epoch 8/10
10/10 - 2s - loss: 205.6881 - loglik: -2.1948e+02 - logprior: 13.7882
Epoch 9/10
10/10 - 2s - loss: 206.4393 - loglik: -2.2105e+02 - logprior: 14.6076
Fitted a model with MAP estimate = -205.2820
Time for alignment: 66.7378
Computed alignments with likelihoods: ['-204.6251', '-205.9496', '-205.3321', '-205.9673', '-205.2820']
Best model has likelihood: -204.6251  (prior= 15.6456 )
time for generating output: 0.1946
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 813.3314 - loglik: -8.1131e+02 - logprior: -2.0171e+00
Epoch 2/10
33/33 - 21s - loss: 719.7892 - loglik: -7.1914e+02 - logprior: -6.4902e-01
Epoch 3/10
33/33 - 20s - loss: 713.1767 - loglik: -7.1261e+02 - logprior: -5.6938e-01
Epoch 4/10
33/33 - 20s - loss: 707.8090 - loglik: -7.0729e+02 - logprior: -5.2226e-01
Epoch 5/10
33/33 - 21s - loss: 708.5833 - loglik: -7.0806e+02 - logprior: -5.2170e-01
Fitted a model with MAP estimate = -707.5704
expansions: [(0, 5), (7, 1), (9, 2), (34, 9), (67, 1), (71, 1), (72, 1), (73, 2), (78, 1), (111, 1), (113, 1), (116, 1), (118, 1), (134, 1), (155, 3), (163, 4), (177, 1), (204, 1), (210, 1), (220, 2), (221, 4), (230, 3)]
discards: [  2   3   4 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 710.2446 - loglik: -7.0734e+02 - logprior: -2.9030e+00
Epoch 2/2
33/33 - 26s - loss: 700.4363 - loglik: -7.0003e+02 - logprior: -4.0877e-01
Fitted a model with MAP estimate = -698.0671
expansions: [(270, 4)]
discards: [  1   5   6  14  41  42  43  44  45 181 192 193 194 195 256 265 266 267
 268]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 706.9014 - loglik: -7.0490e+02 - logprior: -2.0064e+00
Epoch 2/2
33/33 - 24s - loss: 702.5085 - loglik: -7.0251e+02 - logprior: 0.0013
Fitted a model with MAP estimate = -700.3225
expansions: [(0, 4), (1, 1), (255, 4)]
discards: [250 251 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 706.8699 - loglik: -7.0388e+02 - logprior: -2.9891e+00
Epoch 2/10
33/33 - 25s - loss: 701.2418 - loglik: -7.0129e+02 - logprior: 0.0483
Epoch 3/10
33/33 - 24s - loss: 699.9057 - loglik: -7.0012e+02 - logprior: 0.2190
Epoch 4/10
33/33 - 25s - loss: 697.6826 - loglik: -6.9800e+02 - logprior: 0.3163
Epoch 5/10
33/33 - 25s - loss: 697.3295 - loglik: -6.9773e+02 - logprior: 0.3972
Epoch 6/10
33/33 - 24s - loss: 696.0228 - loglik: -6.9651e+02 - logprior: 0.4857
Epoch 7/10
33/33 - 25s - loss: 698.1522 - loglik: -6.9875e+02 - logprior: 0.5967
Fitted a model with MAP estimate = -696.0134
Time for alignment: 483.3798
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 814.8950 - loglik: -8.1288e+02 - logprior: -2.0117e+00
Epoch 2/10
33/33 - 20s - loss: 721.2796 - loglik: -7.2068e+02 - logprior: -5.9483e-01
Epoch 3/10
33/33 - 21s - loss: 710.6906 - loglik: -7.1022e+02 - logprior: -4.6685e-01
Epoch 4/10
33/33 - 20s - loss: 711.2252 - loglik: -7.1081e+02 - logprior: -4.1183e-01
Fitted a model with MAP estimate = -708.1201
expansions: [(0, 4), (9, 1), (34, 6), (67, 1), (71, 1), (72, 1), (73, 2), (78, 2), (113, 1), (116, 1), (118, 1), (132, 2), (155, 3), (163, 4), (176, 1), (203, 1), (205, 1), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 710.5873 - loglik: -7.0780e+02 - logprior: -2.7903e+00
Epoch 2/2
33/33 - 25s - loss: 698.7732 - loglik: -6.9844e+02 - logprior: -3.3266e-01
Fitted a model with MAP estimate = -698.3366
expansions: [(265, 4)]
discards: [  1   5   6  14  41  42 179 190 191 192 259 260 261 262 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 706.4679 - loglik: -7.0447e+02 - logprior: -2.0015e+00
Epoch 2/2
33/33 - 23s - loss: 700.7540 - loglik: -7.0079e+02 - logprior: 0.0322
Fitted a model with MAP estimate = -700.1898
expansions: [(0, 5), (1, 1), (246, 1), (248, 1), (253, 4)]
discards: [249 250 251]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 707.2878 - loglik: -7.0425e+02 - logprior: -3.0407e+00
Epoch 2/10
33/33 - 25s - loss: 700.1273 - loglik: -7.0018e+02 - logprior: 0.0566
Epoch 3/10
33/33 - 25s - loss: 699.6072 - loglik: -6.9985e+02 - logprior: 0.2385
Epoch 4/10
33/33 - 24s - loss: 697.9021 - loglik: -6.9821e+02 - logprior: 0.3108
Epoch 5/10
33/33 - 25s - loss: 697.2377 - loglik: -6.9764e+02 - logprior: 0.4068
Epoch 6/10
33/33 - 25s - loss: 696.0181 - loglik: -6.9650e+02 - logprior: 0.4803
Epoch 7/10
33/33 - 25s - loss: 696.8383 - loglik: -6.9743e+02 - logprior: 0.5885
Fitted a model with MAP estimate = -695.9150
Time for alignment: 458.9964
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 813.2917 - loglik: -8.1129e+02 - logprior: -2.0031e+00
Epoch 2/10
33/33 - 21s - loss: 721.6066 - loglik: -7.2098e+02 - logprior: -6.2557e-01
Epoch 3/10
33/33 - 21s - loss: 712.0694 - loglik: -7.1151e+02 - logprior: -5.5574e-01
Epoch 4/10
33/33 - 21s - loss: 708.8315 - loglik: -7.0833e+02 - logprior: -5.0053e-01
Epoch 5/10
33/33 - 20s - loss: 709.1605 - loglik: -7.0868e+02 - logprior: -4.8230e-01
Fitted a model with MAP estimate = -707.5131
expansions: [(0, 4), (7, 1), (9, 2), (35, 4), (72, 1), (73, 1), (74, 2), (79, 2), (111, 1), (114, 1), (119, 1), (133, 2), (156, 3), (164, 4), (178, 1), (184, 1), (204, 1), (214, 1), (220, 1), (221, 4), (230, 3)]
discards: [  2   3 224 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 710.9111 - loglik: -7.0808e+02 - logprior: -2.8301e+00
Epoch 2/2
33/33 - 25s - loss: 699.5350 - loglik: -6.9924e+02 - logprior: -2.9338e-01
Fitted a model with MAP estimate = -698.4350
expansions: [(0, 4), (264, 4)]
discards: [  5   7  14  41 177 188 189 190 191 260 261 262 263]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 707.4175 - loglik: -7.0444e+02 - logprior: -2.9740e+00
Epoch 2/2
33/33 - 24s - loss: 702.5848 - loglik: -7.0251e+02 - logprior: -7.0256e-02
Fitted a model with MAP estimate = -699.7069
expansions: [(259, 4)]
discards: [  1   4   6   7   8   9 252 255 256 257]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 26s - loss: 706.7289 - loglik: -7.0492e+02 - logprior: -1.8124e+00
Epoch 2/10
33/33 - 24s - loss: 703.6140 - loglik: -7.0380e+02 - logprior: 0.1896
Epoch 3/10
33/33 - 24s - loss: 700.7180 - loglik: -7.0113e+02 - logprior: 0.4134
Epoch 4/10
33/33 - 23s - loss: 696.8831 - loglik: -6.9739e+02 - logprior: 0.5112
Epoch 5/10
33/33 - 24s - loss: 698.4545 - loglik: -6.9905e+02 - logprior: 0.5968
Fitted a model with MAP estimate = -697.3676
Time for alignment: 426.9070
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 811.5793 - loglik: -8.0960e+02 - logprior: -1.9811e+00
Epoch 2/10
33/33 - 21s - loss: 721.7119 - loglik: -7.2118e+02 - logprior: -5.3359e-01
Epoch 3/10
33/33 - 21s - loss: 709.0754 - loglik: -7.0864e+02 - logprior: -4.3734e-01
Epoch 4/10
33/33 - 20s - loss: 708.1248 - loglik: -7.0775e+02 - logprior: -3.7798e-01
Epoch 5/10
33/33 - 21s - loss: 704.2823 - loglik: -7.0392e+02 - logprior: -3.6397e-01
Epoch 6/10
33/33 - 21s - loss: 708.8455 - loglik: -7.0848e+02 - logprior: -3.6153e-01
Fitted a model with MAP estimate = -705.6737
expansions: [(0, 6), (34, 10), (45, 2), (64, 1), (66, 1), (72, 1), (74, 1), (79, 1), (81, 1), (111, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 3), (163, 4), (204, 1), (210, 1), (220, 2), (221, 4), (230, 3)]
discards: [  9 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 709.4161 - loglik: -7.0653e+02 - logprior: -2.8814e+00
Epoch 2/2
33/33 - 27s - loss: 697.4653 - loglik: -6.9704e+02 - logprior: -4.2994e-01
Fitted a model with MAP estimate = -697.6713
expansions: [(273, 4)]
discards: [  1  41  42  43  44  45  46  60 185 195 196 197 259 265 267 268 269 270
 271 272]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 705.0167 - loglik: -7.0305e+02 - logprior: -1.9674e+00
Epoch 2/2
33/33 - 24s - loss: 702.9184 - loglik: -7.0284e+02 - logprior: -7.4788e-02
Fitted a model with MAP estimate = -699.0295
expansions: [(186, 5), (257, 4)]
discards: [  4   5   6 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 705.0096 - loglik: -7.0319e+02 - logprior: -1.8244e+00
Epoch 2/10
33/33 - 25s - loss: 698.9440 - loglik: -6.9908e+02 - logprior: 0.1386
Epoch 3/10
33/33 - 24s - loss: 699.1196 - loglik: -6.9945e+02 - logprior: 0.3295
Fitted a model with MAP estimate = -696.4089
Time for alignment: 407.6853
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 812.3849 - loglik: -8.1039e+02 - logprior: -1.9941e+00
Epoch 2/10
33/33 - 21s - loss: 720.8326 - loglik: -7.2023e+02 - logprior: -5.9863e-01
Epoch 3/10
33/33 - 21s - loss: 709.3214 - loglik: -7.0879e+02 - logprior: -5.3540e-01
Epoch 4/10
33/33 - 21s - loss: 709.4485 - loglik: -7.0896e+02 - logprior: -4.8421e-01
Fitted a model with MAP estimate = -707.3626
expansions: [(0, 5), (5, 1), (10, 1), (33, 2), (34, 2), (63, 2), (72, 1), (73, 2), (78, 1), (91, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 3), (163, 4), (183, 1), (214, 1), (220, 2), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 708.5513 - loglik: -7.0577e+02 - logprior: -2.7837e+00
Epoch 2/2
33/33 - 25s - loss: 699.8442 - loglik: -6.9952e+02 - logprior: -3.2229e-01
Fitted a model with MAP estimate = -698.0167
expansions: [(266, 4)]
discards: [  1   5   6   7  41 179 190 191 192 253 261 262 263 264 265]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 704.9955 - loglik: -7.0300e+02 - logprior: -1.9928e+00
Epoch 2/2
33/33 - 24s - loss: 701.8453 - loglik: -7.0190e+02 - logprior: 0.0500
Fitted a model with MAP estimate = -699.2311
expansions: [(0, 5), (1, 1), (247, 1), (249, 1), (255, 4)]
discards: [251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 705.7366 - loglik: -7.0289e+02 - logprior: -2.8443e+00
Epoch 2/10
33/33 - 25s - loss: 702.0219 - loglik: -7.0211e+02 - logprior: 0.0832
Epoch 3/10
33/33 - 25s - loss: 696.4908 - loglik: -6.9672e+02 - logprior: 0.2339
Epoch 4/10
33/33 - 25s - loss: 696.7951 - loglik: -6.9711e+02 - logprior: 0.3138
Fitted a model with MAP estimate = -696.0929
Time for alignment: 387.0405
Computed alignments with likelihoods: ['-696.0134', '-695.9150', '-697.3676', '-696.4089', '-696.0929']
Best model has likelihood: -695.9150  (prior= 0.6570 )
time for generating output: 0.2914
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7770793036750484
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6875 - loglik: -3.4877e+02 - logprior: -6.1919e+01
Epoch 2/10
10/10 - 1s - loss: 320.4396 - loglik: -3.0614e+02 - logprior: -1.4300e+01
Epoch 3/10
10/10 - 1s - loss: 273.6460 - loglik: -2.6749e+02 - logprior: -6.1608e+00
Epoch 4/10
10/10 - 1s - loss: 249.4502 - loglik: -2.4577e+02 - logprior: -3.6829e+00
Epoch 5/10
10/10 - 1s - loss: 239.3893 - loglik: -2.3669e+02 - logprior: -2.6946e+00
Epoch 6/10
10/10 - 1s - loss: 235.2373 - loglik: -2.3332e+02 - logprior: -1.9142e+00
Epoch 7/10
10/10 - 1s - loss: 233.8745 - loglik: -2.3260e+02 - logprior: -1.2722e+00
Epoch 8/10
10/10 - 1s - loss: 232.4222 - loglik: -2.3153e+02 - logprior: -8.9017e-01
Epoch 9/10
10/10 - 1s - loss: 231.3520 - loglik: -2.3070e+02 - logprior: -6.4851e-01
Epoch 10/10
10/10 - 1s - loss: 231.0190 - loglik: -2.3057e+02 - logprior: -4.5015e-01
Fitted a model with MAP estimate = -230.6540
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 301.4586 - loglik: -2.3219e+02 - logprior: -6.9270e+01
Epoch 2/2
10/10 - 1s - loss: 240.3736 - loglik: -2.1430e+02 - logprior: -2.6072e+01
Fitted a model with MAP estimate = -229.2793
expansions: [(0, 5)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 265.9931 - loglik: -2.1160e+02 - logprior: -5.4395e+01
Epoch 2/2
10/10 - 2s - loss: 217.3829 - loglik: -2.0571e+02 - logprior: -1.1672e+01
Fitted a model with MAP estimate = -209.8566
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.1101 - loglik: -2.0880e+02 - logprior: -5.3313e+01
Epoch 2/10
10/10 - 1s - loss: 217.4715 - loglik: -2.0640e+02 - logprior: -1.1071e+01
Epoch 3/10
10/10 - 1s - loss: 207.6040 - loglik: -2.0527e+02 - logprior: -2.3302e+00
Epoch 4/10
10/10 - 1s - loss: 201.8600 - loglik: -2.0315e+02 - logprior: 1.2939
Epoch 5/10
10/10 - 1s - loss: 199.5303 - loglik: -2.0276e+02 - logprior: 3.2272
Epoch 6/10
10/10 - 1s - loss: 197.3066 - loglik: -2.0155e+02 - logprior: 4.2422
Epoch 7/10
10/10 - 1s - loss: 196.7987 - loglik: -2.0178e+02 - logprior: 4.9800
Epoch 8/10
10/10 - 1s - loss: 196.7853 - loglik: -2.0247e+02 - logprior: 5.6864
Epoch 9/10
10/10 - 1s - loss: 195.9145 - loglik: -2.0217e+02 - logprior: 6.2521
Epoch 10/10
10/10 - 1s - loss: 195.6322 - loglik: -2.0232e+02 - logprior: 6.6838
Fitted a model with MAP estimate = -195.3047
Time for alignment: 49.9761
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6696 - loglik: -3.4875e+02 - logprior: -6.1919e+01
Epoch 2/10
10/10 - 1s - loss: 321.0413 - loglik: -3.0674e+02 - logprior: -1.4303e+01
Epoch 3/10
10/10 - 1s - loss: 273.9474 - loglik: -2.6775e+02 - logprior: -6.1993e+00
Epoch 4/10
10/10 - 1s - loss: 248.7701 - loglik: -2.4496e+02 - logprior: -3.8141e+00
Epoch 5/10
10/10 - 1s - loss: 238.6849 - loglik: -2.3594e+02 - logprior: -2.7404e+00
Epoch 6/10
10/10 - 1s - loss: 234.6034 - loglik: -2.3264e+02 - logprior: -1.9660e+00
Epoch 7/10
10/10 - 1s - loss: 233.1853 - loglik: -2.3195e+02 - logprior: -1.2350e+00
Epoch 8/10
10/10 - 1s - loss: 232.1556 - loglik: -2.3129e+02 - logprior: -8.6698e-01
Epoch 9/10
10/10 - 1s - loss: 231.4455 - loglik: -2.3082e+02 - logprior: -6.2964e-01
Epoch 10/10
10/10 - 1s - loss: 231.3180 - loglik: -2.3091e+02 - logprior: -4.0747e-01
Fitted a model with MAP estimate = -230.6083
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (85, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 300.9150 - loglik: -2.3166e+02 - logprior: -6.9252e+01
Epoch 2/2
10/10 - 1s - loss: 240.3587 - loglik: -2.1427e+02 - logprior: -2.6085e+01
Fitted a model with MAP estimate = -229.1359
expansions: [(0, 5)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 265.3031 - loglik: -2.1093e+02 - logprior: -5.4373e+01
Epoch 2/2
10/10 - 2s - loss: 217.8731 - loglik: -2.0619e+02 - logprior: -1.1683e+01
Fitted a model with MAP estimate = -209.7734
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.1664 - loglik: -2.0882e+02 - logprior: -5.3343e+01
Epoch 2/10
10/10 - 1s - loss: 217.5595 - loglik: -2.0648e+02 - logprior: -1.1075e+01
Epoch 3/10
10/10 - 1s - loss: 206.8435 - loglik: -2.0452e+02 - logprior: -2.3283e+00
Epoch 4/10
10/10 - 1s - loss: 201.9983 - loglik: -2.0329e+02 - logprior: 1.2900
Epoch 5/10
10/10 - 1s - loss: 199.5950 - loglik: -2.0281e+02 - logprior: 3.2144
Epoch 6/10
10/10 - 1s - loss: 196.6932 - loglik: -2.0093e+02 - logprior: 4.2410
Epoch 7/10
10/10 - 1s - loss: 197.7564 - loglik: -2.0273e+02 - logprior: 4.9760
Fitted a model with MAP estimate = -196.5256
Time for alignment: 45.5846
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.4114 - loglik: -3.4849e+02 - logprior: -6.1919e+01
Epoch 2/10
10/10 - 1s - loss: 321.4289 - loglik: -3.0714e+02 - logprior: -1.4292e+01
Epoch 3/10
10/10 - 1s - loss: 274.2112 - loglik: -2.6812e+02 - logprior: -6.0958e+00
Epoch 4/10
10/10 - 1s - loss: 249.9107 - loglik: -2.4633e+02 - logprior: -3.5818e+00
Epoch 5/10
10/10 - 1s - loss: 240.7552 - loglik: -2.3848e+02 - logprior: -2.2766e+00
Epoch 6/10
10/10 - 1s - loss: 235.6525 - loglik: -2.3426e+02 - logprior: -1.3887e+00
Epoch 7/10
10/10 - 1s - loss: 234.4545 - loglik: -2.3371e+02 - logprior: -7.4696e-01
Epoch 8/10
10/10 - 1s - loss: 233.7710 - loglik: -2.3341e+02 - logprior: -3.5920e-01
Epoch 9/10
10/10 - 1s - loss: 232.3407 - loglik: -2.3226e+02 - logprior: -7.9703e-02
Epoch 10/10
10/10 - 1s - loss: 232.0883 - loglik: -2.3222e+02 - logprior: 0.1356
Fitted a model with MAP estimate = -232.0282
expansions: [(11, 1), (12, 3), (13, 5), (38, 3), (39, 2), (45, 1), (60, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (87, 4), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 302.0904 - loglik: -2.3283e+02 - logprior: -6.9257e+01
Epoch 2/2
10/10 - 2s - loss: 240.8099 - loglik: -2.1450e+02 - logprior: -2.6307e+01
Fitted a model with MAP estimate = -228.7939
expansions: [(0, 4)]
discards: [  0  13  14  47  89 110]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 266.1751 - loglik: -2.1189e+02 - logprior: -5.4287e+01
Epoch 2/2
10/10 - 1s - loss: 218.8262 - loglik: -2.0734e+02 - logprior: -1.1487e+01
Fitted a model with MAP estimate = -210.6272
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.0154 - loglik: -2.0872e+02 - logprior: -5.3292e+01
Epoch 2/10
10/10 - 1s - loss: 217.2089 - loglik: -2.0623e+02 - logprior: -1.0977e+01
Epoch 3/10
10/10 - 1s - loss: 207.4094 - loglik: -2.0516e+02 - logprior: -2.2527e+00
Epoch 4/10
10/10 - 1s - loss: 201.7839 - loglik: -2.0315e+02 - logprior: 1.3695
Epoch 5/10
10/10 - 1s - loss: 199.5230 - loglik: -2.0281e+02 - logprior: 3.2906
Epoch 6/10
10/10 - 1s - loss: 197.1831 - loglik: -2.0149e+02 - logprior: 4.3044
Epoch 7/10
10/10 - 1s - loss: 197.0987 - loglik: -2.0214e+02 - logprior: 5.0374
Epoch 8/10
10/10 - 1s - loss: 196.2566 - loglik: -2.0200e+02 - logprior: 5.7467
Epoch 9/10
10/10 - 1s - loss: 196.0992 - loglik: -2.0243e+02 - logprior: 6.3313
Epoch 10/10
10/10 - 1s - loss: 195.6687 - loglik: -2.0243e+02 - logprior: 6.7626
Fitted a model with MAP estimate = -195.3684
Time for alignment: 48.5500
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 410.4199 - loglik: -3.4850e+02 - logprior: -6.1916e+01
Epoch 2/10
10/10 - 1s - loss: 321.5530 - loglik: -3.0725e+02 - logprior: -1.4300e+01
Epoch 3/10
10/10 - 1s - loss: 273.8502 - loglik: -2.6765e+02 - logprior: -6.1997e+00
Epoch 4/10
10/10 - 1s - loss: 249.0151 - loglik: -2.4520e+02 - logprior: -3.8103e+00
Epoch 5/10
10/10 - 1s - loss: 238.9568 - loglik: -2.3645e+02 - logprior: -2.5113e+00
Epoch 6/10
10/10 - 1s - loss: 234.4543 - loglik: -2.3279e+02 - logprior: -1.6608e+00
Epoch 7/10
10/10 - 1s - loss: 233.7191 - loglik: -2.3260e+02 - logprior: -1.1231e+00
Epoch 8/10
10/10 - 1s - loss: 232.0649 - loglik: -2.3135e+02 - logprior: -7.1804e-01
Epoch 9/10
10/10 - 1s - loss: 231.7352 - loglik: -2.3128e+02 - logprior: -4.5171e-01
Epoch 10/10
10/10 - 1s - loss: 231.2196 - loglik: -2.3097e+02 - logprior: -2.4488e-01
Fitted a model with MAP estimate = -231.0440
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 301.5283 - loglik: -2.3229e+02 - logprior: -6.9242e+01
Epoch 2/2
10/10 - 1s - loss: 240.0520 - loglik: -2.1402e+02 - logprior: -2.6028e+01
Fitted a model with MAP estimate = -229.3431
expansions: [(0, 5)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 265.1829 - loglik: -2.1080e+02 - logprior: -5.4380e+01
Epoch 2/2
10/10 - 2s - loss: 218.0822 - loglik: -2.0643e+02 - logprior: -1.1655e+01
Fitted a model with MAP estimate = -209.8851
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 261.6641 - loglik: -2.0837e+02 - logprior: -5.3292e+01
Epoch 2/10
10/10 - 1s - loss: 218.2207 - loglik: -2.0720e+02 - logprior: -1.1024e+01
Epoch 3/10
10/10 - 1s - loss: 207.3462 - loglik: -2.0506e+02 - logprior: -2.2849e+00
Epoch 4/10
10/10 - 1s - loss: 201.9389 - loglik: -2.0328e+02 - logprior: 1.3391
Epoch 5/10
10/10 - 1s - loss: 199.1404 - loglik: -2.0241e+02 - logprior: 3.2683
Epoch 6/10
10/10 - 1s - loss: 198.0161 - loglik: -2.0230e+02 - logprior: 4.2889
Epoch 7/10
10/10 - 1s - loss: 196.9304 - loglik: -2.0195e+02 - logprior: 5.0187
Epoch 8/10
10/10 - 1s - loss: 196.5098 - loglik: -2.0223e+02 - logprior: 5.7191
Epoch 9/10
10/10 - 1s - loss: 195.7360 - loglik: -2.0205e+02 - logprior: 6.3152
Epoch 10/10
10/10 - 1s - loss: 195.7067 - loglik: -2.0243e+02 - logprior: 6.7251
Fitted a model with MAP estimate = -195.3304
Time for alignment: 50.1628
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6445 - loglik: -3.4873e+02 - logprior: -6.1919e+01
Epoch 2/10
10/10 - 1s - loss: 321.4448 - loglik: -3.0715e+02 - logprior: -1.4296e+01
Epoch 3/10
10/10 - 1s - loss: 275.0812 - loglik: -2.6890e+02 - logprior: -6.1777e+00
Epoch 4/10
10/10 - 1s - loss: 250.5317 - loglik: -2.4692e+02 - logprior: -3.6094e+00
Epoch 5/10
10/10 - 1s - loss: 240.0456 - loglik: -2.3760e+02 - logprior: -2.4492e+00
Epoch 6/10
10/10 - 1s - loss: 236.5412 - loglik: -2.3474e+02 - logprior: -1.8035e+00
Epoch 7/10
10/10 - 1s - loss: 234.0297 - loglik: -2.3287e+02 - logprior: -1.1552e+00
Epoch 8/10
10/10 - 1s - loss: 233.0843 - loglik: -2.3227e+02 - logprior: -8.0937e-01
Epoch 9/10
10/10 - 1s - loss: 232.1250 - loglik: -2.3158e+02 - logprior: -5.4858e-01
Epoch 10/10
10/10 - 1s - loss: 232.0657 - loglik: -2.3174e+02 - logprior: -3.2992e-01
Fitted a model with MAP estimate = -231.4694
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 2), (62, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 302.3239 - loglik: -2.3303e+02 - logprior: -6.9290e+01
Epoch 2/2
10/10 - 1s - loss: 240.8531 - loglik: -2.1464e+02 - logprior: -2.6210e+01
Fitted a model with MAP estimate = -229.6504
expansions: [(0, 5)]
discards: [  0  73 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.2337 - loglik: -2.1180e+02 - logprior: -5.4435e+01
Epoch 2/2
10/10 - 2s - loss: 218.2273 - loglik: -2.0653e+02 - logprior: -1.1702e+01
Fitted a model with MAP estimate = -210.0039
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.2360 - loglik: -2.0889e+02 - logprior: -5.3348e+01
Epoch 2/10
10/10 - 1s - loss: 218.0003 - loglik: -2.0693e+02 - logprior: -1.1074e+01
Epoch 3/10
10/10 - 1s - loss: 207.7573 - loglik: -2.0541e+02 - logprior: -2.3450e+00
Epoch 4/10
10/10 - 1s - loss: 201.3501 - loglik: -2.0263e+02 - logprior: 1.2751
Epoch 5/10
10/10 - 1s - loss: 199.4181 - loglik: -2.0262e+02 - logprior: 3.1991
Epoch 6/10
10/10 - 1s - loss: 197.4356 - loglik: -2.0166e+02 - logprior: 4.2218
Epoch 7/10
10/10 - 1s - loss: 196.9013 - loglik: -2.0187e+02 - logprior: 4.9733
Epoch 8/10
10/10 - 1s - loss: 196.3275 - loglik: -2.0202e+02 - logprior: 5.6875
Epoch 9/10
10/10 - 1s - loss: 195.9982 - loglik: -2.0227e+02 - logprior: 6.2671
Epoch 10/10
10/10 - 1s - loss: 195.8201 - loglik: -2.0251e+02 - logprior: 6.6892
Fitted a model with MAP estimate = -195.3854
Time for alignment: 48.0850
Computed alignments with likelihoods: ['-195.3047', '-196.5256', '-195.3684', '-195.3304', '-195.3854']
Best model has likelihood: -195.3047  (prior= 6.8711 )
time for generating output: 0.1470
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.938610662358643
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.3624 - loglik: -1.9318e+02 - logprior: -2.1870e+00
Epoch 2/10
22/22 - 1s - loss: 162.7173 - loglik: -1.6139e+02 - logprior: -1.3285e+00
Epoch 3/10
22/22 - 1s - loss: 155.2543 - loglik: -1.5384e+02 - logprior: -1.4165e+00
Epoch 4/10
22/22 - 1s - loss: 153.9899 - loglik: -1.5267e+02 - logprior: -1.3155e+00
Epoch 5/10
22/22 - 1s - loss: 153.2714 - loglik: -1.5196e+02 - logprior: -1.3149e+00
Epoch 6/10
22/22 - 1s - loss: 153.0078 - loglik: -1.5172e+02 - logprior: -1.2889e+00
Epoch 7/10
22/22 - 1s - loss: 152.8721 - loglik: -1.5159e+02 - logprior: -1.2849e+00
Epoch 8/10
22/22 - 1s - loss: 152.9730 - loglik: -1.5170e+02 - logprior: -1.2742e+00
Fitted a model with MAP estimate = -152.8432
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 156.3000 - loglik: -1.5346e+02 - logprior: -2.8446e+00
Epoch 2/2
22/22 - 1s - loss: 147.9591 - loglik: -1.4653e+02 - logprior: -1.4308e+00
Fitted a model with MAP estimate = -145.5950
expansions: [(0, 2)]
discards: [ 0  9 17 25 29 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.6415 - loglik: -1.4554e+02 - logprior: -2.1061e+00
Epoch 2/2
22/22 - 1s - loss: 144.5897 - loglik: -1.4358e+02 - logprior: -1.0061e+00
Fitted a model with MAP estimate = -144.7660
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.3822 - loglik: -1.4425e+02 - logprior: -1.1307e+00
Epoch 2/10
32/32 - 2s - loss: 142.4292 - loglik: -1.4163e+02 - logprior: -7.9946e-01
Epoch 3/10
32/32 - 2s - loss: 142.4108 - loglik: -1.4163e+02 - logprior: -7.7767e-01
Epoch 4/10
32/32 - 2s - loss: 142.0093 - loglik: -1.4124e+02 - logprior: -7.7200e-01
Epoch 5/10
32/32 - 2s - loss: 141.7894 - loglik: -1.4102e+02 - logprior: -7.6816e-01
Epoch 6/10
32/32 - 2s - loss: 141.3323 - loglik: -1.4056e+02 - logprior: -7.7024e-01
Epoch 7/10
32/32 - 2s - loss: 141.1494 - loglik: -1.4039e+02 - logprior: -7.6225e-01
Epoch 8/10
32/32 - 2s - loss: 141.1357 - loglik: -1.4037e+02 - logprior: -7.6494e-01
Epoch 9/10
32/32 - 2s - loss: 140.9631 - loglik: -1.4021e+02 - logprior: -7.5574e-01
Epoch 10/10
32/32 - 2s - loss: 141.2335 - loglik: -1.4048e+02 - logprior: -7.5328e-01
Fitted a model with MAP estimate = -140.9674
Time for alignment: 66.2020
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.7321 - loglik: -1.9355e+02 - logprior: -2.1862e+00
Epoch 2/10
22/22 - 1s - loss: 163.9534 - loglik: -1.6263e+02 - logprior: -1.3258e+00
Epoch 3/10
22/22 - 1s - loss: 155.7963 - loglik: -1.5435e+02 - logprior: -1.4437e+00
Epoch 4/10
22/22 - 1s - loss: 153.8545 - loglik: -1.5252e+02 - logprior: -1.3346e+00
Epoch 5/10
22/22 - 1s - loss: 153.0323 - loglik: -1.5169e+02 - logprior: -1.3437e+00
Epoch 6/10
22/22 - 1s - loss: 152.4741 - loglik: -1.5115e+02 - logprior: -1.3252e+00
Epoch 7/10
22/22 - 1s - loss: 152.4567 - loglik: -1.5114e+02 - logprior: -1.3129e+00
Epoch 8/10
22/22 - 1s - loss: 152.0815 - loglik: -1.5078e+02 - logprior: -1.3028e+00
Epoch 9/10
22/22 - 1s - loss: 152.2842 - loglik: -1.5099e+02 - logprior: -1.2990e+00
Fitted a model with MAP estimate = -152.7755
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (50, 2), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 155.9166 - loglik: -1.5308e+02 - logprior: -2.8338e+00
Epoch 2/2
22/22 - 1s - loss: 147.8435 - loglik: -1.4644e+02 - logprior: -1.4015e+00
Fitted a model with MAP estimate = -145.7135
expansions: [(0, 2)]
discards: [ 0  9 17 25 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.3698 - loglik: -1.4527e+02 - logprior: -2.0988e+00
Epoch 2/2
22/22 - 1s - loss: 144.6875 - loglik: -1.4369e+02 - logprior: -1.0024e+00
Fitted a model with MAP estimate = -144.8062
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 144.9490 - loglik: -1.4381e+02 - logprior: -1.1349e+00
Epoch 2/10
32/32 - 2s - loss: 142.9509 - loglik: -1.4215e+02 - logprior: -7.9884e-01
Epoch 3/10
32/32 - 2s - loss: 142.1728 - loglik: -1.4139e+02 - logprior: -7.8336e-01
Epoch 4/10
32/32 - 2s - loss: 142.3101 - loglik: -1.4154e+02 - logprior: -7.6977e-01
Fitted a model with MAP estimate = -141.6764
Time for alignment: 56.6837
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.6141 - loglik: -1.9343e+02 - logprior: -2.1889e+00
Epoch 2/10
22/22 - 1s - loss: 163.6443 - loglik: -1.6232e+02 - logprior: -1.3210e+00
Epoch 3/10
22/22 - 1s - loss: 155.8539 - loglik: -1.5445e+02 - logprior: -1.4066e+00
Epoch 4/10
22/22 - 1s - loss: 154.2815 - loglik: -1.5298e+02 - logprior: -1.3045e+00
Epoch 5/10
22/22 - 1s - loss: 153.5998 - loglik: -1.5229e+02 - logprior: -1.3078e+00
Epoch 6/10
22/22 - 1s - loss: 153.3349 - loglik: -1.5205e+02 - logprior: -1.2806e+00
Epoch 7/10
22/22 - 1s - loss: 153.0076 - loglik: -1.5174e+02 - logprior: -1.2698e+00
Epoch 8/10
22/22 - 1s - loss: 153.2106 - loglik: -1.5195e+02 - logprior: -1.2613e+00
Fitted a model with MAP estimate = -153.1536
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 1), (25, 1), (27, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 156.0104 - loglik: -1.5318e+02 - logprior: -2.8284e+00
Epoch 2/2
22/22 - 1s - loss: 147.7359 - loglik: -1.4634e+02 - logprior: -1.3936e+00
Fitted a model with MAP estimate = -145.5848
expansions: [(0, 2)]
discards: [ 0  9 17 22 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.4445 - loglik: -1.4534e+02 - logprior: -2.1001e+00
Epoch 2/2
22/22 - 1s - loss: 144.7735 - loglik: -1.4377e+02 - logprior: -1.0066e+00
Fitted a model with MAP estimate = -144.7861
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 144.8970 - loglik: -1.4377e+02 - logprior: -1.1310e+00
Epoch 2/10
32/32 - 2s - loss: 142.8960 - loglik: -1.4210e+02 - logprior: -7.9884e-01
Epoch 3/10
32/32 - 2s - loss: 142.5034 - loglik: -1.4173e+02 - logprior: -7.7705e-01
Epoch 4/10
32/32 - 2s - loss: 141.8598 - loglik: -1.4108e+02 - logprior: -7.7514e-01
Epoch 5/10
32/32 - 2s - loss: 141.7862 - loglik: -1.4102e+02 - logprior: -7.6407e-01
Epoch 6/10
32/32 - 2s - loss: 141.3870 - loglik: -1.4062e+02 - logprior: -7.6893e-01
Epoch 7/10
32/32 - 2s - loss: 141.1542 - loglik: -1.4039e+02 - logprior: -7.6686e-01
Epoch 8/10
32/32 - 2s - loss: 141.1802 - loglik: -1.4042e+02 - logprior: -7.6423e-01
Fitted a model with MAP estimate = -141.0137
Time for alignment: 60.0696
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.3930 - loglik: -1.9321e+02 - logprior: -2.1868e+00
Epoch 2/10
22/22 - 1s - loss: 163.1862 - loglik: -1.6186e+02 - logprior: -1.3245e+00
Epoch 3/10
22/22 - 1s - loss: 155.7879 - loglik: -1.5437e+02 - logprior: -1.4155e+00
Epoch 4/10
22/22 - 1s - loss: 154.1732 - loglik: -1.5285e+02 - logprior: -1.3194e+00
Epoch 5/10
22/22 - 1s - loss: 153.6593 - loglik: -1.5234e+02 - logprior: -1.3224e+00
Epoch 6/10
22/22 - 1s - loss: 153.3105 - loglik: -1.5202e+02 - logprior: -1.2952e+00
Epoch 7/10
22/22 - 1s - loss: 152.9831 - loglik: -1.5169e+02 - logprior: -1.2905e+00
Epoch 8/10
22/22 - 1s - loss: 153.2311 - loglik: -1.5195e+02 - logprior: -1.2813e+00
Fitted a model with MAP estimate = -153.1259
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 155.9291 - loglik: -1.5310e+02 - logprior: -2.8282e+00
Epoch 2/2
22/22 - 1s - loss: 147.6760 - loglik: -1.4628e+02 - logprior: -1.3968e+00
Fitted a model with MAP estimate = -145.6545
expansions: [(0, 2)]
discards: [ 0  9 17 22 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.3975 - loglik: -1.4530e+02 - logprior: -2.1015e+00
Epoch 2/2
22/22 - 1s - loss: 144.8210 - loglik: -1.4382e+02 - logprior: -1.0047e+00
Fitted a model with MAP estimate = -144.6685
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1306 - loglik: -1.4399e+02 - logprior: -1.1383e+00
Epoch 2/10
32/32 - 2s - loss: 142.7492 - loglik: -1.4195e+02 - logprior: -7.9989e-01
Epoch 3/10
32/32 - 2s - loss: 142.2267 - loglik: -1.4145e+02 - logprior: -7.8076e-01
Epoch 4/10
32/32 - 2s - loss: 142.0927 - loglik: -1.4132e+02 - logprior: -7.7055e-01
Epoch 5/10
32/32 - 2s - loss: 141.7636 - loglik: -1.4099e+02 - logprior: -7.7105e-01
Epoch 6/10
32/32 - 2s - loss: 141.4795 - loglik: -1.4070e+02 - logprior: -7.7558e-01
Epoch 7/10
32/32 - 2s - loss: 141.0468 - loglik: -1.4028e+02 - logprior: -7.6441e-01
Epoch 8/10
32/32 - 2s - loss: 141.1915 - loglik: -1.4043e+02 - logprior: -7.6616e-01
Fitted a model with MAP estimate = -141.0026
Time for alignment: 62.5251
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.2282 - loglik: -1.9304e+02 - logprior: -2.1880e+00
Epoch 2/10
22/22 - 1s - loss: 163.4362 - loglik: -1.6211e+02 - logprior: -1.3223e+00
Epoch 3/10
22/22 - 1s - loss: 155.6212 - loglik: -1.5420e+02 - logprior: -1.4209e+00
Epoch 4/10
22/22 - 1s - loss: 154.1758 - loglik: -1.5285e+02 - logprior: -1.3278e+00
Epoch 5/10
22/22 - 1s - loss: 153.6004 - loglik: -1.5227e+02 - logprior: -1.3285e+00
Epoch 6/10
22/22 - 1s - loss: 153.3440 - loglik: -1.5204e+02 - logprior: -1.2996e+00
Epoch 7/10
22/22 - 1s - loss: 153.0188 - loglik: -1.5173e+02 - logprior: -1.2890e+00
Epoch 8/10
22/22 - 1s - loss: 153.2569 - loglik: -1.5198e+02 - logprior: -1.2796e+00
Fitted a model with MAP estimate = -153.1104
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 155.9557 - loglik: -1.5312e+02 - logprior: -2.8309e+00
Epoch 2/2
22/22 - 1s - loss: 147.8623 - loglik: -1.4647e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -145.6865
expansions: [(0, 2)]
discards: [ 0  9 17 22 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.3705 - loglik: -1.4527e+02 - logprior: -2.1014e+00
Epoch 2/2
22/22 - 1s - loss: 145.0416 - loglik: -1.4404e+02 - logprior: -1.0048e+00
Fitted a model with MAP estimate = -144.7960
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1053 - loglik: -1.4397e+02 - logprior: -1.1355e+00
Epoch 2/10
32/32 - 2s - loss: 142.6727 - loglik: -1.4188e+02 - logprior: -7.9661e-01
Epoch 3/10
32/32 - 2s - loss: 142.3509 - loglik: -1.4157e+02 - logprior: -7.8083e-01
Epoch 4/10
32/32 - 2s - loss: 142.1096 - loglik: -1.4134e+02 - logprior: -7.7027e-01
Epoch 5/10
32/32 - 2s - loss: 141.8024 - loglik: -1.4103e+02 - logprior: -7.7186e-01
Epoch 6/10
32/32 - 2s - loss: 141.2693 - loglik: -1.4050e+02 - logprior: -7.6794e-01
Epoch 7/10
32/32 - 2s - loss: 141.2306 - loglik: -1.4047e+02 - logprior: -7.6086e-01
Epoch 8/10
32/32 - 2s - loss: 140.9772 - loglik: -1.4022e+02 - logprior: -7.6197e-01
Epoch 9/10
32/32 - 2s - loss: 141.1150 - loglik: -1.4036e+02 - logprior: -7.5898e-01
Fitted a model with MAP estimate = -140.9916
Time for alignment: 62.4594
Computed alignments with likelihoods: ['-140.9674', '-141.6764', '-141.0137', '-141.0026', '-140.9916']
Best model has likelihood: -140.9674  (prior= -0.7451 )
time for generating output: 0.1304
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8267815850325836
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 463.0389 - loglik: -4.5014e+02 - logprior: -1.2903e+01
Epoch 2/10
17/17 - 3s - loss: 320.7415 - loglik: -3.1961e+02 - logprior: -1.1288e+00
Epoch 3/10
17/17 - 4s - loss: 268.3111 - loglik: -2.6780e+02 - logprior: -5.1293e-01
Epoch 4/10
17/17 - 4s - loss: 258.1965 - loglik: -2.5804e+02 - logprior: -1.5406e-01
Epoch 5/10
17/17 - 4s - loss: 253.5121 - loglik: -2.5344e+02 - logprior: -7.2306e-02
Epoch 6/10
17/17 - 4s - loss: 253.4984 - loglik: -2.5347e+02 - logprior: -3.2956e-02
Epoch 7/10
17/17 - 4s - loss: 254.2870 - loglik: -2.5423e+02 - logprior: -5.2250e-02
Fitted a model with MAP estimate = -252.9169
expansions: [(0, 37), (24, 2), (25, 1), (50, 1), (59, 1), (80, 1), (82, 1), (92, 1), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 262.8315 - loglik: -2.4675e+02 - logprior: -1.6083e+01
Epoch 2/2
17/17 - 5s - loss: 220.0410 - loglik: -2.1831e+02 - logprior: -1.7270e+00
Fitted a model with MAP estimate = -209.3643
expansions: [(0, 18)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 253.5548 - loglik: -2.4027e+02 - logprior: -1.3286e+01
Epoch 2/2
17/17 - 4s - loss: 227.2544 - loglik: -2.2693e+02 - logprior: -3.2325e-01
Fitted a model with MAP estimate = -221.9354
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 248.3646 - loglik: -2.3669e+02 - logprior: -1.1677e+01
Epoch 2/10
17/17 - 5s - loss: 219.7003 - loglik: -2.1978e+02 - logprior: 0.0752
Epoch 3/10
17/17 - 5s - loss: 211.3514 - loglik: -2.1275e+02 - logprior: 1.3953
Epoch 4/10
17/17 - 5s - loss: 207.8867 - loglik: -2.0987e+02 - logprior: 1.9806
Epoch 5/10
17/17 - 5s - loss: 205.3280 - loglik: -2.0758e+02 - logprior: 2.2502
Epoch 6/10
17/17 - 5s - loss: 204.9974 - loglik: -2.0747e+02 - logprior: 2.4702
Epoch 7/10
17/17 - 5s - loss: 203.9326 - loglik: -2.0659e+02 - logprior: 2.6598
Epoch 8/10
17/17 - 5s - loss: 203.7516 - loglik: -2.0661e+02 - logprior: 2.8626
Epoch 9/10
17/17 - 5s - loss: 204.9175 - loglik: -2.0800e+02 - logprior: 3.0795
Fitted a model with MAP estimate = -203.6042
Time for alignment: 116.4102
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 461.4474 - loglik: -4.4847e+02 - logprior: -1.2978e+01
Epoch 2/10
17/17 - 4s - loss: 308.2980 - loglik: -3.0653e+02 - logprior: -1.7726e+00
Epoch 3/10
17/17 - 4s - loss: 260.7647 - loglik: -2.5923e+02 - logprior: -1.5339e+00
Epoch 4/10
17/17 - 4s - loss: 250.0647 - loglik: -2.4897e+02 - logprior: -1.0944e+00
Epoch 5/10
17/17 - 4s - loss: 247.4012 - loglik: -2.4630e+02 - logprior: -1.1016e+00
Epoch 6/10
17/17 - 4s - loss: 248.3459 - loglik: -2.4724e+02 - logprior: -1.1041e+00
Fitted a model with MAP estimate = -246.7148
expansions: [(29, 1), (49, 1), (80, 1), (89, 1), (96, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 270.7058 - loglik: -2.5326e+02 - logprior: -1.7450e+01
Epoch 2/2
17/17 - 4s - loss: 251.2455 - loglik: -2.4511e+02 - logprior: -6.1389e+00
Fitted a model with MAP estimate = -248.7345
expansions: [(0, 16)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 260.6542 - loglik: -2.4725e+02 - logprior: -1.3406e+01
Epoch 2/2
17/17 - 4s - loss: 240.2792 - loglik: -2.3818e+02 - logprior: -2.1010e+00
Fitted a model with MAP estimate = -235.9738
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 261.0868 - loglik: -2.4516e+02 - logprior: -1.5926e+01
Epoch 2/10
17/17 - 4s - loss: 243.9992 - loglik: -2.4229e+02 - logprior: -1.7092e+00
Epoch 3/10
17/17 - 4s - loss: 240.2566 - loglik: -2.4068e+02 - logprior: 0.4207
Epoch 4/10
17/17 - 4s - loss: 239.2051 - loglik: -2.4004e+02 - logprior: 0.8365
Epoch 5/10
17/17 - 4s - loss: 236.2643 - loglik: -2.3720e+02 - logprior: 0.9355
Epoch 6/10
17/17 - 4s - loss: 237.7068 - loglik: -2.3886e+02 - logprior: 1.1573
Fitted a model with MAP estimate = -236.1780
Time for alignment: 87.6409
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 461.3756 - loglik: -4.4842e+02 - logprior: -1.2957e+01
Epoch 2/10
17/17 - 4s - loss: 315.1656 - loglik: -3.1328e+02 - logprior: -1.8821e+00
Epoch 3/10
17/17 - 4s - loss: 262.8589 - loglik: -2.6107e+02 - logprior: -1.7915e+00
Epoch 4/10
17/17 - 4s - loss: 250.1456 - loglik: -2.4870e+02 - logprior: -1.4491e+00
Epoch 5/10
17/17 - 4s - loss: 246.3159 - loglik: -2.4478e+02 - logprior: -1.5401e+00
Epoch 6/10
17/17 - 4s - loss: 244.8963 - loglik: -2.4337e+02 - logprior: -1.5288e+00
Epoch 7/10
17/17 - 4s - loss: 243.8816 - loglik: -2.4235e+02 - logprior: -1.5358e+00
Epoch 8/10
17/17 - 3s - loss: 244.9925 - loglik: -2.4344e+02 - logprior: -1.5504e+00
Fitted a model with MAP estimate = -244.2137
expansions: [(30, 1), (32, 1), (44, 1), (50, 1), (59, 1), (80, 1), (82, 1), (97, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 267.1607 - loglik: -2.4976e+02 - logprior: -1.7404e+01
Epoch 2/2
17/17 - 4s - loss: 244.5661 - loglik: -2.3855e+02 - logprior: -6.0124e+00
Fitted a model with MAP estimate = -242.7394
expansions: [(0, 17)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 254.4876 - loglik: -2.4122e+02 - logprior: -1.3272e+01
Epoch 2/2
17/17 - 4s - loss: 235.9992 - loglik: -2.3404e+02 - logprior: -1.9625e+00
Fitted a model with MAP estimate = -229.7994
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 55 56 57 58 59 60 61 62
 63 64]
Re-initialized the encoder parameters.
Fitting a model of length 160 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 275.5533 - loglik: -2.6039e+02 - logprior: -1.5160e+01
Epoch 2/10
17/17 - 4s - loss: 257.4834 - loglik: -2.5622e+02 - logprior: -1.2594e+00
Epoch 3/10
17/17 - 4s - loss: 255.4089 - loglik: -2.5594e+02 - logprior: 0.5297
Epoch 4/10
17/17 - 4s - loss: 251.4507 - loglik: -2.5231e+02 - logprior: 0.8548
Epoch 5/10
17/17 - 4s - loss: 254.3702 - loglik: -2.5547e+02 - logprior: 1.0990
Fitted a model with MAP estimate = -250.9809
Time for alignment: 89.2600
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 463.5343 - loglik: -4.5064e+02 - logprior: -1.2893e+01
Epoch 2/10
17/17 - 4s - loss: 318.2642 - loglik: -3.1711e+02 - logprior: -1.1514e+00
Epoch 3/10
17/17 - 4s - loss: 267.5186 - loglik: -2.6700e+02 - logprior: -5.2224e-01
Epoch 4/10
17/17 - 4s - loss: 258.6360 - loglik: -2.5850e+02 - logprior: -1.3124e-01
Epoch 5/10
17/17 - 3s - loss: 253.8815 - loglik: -2.5384e+02 - logprior: -3.8874e-02
Epoch 6/10
17/17 - 4s - loss: 256.6177 - loglik: -2.5663e+02 - logprior: 0.0144
Fitted a model with MAP estimate = -254.2265
expansions: [(0, 38), (24, 2), (25, 1), (50, 1), (60, 1), (82, 1), (93, 1), (117, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 209 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 263.5966 - loglik: -2.4766e+02 - logprior: -1.5934e+01
Epoch 2/2
17/17 - 5s - loss: 218.0696 - loglik: -2.1640e+02 - logprior: -1.6656e+00
Fitted a model with MAP estimate = -208.5902
expansions: [(0, 17)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 254.6184 - loglik: -2.4138e+02 - logprior: -1.3235e+01
Epoch 2/2
17/17 - 4s - loss: 227.3276 - loglik: -2.2698e+02 - logprior: -3.4934e-01
Fitted a model with MAP estimate = -223.1855
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 248.4187 - loglik: -2.3672e+02 - logprior: -1.1695e+01
Epoch 2/10
17/17 - 5s - loss: 219.9096 - loglik: -2.1998e+02 - logprior: 0.0656
Epoch 3/10
17/17 - 5s - loss: 211.0766 - loglik: -2.1244e+02 - logprior: 1.3592
Epoch 4/10
17/17 - 5s - loss: 207.6234 - loglik: -2.0954e+02 - logprior: 1.9132
Epoch 5/10
17/17 - 5s - loss: 205.8952 - loglik: -2.0802e+02 - logprior: 2.1238
Epoch 6/10
17/17 - 5s - loss: 205.8090 - loglik: -2.0815e+02 - logprior: 2.3448
Epoch 7/10
17/17 - 5s - loss: 203.5694 - loglik: -2.0611e+02 - logprior: 2.5450
Epoch 8/10
17/17 - 5s - loss: 203.8354 - loglik: -2.0657e+02 - logprior: 2.7334
Fitted a model with MAP estimate = -203.6803
Time for alignment: 106.0070
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 462.6218 - loglik: -4.4970e+02 - logprior: -1.2921e+01
Epoch 2/10
17/17 - 4s - loss: 313.8165 - loglik: -3.1224e+02 - logprior: -1.5717e+00
Epoch 3/10
17/17 - 4s - loss: 261.9939 - loglik: -2.6061e+02 - logprior: -1.3816e+00
Epoch 4/10
17/17 - 4s - loss: 251.7070 - loglik: -2.5070e+02 - logprior: -1.0021e+00
Epoch 5/10
17/17 - 4s - loss: 248.1910 - loglik: -2.4716e+02 - logprior: -1.0277e+00
Epoch 6/10
17/17 - 4s - loss: 246.9541 - loglik: -2.4591e+02 - logprior: -1.0411e+00
Epoch 7/10
17/17 - 4s - loss: 245.8118 - loglik: -2.4474e+02 - logprior: -1.0738e+00
Epoch 8/10
17/17 - 4s - loss: 246.6953 - loglik: -2.4561e+02 - logprior: -1.0881e+00
Fitted a model with MAP estimate = -246.0552
expansions: [(29, 1), (49, 1), (80, 1), (89, 1), (108, 1), (136, 1)]
discards: [ 0 43]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 273.3104 - loglik: -2.5592e+02 - logprior: -1.7393e+01
Epoch 2/2
17/17 - 4s - loss: 252.4017 - loglik: -2.4649e+02 - logprior: -5.9158e+00
Fitted a model with MAP estimate = -250.6503
expansions: [(0, 17)]
discards: [ 0 43 44 45]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 263.7903 - loglik: -2.5040e+02 - logprior: -1.3388e+01
Epoch 2/2
17/17 - 4s - loss: 242.5333 - loglik: -2.4058e+02 - logprior: -1.9491e+00
Fitted a model with MAP estimate = -237.8328
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 270.4885 - loglik: -2.5436e+02 - logprior: -1.6131e+01
Epoch 2/10
17/17 - 4s - loss: 252.3446 - loglik: -2.5053e+02 - logprior: -1.8165e+00
Epoch 3/10
17/17 - 4s - loss: 247.9178 - loglik: -2.4851e+02 - logprior: 0.5880
Epoch 4/10
17/17 - 4s - loss: 247.5375 - loglik: -2.4849e+02 - logprior: 0.9541
Epoch 5/10
17/17 - 4s - loss: 247.5974 - loglik: -2.4870e+02 - logprior: 1.1022
Fitted a model with MAP estimate = -245.4684
Time for alignment: 89.7636
Computed alignments with likelihoods: ['-203.6042', '-235.9738', '-229.7994', '-203.6803', '-237.8328']
Best model has likelihood: -203.6042  (prior= 3.1904 )
time for generating output: 0.3306
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.44707608467826454
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.0368 - loglik: -2.2578e+02 - logprior: -2.0262e+01
Epoch 2/10
10/10 - 1s - loss: 215.9374 - loglik: -2.1051e+02 - logprior: -5.4317e+00
Epoch 3/10
10/10 - 1s - loss: 199.7829 - loglik: -1.9687e+02 - logprior: -2.9118e+00
Epoch 4/10
10/10 - 1s - loss: 191.5610 - loglik: -1.8924e+02 - logprior: -2.3163e+00
Epoch 5/10
10/10 - 1s - loss: 187.4196 - loglik: -1.8536e+02 - logprior: -2.0624e+00
Epoch 6/10
10/10 - 1s - loss: 185.4201 - loglik: -1.8373e+02 - logprior: -1.6912e+00
Epoch 7/10
10/10 - 1s - loss: 184.4150 - loglik: -1.8293e+02 - logprior: -1.4801e+00
Epoch 8/10
10/10 - 1s - loss: 184.2218 - loglik: -1.8278e+02 - logprior: -1.4416e+00
Epoch 9/10
10/10 - 1s - loss: 183.6446 - loglik: -1.8225e+02 - logprior: -1.3994e+00
Epoch 10/10
10/10 - 1s - loss: 183.3319 - loglik: -1.8194e+02 - logprior: -1.3924e+00
Fitted a model with MAP estimate = -183.2378
expansions: [(0, 2), (7, 2), (8, 2), (31, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (49, 1), (50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 210.4886 - loglik: -1.8391e+02 - logprior: -2.6576e+01
Epoch 2/2
10/10 - 1s - loss: 186.8646 - loglik: -1.7874e+02 - logprior: -8.1202e+00
Fitted a model with MAP estimate = -183.0521
expansions: []
discards: [ 0 10 12 53 55 63]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.5362 - loglik: -1.8034e+02 - logprior: -2.3195e+01
Epoch 2/2
10/10 - 1s - loss: 188.1922 - loglik: -1.7911e+02 - logprior: -9.0816e+00
Fitted a model with MAP estimate = -185.2081
expansions: [(0, 2), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 198.4360 - loglik: -1.7827e+02 - logprior: -2.0167e+01
Epoch 2/10
10/10 - 1s - loss: 182.7340 - loglik: -1.7748e+02 - logprior: -5.2555e+00
Epoch 3/10
10/10 - 1s - loss: 179.3811 - loglik: -1.7717e+02 - logprior: -2.2129e+00
Epoch 4/10
10/10 - 1s - loss: 177.5579 - loglik: -1.7624e+02 - logprior: -1.3152e+00
Epoch 5/10
10/10 - 1s - loss: 176.9420 - loglik: -1.7596e+02 - logprior: -9.8172e-01
Epoch 6/10
10/10 - 1s - loss: 176.6866 - loglik: -1.7608e+02 - logprior: -6.0985e-01
Epoch 7/10
10/10 - 1s - loss: 176.1269 - loglik: -1.7575e+02 - logprior: -3.7216e-01
Epoch 8/10
10/10 - 1s - loss: 176.1514 - loglik: -1.7586e+02 - logprior: -2.9172e-01
Fitted a model with MAP estimate = -175.9713
Time for alignment: 34.1542
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.0977 - loglik: -2.2584e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 215.9035 - loglik: -2.1047e+02 - logprior: -5.4323e+00
Epoch 3/10
10/10 - 1s - loss: 198.5676 - loglik: -1.9565e+02 - logprior: -2.9136e+00
Epoch 4/10
10/10 - 1s - loss: 189.7137 - loglik: -1.8742e+02 - logprior: -2.2929e+00
Epoch 5/10
10/10 - 1s - loss: 187.0063 - loglik: -1.8504e+02 - logprior: -1.9644e+00
Epoch 6/10
10/10 - 1s - loss: 185.1071 - loglik: -1.8355e+02 - logprior: -1.5617e+00
Epoch 7/10
10/10 - 1s - loss: 184.1470 - loglik: -1.8278e+02 - logprior: -1.3674e+00
Epoch 8/10
10/10 - 1s - loss: 184.1760 - loglik: -1.8284e+02 - logprior: -1.3386e+00
Fitted a model with MAP estimate = -183.7098
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 2), (43, 1), (44, 3), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.6287 - loglik: -1.8330e+02 - logprior: -2.6329e+01
Epoch 2/2
10/10 - 1s - loss: 187.8141 - loglik: -1.7978e+02 - logprior: -8.0353e+00
Fitted a model with MAP estimate = -183.5555
expansions: []
discards: [ 0 10 12 53 56]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.1831 - loglik: -1.8008e+02 - logprior: -2.3107e+01
Epoch 2/2
10/10 - 1s - loss: 188.2414 - loglik: -1.7916e+02 - logprior: -9.0839e+00
Fitted a model with MAP estimate = -185.1723
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.2358 - loglik: -1.7807e+02 - logprior: -2.0170e+01
Epoch 2/10
10/10 - 1s - loss: 182.8859 - loglik: -1.7757e+02 - logprior: -5.3203e+00
Epoch 3/10
10/10 - 1s - loss: 179.6794 - loglik: -1.7736e+02 - logprior: -2.3180e+00
Epoch 4/10
10/10 - 1s - loss: 177.9106 - loglik: -1.7648e+02 - logprior: -1.4321e+00
Epoch 5/10
10/10 - 1s - loss: 177.1584 - loglik: -1.7607e+02 - logprior: -1.0875e+00
Epoch 6/10
10/10 - 1s - loss: 176.8268 - loglik: -1.7611e+02 - logprior: -7.1591e-01
Epoch 7/10
10/10 - 1s - loss: 176.4907 - loglik: -1.7601e+02 - logprior: -4.7656e-01
Epoch 8/10
10/10 - 1s - loss: 176.0856 - loglik: -1.7568e+02 - logprior: -4.0505e-01
Epoch 9/10
10/10 - 1s - loss: 176.3103 - loglik: -1.7599e+02 - logprior: -3.2441e-01
Fitted a model with MAP estimate = -176.0821
Time for alignment: 33.1675
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.0843 - loglik: -2.2582e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 216.1510 - loglik: -2.1072e+02 - logprior: -5.4320e+00
Epoch 3/10
10/10 - 1s - loss: 198.5365 - loglik: -1.9561e+02 - logprior: -2.9263e+00
Epoch 4/10
10/10 - 1s - loss: 190.2824 - loglik: -1.8797e+02 - logprior: -2.3156e+00
Epoch 5/10
10/10 - 1s - loss: 186.7487 - loglik: -1.8472e+02 - logprior: -2.0266e+00
Epoch 6/10
10/10 - 1s - loss: 184.7892 - loglik: -1.8317e+02 - logprior: -1.6172e+00
Epoch 7/10
10/10 - 1s - loss: 184.2273 - loglik: -1.8283e+02 - logprior: -1.3928e+00
Epoch 8/10
10/10 - 1s - loss: 184.1794 - loglik: -1.8282e+02 - logprior: -1.3641e+00
Epoch 9/10
10/10 - 1s - loss: 183.4292 - loglik: -1.8211e+02 - logprior: -1.3170e+00
Epoch 10/10
10/10 - 1s - loss: 183.5093 - loglik: -1.8223e+02 - logprior: -1.2801e+00
Fitted a model with MAP estimate = -183.2873
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 3), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.9866 - loglik: -1.8331e+02 - logprior: -2.6674e+01
Epoch 2/2
10/10 - 1s - loss: 187.3878 - loglik: -1.7917e+02 - logprior: -8.2216e+00
Fitted a model with MAP estimate = -183.2905
expansions: []
discards: [ 0 10 12 56]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.8320 - loglik: -1.7963e+02 - logprior: -2.3203e+01
Epoch 2/2
10/10 - 1s - loss: 187.8194 - loglik: -1.7871e+02 - logprior: -9.1142e+00
Fitted a model with MAP estimate = -185.0667
expansions: [(0, 2)]
discards: [ 0 48]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 198.4807 - loglik: -1.7829e+02 - logprior: -2.0195e+01
Epoch 2/10
10/10 - 1s - loss: 183.0057 - loglik: -1.7768e+02 - logprior: -5.3258e+00
Epoch 3/10
10/10 - 1s - loss: 179.7072 - loglik: -1.7741e+02 - logprior: -2.2930e+00
Epoch 4/10
10/10 - 1s - loss: 178.1278 - loglik: -1.7674e+02 - logprior: -1.3906e+00
Epoch 5/10
10/10 - 1s - loss: 177.4659 - loglik: -1.7641e+02 - logprior: -1.0546e+00
Epoch 6/10
10/10 - 1s - loss: 176.8150 - loglik: -1.7613e+02 - logprior: -6.8381e-01
Epoch 7/10
10/10 - 1s - loss: 176.5253 - loglik: -1.7608e+02 - logprior: -4.4086e-01
Epoch 8/10
10/10 - 1s - loss: 176.4935 - loglik: -1.7613e+02 - logprior: -3.6459e-01
Epoch 9/10
10/10 - 1s - loss: 176.4979 - loglik: -1.7621e+02 - logprior: -2.8907e-01
Fitted a model with MAP estimate = -176.2574
Time for alignment: 34.7536
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 245.9167 - loglik: -2.2565e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 216.0507 - loglik: -2.1062e+02 - logprior: -5.4349e+00
Epoch 3/10
10/10 - 1s - loss: 199.8243 - loglik: -1.9690e+02 - logprior: -2.9259e+00
Epoch 4/10
10/10 - 1s - loss: 192.0044 - loglik: -1.8963e+02 - logprior: -2.3708e+00
Epoch 5/10
10/10 - 1s - loss: 186.9812 - loglik: -1.8488e+02 - logprior: -2.1039e+00
Epoch 6/10
10/10 - 1s - loss: 184.8844 - loglik: -1.8316e+02 - logprior: -1.7290e+00
Epoch 7/10
10/10 - 1s - loss: 183.8369 - loglik: -1.8230e+02 - logprior: -1.5370e+00
Epoch 8/10
10/10 - 1s - loss: 183.4126 - loglik: -1.8192e+02 - logprior: -1.4876e+00
Epoch 9/10
10/10 - 1s - loss: 183.1689 - loglik: -1.8176e+02 - logprior: -1.4131e+00
Epoch 10/10
10/10 - 1s - loss: 182.8454 - loglik: -1.8148e+02 - logprior: -1.3653e+00
Fitted a model with MAP estimate = -182.9062
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.0355 - loglik: -1.8341e+02 - logprior: -2.6626e+01
Epoch 2/2
10/10 - 1s - loss: 187.1336 - loglik: -1.7887e+02 - logprior: -8.2678e+00
Fitted a model with MAP estimate = -183.0302
expansions: []
discards: [ 0 10 12 55 63]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.6057 - loglik: -1.7936e+02 - logprior: -2.3246e+01
Epoch 2/2
10/10 - 1s - loss: 188.2101 - loglik: -1.7903e+02 - logprior: -9.1782e+00
Fitted a model with MAP estimate = -185.1058
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.5875 - loglik: -1.7827e+02 - logprior: -2.0313e+01
Epoch 2/10
10/10 - 1s - loss: 182.8725 - loglik: -1.7748e+02 - logprior: -5.3919e+00
Epoch 3/10
10/10 - 1s - loss: 179.6761 - loglik: -1.7735e+02 - logprior: -2.3260e+00
Epoch 4/10
10/10 - 1s - loss: 178.1684 - loglik: -1.7675e+02 - logprior: -1.4199e+00
Epoch 5/10
10/10 - 1s - loss: 177.2765 - loglik: -1.7620e+02 - logprior: -1.0780e+00
Epoch 6/10
10/10 - 1s - loss: 176.9028 - loglik: -1.7619e+02 - logprior: -7.1754e-01
Epoch 7/10
10/10 - 1s - loss: 176.7038 - loglik: -1.7628e+02 - logprior: -4.2669e-01
Epoch 8/10
10/10 - 1s - loss: 176.3293 - loglik: -1.7601e+02 - logprior: -3.2264e-01
Epoch 9/10
10/10 - 1s - loss: 176.2396 - loglik: -1.7598e+02 - logprior: -2.5844e-01
Epoch 10/10
10/10 - 1s - loss: 176.0014 - loglik: -1.7580e+02 - logprior: -2.0107e-01
Fitted a model with MAP estimate = -176.0684
Time for alignment: 36.4832
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 246.1329 - loglik: -2.2587e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 215.6616 - loglik: -2.1023e+02 - logprior: -5.4364e+00
Epoch 3/10
10/10 - 1s - loss: 198.6276 - loglik: -1.9570e+02 - logprior: -2.9300e+00
Epoch 4/10
10/10 - 1s - loss: 189.7801 - loglik: -1.8741e+02 - logprior: -2.3680e+00
Epoch 5/10
10/10 - 1s - loss: 186.1468 - loglik: -1.8409e+02 - logprior: -2.0591e+00
Epoch 6/10
10/10 - 1s - loss: 185.0718 - loglik: -1.8338e+02 - logprior: -1.6958e+00
Epoch 7/10
10/10 - 1s - loss: 184.0854 - loglik: -1.8256e+02 - logprior: -1.5232e+00
Epoch 8/10
10/10 - 1s - loss: 183.6667 - loglik: -1.8218e+02 - logprior: -1.4832e+00
Epoch 9/10
10/10 - 1s - loss: 183.4957 - loglik: -1.8204e+02 - logprior: -1.4513e+00
Epoch 10/10
10/10 - 1s - loss: 183.1798 - loglik: -1.8177e+02 - logprior: -1.4057e+00
Fitted a model with MAP estimate = -183.2758
expansions: [(0, 2), (8, 1), (9, 1), (23, 2), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 210.4646 - loglik: -1.8390e+02 - logprior: -2.6561e+01
Epoch 2/2
10/10 - 1s - loss: 187.5205 - loglik: -1.7934e+02 - logprior: -8.1812e+00
Fitted a model with MAP estimate = -183.4051
expansions: []
discards: [ 0 27 52 54 62]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.8136 - loglik: -1.8053e+02 - logprior: -2.3283e+01
Epoch 2/2
10/10 - 1s - loss: 188.6772 - loglik: -1.7952e+02 - logprior: -9.1571e+00
Fitted a model with MAP estimate = -185.8927
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 199.3211 - loglik: -1.7905e+02 - logprior: -2.0275e+01
Epoch 2/10
10/10 - 1s - loss: 183.7862 - loglik: -1.7841e+02 - logprior: -5.3810e+00
Epoch 3/10
10/10 - 1s - loss: 180.4489 - loglik: -1.7812e+02 - logprior: -2.3273e+00
Epoch 4/10
10/10 - 1s - loss: 179.0356 - loglik: -1.7764e+02 - logprior: -1.3973e+00
Epoch 5/10
10/10 - 1s - loss: 178.0471 - loglik: -1.7696e+02 - logprior: -1.0835e+00
Epoch 6/10
10/10 - 1s - loss: 177.8014 - loglik: -1.7707e+02 - logprior: -7.3577e-01
Epoch 7/10
10/10 - 1s - loss: 177.1357 - loglik: -1.7669e+02 - logprior: -4.4072e-01
Epoch 8/10
10/10 - 1s - loss: 177.3326 - loglik: -1.7701e+02 - logprior: -3.2490e-01
Fitted a model with MAP estimate = -176.9847
Time for alignment: 33.2633
Computed alignments with likelihoods: ['-175.9713', '-176.0821', '-176.2574', '-176.0684', '-176.9847']
Best model has likelihood: -175.9713  (prior= -0.2528 )
time for generating output: 0.1332
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6855721393034826
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.1536 - loglik: -1.8767e+02 - logprior: -1.1486e+01
Epoch 2/10
11/11 - 1s - loss: 157.8372 - loglik: -1.5457e+02 - logprior: -3.2643e+00
Epoch 3/10
11/11 - 1s - loss: 125.8223 - loglik: -1.2339e+02 - logprior: -2.4320e+00
Epoch 4/10
11/11 - 1s - loss: 111.8447 - loglik: -1.0982e+02 - logprior: -2.0272e+00
Epoch 5/10
11/11 - 1s - loss: 106.6793 - loglik: -1.0484e+02 - logprior: -1.8417e+00
Epoch 6/10
11/11 - 1s - loss: 104.8516 - loglik: -1.0298e+02 - logprior: -1.8679e+00
Epoch 7/10
11/11 - 1s - loss: 103.6188 - loglik: -1.0182e+02 - logprior: -1.8019e+00
Epoch 8/10
11/11 - 1s - loss: 103.8902 - loglik: -1.0211e+02 - logprior: -1.7842e+00
Fitted a model with MAP estimate = -103.4858
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 113.2090 - loglik: -9.9584e+01 - logprior: -1.3625e+01
Epoch 2/2
11/11 - 1s - loss: 96.1741 - loglik: -9.1987e+01 - logprior: -4.1871e+00
Fitted a model with MAP estimate = -93.1922
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.5091 - loglik: -9.2391e+01 - logprior: -1.3118e+01
Epoch 2/2
11/11 - 1s - loss: 96.5588 - loglik: -9.1044e+01 - logprior: -5.5150e+00
Fitted a model with MAP estimate = -94.2648
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 102.0828 - loglik: -9.1314e+01 - logprior: -1.0769e+01
Epoch 2/10
11/11 - 1s - loss: 93.0251 - loglik: -9.0080e+01 - logprior: -2.9448e+00
Epoch 3/10
11/11 - 1s - loss: 91.5357 - loglik: -8.9788e+01 - logprior: -1.7473e+00
Epoch 4/10
11/11 - 1s - loss: 91.3685 - loglik: -8.9827e+01 - logprior: -1.5415e+00
Epoch 5/10
11/11 - 1s - loss: 90.6015 - loglik: -8.9133e+01 - logprior: -1.4688e+00
Epoch 6/10
11/11 - 1s - loss: 90.5041 - loglik: -8.9186e+01 - logprior: -1.3178e+00
Epoch 7/10
11/11 - 1s - loss: 90.1074 - loglik: -8.8902e+01 - logprior: -1.2058e+00
Epoch 8/10
11/11 - 1s - loss: 90.1377 - loglik: -8.8945e+01 - logprior: -1.1923e+00
Fitted a model with MAP estimate = -90.0310
Time for alignment: 30.9931
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.0773 - loglik: -1.8759e+02 - logprior: -1.1489e+01
Epoch 2/10
11/11 - 1s - loss: 157.5262 - loglik: -1.5428e+02 - logprior: -3.2471e+00
Epoch 3/10
11/11 - 1s - loss: 124.4102 - loglik: -1.2198e+02 - logprior: -2.4294e+00
Epoch 4/10
11/11 - 1s - loss: 108.1747 - loglik: -1.0607e+02 - logprior: -2.1067e+00
Epoch 5/10
11/11 - 1s - loss: 103.5739 - loglik: -1.0163e+02 - logprior: -1.9489e+00
Epoch 6/10
11/11 - 1s - loss: 102.1992 - loglik: -1.0021e+02 - logprior: -1.9924e+00
Epoch 7/10
11/11 - 1s - loss: 101.1897 - loglik: -9.9278e+01 - logprior: -1.9122e+00
Epoch 8/10
11/11 - 1s - loss: 101.1249 - loglik: -9.9220e+01 - logprior: -1.9048e+00
Epoch 9/10
11/11 - 1s - loss: 100.6243 - loglik: -9.8725e+01 - logprior: -1.8997e+00
Epoch 10/10
11/11 - 1s - loss: 100.8817 - loglik: -9.8990e+01 - logprior: -1.8922e+00
Fitted a model with MAP estimate = -100.6250
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 112.3434 - loglik: -9.8631e+01 - logprior: -1.3713e+01
Epoch 2/2
11/11 - 1s - loss: 95.7554 - loglik: -9.1519e+01 - logprior: -4.2367e+00
Fitted a model with MAP estimate = -93.0429
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.3965 - loglik: -9.2274e+01 - logprior: -1.3122e+01
Epoch 2/2
11/11 - 1s - loss: 96.6133 - loglik: -9.1091e+01 - logprior: -5.5220e+00
Fitted a model with MAP estimate = -94.2261
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.7002 - loglik: -9.0909e+01 - logprior: -1.0792e+01
Epoch 2/10
11/11 - 1s - loss: 93.5834 - loglik: -9.0644e+01 - logprior: -2.9393e+00
Epoch 3/10
11/11 - 1s - loss: 91.6016 - loglik: -8.9853e+01 - logprior: -1.7489e+00
Epoch 4/10
11/11 - 1s - loss: 90.9538 - loglik: -8.9420e+01 - logprior: -1.5335e+00
Epoch 5/10
11/11 - 1s - loss: 90.8483 - loglik: -8.9378e+01 - logprior: -1.4703e+00
Epoch 6/10
11/11 - 1s - loss: 90.2404 - loglik: -8.8928e+01 - logprior: -1.3124e+00
Epoch 7/10
11/11 - 1s - loss: 90.1008 - loglik: -8.8900e+01 - logprior: -1.2008e+00
Epoch 8/10
11/11 - 1s - loss: 90.1052 - loglik: -8.8918e+01 - logprior: -1.1869e+00
Fitted a model with MAP estimate = -90.0278
Time for alignment: 31.5672
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 198.7876 - loglik: -1.8730e+02 - logprior: -1.1488e+01
Epoch 2/10
11/11 - 1s - loss: 158.8082 - loglik: -1.5556e+02 - logprior: -3.2446e+00
Epoch 3/10
11/11 - 1s - loss: 125.0471 - loglik: -1.2262e+02 - logprior: -2.4306e+00
Epoch 4/10
11/11 - 1s - loss: 109.6664 - loglik: -1.0758e+02 - logprior: -2.0884e+00
Epoch 5/10
11/11 - 1s - loss: 105.9230 - loglik: -1.0406e+02 - logprior: -1.8605e+00
Epoch 6/10
11/11 - 1s - loss: 104.2422 - loglik: -1.0236e+02 - logprior: -1.8791e+00
Epoch 7/10
11/11 - 1s - loss: 103.6901 - loglik: -1.0190e+02 - logprior: -1.7856e+00
Epoch 8/10
11/11 - 1s - loss: 103.5597 - loglik: -1.0179e+02 - logprior: -1.7686e+00
Epoch 9/10
11/11 - 1s - loss: 103.0425 - loglik: -1.0129e+02 - logprior: -1.7549e+00
Epoch 10/10
11/11 - 1s - loss: 103.3589 - loglik: -1.0161e+02 - logprior: -1.7454e+00
Fitted a model with MAP estimate = -103.0738
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 111.5139 - loglik: -9.7841e+01 - logprior: -1.3673e+01
Epoch 2/2
11/11 - 1s - loss: 95.5170 - loglik: -9.1376e+01 - logprior: -4.1410e+00
Fitted a model with MAP estimate = -93.0153
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.4342 - loglik: -9.2321e+01 - logprior: -1.3113e+01
Epoch 2/2
11/11 - 1s - loss: 96.5262 - loglik: -9.1032e+01 - logprior: -5.4943e+00
Fitted a model with MAP estimate = -94.2130
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6102 - loglik: -9.0828e+01 - logprior: -1.0782e+01
Epoch 2/10
11/11 - 1s - loss: 93.6054 - loglik: -9.0670e+01 - logprior: -2.9355e+00
Epoch 3/10
11/11 - 1s - loss: 91.7985 - loglik: -9.0057e+01 - logprior: -1.7412e+00
Epoch 4/10
11/11 - 1s - loss: 90.8759 - loglik: -8.9345e+01 - logprior: -1.5310e+00
Epoch 5/10
11/11 - 1s - loss: 90.6396 - loglik: -8.9177e+01 - logprior: -1.4623e+00
Epoch 6/10
11/11 - 1s - loss: 90.4590 - loglik: -8.9152e+01 - logprior: -1.3065e+00
Epoch 7/10
11/11 - 1s - loss: 90.1918 - loglik: -8.8999e+01 - logprior: -1.1927e+00
Epoch 8/10
11/11 - 1s - loss: 90.1788 - loglik: -8.9001e+01 - logprior: -1.1778e+00
Epoch 9/10
11/11 - 1s - loss: 90.0590 - loglik: -8.8887e+01 - logprior: -1.1717e+00
Epoch 10/10
11/11 - 1s - loss: 89.9271 - loglik: -8.8780e+01 - logprior: -1.1476e+00
Fitted a model with MAP estimate = -89.9117
Time for alignment: 33.8889
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.3858 - loglik: -1.8790e+02 - logprior: -1.1488e+01
Epoch 2/10
11/11 - 1s - loss: 157.2109 - loglik: -1.5396e+02 - logprior: -3.2461e+00
Epoch 3/10
11/11 - 1s - loss: 124.5393 - loglik: -1.2211e+02 - logprior: -2.4327e+00
Epoch 4/10
11/11 - 1s - loss: 109.9934 - loglik: -1.0793e+02 - logprior: -2.0672e+00
Epoch 5/10
11/11 - 1s - loss: 105.7372 - loglik: -1.0387e+02 - logprior: -1.8661e+00
Epoch 6/10
11/11 - 1s - loss: 103.9182 - loglik: -1.0202e+02 - logprior: -1.8979e+00
Epoch 7/10
11/11 - 1s - loss: 103.1086 - loglik: -1.0127e+02 - logprior: -1.8370e+00
Epoch 8/10
11/11 - 1s - loss: 103.0700 - loglik: -1.0123e+02 - logprior: -1.8365e+00
Epoch 9/10
11/11 - 1s - loss: 102.8012 - loglik: -1.0098e+02 - logprior: -1.8246e+00
Epoch 10/10
11/11 - 1s - loss: 102.2047 - loglik: -1.0038e+02 - logprior: -1.8198e+00
Fitted a model with MAP estimate = -102.4267
expansions: [(0, 3), (14, 1), (15, 1), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 111.9866 - loglik: -9.8321e+01 - logprior: -1.3666e+01
Epoch 2/2
11/11 - 1s - loss: 95.5578 - loglik: -9.1427e+01 - logprior: -4.1307e+00
Fitted a model with MAP estimate = -93.0168
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.5964 - loglik: -9.2489e+01 - logprior: -1.3107e+01
Epoch 2/2
11/11 - 1s - loss: 96.3163 - loglik: -9.0828e+01 - logprior: -5.4886e+00
Fitted a model with MAP estimate = -94.2012
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 101.5798 - loglik: -9.0812e+01 - logprior: -1.0768e+01
Epoch 2/10
11/11 - 1s - loss: 93.5694 - loglik: -9.0636e+01 - logprior: -2.9332e+00
Epoch 3/10
11/11 - 1s - loss: 92.0023 - loglik: -9.0254e+01 - logprior: -1.7482e+00
Epoch 4/10
11/11 - 1s - loss: 90.8209 - loglik: -8.9298e+01 - logprior: -1.5230e+00
Epoch 5/10
11/11 - 1s - loss: 90.6432 - loglik: -8.9184e+01 - logprior: -1.4588e+00
Epoch 6/10
11/11 - 1s - loss: 90.4218 - loglik: -8.9116e+01 - logprior: -1.3058e+00
Epoch 7/10
11/11 - 1s - loss: 90.2533 - loglik: -8.9061e+01 - logprior: -1.1921e+00
Epoch 8/10
11/11 - 1s - loss: 89.8304 - loglik: -8.8652e+01 - logprior: -1.1784e+00
Epoch 9/10
11/11 - 1s - loss: 90.1064 - loglik: -8.8930e+01 - logprior: -1.1767e+00
Fitted a model with MAP estimate = -89.9781
Time for alignment: 31.5080
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.9725 - loglik: -1.8748e+02 - logprior: -1.1488e+01
Epoch 2/10
11/11 - 1s - loss: 156.8714 - loglik: -1.5361e+02 - logprior: -3.2650e+00
Epoch 3/10
11/11 - 1s - loss: 122.7145 - loglik: -1.2028e+02 - logprior: -2.4310e+00
Epoch 4/10
11/11 - 1s - loss: 109.4223 - loglik: -1.0738e+02 - logprior: -2.0453e+00
Epoch 5/10
11/11 - 1s - loss: 105.4096 - loglik: -1.0354e+02 - logprior: -1.8711e+00
Epoch 6/10
11/11 - 1s - loss: 103.6281 - loglik: -1.0174e+02 - logprior: -1.8873e+00
Epoch 7/10
11/11 - 1s - loss: 102.9990 - loglik: -1.0118e+02 - logprior: -1.8211e+00
Epoch 8/10
11/11 - 1s - loss: 102.7162 - loglik: -1.0089e+02 - logprior: -1.8238e+00
Epoch 9/10
11/11 - 1s - loss: 102.3108 - loglik: -1.0049e+02 - logprior: -1.8195e+00
Epoch 10/10
11/11 - 1s - loss: 102.4906 - loglik: -1.0068e+02 - logprior: -1.8132e+00
Fitted a model with MAP estimate = -102.2114
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 112.4299 - loglik: -9.8759e+01 - logprior: -1.3671e+01
Epoch 2/2
11/11 - 1s - loss: 95.5869 - loglik: -9.1425e+01 - logprior: -4.1619e+00
Fitted a model with MAP estimate = -93.0476
expansions: []
discards: [ 0 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.5235 - loglik: -9.2413e+01 - logprior: -1.3110e+01
Epoch 2/2
11/11 - 1s - loss: 96.3432 - loglik: -9.0827e+01 - logprior: -5.5158e+00
Fitted a model with MAP estimate = -94.2301
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.7593 - loglik: -9.0972e+01 - logprior: -1.0787e+01
Epoch 2/10
11/11 - 1s - loss: 93.5087 - loglik: -9.0569e+01 - logprior: -2.9393e+00
Epoch 3/10
11/11 - 1s - loss: 92.0181 - loglik: -9.0268e+01 - logprior: -1.7505e+00
Epoch 4/10
11/11 - 1s - loss: 90.5976 - loglik: -8.9060e+01 - logprior: -1.5377e+00
Epoch 5/10
11/11 - 1s - loss: 90.7346 - loglik: -8.9263e+01 - logprior: -1.4716e+00
Fitted a model with MAP estimate = -90.4417
Time for alignment: 29.6051
Computed alignments with likelihoods: ['-90.0310', '-90.0278', '-89.9117', '-89.9781', '-90.4417']
Best model has likelihood: -89.9117  (prior= -1.1243 )
time for generating output: 0.1235
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.9717 - loglik: -1.7125e+02 - logprior: -7.1728e-01
Epoch 2/10
42/42 - 3s - loss: 80.7210 - loglik: -7.9760e+01 - logprior: -9.6088e-01
Epoch 3/10
42/42 - 3s - loss: 78.2532 - loglik: -7.7316e+01 - logprior: -9.3701e-01
Epoch 4/10
42/42 - 3s - loss: 77.1151 - loglik: -7.6183e+01 - logprior: -9.3184e-01
Epoch 5/10
42/42 - 3s - loss: 77.0400 - loglik: -7.6118e+01 - logprior: -9.2245e-01
Epoch 6/10
42/42 - 3s - loss: 76.7215 - loglik: -7.5807e+01 - logprior: -9.1475e-01
Epoch 7/10
42/42 - 3s - loss: 76.3278 - loglik: -7.5415e+01 - logprior: -9.1243e-01
Epoch 8/10
42/42 - 3s - loss: 76.0939 - loglik: -7.5185e+01 - logprior: -9.0915e-01
Epoch 9/10
42/42 - 3s - loss: 75.8084 - loglik: -7.4901e+01 - logprior: -9.0723e-01
Epoch 10/10
42/42 - 3s - loss: 75.7913 - loglik: -7.4885e+01 - logprior: -9.0643e-01
Fitted a model with MAP estimate = -75.6502
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (44, 1), (47, 1), (48, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 48.1608 - loglik: -4.7191e+01 - logprior: -9.7013e-01
Epoch 2/2
42/42 - 4s - loss: 35.6348 - loglik: -3.4895e+01 - logprior: -7.4008e-01
Fitted a model with MAP estimate = -34.4231
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 34.8900 - loglik: -3.4292e+01 - logprior: -5.9778e-01
Epoch 2/10
59/59 - 5s - loss: 33.9364 - loglik: -3.3441e+01 - logprior: -4.9561e-01
Epoch 3/10
59/59 - 5s - loss: 33.9835 - loglik: -3.3495e+01 - logprior: -4.8883e-01
Fitted a model with MAP estimate = -32.3461
Time for alignment: 111.9892
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 172.7290 - loglik: -1.7202e+02 - logprior: -7.1356e-01
Epoch 2/10
42/42 - 3s - loss: 82.6079 - loglik: -8.1648e+01 - logprior: -9.6022e-01
Epoch 3/10
42/42 - 3s - loss: 79.3642 - loglik: -7.8425e+01 - logprior: -9.3955e-01
Epoch 4/10
42/42 - 3s - loss: 79.1641 - loglik: -7.8232e+01 - logprior: -9.3243e-01
Epoch 5/10
42/42 - 3s - loss: 78.2805 - loglik: -7.7358e+01 - logprior: -9.2258e-01
Epoch 6/10
42/42 - 3s - loss: 78.0737 - loglik: -7.7156e+01 - logprior: -9.1721e-01
Epoch 7/10
42/42 - 3s - loss: 77.4951 - loglik: -7.6583e+01 - logprior: -9.1190e-01
Epoch 8/10
42/42 - 3s - loss: 77.5092 - loglik: -7.6600e+01 - logprior: -9.0931e-01
Fitted a model with MAP estimate = -77.2963
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 48.3033 - loglik: -4.7337e+01 - logprior: -9.6603e-01
Epoch 2/2
42/42 - 4s - loss: 35.8523 - loglik: -3.5113e+01 - logprior: -7.3922e-01
Fitted a model with MAP estimate = -34.4470
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 34.6410 - loglik: -3.4045e+01 - logprior: -5.9595e-01
Epoch 2/10
59/59 - 5s - loss: 34.0308 - loglik: -3.3535e+01 - logprior: -4.9623e-01
Epoch 3/10
59/59 - 5s - loss: 34.1535 - loglik: -3.3666e+01 - logprior: -4.8780e-01
Fitted a model with MAP estimate = -32.3504
Time for alignment: 104.6759
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.3151 - loglik: -1.7060e+02 - logprior: -7.1878e-01
Epoch 2/10
42/42 - 3s - loss: 81.4510 - loglik: -8.0488e+01 - logprior: -9.6257e-01
Epoch 3/10
42/42 - 3s - loss: 78.7789 - loglik: -7.7841e+01 - logprior: -9.3753e-01
Epoch 4/10
42/42 - 3s - loss: 77.9846 - loglik: -7.7055e+01 - logprior: -9.2993e-01
Epoch 5/10
42/42 - 3s - loss: 77.3012 - loglik: -7.6379e+01 - logprior: -9.2207e-01
Epoch 6/10
42/42 - 3s - loss: 76.9927 - loglik: -7.6077e+01 - logprior: -9.1548e-01
Epoch 7/10
42/42 - 3s - loss: 77.3090 - loglik: -7.6398e+01 - logprior: -9.1098e-01
Fitted a model with MAP estimate = -76.5215
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 48.0960 - loglik: -4.7131e+01 - logprior: -9.6492e-01
Epoch 2/2
42/42 - 4s - loss: 35.8383 - loglik: -3.5100e+01 - logprior: -7.3840e-01
Fitted a model with MAP estimate = -34.3783
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 34.3335 - loglik: -3.3734e+01 - logprior: -5.9961e-01
Epoch 2/10
59/59 - 5s - loss: 34.3570 - loglik: -3.3860e+01 - logprior: -4.9687e-01
Fitted a model with MAP estimate = -33.0404
Time for alignment: 96.7168
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.7266 - loglik: -1.7101e+02 - logprior: -7.1847e-01
Epoch 2/10
42/42 - 3s - loss: 80.9740 - loglik: -8.0010e+01 - logprior: -9.6353e-01
Epoch 3/10
42/42 - 3s - loss: 78.5274 - loglik: -7.7591e+01 - logprior: -9.3671e-01
Epoch 4/10
42/42 - 3s - loss: 78.0343 - loglik: -7.7104e+01 - logprior: -9.3026e-01
Epoch 5/10
42/42 - 3s - loss: 77.4507 - loglik: -7.6529e+01 - logprior: -9.2140e-01
Epoch 6/10
42/42 - 3s - loss: 76.9903 - loglik: -7.6076e+01 - logprior: -9.1481e-01
Epoch 7/10
42/42 - 3s - loss: 76.9241 - loglik: -7.6013e+01 - logprior: -9.1119e-01
Epoch 8/10
42/42 - 3s - loss: 76.1878 - loglik: -7.5280e+01 - logprior: -9.0771e-01
Epoch 9/10
42/42 - 3s - loss: 76.7700 - loglik: -7.5864e+01 - logprior: -9.0614e-01
Fitted a model with MAP estimate = -76.2966
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 47.7883 - loglik: -4.6814e+01 - logprior: -9.7402e-01
Epoch 2/2
42/42 - 4s - loss: 36.3393 - loglik: -3.5603e+01 - logprior: -7.3617e-01
Fitted a model with MAP estimate = -34.4450
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 34.7881 - loglik: -3.4188e+01 - logprior: -6.0022e-01
Epoch 2/10
59/59 - 5s - loss: 33.9244 - loglik: -3.3422e+01 - logprior: -5.0245e-01
Epoch 3/10
59/59 - 5s - loss: 33.9646 - loglik: -3.3473e+01 - logprior: -4.9150e-01
Fitted a model with MAP estimate = -32.3460
Time for alignment: 107.3775
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.6713 - loglik: -1.7096e+02 - logprior: -7.1592e-01
Epoch 2/10
42/42 - 3s - loss: 81.7102 - loglik: -8.0753e+01 - logprior: -9.5702e-01
Epoch 3/10
42/42 - 3s - loss: 78.5709 - loglik: -7.7634e+01 - logprior: -9.3703e-01
Epoch 4/10
42/42 - 3s - loss: 77.9799 - loglik: -7.7049e+01 - logprior: -9.3049e-01
Epoch 5/10
42/42 - 3s - loss: 77.5039 - loglik: -7.6582e+01 - logprior: -9.2226e-01
Epoch 6/10
42/42 - 3s - loss: 77.1199 - loglik: -7.6204e+01 - logprior: -9.1591e-01
Epoch 7/10
42/42 - 3s - loss: 76.6152 - loglik: -7.5703e+01 - logprior: -9.1214e-01
Epoch 8/10
42/42 - 3s - loss: 76.7925 - loglik: -7.5884e+01 - logprior: -9.0851e-01
Fitted a model with MAP estimate = -76.4225
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 48.0108 - loglik: -4.7042e+01 - logprior: -9.6873e-01
Epoch 2/2
42/42 - 4s - loss: 35.9124 - loglik: -3.5172e+01 - logprior: -7.4030e-01
Fitted a model with MAP estimate = -34.4872
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 34.6422 - loglik: -3.4040e+01 - logprior: -6.0254e-01
Epoch 2/10
59/59 - 5s - loss: 34.2681 - loglik: -3.3768e+01 - logprior: -4.9970e-01
Epoch 3/10
59/59 - 5s - loss: 33.8625 - loglik: -3.3371e+01 - logprior: -4.9198e-01
Epoch 4/10
59/59 - 5s - loss: 32.3030 - loglik: -3.1820e+01 - logprior: -4.8339e-01
Epoch 5/10
59/59 - 5s - loss: 32.2163 - loglik: -3.1744e+01 - logprior: -4.7269e-01
Epoch 6/10
59/59 - 5s - loss: 31.9852 - loglik: -3.1519e+01 - logprior: -4.6663e-01
Epoch 7/10
59/59 - 5s - loss: 31.7677 - loglik: -3.1303e+01 - logprior: -4.6480e-01
Epoch 8/10
59/59 - 5s - loss: 31.2939 - loglik: -3.0835e+01 - logprior: -4.5929e-01
Epoch 9/10
59/59 - 5s - loss: 31.3985 - loglik: -3.0945e+01 - logprior: -4.5350e-01
Fitted a model with MAP estimate = -31.0509
Time for alignment: 133.9385
Computed alignments with likelihoods: ['-32.3461', '-32.3504', '-33.0404', '-32.3460', '-31.0509']
Best model has likelihood: -31.0509  (prior= -0.4517 )
time for generating output: 0.1510
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.5757780784844384
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 474.1483 - loglik: -4.5083e+02 - logprior: -2.3320e+01
Epoch 2/10
14/14 - 3s - loss: 411.3598 - loglik: -4.0792e+02 - logprior: -3.4362e+00
Epoch 3/10
14/14 - 3s - loss: 374.2112 - loglik: -3.7275e+02 - logprior: -1.4598e+00
Epoch 4/10
14/14 - 3s - loss: 360.3636 - loglik: -3.5917e+02 - logprior: -1.1956e+00
Epoch 5/10
14/14 - 3s - loss: 356.3421 - loglik: -3.5521e+02 - logprior: -1.1350e+00
Epoch 6/10
14/14 - 3s - loss: 355.6389 - loglik: -3.5479e+02 - logprior: -8.5332e-01
Epoch 7/10
14/14 - 3s - loss: 354.6159 - loglik: -3.5389e+02 - logprior: -7.3075e-01
Epoch 8/10
14/14 - 3s - loss: 353.5345 - loglik: -3.5292e+02 - logprior: -6.1686e-01
Epoch 9/10
14/14 - 3s - loss: 354.6089 - loglik: -3.5406e+02 - logprior: -5.4906e-01
Fitted a model with MAP estimate = -353.5181
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (36, 2), (38, 2), (43, 1), (44, 2), (66, 1), (76, 1), (78, 2), (79, 3), (100, 1), (102, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0  1 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 386.1602 - loglik: -3.5835e+02 - logprior: -2.7809e+01
Epoch 2/2
14/14 - 3s - loss: 356.1891 - loglik: -3.4660e+02 - logprior: -9.5940e+00
Fitted a model with MAP estimate = -354.5739
expansions: [(0, 25), (15, 1), (56, 2), (79, 2)]
discards: [ 0 46 92]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 368.8409 - loglik: -3.4800e+02 - logprior: -2.0840e+01
Epoch 2/2
14/14 - 4s - loss: 346.0520 - loglik: -3.4361e+02 - logprior: -2.4370e+00
Fitted a model with MAP estimate = -342.5523
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 81
 82]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 368.8886 - loglik: -3.4401e+02 - logprior: -2.4882e+01
Epoch 2/10
14/14 - 3s - loss: 347.7016 - loglik: -3.4460e+02 - logprior: -3.1036e+00
Epoch 3/10
14/14 - 3s - loss: 342.4131 - loglik: -3.4306e+02 - logprior: 0.6500
Epoch 4/10
14/14 - 3s - loss: 340.4204 - loglik: -3.4219e+02 - logprior: 1.7709
Epoch 5/10
14/14 - 3s - loss: 339.1146 - loglik: -3.4148e+02 - logprior: 2.3694
Epoch 6/10
14/14 - 3s - loss: 339.3172 - loglik: -3.4203e+02 - logprior: 2.7086
Fitted a model with MAP estimate = -338.3463
Time for alignment: 79.0102
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 473.4841 - loglik: -4.5016e+02 - logprior: -2.3322e+01
Epoch 2/10
14/14 - 3s - loss: 410.1243 - loglik: -4.0667e+02 - logprior: -3.4545e+00
Epoch 3/10
14/14 - 3s - loss: 371.7494 - loglik: -3.7023e+02 - logprior: -1.5234e+00
Epoch 4/10
14/14 - 3s - loss: 359.2699 - loglik: -3.5808e+02 - logprior: -1.1894e+00
Epoch 5/10
14/14 - 3s - loss: 357.7182 - loglik: -3.5677e+02 - logprior: -9.4726e-01
Epoch 6/10
14/14 - 3s - loss: 354.4814 - loglik: -3.5367e+02 - logprior: -8.1390e-01
Epoch 7/10
14/14 - 3s - loss: 354.0226 - loglik: -3.5337e+02 - logprior: -6.5719e-01
Epoch 8/10
14/14 - 3s - loss: 354.8330 - loglik: -3.5423e+02 - logprior: -6.0660e-01
Fitted a model with MAP estimate = -353.9340
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (19, 1), (36, 2), (43, 1), (44, 2), (66, 2), (76, 1), (78, 1), (79, 3), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 382.3960 - loglik: -3.5460e+02 - logprior: -2.7801e+01
Epoch 2/2
14/14 - 3s - loss: 356.2656 - loglik: -3.4658e+02 - logprior: -9.6848e+00
Fitted a model with MAP estimate = -352.1604
expansions: [(0, 23), (16, 1), (56, 2), (79, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 365.5877 - loglik: -3.4479e+02 - logprior: -2.0797e+01
Epoch 2/2
14/14 - 4s - loss: 345.1999 - loglik: -3.4261e+02 - logprior: -2.5911e+00
Fitted a model with MAP estimate = -341.2003
expansions: [(79, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 34 35
 81 82 83]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 371.3867 - loglik: -3.4659e+02 - logprior: -2.4799e+01
Epoch 2/10
14/14 - 3s - loss: 347.1965 - loglik: -3.4413e+02 - logprior: -3.0686e+00
Epoch 3/10
14/14 - 3s - loss: 342.8303 - loglik: -3.4347e+02 - logprior: 0.6383
Epoch 4/10
14/14 - 3s - loss: 341.0858 - loglik: -3.4284e+02 - logprior: 1.7591
Epoch 5/10
14/14 - 3s - loss: 338.3027 - loglik: -3.4064e+02 - logprior: 2.3422
Epoch 6/10
14/14 - 3s - loss: 340.2224 - loglik: -3.4286e+02 - logprior: 2.6375
Fitted a model with MAP estimate = -338.6211
Time for alignment: 74.6827
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.1168 - loglik: -4.5080e+02 - logprior: -2.3317e+01
Epoch 2/10
14/14 - 3s - loss: 411.3129 - loglik: -4.0784e+02 - logprior: -3.4759e+00
Epoch 3/10
14/14 - 3s - loss: 373.9694 - loglik: -3.7240e+02 - logprior: -1.5684e+00
Epoch 4/10
14/14 - 3s - loss: 361.5720 - loglik: -3.6029e+02 - logprior: -1.2787e+00
Epoch 5/10
14/14 - 3s - loss: 357.1227 - loglik: -3.5598e+02 - logprior: -1.1407e+00
Epoch 6/10
14/14 - 3s - loss: 354.9985 - loglik: -3.5406e+02 - logprior: -9.3584e-01
Epoch 7/10
14/14 - 3s - loss: 355.4478 - loglik: -3.5465e+02 - logprior: -8.0096e-01
Fitted a model with MAP estimate = -354.6138
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (18, 1), (35, 2), (43, 1), (44, 2), (66, 1), (67, 2), (70, 1), (79, 2), (80, 3), (81, 1), (100, 1), (102, 2), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46 47]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 384.4720 - loglik: -3.5660e+02 - logprior: -2.7867e+01
Epoch 2/2
14/14 - 3s - loss: 356.7665 - loglik: -3.4707e+02 - logprior: -9.7001e+00
Fitted a model with MAP estimate = -352.8926
expansions: [(0, 25), (16, 1), (56, 2), (58, 1)]
discards: [  0  79  97 123]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 367.1215 - loglik: -3.4637e+02 - logprior: -2.0753e+01
Epoch 2/2
14/14 - 4s - loss: 345.1692 - loglik: -3.4271e+02 - logprior: -2.4631e+00
Fitted a model with MAP estimate = -341.6142
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 43 81 82 83 84]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 371.7093 - loglik: -3.4730e+02 - logprior: -2.4412e+01
Epoch 2/10
14/14 - 3s - loss: 347.3348 - loglik: -3.4444e+02 - logprior: -2.8937e+00
Epoch 3/10
14/14 - 3s - loss: 343.1690 - loglik: -3.4380e+02 - logprior: 0.6316
Epoch 4/10
14/14 - 3s - loss: 341.7932 - loglik: -3.4352e+02 - logprior: 1.7306
Epoch 5/10
14/14 - 3s - loss: 341.8308 - loglik: -3.4420e+02 - logprior: 2.3695
Fitted a model with MAP estimate = -340.0200
Time for alignment: 71.4763
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 473.6665 - loglik: -4.5035e+02 - logprior: -2.3312e+01
Epoch 2/10
14/14 - 3s - loss: 410.4957 - loglik: -4.0705e+02 - logprior: -3.4423e+00
Epoch 3/10
14/14 - 3s - loss: 374.2605 - loglik: -3.7273e+02 - logprior: -1.5293e+00
Epoch 4/10
14/14 - 3s - loss: 362.1049 - loglik: -3.6078e+02 - logprior: -1.3225e+00
Epoch 5/10
14/14 - 3s - loss: 356.1152 - loglik: -3.5492e+02 - logprior: -1.1964e+00
Epoch 6/10
14/14 - 3s - loss: 356.0557 - loglik: -3.5510e+02 - logprior: -9.5458e-01
Epoch 7/10
14/14 - 3s - loss: 354.1663 - loglik: -3.5328e+02 - logprior: -8.8418e-01
Epoch 8/10
14/14 - 3s - loss: 354.3673 - loglik: -3.5352e+02 - logprior: -8.4849e-01
Fitted a model with MAP estimate = -353.7983
expansions: [(5, 1), (11, 1), (16, 5), (17, 1), (19, 1), (36, 1), (38, 2), (43, 1), (44, 2), (66, 1), (67, 2), (77, 1), (79, 2), (80, 3), (81, 1), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 382.3533 - loglik: -3.5450e+02 - logprior: -2.7851e+01
Epoch 2/2
14/14 - 3s - loss: 354.7464 - loglik: -3.4504e+02 - logprior: -9.7043e+00
Fitted a model with MAP estimate = -351.6287
expansions: [(0, 24), (16, 1), (57, 2)]
discards: [ 0 47 80]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 367.2047 - loglik: -3.4638e+02 - logprior: -2.0828e+01
Epoch 2/2
14/14 - 4s - loss: 344.4203 - loglik: -3.4190e+02 - logprior: -2.5210e+00
Fitted a model with MAP estimate = -341.6031
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  81  82 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 371.1670 - loglik: -3.4657e+02 - logprior: -2.4595e+01
Epoch 2/10
14/14 - 3s - loss: 348.1537 - loglik: -3.4515e+02 - logprior: -3.0081e+00
Epoch 3/10
14/14 - 3s - loss: 344.2314 - loglik: -3.4481e+02 - logprior: 0.5750
Epoch 4/10
14/14 - 3s - loss: 341.2589 - loglik: -3.4296e+02 - logprior: 1.7016
Epoch 5/10
14/14 - 3s - loss: 339.4703 - loglik: -3.4178e+02 - logprior: 2.3060
Epoch 6/10
14/14 - 3s - loss: 340.3035 - loglik: -3.4296e+02 - logprior: 2.6604
Fitted a model with MAP estimate = -339.2480
Time for alignment: 74.5290
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 474.7214 - loglik: -4.5140e+02 - logprior: -2.3320e+01
Epoch 2/10
14/14 - 3s - loss: 408.3998 - loglik: -4.0491e+02 - logprior: -3.4862e+00
Epoch 3/10
14/14 - 3s - loss: 371.6742 - loglik: -3.6993e+02 - logprior: -1.7403e+00
Epoch 4/10
14/14 - 3s - loss: 360.2741 - loglik: -3.5878e+02 - logprior: -1.4946e+00
Epoch 5/10
14/14 - 3s - loss: 357.8201 - loglik: -3.5665e+02 - logprior: -1.1663e+00
Epoch 6/10
14/14 - 3s - loss: 354.4484 - loglik: -3.5349e+02 - logprior: -9.5505e-01
Epoch 7/10
14/14 - 3s - loss: 356.3290 - loglik: -3.5550e+02 - logprior: -8.2524e-01
Fitted a model with MAP estimate = -354.6167
expansions: [(3, 1), (5, 1), (16, 5), (17, 1), (19, 1), (35, 2), (41, 1), (44, 2), (57, 1), (65, 1), (66, 2), (76, 1), (78, 2), (79, 3), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 382.1133 - loglik: -3.5432e+02 - logprior: -2.7789e+01
Epoch 2/2
14/14 - 3s - loss: 355.9940 - loglik: -3.4628e+02 - logprior: -9.7173e+00
Fitted a model with MAP estimate = -352.0599
expansions: [(0, 21), (16, 1)]
discards: [ 0 17 18 79 95 98]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 369.1722 - loglik: -3.4827e+02 - logprior: -2.0901e+01
Epoch 2/2
14/14 - 4s - loss: 347.1347 - loglik: -3.4439e+02 - logprior: -2.7486e+00
Fitted a model with MAP estimate = -344.4302
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 75]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 372.2036 - loglik: -3.4739e+02 - logprior: -2.4810e+01
Epoch 2/10
14/14 - 3s - loss: 347.5616 - loglik: -3.4451e+02 - logprior: -3.0563e+00
Epoch 3/10
14/14 - 3s - loss: 344.1590 - loglik: -3.4481e+02 - logprior: 0.6521
Epoch 4/10
14/14 - 3s - loss: 342.3543 - loglik: -3.4414e+02 - logprior: 1.7881
Epoch 5/10
14/14 - 3s - loss: 340.2261 - loglik: -3.4261e+02 - logprior: 2.3862
Epoch 6/10
14/14 - 3s - loss: 341.2198 - loglik: -3.4395e+02 - logprior: 2.7273
Fitted a model with MAP estimate = -340.1322
Time for alignment: 70.9620
Computed alignments with likelihoods: ['-338.3463', '-338.6211', '-340.0200', '-339.2480', '-340.1322']
Best model has likelihood: -338.3463  (prior= 2.8598 )
time for generating output: 0.1720
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9592265657839428
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 857.5469 - loglik: -8.5485e+02 - logprior: -2.6968e+00
Epoch 2/10
29/29 - 18s - loss: 673.6641 - loglik: -6.7202e+02 - logprior: -1.6482e+00
Epoch 3/10
29/29 - 19s - loss: 650.2457 - loglik: -6.4829e+02 - logprior: -1.9568e+00
Epoch 4/10
29/29 - 19s - loss: 643.6851 - loglik: -6.4176e+02 - logprior: -1.9218e+00
Epoch 5/10
29/29 - 19s - loss: 642.3835 - loglik: -6.4049e+02 - logprior: -1.8896e+00
Epoch 6/10
29/29 - 19s - loss: 641.8158 - loglik: -6.3994e+02 - logprior: -1.8747e+00
Epoch 7/10
29/29 - 19s - loss: 641.7465 - loglik: -6.3987e+02 - logprior: -1.8730e+00
Epoch 8/10
29/29 - 19s - loss: 643.1525 - loglik: -6.4128e+02 - logprior: -1.8716e+00
Fitted a model with MAP estimate = -641.2748
expansions: [(16, 1), (25, 2), (28, 1), (29, 1), (30, 1), (37, 1), (39, 1), (41, 2), (48, 2), (49, 2), (76, 1), (86, 1), (88, 1), (89, 3), (120, 2), (121, 1), (123, 1), (124, 2), (125, 1), (142, 2), (144, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (182, 1), (184, 2), (185, 1), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (233, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 625.2336 - loglik: -6.2025e+02 - logprior: -4.9857e+00
Epoch 2/2
29/29 - 25s - loss: 598.9759 - loglik: -5.9685e+02 - logprior: -2.1267e+00
Fitted a model with MAP estimate = -592.8656
expansions: [(0, 2), (140, 1)]
discards: [  0  48  59  60 150 169 262 301 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 602.1420 - loglik: -5.9961e+02 - logprior: -2.5326e+00
Epoch 2/2
29/29 - 25s - loss: 591.8204 - loglik: -5.9193e+02 - logprior: 0.1128
Fitted a model with MAP estimate = -592.3351
expansions: [(322, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 601.7440 - loglik: -5.9749e+02 - logprior: -4.2572e+00
Epoch 2/10
29/29 - 25s - loss: 595.0298 - loglik: -5.9479e+02 - logprior: -2.3593e-01
Epoch 3/10
29/29 - 25s - loss: 591.5317 - loglik: -5.9184e+02 - logprior: 0.3045
Epoch 4/10
29/29 - 25s - loss: 589.6937 - loglik: -5.9053e+02 - logprior: 0.8351
Epoch 5/10
29/29 - 25s - loss: 590.7520 - loglik: -5.9160e+02 - logprior: 0.8442
Fitted a model with MAP estimate = -589.4154
Time for alignment: 488.9196
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 856.2269 - loglik: -8.5353e+02 - logprior: -2.7003e+00
Epoch 2/10
29/29 - 19s - loss: 669.6430 - loglik: -6.6797e+02 - logprior: -1.6706e+00
Epoch 3/10
29/29 - 19s - loss: 643.5078 - loglik: -6.4160e+02 - logprior: -1.9097e+00
Epoch 4/10
29/29 - 19s - loss: 635.2573 - loglik: -6.3327e+02 - logprior: -1.9844e+00
Epoch 5/10
29/29 - 19s - loss: 635.0183 - loglik: -6.3302e+02 - logprior: -1.9987e+00
Epoch 6/10
29/29 - 19s - loss: 632.9824 - loglik: -6.3099e+02 - logprior: -1.9897e+00
Epoch 7/10
29/29 - 19s - loss: 633.9896 - loglik: -6.3200e+02 - logprior: -1.9862e+00
Fitted a model with MAP estimate = -632.3989
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 2), (42, 1), (48, 2), (49, 1), (50, 1), (73, 1), (76, 1), (87, 1), (88, 1), (89, 1), (94, 1), (117, 8), (121, 3), (123, 1), (124, 1), (128, 1), (142, 1), (144, 1), (151, 1), (154, 1), (155, 1), (163, 1), (172, 1), (182, 1), (183, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (216, 1), (218, 2), (219, 2), (249, 1), (250, 1), (252, 2), (256, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 618.6929 - loglik: -6.1370e+02 - logprior: -4.9942e+00
Epoch 2/2
29/29 - 26s - loss: 591.7277 - loglik: -5.8957e+02 - logprior: -2.1603e+00
Fitted a model with MAP estimate = -586.6940
expansions: [(0, 2)]
discards: [  0  62 140 141 142 143 144 145 149 306 333]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 595.0790 - loglik: -5.9243e+02 - logprior: -2.6525e+00
Epoch 2/2
29/29 - 25s - loss: 588.0032 - loglik: -5.8770e+02 - logprior: -3.0800e-01
Fitted a model with MAP estimate = -585.5382
expansions: [(138, 5)]
discards: [  0 328]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 593.5973 - loglik: -5.8944e+02 - logprior: -4.1607e+00
Epoch 2/10
29/29 - 26s - loss: 588.7512 - loglik: -5.8845e+02 - logprior: -2.9714e-01
Epoch 3/10
29/29 - 26s - loss: 583.4851 - loglik: -5.8399e+02 - logprior: 0.5079
Epoch 4/10
29/29 - 25s - loss: 583.0113 - loglik: -5.8382e+02 - logprior: 0.8061
Epoch 5/10
29/29 - 26s - loss: 581.3761 - loglik: -5.8222e+02 - logprior: 0.8456
Epoch 6/10
29/29 - 25s - loss: 581.2756 - loglik: -5.8254e+02 - logprior: 1.2637
Epoch 7/10
29/29 - 26s - loss: 581.8844 - loglik: -5.8317e+02 - logprior: 1.2818
Fitted a model with MAP estimate = -580.7262
Time for alignment: 527.3808
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 855.7798 - loglik: -8.5307e+02 - logprior: -2.7113e+00
Epoch 2/10
29/29 - 19s - loss: 668.3826 - loglik: -6.6663e+02 - logprior: -1.7502e+00
Epoch 3/10
29/29 - 19s - loss: 641.4581 - loglik: -6.3943e+02 - logprior: -2.0280e+00
Epoch 4/10
29/29 - 19s - loss: 634.8774 - loglik: -6.3282e+02 - logprior: -2.0613e+00
Epoch 5/10
29/29 - 19s - loss: 632.0871 - loglik: -6.3000e+02 - logprior: -2.0868e+00
Epoch 6/10
29/29 - 19s - loss: 629.5239 - loglik: -6.2738e+02 - logprior: -2.1435e+00
Epoch 7/10
29/29 - 19s - loss: 628.1855 - loglik: -6.2602e+02 - logprior: -2.1684e+00
Epoch 8/10
29/29 - 19s - loss: 627.6188 - loglik: -6.2546e+02 - logprior: -2.1631e+00
Epoch 9/10
29/29 - 19s - loss: 627.5193 - loglik: -6.2535e+02 - logprior: -2.1657e+00
Epoch 10/10
29/29 - 19s - loss: 628.3105 - loglik: -6.2615e+02 - logprior: -2.1572e+00
Fitted a model with MAP estimate = -627.3950
expansions: [(16, 1), (22, 1), (24, 2), (29, 1), (30, 1), (31, 1), (35, 1), (36, 1), (38, 1), (47, 1), (48, 1), (72, 2), (85, 1), (86, 1), (87, 1), (88, 1), (121, 2), (123, 1), (125, 1), (128, 1), (141, 3), (151, 1), (153, 1), (154, 1), (155, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (262, 1), (264, 1), (269, 2), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 612.1260 - loglik: -6.0712e+02 - logprior: -5.0094e+00
Epoch 2/2
29/29 - 25s - loss: 590.4547 - loglik: -5.8839e+02 - logprior: -2.0627e+00
Fitted a model with MAP estimate = -586.0937
expansions: [(0, 2)]
discards: [  0  39 165 258 297]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 592.7985 - loglik: -5.8995e+02 - logprior: -2.8446e+00
Epoch 2/2
29/29 - 25s - loss: 583.9540 - loglik: -5.8390e+02 - logprior: -5.4658e-02
Fitted a model with MAP estimate = -584.0787
expansions: []
discards: [  0 324]
Re-initialized the encoder parameters.
Fitting a model of length 333 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 594.7308 - loglik: -5.9056e+02 - logprior: -4.1724e+00
Epoch 2/10
29/29 - 24s - loss: 585.0717 - loglik: -5.8488e+02 - logprior: -1.9484e-01
Epoch 3/10
29/29 - 25s - loss: 585.4136 - loglik: -5.8594e+02 - logprior: 0.5241
Fitted a model with MAP estimate = -582.9625
Time for alignment: 474.7230
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 856.4301 - loglik: -8.5373e+02 - logprior: -2.6975e+00
Epoch 2/10
29/29 - 19s - loss: 676.3229 - loglik: -6.7471e+02 - logprior: -1.6081e+00
Epoch 3/10
29/29 - 19s - loss: 646.5090 - loglik: -6.4457e+02 - logprior: -1.9381e+00
Epoch 4/10
29/29 - 19s - loss: 642.5634 - loglik: -6.4067e+02 - logprior: -1.8942e+00
Epoch 5/10
29/29 - 19s - loss: 640.2170 - loglik: -6.3836e+02 - logprior: -1.8620e+00
Epoch 6/10
29/29 - 19s - loss: 638.2300 - loglik: -6.3635e+02 - logprior: -1.8764e+00
Epoch 7/10
29/29 - 19s - loss: 639.2645 - loglik: -6.3738e+02 - logprior: -1.8821e+00
Fitted a model with MAP estimate = -637.2426
expansions: [(16, 1), (17, 1), (24, 2), (29, 1), (30, 1), (31, 1), (41, 1), (48, 2), (49, 1), (50, 1), (64, 1), (72, 1), (76, 1), (89, 2), (90, 3), (119, 2), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (128, 1), (142, 2), (144, 1), (153, 1), (155, 1), (156, 1), (163, 1), (172, 1), (173, 1), (185, 2), (186, 1), (191, 1), (192, 1), (205, 1), (216, 1), (218, 2), (219, 2), (240, 1), (250, 1), (251, 3), (258, 1), (260, 2), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 344 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 622.9451 - loglik: -6.1784e+02 - logprior: -5.1082e+00
Epoch 2/2
29/29 - 26s - loss: 595.7702 - loglik: -5.9353e+02 - logprior: -2.2390e+00
Fitted a model with MAP estimate = -592.9451
expansions: [(0, 2)]
discards: [  0  56 104 107 171 329 333]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 599.5454 - loglik: -5.9659e+02 - logprior: -2.9553e+00
Epoch 2/2
29/29 - 25s - loss: 593.0613 - loglik: -5.9286e+02 - logprior: -2.0551e-01
Fitted a model with MAP estimate = -591.7853
expansions: []
discards: [  0 297]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 602.4579 - loglik: -5.9804e+02 - logprior: -4.4163e+00
Epoch 2/10
29/29 - 25s - loss: 595.0115 - loglik: -5.9449e+02 - logprior: -5.2566e-01
Epoch 3/10
29/29 - 25s - loss: 590.9049 - loglik: -5.9137e+02 - logprior: 0.4679
Epoch 4/10
29/29 - 25s - loss: 591.3134 - loglik: -5.9206e+02 - logprior: 0.7493
Fitted a model with MAP estimate = -589.6101
Time for alignment: 447.7630
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 855.7813 - loglik: -8.5308e+02 - logprior: -2.7047e+00
Epoch 2/10
29/29 - 19s - loss: 671.9088 - loglik: -6.7014e+02 - logprior: -1.7720e+00
Epoch 3/10
29/29 - 19s - loss: 644.4399 - loglik: -6.4239e+02 - logprior: -2.0530e+00
Epoch 4/10
29/29 - 19s - loss: 639.3527 - loglik: -6.3729e+02 - logprior: -2.0599e+00
Epoch 5/10
29/29 - 19s - loss: 635.6523 - loglik: -6.3358e+02 - logprior: -2.0678e+00
Epoch 6/10
29/29 - 19s - loss: 637.6140 - loglik: -6.3553e+02 - logprior: -2.0800e+00
Fitted a model with MAP estimate = -635.3809
expansions: [(16, 1), (17, 1), (24, 2), (26, 1), (28, 1), (31, 1), (36, 1), (38, 1), (48, 2), (49, 1), (51, 1), (72, 1), (76, 1), (87, 1), (88, 2), (89, 2), (94, 1), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (142, 1), (148, 1), (151, 1), (153, 1), (154, 1), (155, 1), (166, 2), (181, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (262, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 618.9651 - loglik: -6.1400e+02 - logprior: -4.9629e+00
Epoch 2/2
29/29 - 26s - loss: 596.0138 - loglik: -5.9396e+02 - logprior: -2.0567e+00
Fitted a model with MAP estimate = -591.7617
expansions: [(0, 2), (31, 1), (61, 1)]
discards: [  0 104 106 198 262 301 328]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 597.7746 - loglik: -5.9480e+02 - logprior: -2.9709e+00
Epoch 2/2
29/29 - 25s - loss: 591.8119 - loglik: -5.9150e+02 - logprior: -3.1292e-01
Fitted a model with MAP estimate = -588.7760
expansions: []
discards: [  0  65  66 329]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 601.2550 - loglik: -5.9699e+02 - logprior: -4.2683e+00
Epoch 2/10
29/29 - 25s - loss: 594.6564 - loglik: -5.9437e+02 - logprior: -2.9058e-01
Epoch 3/10
29/29 - 25s - loss: 589.6347 - loglik: -5.9016e+02 - logprior: 0.5223
Epoch 4/10
29/29 - 25s - loss: 589.9076 - loglik: -5.9071e+02 - logprior: 0.8044
Fitted a model with MAP estimate = -588.9624
Time for alignment: 427.3246
Computed alignments with likelihoods: ['-589.4154', '-580.7262', '-582.9625', '-589.6101', '-588.7760']
Best model has likelihood: -580.7262  (prior= 1.4766 )
time for generating output: 0.3422
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8685962373371925
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0754 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6538 - loglik: -1.0668e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9377 - loglik: -8.6663e+01 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 108.2136 - loglik: -7.3454e+01 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 89.7217 - loglik: -6.9642e+01 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 79.8159 - loglik: -6.9417e+01 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 73.9912 - loglik: -6.9908e+01 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 70.4397 - loglik: -7.0168e+01 - logprior: -2.7166e-01
Epoch 9/10
10/10 - 0s - loss: 68.0781 - loglik: -7.0308e+01 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 66.3905 - loglik: -7.0475e+01 - logprior: 4.0846
Fitted a model with MAP estimate = -65.6313
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 714.2719 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0526 - loglik: -5.4741e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5150
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4005 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8276 - loglik: -4.9715e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.8977
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 476.7534 - loglik: -4.9040e+01 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 163.3159 - loglik: -4.9661e+01 - logprior: -1.1365e+02
Epoch 3/10
10/10 - 0s - loss: 97.3659 - loglik: -5.0363e+01 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 69.3514 - loglik: -5.0902e+01 - logprior: -1.8449e+01
Epoch 5/10
10/10 - 0s - loss: 53.9330 - loglik: -5.1402e+01 - logprior: -2.5315e+00
Epoch 6/10
10/10 - 0s - loss: 44.9033 - loglik: -5.1808e+01 - logprior: 6.9045
Epoch 7/10
10/10 - 0s - loss: 39.2610 - loglik: -5.2105e+01 - logprior: 12.8444
Epoch 8/10
10/10 - 0s - loss: 35.4097 - loglik: -5.2333e+01 - logprior: 16.9233
Epoch 9/10
10/10 - 0s - loss: 32.5349 - loglik: -5.2512e+01 - logprior: 19.9766
Epoch 10/10
10/10 - 0s - loss: 30.2288 - loglik: -5.2659e+01 - logprior: 22.4302
Fitted a model with MAP estimate = -29.1070
Time for alignment: 26.6255
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0754 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6538 - loglik: -1.0668e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9377 - loglik: -8.6663e+01 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 108.2136 - loglik: -7.3454e+01 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 89.7217 - loglik: -6.9642e+01 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 79.8159 - loglik: -6.9417e+01 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 73.9913 - loglik: -6.9908e+01 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 70.4396 - loglik: -7.0168e+01 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 68.0781 - loglik: -7.0308e+01 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 66.3905 - loglik: -7.0475e+01 - logprior: 4.0846
Fitted a model with MAP estimate = -65.6313
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 714.2719 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0526 - loglik: -5.4741e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5150
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4005 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8276 - loglik: -4.9715e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.8977
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.7535 - loglik: -4.9040e+01 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 163.3160 - loglik: -4.9661e+01 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 97.3660 - loglik: -5.0363e+01 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 69.3516 - loglik: -5.0902e+01 - logprior: -1.8450e+01
Epoch 5/10
10/10 - 0s - loss: 53.9331 - loglik: -5.1401e+01 - logprior: -2.5319e+00
Epoch 6/10
10/10 - 0s - loss: 44.9034 - loglik: -5.1808e+01 - logprior: 6.9042
Epoch 7/10
10/10 - 0s - loss: 39.2611 - loglik: -5.2105e+01 - logprior: 12.8441
Epoch 8/10
10/10 - 0s - loss: 35.4098 - loglik: -5.2333e+01 - logprior: 16.9229
Epoch 9/10
10/10 - 0s - loss: 32.5351 - loglik: -5.2511e+01 - logprior: 19.9763
Epoch 10/10
10/10 - 0s - loss: 30.2288 - loglik: -5.2659e+01 - logprior: 22.4299
Fitted a model with MAP estimate = -29.1073
Time for alignment: 23.8472
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0754 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6538 - loglik: -1.0668e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9377 - loglik: -8.6663e+01 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 108.2136 - loglik: -7.3454e+01 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 89.7217 - loglik: -6.9642e+01 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 79.8159 - loglik: -6.9417e+01 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 73.9913 - loglik: -6.9908e+01 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 70.4396 - loglik: -7.0168e+01 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 68.0781 - loglik: -7.0308e+01 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 66.3905 - loglik: -7.0475e+01 - logprior: 4.0846
Fitted a model with MAP estimate = -65.6314
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 714.2719 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0526 - loglik: -5.4741e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5150
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 505.4005 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8276 - loglik: -4.9715e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.8977
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.7535 - loglik: -4.9040e+01 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 163.3160 - loglik: -4.9661e+01 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 97.3660 - loglik: -5.0363e+01 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 69.3515 - loglik: -5.0902e+01 - logprior: -1.8450e+01
Epoch 5/10
10/10 - 0s - loss: 53.9331 - loglik: -5.1401e+01 - logprior: -2.5318e+00
Epoch 6/10
10/10 - 0s - loss: 44.9034 - loglik: -5.1808e+01 - logprior: 6.9043
Epoch 7/10
10/10 - 0s - loss: 39.2611 - loglik: -5.2105e+01 - logprior: 12.8442
Epoch 8/10
10/10 - 0s - loss: 35.4097 - loglik: -5.2333e+01 - logprior: 16.9230
Epoch 9/10
10/10 - 0s - loss: 32.5349 - loglik: -5.2511e+01 - logprior: 19.9764
Epoch 10/10
10/10 - 0s - loss: 30.2288 - loglik: -5.2659e+01 - logprior: 22.4300
Fitted a model with MAP estimate = -29.1073
Time for alignment: 23.3497
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0754 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6538 - loglik: -1.0668e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9377 - loglik: -8.6663e+01 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 108.2136 - loglik: -7.3454e+01 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 89.7217 - loglik: -6.9642e+01 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 79.8159 - loglik: -6.9417e+01 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 73.9912 - loglik: -6.9908e+01 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 70.4397 - loglik: -7.0168e+01 - logprior: -2.7166e-01
Epoch 9/10
10/10 - 0s - loss: 68.0781 - loglik: -7.0308e+01 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 66.3905 - loglik: -7.0475e+01 - logprior: 4.0846
Fitted a model with MAP estimate = -65.6314
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 714.2719 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0526 - loglik: -5.4741e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5150
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4005 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8276 - loglik: -4.9715e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.8977
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.7535 - loglik: -4.9040e+01 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 163.3161 - loglik: -4.9661e+01 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 97.3663 - loglik: -5.0363e+01 - logprior: -4.7004e+01
Epoch 4/10
10/10 - 0s - loss: 69.3520 - loglik: -5.0902e+01 - logprior: -1.8450e+01
Epoch 5/10
10/10 - 0s - loss: 53.9338 - loglik: -5.1401e+01 - logprior: -2.5324e+00
Epoch 6/10
10/10 - 0s - loss: 44.9043 - loglik: -5.1808e+01 - logprior: 6.9034
Epoch 7/10
10/10 - 0s - loss: 39.2621 - loglik: -5.2105e+01 - logprior: 12.8431
Epoch 8/10
10/10 - 0s - loss: 35.4110 - loglik: -5.2333e+01 - logprior: 16.9218
Epoch 9/10
10/10 - 0s - loss: 32.5364 - loglik: -5.2511e+01 - logprior: 19.9750
Epoch 10/10
10/10 - 0s - loss: 30.2304 - loglik: -5.2659e+01 - logprior: 22.4285
Fitted a model with MAP estimate = -29.1089
Time for alignment: 23.2429
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0754 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6538 - loglik: -1.0668e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9377 - loglik: -8.6663e+01 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 108.2136 - loglik: -7.3454e+01 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 89.7217 - loglik: -6.9642e+01 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 79.8159 - loglik: -6.9417e+01 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 73.9912 - loglik: -6.9908e+01 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 70.4397 - loglik: -7.0168e+01 - logprior: -2.7166e-01
Epoch 9/10
10/10 - 0s - loss: 68.0782 - loglik: -7.0308e+01 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 66.3905 - loglik: -7.0475e+01 - logprior: 4.0846
Fitted a model with MAP estimate = -65.6314
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 714.2719 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0526 - loglik: -5.4741e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5150
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4005 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8276 - loglik: -4.9715e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.8976
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.7534 - loglik: -4.9040e+01 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 163.3159 - loglik: -4.9661e+01 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 97.3661 - loglik: -5.0363e+01 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 69.3517 - loglik: -5.0902e+01 - logprior: -1.8450e+01
Epoch 5/10
10/10 - 0s - loss: 53.9334 - loglik: -5.1401e+01 - logprior: -2.5320e+00
Epoch 6/10
10/10 - 0s - loss: 44.9039 - loglik: -5.1808e+01 - logprior: 6.9039
Epoch 7/10
10/10 - 0s - loss: 39.2617 - loglik: -5.2105e+01 - logprior: 12.8437
Epoch 8/10
10/10 - 0s - loss: 35.4105 - loglik: -5.2333e+01 - logprior: 16.9224
Epoch 9/10
10/10 - 0s - loss: 32.5358 - loglik: -5.2511e+01 - logprior: 19.9757
Epoch 10/10
10/10 - 0s - loss: 30.2297 - loglik: -5.2659e+01 - logprior: 22.4292
Fitted a model with MAP estimate = -29.1082
Time for alignment: 22.9603
Computed alignments with likelihoods: ['-29.1070', '-29.1073', '-29.1073', '-29.1089', '-29.1082']
Best model has likelihood: -29.1070  (prior= 23.6242 )
time for generating output: 0.0837
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.2538 - loglik: -4.7308e+02 - logprior: -4.1784e+00
Epoch 2/10
16/16 - 4s - loss: 441.2908 - loglik: -4.4028e+02 - logprior: -1.0145e+00
Epoch 3/10
16/16 - 4s - loss: 416.5044 - loglik: -4.1520e+02 - logprior: -1.3040e+00
Epoch 4/10
16/16 - 4s - loss: 409.1140 - loglik: -4.0783e+02 - logprior: -1.2872e+00
Epoch 5/10
16/16 - 4s - loss: 403.8764 - loglik: -4.0264e+02 - logprior: -1.2330e+00
Epoch 6/10
16/16 - 4s - loss: 403.3317 - loglik: -4.0204e+02 - logprior: -1.2874e+00
Epoch 7/10
16/16 - 4s - loss: 400.7632 - loglik: -3.9946e+02 - logprior: -1.3078e+00
Epoch 8/10
16/16 - 4s - loss: 401.1554 - loglik: -3.9981e+02 - logprior: -1.3476e+00
Fitted a model with MAP estimate = -400.7139
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 2), (30, 1), (41, 1), (42, 1), (43, 1), (48, 2), (51, 1), (53, 1), (56, 1), (69, 1), (72, 1), (74, 3), (94, 3), (95, 1), (96, 2), (99, 1), (105, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 407.3623 - loglik: -4.0360e+02 - logprior: -3.7609e+00
Epoch 2/2
33/33 - 6s - loss: 396.8441 - loglik: -3.9587e+02 - logprior: -9.7578e-01
Fitted a model with MAP estimate = -394.2670
expansions: [(176, 2)]
discards: [  0  32  33  93 145 173 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 404.3184 - loglik: -4.0078e+02 - logprior: -3.5351e+00
Epoch 2/2
33/33 - 6s - loss: 398.6033 - loglik: -3.9787e+02 - logprior: -7.3509e-01
Fitted a model with MAP estimate = -396.3871
expansions: [(0, 1), (33, 1), (170, 2)]
discards: [114 168 169]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 400.1022 - loglik: -3.9770e+02 - logprior: -2.3982e+00
Epoch 2/10
33/33 - 6s - loss: 395.8923 - loglik: -3.9546e+02 - logprior: -4.3648e-01
Epoch 3/10
33/33 - 6s - loss: 394.8537 - loglik: -3.9458e+02 - logprior: -2.7494e-01
Epoch 4/10
33/33 - 6s - loss: 391.4856 - loglik: -3.9127e+02 - logprior: -2.1964e-01
Epoch 5/10
33/33 - 6s - loss: 389.8300 - loglik: -3.8966e+02 - logprior: -1.7408e-01
Epoch 6/10
33/33 - 6s - loss: 390.4853 - loglik: -3.9036e+02 - logprior: -1.2856e-01
Fitted a model with MAP estimate = -388.6509
Time for alignment: 138.8327
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 476.9111 - loglik: -4.7273e+02 - logprior: -4.1845e+00
Epoch 2/10
16/16 - 4s - loss: 442.3129 - loglik: -4.4131e+02 - logprior: -1.0018e+00
Epoch 3/10
16/16 - 4s - loss: 417.4984 - loglik: -4.1621e+02 - logprior: -1.2835e+00
Epoch 4/10
16/16 - 4s - loss: 409.0101 - loglik: -4.0771e+02 - logprior: -1.2978e+00
Epoch 5/10
16/16 - 4s - loss: 405.3620 - loglik: -4.0408e+02 - logprior: -1.2771e+00
Epoch 6/10
16/16 - 4s - loss: 403.2576 - loglik: -4.0191e+02 - logprior: -1.3496e+00
Epoch 7/10
16/16 - 4s - loss: 401.5949 - loglik: -4.0024e+02 - logprior: -1.3539e+00
Epoch 8/10
16/16 - 4s - loss: 402.2326 - loglik: -4.0087e+02 - logprior: -1.3654e+00
Fitted a model with MAP estimate = -401.3364
expansions: [(14, 1), (17, 1), (24, 1), (27, 1), (28, 2), (30, 1), (43, 1), (44, 1), (49, 2), (51, 1), (55, 1), (57, 1), (58, 1), (70, 1), (72, 1), (73, 2), (74, 2), (75, 2), (94, 3), (95, 1), (96, 2), (99, 1), (102, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.8126 - loglik: -4.0404e+02 - logprior: -3.7722e+00
Epoch 2/2
33/33 - 6s - loss: 396.9084 - loglik: -3.9591e+02 - logprior: -9.9604e-01
Fitted a model with MAP estimate = -394.0751
expansions: [(179, 2)]
discards: [  0  32  33  72 148 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 403.6936 - loglik: -4.0014e+02 - logprior: -3.5544e+00
Epoch 2/2
33/33 - 6s - loss: 398.4356 - loglik: -3.9768e+02 - logprior: -7.5250e-01
Fitted a model with MAP estimate = -396.0892
expansions: [(0, 1), (173, 2)]
discards: [117 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 400.2731 - loglik: -3.9788e+02 - logprior: -2.3971e+00
Epoch 2/10
33/33 - 6s - loss: 396.4460 - loglik: -3.9599e+02 - logprior: -4.5099e-01
Epoch 3/10
33/33 - 6s - loss: 394.1826 - loglik: -3.9390e+02 - logprior: -2.7934e-01
Epoch 4/10
33/33 - 6s - loss: 392.0009 - loglik: -3.9177e+02 - logprior: -2.2877e-01
Epoch 5/10
33/33 - 6s - loss: 389.8730 - loglik: -3.8968e+02 - logprior: -1.9700e-01
Epoch 6/10
33/33 - 6s - loss: 389.0577 - loglik: -3.8891e+02 - logprior: -1.4876e-01
Epoch 7/10
33/33 - 6s - loss: 387.7209 - loglik: -3.8763e+02 - logprior: -9.1033e-02
Epoch 8/10
33/33 - 6s - loss: 389.0084 - loglik: -3.8897e+02 - logprior: -4.2309e-02
Fitted a model with MAP estimate = -387.8848
Time for alignment: 152.2678
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.0060 - loglik: -4.7283e+02 - logprior: -4.1784e+00
Epoch 2/10
16/16 - 4s - loss: 441.5665 - loglik: -4.4056e+02 - logprior: -1.0096e+00
Epoch 3/10
16/16 - 4s - loss: 416.8640 - loglik: -4.1555e+02 - logprior: -1.3186e+00
Epoch 4/10
16/16 - 4s - loss: 406.9478 - loglik: -4.0555e+02 - logprior: -1.3993e+00
Epoch 5/10
16/16 - 4s - loss: 403.1310 - loglik: -4.0182e+02 - logprior: -1.3067e+00
Epoch 6/10
16/16 - 4s - loss: 402.6649 - loglik: -4.0132e+02 - logprior: -1.3477e+00
Epoch 7/10
16/16 - 4s - loss: 400.7208 - loglik: -3.9939e+02 - logprior: -1.3290e+00
Epoch 8/10
16/16 - 4s - loss: 401.7514 - loglik: -4.0040e+02 - logprior: -1.3487e+00
Fitted a model with MAP estimate = -400.6853
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 2), (30, 1), (43, 1), (49, 3), (50, 2), (51, 1), (57, 1), (58, 1), (70, 1), (72, 1), (73, 2), (74, 2), (75, 2), (94, 2), (95, 4), (99, 1), (113, 1), (116, 1), (120, 1), (123, 1), (126, 1), (129, 1), (130, 1), (134, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 407.5077 - loglik: -4.0372e+02 - logprior: -3.7897e+00
Epoch 2/2
33/33 - 6s - loss: 396.1058 - loglik: -3.9506e+02 - logprior: -1.0417e+00
Fitted a model with MAP estimate = -394.0431
expansions: [(163, 1), (180, 2)]
discards: [  0  31  32  33  60  73  95 119 120 177 178 179]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 405.0770 - loglik: -4.0153e+02 - logprior: -3.5451e+00
Epoch 2/2
33/33 - 6s - loss: 399.3199 - loglik: -3.9857e+02 - logprior: -7.5280e-01
Fitted a model with MAP estimate = -396.9217
expansions: [(0, 1), (30, 3), (171, 2)]
discards: [169 170]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 400.4336 - loglik: -3.9806e+02 - logprior: -2.3783e+00
Epoch 2/10
33/33 - 6s - loss: 395.2344 - loglik: -3.9478e+02 - logprior: -4.5217e-01
Epoch 3/10
33/33 - 6s - loss: 393.3411 - loglik: -3.9307e+02 - logprior: -2.7555e-01
Epoch 4/10
33/33 - 6s - loss: 391.2267 - loglik: -3.9100e+02 - logprior: -2.2538e-01
Epoch 5/10
33/33 - 6s - loss: 389.6424 - loglik: -3.8946e+02 - logprior: -1.8447e-01
Epoch 6/10
33/33 - 6s - loss: 388.1928 - loglik: -3.8805e+02 - logprior: -1.4606e-01
Epoch 7/10
33/33 - 6s - loss: 387.8366 - loglik: -3.8773e+02 - logprior: -1.1001e-01
Epoch 8/10
33/33 - 6s - loss: 387.5896 - loglik: -3.8754e+02 - logprior: -5.1515e-02
Epoch 9/10
33/33 - 6s - loss: 387.4375 - loglik: -3.8745e+02 - logprior: 0.0099
Epoch 10/10
33/33 - 6s - loss: 386.4550 - loglik: -3.8652e+02 - logprior: 0.0655
Fitted a model with MAP estimate = -386.8649
Time for alignment: 164.0827
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 476.9367 - loglik: -4.7276e+02 - logprior: -4.1741e+00
Epoch 2/10
16/16 - 4s - loss: 440.7448 - loglik: -4.3976e+02 - logprior: -9.8356e-01
Epoch 3/10
16/16 - 4s - loss: 417.2542 - loglik: -4.1607e+02 - logprior: -1.1879e+00
Epoch 4/10
16/16 - 4s - loss: 409.4991 - loglik: -4.0828e+02 - logprior: -1.2185e+00
Epoch 5/10
16/16 - 4s - loss: 404.3093 - loglik: -4.0316e+02 - logprior: -1.1444e+00
Epoch 6/10
16/16 - 4s - loss: 402.7528 - loglik: -4.0159e+02 - logprior: -1.1659e+00
Epoch 7/10
16/16 - 4s - loss: 402.0745 - loglik: -4.0091e+02 - logprior: -1.1622e+00
Epoch 8/10
16/16 - 4s - loss: 401.4528 - loglik: -4.0029e+02 - logprior: -1.1628e+00
Epoch 9/10
16/16 - 4s - loss: 400.0542 - loglik: -3.9889e+02 - logprior: -1.1692e+00
Epoch 10/10
16/16 - 4s - loss: 401.0182 - loglik: -3.9984e+02 - logprior: -1.1739e+00
Fitted a model with MAP estimate = -400.4676
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (50, 1), (56, 1), (58, 1), (69, 1), (72, 2), (74, 3), (93, 1), (94, 6), (98, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (139, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 408.2949 - loglik: -4.0456e+02 - logprior: -3.7359e+00
Epoch 2/2
33/33 - 6s - loss: 397.1128 - loglik: -3.9612e+02 - logprior: -9.9267e-01
Fitted a model with MAP estimate = -394.4639
expansions: [(93, 1), (176, 2)]
discards: [  0  32  33  58 117 168 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 404.2488 - loglik: -4.0070e+02 - logprior: -3.5487e+00
Epoch 2/2
33/33 - 6s - loss: 398.5167 - loglik: -3.9766e+02 - logprior: -8.5265e-01
Fitted a model with MAP estimate = -395.9308
expansions: [(0, 1), (33, 1), (171, 2)]
discards: [169 170]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 399.9644 - loglik: -3.9759e+02 - logprior: -2.3761e+00
Epoch 2/10
33/33 - 6s - loss: 394.8158 - loglik: -3.9438e+02 - logprior: -4.3949e-01
Epoch 3/10
33/33 - 6s - loss: 393.5921 - loglik: -3.9333e+02 - logprior: -2.6188e-01
Epoch 4/10
33/33 - 6s - loss: 391.5999 - loglik: -3.9139e+02 - logprior: -2.1045e-01
Epoch 5/10
33/33 - 6s - loss: 389.6391 - loglik: -3.8947e+02 - logprior: -1.7344e-01
Epoch 6/10
33/33 - 6s - loss: 387.8874 - loglik: -3.8778e+02 - logprior: -1.1122e-01
Epoch 7/10
33/33 - 6s - loss: 388.2585 - loglik: -3.8818e+02 - logprior: -7.9686e-02
Fitted a model with MAP estimate = -387.6381
Time for alignment: 154.1449
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.2395 - loglik: -4.7306e+02 - logprior: -4.1814e+00
Epoch 2/10
16/16 - 4s - loss: 440.1743 - loglik: -4.3917e+02 - logprior: -1.0055e+00
Epoch 3/10
16/16 - 4s - loss: 414.6360 - loglik: -4.1338e+02 - logprior: -1.2610e+00
Epoch 4/10
16/16 - 4s - loss: 408.2199 - loglik: -4.0694e+02 - logprior: -1.2771e+00
Epoch 5/10
16/16 - 4s - loss: 404.7955 - loglik: -4.0358e+02 - logprior: -1.2182e+00
Epoch 6/10
16/16 - 4s - loss: 403.3328 - loglik: -4.0207e+02 - logprior: -1.2655e+00
Epoch 7/10
16/16 - 4s - loss: 401.7980 - loglik: -4.0054e+02 - logprior: -1.2566e+00
Epoch 8/10
16/16 - 4s - loss: 400.7553 - loglik: -3.9948e+02 - logprior: -1.2762e+00
Epoch 9/10
16/16 - 4s - loss: 401.8885 - loglik: -4.0062e+02 - logprior: -1.2717e+00
Fitted a model with MAP estimate = -400.8117
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (44, 1), (48, 2), (49, 1), (50, 1), (56, 1), (57, 1), (69, 1), (72, 1), (74, 3), (93, 1), (94, 2), (95, 5), (102, 1), (116, 1), (117, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 406.8146 - loglik: -4.0307e+02 - logprior: -3.7454e+00
Epoch 2/2
33/33 - 6s - loss: 397.1464 - loglik: -3.9613e+02 - logprior: -1.0210e+00
Fitted a model with MAP estimate = -394.2709
expansions: [(177, 2)]
discards: [  0  32  33  57  72  94 113 116 117 118 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 405.5237 - loglik: -4.0195e+02 - logprior: -3.5719e+00
Epoch 2/2
33/33 - 6s - loss: 400.3090 - loglik: -3.9950e+02 - logprior: -8.0718e-01
Fitted a model with MAP estimate = -397.1976
expansions: [(0, 1), (33, 1), (109, 2), (166, 2)]
discards: [164 165]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 400.8700 - loglik: -3.9845e+02 - logprior: -2.4166e+00
Epoch 2/10
33/33 - 6s - loss: 397.0363 - loglik: -3.9658e+02 - logprior: -4.5518e-01
Epoch 3/10
33/33 - 6s - loss: 394.1256 - loglik: -3.9385e+02 - logprior: -2.7333e-01
Epoch 4/10
33/33 - 6s - loss: 392.5838 - loglik: -3.9235e+02 - logprior: -2.3858e-01
Epoch 5/10
33/33 - 6s - loss: 390.3539 - loglik: -3.9017e+02 - logprior: -1.8223e-01
Epoch 6/10
33/33 - 6s - loss: 389.7082 - loglik: -3.8955e+02 - logprior: -1.5757e-01
Epoch 7/10
33/33 - 6s - loss: 389.5572 - loglik: -3.8946e+02 - logprior: -9.9157e-02
Epoch 8/10
33/33 - 6s - loss: 389.2296 - loglik: -3.8917e+02 - logprior: -5.4739e-02
Epoch 9/10
33/33 - 6s - loss: 388.6366 - loglik: -3.8864e+02 - logprior: -5.4056e-04
Epoch 10/10
33/33 - 6s - loss: 388.8230 - loglik: -3.8889e+02 - logprior: 0.0678
Fitted a model with MAP estimate = -388.5311
Time for alignment: 164.4522
Computed alignments with likelihoods: ['-388.6509', '-387.8848', '-386.8649', '-387.6381', '-388.5311']
Best model has likelihood: -386.8649  (prior= 0.1243 )
time for generating output: 0.2046
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.8207126948775055
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 292.1421 - loglik: -2.7721e+02 - logprior: -1.4932e+01
Epoch 2/10
10/10 - 1s - loss: 253.3498 - loglik: -2.4945e+02 - logprior: -3.9042e+00
Epoch 3/10
10/10 - 1s - loss: 225.4723 - loglik: -2.2321e+02 - logprior: -2.2628e+00
Epoch 4/10
10/10 - 1s - loss: 209.3008 - loglik: -2.0737e+02 - logprior: -1.9352e+00
Epoch 5/10
10/10 - 1s - loss: 205.9286 - loglik: -2.0420e+02 - logprior: -1.7313e+00
Epoch 6/10
10/10 - 1s - loss: 202.1246 - loglik: -2.0049e+02 - logprior: -1.6347e+00
Epoch 7/10
10/10 - 1s - loss: 203.4991 - loglik: -2.0190e+02 - logprior: -1.6041e+00
Fitted a model with MAP estimate = -202.0525
expansions: [(20, 1), (21, 1), (34, 1), (37, 1), (39, 1), (44, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 224.9443 - loglik: -2.0768e+02 - logprior: -1.7261e+01
Epoch 2/2
10/10 - 1s - loss: 206.4036 - loglik: -1.9898e+02 - logprior: -7.4253e+00
Fitted a model with MAP estimate = -204.6429
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.1283 - loglik: -2.0217e+02 - logprior: -1.6958e+01
Epoch 2/2
10/10 - 1s - loss: 206.4083 - loglik: -2.0037e+02 - logprior: -6.0349e+00
Fitted a model with MAP estimate = -202.9461
expansions: [(0, 14), (48, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 214.5424 - loglik: -2.0000e+02 - logprior: -1.4538e+01
Epoch 2/10
10/10 - 1s - loss: 196.8088 - loglik: -1.9227e+02 - logprior: -4.5418e+00
Epoch 3/10
10/10 - 1s - loss: 190.7352 - loglik: -1.8818e+02 - logprior: -2.5567e+00
Epoch 4/10
10/10 - 1s - loss: 187.2262 - loglik: -1.8537e+02 - logprior: -1.8566e+00
Epoch 5/10
10/10 - 1s - loss: 184.8192 - loglik: -1.8332e+02 - logprior: -1.4996e+00
Epoch 6/10
10/10 - 1s - loss: 186.3609 - loglik: -1.8508e+02 - logprior: -1.2786e+00
Fitted a model with MAP estimate = -184.8422
Time for alignment: 37.7254
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.5317 - loglik: -2.7660e+02 - logprior: -1.4928e+01
Epoch 2/10
10/10 - 1s - loss: 253.1671 - loglik: -2.4927e+02 - logprior: -3.9010e+00
Epoch 3/10
10/10 - 1s - loss: 227.8115 - loglik: -2.2551e+02 - logprior: -2.2998e+00
Epoch 4/10
10/10 - 1s - loss: 212.3431 - loglik: -2.1036e+02 - logprior: -1.9798e+00
Epoch 5/10
10/10 - 1s - loss: 206.5127 - loglik: -2.0474e+02 - logprior: -1.7762e+00
Epoch 6/10
10/10 - 1s - loss: 202.2798 - loglik: -2.0061e+02 - logprior: -1.6664e+00
Epoch 7/10
10/10 - 1s - loss: 202.9769 - loglik: -2.0142e+02 - logprior: -1.5602e+00
Fitted a model with MAP estimate = -201.0696
expansions: [(5, 1), (8, 1), (10, 1), (20, 1), (25, 1), (34, 1), (37, 1), (39, 1), (44, 1), (48, 2), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 224.7343 - loglik: -2.0760e+02 - logprior: -1.7131e+01
Epoch 2/2
10/10 - 1s - loss: 202.3940 - loglik: -1.9510e+02 - logprior: -7.2897e+00
Fitted a model with MAP estimate = -199.2047
expansions: []
discards: [ 0 51 56 67]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 217.5621 - loglik: -2.0079e+02 - logprior: -1.6768e+01
Epoch 2/2
10/10 - 1s - loss: 200.4767 - loglik: -1.9465e+02 - logprior: -5.8241e+00
Fitted a model with MAP estimate = -198.1313
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 209.9353 - loglik: -1.9558e+02 - logprior: -1.4359e+01
Epoch 2/10
10/10 - 1s - loss: 190.4419 - loglik: -1.8609e+02 - logprior: -4.3531e+00
Epoch 3/10
10/10 - 1s - loss: 182.9308 - loglik: -1.8047e+02 - logprior: -2.4618e+00
Epoch 4/10
10/10 - 1s - loss: 181.9169 - loglik: -1.8017e+02 - logprior: -1.7515e+00
Epoch 5/10
10/10 - 1s - loss: 180.4526 - loglik: -1.7894e+02 - logprior: -1.5138e+00
Epoch 6/10
10/10 - 1s - loss: 180.5049 - loglik: -1.7912e+02 - logprior: -1.3888e+00
Fitted a model with MAP estimate = -179.2805
Time for alignment: 38.3132
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 292.2010 - loglik: -2.7727e+02 - logprior: -1.4934e+01
Epoch 2/10
10/10 - 1s - loss: 253.7489 - loglik: -2.4985e+02 - logprior: -3.9012e+00
Epoch 3/10
10/10 - 1s - loss: 225.5815 - loglik: -2.2331e+02 - logprior: -2.2704e+00
Epoch 4/10
10/10 - 1s - loss: 210.9162 - loglik: -2.0896e+02 - logprior: -1.9556e+00
Epoch 5/10
10/10 - 1s - loss: 207.4254 - loglik: -2.0559e+02 - logprior: -1.8337e+00
Epoch 6/10
10/10 - 1s - loss: 202.6083 - loglik: -2.0077e+02 - logprior: -1.8394e+00
Epoch 7/10
10/10 - 1s - loss: 202.3422 - loglik: -2.0051e+02 - logprior: -1.8285e+00
Epoch 8/10
10/10 - 1s - loss: 201.0899 - loglik: -1.9928e+02 - logprior: -1.8077e+00
Epoch 9/10
10/10 - 1s - loss: 200.6938 - loglik: -1.9890e+02 - logprior: -1.7985e+00
Epoch 10/10
10/10 - 1s - loss: 201.6512 - loglik: -1.9987e+02 - logprior: -1.7848e+00
Fitted a model with MAP estimate = -200.6277
expansions: [(21, 1), (26, 1), (34, 1), (35, 1), (36, 1), (37, 1), (47, 2), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.3709 - loglik: -2.0905e+02 - logprior: -1.7318e+01
Epoch 2/2
10/10 - 1s - loss: 206.3886 - loglik: -1.9897e+02 - logprior: -7.4172e+00
Fitted a model with MAP estimate = -204.1628
expansions: [(0, 12)]
discards: [ 0 37 39 53]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 216.9511 - loglik: -2.0245e+02 - logprior: -1.4497e+01
Epoch 2/2
10/10 - 1s - loss: 197.1709 - loglik: -1.9259e+02 - logprior: -4.5857e+00
Fitted a model with MAP estimate = -194.4536
expansions: [(30, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 215.3534 - loglik: -1.9900e+02 - logprior: -1.6353e+01
Epoch 2/10
10/10 - 1s - loss: 202.1165 - loglik: -1.9705e+02 - logprior: -5.0617e+00
Epoch 3/10
10/10 - 1s - loss: 196.7799 - loglik: -1.9441e+02 - logprior: -2.3685e+00
Epoch 4/10
10/10 - 1s - loss: 197.1781 - loglik: -1.9561e+02 - logprior: -1.5646e+00
Fitted a model with MAP estimate = -195.6379
Time for alignment: 37.8710
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 290.3897 - loglik: -2.7546e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 253.4547 - loglik: -2.4956e+02 - logprior: -3.8930e+00
Epoch 3/10
10/10 - 1s - loss: 228.4443 - loglik: -2.2620e+02 - logprior: -2.2407e+00
Epoch 4/10
10/10 - 1s - loss: 212.1384 - loglik: -2.1024e+02 - logprior: -1.8941e+00
Epoch 5/10
10/10 - 1s - loss: 206.2029 - loglik: -2.0448e+02 - logprior: -1.7257e+00
Epoch 6/10
10/10 - 1s - loss: 204.3702 - loglik: -2.0267e+02 - logprior: -1.7044e+00
Epoch 7/10
10/10 - 1s - loss: 202.2955 - loglik: -2.0055e+02 - logprior: -1.7436e+00
Epoch 8/10
10/10 - 1s - loss: 202.4166 - loglik: -2.0065e+02 - logprior: -1.7707e+00
Fitted a model with MAP estimate = -201.8444
expansions: [(19, 1), (33, 1), (36, 3), (45, 2), (46, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 224.8446 - loglik: -2.0755e+02 - logprior: -1.7298e+01
Epoch 2/2
10/10 - 1s - loss: 206.1174 - loglik: -1.9864e+02 - logprior: -7.4793e+00
Fitted a model with MAP estimate = -203.4146
expansions: []
discards: [ 0 38 39]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 217.3790 - loglik: -2.0045e+02 - logprior: -1.6929e+01
Epoch 2/2
10/10 - 1s - loss: 205.4391 - loglik: -1.9943e+02 - logprior: -6.0133e+00
Fitted a model with MAP estimate = -201.6824
expansions: [(0, 9)]
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 212.9191 - loglik: -1.9869e+02 - logprior: -1.4228e+01
Epoch 2/10
10/10 - 1s - loss: 196.9456 - loglik: -1.9257e+02 - logprior: -4.3739e+00
Epoch 3/10
10/10 - 1s - loss: 192.1069 - loglik: -1.8962e+02 - logprior: -2.4898e+00
Epoch 4/10
10/10 - 1s - loss: 190.3119 - loglik: -1.8840e+02 - logprior: -1.9094e+00
Epoch 5/10
10/10 - 1s - loss: 189.0074 - loglik: -1.8729e+02 - logprior: -1.7204e+00
Epoch 6/10
10/10 - 1s - loss: 188.0041 - loglik: -1.8645e+02 - logprior: -1.5494e+00
Epoch 7/10
10/10 - 1s - loss: 188.0205 - loglik: -1.8664e+02 - logprior: -1.3801e+00
Fitted a model with MAP estimate = -187.6589
Time for alignment: 40.7006
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 292.3080 - loglik: -2.7738e+02 - logprior: -1.4933e+01
Epoch 2/10
10/10 - 1s - loss: 252.2004 - loglik: -2.4829e+02 - logprior: -3.9074e+00
Epoch 3/10
10/10 - 1s - loss: 227.8934 - loglik: -2.2561e+02 - logprior: -2.2792e+00
Epoch 4/10
10/10 - 1s - loss: 211.7394 - loglik: -2.0978e+02 - logprior: -1.9598e+00
Epoch 5/10
10/10 - 1s - loss: 206.0698 - loglik: -2.0421e+02 - logprior: -1.8593e+00
Epoch 6/10
10/10 - 1s - loss: 202.0273 - loglik: -2.0017e+02 - logprior: -1.8531e+00
Epoch 7/10
10/10 - 1s - loss: 202.0279 - loglik: -2.0026e+02 - logprior: -1.7713e+00
Fitted a model with MAP estimate = -200.8760
expansions: [(20, 1), (21, 1), (27, 1), (32, 1), (37, 1), (40, 1), (46, 1), (47, 1), (48, 2), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 224.5833 - loglik: -2.0730e+02 - logprior: -1.7283e+01
Epoch 2/2
10/10 - 1s - loss: 204.6147 - loglik: -1.9725e+02 - logprior: -7.3606e+00
Fitted a model with MAP estimate = -201.2084
expansions: []
discards: [ 0 34]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 215.1219 - loglik: -1.9833e+02 - logprior: -1.6794e+01
Epoch 2/2
10/10 - 1s - loss: 201.0298 - loglik: -1.9516e+02 - logprior: -5.8655e+00
Fitted a model with MAP estimate = -198.1804
expansions: [(0, 7), (33, 1)]
discards: [18]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 209.5260 - loglik: -1.9536e+02 - logprior: -1.4165e+01
Epoch 2/10
10/10 - 1s - loss: 194.5592 - loglik: -1.9032e+02 - logprior: -4.2372e+00
Epoch 3/10
10/10 - 1s - loss: 190.4782 - loglik: -1.8813e+02 - logprior: -2.3502e+00
Epoch 4/10
10/10 - 1s - loss: 188.3141 - loglik: -1.8656e+02 - logprior: -1.7529e+00
Epoch 5/10
10/10 - 1s - loss: 186.6644 - loglik: -1.8512e+02 - logprior: -1.5490e+00
Epoch 6/10
10/10 - 1s - loss: 187.1278 - loglik: -1.8572e+02 - logprior: -1.4113e+00
Fitted a model with MAP estimate = -185.7897
Time for alignment: 36.3145
Computed alignments with likelihoods: ['-184.8422', '-179.2805', '-194.4536', '-187.6589', '-185.7897']
Best model has likelihood: -179.2805  (prior= -1.3301 )
time for generating output: 0.1819
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3701225259189444
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.6996 - loglik: -5.6089e+02 - logprior: -5.2809e+01
Epoch 2/10
10/10 - 2s - loss: 518.0510 - loglik: -5.0924e+02 - logprior: -8.8122e+00
Epoch 3/10
10/10 - 2s - loss: 455.9674 - loglik: -4.5405e+02 - logprior: -1.9184e+00
Epoch 4/10
10/10 - 2s - loss: 418.8470 - loglik: -4.1871e+02 - logprior: -1.3804e-01
Epoch 5/10
10/10 - 2s - loss: 405.6616 - loglik: -4.0630e+02 - logprior: 0.6384
Epoch 6/10
10/10 - 2s - loss: 400.1613 - loglik: -4.0155e+02 - logprior: 1.3850
Epoch 7/10
10/10 - 2s - loss: 397.9026 - loglik: -3.9973e+02 - logprior: 1.8227
Epoch 8/10
10/10 - 2s - loss: 395.5191 - loglik: -3.9753e+02 - logprior: 2.0153
Epoch 9/10
10/10 - 2s - loss: 396.7351 - loglik: -3.9890e+02 - logprior: 2.1675
Fitted a model with MAP estimate = -395.7206
expansions: [(0, 3), (19, 2), (22, 2), (23, 1), (24, 1), (30, 3), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (52, 1), (71, 1), (81, 1), (82, 1), (94, 1), (102, 1), (105, 2), (107, 1), (114, 1), (117, 3), (119, 1), (144, 3), (153, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 474.2368 - loglik: -4.0641e+02 - logprior: -6.7831e+01
Epoch 2/2
10/10 - 2s - loss: 399.8105 - loglik: -3.8452e+02 - logprior: -1.5286e+01
Fitted a model with MAP estimate = -386.4897
expansions: []
discards: [  0   1   2  23  27  39 146 176]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 436.5690 - loglik: -3.8779e+02 - logprior: -4.8777e+01
Epoch 2/2
10/10 - 2s - loss: 390.9018 - loglik: -3.8351e+02 - logprior: -7.3936e+00
Fitted a model with MAP estimate = -382.1240
expansions: [(0, 11)]
discards: [  0  55 127]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 435.8352 - loglik: -3.8915e+02 - logprior: -4.6688e+01
Epoch 2/10
10/10 - 2s - loss: 389.1300 - loglik: -3.8222e+02 - logprior: -6.9076e+00
Epoch 3/10
10/10 - 2s - loss: 375.4268 - loglik: -3.7673e+02 - logprior: 1.2994
Epoch 4/10
10/10 - 2s - loss: 371.4728 - loglik: -3.7640e+02 - logprior: 4.9286
Epoch 5/10
10/10 - 2s - loss: 366.2556 - loglik: -3.7316e+02 - logprior: 6.9020
Epoch 6/10
10/10 - 3s - loss: 363.8565 - loglik: -3.7180e+02 - logprior: 7.9483
Epoch 7/10
10/10 - 2s - loss: 362.1541 - loglik: -3.7090e+02 - logprior: 8.7453
Epoch 8/10
10/10 - 2s - loss: 360.9111 - loglik: -3.7023e+02 - logprior: 9.3223
Epoch 9/10
10/10 - 2s - loss: 361.6612 - loglik: -3.7148e+02 - logprior: 9.8171
Fitted a model with MAP estimate = -360.3486
Time for alignment: 70.7037
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.4432 - loglik: -5.6063e+02 - logprior: -5.2812e+01
Epoch 2/10
10/10 - 2s - loss: 516.7666 - loglik: -5.0798e+02 - logprior: -8.7872e+00
Epoch 3/10
10/10 - 2s - loss: 453.7092 - loglik: -4.5194e+02 - logprior: -1.7661e+00
Epoch 4/10
10/10 - 2s - loss: 420.0309 - loglik: -4.2017e+02 - logprior: 0.1379
Epoch 5/10
10/10 - 2s - loss: 407.7414 - loglik: -4.0887e+02 - logprior: 1.1241
Epoch 6/10
10/10 - 2s - loss: 402.3206 - loglik: -4.0413e+02 - logprior: 1.8139
Epoch 7/10
10/10 - 2s - loss: 400.7130 - loglik: -4.0294e+02 - logprior: 2.2266
Epoch 8/10
10/10 - 2s - loss: 399.8317 - loglik: -4.0232e+02 - logprior: 2.4895
Epoch 9/10
10/10 - 2s - loss: 398.4946 - loglik: -4.0119e+02 - logprior: 2.6977
Epoch 10/10
10/10 - 2s - loss: 398.6105 - loglik: -4.0151e+02 - logprior: 2.8989
Fitted a model with MAP estimate = -398.5268
expansions: [(9, 3), (19, 1), (20, 1), (22, 2), (23, 2), (25, 1), (29, 3), (43, 1), (44, 1), (45, 1), (46, 1), (52, 1), (75, 1), (81, 1), (82, 1), (94, 1), (101, 1), (104, 1), (105, 2), (106, 2), (113, 2), (116, 2), (145, 4), (146, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 468.3875 - loglik: -4.0831e+02 - logprior: -6.0080e+01
Epoch 2/2
10/10 - 2s - loss: 409.0344 - loglik: -3.8843e+02 - logprior: -2.0609e+01
Fitted a model with MAP estimate = -397.4126
expansions: [(0, 15)]
discards: [  0  23  27  30 140]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 436.2057 - loglik: -3.8981e+02 - logprior: -4.6398e+01
Epoch 2/2
10/10 - 3s - loss: 390.2573 - loglik: -3.8339e+02 - logprior: -6.8629e+00
Fitted a model with MAP estimate = -381.2723
expansions: [(188, 3)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 438.3521 - loglik: -3.8460e+02 - logprior: -5.3753e+01
Epoch 2/10
10/10 - 2s - loss: 389.0066 - loglik: -3.8015e+02 - logprior: -8.8589e+00
Epoch 3/10
10/10 - 2s - loss: 374.7209 - loglik: -3.7616e+02 - logprior: 1.4433
Epoch 4/10
10/10 - 2s - loss: 370.0048 - loglik: -3.7537e+02 - logprior: 5.3694
Epoch 5/10
10/10 - 2s - loss: 364.2440 - loglik: -3.7164e+02 - logprior: 7.4000
Epoch 6/10
10/10 - 2s - loss: 361.1740 - loglik: -3.6963e+02 - logprior: 8.4600
Epoch 7/10
10/10 - 2s - loss: 360.3333 - loglik: -3.6942e+02 - logprior: 9.0877
Epoch 8/10
10/10 - 2s - loss: 357.6165 - loglik: -3.6732e+02 - logprior: 9.7015
Epoch 9/10
10/10 - 2s - loss: 359.1434 - loglik: -3.6949e+02 - logprior: 10.3514
Fitted a model with MAP estimate = -357.9367
Time for alignment: 71.7668
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 614.1813 - loglik: -5.6137e+02 - logprior: -5.2814e+01
Epoch 2/10
10/10 - 2s - loss: 515.2518 - loglik: -5.0647e+02 - logprior: -8.7843e+00
Epoch 3/10
10/10 - 2s - loss: 453.1153 - loglik: -4.5149e+02 - logprior: -1.6276e+00
Epoch 4/10
10/10 - 2s - loss: 421.6776 - loglik: -4.2210e+02 - logprior: 0.4206
Epoch 5/10
10/10 - 2s - loss: 405.0848 - loglik: -4.0634e+02 - logprior: 1.2585
Epoch 6/10
10/10 - 2s - loss: 401.9803 - loglik: -4.0387e+02 - logprior: 1.8899
Epoch 7/10
10/10 - 2s - loss: 398.2122 - loglik: -4.0039e+02 - logprior: 2.1774
Epoch 8/10
10/10 - 2s - loss: 397.5857 - loglik: -3.9997e+02 - logprior: 2.3889
Epoch 9/10
10/10 - 2s - loss: 397.3263 - loglik: -3.9991e+02 - logprior: 2.5873
Epoch 10/10
10/10 - 2s - loss: 396.1984 - loglik: -3.9901e+02 - logprior: 2.8080
Fitted a model with MAP estimate = -395.9852
expansions: [(0, 3), (19, 2), (22, 2), (23, 1), (25, 1), (29, 3), (44, 3), (45, 1), (46, 1), (52, 1), (71, 1), (75, 3), (81, 2), (100, 1), (104, 1), (107, 2), (114, 1), (116, 1), (117, 2), (118, 1), (145, 3), (146, 1), (149, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 477.5558 - loglik: -4.0940e+02 - logprior: -6.8161e+01
Epoch 2/2
10/10 - 3s - loss: 401.7226 - loglik: -3.8581e+02 - logprior: -1.5911e+01
Fitted a model with MAP estimate = -389.1356
expansions: []
discards: [  0   1   2  23  27  38  96 103 133 148 187 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 441.2599 - loglik: -3.9210e+02 - logprior: -4.9156e+01
Epoch 2/2
10/10 - 2s - loss: 394.8138 - loglik: -3.8693e+02 - logprior: -7.8810e+00
Fitted a model with MAP estimate = -386.6026
expansions: [(0, 13)]
discards: [  0  54 168 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 442.5945 - loglik: -3.9551e+02 - logprior: -4.7085e+01
Epoch 2/10
10/10 - 2s - loss: 393.2617 - loglik: -3.8583e+02 - logprior: -7.4279e+00
Epoch 3/10
10/10 - 2s - loss: 381.8716 - loglik: -3.8264e+02 - logprior: 0.7689
Epoch 4/10
10/10 - 2s - loss: 376.9039 - loglik: -3.8134e+02 - logprior: 4.4379
Epoch 5/10
10/10 - 2s - loss: 370.4768 - loglik: -3.7686e+02 - logprior: 6.3813
Epoch 6/10
10/10 - 2s - loss: 367.6926 - loglik: -3.7519e+02 - logprior: 7.4992
Epoch 7/10
10/10 - 3s - loss: 368.1327 - loglik: -3.7641e+02 - logprior: 8.2760
Fitted a model with MAP estimate = -366.7062
Time for alignment: 68.2460
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.6263 - loglik: -5.6081e+02 - logprior: -5.2815e+01
Epoch 2/10
10/10 - 2s - loss: 515.9366 - loglik: -5.0714e+02 - logprior: -8.7928e+00
Epoch 3/10
10/10 - 2s - loss: 455.0530 - loglik: -4.5334e+02 - logprior: -1.7113e+00
Epoch 4/10
10/10 - 2s - loss: 420.7524 - loglik: -4.2090e+02 - logprior: 0.1428
Epoch 5/10
10/10 - 2s - loss: 407.0770 - loglik: -4.0810e+02 - logprior: 1.0202
Epoch 6/10
10/10 - 2s - loss: 402.4002 - loglik: -4.0407e+02 - logprior: 1.6715
Epoch 7/10
10/10 - 2s - loss: 400.3608 - loglik: -4.0238e+02 - logprior: 2.0206
Epoch 8/10
10/10 - 2s - loss: 398.9354 - loglik: -4.0110e+02 - logprior: 2.1604
Epoch 9/10
10/10 - 2s - loss: 398.8492 - loglik: -4.0119e+02 - logprior: 2.3429
Epoch 10/10
10/10 - 2s - loss: 398.1633 - loglik: -4.0073e+02 - logprior: 2.5655
Fitted a model with MAP estimate = -397.9207
expansions: [(9, 1), (10, 1), (12, 1), (20, 1), (22, 1), (23, 2), (25, 1), (29, 3), (43, 1), (44, 2), (45, 1), (46, 1), (52, 1), (74, 1), (81, 2), (97, 1), (104, 1), (105, 2), (106, 2), (112, 1), (113, 1), (145, 3), (150, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 470.9528 - loglik: -4.1084e+02 - logprior: -6.0111e+01
Epoch 2/2
10/10 - 2s - loss: 411.6085 - loglik: -3.9091e+02 - logprior: -2.0700e+01
Fitted a model with MAP estimate = -398.9096
expansions: [(0, 14)]
discards: [  0  98 129 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 438.3560 - loglik: -3.9177e+02 - logprior: -4.6583e+01
Epoch 2/2
10/10 - 3s - loss: 391.0565 - loglik: -3.8403e+02 - logprior: -7.0310e+00
Fitted a model with MAP estimate = -382.2169
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 41]
Re-initialized the encoder parameters.
Fitting a model of length 198 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 440.0684 - loglik: -3.8705e+02 - logprior: -5.3016e+01
Epoch 2/10
10/10 - 2s - loss: 391.0473 - loglik: -3.8259e+02 - logprior: -8.4606e+00
Epoch 3/10
10/10 - 2s - loss: 378.9797 - loglik: -3.8046e+02 - logprior: 1.4823
Epoch 4/10
10/10 - 2s - loss: 373.0363 - loglik: -3.7837e+02 - logprior: 5.3385
Epoch 5/10
10/10 - 2s - loss: 367.0199 - loglik: -3.7434e+02 - logprior: 7.3190
Epoch 6/10
10/10 - 2s - loss: 364.7094 - loglik: -3.7303e+02 - logprior: 8.3243
Epoch 7/10
10/10 - 2s - loss: 362.6305 - loglik: -3.7157e+02 - logprior: 8.9439
Epoch 8/10
10/10 - 2s - loss: 362.3016 - loglik: -3.7190e+02 - logprior: 9.5971
Epoch 9/10
10/10 - 2s - loss: 360.5179 - loglik: -3.7074e+02 - logprior: 10.2178
Epoch 10/10
10/10 - 2s - loss: 361.3515 - loglik: -3.7212e+02 - logprior: 10.7644
Fitted a model with MAP estimate = -360.4948
Time for alignment: 72.9967
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.8336 - loglik: -5.6103e+02 - logprior: -5.2807e+01
Epoch 2/10
10/10 - 2s - loss: 516.4307 - loglik: -5.0761e+02 - logprior: -8.8162e+00
Epoch 3/10
10/10 - 2s - loss: 455.4697 - loglik: -4.5369e+02 - logprior: -1.7822e+00
Epoch 4/10
10/10 - 2s - loss: 423.4635 - loglik: -4.2355e+02 - logprior: 0.0853
Epoch 5/10
10/10 - 2s - loss: 408.9960 - loglik: -4.1007e+02 - logprior: 1.0720
Epoch 6/10
10/10 - 2s - loss: 405.5275 - loglik: -4.0739e+02 - logprior: 1.8601
Epoch 7/10
10/10 - 2s - loss: 401.8781 - loglik: -4.0407e+02 - logprior: 2.1911
Epoch 8/10
10/10 - 2s - loss: 402.1570 - loglik: -4.0452e+02 - logprior: 2.3605
Fitted a model with MAP estimate = -401.1056
expansions: [(15, 1), (19, 2), (20, 1), (22, 1), (23, 1), (25, 1), (30, 3), (44, 1), (45, 2), (46, 1), (52, 1), (75, 1), (81, 2), (94, 1), (98, 1), (102, 2), (106, 1), (107, 2), (114, 1), (117, 2), (119, 1), (145, 3), (151, 3), (152, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 460.9990 - loglik: -4.1347e+02 - logprior: -4.7532e+01
Epoch 2/2
10/10 - 2s - loss: 399.7870 - loglik: -3.9166e+02 - logprior: -8.1308e+00
Fitted a model with MAP estimate = -389.0892
expansions: [(92, 2)]
discards: [  0  15  21  23  37  56  97 117 122 174 175 176 184 185]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 459.9080 - loglik: -4.0044e+02 - logprior: -5.9469e+01
Epoch 2/2
10/10 - 2s - loss: 412.5595 - loglik: -3.9177e+02 - logprior: -2.0788e+01
Fitted a model with MAP estimate = -404.9791
expansions: [(0, 16)]
discards: [ 0 86]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 443.0860 - loglik: -3.9653e+02 - logprior: -4.6552e+01
Epoch 2/10
10/10 - 3s - loss: 396.9223 - loglik: -3.8973e+02 - logprior: -7.1917e+00
Epoch 3/10
10/10 - 3s - loss: 381.5529 - loglik: -3.8260e+02 - logprior: 1.0517
Epoch 4/10
10/10 - 2s - loss: 378.2922 - loglik: -3.8284e+02 - logprior: 4.5436
Epoch 5/10
10/10 - 2s - loss: 373.5777 - loglik: -3.8003e+02 - logprior: 6.4522
Epoch 6/10
10/10 - 3s - loss: 370.4259 - loglik: -3.7801e+02 - logprior: 7.5855
Epoch 7/10
10/10 - 2s - loss: 367.8633 - loglik: -3.7621e+02 - logprior: 8.3443
Epoch 8/10
10/10 - 3s - loss: 368.4503 - loglik: -3.7737e+02 - logprior: 8.9149
Fitted a model with MAP estimate = -367.3089
Time for alignment: 64.4850
Computed alignments with likelihoods: ['-360.3486', '-357.9367', '-366.7062', '-360.4948', '-367.3089']
Best model has likelihood: -357.9367  (prior= 10.6469 )
time for generating output: 0.2207
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7609460516028147
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.4738 - loglik: -2.3536e+02 - logprior: -3.1136e+00
Epoch 2/10
19/19 - 2s - loss: 208.2375 - loglik: -2.0710e+02 - logprior: -1.1375e+00
Epoch 3/10
19/19 - 2s - loss: 197.8968 - loglik: -1.9670e+02 - logprior: -1.1958e+00
Epoch 4/10
19/19 - 2s - loss: 195.2629 - loglik: -1.9412e+02 - logprior: -1.1413e+00
Epoch 5/10
19/19 - 2s - loss: 194.3978 - loglik: -1.9328e+02 - logprior: -1.1148e+00
Epoch 6/10
19/19 - 2s - loss: 193.6419 - loglik: -1.9252e+02 - logprior: -1.1170e+00
Epoch 7/10
19/19 - 1s - loss: 193.9039 - loglik: -1.9280e+02 - logprior: -1.1025e+00
Fitted a model with MAP estimate = -186.4598
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 195.7804 - loglik: -1.9173e+02 - logprior: -4.0456e+00
Epoch 2/2
19/19 - 2s - loss: 187.5910 - loglik: -1.8620e+02 - logprior: -1.3947e+00
Fitted a model with MAP estimate = -179.7428
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 27 28 63]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 193.4240 - loglik: -1.8969e+02 - logprior: -3.7324e+00
Epoch 2/2
19/19 - 2s - loss: 189.0833 - loglik: -1.8751e+02 - logprior: -1.5721e+00
Fitted a model with MAP estimate = -180.7956
expansions: [(0, 4), (22, 3), (25, 1)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.6216 - loglik: -1.7974e+02 - logprior: -1.8774e+00
Epoch 2/10
23/23 - 2s - loss: 178.7294 - loglik: -1.7776e+02 - logprior: -9.7237e-01
Epoch 3/10
23/23 - 2s - loss: 177.9599 - loglik: -1.7702e+02 - logprior: -9.4138e-01
Epoch 4/10
23/23 - 2s - loss: 177.4799 - loglik: -1.7657e+02 - logprior: -9.0873e-01
Epoch 5/10
23/23 - 2s - loss: 176.8014 - loglik: -1.7590e+02 - logprior: -8.9646e-01
Epoch 6/10
23/23 - 2s - loss: 176.7899 - loglik: -1.7590e+02 - logprior: -8.8654e-01
Epoch 7/10
23/23 - 2s - loss: 176.3277 - loglik: -1.7545e+02 - logprior: -8.7332e-01
Epoch 8/10
23/23 - 2s - loss: 176.3864 - loglik: -1.7552e+02 - logprior: -8.6632e-01
Fitted a model with MAP estimate = -176.2382
Time for alignment: 60.9910
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.5330 - loglik: -2.3543e+02 - logprior: -3.1036e+00
Epoch 2/10
19/19 - 2s - loss: 207.4626 - loglik: -2.0632e+02 - logprior: -1.1412e+00
Epoch 3/10
19/19 - 2s - loss: 197.5472 - loglik: -1.9634e+02 - logprior: -1.2098e+00
Epoch 4/10
19/19 - 2s - loss: 195.5044 - loglik: -1.9439e+02 - logprior: -1.1164e+00
Epoch 5/10
19/19 - 1s - loss: 194.7373 - loglik: -1.9365e+02 - logprior: -1.0887e+00
Epoch 6/10
19/19 - 2s - loss: 194.4704 - loglik: -1.9341e+02 - logprior: -1.0611e+00
Epoch 7/10
19/19 - 2s - loss: 194.1886 - loglik: -1.9312e+02 - logprior: -1.0648e+00
Epoch 8/10
19/19 - 2s - loss: 194.0701 - loglik: -1.9301e+02 - logprior: -1.0591e+00
Epoch 9/10
19/19 - 1s - loss: 194.1452 - loglik: -1.9309e+02 - logprior: -1.0518e+00
Fitted a model with MAP estimate = -187.2987
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (46, 3), (47, 1), (48, 2), (49, 2), (52, 2), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 196.4314 - loglik: -1.9230e+02 - logprior: -4.1307e+00
Epoch 2/2
19/19 - 2s - loss: 187.6805 - loglik: -1.8624e+02 - logprior: -1.4422e+00
Fitted a model with MAP estimate = -179.5524
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 62 70 76]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 193.0509 - loglik: -1.8933e+02 - logprior: -3.7213e+00
Epoch 2/2
19/19 - 2s - loss: 188.4933 - loglik: -1.8690e+02 - logprior: -1.5936e+00
Fitted a model with MAP estimate = -180.2608
expansions: [(0, 4), (21, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 180.8298 - loglik: -1.7897e+02 - logprior: -1.8627e+00
Epoch 2/10
23/23 - 2s - loss: 177.5932 - loglik: -1.7665e+02 - logprior: -9.4637e-01
Epoch 3/10
23/23 - 2s - loss: 177.1941 - loglik: -1.7626e+02 - logprior: -9.2913e-01
Epoch 4/10
23/23 - 2s - loss: 176.8718 - loglik: -1.7598e+02 - logprior: -8.9046e-01
Epoch 5/10
23/23 - 2s - loss: 176.0574 - loglik: -1.7518e+02 - logprior: -8.7963e-01
Epoch 6/10
23/23 - 2s - loss: 175.9612 - loglik: -1.7509e+02 - logprior: -8.7064e-01
Epoch 7/10
23/23 - 2s - loss: 175.6520 - loglik: -1.7480e+02 - logprior: -8.4995e-01
Epoch 8/10
23/23 - 2s - loss: 175.6464 - loglik: -1.7480e+02 - logprior: -8.4447e-01
Epoch 9/10
23/23 - 2s - loss: 175.4734 - loglik: -1.7464e+02 - logprior: -8.2860e-01
Epoch 10/10
23/23 - 2s - loss: 175.5052 - loglik: -1.7469e+02 - logprior: -8.1694e-01
Fitted a model with MAP estimate = -175.4244
Time for alignment: 67.0059
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 238.2033 - loglik: -2.3509e+02 - logprior: -3.1088e+00
Epoch 2/10
19/19 - 2s - loss: 208.4029 - loglik: -2.0727e+02 - logprior: -1.1372e+00
Epoch 3/10
19/19 - 2s - loss: 198.0167 - loglik: -1.9684e+02 - logprior: -1.1802e+00
Epoch 4/10
19/19 - 1s - loss: 195.2160 - loglik: -1.9406e+02 - logprior: -1.1587e+00
Epoch 5/10
19/19 - 1s - loss: 194.1615 - loglik: -1.9303e+02 - logprior: -1.1300e+00
Epoch 6/10
19/19 - 1s - loss: 193.2863 - loglik: -1.9217e+02 - logprior: -1.1199e+00
Epoch 7/10
19/19 - 1s - loss: 193.1706 - loglik: -1.9206e+02 - logprior: -1.1061e+00
Epoch 8/10
19/19 - 1s - loss: 193.0071 - loglik: -1.9189e+02 - logprior: -1.1142e+00
Epoch 9/10
19/19 - 1s - loss: 193.1090 - loglik: -1.9200e+02 - logprior: -1.1057e+00
Fitted a model with MAP estimate = -185.9680
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 2), (18, 1), (23, 1), (31, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.0499 - loglik: -1.9189e+02 - logprior: -4.1608e+00
Epoch 2/2
19/19 - 2s - loss: 188.9787 - loglik: -1.8763e+02 - logprior: -1.3475e+00
Fitted a model with MAP estimate = -180.4843
expansions: [(9, 2), (19, 4)]
discards: [ 0  1 23 57]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 190.7296 - loglik: -1.8784e+02 - logprior: -2.8846e+00
Epoch 2/2
19/19 - 2s - loss: 186.3704 - loglik: -1.8541e+02 - logprior: -9.6092e-01
Fitted a model with MAP estimate = -178.9196
expansions: [(0, 2)]
discards: [ 8  9 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 182.0696 - loglik: -1.7977e+02 - logprior: -2.2988e+00
Epoch 2/10
23/23 - 2s - loss: 179.4735 - loglik: -1.7848e+02 - logprior: -9.9716e-01
Epoch 3/10
23/23 - 2s - loss: 178.9959 - loglik: -1.7806e+02 - logprior: -9.3778e-01
Epoch 4/10
23/23 - 2s - loss: 178.1762 - loglik: -1.7730e+02 - logprior: -8.7594e-01
Epoch 5/10
23/23 - 2s - loss: 177.8607 - loglik: -1.7699e+02 - logprior: -8.6596e-01
Epoch 6/10
23/23 - 2s - loss: 177.3636 - loglik: -1.7652e+02 - logprior: -8.4795e-01
Epoch 7/10
23/23 - 2s - loss: 177.8679 - loglik: -1.7704e+02 - logprior: -8.2955e-01
Fitted a model with MAP estimate = -177.3732
Time for alignment: 61.2669
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.3364 - loglik: -2.3523e+02 - logprior: -3.1079e+00
Epoch 2/10
19/19 - 2s - loss: 206.7238 - loglik: -2.0559e+02 - logprior: -1.1324e+00
Epoch 3/10
19/19 - 1s - loss: 197.3710 - loglik: -1.9616e+02 - logprior: -1.2138e+00
Epoch 4/10
19/19 - 2s - loss: 195.4510 - loglik: -1.9432e+02 - logprior: -1.1322e+00
Epoch 5/10
19/19 - 2s - loss: 194.3818 - loglik: -1.9328e+02 - logprior: -1.1026e+00
Epoch 6/10
19/19 - 2s - loss: 193.6751 - loglik: -1.9260e+02 - logprior: -1.0783e+00
Epoch 7/10
19/19 - 2s - loss: 193.5621 - loglik: -1.9248e+02 - logprior: -1.0792e+00
Epoch 8/10
19/19 - 1s - loss: 193.2378 - loglik: -1.9216e+02 - logprior: -1.0729e+00
Epoch 9/10
19/19 - 2s - loss: 193.3627 - loglik: -1.9229e+02 - logprior: -1.0712e+00
Fitted a model with MAP estimate = -186.4587
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (47, 2), (48, 2), (49, 4), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.1122 - loglik: -1.9197e+02 - logprior: -4.1383e+00
Epoch 2/2
19/19 - 2s - loss: 187.7328 - loglik: -1.8632e+02 - logprior: -1.4169e+00
Fitted a model with MAP estimate = -179.6432
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 63 65]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 192.8628 - loglik: -1.8913e+02 - logprior: -3.7296e+00
Epoch 2/2
19/19 - 2s - loss: 188.5789 - loglik: -1.8706e+02 - logprior: -1.5193e+00
Fitted a model with MAP estimate = -180.2693
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 180.7741 - loglik: -1.7891e+02 - logprior: -1.8613e+00
Epoch 2/10
23/23 - 2s - loss: 178.2423 - loglik: -1.7730e+02 - logprior: -9.4211e-01
Epoch 3/10
23/23 - 2s - loss: 178.3226 - loglik: -1.7742e+02 - logprior: -9.0435e-01
Fitted a model with MAP estimate = -177.5927
Time for alignment: 52.8191
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.5750 - loglik: -2.3546e+02 - logprior: -3.1117e+00
Epoch 2/10
19/19 - 2s - loss: 207.8488 - loglik: -2.0670e+02 - logprior: -1.1515e+00
Epoch 3/10
19/19 - 1s - loss: 196.5051 - loglik: -1.9526e+02 - logprior: -1.2432e+00
Epoch 4/10
19/19 - 1s - loss: 194.4214 - loglik: -1.9323e+02 - logprior: -1.1937e+00
Epoch 5/10
19/19 - 1s - loss: 193.9465 - loglik: -1.9280e+02 - logprior: -1.1507e+00
Epoch 6/10
19/19 - 2s - loss: 193.1861 - loglik: -1.9204e+02 - logprior: -1.1481e+00
Epoch 7/10
19/19 - 2s - loss: 193.1393 - loglik: -1.9202e+02 - logprior: -1.1168e+00
Epoch 8/10
19/19 - 2s - loss: 193.0802 - loglik: -1.9198e+02 - logprior: -1.1004e+00
Epoch 9/10
19/19 - 2s - loss: 192.8744 - loglik: -1.9177e+02 - logprior: -1.0999e+00
Epoch 10/10
19/19 - 2s - loss: 192.7390 - loglik: -1.9164e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -186.0420
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (18, 1), (23, 1), (42, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.0974 - loglik: -1.9188e+02 - logprior: -4.2170e+00
Epoch 2/2
19/19 - 2s - loss: 188.4790 - loglik: -1.8712e+02 - logprior: -1.3612e+00
Fitted a model with MAP estimate = -180.0975
expansions: [(9, 2)]
discards: [ 0 58]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 191.1513 - loglik: -1.8730e+02 - logprior: -3.8549e+00
Epoch 2/2
19/19 - 2s - loss: 186.8959 - loglik: -1.8548e+02 - logprior: -1.4133e+00
Fitted a model with MAP estimate = -179.1289
expansions: [(24, 3), (28, 1)]
discards: [ 0  1  9 10 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 182.5600 - loglik: -1.8035e+02 - logprior: -2.2102e+00
Epoch 2/10
23/23 - 2s - loss: 178.9247 - loglik: -1.7808e+02 - logprior: -8.4634e-01
Epoch 3/10
23/23 - 2s - loss: 178.2899 - loglik: -1.7747e+02 - logprior: -8.1782e-01
Epoch 4/10
23/23 - 2s - loss: 178.1317 - loglik: -1.7736e+02 - logprior: -7.7332e-01
Epoch 5/10
23/23 - 2s - loss: 177.3365 - loglik: -1.7655e+02 - logprior: -7.8292e-01
Epoch 6/10
23/23 - 2s - loss: 177.2288 - loglik: -1.7647e+02 - logprior: -7.5950e-01
Epoch 7/10
23/23 - 2s - loss: 176.9389 - loglik: -1.7617e+02 - logprior: -7.6574e-01
Epoch 8/10
23/23 - 2s - loss: 176.9268 - loglik: -1.7618e+02 - logprior: -7.4785e-01
Epoch 9/10
23/23 - 2s - loss: 176.9604 - loglik: -1.7623e+02 - logprior: -7.3499e-01
Fitted a model with MAP estimate = -176.8350
Time for alignment: 64.4579
Computed alignments with likelihoods: ['-176.2382', '-175.4244', '-177.3732', '-177.5927', '-176.8350']
Best model has likelihood: -175.4244  (prior= -0.8020 )
time for generating output: 0.1247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.847926267281106
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 797.4655 - loglik: -7.9172e+02 - logprior: -5.7451e+00
Epoch 2/10
22/22 - 12s - loss: 683.6179 - loglik: -6.8304e+02 - logprior: -5.8152e-01
Epoch 3/10
22/22 - 12s - loss: 646.7664 - loglik: -6.4518e+02 - logprior: -1.5907e+00
Epoch 4/10
22/22 - 12s - loss: 639.8215 - loglik: -6.3833e+02 - logprior: -1.4918e+00
Epoch 5/10
22/22 - 12s - loss: 635.9275 - loglik: -6.3443e+02 - logprior: -1.5003e+00
Epoch 6/10
22/22 - 12s - loss: 637.3084 - loglik: -6.3583e+02 - logprior: -1.4762e+00
Fitted a model with MAP estimate = -636.4851
expansions: [(12, 1), (13, 1), (14, 1), (31, 1), (32, 3), (33, 3), (34, 2), (45, 1), (47, 1), (48, 1), (49, 1), (69, 1), (70, 2), (71, 1), (75, 2), (78, 1), (80, 1), (96, 1), (97, 1), (102, 1), (103, 2), (108, 1), (117, 1), (119, 2), (142, 2), (143, 1), (148, 1), (149, 1), (151, 1), (155, 1), (156, 3), (157, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 1), (194, 1), (206, 1), (209, 1), (210, 1), (213, 1), (214, 2), (216, 2), (222, 3), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 321 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 639.8207 - loglik: -6.3144e+02 - logprior: -8.3773e+00
Epoch 2/2
22/22 - 17s - loss: 622.5485 - loglik: -6.2017e+02 - logprior: -2.3767e+00
Fitted a model with MAP estimate = -617.0617
expansions: [(0, 3), (103, 1), (294, 1)]
discards: [  0  35  40 175 195 196 197 222 232 233 278 286 287]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 626.2015 - loglik: -6.2145e+02 - logprior: -4.7500e+00
Epoch 2/2
22/22 - 17s - loss: 613.5449 - loglik: -6.1470e+02 - logprior: 1.1577
Fitted a model with MAP estimate = -613.8217
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 630.1830 - loglik: -6.2284e+02 - logprior: -7.3416e+00
Epoch 2/10
22/22 - 16s - loss: 619.9954 - loglik: -6.1859e+02 - logprior: -1.4102e+00
Epoch 3/10
22/22 - 16s - loss: 615.4625 - loglik: -6.1567e+02 - logprior: 0.2086
Epoch 4/10
22/22 - 16s - loss: 611.8694 - loglik: -6.1443e+02 - logprior: 2.5574
Epoch 5/10
22/22 - 16s - loss: 609.9386 - loglik: -6.1272e+02 - logprior: 2.7822
Epoch 6/10
22/22 - 16s - loss: 609.5091 - loglik: -6.1243e+02 - logprior: 2.9236
Epoch 7/10
22/22 - 16s - loss: 608.8148 - loglik: -6.1193e+02 - logprior: 3.1162
Epoch 8/10
22/22 - 16s - loss: 608.9583 - loglik: -6.1231e+02 - logprior: 3.3476
Fitted a model with MAP estimate = -608.3932
Time for alignment: 334.6030
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 796.4446 - loglik: -7.9072e+02 - logprior: -5.7268e+00
Epoch 2/10
22/22 - 12s - loss: 687.5821 - loglik: -6.8703e+02 - logprior: -5.4841e-01
Epoch 3/10
22/22 - 12s - loss: 651.1679 - loglik: -6.4969e+02 - logprior: -1.4802e+00
Epoch 4/10
22/22 - 12s - loss: 640.0435 - loglik: -6.3857e+02 - logprior: -1.4752e+00
Epoch 5/10
22/22 - 12s - loss: 637.6177 - loglik: -6.3607e+02 - logprior: -1.5450e+00
Epoch 6/10
22/22 - 12s - loss: 639.9546 - loglik: -6.3839e+02 - logprior: -1.5652e+00
Fitted a model with MAP estimate = -637.3095
expansions: [(14, 1), (15, 1), (32, 1), (33, 2), (35, 2), (36, 1), (46, 3), (48, 2), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (77, 1), (80, 1), (82, 1), (84, 1), (98, 1), (100, 1), (104, 1), (105, 2), (110, 1), (119, 1), (121, 2), (132, 1), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 2), (158, 1), (178, 2), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 1), (194, 1), (206, 1), (207, 1), (208, 1), (209, 1), (213, 2), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 642.0235 - loglik: -6.3371e+02 - logprior: -8.3136e+00
Epoch 2/2
22/22 - 17s - loss: 619.5341 - loglik: -6.1736e+02 - logprior: -2.1703e+00
Fitted a model with MAP estimate = -617.4401
expansions: [(0, 3), (194, 1)]
discards: [  0  39  55 176 229]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 624.8544 - loglik: -6.2002e+02 - logprior: -4.8297e+00
Epoch 2/2
22/22 - 17s - loss: 616.5556 - loglik: -6.1772e+02 - logprior: 1.1610
Fitted a model with MAP estimate = -613.9733
expansions: []
discards: [  0   1   2  37  38 269]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 630.0546 - loglik: -6.2273e+02 - logprior: -7.3293e+00
Epoch 2/10
22/22 - 16s - loss: 621.5456 - loglik: -6.2016e+02 - logprior: -1.3891e+00
Epoch 3/10
22/22 - 16s - loss: 616.9338 - loglik: -6.1713e+02 - logprior: 0.2012
Epoch 4/10
22/22 - 16s - loss: 613.0120 - loglik: -6.1562e+02 - logprior: 2.6034
Epoch 5/10
22/22 - 16s - loss: 612.2883 - loglik: -6.1508e+02 - logprior: 2.7945
Epoch 6/10
22/22 - 16s - loss: 611.2774 - loglik: -6.1423e+02 - logprior: 2.9484
Epoch 7/10
22/22 - 16s - loss: 608.2151 - loglik: -6.1129e+02 - logprior: 3.0733
Epoch 8/10
22/22 - 16s - loss: 609.9003 - loglik: -6.1324e+02 - logprior: 3.3410
Fitted a model with MAP estimate = -609.4410
Time for alignment: 333.3715
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 797.1475 - loglik: -7.9141e+02 - logprior: -5.7357e+00
Epoch 2/10
22/22 - 12s - loss: 687.7328 - loglik: -6.8709e+02 - logprior: -6.4498e-01
Epoch 3/10
22/22 - 12s - loss: 647.6492 - loglik: -6.4592e+02 - logprior: -1.7287e+00
Epoch 4/10
22/22 - 12s - loss: 641.4869 - loglik: -6.3994e+02 - logprior: -1.5508e+00
Epoch 5/10
22/22 - 12s - loss: 638.9498 - loglik: -6.3743e+02 - logprior: -1.5167e+00
Epoch 6/10
22/22 - 12s - loss: 638.3310 - loglik: -6.3683e+02 - logprior: -1.5059e+00
Epoch 7/10
22/22 - 12s - loss: 636.9957 - loglik: -6.3549e+02 - logprior: -1.5017e+00
Epoch 8/10
22/22 - 12s - loss: 636.9175 - loglik: -6.3543e+02 - logprior: -1.4858e+00
Epoch 9/10
22/22 - 12s - loss: 637.1711 - loglik: -6.3569e+02 - logprior: -1.4766e+00
Fitted a model with MAP estimate = -636.7199
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (48, 1), (49, 1), (59, 1), (61, 1), (69, 1), (70, 1), (75, 1), (76, 2), (79, 1), (81, 1), (97, 1), (99, 1), (104, 2), (109, 1), (118, 1), (120, 2), (136, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 2), (156, 2), (158, 1), (178, 2), (179, 1), (180, 1), (184, 1), (185, 1), (194, 2), (202, 1), (206, 1), (207, 1), (208, 1), (209, 1), (214, 2), (215, 1), (224, 1), (225, 1), (227, 1), (236, 1), (238, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 637.9863 - loglik: -6.3272e+02 - logprior: -5.2620e+00
Epoch 2/2
22/22 - 17s - loss: 617.3212 - loglik: -6.1809e+02 - logprior: 0.7721
Fitted a model with MAP estimate = -615.5942
expansions: [(102, 1), (128, 1)]
discards: [ 34 175 268]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 624.2237 - loglik: -6.1969e+02 - logprior: -4.5356e+00
Epoch 2/2
22/22 - 17s - loss: 618.0139 - loglik: -6.1933e+02 - logprior: 1.3134
Fitted a model with MAP estimate = -614.0937
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 623.9609 - loglik: -6.1972e+02 - logprior: -4.2414e+00
Epoch 2/10
22/22 - 16s - loss: 614.8027 - loglik: -6.1647e+02 - logprior: 1.6683
Epoch 3/10
22/22 - 17s - loss: 613.9714 - loglik: -6.1627e+02 - logprior: 2.2977
Epoch 4/10
22/22 - 17s - loss: 611.0469 - loglik: -6.1375e+02 - logprior: 2.7042
Epoch 5/10
22/22 - 17s - loss: 607.3389 - loglik: -6.1023e+02 - logprior: 2.8918
Epoch 6/10
22/22 - 17s - loss: 611.7985 - loglik: -6.1489e+02 - logprior: 3.0903
Fitted a model with MAP estimate = -608.2705
Time for alignment: 336.1375
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 796.8555 - loglik: -7.9113e+02 - logprior: -5.7261e+00
Epoch 2/10
22/22 - 12s - loss: 687.1278 - loglik: -6.8656e+02 - logprior: -5.6978e-01
Epoch 3/10
22/22 - 12s - loss: 645.4453 - loglik: -6.4393e+02 - logprior: -1.5190e+00
Epoch 4/10
22/22 - 12s - loss: 639.5711 - loglik: -6.3812e+02 - logprior: -1.4479e+00
Epoch 5/10
22/22 - 12s - loss: 638.2825 - loglik: -6.3686e+02 - logprior: -1.4222e+00
Epoch 6/10
22/22 - 12s - loss: 637.7112 - loglik: -6.3631e+02 - logprior: -1.3982e+00
Epoch 7/10
22/22 - 12s - loss: 636.1110 - loglik: -6.3471e+02 - logprior: -1.4046e+00
Epoch 8/10
22/22 - 12s - loss: 637.6948 - loglik: -6.3632e+02 - logprior: -1.3731e+00
Fitted a model with MAP estimate = -636.3047
expansions: [(12, 1), (13, 1), (32, 1), (33, 2), (34, 3), (35, 1), (45, 2), (48, 1), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (79, 1), (81, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (136, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 2), (156, 1), (158, 1), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 2), (194, 1), (207, 1), (208, 1), (209, 1), (213, 2), (215, 1), (224, 1), (225, 1), (227, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 640.6947 - loglik: -6.3249e+02 - logprior: -8.2023e+00
Epoch 2/2
22/22 - 17s - loss: 620.6311 - loglik: -6.1842e+02 - logprior: -2.2065e+00
Fitted a model with MAP estimate = -617.4448
expansions: [(0, 3), (102, 1)]
discards: [  0  39 176 219 229]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 623.7691 - loglik: -6.1914e+02 - logprior: -4.6255e+00
Epoch 2/2
22/22 - 17s - loss: 615.5287 - loglik: -6.1673e+02 - logprior: 1.1975
Fitted a model with MAP estimate = -613.9597
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 629.5359 - loglik: -6.2220e+02 - logprior: -7.3352e+00
Epoch 2/10
22/22 - 16s - loss: 622.3310 - loglik: -6.2103e+02 - logprior: -1.2977e+00
Epoch 3/10
22/22 - 16s - loss: 613.6420 - loglik: -6.1391e+02 - logprior: 0.2638
Epoch 4/10
22/22 - 16s - loss: 612.5219 - loglik: -6.1527e+02 - logprior: 2.7475
Epoch 5/10
22/22 - 16s - loss: 610.6826 - loglik: -6.1365e+02 - logprior: 2.9723
Epoch 6/10
22/22 - 17s - loss: 607.9965 - loglik: -6.1113e+02 - logprior: 3.1354
Epoch 7/10
22/22 - 16s - loss: 609.7034 - loglik: -6.1307e+02 - logprior: 3.3670
Fitted a model with MAP estimate = -608.5239
Time for alignment: 339.9137
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 796.5800 - loglik: -7.9085e+02 - logprior: -5.7291e+00
Epoch 2/10
22/22 - 12s - loss: 684.5912 - loglik: -6.8402e+02 - logprior: -5.7429e-01
Epoch 3/10
22/22 - 12s - loss: 645.9380 - loglik: -6.4435e+02 - logprior: -1.5922e+00
Epoch 4/10
22/22 - 12s - loss: 639.9642 - loglik: -6.3849e+02 - logprior: -1.4788e+00
Epoch 5/10
22/22 - 12s - loss: 637.4767 - loglik: -6.3601e+02 - logprior: -1.4626e+00
Epoch 6/10
22/22 - 12s - loss: 638.5323 - loglik: -6.3707e+02 - logprior: -1.4614e+00
Fitted a model with MAP estimate = -636.9045
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 2), (47, 1), (48, 2), (49, 1), (66, 1), (70, 1), (71, 1), (76, 1), (77, 2), (80, 1), (82, 1), (84, 1), (98, 1), (100, 1), (105, 2), (110, 1), (119, 1), (121, 2), (137, 1), (144, 1), (145, 2), (149, 1), (153, 1), (156, 3), (158, 1), (175, 1), (178, 2), (179, 1), (180, 1), (184, 1), (185, 1), (194, 2), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 640.8408 - loglik: -6.3262e+02 - logprior: -8.2224e+00
Epoch 2/2
22/22 - 17s - loss: 622.9843 - loglik: -6.2093e+02 - logprior: -2.0579e+00
Fitted a model with MAP estimate = -618.6514
expansions: [(0, 3), (130, 1)]
discards: [  0  34  56 177]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 626.3599 - loglik: -6.2162e+02 - logprior: -4.7427e+00
Epoch 2/2
22/22 - 17s - loss: 616.3192 - loglik: -6.1748e+02 - logprior: 1.1631
Fitted a model with MAP estimate = -614.5978
expansions: []
discards: [  0   1   2 100]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 631.2833 - loglik: -6.2401e+02 - logprior: -7.2754e+00
Epoch 2/10
22/22 - 16s - loss: 620.4444 - loglik: -6.1905e+02 - logprior: -1.3940e+00
Epoch 3/10
22/22 - 16s - loss: 617.6143 - loglik: -6.1786e+02 - logprior: 0.2483
Epoch 4/10
22/22 - 16s - loss: 613.4153 - loglik: -6.1603e+02 - logprior: 2.6197
Epoch 5/10
22/22 - 16s - loss: 611.3785 - loglik: -6.1416e+02 - logprior: 2.7800
Epoch 6/10
22/22 - 16s - loss: 609.4867 - loglik: -6.1239e+02 - logprior: 2.9012
Epoch 7/10
22/22 - 16s - loss: 609.4409 - loglik: -6.1254e+02 - logprior: 3.0951
Epoch 8/10
22/22 - 16s - loss: 611.5424 - loglik: -6.1494e+02 - logprior: 3.3948
Fitted a model with MAP estimate = -609.3509
Time for alignment: 328.9771
Computed alignments with likelihoods: ['-608.3932', '-609.4410', '-608.2705', '-608.5239', '-609.3509']
Best model has likelihood: -608.2705  (prior= 3.2169 )
time for generating output: 0.3429
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9098373852841732
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 172.2723 - loglik: -7.9882e+01 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 95.0649 - loglik: -6.8475e+01 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 73.1042 - loglik: -5.9804e+01 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 63.8210 - loglik: -5.5631e+01 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 1s - loss: 59.0931 - loglik: -5.3508e+01 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 56.9755 - loglik: -5.2824e+01 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 0s - loss: 55.8668 - loglik: -5.2525e+01 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 55.2499 - loglik: -5.2421e+01 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 54.9400 - loglik: -5.2477e+01 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 54.7234 - loglik: -5.2562e+01 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -54.6323
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.8453 - loglik: -4.9196e+01 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 86.2875 - loglik: -4.6151e+01 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -69.3836
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.4026 - loglik: -4.3469e+01 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 68.8645 - loglik: -4.3803e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 0s - loss: 56.4869 - loglik: -4.4198e+01 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 0s - loss: 51.5536 - loglik: -4.4401e+01 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 0s - loss: 48.9845 - loglik: -4.4512e+01 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 47.5962 - loglik: -4.4565e+01 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 46.8453 - loglik: -4.4671e+01 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 46.1903 - loglik: -4.4650e+01 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 45.3108 - loglik: -4.4233e+01 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 44.9216 - loglik: -4.4187e+01 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -44.7878
Time for alignment: 24.9465
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.2723 - loglik: -7.9882e+01 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 95.0649 - loglik: -6.8475e+01 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 0s - loss: 73.1042 - loglik: -5.9804e+01 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 0s - loss: 63.8210 - loglik: -5.5631e+01 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 59.0931 - loglik: -5.3508e+01 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 56.9755 - loglik: -5.2824e+01 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 0s - loss: 55.8668 - loglik: -5.2525e+01 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 55.2499 - loglik: -5.2421e+01 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 54.9400 - loglik: -5.2477e+01 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 54.7235 - loglik: -5.2562e+01 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -54.6323
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 172.8453 - loglik: -4.9196e+01 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 86.2875 - loglik: -4.6151e+01 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -69.3836
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4026 - loglik: -4.3469e+01 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 0s - loss: 68.8645 - loglik: -4.3803e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 0s - loss: 56.4869 - loglik: -4.4198e+01 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 51.5536 - loglik: -4.4401e+01 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 48.9845 - loglik: -4.4512e+01 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 47.5962 - loglik: -4.4565e+01 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 46.8453 - loglik: -4.4671e+01 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 46.1903 - loglik: -4.4650e+01 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 45.3108 - loglik: -4.4232e+01 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 44.9216 - loglik: -4.4187e+01 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -44.7878
Time for alignment: 20.7873
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.2723 - loglik: -7.9882e+01 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 95.0649 - loglik: -6.8475e+01 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 73.1042 - loglik: -5.9804e+01 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 63.8210 - loglik: -5.5631e+01 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 59.0931 - loglik: -5.3508e+01 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 56.9755 - loglik: -5.2824e+01 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 0s - loss: 55.8668 - loglik: -5.2525e+01 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 0s - loss: 55.2499 - loglik: -5.2421e+01 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 54.9400 - loglik: -5.2477e+01 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 0s - loss: 54.7235 - loglik: -5.2562e+01 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -54.6323
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.8453 - loglik: -4.9196e+01 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 86.2875 - loglik: -4.6151e+01 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -69.3836
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4027 - loglik: -4.3469e+01 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 68.8645 - loglik: -4.3803e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 56.4869 - loglik: -4.4198e+01 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 51.5536 - loglik: -4.4401e+01 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 48.9845 - loglik: -4.4512e+01 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 47.5962 - loglik: -4.4565e+01 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 0s - loss: 46.8453 - loglik: -4.4671e+01 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 46.1903 - loglik: -4.4650e+01 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 0s - loss: 45.3108 - loglik: -4.4233e+01 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 0s - loss: 44.9216 - loglik: -4.4187e+01 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -44.7878
Time for alignment: 23.5326
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.2723 - loglik: -7.9882e+01 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 0s - loss: 95.0649 - loglik: -6.8475e+01 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 73.1042 - loglik: -5.9804e+01 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 63.8210 - loglik: -5.5631e+01 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 1s - loss: 59.0931 - loglik: -5.3508e+01 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 1s - loss: 56.9755 - loglik: -5.2824e+01 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 1s - loss: 55.8668 - loglik: -5.2525e+01 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 0s - loss: 55.2499 - loglik: -5.2421e+01 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 0s - loss: 54.9400 - loglik: -5.2477e+01 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 54.7234 - loglik: -5.2562e+01 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -54.6324
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.8453 - loglik: -4.9196e+01 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 1s - loss: 86.2875 - loglik: -4.6151e+01 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -69.3836
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4026 - loglik: -4.3469e+01 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 68.8645 - loglik: -4.3803e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 56.4869 - loglik: -4.4198e+01 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 51.5536 - loglik: -4.4401e+01 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 48.9845 - loglik: -4.4512e+01 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 47.5962 - loglik: -4.4565e+01 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 46.8453 - loglik: -4.4671e+01 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 46.1903 - loglik: -4.4649e+01 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 45.3107 - loglik: -4.4232e+01 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 44.9216 - loglik: -4.4187e+01 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -44.7878
Time for alignment: 23.2034
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.2723 - loglik: -7.9882e+01 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 0s - loss: 95.0649 - loglik: -6.8475e+01 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 73.1042 - loglik: -5.9804e+01 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 63.8210 - loglik: -5.5631e+01 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 1s - loss: 59.0932 - loglik: -5.3508e+01 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 56.9755 - loglik: -5.2824e+01 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 0s - loss: 55.8668 - loglik: -5.2525e+01 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 55.2499 - loglik: -5.2421e+01 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 0s - loss: 54.9400 - loglik: -5.2477e+01 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 54.7234 - loglik: -5.2562e+01 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -54.6323
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.8453 - loglik: -4.9196e+01 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 86.2875 - loglik: -4.6151e+01 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -69.3836
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4026 - loglik: -4.3469e+01 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 0s - loss: 68.8645 - loglik: -4.3803e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 0s - loss: 56.4869 - loglik: -4.4198e+01 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 0s - loss: 51.5536 - loglik: -4.4401e+01 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 48.9845 - loglik: -4.4512e+01 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 47.5962 - loglik: -4.4565e+01 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 46.8453 - loglik: -4.4671e+01 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 46.1903 - loglik: -4.4650e+01 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 0s - loss: 45.3108 - loglik: -4.4232e+01 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 44.9216 - loglik: -4.4187e+01 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -44.7879
Time for alignment: 22.7648
Computed alignments with likelihoods: ['-44.7878', '-44.7878', '-44.7878', '-44.7878', '-44.7879']
Best model has likelihood: -44.7878  (prior= -0.5813 )
time for generating output: 0.1083
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.6286 - loglik: -3.5032e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 2s - loss: 310.5128 - loglik: -3.0757e+02 - logprior: -2.9457e+00
Epoch 3/10
11/11 - 2s - loss: 268.5008 - loglik: -2.6653e+02 - logprior: -1.9714e+00
Epoch 4/10
11/11 - 2s - loss: 249.2402 - loglik: -2.4712e+02 - logprior: -2.1245e+00
Epoch 5/10
11/11 - 2s - loss: 245.5300 - loglik: -2.4335e+02 - logprior: -2.1841e+00
Epoch 6/10
11/11 - 2s - loss: 240.5491 - loglik: -2.3837e+02 - logprior: -2.1781e+00
Epoch 7/10
11/11 - 2s - loss: 240.3585 - loglik: -2.3827e+02 - logprior: -2.0879e+00
Epoch 8/10
11/11 - 2s - loss: 239.3391 - loglik: -2.3733e+02 - logprior: -2.0115e+00
Epoch 9/10
11/11 - 2s - loss: 237.2540 - loglik: -2.3525e+02 - logprior: -2.0074e+00
Epoch 10/10
11/11 - 2s - loss: 239.0063 - loglik: -2.3698e+02 - logprior: -2.0264e+00
Fitted a model with MAP estimate = -238.3502
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 254.0108 - loglik: -2.3979e+02 - logprior: -1.4223e+01
Epoch 2/2
11/11 - 2s - loss: 232.6342 - loglik: -2.2658e+02 - logprior: -6.0505e+00
Fitted a model with MAP estimate = -226.5406
expansions: [(0, 20)]
discards: [  0   8  14  84  87 110]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 228.0672 - loglik: -2.2057e+02 - logprior: -7.5003e+00
Epoch 2/2
22/22 - 3s - loss: 213.1110 - loglik: -2.1144e+02 - logprior: -1.6730e+00
Fitted a model with MAP estimate = -211.8313
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 93]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 234.4319 - loglik: -2.2081e+02 - logprior: -1.3623e+01
Epoch 2/10
11/11 - 2s - loss: 221.0494 - loglik: -2.1724e+02 - logprior: -3.8051e+00
Epoch 3/10
11/11 - 2s - loss: 216.5883 - loglik: -2.1528e+02 - logprior: -1.3069e+00
Epoch 4/10
11/11 - 2s - loss: 215.3396 - loglik: -2.1476e+02 - logprior: -5.7970e-01
Epoch 5/10
11/11 - 2s - loss: 215.4936 - loglik: -2.1517e+02 - logprior: -3.2537e-01
Fitted a model with MAP estimate = -214.3696
Time for alignment: 65.3757
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.3907 - loglik: -3.5008e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 2s - loss: 310.8946 - loglik: -3.0795e+02 - logprior: -2.9465e+00
Epoch 3/10
11/11 - 2s - loss: 267.1710 - loglik: -2.6522e+02 - logprior: -1.9524e+00
Epoch 4/10
11/11 - 2s - loss: 250.4837 - loglik: -2.4840e+02 - logprior: -2.0871e+00
Epoch 5/10
11/11 - 2s - loss: 244.2493 - loglik: -2.4211e+02 - logprior: -2.1429e+00
Epoch 6/10
11/11 - 2s - loss: 242.2031 - loglik: -2.4003e+02 - logprior: -2.1731e+00
Epoch 7/10
11/11 - 2s - loss: 240.2826 - loglik: -2.3815e+02 - logprior: -2.1305e+00
Epoch 8/10
11/11 - 2s - loss: 236.5730 - loglik: -2.3450e+02 - logprior: -2.0742e+00
Epoch 9/10
11/11 - 2s - loss: 240.0833 - loglik: -2.3801e+02 - logprior: -2.0720e+00
Fitted a model with MAP estimate = -238.3271
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 254.6957 - loglik: -2.4049e+02 - logprior: -1.4210e+01
Epoch 2/2
11/11 - 2s - loss: 228.6031 - loglik: -2.2262e+02 - logprior: -5.9820e+00
Fitted a model with MAP estimate = -225.8994
expansions: [(0, 19)]
discards: [ 0  7 83 86]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 234.4683 - loglik: -2.2275e+02 - logprior: -1.1720e+01
Epoch 2/2
11/11 - 3s - loss: 220.0306 - loglik: -2.1679e+02 - logprior: -3.2450e+00
Fitted a model with MAP estimate = -215.5083
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 92]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 232.1653 - loglik: -2.1982e+02 - logprior: -1.2341e+01
Epoch 2/10
11/11 - 2s - loss: 220.6286 - loglik: -2.1774e+02 - logprior: -2.8936e+00
Epoch 3/10
11/11 - 2s - loss: 219.8737 - loglik: -2.1870e+02 - logprior: -1.1690e+00
Epoch 4/10
11/11 - 2s - loss: 216.1339 - loglik: -2.1549e+02 - logprior: -6.4212e-01
Epoch 5/10
11/11 - 2s - loss: 215.6535 - loglik: -2.1521e+02 - logprior: -4.4215e-01
Epoch 6/10
11/11 - 2s - loss: 215.4180 - loglik: -2.1510e+02 - logprior: -3.1958e-01
Epoch 7/10
11/11 - 2s - loss: 214.6659 - loglik: -2.1441e+02 - logprior: -2.5156e-01
Epoch 8/10
11/11 - 2s - loss: 214.1818 - loglik: -2.1395e+02 - logprior: -2.2869e-01
Epoch 9/10
11/11 - 2s - loss: 214.4085 - loglik: -2.1422e+02 - logprior: -1.9132e-01
Fitted a model with MAP estimate = -214.0652
Time for alignment: 70.4859
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.5475 - loglik: -3.5024e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 2s - loss: 308.5555 - loglik: -3.0561e+02 - logprior: -2.9434e+00
Epoch 3/10
11/11 - 2s - loss: 270.1429 - loglik: -2.6822e+02 - logprior: -1.9252e+00
Epoch 4/10
11/11 - 2s - loss: 248.3564 - loglik: -2.4631e+02 - logprior: -2.0426e+00
Epoch 5/10
11/11 - 2s - loss: 243.1903 - loglik: -2.4110e+02 - logprior: -2.0873e+00
Epoch 6/10
11/11 - 2s - loss: 241.8214 - loglik: -2.3977e+02 - logprior: -2.0537e+00
Epoch 7/10
11/11 - 2s - loss: 239.4505 - loglik: -2.3747e+02 - logprior: -1.9798e+00
Epoch 8/10
11/11 - 2s - loss: 238.0974 - loglik: -2.3614e+02 - logprior: -1.9549e+00
Epoch 9/10
11/11 - 2s - loss: 238.7656 - loglik: -2.3677e+02 - logprior: -1.9914e+00
Fitted a model with MAP estimate = -238.1430
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 254.0662 - loglik: -2.3989e+02 - logprior: -1.4174e+01
Epoch 2/2
11/11 - 2s - loss: 231.0783 - loglik: -2.2501e+02 - logprior: -6.0673e+00
Fitted a model with MAP estimate = -225.1285
expansions: [(0, 18)]
discards: [  0   8  14  77  78  85  88 111]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 233.7144 - loglik: -2.2199e+02 - logprior: -1.1720e+01
Epoch 2/2
11/11 - 3s - loss: 217.9882 - loglik: -2.1472e+02 - logprior: -3.2727e+00
Fitted a model with MAP estimate = -214.8819
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 232.6916 - loglik: -2.2019e+02 - logprior: -1.2500e+01
Epoch 2/10
11/11 - 2s - loss: 218.6849 - loglik: -2.1575e+02 - logprior: -2.9323e+00
Epoch 3/10
11/11 - 2s - loss: 217.3105 - loglik: -2.1613e+02 - logprior: -1.1777e+00
Epoch 4/10
11/11 - 2s - loss: 216.7666 - loglik: -2.1613e+02 - logprior: -6.3359e-01
Epoch 5/10
11/11 - 2s - loss: 214.0592 - loglik: -2.1364e+02 - logprior: -4.2380e-01
Epoch 6/10
11/11 - 2s - loss: 213.8757 - loglik: -2.1360e+02 - logprior: -2.7397e-01
Epoch 7/10
11/11 - 2s - loss: 214.8413 - loglik: -2.1465e+02 - logprior: -1.8803e-01
Fitted a model with MAP estimate = -213.4829
Time for alignment: 64.7546
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.5132 - loglik: -3.5021e+02 - logprior: -1.2306e+01
Epoch 2/10
11/11 - 2s - loss: 310.2404 - loglik: -3.0731e+02 - logprior: -2.9346e+00
Epoch 3/10
11/11 - 2s - loss: 268.8343 - loglik: -2.6688e+02 - logprior: -1.9555e+00
Epoch 4/10
11/11 - 2s - loss: 249.7150 - loglik: -2.4758e+02 - logprior: -2.1393e+00
Epoch 5/10
11/11 - 2s - loss: 244.0987 - loglik: -2.4183e+02 - logprior: -2.2673e+00
Epoch 6/10
11/11 - 2s - loss: 242.2200 - loglik: -2.3980e+02 - logprior: -2.4235e+00
Epoch 7/10
11/11 - 2s - loss: 240.1658 - loglik: -2.3776e+02 - logprior: -2.4080e+00
Epoch 8/10
11/11 - 2s - loss: 238.6991 - loglik: -2.3635e+02 - logprior: -2.3468e+00
Epoch 9/10
11/11 - 2s - loss: 237.3666 - loglik: -2.3497e+02 - logprior: -2.3963e+00
Epoch 10/10
11/11 - 2s - loss: 237.3251 - loglik: -2.3492e+02 - logprior: -2.4078e+00
Fitted a model with MAP estimate = -237.5780
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 2), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 252.7071 - loglik: -2.3837e+02 - logprior: -1.4341e+01
Epoch 2/2
11/11 - 2s - loss: 227.4149 - loglik: -2.2148e+02 - logprior: -5.9304e+00
Fitted a model with MAP estimate = -223.6668
expansions: [(0, 26)]
discards: [ 0 75 77 85]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 228.2293 - loglik: -2.2066e+02 - logprior: -7.5673e+00
Epoch 2/2
22/22 - 3s - loss: 217.0768 - loglik: -2.1533e+02 - logprior: -1.7433e+00
Fitted a model with MAP estimate = -216.2001
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 233.0598 - loglik: -2.1938e+02 - logprior: -1.3676e+01
Epoch 2/10
11/11 - 2s - loss: 221.6157 - loglik: -2.1770e+02 - logprior: -3.9171e+00
Epoch 3/10
11/11 - 2s - loss: 216.0863 - loglik: -2.1478e+02 - logprior: -1.3058e+00
Epoch 4/10
11/11 - 2s - loss: 215.5648 - loglik: -2.1502e+02 - logprior: -5.4882e-01
Epoch 5/10
11/11 - 2s - loss: 213.9569 - loglik: -2.1367e+02 - logprior: -2.8368e-01
Epoch 6/10
11/11 - 2s - loss: 214.7027 - loglik: -2.1447e+02 - logprior: -2.2966e-01
Fitted a model with MAP estimate = -213.8340
Time for alignment: 65.6679
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.5152 - loglik: -3.5020e+02 - logprior: -1.2312e+01
Epoch 2/10
11/11 - 2s - loss: 309.3388 - loglik: -3.0639e+02 - logprior: -2.9462e+00
Epoch 3/10
11/11 - 2s - loss: 269.1367 - loglik: -2.6719e+02 - logprior: -1.9454e+00
Epoch 4/10
11/11 - 2s - loss: 248.1079 - loglik: -2.4603e+02 - logprior: -2.0761e+00
Epoch 5/10
11/11 - 2s - loss: 243.8559 - loglik: -2.4171e+02 - logprior: -2.1438e+00
Epoch 6/10
11/11 - 2s - loss: 239.8782 - loglik: -2.3774e+02 - logprior: -2.1392e+00
Epoch 7/10
11/11 - 2s - loss: 239.4658 - loglik: -2.3737e+02 - logprior: -2.0930e+00
Epoch 8/10
11/11 - 2s - loss: 238.1683 - loglik: -2.3610e+02 - logprior: -2.0671e+00
Epoch 9/10
11/11 - 2s - loss: 236.4949 - loglik: -2.3439e+02 - logprior: -2.1084e+00
Epoch 10/10
11/11 - 2s - loss: 236.7545 - loglik: -2.3462e+02 - logprior: -2.1326e+00
Fitted a model with MAP estimate = -237.0393
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 5), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 251.9113 - loglik: -2.3776e+02 - logprior: -1.4154e+01
Epoch 2/2
11/11 - 2s - loss: 228.0557 - loglik: -2.2220e+02 - logprior: -5.8553e+00
Fitted a model with MAP estimate = -223.7896
expansions: [(0, 19)]
discards: [ 0  8 14 77 78]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 232.6113 - loglik: -2.2094e+02 - logprior: -1.1669e+01
Epoch 2/2
11/11 - 3s - loss: 216.7085 - loglik: -2.1349e+02 - logprior: -3.2151e+00
Fitted a model with MAP estimate = -214.4864
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 230.4124 - loglik: -2.1794e+02 - logprior: -1.2468e+01
Epoch 2/10
11/11 - 2s - loss: 219.7201 - loglik: -2.1684e+02 - logprior: -2.8796e+00
Epoch 3/10
11/11 - 2s - loss: 216.8040 - loglik: -2.1568e+02 - logprior: -1.1218e+00
Epoch 4/10
11/11 - 2s - loss: 215.7073 - loglik: -2.1511e+02 - logprior: -5.9419e-01
Epoch 5/10
11/11 - 2s - loss: 214.4151 - loglik: -2.1402e+02 - logprior: -3.9077e-01
Epoch 6/10
11/11 - 2s - loss: 213.9210 - loglik: -2.1367e+02 - logprior: -2.4929e-01
Epoch 7/10
11/11 - 2s - loss: 212.6828 - loglik: -2.1251e+02 - logprior: -1.7365e-01
Epoch 8/10
11/11 - 2s - loss: 212.3121 - loglik: -2.1216e+02 - logprior: -1.5598e-01
Epoch 9/10
11/11 - 2s - loss: 212.2813 - loglik: -2.1216e+02 - logprior: -1.1848e-01
Epoch 10/10
11/11 - 2s - loss: 214.0182 - loglik: -2.1395e+02 - logprior: -6.6765e-02
Fitted a model with MAP estimate = -212.5162
Time for alignment: 72.2463
Computed alignments with likelihoods: ['-211.8313', '-214.0652', '-213.4829', '-213.8340', '-212.5162']
Best model has likelihood: -211.8313  (prior= -1.3721 )
time for generating output: 0.2456
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.6383720930232558
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 757.1221 - loglik: -7.5543e+02 - logprior: -1.6919e+00
Epoch 2/10
39/39 - 16s - loss: 638.7866 - loglik: -6.3714e+02 - logprior: -1.6485e+00
Epoch 3/10
39/39 - 16s - loss: 627.9930 - loglik: -6.2627e+02 - logprior: -1.7241e+00
Epoch 4/10
39/39 - 16s - loss: 624.6234 - loglik: -6.2290e+02 - logprior: -1.7265e+00
Epoch 5/10
39/39 - 16s - loss: 623.7363 - loglik: -6.2200e+02 - logprior: -1.7400e+00
Epoch 6/10
39/39 - 16s - loss: 623.0339 - loglik: -6.2128e+02 - logprior: -1.7542e+00
Epoch 7/10
39/39 - 16s - loss: 623.0905 - loglik: -6.2133e+02 - logprior: -1.7585e+00
Fitted a model with MAP estimate = -579.5755
expansions: [(12, 2), (14, 2), (16, 1), (20, 1), (36, 1), (37, 1), (40, 1), (46, 2), (47, 2), (59, 2), (60, 2), (62, 5), (67, 1), (68, 1), (69, 1), (71, 1), (91, 1), (98, 2), (122, 1), (125, 5), (126, 2), (127, 2), (136, 1), (137, 2), (138, 1), (139, 1), (140, 3), (141, 2), (142, 2), (144, 2), (145, 1), (157, 1), (158, 2), (159, 2), (164, 1), (165, 1), (166, 1), (167, 3), (169, 4), (170, 2), (179, 1), (180, 2), (181, 1), (193, 1), (194, 1), (202, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 616.1489 - loglik: -6.1338e+02 - logprior: -2.7682e+00
Epoch 2/2
39/39 - 25s - loss: 597.0755 - loglik: -5.9594e+02 - logprior: -1.1318e+00
Fitted a model with MAP estimate = -553.2337
expansions: [(0, 2), (233, 1), (238, 1)]
discards: [  0  12  74  78  79 124 158 159 181 188 190 195 215 216 217 218 229 230
 251 290 292]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 600.9734 - loglik: -5.9939e+02 - logprior: -1.5793e+00
Epoch 2/2
39/39 - 23s - loss: 595.9558 - loglik: -5.9541e+02 - logprior: -5.4346e-01
Fitted a model with MAP estimate = -553.9759
expansions: []
discards: [  0 200]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 554.6537 - loglik: -5.5296e+02 - logprior: -1.6931e+00
Epoch 2/10
45/45 - 26s - loss: 550.6537 - loglik: -5.5014e+02 - logprior: -5.1626e-01
Epoch 3/10
45/45 - 26s - loss: 548.4054 - loglik: -5.4796e+02 - logprior: -4.4871e-01
Epoch 4/10
45/45 - 26s - loss: 547.4241 - loglik: -5.4697e+02 - logprior: -4.5089e-01
Epoch 5/10
45/45 - 26s - loss: 544.9197 - loglik: -5.4457e+02 - logprior: -3.5190e-01
Epoch 6/10
45/45 - 26s - loss: 544.8151 - loglik: -5.4454e+02 - logprior: -2.7512e-01
Epoch 7/10
45/45 - 26s - loss: 545.4446 - loglik: -5.4518e+02 - logprior: -2.6038e-01
Fitted a model with MAP estimate = -544.7169
Time for alignment: 534.1931
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 756.7762 - loglik: -7.5509e+02 - logprior: -1.6840e+00
Epoch 2/10
39/39 - 16s - loss: 634.7338 - loglik: -6.3313e+02 - logprior: -1.6083e+00
Epoch 3/10
39/39 - 16s - loss: 624.4048 - loglik: -6.2268e+02 - logprior: -1.7251e+00
Epoch 4/10
39/39 - 16s - loss: 622.1919 - loglik: -6.2050e+02 - logprior: -1.6923e+00
Epoch 5/10
39/39 - 16s - loss: 621.1656 - loglik: -6.1948e+02 - logprior: -1.6886e+00
Epoch 6/10
39/39 - 16s - loss: 620.6689 - loglik: -6.1898e+02 - logprior: -1.6893e+00
Epoch 7/10
39/39 - 16s - loss: 620.9139 - loglik: -6.1923e+02 - logprior: -1.6887e+00
Fitted a model with MAP estimate = -576.9729
expansions: [(12, 2), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 2), (59, 2), (60, 1), (62, 5), (63, 1), (66, 1), (68, 1), (69, 1), (70, 1), (98, 2), (119, 1), (125, 4), (126, 2), (128, 1), (136, 2), (137, 3), (138, 2), (140, 1), (141, 1), (142, 1), (143, 1), (158, 1), (160, 4), (161, 1), (164, 1), (165, 1), (166, 1), (167, 5), (168, 3), (169, 3), (180, 2), (191, 1), (193, 1), (194, 1), (206, 1), (209, 2), (210, 1), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 613.8051 - loglik: -6.1108e+02 - logprior: -2.7263e+00
Epoch 2/2
39/39 - 25s - loss: 597.1710 - loglik: -5.9613e+02 - logprior: -1.0456e+00
Fitted a model with MAP estimate = -553.6370
expansions: [(0, 2)]
discards: [  0  77  78 122 156 157 174 177 205 206 227 245 281 286]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 601.0671 - loglik: -5.9952e+02 - logprior: -1.5482e+00
Epoch 2/2
39/39 - 23s - loss: 596.6398 - loglik: -5.9614e+02 - logprior: -4.9845e-01
Fitted a model with MAP estimate = -553.9529
expansions: [(200, 1), (201, 1), (202, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 555.3181 - loglik: -5.5367e+02 - logprior: -1.6497e+00
Epoch 2/10
45/45 - 27s - loss: 548.6465 - loglik: -5.4818e+02 - logprior: -4.7067e-01
Epoch 3/10
45/45 - 27s - loss: 548.4169 - loglik: -5.4801e+02 - logprior: -4.0270e-01
Epoch 4/10
45/45 - 26s - loss: 545.6890 - loglik: -5.4536e+02 - logprior: -3.2597e-01
Epoch 5/10
45/45 - 27s - loss: 544.1956 - loglik: -5.4383e+02 - logprior: -3.6825e-01
Epoch 6/10
45/45 - 26s - loss: 545.2672 - loglik: -5.4506e+02 - logprior: -2.0716e-01
Fitted a model with MAP estimate = -544.1542
Time for alignment: 509.2577
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 758.9507 - loglik: -7.5729e+02 - logprior: -1.6581e+00
Epoch 2/10
39/39 - 16s - loss: 640.9476 - loglik: -6.3938e+02 - logprior: -1.5716e+00
Epoch 3/10
39/39 - 16s - loss: 630.6016 - loglik: -6.2897e+02 - logprior: -1.6322e+00
Epoch 4/10
39/39 - 16s - loss: 628.4915 - loglik: -6.2689e+02 - logprior: -1.6016e+00
Epoch 5/10
39/39 - 16s - loss: 627.3691 - loglik: -6.2576e+02 - logprior: -1.6104e+00
Epoch 6/10
39/39 - 16s - loss: 627.3668 - loglik: -6.2575e+02 - logprior: -1.6131e+00
Epoch 7/10
39/39 - 16s - loss: 626.7391 - loglik: -6.2513e+02 - logprior: -1.6130e+00
Epoch 8/10
39/39 - 16s - loss: 626.7357 - loglik: -6.2512e+02 - logprior: -1.6178e+00
Epoch 9/10
39/39 - 16s - loss: 626.8363 - loglik: -6.2522e+02 - logprior: -1.6138e+00
Fitted a model with MAP estimate = -583.0133
expansions: [(12, 3), (14, 1), (16, 1), (17, 1), (36, 1), (37, 1), (40, 1), (46, 2), (47, 2), (59, 2), (60, 1), (63, 3), (64, 1), (67, 2), (68, 2), (69, 2), (91, 1), (120, 1), (125, 10), (126, 2), (127, 1), (129, 1), (133, 1), (135, 1), (136, 4), (137, 1), (141, 2), (142, 1), (162, 1), (163, 1), (167, 2), (168, 4), (169, 3), (180, 2), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 616.9647 - loglik: -6.1417e+02 - logprior: -2.7960e+00
Epoch 2/2
39/39 - 24s - loss: 598.9668 - loglik: -5.9773e+02 - logprior: -1.2322e+00
Fitted a model with MAP estimate = -554.9698
expansions: [(0, 2), (157, 3)]
discards: [  0  77  86  92 179 180 181 190 221 223 224 242 281 283]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 600.3181 - loglik: -5.9873e+02 - logprior: -1.5916e+00
Epoch 2/2
39/39 - 23s - loss: 595.9464 - loglik: -5.9542e+02 - logprior: -5.2472e-01
Fitted a model with MAP estimate = -553.4222
expansions: []
discards: [  0  13 154 155 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 557.3113 - loglik: -5.5570e+02 - logprior: -1.6081e+00
Epoch 2/10
45/45 - 25s - loss: 551.6960 - loglik: -5.5126e+02 - logprior: -4.3414e-01
Epoch 3/10
45/45 - 25s - loss: 550.2123 - loglik: -5.4975e+02 - logprior: -4.6308e-01
Epoch 4/10
45/45 - 25s - loss: 547.9044 - loglik: -5.4759e+02 - logprior: -3.1511e-01
Epoch 5/10
45/45 - 25s - loss: 546.7766 - loglik: -5.4641e+02 - logprior: -3.6730e-01
Epoch 6/10
45/45 - 25s - loss: 546.2238 - loglik: -5.4602e+02 - logprior: -1.9979e-01
Epoch 7/10
45/45 - 25s - loss: 547.1953 - loglik: -5.4706e+02 - logprior: -1.3926e-01
Fitted a model with MAP estimate = -546.0326
Time for alignment: 556.5950
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 757.0568 - loglik: -7.5538e+02 - logprior: -1.6726e+00
Epoch 2/10
39/39 - 16s - loss: 639.1872 - loglik: -6.3759e+02 - logprior: -1.6008e+00
Epoch 3/10
39/39 - 16s - loss: 627.6737 - loglik: -6.2591e+02 - logprior: -1.7670e+00
Epoch 4/10
39/39 - 16s - loss: 625.1265 - loglik: -6.2338e+02 - logprior: -1.7503e+00
Epoch 5/10
39/39 - 16s - loss: 624.1939 - loglik: -6.2245e+02 - logprior: -1.7479e+00
Epoch 6/10
39/39 - 16s - loss: 623.6581 - loglik: -6.2189e+02 - logprior: -1.7653e+00
Epoch 7/10
39/39 - 16s - loss: 623.6201 - loglik: -6.2185e+02 - logprior: -1.7702e+00
Epoch 8/10
39/39 - 16s - loss: 623.7794 - loglik: -6.2200e+02 - logprior: -1.7775e+00
Fitted a model with MAP estimate = -581.9287
expansions: [(12, 3), (14, 2), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 2), (59, 1), (60, 2), (62, 5), (63, 1), (66, 1), (67, 1), (68, 1), (70, 1), (93, 1), (97, 2), (125, 6), (127, 1), (128, 1), (130, 1), (136, 2), (137, 2), (139, 1), (140, 1), (141, 1), (142, 1), (146, 2), (152, 1), (157, 1), (158, 2), (159, 2), (165, 2), (166, 1), (167, 1), (169, 4), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (209, 2), (210, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 613.8534 - loglik: -6.1117e+02 - logprior: -2.6823e+00
Epoch 2/2
39/39 - 25s - loss: 597.0791 - loglik: -5.9606e+02 - logprior: -1.0224e+00
Fitted a model with MAP estimate = -554.7696
expansions: [(0, 2), (230, 1)]
discards: [  0  12  13  79  80 124 157 178 192 211 246 283]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 599.4518 - loglik: -5.9789e+02 - logprior: -1.5615e+00
Epoch 2/2
39/39 - 23s - loss: 595.3967 - loglik: -5.9489e+02 - logprior: -5.0621e-01
Fitted a model with MAP estimate = -552.9303
expansions: []
discards: [  0 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 556.2360 - loglik: -5.5458e+02 - logprior: -1.6524e+00
Epoch 2/10
45/45 - 25s - loss: 550.1440 - loglik: -5.4969e+02 - logprior: -4.5726e-01
Epoch 3/10
45/45 - 26s - loss: 550.1460 - loglik: -5.4977e+02 - logprior: -3.7677e-01
Fitted a model with MAP estimate = -547.5779
Time for alignment: 444.3851
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 755.6423 - loglik: -7.5401e+02 - logprior: -1.6372e+00
Epoch 2/10
39/39 - 16s - loss: 634.6418 - loglik: -6.3317e+02 - logprior: -1.4712e+00
Epoch 3/10
39/39 - 16s - loss: 625.1677 - loglik: -6.2358e+02 - logprior: -1.5849e+00
Epoch 4/10
39/39 - 16s - loss: 623.0876 - loglik: -6.2153e+02 - logprior: -1.5551e+00
Epoch 5/10
39/39 - 16s - loss: 621.9062 - loglik: -6.2033e+02 - logprior: -1.5723e+00
Epoch 6/10
39/39 - 16s - loss: 621.2036 - loglik: -6.1960e+02 - logprior: -1.5993e+00
Epoch 7/10
39/39 - 16s - loss: 621.0240 - loglik: -6.1940e+02 - logprior: -1.6192e+00
Epoch 8/10
39/39 - 16s - loss: 621.1989 - loglik: -6.1957e+02 - logprior: -1.6250e+00
Fitted a model with MAP estimate = -577.2029
expansions: [(12, 3), (14, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (39, 1), (45, 2), (46, 1), (59, 4), (62, 3), (65, 1), (66, 1), (67, 1), (68, 1), (70, 1), (120, 1), (125, 2), (126, 2), (127, 2), (138, 2), (139, 2), (140, 1), (148, 1), (161, 1), (162, 1), (163, 5), (164, 1), (165, 1), (166, 3), (167, 1), (168, 3), (169, 3), (170, 2), (174, 1), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (202, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 612.8316 - loglik: -6.1019e+02 - logprior: -2.6397e+00
Epoch 2/2
39/39 - 23s - loss: 597.0468 - loglik: -5.9602e+02 - logprior: -1.0225e+00
Fitted a model with MAP estimate = -553.7590
expansions: [(0, 2), (151, 2)]
discards: [  0  12  77 154 155 156 202 203 204 219 223 240 276]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 602.4968 - loglik: -6.0090e+02 - logprior: -1.5962e+00
Epoch 2/2
39/39 - 23s - loss: 597.7984 - loglik: -5.9726e+02 - logprior: -5.4219e-01
Fitted a model with MAP estimate = -554.8518
expansions: [(201, 2)]
discards: [  0  57 151]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 556.6789 - loglik: -5.5509e+02 - logprior: -1.5864e+00
Epoch 2/10
45/45 - 26s - loss: 550.3779 - loglik: -5.4989e+02 - logprior: -4.9089e-01
Epoch 3/10
45/45 - 25s - loss: 549.7307 - loglik: -5.4927e+02 - logprior: -4.6422e-01
Epoch 4/10
45/45 - 25s - loss: 546.1949 - loglik: -5.4573e+02 - logprior: -4.6290e-01
Epoch 5/10
45/45 - 26s - loss: 547.5499 - loglik: -5.4727e+02 - logprior: -2.7595e-01
Fitted a model with MAP estimate = -545.8743
Time for alignment: 489.6830
Computed alignments with likelihoods: ['-544.7169', '-544.1542', '-546.0326', '-547.5779', '-545.8743']
Best model has likelihood: -544.1542  (prior= -0.2525 )
time for generating output: 0.2967
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.9023944130362488
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 445.9706 - loglik: -4.4494e+02 - logprior: -1.0284e+00
Epoch 2/10
30/30 - 7s - loss: 374.7812 - loglik: -3.7367e+02 - logprior: -1.1142e+00
Epoch 3/10
30/30 - 7s - loss: 362.9380 - loglik: -3.6183e+02 - logprior: -1.1061e+00
Epoch 4/10
30/30 - 6s - loss: 360.9832 - loglik: -3.5988e+02 - logprior: -1.1004e+00
Epoch 5/10
30/30 - 7s - loss: 360.3294 - loglik: -3.5925e+02 - logprior: -1.0777e+00
Epoch 6/10
30/30 - 6s - loss: 359.6359 - loglik: -3.5857e+02 - logprior: -1.0651e+00
Epoch 7/10
30/30 - 6s - loss: 359.1588 - loglik: -3.5810e+02 - logprior: -1.0563e+00
Epoch 8/10
30/30 - 6s - loss: 358.8015 - loglik: -3.5775e+02 - logprior: -1.0541e+00
Epoch 9/10
30/30 - 6s - loss: 358.8044 - loglik: -3.5775e+02 - logprior: -1.0524e+00
Fitted a model with MAP estimate = -349.0297
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 2), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (102, 2), (111, 1), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 352.7753 - loglik: -3.5166e+02 - logprior: -1.1195e+00
Epoch 2/2
61/61 - 11s - loss: 345.9901 - loglik: -3.4517e+02 - logprior: -8.1637e-01
Fitted a model with MAP estimate = -334.5198
expansions: []
discards: [ 25  48  51  92  94 100 127 134 149 152 156]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 347.2879 - loglik: -3.4630e+02 - logprior: -9.9290e-01
Epoch 2/2
61/61 - 10s - loss: 345.8866 - loglik: -3.4518e+02 - logprior: -7.0284e-01
Fitted a model with MAP estimate = -334.9522
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 19s - loss: 333.5805 - loglik: -3.3294e+02 - logprior: -6.4238e-01
Epoch 2/10
87/87 - 14s - loss: 333.0179 - loglik: -3.3249e+02 - logprior: -5.3202e-01
Epoch 3/10
87/87 - 14s - loss: 331.8243 - loglik: -3.3131e+02 - logprior: -5.1504e-01
Epoch 4/10
87/87 - 14s - loss: 330.6582 - loglik: -3.3017e+02 - logprior: -4.9188e-01
Epoch 5/10
87/87 - 14s - loss: 330.3876 - loglik: -3.2991e+02 - logprior: -4.7939e-01
Epoch 6/10
87/87 - 14s - loss: 329.8734 - loglik: -3.2941e+02 - logprior: -4.6415e-01
Epoch 7/10
87/87 - 14s - loss: 330.4619 - loglik: -3.3002e+02 - logprior: -4.4320e-01
Fitted a model with MAP estimate = -330.4027
Time for alignment: 325.3190
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 446.1806 - loglik: -4.4515e+02 - logprior: -1.0293e+00
Epoch 2/10
30/30 - 6s - loss: 375.3905 - loglik: -3.7428e+02 - logprior: -1.1136e+00
Epoch 3/10
30/30 - 6s - loss: 363.6270 - loglik: -3.6253e+02 - logprior: -1.0967e+00
Epoch 4/10
30/30 - 6s - loss: 361.2895 - loglik: -3.6019e+02 - logprior: -1.0962e+00
Epoch 5/10
30/30 - 6s - loss: 360.2583 - loglik: -3.5917e+02 - logprior: -1.0848e+00
Epoch 6/10
30/30 - 7s - loss: 358.7290 - loglik: -3.5766e+02 - logprior: -1.0727e+00
Epoch 7/10
30/30 - 6s - loss: 358.9295 - loglik: -3.5786e+02 - logprior: -1.0681e+00
Fitted a model with MAP estimate = -348.5494
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 352.4751 - loglik: -3.5137e+02 - logprior: -1.1047e+00
Epoch 2/2
61/61 - 10s - loss: 345.8768 - loglik: -3.4506e+02 - logprior: -8.1398e-01
Fitted a model with MAP estimate = -334.6993
expansions: []
discards: [ 29  51  92  94 126 133 147 151 154]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 347.1332 - loglik: -3.4614e+02 - logprior: -9.9317e-01
Epoch 2/2
61/61 - 10s - loss: 345.8645 - loglik: -3.4515e+02 - logprior: -7.1500e-01
Fitted a model with MAP estimate = -334.7713
expansions: []
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 333.6651 - loglik: -3.3302e+02 - logprior: -6.4456e-01
Epoch 2/10
87/87 - 14s - loss: 333.2014 - loglik: -3.3266e+02 - logprior: -5.3759e-01
Epoch 3/10
87/87 - 14s - loss: 331.4763 - loglik: -3.3096e+02 - logprior: -5.1936e-01
Epoch 4/10
87/87 - 14s - loss: 331.1730 - loglik: -3.3068e+02 - logprior: -4.9776e-01
Epoch 5/10
87/87 - 14s - loss: 330.3636 - loglik: -3.2988e+02 - logprior: -4.8462e-01
Epoch 6/10
87/87 - 14s - loss: 329.9044 - loglik: -3.2943e+02 - logprior: -4.6970e-01
Epoch 7/10
87/87 - 14s - loss: 330.2932 - loglik: -3.2984e+02 - logprior: -4.5253e-01
Fitted a model with MAP estimate = -330.3780
Time for alignment: 311.9360
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 446.5049 - loglik: -4.4548e+02 - logprior: -1.0289e+00
Epoch 2/10
30/30 - 6s - loss: 375.5890 - loglik: -3.7447e+02 - logprior: -1.1215e+00
Epoch 3/10
30/30 - 6s - loss: 362.8021 - loglik: -3.6168e+02 - logprior: -1.1228e+00
Epoch 4/10
30/30 - 6s - loss: 359.9413 - loglik: -3.5881e+02 - logprior: -1.1278e+00
Epoch 5/10
30/30 - 7s - loss: 359.6528 - loglik: -3.5855e+02 - logprior: -1.1046e+00
Epoch 6/10
30/30 - 6s - loss: 358.8585 - loglik: -3.5776e+02 - logprior: -1.0944e+00
Epoch 7/10
30/30 - 7s - loss: 358.8223 - loglik: -3.5774e+02 - logprior: -1.0858e+00
Epoch 8/10
30/30 - 6s - loss: 358.0504 - loglik: -3.5697e+02 - logprior: -1.0854e+00
Epoch 9/10
30/30 - 6s - loss: 358.6575 - loglik: -3.5758e+02 - logprior: -1.0804e+00
Fitted a model with MAP estimate = -348.4487
expansions: [(14, 1), (15, 1), (16, 1), (23, 2), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (99, 1), (111, 1), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 352.7474 - loglik: -3.5163e+02 - logprior: -1.1152e+00
Epoch 2/2
61/61 - 11s - loss: 346.4944 - loglik: -3.4568e+02 - logprior: -8.1126e-01
Fitted a model with MAP estimate = -334.3429
expansions: []
discards: [ 25  30  49  52  93  95 127 148 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 347.1686 - loglik: -3.4618e+02 - logprior: -9.9076e-01
Epoch 2/2
61/61 - 10s - loss: 345.7617 - loglik: -3.4506e+02 - logprior: -6.9967e-01
Fitted a model with MAP estimate = -334.9654
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 333.6567 - loglik: -3.3302e+02 - logprior: -6.3995e-01
Epoch 2/10
87/87 - 14s - loss: 332.9062 - loglik: -3.3238e+02 - logprior: -5.3005e-01
Epoch 3/10
87/87 - 13s - loss: 331.6985 - loglik: -3.3119e+02 - logprior: -5.0911e-01
Epoch 4/10
87/87 - 14s - loss: 331.2519 - loglik: -3.3076e+02 - logprior: -4.8932e-01
Epoch 5/10
87/87 - 14s - loss: 330.0070 - loglik: -3.2953e+02 - logprior: -4.7563e-01
Epoch 6/10
87/87 - 14s - loss: 330.2844 - loglik: -3.2982e+02 - logprior: -4.6157e-01
Fitted a model with MAP estimate = -330.0368
Time for alignment: 310.1596
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 446.3554 - loglik: -4.4532e+02 - logprior: -1.0354e+00
Epoch 2/10
30/30 - 6s - loss: 374.3047 - loglik: -3.7318e+02 - logprior: -1.1274e+00
Epoch 3/10
30/30 - 6s - loss: 363.9758 - loglik: -3.6288e+02 - logprior: -1.0976e+00
Epoch 4/10
30/30 - 7s - loss: 362.1725 - loglik: -3.6108e+02 - logprior: -1.0918e+00
Epoch 5/10
30/30 - 7s - loss: 361.2697 - loglik: -3.6020e+02 - logprior: -1.0728e+00
Epoch 6/10
30/30 - 6s - loss: 360.2483 - loglik: -3.5919e+02 - logprior: -1.0581e+00
Epoch 7/10
30/30 - 7s - loss: 360.4571 - loglik: -3.5940e+02 - logprior: -1.0530e+00
Fitted a model with MAP estimate = -349.6676
expansions: [(14, 1), (15, 1), (17, 1), (20, 1), (22, 2), (29, 1), (35, 1), (36, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (74, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 352.5587 - loglik: -3.5143e+02 - logprior: -1.1246e+00
Epoch 2/2
61/61 - 11s - loss: 345.9791 - loglik: -3.4515e+02 - logprior: -8.2844e-01
Fitted a model with MAP estimate = -334.6715
expansions: []
discards: [ 25  48  51  92  95 102 127 134 148 152 156]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 347.0455 - loglik: -3.4605e+02 - logprior: -9.9774e-01
Epoch 2/2
61/61 - 10s - loss: 345.8365 - loglik: -3.4512e+02 - logprior: -7.1307e-01
Fitted a model with MAP estimate = -334.8921
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 333.4733 - loglik: -3.3282e+02 - logprior: -6.5031e-01
Epoch 2/10
87/87 - 14s - loss: 333.2007 - loglik: -3.3266e+02 - logprior: -5.3722e-01
Epoch 3/10
87/87 - 14s - loss: 331.5871 - loglik: -3.3107e+02 - logprior: -5.1609e-01
Epoch 4/10
87/87 - 14s - loss: 331.2883 - loglik: -3.3079e+02 - logprior: -4.9976e-01
Epoch 5/10
87/87 - 14s - loss: 330.1257 - loglik: -3.2964e+02 - logprior: -4.8424e-01
Epoch 6/10
87/87 - 14s - loss: 330.2319 - loglik: -3.2976e+02 - logprior: -4.7198e-01
Fitted a model with MAP estimate = -330.0431
Time for alignment: 299.3019
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 445.7023 - loglik: -4.4468e+02 - logprior: -1.0265e+00
Epoch 2/10
30/30 - 6s - loss: 374.9168 - loglik: -3.7381e+02 - logprior: -1.1049e+00
Epoch 3/10
30/30 - 6s - loss: 362.6230 - loglik: -3.6152e+02 - logprior: -1.1037e+00
Epoch 4/10
30/30 - 7s - loss: 361.1932 - loglik: -3.6009e+02 - logprior: -1.1059e+00
Epoch 5/10
30/30 - 6s - loss: 359.9949 - loglik: -3.5891e+02 - logprior: -1.0839e+00
Epoch 6/10
30/30 - 7s - loss: 359.3246 - loglik: -3.5825e+02 - logprior: -1.0716e+00
Epoch 7/10
30/30 - 6s - loss: 358.9627 - loglik: -3.5790e+02 - logprior: -1.0620e+00
Epoch 8/10
30/30 - 6s - loss: 358.6254 - loglik: -3.5757e+02 - logprior: -1.0586e+00
Epoch 9/10
30/30 - 6s - loss: 358.9230 - loglik: -3.5786e+02 - logprior: -1.0585e+00
Fitted a model with MAP estimate = -348.8333
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 352.9037 - loglik: -3.5179e+02 - logprior: -1.1155e+00
Epoch 2/2
61/61 - 11s - loss: 345.4792 - loglik: -3.4467e+02 - logprior: -8.0768e-01
Fitted a model with MAP estimate = -334.7608
expansions: []
discards: [ 30  48  51  92  94 102 133 147 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 347.5887 - loglik: -3.4660e+02 - logprior: -9.9112e-01
Epoch 2/2
61/61 - 10s - loss: 345.6007 - loglik: -3.4489e+02 - logprior: -7.1027e-01
Fitted a model with MAP estimate = -334.9498
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 333.7420 - loglik: -3.3310e+02 - logprior: -6.4429e-01
Epoch 2/10
87/87 - 14s - loss: 332.7245 - loglik: -3.3219e+02 - logprior: -5.3006e-01
Epoch 3/10
87/87 - 14s - loss: 331.8612 - loglik: -3.3135e+02 - logprior: -5.1445e-01
Epoch 4/10
87/87 - 14s - loss: 331.1483 - loglik: -3.3066e+02 - logprior: -4.9299e-01
Epoch 5/10
87/87 - 14s - loss: 330.7010 - loglik: -3.3022e+02 - logprior: -4.7842e-01
Epoch 6/10
87/87 - 14s - loss: 329.1746 - loglik: -3.2871e+02 - logprior: -4.6176e-01
Epoch 7/10
87/87 - 14s - loss: 330.1882 - loglik: -3.2974e+02 - logprior: -4.4409e-01
Fitted a model with MAP estimate = -330.3759
Time for alignment: 324.0189
Computed alignments with likelihoods: ['-330.4027', '-330.3780', '-330.0368', '-330.0431', '-330.3759']
Best model has likelihood: -330.0368  (prior= -0.4669 )
time for generating output: 0.2806
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6757741347905283
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1243.0995 - loglik: -1.2420e+03 - logprior: -1.0692e+00
Epoch 2/10
40/40 - 39s - loss: 1143.0292 - loglik: -1.1432e+03 - logprior: 0.1277
Epoch 3/10
40/40 - 39s - loss: 1133.3170 - loglik: -1.1335e+03 - logprior: 0.1934
Epoch 4/10
40/40 - 39s - loss: 1127.6564 - loglik: -1.1279e+03 - logprior: 0.2237
Epoch 5/10
40/40 - 39s - loss: 1121.8143 - loglik: -1.1220e+03 - logprior: 0.1950
Epoch 6/10
40/40 - 39s - loss: 1119.1410 - loglik: -1.1193e+03 - logprior: 0.1543
Epoch 7/10
40/40 - 39s - loss: 1118.0809 - loglik: -1.1182e+03 - logprior: 0.1058
Epoch 8/10
40/40 - 39s - loss: 1118.0311 - loglik: -1.1181e+03 - logprior: 0.0968
Epoch 9/10
40/40 - 39s - loss: 1116.1218 - loglik: -1.1162e+03 - logprior: 0.0658
Epoch 10/10
40/40 - 39s - loss: 1115.9615 - loglik: -1.1160e+03 - logprior: 0.0526
Fitted a model with MAP estimate = -849.3644
expansions: [(25, 1), (97, 2), (125, 1), (127, 1), (172, 1), (209, 1), (296, 1), (298, 1), (324, 66), (327, 1), (330, 50)]
discards: [  1  85 189 236]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 1072.4109 - loglik: -1.0718e+03 - logprior: -6.4453e-01
Epoch 2/2
80/80 - 66s - loss: 1036.3839 - loglik: -1.0368e+03 - logprior: 0.3796
Fitted a model with MAP estimate = -773.4384
expansions: [(439, 2), (452, 2)]
discards: [ 97 338 361 362 448]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 70s - loss: 1039.7212 - loglik: -1.0394e+03 - logprior: -3.2994e-01
Epoch 2/2
80/80 - 66s - loss: 1035.0187 - loglik: -1.0354e+03 - logprior: 0.4175
Fitted a model with MAP estimate = -773.4447
expansions: []
discards: [431 432 449 450]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 769.5231 - loglik: -7.6928e+02 - logprior: -2.4343e-01
Epoch 2/10
56/56 - 88s - loss: 766.5637 - loglik: -7.6675e+02 - logprior: 0.1890
Epoch 3/10
56/56 - 89s - loss: 761.3563 - loglik: -7.6149e+02 - logprior: 0.1307
Epoch 4/10
56/56 - 89s - loss: 760.9781 - loglik: -7.6128e+02 - logprior: 0.3008
Epoch 5/10
56/56 - 89s - loss: 757.3846 - loglik: -7.5776e+02 - logprior: 0.3798
Epoch 6/10
56/56 - 89s - loss: 751.3873 - loglik: -7.5160e+02 - logprior: 0.2088
Epoch 7/10
56/56 - 89s - loss: 751.4468 - loglik: -7.5190e+02 - logprior: 0.4546
Fitted a model with MAP estimate = -752.8303
Time for alignment: 1713.1051
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 43s - loss: 1246.0291 - loglik: -1.2450e+03 - logprior: -1.0733e+00
Epoch 2/10
40/40 - 39s - loss: 1148.0590 - loglik: -1.1483e+03 - logprior: 0.2056
Epoch 3/10
40/40 - 39s - loss: 1140.1133 - loglik: -1.1404e+03 - logprior: 0.2450
Epoch 4/10
40/40 - 39s - loss: 1133.7753 - loglik: -1.1340e+03 - logprior: 0.2543
Epoch 5/10
40/40 - 39s - loss: 1127.2439 - loglik: -1.1275e+03 - logprior: 0.2087
Epoch 6/10
40/40 - 39s - loss: 1124.2167 - loglik: -1.1244e+03 - logprior: 0.1623
Epoch 7/10
40/40 - 39s - loss: 1122.8762 - loglik: -1.1230e+03 - logprior: 0.1346
Epoch 8/10
40/40 - 39s - loss: 1121.9226 - loglik: -1.1220e+03 - logprior: 0.1062
Epoch 9/10
40/40 - 39s - loss: 1120.5736 - loglik: -1.1207e+03 - logprior: 0.0773
Epoch 10/10
40/40 - 39s - loss: 1120.0826 - loglik: -1.1201e+03 - logprior: 0.0449
Fitted a model with MAP estimate = -854.0099
expansions: [(62, 1), (125, 1), (127, 1), (305, 1), (306, 1), (308, 2), (328, 2), (329, 1), (330, 119)]
discards: [  1 224 228]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1088.6399 - loglik: -1.0873e+03 - logprior: -1.3061e+00
Epoch 2/2
80/80 - 67s - loss: 1041.0535 - loglik: -1.0402e+03 - logprior: -8.4567e-01
Fitted a model with MAP estimate = -776.2967
expansions: [(343, 1), (347, 2), (352, 1), (353, 2), (355, 1), (362, 1), (378, 1), (379, 4), (380, 2), (382, 1), (396, 1), (399, 1), (414, 1), (418, 2)]
discards: [225 310 333 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449
 450 451 452 453 454 455]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1041.3251 - loglik: -1.0411e+03 - logprior: -2.6750e-01
Epoch 2/2
80/80 - 67s - loss: 1035.6829 - loglik: -1.0363e+03 - logprior: 0.6047
Fitted a model with MAP estimate = -774.1773
expansions: [(442, 1), (453, 2)]
discards: [240 345 354 387]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 769.1208 - loglik: -7.6904e+02 - logprior: -8.5637e-02
Epoch 2/10
113/113 - 93s - loss: 760.7567 - loglik: -7.6086e+02 - logprior: 0.1023
Epoch 3/10
113/113 - 93s - loss: 764.8481 - loglik: -7.6508e+02 - logprior: 0.2355
Fitted a model with MAP estimate = -757.2198
Time for alignment: 1375.8056
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1230.7563 - loglik: -1.2290e+03 - logprior: -1.7414e+00
Epoch 2/10
40/40 - 39s - loss: 1111.4630 - loglik: -1.1086e+03 - logprior: -2.9048e+00
Epoch 3/10
40/40 - 39s - loss: 1102.4791 - loglik: -1.0996e+03 - logprior: -2.8834e+00
Epoch 4/10
40/40 - 39s - loss: 1096.0265 - loglik: -1.0933e+03 - logprior: -2.7574e+00
Epoch 5/10
40/40 - 39s - loss: 1091.3629 - loglik: -1.0887e+03 - logprior: -2.6444e+00
Epoch 6/10
40/40 - 39s - loss: 1089.4747 - loglik: -1.0869e+03 - logprior: -2.6205e+00
Epoch 7/10
40/40 - 39s - loss: 1087.8668 - loglik: -1.0852e+03 - logprior: -2.6274e+00
Epoch 8/10
40/40 - 39s - loss: 1087.8103 - loglik: -1.0852e+03 - logprior: -2.6333e+00
Epoch 9/10
40/40 - 39s - loss: 1085.2029 - loglik: -1.0826e+03 - logprior: -2.6396e+00
Epoch 10/10
40/40 - 39s - loss: 1084.7446 - loglik: -1.0821e+03 - logprior: -2.6466e+00
Fitted a model with MAP estimate = -831.4598
expansions: [(124, 1), (169, 1), (211, 1), (330, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 108 109
 110 111 112 113 114 115 116 117 118 119 121 122 238 239 240 241 242 243
 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261
 262 263 264 265 266 267 268 269 270 271 272 273 274 275 277 278 279 280
 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298
 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316
 317 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 15s - loss: 1277.2588 - loglik: -1.2725e+03 - logprior: -4.7372e+00
Epoch 2/2
40/40 - 12s - loss: 1247.3889 - loglik: -1.2454e+03 - logprior: -2.0178e+00
Fitted a model with MAP estimate = -907.2746
expansions: [(0, 298), (3, 1), (5, 8), (50, 3), (116, 1)]
discards: [  0  51 100 101 117 118 119 120 121 122 123 124]
Re-initialized the encoder parameters.
Fitting a model of length 424 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 61s - loss: 1132.0833 - loglik: -1.1303e+03 - logprior: -1.8103e+00
Epoch 2/2
40/40 - 59s - loss: 1048.8137 - loglik: -1.0485e+03 - logprior: -2.8310e-01
Fitted a model with MAP estimate = -777.2715
expansions: [(22, 1), (26, 1), (39, 2), (65, 1), (66, 1), (105, 1), (119, 2), (151, 1), (153, 3), (159, 1), (191, 1), (230, 1), (239, 1), (262, 1), (276, 2), (277, 1), (278, 2), (280, 1), (281, 1), (282, 1), (305, 2), (405, 1), (412, 1), (424, 2)]
discards: [358 359]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 97s - loss: 767.9548 - loglik: -7.6734e+02 - logprior: -6.1875e-01
Epoch 2/10
113/113 - 94s - loss: 764.8190 - loglik: -7.6433e+02 - logprior: -4.8620e-01
Epoch 3/10
113/113 - 94s - loss: 763.6498 - loglik: -7.6320e+02 - logprior: -4.5000e-01
Epoch 4/10
113/113 - 93s - loss: 756.5644 - loglik: -7.5627e+02 - logprior: -2.9676e-01
Epoch 5/10
113/113 - 94s - loss: 758.1127 - loglik: -7.5769e+02 - logprior: -4.2118e-01
Fitted a model with MAP estimate = -756.3698
Time for alignment: 1302.6700
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1234.7634 - loglik: -1.2330e+03 - logprior: -1.7192e+00
Epoch 2/10
40/40 - 39s - loss: 1113.2440 - loglik: -1.1104e+03 - logprior: -2.8324e+00
Epoch 3/10
40/40 - 39s - loss: 1101.5518 - loglik: -1.0986e+03 - logprior: -2.9516e+00
Epoch 4/10
40/40 - 39s - loss: 1095.7288 - loglik: -1.0929e+03 - logprior: -2.7921e+00
Epoch 5/10
40/40 - 39s - loss: 1090.7834 - loglik: -1.0880e+03 - logprior: -2.7529e+00
Epoch 6/10
40/40 - 39s - loss: 1088.2788 - loglik: -1.0855e+03 - logprior: -2.7565e+00
Epoch 7/10
40/40 - 39s - loss: 1086.8907 - loglik: -1.0841e+03 - logprior: -2.7612e+00
Epoch 8/10
40/40 - 39s - loss: 1086.8347 - loglik: -1.0841e+03 - logprior: -2.7747e+00
Epoch 9/10
40/40 - 39s - loss: 1085.5845 - loglik: -1.0828e+03 - logprior: -2.8021e+00
Epoch 10/10
40/40 - 39s - loss: 1085.2990 - loglik: -1.0825e+03 - logprior: -2.8108e+00
Fitted a model with MAP estimate = -830.1276
expansions: [(122, 1), (126, 1), (134, 1), (170, 6), (171, 1), (330, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 123 124 238 239 240 241 242 243 244 245 246 247 248 249 250
 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268
 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286
 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304
 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322
 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 16s - loss: 1269.0813 - loglik: -1.2650e+03 - logprior: -4.1112e+00
Epoch 2/2
40/40 - 12s - loss: 1232.2585 - loglik: -1.2308e+03 - logprior: -1.4304e+00
Fitted a model with MAP estimate = -897.9632
expansions: [(0, 317), (13, 2), (14, 2), (15, 9), (120, 2), (129, 1), (130, 1)]
discards: [ 11  64  65  66  67  68  69 131 132 133 134 135 136 137]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1084.9692 - loglik: -1.0836e+03 - logprior: -1.3290e+00
Epoch 2/2
80/80 - 68s - loss: 1037.8744 - loglik: -1.0377e+03 - logprior: -2.0709e-01
Fitted a model with MAP estimate = -772.6458
expansions: [(256, 1), (265, 1), (281, 1), (294, 1), (301, 1), (304, 1), (455, 1)]
discards: [  0  20  21  85  86 107 164 165 204 330 336 439 440]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 773.0618 - loglik: -7.7246e+02 - logprior: -5.9706e-01
Epoch 2/10
113/113 - 93s - loss: 758.0094 - loglik: -7.5765e+02 - logprior: -3.6020e-01
Epoch 3/10
113/113 - 93s - loss: 765.0741 - loglik: -7.6478e+02 - logprior: -2.9728e-01
Fitted a model with MAP estimate = -758.2097
Time for alignment: 1148.5393
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 43s - loss: 1244.3678 - loglik: -1.2433e+03 - logprior: -1.0308e+00
Epoch 2/10
40/40 - 39s - loss: 1139.3330 - loglik: -1.1393e+03 - logprior: -2.9012e-04
Epoch 3/10
40/40 - 39s - loss: 1130.6344 - loglik: -1.1307e+03 - logprior: 0.1026
Epoch 4/10
40/40 - 39s - loss: 1125.2831 - loglik: -1.1254e+03 - logprior: 0.1179
Epoch 5/10
40/40 - 39s - loss: 1119.2440 - loglik: -1.1193e+03 - logprior: 0.1043
Epoch 6/10
40/40 - 39s - loss: 1117.0721 - loglik: -1.1171e+03 - logprior: 0.0742
Epoch 7/10
40/40 - 39s - loss: 1115.9170 - loglik: -1.1160e+03 - logprior: 0.0344
Epoch 8/10
40/40 - 39s - loss: 1114.9573 - loglik: -1.1150e+03 - logprior: 0.0012
Epoch 9/10
40/40 - 39s - loss: 1114.2781 - loglik: -1.1143e+03 - logprior: -2.6757e-02
Epoch 10/10
40/40 - 39s - loss: 1113.9475 - loglik: -1.1139e+03 - logprior: -3.3880e-02
Fitted a model with MAP estimate = -848.2588
expansions: [(25, 1), (122, 2), (125, 1), (126, 1), (201, 1), (209, 2), (292, 2), (293, 2), (294, 6), (295, 2), (296, 3), (319, 48), (320, 11), (329, 8), (330, 42)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 72s - loss: 1073.6002 - loglik: -1.0726e+03 - logprior: -9.9955e-01
Epoch 2/2
80/80 - 69s - loss: 1037.1428 - loglik: -1.0368e+03 - logprior: -3.2518e-01
Fitted a model with MAP estimate = -773.7209
expansions: [(396, 6), (448, 1), (455, 1)]
discards: [123 215 300 306 307 312 313 314 358 359 360 361 362 363 364 365 459 460]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 1040.7139 - loglik: -1.0404e+03 - logprior: -3.1614e-01
Epoch 2/2
80/80 - 66s - loss: 1035.9666 - loglik: -1.0365e+03 - logprior: 0.5346
Fitted a model with MAP estimate = -774.2211
expansions: [(307, 1), (438, 1), (451, 2)]
discards: [383 384]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 766.5380 - loglik: -7.6642e+02 - logprior: -1.2108e-01
Epoch 2/10
113/113 - 93s - loss: 763.7428 - loglik: -7.6382e+02 - logprior: 0.0789
Epoch 3/10
113/113 - 93s - loss: 764.5267 - loglik: -7.6478e+02 - logprior: 0.2485
Fitted a model with MAP estimate = -756.8868
Time for alignment: 1378.1000
Computed alignments with likelihoods: ['-752.8303', '-757.2198', '-756.3698', '-758.2097', '-756.8868']
Best model has likelihood: -752.8303  (prior= 0.5250 )
time for generating output: 0.4540
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7621632736993625
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 449.5383 - loglik: -4.4667e+02 - logprior: -2.8667e+00
Epoch 2/10
19/19 - 4s - loss: 282.2389 - loglik: -2.8066e+02 - logprior: -1.5820e+00
Epoch 3/10
19/19 - 4s - loss: 211.7849 - loglik: -2.0958e+02 - logprior: -2.2014e+00
Epoch 4/10
19/19 - 4s - loss: 201.5588 - loglik: -1.9923e+02 - logprior: -2.3238e+00
Epoch 5/10
19/19 - 4s - loss: 198.3012 - loglik: -1.9607e+02 - logprior: -2.2305e+00
Epoch 6/10
19/19 - 4s - loss: 196.1431 - loglik: -1.9398e+02 - logprior: -2.1624e+00
Epoch 7/10
19/19 - 4s - loss: 195.0064 - loglik: -1.9288e+02 - logprior: -2.1266e+00
Epoch 8/10
19/19 - 4s - loss: 193.6183 - loglik: -1.9150e+02 - logprior: -2.1147e+00
Epoch 9/10
19/19 - 4s - loss: 193.2902 - loglik: -1.9118e+02 - logprior: -2.1122e+00
Epoch 10/10
19/19 - 4s - loss: 192.9984 - loglik: -1.9090e+02 - logprior: -2.1027e+00
Fitted a model with MAP estimate = -187.8418
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 183.5657 - loglik: -1.8046e+02 - logprior: -3.1047e+00
Epoch 2/2
19/19 - 5s - loss: 144.6167 - loglik: -1.4321e+02 - logprior: -1.4052e+00
Fitted a model with MAP estimate = -143.3933
expansions: []
discards: [ 50  70  77 127 130 141 155 157 162]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 146.6515 - loglik: -1.4365e+02 - logprior: -3.0031e+00
Epoch 2/2
19/19 - 5s - loss: 141.0679 - loglik: -1.3998e+02 - logprior: -1.0910e+00
Fitted a model with MAP estimate = -142.1115
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.9538 - loglik: -1.4286e+02 - logprior: -2.0988e+00
Epoch 2/10
22/22 - 6s - loss: 139.9785 - loglik: -1.3905e+02 - logprior: -9.3330e-01
Epoch 3/10
22/22 - 6s - loss: 138.1442 - loglik: -1.3723e+02 - logprior: -9.1316e-01
Epoch 4/10
22/22 - 6s - loss: 135.0353 - loglik: -1.3413e+02 - logprior: -9.0193e-01
Epoch 5/10
22/22 - 6s - loss: 133.4691 - loglik: -1.3261e+02 - logprior: -8.6030e-01
Epoch 6/10
22/22 - 6s - loss: 133.5863 - loglik: -1.3275e+02 - logprior: -8.3670e-01
Fitted a model with MAP estimate = -131.4896
Time for alignment: 148.0887
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.1786 - loglik: -4.4631e+02 - logprior: -2.8704e+00
Epoch 2/10
19/19 - 4s - loss: 281.6508 - loglik: -2.8007e+02 - logprior: -1.5844e+00
Epoch 3/10
19/19 - 4s - loss: 211.9324 - loglik: -2.0972e+02 - logprior: -2.2099e+00
Epoch 4/10
19/19 - 4s - loss: 202.5058 - loglik: -2.0019e+02 - logprior: -2.3167e+00
Epoch 5/10
19/19 - 4s - loss: 199.0156 - loglik: -1.9681e+02 - logprior: -2.2105e+00
Epoch 6/10
19/19 - 4s - loss: 196.4389 - loglik: -1.9430e+02 - logprior: -2.1362e+00
Epoch 7/10
19/19 - 4s - loss: 195.6361 - loglik: -1.9355e+02 - logprior: -2.0880e+00
Epoch 8/10
19/19 - 4s - loss: 194.1281 - loglik: -1.9206e+02 - logprior: -2.0717e+00
Epoch 9/10
19/19 - 4s - loss: 193.5108 - loglik: -1.9144e+02 - logprior: -2.0718e+00
Epoch 10/10
19/19 - 4s - loss: 193.7460 - loglik: -1.9168e+02 - logprior: -2.0658e+00
Fitted a model with MAP estimate = -188.3361
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 183.5555 - loglik: -1.8044e+02 - logprior: -3.1142e+00
Epoch 2/2
19/19 - 5s - loss: 143.9031 - loglik: -1.4250e+02 - logprior: -1.4021e+00
Fitted a model with MAP estimate = -143.3246
expansions: []
discards: [ 50  70  77 127 130 141 155 157 162]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 146.7856 - loglik: -1.4380e+02 - logprior: -2.9898e+00
Epoch 2/2
19/19 - 5s - loss: 140.8123 - loglik: -1.3973e+02 - logprior: -1.0824e+00
Fitted a model with MAP estimate = -142.1678
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 143.9526 - loglik: -1.4186e+02 - logprior: -2.0941e+00
Epoch 2/10
22/22 - 6s - loss: 141.0285 - loglik: -1.4010e+02 - logprior: -9.2858e-01
Epoch 3/10
22/22 - 6s - loss: 137.3927 - loglik: -1.3649e+02 - logprior: -9.0250e-01
Epoch 4/10
22/22 - 6s - loss: 136.1984 - loglik: -1.3529e+02 - logprior: -9.0949e-01
Epoch 5/10
22/22 - 6s - loss: 133.7918 - loglik: -1.3293e+02 - logprior: -8.6136e-01
Epoch 6/10
22/22 - 6s - loss: 132.8066 - loglik: -1.3198e+02 - logprior: -8.2350e-01
Epoch 7/10
22/22 - 6s - loss: 132.4021 - loglik: -1.3161e+02 - logprior: -7.9204e-01
Epoch 8/10
22/22 - 6s - loss: 129.0768 - loglik: -1.2832e+02 - logprior: -7.5720e-01
Epoch 9/10
22/22 - 6s - loss: 129.8005 - loglik: -1.2907e+02 - logprior: -7.3042e-01
Fitted a model with MAP estimate = -129.8217
Time for alignment: 164.2473
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 449.8965 - loglik: -4.4702e+02 - logprior: -2.8733e+00
Epoch 2/10
19/19 - 4s - loss: 282.0419 - loglik: -2.8044e+02 - logprior: -1.6038e+00
Epoch 3/10
19/19 - 4s - loss: 212.2716 - loglik: -2.1005e+02 - logprior: -2.2255e+00
Epoch 4/10
19/19 - 4s - loss: 200.5769 - loglik: -1.9823e+02 - logprior: -2.3454e+00
Epoch 5/10
19/19 - 4s - loss: 197.2615 - loglik: -1.9500e+02 - logprior: -2.2619e+00
Epoch 6/10
19/19 - 4s - loss: 195.8558 - loglik: -1.9366e+02 - logprior: -2.1921e+00
Epoch 7/10
19/19 - 4s - loss: 194.6531 - loglik: -1.9250e+02 - logprior: -2.1532e+00
Epoch 8/10
19/19 - 4s - loss: 192.5776 - loglik: -1.9044e+02 - logprior: -2.1407e+00
Epoch 9/10
19/19 - 4s - loss: 190.9644 - loglik: -1.8883e+02 - logprior: -2.1324e+00
Epoch 10/10
19/19 - 4s - loss: 193.0107 - loglik: -1.9088e+02 - logprior: -2.1278e+00
Fitted a model with MAP estimate = -186.7578
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 1), (61, 2), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 183.1090 - loglik: -1.8038e+02 - logprior: -2.7255e+00
Epoch 2/2
19/19 - 5s - loss: 145.9916 - loglik: -1.4494e+02 - logprior: -1.0468e+00
Fitted a model with MAP estimate = -143.4047
expansions: []
discards: [ 50  78 126 129 140 154 156 161]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 147.5313 - loglik: -1.4491e+02 - logprior: -2.6227e+00
Epoch 2/2
19/19 - 5s - loss: 141.4697 - loglik: -1.4065e+02 - logprior: -8.2386e-01
Fitted a model with MAP estimate = -141.8471
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.5687 - loglik: -1.4248e+02 - logprior: -2.0856e+00
Epoch 2/10
22/22 - 6s - loss: 139.2114 - loglik: -1.3822e+02 - logprior: -9.8801e-01
Epoch 3/10
22/22 - 6s - loss: 138.0327 - loglik: -1.3709e+02 - logprior: -9.4565e-01
Epoch 4/10
22/22 - 6s - loss: 136.1667 - loglik: -1.3526e+02 - logprior: -9.1062e-01
Epoch 5/10
22/22 - 6s - loss: 133.6455 - loglik: -1.3277e+02 - logprior: -8.8046e-01
Epoch 6/10
22/22 - 6s - loss: 133.8209 - loglik: -1.3299e+02 - logprior: -8.2687e-01
Fitted a model with MAP estimate = -131.5624
Time for alignment: 146.7812
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.5948 - loglik: -4.4672e+02 - logprior: -2.8738e+00
Epoch 2/10
19/19 - 4s - loss: 284.0652 - loglik: -2.8247e+02 - logprior: -1.5928e+00
Epoch 3/10
19/19 - 4s - loss: 213.3164 - loglik: -2.1112e+02 - logprior: -2.2011e+00
Epoch 4/10
19/19 - 4s - loss: 202.6089 - loglik: -2.0026e+02 - logprior: -2.3488e+00
Epoch 5/10
19/19 - 4s - loss: 198.4073 - loglik: -1.9615e+02 - logprior: -2.2587e+00
Epoch 6/10
19/19 - 4s - loss: 196.6658 - loglik: -1.9448e+02 - logprior: -2.1884e+00
Epoch 7/10
19/19 - 4s - loss: 195.2162 - loglik: -1.9307e+02 - logprior: -2.1419e+00
Epoch 8/10
19/19 - 4s - loss: 193.9937 - loglik: -1.9188e+02 - logprior: -2.1166e+00
Epoch 9/10
19/19 - 4s - loss: 192.9727 - loglik: -1.9086e+02 - logprior: -2.1137e+00
Epoch 10/10
19/19 - 4s - loss: 193.0205 - loglik: -1.9091e+02 - logprior: -2.1066e+00
Fitted a model with MAP estimate = -188.0176
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 1), (104, 1), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 183.0702 - loglik: -1.8034e+02 - logprior: -2.7311e+00
Epoch 2/2
19/19 - 5s - loss: 144.7619 - loglik: -1.4383e+02 - logprior: -9.3696e-01
Fitted a model with MAP estimate = -143.5528
expansions: []
discards: [ 50  70  77 127 153 155 160]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 147.1659 - loglik: -1.4454e+02 - logprior: -2.6262e+00
Epoch 2/2
19/19 - 5s - loss: 141.4796 - loglik: -1.4076e+02 - logprior: -7.1923e-01
Fitted a model with MAP estimate = -142.2659
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 144.8964 - loglik: -1.4307e+02 - logprior: -1.8306e+00
Epoch 2/10
22/22 - 6s - loss: 140.5957 - loglik: -1.3963e+02 - logprior: -9.6378e-01
Epoch 3/10
22/22 - 6s - loss: 137.7673 - loglik: -1.3683e+02 - logprior: -9.3395e-01
Epoch 4/10
22/22 - 6s - loss: 135.2450 - loglik: -1.3437e+02 - logprior: -8.7633e-01
Epoch 5/10
22/22 - 6s - loss: 135.2854 - loglik: -1.3443e+02 - logprior: -8.5467e-01
Fitted a model with MAP estimate = -133.1966
Time for alignment: 142.7661
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.4977 - loglik: -4.4662e+02 - logprior: -2.8750e+00
Epoch 2/10
19/19 - 4s - loss: 281.4030 - loglik: -2.7980e+02 - logprior: -1.6002e+00
Epoch 3/10
19/19 - 4s - loss: 211.1213 - loglik: -2.0891e+02 - logprior: -2.2153e+00
Epoch 4/10
19/19 - 4s - loss: 203.1789 - loglik: -2.0085e+02 - logprior: -2.3316e+00
Epoch 5/10
19/19 - 4s - loss: 198.8496 - loglik: -1.9661e+02 - logprior: -2.2433e+00
Epoch 6/10
19/19 - 4s - loss: 195.1685 - loglik: -1.9300e+02 - logprior: -2.1724e+00
Epoch 7/10
19/19 - 4s - loss: 196.0418 - loglik: -1.9391e+02 - logprior: -2.1350e+00
Fitted a model with MAP estimate = -188.2294
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (105, 1), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 181.0867 - loglik: -1.7798e+02 - logprior: -3.1070e+00
Epoch 2/2
19/19 - 5s - loss: 144.5581 - loglik: -1.4321e+02 - logprior: -1.3475e+00
Fitted a model with MAP estimate = -142.9315
expansions: []
discards: [ 50  76 151 153 158]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 146.9130 - loglik: -1.4389e+02 - logprior: -3.0184e+00
Epoch 2/2
19/19 - 5s - loss: 141.3353 - loglik: -1.4020e+02 - logprior: -1.1388e+00
Fitted a model with MAP estimate = -141.7154
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 144.4671 - loglik: -1.4231e+02 - logprior: -2.1611e+00
Epoch 2/10
22/22 - 6s - loss: 140.7760 - loglik: -1.3979e+02 - logprior: -9.8239e-01
Epoch 3/10
22/22 - 6s - loss: 136.1915 - loglik: -1.3524e+02 - logprior: -9.4843e-01
Epoch 4/10
22/22 - 6s - loss: 136.9924 - loglik: -1.3608e+02 - logprior: -9.1455e-01
Fitted a model with MAP estimate = -134.6028
Time for alignment: 123.4666
Computed alignments with likelihoods: ['-131.4896', '-129.8217', '-131.5624', '-133.1966', '-134.6028']
Best model has likelihood: -129.8217  (prior= -0.6810 )
time for generating output: 0.1794
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 606.3050 - loglik: -5.7421e+02 - logprior: -3.2098e+01
Epoch 2/10
12/12 - 3s - loss: 522.6865 - loglik: -5.1931e+02 - logprior: -3.3774e+00
Epoch 3/10
12/12 - 3s - loss: 460.8939 - loglik: -4.6033e+02 - logprior: -5.5971e-01
Epoch 4/10
12/12 - 3s - loss: 432.9500 - loglik: -4.3322e+02 - logprior: 0.2667
Epoch 5/10
12/12 - 3s - loss: 427.0852 - loglik: -4.2794e+02 - logprior: 0.8537
Epoch 6/10
12/12 - 3s - loss: 423.9779 - loglik: -4.2514e+02 - logprior: 1.1626
Epoch 7/10
12/12 - 3s - loss: 421.9637 - loglik: -4.2331e+02 - logprior: 1.3468
Epoch 8/10
12/12 - 3s - loss: 421.4453 - loglik: -4.2290e+02 - logprior: 1.4561
Epoch 9/10
12/12 - 3s - loss: 420.7507 - loglik: -4.2233e+02 - logprior: 1.5798
Epoch 10/10
12/12 - 3s - loss: 420.3212 - loglik: -4.2197e+02 - logprior: 1.6495
Fitted a model with MAP estimate = -420.2023
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 2), (86, 1), (87, 1), (90, 1), (91, 1), (101, 1), (102, 1), (103, 1), (127, 4), (129, 3), (137, 1), (146, 3), (149, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 454.5290 - loglik: -4.1765e+02 - logprior: -3.6879e+01
Epoch 2/2
12/12 - 4s - loss: 414.8982 - loglik: -4.0391e+02 - logprior: -1.0991e+01
Fitted a model with MAP estimate = -409.6894
expansions: [(0, 4)]
discards: [  0  35  36  72  97 189 218]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 429.6208 - loglik: -4.0254e+02 - logprior: -2.7077e+01
Epoch 2/2
12/12 - 4s - loss: 403.1466 - loglik: -4.0223e+02 - logprior: -9.1900e-01
Fitted a model with MAP estimate = -397.7833
expansions: []
discards: [  1   2   3 130]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 429.1033 - loglik: -4.0292e+02 - logprior: -2.6183e+01
Epoch 2/10
12/12 - 4s - loss: 400.7466 - loglik: -4.0026e+02 - logprior: -4.8996e-01
Epoch 3/10
12/12 - 3s - loss: 395.8188 - loglik: -4.0007e+02 - logprior: 4.2526
Epoch 4/10
12/12 - 4s - loss: 393.5980 - loglik: -3.9994e+02 - logprior: 6.3441
Epoch 5/10
12/12 - 3s - loss: 393.3044 - loglik: -4.0073e+02 - logprior: 7.4213
Epoch 6/10
12/12 - 3s - loss: 392.0366 - loglik: -4.0009e+02 - logprior: 8.0496
Epoch 7/10
12/12 - 3s - loss: 389.5011 - loglik: -3.9799e+02 - logprior: 8.4920
Epoch 8/10
12/12 - 3s - loss: 391.2945 - loglik: -4.0020e+02 - logprior: 8.9103
Fitted a model with MAP estimate = -390.1865
Time for alignment: 94.4191
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 608.1906 - loglik: -5.7609e+02 - logprior: -3.2100e+01
Epoch 2/10
12/12 - 3s - loss: 521.3497 - loglik: -5.1797e+02 - logprior: -3.3843e+00
Epoch 3/10
12/12 - 3s - loss: 460.0042 - loglik: -4.5915e+02 - logprior: -8.5157e-01
Epoch 4/10
12/12 - 3s - loss: 433.0980 - loglik: -4.3268e+02 - logprior: -4.1971e-01
Epoch 5/10
12/12 - 3s - loss: 425.7563 - loglik: -4.2594e+02 - logprior: 0.1876
Epoch 6/10
12/12 - 3s - loss: 422.6425 - loglik: -4.2320e+02 - logprior: 0.5541
Epoch 7/10
12/12 - 3s - loss: 420.6849 - loglik: -4.2130e+02 - logprior: 0.6192
Epoch 8/10
12/12 - 3s - loss: 418.7564 - loglik: -4.1955e+02 - logprior: 0.7970
Epoch 9/10
12/12 - 3s - loss: 419.5993 - loglik: -4.2054e+02 - logprior: 0.9408
Fitted a model with MAP estimate = -418.4380
expansions: [(11, 3), (19, 1), (24, 1), (31, 2), (32, 1), (41, 1), (44, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (118, 1), (126, 5), (127, 1), (129, 1), (130, 1), (138, 1), (146, 1), (147, 3), (150, 1), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 452.1478 - loglik: -4.1521e+02 - logprior: -3.6942e+01
Epoch 2/2
12/12 - 4s - loss: 414.7748 - loglik: -4.0385e+02 - logprior: -1.0923e+01
Fitted a model with MAP estimate = -408.2289
expansions: [(0, 4), (105, 1), (184, 1)]
discards: [  0  94 153 185 186 216]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 429.6367 - loglik: -4.0256e+02 - logprior: -2.7072e+01
Epoch 2/2
12/12 - 4s - loss: 402.3771 - loglik: -4.0140e+02 - logprior: -9.7564e-01
Fitted a model with MAP estimate = -396.9601
expansions: [(186, 2)]
discards: [  1   2   3 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 427.6997 - loglik: -4.0154e+02 - logprior: -2.6161e+01
Epoch 2/10
12/12 - 4s - loss: 399.5887 - loglik: -3.9907e+02 - logprior: -5.1472e-01
Epoch 3/10
12/12 - 4s - loss: 395.9717 - loglik: -4.0024e+02 - logprior: 4.2653
Epoch 4/10
12/12 - 4s - loss: 391.0986 - loglik: -3.9746e+02 - logprior: 6.3588
Epoch 5/10
12/12 - 4s - loss: 390.7637 - loglik: -3.9818e+02 - logprior: 7.4115
Epoch 6/10
12/12 - 4s - loss: 390.9579 - loglik: -3.9903e+02 - logprior: 8.0752
Fitted a model with MAP estimate = -389.5451
Time for alignment: 84.3078
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 607.2504 - loglik: -5.7515e+02 - logprior: -3.2105e+01
Epoch 2/10
12/12 - 3s - loss: 523.1552 - loglik: -5.1976e+02 - logprior: -3.3939e+00
Epoch 3/10
12/12 - 3s - loss: 460.3351 - loglik: -4.5955e+02 - logprior: -7.8673e-01
Epoch 4/10
12/12 - 3s - loss: 430.2991 - loglik: -4.3013e+02 - logprior: -1.6863e-01
Epoch 5/10
12/12 - 3s - loss: 425.4861 - loglik: -4.2592e+02 - logprior: 0.4292
Epoch 6/10
12/12 - 3s - loss: 422.2645 - loglik: -4.2309e+02 - logprior: 0.8239
Epoch 7/10
12/12 - 3s - loss: 420.3219 - loglik: -4.2135e+02 - logprior: 1.0284
Epoch 8/10
12/12 - 3s - loss: 420.3203 - loglik: -4.2150e+02 - logprior: 1.1844
Epoch 9/10
12/12 - 3s - loss: 418.8970 - loglik: -4.2020e+02 - logprior: 1.3029
Epoch 10/10
12/12 - 3s - loss: 418.1644 - loglik: -4.1955e+02 - logprior: 1.3856
Fitted a model with MAP estimate = -418.3644
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (41, 1), (48, 1), (60, 2), (61, 1), (62, 1), (63, 1), (74, 2), (76, 2), (77, 2), (86, 1), (88, 1), (90, 1), (91, 2), (101, 1), (102, 1), (121, 1), (127, 1), (129, 5), (137, 1), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 454.2235 - loglik: -4.1713e+02 - logprior: -3.7098e+01
Epoch 2/2
12/12 - 4s - loss: 417.0303 - loglik: -4.0573e+02 - logprior: -1.1300e+01
Fitted a model with MAP estimate = -411.0882
expansions: [(0, 4), (156, 5)]
discards: [  0  35  36  72  95 116 161 163 188 218]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 432.3208 - loglik: -4.0507e+02 - logprior: -2.7254e+01
Epoch 2/2
12/12 - 4s - loss: 402.8267 - loglik: -4.0181e+02 - logprior: -1.0149e+00
Fitted a model with MAP estimate = -397.5766
expansions: []
discards: [  1   2   3  91 156 157 158]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 428.8547 - loglik: -4.0260e+02 - logprior: -2.6259e+01
Epoch 2/10
12/12 - 3s - loss: 402.6606 - loglik: -4.0202e+02 - logprior: -6.4445e-01
Epoch 3/10
12/12 - 3s - loss: 395.9884 - loglik: -4.0018e+02 - logprior: 4.1923
Epoch 4/10
12/12 - 3s - loss: 394.2128 - loglik: -4.0046e+02 - logprior: 6.2463
Epoch 5/10
12/12 - 3s - loss: 391.5421 - loglik: -3.9887e+02 - logprior: 7.3264
Epoch 6/10
12/12 - 3s - loss: 392.8624 - loglik: -4.0080e+02 - logprior: 7.9336
Fitted a model with MAP estimate = -391.1641
Time for alignment: 86.3667
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 606.4285 - loglik: -5.7432e+02 - logprior: -3.2105e+01
Epoch 2/10
12/12 - 3s - loss: 524.2230 - loglik: -5.2084e+02 - logprior: -3.3791e+00
Epoch 3/10
12/12 - 3s - loss: 461.4059 - loglik: -4.6065e+02 - logprior: -7.5398e-01
Epoch 4/10
12/12 - 3s - loss: 432.7389 - loglik: -4.3277e+02 - logprior: 0.0341
Epoch 5/10
12/12 - 3s - loss: 424.3497 - loglik: -4.2493e+02 - logprior: 0.5798
Epoch 6/10
12/12 - 3s - loss: 422.0962 - loglik: -4.2306e+02 - logprior: 0.9669
Epoch 7/10
12/12 - 3s - loss: 421.4194 - loglik: -4.2264e+02 - logprior: 1.2220
Epoch 8/10
12/12 - 3s - loss: 420.5730 - loglik: -4.2198e+02 - logprior: 1.4096
Epoch 9/10
12/12 - 3s - loss: 420.2396 - loglik: -4.2178e+02 - logprior: 1.5413
Epoch 10/10
12/12 - 3s - loss: 419.0945 - loglik: -4.2072e+02 - logprior: 1.6261
Fitted a model with MAP estimate = -419.0175
expansions: [(11, 3), (21, 1), (30, 1), (31, 3), (32, 1), (41, 1), (61, 2), (62, 2), (63, 2), (75, 1), (76, 2), (77, 1), (78, 1), (87, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (127, 5), (129, 3), (137, 1), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 454.3470 - loglik: -4.1717e+02 - logprior: -3.7173e+01
Epoch 2/2
12/12 - 4s - loss: 417.4320 - loglik: -4.0631e+02 - logprior: -1.1121e+01
Fitted a model with MAP estimate = -411.9773
expansions: [(0, 4), (109, 1), (181, 3)]
discards: [  0  34  71 155 184 213]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 432.8165 - loglik: -4.0559e+02 - logprior: -2.7230e+01
Epoch 2/2
12/12 - 4s - loss: 401.5961 - loglik: -4.0056e+02 - logprior: -1.0334e+00
Fitted a model with MAP estimate = -398.1028
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 428.2708 - loglik: -4.0203e+02 - logprior: -2.6240e+01
Epoch 2/10
12/12 - 3s - loss: 399.4761 - loglik: -3.9899e+02 - logprior: -4.8360e-01
Epoch 3/10
12/12 - 4s - loss: 398.1741 - loglik: -4.0243e+02 - logprior: 4.2533
Epoch 4/10
12/12 - 3s - loss: 390.3082 - loglik: -3.9663e+02 - logprior: 6.3256
Epoch 5/10
12/12 - 3s - loss: 394.1124 - loglik: -4.0152e+02 - logprior: 7.4089
Fitted a model with MAP estimate = -391.1979
Time for alignment: 82.2287
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 606.4893 - loglik: -5.7439e+02 - logprior: -3.2095e+01
Epoch 2/10
12/12 - 3s - loss: 525.3634 - loglik: -5.2197e+02 - logprior: -3.3894e+00
Epoch 3/10
12/12 - 3s - loss: 463.3069 - loglik: -4.6260e+02 - logprior: -7.1010e-01
Epoch 4/10
12/12 - 3s - loss: 436.8912 - loglik: -4.3680e+02 - logprior: -9.5989e-02
Epoch 5/10
12/12 - 3s - loss: 427.3470 - loglik: -4.2765e+02 - logprior: 0.3036
Epoch 6/10
12/12 - 3s - loss: 422.5513 - loglik: -4.2322e+02 - logprior: 0.6718
Epoch 7/10
12/12 - 3s - loss: 422.9127 - loglik: -4.2381e+02 - logprior: 0.8987
Fitted a model with MAP estimate = -421.4628
expansions: [(11, 3), (19, 1), (30, 1), (31, 2), (39, 1), (41, 1), (45, 1), (60, 2), (61, 2), (62, 2), (76, 2), (77, 2), (86, 1), (90, 1), (91, 1), (93, 1), (101, 1), (102, 1), (122, 2), (126, 1), (127, 2), (129, 2), (131, 4), (138, 1), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 457.9183 - loglik: -4.2075e+02 - logprior: -3.7164e+01
Epoch 2/2
12/12 - 4s - loss: 418.6849 - loglik: -4.0749e+02 - logprior: -1.1192e+01
Fitted a model with MAP estimate = -413.3377
expansions: [(0, 4)]
discards: [  0  70  92 128 147 153 161 162 163 189 218]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 436.3445 - loglik: -4.0896e+02 - logprior: -2.7385e+01
Epoch 2/2
12/12 - 3s - loss: 404.4622 - loglik: -4.0305e+02 - logprior: -1.4119e+00
Fitted a model with MAP estimate = -400.9519
expansions: []
discards: [  1   2   3 157]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 430.6603 - loglik: -4.0409e+02 - logprior: -2.6567e+01
Epoch 2/10
12/12 - 3s - loss: 402.9563 - loglik: -4.0214e+02 - logprior: -8.1588e-01
Epoch 3/10
12/12 - 3s - loss: 399.3935 - loglik: -4.0332e+02 - logprior: 3.9258
Epoch 4/10
12/12 - 3s - loss: 395.3063 - loglik: -4.0131e+02 - logprior: 6.0084
Epoch 5/10
12/12 - 3s - loss: 394.7173 - loglik: -4.0179e+02 - logprior: 7.0717
Epoch 6/10
12/12 - 3s - loss: 394.1448 - loglik: -4.0184e+02 - logprior: 7.6935
Epoch 7/10
12/12 - 3s - loss: 391.9351 - loglik: -4.0007e+02 - logprior: 8.1356
Epoch 8/10
12/12 - 3s - loss: 393.7766 - loglik: -4.0233e+02 - logprior: 8.5545
Fitted a model with MAP estimate = -392.3537
Time for alignment: 83.8954
Computed alignments with likelihoods: ['-390.1865', '-389.5451', '-391.1641', '-391.1979', '-392.3537']
Best model has likelihood: -389.5451  (prior= 8.2868 )
time for generating output: 0.2254
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9364550767976078
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 604.5462 - loglik: -5.9727e+02 - logprior: -7.2765e+00
Epoch 2/10
21/21 - 7s - loss: 474.2374 - loglik: -4.7250e+02 - logprior: -1.7414e+00
Epoch 3/10
21/21 - 7s - loss: 435.5591 - loglik: -4.3318e+02 - logprior: -2.3825e+00
Epoch 4/10
21/21 - 6s - loss: 425.6227 - loglik: -4.2312e+02 - logprior: -2.5012e+00
Epoch 5/10
21/21 - 6s - loss: 423.1964 - loglik: -4.2088e+02 - logprior: -2.3182e+00
Epoch 6/10
21/21 - 7s - loss: 424.3435 - loglik: -4.2195e+02 - logprior: -2.3906e+00
Fitted a model with MAP estimate = -422.8327
expansions: [(7, 1), (17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (62, 1), (63, 3), (75, 8), (80, 1), (96, 1), (99, 2), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (147, 2), (154, 1), (160, 1), (161, 1), (162, 2), (163, 1), (164, 1), (167, 1), (168, 2), (171, 1), (178, 1), (181, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 421.1626 - loglik: -4.1141e+02 - logprior: -9.7516e+00
Epoch 2/2
21/21 - 9s - loss: 393.3281 - loglik: -3.9025e+02 - logprior: -3.0805e+00
Fitted a model with MAP estimate = -390.2404
expansions: [(0, 5)]
discards: [  0  24  28  81  97 131 151 217]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 401.0381 - loglik: -3.9467e+02 - logprior: -6.3641e+00
Epoch 2/2
21/21 - 9s - loss: 389.7315 - loglik: -3.8977e+02 - logprior: 0.0349
Fitted a model with MAP estimate = -387.2655
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 397.8159 - loglik: -3.9188e+02 - logprior: -5.9338e+00
Epoch 2/10
21/21 - 8s - loss: 389.1236 - loglik: -3.8983e+02 - logprior: 0.7028
Epoch 3/10
21/21 - 9s - loss: 386.4284 - loglik: -3.8799e+02 - logprior: 1.5659
Epoch 4/10
21/21 - 8s - loss: 385.6805 - loglik: -3.8759e+02 - logprior: 1.9074
Epoch 5/10
21/21 - 8s - loss: 384.3687 - loglik: -3.8646e+02 - logprior: 2.0956
Epoch 6/10
21/21 - 9s - loss: 384.5093 - loglik: -3.8673e+02 - logprior: 2.2170
Fitted a model with MAP estimate = -383.4647
Time for alignment: 168.4037
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 604.6506 - loglik: -5.9740e+02 - logprior: -7.2482e+00
Epoch 2/10
21/21 - 6s - loss: 482.2187 - loglik: -4.8052e+02 - logprior: -1.7024e+00
Epoch 3/10
21/21 - 6s - loss: 444.2355 - loglik: -4.4212e+02 - logprior: -2.1170e+00
Epoch 4/10
21/21 - 6s - loss: 435.4495 - loglik: -4.3340e+02 - logprior: -2.0454e+00
Epoch 5/10
21/21 - 6s - loss: 433.5107 - loglik: -4.3152e+02 - logprior: -1.9886e+00
Epoch 6/10
21/21 - 7s - loss: 431.6857 - loglik: -4.2970e+02 - logprior: -1.9866e+00
Epoch 7/10
21/21 - 6s - loss: 431.0882 - loglik: -4.2907e+02 - logprior: -2.0198e+00
Epoch 8/10
21/21 - 6s - loss: 431.6334 - loglik: -4.2961e+02 - logprior: -2.0264e+00
Fitted a model with MAP estimate = -430.8537
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (74, 2), (75, 6), (78, 1), (97, 1), (98, 1), (100, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (124, 1), (146, 1), (147, 1), (148, 1), (155, 1), (161, 2), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 421.5848 - loglik: -4.1487e+02 - logprior: -6.7170e+00
Epoch 2/2
21/21 - 9s - loss: 395.1474 - loglik: -3.9489e+02 - logprior: -2.5322e-01
Fitted a model with MAP estimate = -393.5436
expansions: []
discards: [ 24  49  80 218]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 404.1916 - loglik: -3.9791e+02 - logprior: -6.2857e+00
Epoch 2/2
21/21 - 8s - loss: 393.7239 - loglik: -3.9400e+02 - logprior: 0.2757
Fitted a model with MAP estimate = -393.0322
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 402.6935 - loglik: -3.9669e+02 - logprior: -5.9997e+00
Epoch 2/10
21/21 - 9s - loss: 394.2999 - loglik: -3.9492e+02 - logprior: 0.6219
Epoch 3/10
21/21 - 9s - loss: 391.4452 - loglik: -3.9295e+02 - logprior: 1.5054
Epoch 4/10
21/21 - 9s - loss: 391.4417 - loglik: -3.9326e+02 - logprior: 1.8210
Epoch 5/10
21/21 - 9s - loss: 389.2701 - loglik: -3.9124e+02 - logprior: 1.9650
Epoch 6/10
21/21 - 8s - loss: 389.0761 - loglik: -3.9123e+02 - logprior: 2.1503
Epoch 7/10
21/21 - 9s - loss: 388.6540 - loglik: -3.9095e+02 - logprior: 2.2995
Epoch 8/10
21/21 - 9s - loss: 388.6423 - loglik: -3.9110e+02 - logprior: 2.4582
Epoch 9/10
21/21 - 9s - loss: 387.9112 - loglik: -3.9055e+02 - logprior: 2.6432
Epoch 10/10
21/21 - 8s - loss: 387.9099 - loglik: -3.9073e+02 - logprior: 2.8171
Fitted a model with MAP estimate = -387.3264
Time for alignment: 214.2321
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 603.8793 - loglik: -5.9661e+02 - logprior: -7.2676e+00
Epoch 2/10
21/21 - 6s - loss: 481.4935 - loglik: -4.7977e+02 - logprior: -1.7283e+00
Epoch 3/10
21/21 - 6s - loss: 442.6812 - loglik: -4.4056e+02 - logprior: -2.1183e+00
Epoch 4/10
21/21 - 6s - loss: 436.1130 - loglik: -4.3414e+02 - logprior: -1.9731e+00
Epoch 5/10
21/21 - 7s - loss: 433.5696 - loglik: -4.3175e+02 - logprior: -1.8165e+00
Epoch 6/10
21/21 - 6s - loss: 432.1241 - loglik: -4.3027e+02 - logprior: -1.8585e+00
Epoch 7/10
21/21 - 6s - loss: 432.0698 - loglik: -4.3018e+02 - logprior: -1.8898e+00
Epoch 8/10
21/21 - 6s - loss: 432.9242 - loglik: -4.3099e+02 - logprior: -1.9341e+00
Fitted a model with MAP estimate = -431.4569
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (38, 2), (39, 2), (49, 1), (59, 1), (76, 7), (78, 1), (80, 1), (96, 1), (99, 1), (115, 1), (118, 1), (119, 1), (120, 1), (121, 1), (143, 1), (145, 1), (146, 1), (147, 2), (149, 1), (157, 1), (160, 1), (162, 2), (163, 1), (164, 1), (169, 3), (171, 1), (178, 1), (179, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 423.0866 - loglik: -4.1634e+02 - logprior: -6.7424e+00
Epoch 2/2
21/21 - 9s - loss: 401.3669 - loglik: -4.0115e+02 - logprior: -2.1509e-01
Fitted a model with MAP estimate = -396.9286
expansions: [(79, 3), (94, 1), (145, 1)]
discards: [ 24  52 179 183 215]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 407.6749 - loglik: -4.0144e+02 - logprior: -6.2375e+00
Epoch 2/2
21/21 - 9s - loss: 396.4681 - loglik: -3.9669e+02 - logprior: 0.2181
Fitted a model with MAP estimate = -394.7479
expansions: []
discards: [79]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 405.8332 - loglik: -3.9981e+02 - logprior: -6.0207e+00
Epoch 2/10
21/21 - 8s - loss: 395.8535 - loglik: -3.9645e+02 - logprior: 0.5936
Epoch 3/10
21/21 - 9s - loss: 393.1599 - loglik: -3.9462e+02 - logprior: 1.4561
Epoch 4/10
21/21 - 8s - loss: 393.7614 - loglik: -3.9556e+02 - logprior: 1.7981
Fitted a model with MAP estimate = -391.7437
Time for alignment: 160.9572
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 605.1970 - loglik: -5.9794e+02 - logprior: -7.2594e+00
Epoch 2/10
21/21 - 6s - loss: 487.0944 - loglik: -4.8536e+02 - logprior: -1.7381e+00
Epoch 3/10
21/21 - 7s - loss: 450.3108 - loglik: -4.4831e+02 - logprior: -1.9966e+00
Epoch 4/10
21/21 - 6s - loss: 439.0286 - loglik: -4.3715e+02 - logprior: -1.8824e+00
Epoch 5/10
21/21 - 7s - loss: 438.6769 - loglik: -4.3686e+02 - logprior: -1.8142e+00
Epoch 6/10
21/21 - 6s - loss: 434.5114 - loglik: -4.3270e+02 - logprior: -1.8129e+00
Epoch 7/10
21/21 - 6s - loss: 435.9366 - loglik: -4.3410e+02 - logprior: -1.8337e+00
Fitted a model with MAP estimate = -434.9574
expansions: [(17, 1), (20, 1), (21, 2), (22, 3), (23, 1), (25, 1), (38, 1), (39, 1), (40, 1), (41, 1), (47, 1), (50, 1), (64, 4), (66, 1), (76, 5), (77, 2), (78, 2), (97, 1), (100, 2), (102, 1), (115, 1), (119, 1), (120, 1), (121, 4), (146, 1), (147, 2), (154, 1), (157, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 424.7087 - loglik: -4.1794e+02 - logprior: -6.7678e+00
Epoch 2/2
21/21 - 9s - loss: 398.9176 - loglik: -3.9869e+02 - logprior: -2.2847e-01
Fitted a model with MAP estimate = -395.8672
expansions: []
discards: [ 27  81 105 132 158 219]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 407.3783 - loglik: -4.0114e+02 - logprior: -6.2431e+00
Epoch 2/2
21/21 - 8s - loss: 396.5814 - loglik: -3.9696e+02 - logprior: 0.3785
Fitted a model with MAP estimate = -395.2459
expansions: []
discards: [83]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 405.3014 - loglik: -3.9932e+02 - logprior: -5.9780e+00
Epoch 2/10
21/21 - 9s - loss: 396.6213 - loglik: -3.9734e+02 - logprior: 0.7145
Epoch 3/10
21/21 - 9s - loss: 394.9187 - loglik: -3.9646e+02 - logprior: 1.5400
Epoch 4/10
21/21 - 9s - loss: 394.2073 - loglik: -3.9610e+02 - logprior: 1.8944
Epoch 5/10
21/21 - 9s - loss: 392.7133 - loglik: -3.9478e+02 - logprior: 2.0644
Epoch 6/10
21/21 - 9s - loss: 391.2759 - loglik: -3.9348e+02 - logprior: 2.2070
Epoch 7/10
21/21 - 9s - loss: 391.6725 - loglik: -3.9401e+02 - logprior: 2.3412
Fitted a model with MAP estimate = -390.9136
Time for alignment: 181.2578
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 603.9128 - loglik: -5.9664e+02 - logprior: -7.2743e+00
Epoch 2/10
21/21 - 6s - loss: 483.8470 - loglik: -4.8207e+02 - logprior: -1.7758e+00
Epoch 3/10
21/21 - 6s - loss: 449.4990 - loglik: -4.4714e+02 - logprior: -2.3578e+00
Epoch 4/10
21/21 - 6s - loss: 442.0142 - loglik: -4.3982e+02 - logprior: -2.1921e+00
Epoch 5/10
21/21 - 7s - loss: 436.9710 - loglik: -4.3477e+02 - logprior: -2.2057e+00
Epoch 6/10
21/21 - 7s - loss: 436.4631 - loglik: -4.3421e+02 - logprior: -2.2569e+00
Epoch 7/10
21/21 - 6s - loss: 433.9991 - loglik: -4.3174e+02 - logprior: -2.2548e+00
Epoch 8/10
21/21 - 7s - loss: 434.9512 - loglik: -4.3271e+02 - logprior: -2.2372e+00
Fitted a model with MAP estimate = -434.3283
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (49, 1), (59, 1), (76, 7), (78, 1), (97, 1), (100, 2), (102, 1), (119, 2), (120, 1), (121, 1), (122, 1), (147, 2), (148, 1), (150, 1), (155, 1), (161, 2), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1), (178, 1), (179, 1)]
discards: [  0 105]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 433.5268 - loglik: -4.2366e+02 - logprior: -9.8672e+00
Epoch 2/2
21/21 - 9s - loss: 407.3299 - loglik: -4.0415e+02 - logprior: -3.1844e+00
Fitted a model with MAP estimate = -404.2711
expansions: [(0, 5), (80, 3)]
discards: [  0  13  18  28  50 129 149 215]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 415.0423 - loglik: -4.0843e+02 - logprior: -6.6153e+00
Epoch 2/2
21/21 - 8s - loss: 400.1951 - loglik: -4.0002e+02 - logprior: -1.8009e-01
Fitted a model with MAP estimate = -399.1552
expansions: [(97, 1)]
discards: [ 1  2  3  4 82]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 409.5078 - loglik: -4.0336e+02 - logprior: -6.1433e+00
Epoch 2/10
21/21 - 9s - loss: 401.0796 - loglik: -4.0162e+02 - logprior: 0.5377
Epoch 3/10
21/21 - 9s - loss: 397.8829 - loglik: -3.9927e+02 - logprior: 1.3895
Epoch 4/10
21/21 - 9s - loss: 396.9420 - loglik: -3.9872e+02 - logprior: 1.7782
Epoch 5/10
21/21 - 8s - loss: 395.8301 - loglik: -3.9780e+02 - logprior: 1.9708
Epoch 6/10
21/21 - 8s - loss: 395.7954 - loglik: -3.9793e+02 - logprior: 2.1309
Epoch 7/10
21/21 - 9s - loss: 396.1082 - loglik: -3.9841e+02 - logprior: 2.2995
Fitted a model with MAP estimate = -394.7345
Time for alignment: 186.0975
Computed alignments with likelihoods: ['-383.4647', '-387.3264', '-391.7437', '-390.9136', '-394.7345']
Best model has likelihood: -383.4647  (prior= 2.3009 )
time for generating output: 0.3913
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5894370175108266
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.5466 - loglik: -2.8055e+02 - logprior: -2.9973e+00
Epoch 2/10
19/19 - 2s - loss: 253.6780 - loglik: -2.5282e+02 - logprior: -8.5407e-01
Epoch 3/10
19/19 - 2s - loss: 244.7854 - loglik: -2.4394e+02 - logprior: -8.4985e-01
Epoch 4/10
19/19 - 2s - loss: 241.8874 - loglik: -2.4113e+02 - logprior: -7.6213e-01
Epoch 5/10
19/19 - 2s - loss: 240.6613 - loglik: -2.3993e+02 - logprior: -7.3266e-01
Epoch 6/10
19/19 - 2s - loss: 239.6261 - loglik: -2.3891e+02 - logprior: -7.1851e-01
Epoch 7/10
19/19 - 2s - loss: 239.5655 - loglik: -2.3885e+02 - logprior: -7.1114e-01
Epoch 8/10
19/19 - 2s - loss: 238.9154 - loglik: -2.3820e+02 - logprior: -7.1246e-01
Epoch 9/10
19/19 - 2s - loss: 238.6241 - loglik: -2.3791e+02 - logprior: -7.1486e-01
Epoch 10/10
19/19 - 2s - loss: 238.4832 - loglik: -2.3777e+02 - logprior: -7.1672e-01
Fitted a model with MAP estimate = -237.5077
expansions: [(0, 10), (12, 1), (16, 1), (17, 1), (18, 2), (19, 2), (30, 2), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 247.6937 - loglik: -2.4390e+02 - logprior: -3.7934e+00
Epoch 2/2
19/19 - 3s - loss: 238.7404 - loglik: -2.3760e+02 - logprior: -1.1381e+00
Fitted a model with MAP estimate = -236.0304
expansions: [(0, 8), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 47 77 80 82]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 242.7425 - loglik: -2.3904e+02 - logprior: -3.7026e+00
Epoch 2/2
19/19 - 3s - loss: 237.8747 - loglik: -2.3671e+02 - logprior: -1.1645e+00
Fitted a model with MAP estimate = -235.8592
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  8 78]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 239.8003 - loglik: -2.3668e+02 - logprior: -3.1232e+00
Epoch 2/10
19/19 - 3s - loss: 236.5072 - loglik: -2.3562e+02 - logprior: -8.8346e-01
Epoch 3/10
19/19 - 3s - loss: 234.8549 - loglik: -2.3411e+02 - logprior: -7.4758e-01
Epoch 4/10
19/19 - 3s - loss: 234.3457 - loglik: -2.3367e+02 - logprior: -6.7249e-01
Epoch 5/10
19/19 - 3s - loss: 233.6161 - loglik: -2.3298e+02 - logprior: -6.3578e-01
Epoch 6/10
19/19 - 3s - loss: 232.3282 - loglik: -2.3171e+02 - logprior: -6.2071e-01
Epoch 7/10
19/19 - 3s - loss: 231.5597 - loglik: -2.3095e+02 - logprior: -6.0730e-01
Epoch 8/10
19/19 - 3s - loss: 231.3572 - loglik: -2.3077e+02 - logprior: -5.9076e-01
Epoch 9/10
19/19 - 3s - loss: 230.6057 - loglik: -2.3002e+02 - logprior: -5.8225e-01
Epoch 10/10
19/19 - 3s - loss: 231.3769 - loglik: -2.3080e+02 - logprior: -5.7284e-01
Fitted a model with MAP estimate = -230.6741
Time for alignment: 91.1148
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.3498 - loglik: -2.8134e+02 - logprior: -3.0061e+00
Epoch 2/10
19/19 - 2s - loss: 253.6870 - loglik: -2.5283e+02 - logprior: -8.5330e-01
Epoch 3/10
19/19 - 2s - loss: 244.0449 - loglik: -2.4321e+02 - logprior: -8.3928e-01
Epoch 4/10
19/19 - 2s - loss: 241.8153 - loglik: -2.4107e+02 - logprior: -7.4407e-01
Epoch 5/10
19/19 - 2s - loss: 240.9028 - loglik: -2.4018e+02 - logprior: -7.1853e-01
Epoch 6/10
19/19 - 2s - loss: 240.4169 - loglik: -2.3971e+02 - logprior: -7.0707e-01
Epoch 7/10
19/19 - 2s - loss: 239.4899 - loglik: -2.3879e+02 - logprior: -6.9884e-01
Epoch 8/10
19/19 - 2s - loss: 238.7374 - loglik: -2.3804e+02 - logprior: -7.0064e-01
Epoch 9/10
19/19 - 2s - loss: 239.2030 - loglik: -2.3851e+02 - logprior: -6.9572e-01
Fitted a model with MAP estimate = -237.9509
expansions: [(0, 10), (21, 3), (56, 1), (58, 3), (59, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 247.3363 - loglik: -2.4358e+02 - logprior: -3.7587e+00
Epoch 2/2
19/19 - 3s - loss: 239.5236 - loglik: -2.3843e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -236.9653
expansions: [(0, 8), (33, 1), (34, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 242.7756 - loglik: -2.3906e+02 - logprior: -3.7171e+00
Epoch 2/2
19/19 - 3s - loss: 238.1594 - loglik: -2.3696e+02 - logprior: -1.2025e+00
Fitted a model with MAP estimate = -236.4216
expansions: [(0, 7)]
discards: [0 1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.5034 - loglik: -2.3731e+02 - logprior: -3.1933e+00
Epoch 2/10
19/19 - 2s - loss: 236.8817 - loglik: -2.3595e+02 - logprior: -9.3338e-01
Epoch 3/10
19/19 - 3s - loss: 236.2233 - loglik: -2.3548e+02 - logprior: -7.3999e-01
Epoch 4/10
19/19 - 3s - loss: 235.2466 - loglik: -2.3459e+02 - logprior: -6.5436e-01
Epoch 5/10
19/19 - 3s - loss: 234.2286 - loglik: -2.3359e+02 - logprior: -6.3954e-01
Epoch 6/10
19/19 - 3s - loss: 233.3243 - loglik: -2.3270e+02 - logprior: -6.2828e-01
Epoch 7/10
19/19 - 2s - loss: 232.6185 - loglik: -2.3199e+02 - logprior: -6.2956e-01
Epoch 8/10
19/19 - 3s - loss: 232.8322 - loglik: -2.3220e+02 - logprior: -6.3556e-01
Fitted a model with MAP estimate = -232.2166
Time for alignment: 80.9898
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.7644 - loglik: -2.8077e+02 - logprior: -2.9940e+00
Epoch 2/10
19/19 - 2s - loss: 255.7915 - loglik: -2.5493e+02 - logprior: -8.6536e-01
Epoch 3/10
19/19 - 2s - loss: 244.5944 - loglik: -2.4372e+02 - logprior: -8.6951e-01
Epoch 4/10
19/19 - 2s - loss: 242.2565 - loglik: -2.4145e+02 - logprior: -8.0291e-01
Epoch 5/10
19/19 - 2s - loss: 240.6151 - loglik: -2.3983e+02 - logprior: -7.8383e-01
Epoch 6/10
19/19 - 2s - loss: 239.7623 - loglik: -2.3899e+02 - logprior: -7.7337e-01
Epoch 7/10
19/19 - 2s - loss: 239.2903 - loglik: -2.3853e+02 - logprior: -7.5586e-01
Epoch 8/10
19/19 - 2s - loss: 238.9454 - loglik: -2.3820e+02 - logprior: -7.4648e-01
Epoch 9/10
19/19 - 2s - loss: 238.3994 - loglik: -2.3765e+02 - logprior: -7.4453e-01
Epoch 10/10
19/19 - 2s - loss: 238.7577 - loglik: -2.3803e+02 - logprior: -7.2868e-01
Fitted a model with MAP estimate = -237.3609
expansions: [(0, 10), (17, 3), (20, 1), (23, 1), (42, 1), (54, 1), (57, 2), (59, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 248.0331 - loglik: -2.4423e+02 - logprior: -3.8029e+00
Epoch 2/2
19/19 - 3s - loss: 238.4886 - loglik: -2.3739e+02 - logprior: -1.1033e+00
Fitted a model with MAP estimate = -235.8176
expansions: [(0, 8), (27, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 75 82 83]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 242.5675 - loglik: -2.3887e+02 - logprior: -3.6935e+00
Epoch 2/2
19/19 - 3s - loss: 237.9618 - loglik: -2.3683e+02 - logprior: -1.1324e+00
Fitted a model with MAP estimate = -236.0725
expansions: [(0, 8), (28, 1)]
discards: [0 1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 239.8344 - loglik: -2.3674e+02 - logprior: -3.0898e+00
Epoch 2/10
19/19 - 3s - loss: 236.3168 - loglik: -2.3548e+02 - logprior: -8.4155e-01
Epoch 3/10
19/19 - 3s - loss: 234.9708 - loglik: -2.3426e+02 - logprior: -7.0856e-01
Epoch 4/10
19/19 - 3s - loss: 233.8468 - loglik: -2.3318e+02 - logprior: -6.6685e-01
Epoch 5/10
19/19 - 3s - loss: 233.3386 - loglik: -2.3270e+02 - logprior: -6.4175e-01
Epoch 6/10
19/19 - 3s - loss: 232.3647 - loglik: -2.3173e+02 - logprior: -6.3669e-01
Epoch 7/10
19/19 - 3s - loss: 232.0559 - loglik: -2.3144e+02 - logprior: -6.2048e-01
Epoch 8/10
19/19 - 3s - loss: 231.4902 - loglik: -2.3088e+02 - logprior: -6.1135e-01
Epoch 9/10
19/19 - 3s - loss: 231.2268 - loglik: -2.3062e+02 - logprior: -6.0340e-01
Epoch 10/10
19/19 - 3s - loss: 231.4674 - loglik: -2.3087e+02 - logprior: -5.9243e-01
Fitted a model with MAP estimate = -230.9851
Time for alignment: 89.0107
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.4732 - loglik: -2.8146e+02 - logprior: -3.0091e+00
Epoch 2/10
19/19 - 2s - loss: 254.6148 - loglik: -2.5375e+02 - logprior: -8.6263e-01
Epoch 3/10
19/19 - 2s - loss: 244.6312 - loglik: -2.4376e+02 - logprior: -8.7029e-01
Epoch 4/10
19/19 - 2s - loss: 241.8693 - loglik: -2.4109e+02 - logprior: -7.8328e-01
Epoch 5/10
19/19 - 2s - loss: 240.8941 - loglik: -2.4013e+02 - logprior: -7.6020e-01
Epoch 6/10
19/19 - 2s - loss: 240.2094 - loglik: -2.3946e+02 - logprior: -7.4866e-01
Epoch 7/10
19/19 - 2s - loss: 239.3336 - loglik: -2.3860e+02 - logprior: -7.3651e-01
Epoch 8/10
19/19 - 2s - loss: 239.4953 - loglik: -2.3877e+02 - logprior: -7.3027e-01
Fitted a model with MAP estimate = -238.0579
expansions: [(0, 10), (21, 3), (23, 1), (35, 1), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 246.8621 - loglik: -2.4312e+02 - logprior: -3.7420e+00
Epoch 2/2
19/19 - 3s - loss: 239.0230 - loglik: -2.3791e+02 - logprior: -1.1143e+00
Fitted a model with MAP estimate = -236.7591
expansions: [(0, 8), (32, 1), (33, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 79]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 242.6136 - loglik: -2.3888e+02 - logprior: -3.7320e+00
Epoch 2/2
19/19 - 3s - loss: 238.1989 - loglik: -2.3697e+02 - logprior: -1.2249e+00
Fitted a model with MAP estimate = -236.1781
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  5  6  7  8 75 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.6414 - loglik: -2.3744e+02 - logprior: -3.1964e+00
Epoch 2/10
19/19 - 3s - loss: 236.9230 - loglik: -2.3598e+02 - logprior: -9.3978e-01
Epoch 3/10
19/19 - 3s - loss: 236.2713 - loglik: -2.3554e+02 - logprior: -7.3414e-01
Epoch 4/10
19/19 - 3s - loss: 234.9881 - loglik: -2.3432e+02 - logprior: -6.7217e-01
Epoch 5/10
19/19 - 3s - loss: 233.9003 - loglik: -2.3325e+02 - logprior: -6.4780e-01
Epoch 6/10
19/19 - 3s - loss: 233.6409 - loglik: -2.3300e+02 - logprior: -6.4052e-01
Epoch 7/10
19/19 - 3s - loss: 232.6462 - loglik: -2.3202e+02 - logprior: -6.2889e-01
Epoch 8/10
19/19 - 3s - loss: 232.0475 - loglik: -2.3143e+02 - logprior: -6.2045e-01
Epoch 9/10
19/19 - 3s - loss: 232.3044 - loglik: -2.3171e+02 - logprior: -5.9736e-01
Fitted a model with MAP estimate = -231.8813
Time for alignment: 80.7103
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.8197 - loglik: -2.8081e+02 - logprior: -3.0049e+00
Epoch 2/10
19/19 - 2s - loss: 254.3078 - loglik: -2.5344e+02 - logprior: -8.6487e-01
Epoch 3/10
19/19 - 2s - loss: 245.3417 - loglik: -2.4448e+02 - logprior: -8.5849e-01
Epoch 4/10
19/19 - 2s - loss: 242.4463 - loglik: -2.4168e+02 - logprior: -7.6141e-01
Epoch 5/10
19/19 - 2s - loss: 241.1359 - loglik: -2.4039e+02 - logprior: -7.4971e-01
Epoch 6/10
19/19 - 2s - loss: 240.7386 - loglik: -2.4000e+02 - logprior: -7.4110e-01
Epoch 7/10
19/19 - 2s - loss: 239.3547 - loglik: -2.3862e+02 - logprior: -7.3773e-01
Epoch 8/10
19/19 - 2s - loss: 239.5624 - loglik: -2.3882e+02 - logprior: -7.3970e-01
Fitted a model with MAP estimate = -238.3242
expansions: [(0, 10), (13, 2), (14, 3), (15, 2), (36, 1), (57, 2), (58, 2), (59, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 246.4627 - loglik: -2.4272e+02 - logprior: -3.7437e+00
Epoch 2/2
19/19 - 3s - loss: 238.9150 - loglik: -2.3780e+02 - logprior: -1.1121e+00
Fitted a model with MAP estimate = -236.5380
expansions: [(0, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 15 78 79 83]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 243.1603 - loglik: -2.3953e+02 - logprior: -3.6253e+00
Epoch 2/2
19/19 - 3s - loss: 239.0876 - loglik: -2.3792e+02 - logprior: -1.1671e+00
Fitted a model with MAP estimate = -237.3870
expansions: [(0, 9)]
discards: [0 1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.9577 - loglik: -2.3777e+02 - logprior: -3.1910e+00
Epoch 2/10
19/19 - 3s - loss: 237.1257 - loglik: -2.3619e+02 - logprior: -9.3838e-01
Epoch 3/10
19/19 - 3s - loss: 236.3807 - loglik: -2.3563e+02 - logprior: -7.4941e-01
Epoch 4/10
19/19 - 3s - loss: 235.2725 - loglik: -2.3458e+02 - logprior: -6.8870e-01
Epoch 5/10
19/19 - 3s - loss: 234.3785 - loglik: -2.3372e+02 - logprior: -6.5866e-01
Epoch 6/10
19/19 - 3s - loss: 233.7655 - loglik: -2.3313e+02 - logprior: -6.3806e-01
Epoch 7/10
19/19 - 3s - loss: 233.4656 - loglik: -2.3285e+02 - logprior: -6.2048e-01
Epoch 8/10
19/19 - 3s - loss: 232.6496 - loglik: -2.3205e+02 - logprior: -6.0434e-01
Epoch 9/10
19/19 - 3s - loss: 232.4763 - loglik: -2.3188e+02 - logprior: -5.9968e-01
Epoch 10/10
19/19 - 3s - loss: 232.2941 - loglik: -2.3171e+02 - logprior: -5.8496e-01
Fitted a model with MAP estimate = -232.1677
Time for alignment: 83.0662
Computed alignments with likelihoods: ['-230.6741', '-232.2166', '-230.9851', '-231.8813', '-232.1677']
Best model has likelihood: -230.6741  (prior= -0.5636 )
time for generating output: 0.1913
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.6837368421052632
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 732.9218 - loglik: -7.2106e+02 - logprior: -1.1857e+01
Epoch 2/10
17/17 - 7s - loss: 554.1375 - loglik: -5.5239e+02 - logprior: -1.7507e+00
Epoch 3/10
17/17 - 6s - loss: 459.6923 - loglik: -4.5647e+02 - logprior: -3.2253e+00
Epoch 4/10
17/17 - 7s - loss: 436.3476 - loglik: -4.3261e+02 - logprior: -3.7385e+00
Epoch 5/10
17/17 - 7s - loss: 432.0558 - loglik: -4.2857e+02 - logprior: -3.4889e+00
Epoch 6/10
17/17 - 7s - loss: 428.3328 - loglik: -4.2511e+02 - logprior: -3.2219e+00
Epoch 7/10
17/17 - 7s - loss: 426.5053 - loglik: -4.2331e+02 - logprior: -3.1912e+00
Epoch 8/10
17/17 - 7s - loss: 429.7425 - loglik: -4.2655e+02 - logprior: -3.1949e+00
Fitted a model with MAP estimate = -427.0193
expansions: [(9, 1), (12, 1), (14, 3), (15, 2), (24, 1), (26, 1), (32, 1), (46, 1), (47, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (110, 1), (114, 1), (115, 2), (116, 1), (117, 1), (129, 2), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (161, 1), (162, 1), (176, 1), (179, 1), (180, 1), (181, 1), (188, 1), (190, 1), (191, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 430.2468 - loglik: -4.1453e+02 - logprior: -1.5719e+01
Epoch 2/2
17/17 - 9s - loss: 392.4409 - loglik: -3.8788e+02 - logprior: -4.5596e+00
Fitted a model with MAP estimate = -387.6640
expansions: [(0, 9)]
discards: [  0  16  19 139 157]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.3474 - loglik: -3.8760e+02 - logprior: -1.0745e+01
Epoch 2/2
17/17 - 9s - loss: 382.0112 - loglik: -3.8251e+02 - logprior: 0.4991
Fitted a model with MAP estimate = -379.7410
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 394.8057 - loglik: -3.8493e+02 - logprior: -9.8775e+00
Epoch 2/10
17/17 - 8s - loss: 380.7292 - loglik: -3.8200e+02 - logprior: 1.2699
Epoch 3/10
17/17 - 8s - loss: 377.5760 - loglik: -3.8053e+02 - logprior: 2.9571
Epoch 4/10
17/17 - 9s - loss: 378.3396 - loglik: -3.8210e+02 - logprior: 3.7643
Fitted a model with MAP estimate = -374.8448
Time for alignment: 161.7083
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 731.2095 - loglik: -7.1934e+02 - logprior: -1.1870e+01
Epoch 2/10
17/17 - 7s - loss: 552.0082 - loglik: -5.5031e+02 - logprior: -1.6949e+00
Epoch 3/10
17/17 - 7s - loss: 456.9768 - loglik: -4.5390e+02 - logprior: -3.0738e+00
Epoch 4/10
17/17 - 6s - loss: 439.0045 - loglik: -4.3571e+02 - logprior: -3.2898e+00
Epoch 5/10
17/17 - 6s - loss: 432.9949 - loglik: -4.3004e+02 - logprior: -2.9521e+00
Epoch 6/10
17/17 - 7s - loss: 432.3960 - loglik: -4.2959e+02 - logprior: -2.8062e+00
Epoch 7/10
17/17 - 7s - loss: 431.1340 - loglik: -4.2836e+02 - logprior: -2.7784e+00
Epoch 8/10
17/17 - 6s - loss: 430.5356 - loglik: -4.2776e+02 - logprior: -2.7765e+00
Epoch 9/10
17/17 - 7s - loss: 430.3441 - loglik: -4.2759e+02 - logprior: -2.7573e+00
Epoch 10/10
17/17 - 6s - loss: 428.0682 - loglik: -4.2534e+02 - logprior: -2.7261e+00
Fitted a model with MAP estimate = -429.2365
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 3), (43, 1), (55, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (71, 1), (86, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 2), (116, 1), (130, 3), (131, 2), (150, 1), (151, 1), (160, 2), (161, 1), (162, 1), (179, 1), (180, 1), (181, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 429.0960 - loglik: -4.1335e+02 - logprior: -1.5745e+01
Epoch 2/2
17/17 - 8s - loss: 392.6626 - loglik: -3.8813e+02 - logprior: -4.5312e+00
Fitted a model with MAP estimate = -387.9150
expansions: [(0, 9)]
discards: [  0  39 138 157 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.4367 - loglik: -3.8773e+02 - logprior: -1.0707e+01
Epoch 2/2
17/17 - 9s - loss: 382.0247 - loglik: -3.8254e+02 - logprior: 0.5183
Fitted a model with MAP estimate = -379.7504
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 395.1155 - loglik: -3.8532e+02 - logprior: -9.7983e+00
Epoch 2/10
17/17 - 9s - loss: 381.1827 - loglik: -3.8251e+02 - logprior: 1.3320
Epoch 3/10
17/17 - 9s - loss: 377.9549 - loglik: -3.8101e+02 - logprior: 3.0531
Epoch 4/10
17/17 - 8s - loss: 377.1548 - loglik: -3.8099e+02 - logprior: 3.8347
Epoch 5/10
17/17 - 8s - loss: 374.2600 - loglik: -3.7840e+02 - logprior: 4.1445
Epoch 6/10
17/17 - 9s - loss: 373.1065 - loglik: -3.7747e+02 - logprior: 4.3668
Epoch 7/10
17/17 - 9s - loss: 373.6458 - loglik: -3.7828e+02 - logprior: 4.6297
Fitted a model with MAP estimate = -372.2580
Time for alignment: 199.6555
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 731.1373 - loglik: -7.1927e+02 - logprior: -1.1868e+01
Epoch 2/10
17/17 - 7s - loss: 554.2438 - loglik: -5.5254e+02 - logprior: -1.7053e+00
Epoch 3/10
17/17 - 6s - loss: 457.0262 - loglik: -4.5375e+02 - logprior: -3.2812e+00
Epoch 4/10
17/17 - 7s - loss: 437.0376 - loglik: -4.3342e+02 - logprior: -3.6219e+00
Epoch 5/10
17/17 - 7s - loss: 432.6953 - loglik: -4.2938e+02 - logprior: -3.3152e+00
Epoch 6/10
17/17 - 6s - loss: 431.0572 - loglik: -4.2798e+02 - logprior: -3.0788e+00
Epoch 7/10
17/17 - 7s - loss: 429.3140 - loglik: -4.2625e+02 - logprior: -3.0680e+00
Epoch 8/10
17/17 - 6s - loss: 428.5375 - loglik: -4.2547e+02 - logprior: -3.0659e+00
Epoch 9/10
17/17 - 7s - loss: 427.0797 - loglik: -4.2403e+02 - logprior: -3.0469e+00
Epoch 10/10
17/17 - 6s - loss: 427.1300 - loglik: -4.2412e+02 - logprior: -3.0084e+00
Fitted a model with MAP estimate = -427.4084
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (53, 2), (55, 1), (57, 1), (59, 1), (60, 1), (62, 1), (85, 1), (86, 1), (94, 2), (97, 1), (109, 1), (113, 1), (114, 2), (115, 1), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (165, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 427.7196 - loglik: -4.1206e+02 - logprior: -1.5663e+01
Epoch 2/2
17/17 - 9s - loss: 391.1541 - loglik: -3.8676e+02 - logprior: -4.3983e+00
Fitted a model with MAP estimate = -387.1899
expansions: [(0, 9)]
discards: [  0 137 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 397.4283 - loglik: -3.8669e+02 - logprior: -1.0735e+01
Epoch 2/2
17/17 - 9s - loss: 382.1116 - loglik: -3.8260e+02 - logprior: 0.4856
Fitted a model with MAP estimate = -379.1603
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 394.4807 - loglik: -3.8463e+02 - logprior: -9.8497e+00
Epoch 2/10
17/17 - 8s - loss: 382.4119 - loglik: -3.8373e+02 - logprior: 1.3170
Epoch 3/10
17/17 - 9s - loss: 376.6502 - loglik: -3.7968e+02 - logprior: 3.0279
Epoch 4/10
17/17 - 9s - loss: 375.6292 - loglik: -3.7945e+02 - logprior: 3.8189
Epoch 5/10
17/17 - 9s - loss: 374.4031 - loglik: -3.7858e+02 - logprior: 4.1781
Epoch 6/10
17/17 - 8s - loss: 372.5141 - loglik: -3.7686e+02 - logprior: 4.3476
Epoch 7/10
17/17 - 9s - loss: 370.4090 - loglik: -3.7504e+02 - logprior: 4.6285
Epoch 8/10
17/17 - 8s - loss: 371.7917 - loglik: -3.7667e+02 - logprior: 4.8775
Fitted a model with MAP estimate = -371.1571
Time for alignment: 209.7819
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 731.5985 - loglik: -7.1973e+02 - logprior: -1.1871e+01
Epoch 2/10
17/17 - 7s - loss: 558.4894 - loglik: -5.5676e+02 - logprior: -1.7321e+00
Epoch 3/10
17/17 - 7s - loss: 459.9962 - loglik: -4.5687e+02 - logprior: -3.1241e+00
Epoch 4/10
17/17 - 6s - loss: 439.8413 - loglik: -4.3630e+02 - logprior: -3.5377e+00
Epoch 5/10
17/17 - 7s - loss: 433.7905 - loglik: -4.3043e+02 - logprior: -3.3574e+00
Epoch 6/10
17/17 - 7s - loss: 432.7812 - loglik: -4.2958e+02 - logprior: -3.2048e+00
Epoch 7/10
17/17 - 7s - loss: 430.8087 - loglik: -4.2764e+02 - logprior: -3.1668e+00
Epoch 8/10
17/17 - 6s - loss: 428.6749 - loglik: -4.2551e+02 - logprior: -3.1647e+00
Epoch 9/10
17/17 - 6s - loss: 430.5509 - loglik: -4.2741e+02 - logprior: -3.1416e+00
Fitted a model with MAP estimate = -429.5604
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (23, 1), (24, 1), (28, 1), (33, 1), (46, 1), (55, 1), (56, 1), (57, 1), (58, 1), (60, 1), (61, 1), (63, 1), (71, 1), (86, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (162, 1), (163, 1), (176, 1), (179, 1), (180, 1), (181, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 429.4383 - loglik: -4.1387e+02 - logprior: -1.5566e+01
Epoch 2/2
17/17 - 8s - loss: 392.6447 - loglik: -3.8837e+02 - logprior: -4.2785e+00
Fitted a model with MAP estimate = -387.4134
expansions: [(0, 9)]
discards: [  0 140 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.0478 - loglik: -3.8733e+02 - logprior: -1.0722e+01
Epoch 2/2
17/17 - 9s - loss: 381.2643 - loglik: -3.8175e+02 - logprior: 0.4904
Fitted a model with MAP estimate = -379.5283
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 394.6457 - loglik: -3.8479e+02 - logprior: -9.8547e+00
Epoch 2/10
17/17 - 9s - loss: 381.4140 - loglik: -3.8271e+02 - logprior: 1.2926
Epoch 3/10
17/17 - 9s - loss: 378.7309 - loglik: -3.8175e+02 - logprior: 3.0186
Epoch 4/10
17/17 - 8s - loss: 375.1985 - loglik: -3.7900e+02 - logprior: 3.7977
Epoch 5/10
17/17 - 9s - loss: 373.4177 - loglik: -3.7758e+02 - logprior: 4.1644
Epoch 6/10
17/17 - 9s - loss: 375.0898 - loglik: -3.7945e+02 - logprior: 4.3567
Fitted a model with MAP estimate = -372.3997
Time for alignment: 183.4931
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 731.0865 - loglik: -7.1921e+02 - logprior: -1.1873e+01
Epoch 2/10
17/17 - 7s - loss: 548.3723 - loglik: -5.4673e+02 - logprior: -1.6466e+00
Epoch 3/10
17/17 - 6s - loss: 454.5127 - loglik: -4.5166e+02 - logprior: -2.8530e+00
Epoch 4/10
17/17 - 7s - loss: 437.6251 - loglik: -4.3453e+02 - logprior: -3.0990e+00
Epoch 5/10
17/17 - 6s - loss: 433.1584 - loglik: -4.3039e+02 - logprior: -2.7689e+00
Epoch 6/10
17/17 - 6s - loss: 431.1392 - loglik: -4.2853e+02 - logprior: -2.6135e+00
Epoch 7/10
17/17 - 7s - loss: 431.4378 - loglik: -4.2881e+02 - logprior: -2.6257e+00
Fitted a model with MAP estimate = -429.5271
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (87, 3), (95, 2), (96, 1), (97, 1), (114, 2), (115, 3), (129, 3), (130, 2), (141, 1), (155, 1), (160, 1), (162, 1), (163, 1), (176, 1), (179, 1), (181, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 429.6306 - loglik: -4.1391e+02 - logprior: -1.5721e+01
Epoch 2/2
17/17 - 9s - loss: 392.2452 - loglik: -3.8770e+02 - logprior: -4.5494e+00
Fitted a model with MAP estimate = -387.3879
expansions: [(0, 9)]
discards: [  0 103 136 157 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 399.1097 - loglik: -3.8834e+02 - logprior: -1.0771e+01
Epoch 2/2
17/17 - 9s - loss: 382.5900 - loglik: -3.8300e+02 - logprior: 0.4061
Fitted a model with MAP estimate = -379.2684
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 395.3459 - loglik: -3.8541e+02 - logprior: -9.9387e+00
Epoch 2/10
17/17 - 8s - loss: 380.7190 - loglik: -3.8195e+02 - logprior: 1.2327
Epoch 3/10
17/17 - 9s - loss: 378.0936 - loglik: -3.8101e+02 - logprior: 2.9180
Epoch 4/10
17/17 - 8s - loss: 375.7458 - loglik: -3.7944e+02 - logprior: 3.6903
Epoch 5/10
17/17 - 9s - loss: 373.6761 - loglik: -3.7774e+02 - logprior: 4.0616
Epoch 6/10
17/17 - 9s - loss: 373.2592 - loglik: -3.7751e+02 - logprior: 4.2541
Epoch 7/10
17/17 - 8s - loss: 372.8298 - loglik: -3.7734e+02 - logprior: 4.5150
Epoch 8/10
17/17 - 9s - loss: 371.2502 - loglik: -3.7602e+02 - logprior: 4.7712
Epoch 9/10
17/17 - 8s - loss: 373.3273 - loglik: -3.7839e+02 - logprior: 5.0662
Fitted a model with MAP estimate = -371.1165
Time for alignment: 196.2644
Computed alignments with likelihoods: ['-374.8448', '-372.2580', '-371.1571', '-372.3997', '-371.1165']
Best model has likelihood: -371.1165  (prior= 5.2185 )
time for generating output: 0.2786
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9544235924932976
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.4001 - loglik: -1.5938e+02 - logprior: -2.0017e+01
Epoch 2/10
10/10 - 0s - loss: 148.3506 - loglik: -1.4263e+02 - logprior: -5.7190e+00
Epoch 3/10
10/10 - 1s - loss: 130.8638 - loglik: -1.2763e+02 - logprior: -3.2348e+00
Epoch 4/10
10/10 - 1s - loss: 119.9354 - loglik: -1.1732e+02 - logprior: -2.6143e+00
Epoch 5/10
10/10 - 1s - loss: 115.7068 - loglik: -1.1328e+02 - logprior: -2.4314e+00
Epoch 6/10
10/10 - 0s - loss: 114.3757 - loglik: -1.1208e+02 - logprior: -2.2918e+00
Epoch 7/10
10/10 - 0s - loss: 113.7149 - loglik: -1.1156e+02 - logprior: -2.1507e+00
Epoch 8/10
10/10 - 1s - loss: 113.1660 - loglik: -1.1112e+02 - logprior: -2.0416e+00
Epoch 9/10
10/10 - 0s - loss: 113.1267 - loglik: -1.1113e+02 - logprior: -1.9925e+00
Epoch 10/10
10/10 - 1s - loss: 112.7984 - loglik: -1.1082e+02 - logprior: -1.9760e+00
Fitted a model with MAP estimate = -112.8203
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.4648 - loglik: -1.1305e+02 - logprior: -2.2411e+01
Epoch 2/2
10/10 - 1s - loss: 116.6027 - loglik: -1.0688e+02 - logprior: -9.7256e+00
Fitted a model with MAP estimate = -112.8301
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 122.2155 - loglik: -1.0441e+02 - logprior: -1.7806e+01
Epoch 2/2
10/10 - 1s - loss: 108.2340 - loglik: -1.0336e+02 - logprior: -4.8709e+00
Fitted a model with MAP estimate = -106.2245
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.9503 - loglik: -1.0469e+02 - logprior: -2.0257e+01
Epoch 2/10
10/10 - 1s - loss: 109.7400 - loglik: -1.0383e+02 - logprior: -5.9126e+00
Epoch 3/10
10/10 - 1s - loss: 106.6901 - loglik: -1.0353e+02 - logprior: -3.1577e+00
Epoch 4/10
10/10 - 1s - loss: 105.3262 - loglik: -1.0328e+02 - logprior: -2.0441e+00
Epoch 5/10
10/10 - 0s - loss: 104.7884 - loglik: -1.0335e+02 - logprior: -1.4342e+00
Epoch 6/10
10/10 - 1s - loss: 104.5396 - loglik: -1.0334e+02 - logprior: -1.2000e+00
Epoch 7/10
10/10 - 1s - loss: 104.2198 - loglik: -1.0323e+02 - logprior: -9.8710e-01
Epoch 8/10
10/10 - 1s - loss: 104.4020 - loglik: -1.0350e+02 - logprior: -9.0305e-01
Fitted a model with MAP estimate = -104.1625
Time for alignment: 28.5628
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.3900 - loglik: -1.5937e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 148.1875 - loglik: -1.4246e+02 - logprior: -5.7258e+00
Epoch 3/10
10/10 - 1s - loss: 130.2460 - loglik: -1.2693e+02 - logprior: -3.3126e+00
Epoch 4/10
10/10 - 1s - loss: 118.6144 - loglik: -1.1580e+02 - logprior: -2.8129e+00
Epoch 5/10
10/10 - 1s - loss: 113.8401 - loglik: -1.1111e+02 - logprior: -2.7330e+00
Epoch 6/10
10/10 - 1s - loss: 112.1083 - loglik: -1.0946e+02 - logprior: -2.6514e+00
Epoch 7/10
10/10 - 1s - loss: 111.3877 - loglik: -1.0887e+02 - logprior: -2.5159e+00
Epoch 8/10
10/10 - 1s - loss: 111.2823 - loglik: -1.0891e+02 - logprior: -2.3726e+00
Epoch 9/10
10/10 - 1s - loss: 110.9397 - loglik: -1.0866e+02 - logprior: -2.2782e+00
Epoch 10/10
10/10 - 1s - loss: 110.5881 - loglik: -1.0833e+02 - logprior: -2.2548e+00
Fitted a model with MAP estimate = -110.7288
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.2863 - loglik: -1.1286e+02 - logprior: -2.2425e+01
Epoch 2/2
10/10 - 1s - loss: 115.7528 - loglik: -1.0604e+02 - logprior: -9.7128e+00
Fitted a model with MAP estimate = -112.5121
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.4821 - loglik: -1.0370e+02 - logprior: -1.7779e+01
Epoch 2/2
10/10 - 1s - loss: 107.8550 - loglik: -1.0299e+02 - logprior: -4.8664e+00
Fitted a model with MAP estimate = -106.1119
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.7983 - loglik: -1.0454e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 109.9565 - loglik: -1.0404e+02 - logprior: -5.9138e+00
Epoch 3/10
10/10 - 1s - loss: 106.6433 - loglik: -1.0349e+02 - logprior: -3.1560e+00
Epoch 4/10
10/10 - 0s - loss: 105.2325 - loglik: -1.0319e+02 - logprior: -2.0434e+00
Epoch 5/10
10/10 - 1s - loss: 104.9137 - loglik: -1.0349e+02 - logprior: -1.4284e+00
Epoch 6/10
10/10 - 0s - loss: 104.2141 - loglik: -1.0302e+02 - logprior: -1.1942e+00
Epoch 7/10
10/10 - 1s - loss: 104.5497 - loglik: -1.0357e+02 - logprior: -9.8034e-01
Fitted a model with MAP estimate = -104.2389
Time for alignment: 27.4429
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.4507 - loglik: -1.5943e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 148.1140 - loglik: -1.4239e+02 - logprior: -5.7260e+00
Epoch 3/10
10/10 - 1s - loss: 131.0934 - loglik: -1.2783e+02 - logprior: -3.2670e+00
Epoch 4/10
10/10 - 1s - loss: 120.2418 - loglik: -1.1757e+02 - logprior: -2.6708e+00
Epoch 5/10
10/10 - 1s - loss: 114.5226 - loglik: -1.1193e+02 - logprior: -2.5965e+00
Epoch 6/10
10/10 - 1s - loss: 111.9642 - loglik: -1.0939e+02 - logprior: -2.5761e+00
Epoch 7/10
10/10 - 1s - loss: 111.5767 - loglik: -1.0907e+02 - logprior: -2.5075e+00
Epoch 8/10
10/10 - 1s - loss: 110.7497 - loglik: -1.0836e+02 - logprior: -2.3900e+00
Epoch 9/10
10/10 - 1s - loss: 110.5441 - loglik: -1.0824e+02 - logprior: -2.2995e+00
Epoch 10/10
10/10 - 1s - loss: 110.4501 - loglik: -1.0819e+02 - logprior: -2.2615e+00
Fitted a model with MAP estimate = -110.4151
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 134.8497 - loglik: -1.1242e+02 - logprior: -2.2433e+01
Epoch 2/2
10/10 - 1s - loss: 115.5941 - loglik: -1.0589e+02 - logprior: -9.7008e+00
Fitted a model with MAP estimate = -112.4544
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.6140 - loglik: -1.0384e+02 - logprior: -1.7777e+01
Epoch 2/2
10/10 - 1s - loss: 107.8723 - loglik: -1.0301e+02 - logprior: -4.8595e+00
Fitted a model with MAP estimate = -106.1340
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.7162 - loglik: -1.0447e+02 - logprior: -2.0246e+01
Epoch 2/10
10/10 - 1s - loss: 110.1030 - loglik: -1.0420e+02 - logprior: -5.9029e+00
Epoch 3/10
10/10 - 1s - loss: 106.4931 - loglik: -1.0334e+02 - logprior: -3.1570e+00
Epoch 4/10
10/10 - 1s - loss: 105.2710 - loglik: -1.0322e+02 - logprior: -2.0462e+00
Epoch 5/10
10/10 - 1s - loss: 104.9511 - loglik: -1.0353e+02 - logprior: -1.4178e+00
Epoch 6/10
10/10 - 1s - loss: 104.2828 - loglik: -1.0309e+02 - logprior: -1.1895e+00
Epoch 7/10
10/10 - 1s - loss: 104.5001 - loglik: -1.0352e+02 - logprior: -9.7705e-01
Fitted a model with MAP estimate = -104.2513
Time for alignment: 28.3381
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.5216 - loglik: -1.5950e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 148.2762 - loglik: -1.4255e+02 - logprior: -5.7225e+00
Epoch 3/10
10/10 - 1s - loss: 131.0317 - loglik: -1.2774e+02 - logprior: -3.2940e+00
Epoch 4/10
10/10 - 0s - loss: 119.5422 - loglik: -1.1676e+02 - logprior: -2.7809e+00
Epoch 5/10
10/10 - 1s - loss: 113.9051 - loglik: -1.1120e+02 - logprior: -2.7068e+00
Epoch 6/10
10/10 - 0s - loss: 112.6576 - loglik: -1.1000e+02 - logprior: -2.6563e+00
Epoch 7/10
10/10 - 0s - loss: 111.5522 - loglik: -1.0902e+02 - logprior: -2.5274e+00
Epoch 8/10
10/10 - 0s - loss: 111.0589 - loglik: -1.0868e+02 - logprior: -2.3759e+00
Epoch 9/10
10/10 - 1s - loss: 111.1952 - loglik: -1.0891e+02 - logprior: -2.2859e+00
Fitted a model with MAP estimate = -110.8912
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 135.3074 - loglik: -1.1288e+02 - logprior: -2.2425e+01
Epoch 2/2
10/10 - 1s - loss: 115.8167 - loglik: -1.0612e+02 - logprior: -9.6975e+00
Fitted a model with MAP estimate = -112.4823
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 121.6663 - loglik: -1.0389e+02 - logprior: -1.7778e+01
Epoch 2/2
10/10 - 1s - loss: 107.8720 - loglik: -1.0300e+02 - logprior: -4.8684e+00
Fitted a model with MAP estimate = -106.1196
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 125.0321 - loglik: -1.0479e+02 - logprior: -2.0239e+01
Epoch 2/10
10/10 - 1s - loss: 109.7142 - loglik: -1.0381e+02 - logprior: -5.9035e+00
Epoch 3/10
10/10 - 1s - loss: 106.7599 - loglik: -1.0360e+02 - logprior: -3.1604e+00
Epoch 4/10
10/10 - 1s - loss: 105.0379 - loglik: -1.0298e+02 - logprior: -2.0537e+00
Epoch 5/10
10/10 - 1s - loss: 104.7002 - loglik: -1.0328e+02 - logprior: -1.4219e+00
Epoch 6/10
10/10 - 1s - loss: 104.7191 - loglik: -1.0352e+02 - logprior: -1.1963e+00
Fitted a model with MAP estimate = -104.3536
Time for alignment: 26.1105
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.5105 - loglik: -1.5949e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 148.0390 - loglik: -1.4232e+02 - logprior: -5.7201e+00
Epoch 3/10
10/10 - 1s - loss: 130.6178 - loglik: -1.2738e+02 - logprior: -3.2410e+00
Epoch 4/10
10/10 - 0s - loss: 119.7946 - loglik: -1.1718e+02 - logprior: -2.6133e+00
Epoch 5/10
10/10 - 1s - loss: 115.6675 - loglik: -1.1324e+02 - logprior: -2.4301e+00
Epoch 6/10
10/10 - 1s - loss: 114.1649 - loglik: -1.1187e+02 - logprior: -2.2972e+00
Epoch 7/10
10/10 - 1s - loss: 113.5831 - loglik: -1.1141e+02 - logprior: -2.1749e+00
Epoch 8/10
10/10 - 1s - loss: 113.0834 - loglik: -1.1101e+02 - logprior: -2.0716e+00
Epoch 9/10
10/10 - 1s - loss: 112.8535 - loglik: -1.1085e+02 - logprior: -2.0016e+00
Epoch 10/10
10/10 - 1s - loss: 112.4684 - loglik: -1.1049e+02 - logprior: -1.9734e+00
Fitted a model with MAP estimate = -112.5633
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 135.2526 - loglik: -1.1284e+02 - logprior: -2.2414e+01
Epoch 2/2
10/10 - 1s - loss: 116.7262 - loglik: -1.0700e+02 - logprior: -9.7308e+00
Fitted a model with MAP estimate = -112.8179
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 122.2719 - loglik: -1.0446e+02 - logprior: -1.7807e+01
Epoch 2/2
10/10 - 1s - loss: 108.1980 - loglik: -1.0333e+02 - logprior: -4.8650e+00
Fitted a model with MAP estimate = -106.2632
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.9937 - loglik: -1.0475e+02 - logprior: -2.0240e+01
Epoch 2/10
10/10 - 1s - loss: 109.9622 - loglik: -1.0406e+02 - logprior: -5.9000e+00
Epoch 3/10
10/10 - 1s - loss: 106.5051 - loglik: -1.0335e+02 - logprior: -3.1514e+00
Epoch 4/10
10/10 - 1s - loss: 105.3213 - loglik: -1.0328e+02 - logprior: -2.0372e+00
Epoch 5/10
10/10 - 1s - loss: 104.9092 - loglik: -1.0348e+02 - logprior: -1.4272e+00
Epoch 6/10
10/10 - 1s - loss: 104.2758 - loglik: -1.0308e+02 - logprior: -1.1928e+00
Epoch 7/10
10/10 - 1s - loss: 104.4659 - loglik: -1.0348e+02 - logprior: -9.8222e-01
Fitted a model with MAP estimate = -104.2598
Time for alignment: 27.7757
Computed alignments with likelihoods: ['-104.1625', '-104.2389', '-104.2513', '-104.3536', '-104.2598']
Best model has likelihood: -104.1625  (prior= -0.8689 )
time for generating output: 0.0952
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9928712871287129
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 467.1448 - loglik: -4.6536e+02 - logprior: -1.7813e+00
Epoch 2/10
39/39 - 7s - loss: 384.7067 - loglik: -3.8361e+02 - logprior: -1.0959e+00
Epoch 3/10
39/39 - 7s - loss: 377.9155 - loglik: -3.7684e+02 - logprior: -1.0705e+00
Epoch 4/10
39/39 - 7s - loss: 375.9384 - loglik: -3.7488e+02 - logprior: -1.0578e+00
Epoch 5/10
39/39 - 7s - loss: 374.8122 - loglik: -3.7375e+02 - logprior: -1.0649e+00
Epoch 6/10
39/39 - 7s - loss: 373.7267 - loglik: -3.7267e+02 - logprior: -1.0520e+00
Epoch 7/10
39/39 - 7s - loss: 373.7741 - loglik: -3.7273e+02 - logprior: -1.0437e+00
Fitted a model with MAP estimate = -308.0220
expansions: [(0, 11), (10, 1), (11, 1), (18, 1), (28, 1), (29, 1), (30, 3), (37, 2), (44, 3), (71, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (102, 1), (107, 1), (112, 1), (126, 9), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 356.8490 - loglik: -3.5407e+02 - logprior: -2.7803e+00
Epoch 2/2
39/39 - 10s - loss: 341.7698 - loglik: -3.4025e+02 - logprior: -1.5225e+00
Fitted a model with MAP estimate = -284.0696
expansions: [(47, 2), (163, 1), (164, 3)]
discards: [  1   2   3   4   5   6   7   8   9  66 173]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 345.9009 - loglik: -3.4383e+02 - logprior: -2.0678e+00
Epoch 2/2
39/39 - 10s - loss: 340.5406 - loglik: -3.3961e+02 - logprior: -9.3066e-01
Fitted a model with MAP estimate = -283.8159
expansions: [(0, 15)]
discards: [ 38 157 158]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 281.7284 - loglik: -2.8008e+02 - logprior: -1.6440e+00
Epoch 2/10
52/52 - 12s - loss: 277.4564 - loglik: -2.7630e+02 - logprior: -1.1594e+00
Epoch 3/10
52/52 - 12s - loss: 271.1852 - loglik: -2.7001e+02 - logprior: -1.1763e+00
Epoch 4/10
52/52 - 12s - loss: 273.5334 - loglik: -2.7242e+02 - logprior: -1.1106e+00
Fitted a model with MAP estimate = -271.8987
Time for alignment: 212.0850
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 466.8162 - loglik: -4.6505e+02 - logprior: -1.7694e+00
Epoch 2/10
39/39 - 7s - loss: 386.9464 - loglik: -3.8590e+02 - logprior: -1.0473e+00
Epoch 3/10
39/39 - 7s - loss: 379.0170 - loglik: -3.7797e+02 - logprior: -1.0485e+00
Epoch 4/10
39/39 - 7s - loss: 376.8405 - loglik: -3.7581e+02 - logprior: -1.0281e+00
Epoch 5/10
39/39 - 7s - loss: 375.5921 - loglik: -3.7460e+02 - logprior: -9.9688e-01
Epoch 6/10
39/39 - 7s - loss: 374.8386 - loglik: -3.7384e+02 - logprior: -9.9478e-01
Epoch 7/10
39/39 - 7s - loss: 374.4281 - loglik: -3.7344e+02 - logprior: -9.9032e-01
Epoch 8/10
39/39 - 7s - loss: 374.2589 - loglik: -3.7326e+02 - logprior: -1.0018e+00
Epoch 9/10
39/39 - 8s - loss: 374.5769 - loglik: -3.7358e+02 - logprior: -9.9654e-01
Fitted a model with MAP estimate = -309.0489
expansions: [(0, 13), (10, 1), (15, 1), (18, 1), (28, 1), (30, 1), (34, 3), (38, 2), (44, 1), (45, 1), (69, 1), (72, 1), (89, 3), (91, 1), (103, 1), (107, 1), (108, 1), (127, 10), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 359.7125 - loglik: -3.5704e+02 - logprior: -2.6717e+00
Epoch 2/2
39/39 - 10s - loss: 344.1969 - loglik: -3.4284e+02 - logprior: -1.3579e+00
Fitted a model with MAP estimate = -285.4474
expansions: [(51, 2), (118, 1), (165, 2), (166, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 59]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 346.4775 - loglik: -3.4451e+02 - logprior: -1.9711e+00
Epoch 2/2
39/39 - 9s - loss: 340.8494 - loglik: -3.4001e+02 - logprior: -8.3626e-01
Fitted a model with MAP estimate = -284.5481
expansions: [(0, 15)]
discards: [157]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 283.3636 - loglik: -2.8177e+02 - logprior: -1.5894e+00
Epoch 2/10
52/52 - 12s - loss: 277.7810 - loglik: -2.7652e+02 - logprior: -1.2632e+00
Epoch 3/10
52/52 - 12s - loss: 274.1674 - loglik: -2.7294e+02 - logprior: -1.2244e+00
Epoch 4/10
52/52 - 13s - loss: 275.7833 - loglik: -2.7462e+02 - logprior: -1.1622e+00
Fitted a model with MAP estimate = -273.9445
Time for alignment: 228.8193
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 469.8406 - loglik: -4.6809e+02 - logprior: -1.7459e+00
Epoch 2/10
39/39 - 8s - loss: 391.7858 - loglik: -3.9075e+02 - logprior: -1.0369e+00
Epoch 3/10
39/39 - 7s - loss: 383.2889 - loglik: -3.8221e+02 - logprior: -1.0743e+00
Epoch 4/10
39/39 - 7s - loss: 380.9480 - loglik: -3.7999e+02 - logprior: -9.5421e-01
Epoch 5/10
39/39 - 7s - loss: 379.2888 - loglik: -3.7835e+02 - logprior: -9.3810e-01
Epoch 6/10
39/39 - 7s - loss: 378.9808 - loglik: -3.7806e+02 - logprior: -9.1853e-01
Epoch 7/10
39/39 - 7s - loss: 378.5836 - loglik: -3.7767e+02 - logprior: -9.1713e-01
Epoch 8/10
39/39 - 7s - loss: 378.1866 - loglik: -3.7727e+02 - logprior: -9.1262e-01
Epoch 9/10
39/39 - 7s - loss: 378.3260 - loglik: -3.7740e+02 - logprior: -9.2400e-01
Fitted a model with MAP estimate = -311.1415
expansions: [(0, 4), (10, 1), (11, 1), (18, 1), (19, 1), (27, 2), (29, 1), (33, 1), (37, 2), (43, 1), (44, 2), (45, 2), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (126, 8), (127, 2), (134, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 363.1661 - loglik: -3.6024e+02 - logprior: -2.9238e+00
Epoch 2/2
39/39 - 9s - loss: 347.8159 - loglik: -3.4650e+02 - logprior: -1.3188e+00
Fitted a model with MAP estimate = -286.0974
expansions: []
discards: [ 2  3 35 50 59 62]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 351.3624 - loglik: -3.4908e+02 - logprior: -2.2824e+00
Epoch 2/2
39/39 - 9s - loss: 347.2559 - loglik: -3.4611e+02 - logprior: -1.1419e+00
Fitted a model with MAP estimate = -287.6258
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 286.1402 - loglik: -2.8474e+02 - logprior: -1.3978e+00
Epoch 2/10
52/52 - 11s - loss: 284.7556 - loglik: -2.8411e+02 - logprior: -6.4820e-01
Epoch 3/10
52/52 - 11s - loss: 280.1839 - loglik: -2.7958e+02 - logprior: -6.0279e-01
Epoch 4/10
52/52 - 10s - loss: 280.5512 - loglik: -2.8001e+02 - logprior: -5.3740e-01
Fitted a model with MAP estimate = -279.5684
Time for alignment: 219.8340
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 468.3818 - loglik: -4.6663e+02 - logprior: -1.7520e+00
Epoch 2/10
39/39 - 7s - loss: 389.8026 - loglik: -3.8885e+02 - logprior: -9.5107e-01
Epoch 3/10
39/39 - 7s - loss: 382.4267 - loglik: -3.8152e+02 - logprior: -9.0191e-01
Epoch 4/10
39/39 - 8s - loss: 380.3630 - loglik: -3.7947e+02 - logprior: -8.9158e-01
Epoch 5/10
39/39 - 8s - loss: 379.0909 - loglik: -3.7822e+02 - logprior: -8.7307e-01
Epoch 6/10
39/39 - 7s - loss: 377.8837 - loglik: -3.7699e+02 - logprior: -8.9664e-01
Epoch 7/10
39/39 - 7s - loss: 376.9837 - loglik: -3.7609e+02 - logprior: -8.9687e-01
Epoch 8/10
39/39 - 8s - loss: 376.4836 - loglik: -3.7559e+02 - logprior: -8.9848e-01
Epoch 9/10
39/39 - 7s - loss: 376.2477 - loglik: -3.7535e+02 - logprior: -8.9476e-01
Epoch 10/10
39/39 - 7s - loss: 376.2033 - loglik: -3.7530e+02 - logprior: -9.0074e-01
Fitted a model with MAP estimate = -309.5592
expansions: [(0, 11), (10, 1), (15, 1), (22, 1), (28, 1), (30, 1), (33, 4), (37, 1), (43, 1), (44, 1), (56, 1), (70, 3), (71, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (102, 2), (104, 1), (107, 1), (129, 5), (134, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 359.6380 - loglik: -3.5693e+02 - logprior: -2.7115e+00
Epoch 2/2
39/39 - 10s - loss: 344.8876 - loglik: -3.4368e+02 - logprior: -1.2123e+00
Fitted a model with MAP estimate = -285.6802
expansions: [(165, 4)]
discards: [  1   2   3   4   5   6   7   8   9  96 176]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 349.9939 - loglik: -3.4805e+02 - logprior: -1.9477e+00
Epoch 2/2
39/39 - 10s - loss: 344.2860 - loglik: -3.4357e+02 - logprior: -7.1247e-01
Fitted a model with MAP estimate = -285.9763
expansions: [(0, 14)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 14s - loss: 283.4599 - loglik: -2.8184e+02 - logprior: -1.6195e+00
Epoch 2/10
52/52 - 13s - loss: 279.0718 - loglik: -2.7784e+02 - logprior: -1.2355e+00
Epoch 3/10
52/52 - 13s - loss: 276.2151 - loglik: -2.7503e+02 - logprior: -1.1872e+00
Epoch 4/10
52/52 - 12s - loss: 276.5307 - loglik: -2.7541e+02 - logprior: -1.1220e+00
Fitted a model with MAP estimate = -275.2685
Time for alignment: 234.4561
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 468.9482 - loglik: -4.6721e+02 - logprior: -1.7415e+00
Epoch 2/10
39/39 - 7s - loss: 388.5386 - loglik: -3.8759e+02 - logprior: -9.5320e-01
Epoch 3/10
39/39 - 7s - loss: 380.9211 - loglik: -3.8000e+02 - logprior: -9.1962e-01
Epoch 4/10
39/39 - 7s - loss: 378.6157 - loglik: -3.7772e+02 - logprior: -8.9160e-01
Epoch 5/10
39/39 - 8s - loss: 378.1535 - loglik: -3.7730e+02 - logprior: -8.5391e-01
Epoch 6/10
39/39 - 7s - loss: 376.7684 - loglik: -3.7593e+02 - logprior: -8.4172e-01
Epoch 7/10
39/39 - 7s - loss: 376.6197 - loglik: -3.7577e+02 - logprior: -8.4961e-01
Epoch 8/10
39/39 - 7s - loss: 376.4246 - loglik: -3.7558e+02 - logprior: -8.4815e-01
Epoch 9/10
39/39 - 7s - loss: 376.4738 - loglik: -3.7561e+02 - logprior: -8.5914e-01
Fitted a model with MAP estimate = -309.9980
expansions: [(0, 11), (12, 1), (15, 1), (18, 1), (28, 2), (29, 3), (38, 1), (43, 2), (44, 2), (45, 1), (56, 1), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (104, 2), (107, 1), (125, 2), (134, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 364.3434 - loglik: -3.6158e+02 - logprior: -2.7656e+00
Epoch 2/2
39/39 - 10s - loss: 349.4934 - loglik: -3.4825e+02 - logprior: -1.2483e+00
Fitted a model with MAP estimate = -288.7410
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  45  46  47  63  66 162]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 358.5975 - loglik: -3.5667e+02 - logprior: -1.9259e+00
Epoch 2/2
39/39 - 9s - loss: 354.4188 - loglik: -3.5378e+02 - logprior: -6.4323e-01
Fitted a model with MAP estimate = -293.9964
expansions: [(0, 14), (36, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 14s - loss: 291.7308 - loglik: -2.9013e+02 - logprior: -1.6043e+00
Epoch 2/10
52/52 - 12s - loss: 286.1052 - loglik: -2.8491e+02 - logprior: -1.1997e+00
Epoch 3/10
52/52 - 11s - loss: 284.0199 - loglik: -2.8285e+02 - logprior: -1.1688e+00
Epoch 4/10
52/52 - 11s - loss: 283.1648 - loglik: -2.8205e+02 - logprior: -1.1114e+00
Epoch 5/10
52/52 - 11s - loss: 282.6961 - loglik: -2.8166e+02 - logprior: -1.0407e+00
Epoch 6/10
52/52 - 12s - loss: 281.6990 - loglik: -2.8071e+02 - logprior: -9.9375e-01
Epoch 7/10
52/52 - 11s - loss: 281.4526 - loglik: -2.8052e+02 - logprior: -9.3571e-01
Epoch 8/10
52/52 - 11s - loss: 280.6322 - loglik: -2.7975e+02 - logprior: -8.8670e-01
Epoch 9/10
52/52 - 13s - loss: 281.1619 - loglik: -2.8035e+02 - logprior: -8.1221e-01
Fitted a model with MAP estimate = -280.6464
Time for alignment: 277.5233
Computed alignments with likelihoods: ['-271.8987', '-273.9445', '-279.5684', '-275.2685', '-280.6464']
Best model has likelihood: -271.8987  (prior= -1.0694 )
time for generating output: 0.5641
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.20963901323935613
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 817.1274 - loglik: -8.1563e+02 - logprior: -1.4947e+00
Epoch 2/10
37/37 - 20s - loss: 731.8775 - loglik: -7.3139e+02 - logprior: -4.8823e-01
Epoch 3/10
37/37 - 20s - loss: 717.3386 - loglik: -7.1696e+02 - logprior: -3.7517e-01
Epoch 4/10
37/37 - 20s - loss: 716.3140 - loglik: -7.1595e+02 - logprior: -3.6127e-01
Epoch 5/10
37/37 - 20s - loss: 711.5991 - loglik: -7.1123e+02 - logprior: -3.6722e-01
Epoch 6/10
37/37 - 20s - loss: 711.4126 - loglik: -7.1095e+02 - logprior: -4.5967e-01
Epoch 7/10
37/37 - 20s - loss: 711.1119 - loglik: -7.1071e+02 - logprior: -4.0251e-01
Epoch 8/10
37/37 - 20s - loss: 712.5124 - loglik: -7.1193e+02 - logprior: -5.7850e-01
Fitted a model with MAP estimate = -710.7647
expansions: [(0, 4), (30, 1), (32, 1), (33, 3), (34, 2), (38, 1), (66, 1), (73, 2), (90, 4), (91, 12), (92, 5), (138, 1), (197, 8), (229, 1), (239, 1)]
discards: [104 105 119 152]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 724.3199 - loglik: -7.2198e+02 - logprior: -2.3431e+00
Epoch 2/2
37/37 - 25s - loss: 708.9395 - loglik: -7.0851e+02 - logprior: -4.2508e-01
Fitted a model with MAP estimate = -705.2682
expansions: [(0, 2), (45, 1), (89, 1), (124, 1)]
discards: [  0 114 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 712.5002 - loglik: -7.1071e+02 - logprior: -1.7942e+00
Epoch 2/2
37/37 - 25s - loss: 707.6765 - loglik: -7.0748e+02 - logprior: -1.9503e-01
Fitted a model with MAP estimate = -704.8698
expansions: [(0, 2), (189, 4)]
discards: [  1   2  88 110 231]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 711.0655 - loglik: -7.0908e+02 - logprior: -1.9877e+00
Epoch 2/10
37/37 - 25s - loss: 707.4330 - loglik: -7.0737e+02 - logprior: -6.2670e-02
Epoch 3/10
37/37 - 25s - loss: 704.1398 - loglik: -7.0426e+02 - logprior: 0.1247
Epoch 4/10
37/37 - 25s - loss: 698.6716 - loglik: -6.9872e+02 - logprior: 0.0459
Epoch 5/10
37/37 - 25s - loss: 696.4355 - loglik: -6.9664e+02 - logprior: 0.2034
Epoch 6/10
37/37 - 25s - loss: 696.3711 - loglik: -6.9659e+02 - logprior: 0.2177
Epoch 7/10
37/37 - 25s - loss: 696.4128 - loglik: -6.9675e+02 - logprior: 0.3355
Fitted a model with MAP estimate = -695.1520
Time for alignment: 568.8013
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 815.8788 - loglik: -8.1437e+02 - logprior: -1.5051e+00
Epoch 2/10
37/37 - 20s - loss: 733.2518 - loglik: -7.3284e+02 - logprior: -4.1113e-01
Epoch 3/10
37/37 - 20s - loss: 724.4958 - loglik: -7.2405e+02 - logprior: -4.4818e-01
Epoch 4/10
37/37 - 20s - loss: 719.9420 - loglik: -7.1935e+02 - logprior: -5.9045e-01
Epoch 5/10
37/37 - 20s - loss: 716.6550 - loglik: -7.1611e+02 - logprior: -5.4257e-01
Epoch 6/10
37/37 - 20s - loss: 716.5283 - loglik: -7.1598e+02 - logprior: -5.5296e-01
Epoch 7/10
37/37 - 20s - loss: 716.6276 - loglik: -7.1603e+02 - logprior: -6.0231e-01
Fitted a model with MAP estimate = -715.3452
expansions: [(0, 4), (30, 1), (32, 1), (33, 3), (34, 2), (66, 1), (93, 3), (94, 7), (105, 1), (116, 1), (117, 1), (138, 6), (140, 1), (197, 12), (210, 3), (239, 1), (240, 1)]
discards: [100 101 102 103 107 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 725.8835 - loglik: -7.2339e+02 - logprior: -2.4932e+00
Epoch 2/2
37/37 - 25s - loss: 708.5283 - loglik: -7.0791e+02 - logprior: -6.2317e-01
Fitted a model with MAP estimate = -705.8175
expansions: [(0, 2), (43, 1), (44, 1), (162, 1), (171, 1), (175, 11), (213, 1)]
discards: [  0 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 709.7671 - loglik: -7.0786e+02 - logprior: -1.9044e+00
Epoch 2/2
37/37 - 27s - loss: 703.1431 - loglik: -7.0281e+02 - logprior: -3.3335e-01
Fitted a model with MAP estimate = -700.8253
expansions: [(0, 2), (182, 1)]
discards: [  1   2   3 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 709.1430 - loglik: -7.0696e+02 - logprior: -2.1795e+00
Epoch 2/10
37/37 - 27s - loss: 703.1974 - loglik: -7.0299e+02 - logprior: -2.1115e-01
Epoch 3/10
37/37 - 26s - loss: 700.2744 - loglik: -7.0028e+02 - logprior: 0.0012
Epoch 4/10
37/37 - 27s - loss: 696.5540 - loglik: -6.9666e+02 - logprior: 0.1038
Epoch 5/10
37/37 - 26s - loss: 695.9835 - loglik: -6.9615e+02 - logprior: 0.1644
Epoch 6/10
37/37 - 26s - loss: 692.4918 - loglik: -6.9271e+02 - logprior: 0.2171
Epoch 7/10
37/37 - 26s - loss: 694.6022 - loglik: -6.9494e+02 - logprior: 0.3423
Fitted a model with MAP estimate = -692.0798
Time for alignment: 563.6213
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 25s - loss: 816.0366 - loglik: -8.1454e+02 - logprior: -1.4967e+00
Epoch 2/10
37/37 - 20s - loss: 730.4911 - loglik: -7.2992e+02 - logprior: -5.6837e-01
Epoch 3/10
37/37 - 20s - loss: 720.7531 - loglik: -7.2012e+02 - logprior: -6.2976e-01
Epoch 4/10
37/37 - 20s - loss: 715.4297 - loglik: -7.1492e+02 - logprior: -5.0897e-01
Epoch 5/10
37/37 - 20s - loss: 709.9913 - loglik: -7.0943e+02 - logprior: -5.5652e-01
Epoch 6/10
37/37 - 20s - loss: 711.1489 - loglik: -7.1064e+02 - logprior: -5.1253e-01
Fitted a model with MAP estimate = -710.4763
expansions: [(0, 3), (30, 1), (32, 4), (33, 2), (65, 1), (91, 4), (92, 15), (99, 1), (100, 1), (197, 10), (239, 1)]
discards: [104 105 116 134 135 136 137 138 139 152 153 154]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 726.4167 - loglik: -7.2410e+02 - logprior: -2.3157e+00
Epoch 2/2
37/37 - 24s - loss: 711.2139 - loglik: -7.1078e+02 - logprior: -4.3568e-01
Fitted a model with MAP estimate = -707.9353
expansions: [(0, 3), (38, 1), (41, 1), (42, 1), (176, 3), (223, 1), (226, 3), (228, 3)]
discards: [103 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 714.5910 - loglik: -7.1191e+02 - logprior: -2.6762e+00
Epoch 2/2
37/37 - 25s - loss: 706.3162 - loglik: -7.0594e+02 - logprior: -3.7717e-01
Fitted a model with MAP estimate = -705.1214
expansions: []
discards: [  0   1   2 182 236 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 712.0983 - loglik: -7.1033e+02 - logprior: -1.7711e+00
Epoch 2/10
37/37 - 24s - loss: 707.4923 - loglik: -7.0746e+02 - logprior: -2.8430e-02
Epoch 3/10
37/37 - 24s - loss: 705.0294 - loglik: -7.0519e+02 - logprior: 0.1630
Epoch 4/10
37/37 - 24s - loss: 701.6754 - loglik: -7.0192e+02 - logprior: 0.2444
Epoch 5/10
37/37 - 24s - loss: 699.3026 - loglik: -6.9961e+02 - logprior: 0.3120
Epoch 6/10
37/37 - 24s - loss: 697.1541 - loglik: -6.9744e+02 - logprior: 0.2842
Epoch 7/10
37/37 - 24s - loss: 699.2791 - loglik: -6.9970e+02 - logprior: 0.4227
Fitted a model with MAP estimate = -697.0205
Time for alignment: 518.1676
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 816.2153 - loglik: -8.1474e+02 - logprior: -1.4782e+00
Epoch 2/10
37/37 - 20s - loss: 734.4001 - loglik: -7.3403e+02 - logprior: -3.6798e-01
Epoch 3/10
37/37 - 20s - loss: 724.3400 - loglik: -7.2402e+02 - logprior: -3.2468e-01
Epoch 4/10
37/37 - 20s - loss: 721.8937 - loglik: -7.2157e+02 - logprior: -3.2690e-01
Epoch 5/10
37/37 - 20s - loss: 717.3425 - loglik: -7.1703e+02 - logprior: -3.1695e-01
Epoch 6/10
37/37 - 20s - loss: 715.0583 - loglik: -7.1473e+02 - logprior: -3.2867e-01
Epoch 7/10
37/37 - 20s - loss: 717.6191 - loglik: -7.1730e+02 - logprior: -3.1548e-01
Fitted a model with MAP estimate = -715.6139
expansions: [(0, 4), (30, 1), (32, 1), (34, 2), (35, 2), (36, 1), (66, 1), (115, 2), (116, 1), (134, 4), (135, 1), (197, 12), (239, 1)]
discards: [163 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 727.6217 - loglik: -7.2521e+02 - logprior: -2.4085e+00
Epoch 2/2
37/37 - 23s - loss: 710.8431 - loglik: -7.1026e+02 - logprior: -5.8047e-01
Fitted a model with MAP estimate = -708.8230
expansions: [(0, 2), (43, 1), (44, 1), (111, 4), (152, 1), (154, 1), (205, 1), (227, 3)]
discards: [  0 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 714.3239 - loglik: -7.1252e+02 - logprior: -1.8059e+00
Epoch 2/2
37/37 - 25s - loss: 709.0970 - loglik: -7.0884e+02 - logprior: -2.6059e-01
Fitted a model with MAP estimate = -705.8086
expansions: [(0, 2), (181, 4)]
discards: [  1   2   3 114]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 712.4661 - loglik: -7.1042e+02 - logprior: -2.0469e+00
Epoch 2/10
37/37 - 25s - loss: 706.4518 - loglik: -7.0635e+02 - logprior: -1.0048e-01
Epoch 3/10
37/37 - 25s - loss: 703.6062 - loglik: -7.0369e+02 - logprior: 0.0806
Epoch 4/10
37/37 - 25s - loss: 700.0969 - loglik: -7.0026e+02 - logprior: 0.1603
Epoch 5/10
37/37 - 25s - loss: 698.0040 - loglik: -6.9818e+02 - logprior: 0.1726
Epoch 6/10
37/37 - 25s - loss: 697.1292 - loglik: -6.9741e+02 - logprior: 0.2771
Epoch 7/10
37/37 - 25s - loss: 693.5716 - loglik: -6.9391e+02 - logprior: 0.3420
Epoch 8/10
37/37 - 25s - loss: 695.9424 - loglik: -6.9635e+02 - logprior: 0.4026
Fitted a model with MAP estimate = -694.9430
Time for alignment: 564.1910
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 816.7484 - loglik: -8.1526e+02 - logprior: -1.4930e+00
Epoch 2/10
37/37 - 20s - loss: 731.1754 - loglik: -7.3082e+02 - logprior: -3.5410e-01
Epoch 3/10
37/37 - 20s - loss: 719.1110 - loglik: -7.1872e+02 - logprior: -3.9058e-01
Epoch 4/10
37/37 - 20s - loss: 713.5150 - loglik: -7.1311e+02 - logprior: -4.0909e-01
Epoch 5/10
37/37 - 20s - loss: 711.6111 - loglik: -7.1104e+02 - logprior: -5.6912e-01
Epoch 6/10
37/37 - 20s - loss: 708.9389 - loglik: -7.0848e+02 - logprior: -4.6211e-01
Epoch 7/10
37/37 - 20s - loss: 710.0453 - loglik: -7.0947e+02 - logprior: -5.7523e-01
Fitted a model with MAP estimate = -709.2433
expansions: [(0, 4), (33, 2), (35, 3), (39, 1), (59, 1), (60, 2), (90, 16), (100, 1), (197, 12), (239, 1), (240, 1)]
discards: [102 160 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 721.9698 - loglik: -7.1951e+02 - logprior: -2.4547e+00
Epoch 2/2
37/37 - 25s - loss: 707.8137 - loglik: -7.0723e+02 - logprior: -5.8448e-01
Fitted a model with MAP estimate = -704.2209
expansions: [(0, 2), (39, 1), (40, 1), (109, 5), (110, 3), (236, 3)]
discards: [  0  71 181 182 183]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 711.3170 - loglik: -7.0939e+02 - logprior: -1.9239e+00
Epoch 2/2
37/37 - 26s - loss: 704.7532 - loglik: -7.0446e+02 - logprior: -2.8954e-01
Fitted a model with MAP estimate = -702.3961
expansions: [(0, 2), (191, 4)]
discards: [  1   2 114 115 116]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 708.9545 - loglik: -7.0674e+02 - logprior: -2.2147e+00
Epoch 2/10
37/37 - 26s - loss: 703.8071 - loglik: -7.0354e+02 - logprior: -2.6429e-01
Epoch 3/10
37/37 - 26s - loss: 700.1033 - loglik: -7.0001e+02 - logprior: -9.0900e-02
Epoch 4/10
37/37 - 26s - loss: 697.8478 - loglik: -6.9782e+02 - logprior: -2.6967e-02
Epoch 5/10
37/37 - 26s - loss: 692.4911 - loglik: -6.9247e+02 - logprior: -2.2995e-02
Epoch 6/10
37/37 - 26s - loss: 692.4693 - loglik: -6.9258e+02 - logprior: 0.1111
Epoch 7/10
37/37 - 26s - loss: 691.8143 - loglik: -6.9196e+02 - logprior: 0.1419
Epoch 8/10
37/37 - 26s - loss: 691.7249 - loglik: -6.9195e+02 - logprior: 0.2268
Epoch 9/10
37/37 - 26s - loss: 693.6315 - loglik: -6.9387e+02 - logprior: 0.2422
Fitted a model with MAP estimate = -690.9365
Time for alignment: 608.3715
Computed alignments with likelihoods: ['-695.1520', '-692.0798', '-697.0205', '-694.9430', '-690.9365']
Best model has likelihood: -690.9365  (prior= 0.4049 )
time for generating output: 0.2486
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.9068893528183716
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0499 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1454 - loglik: -2.6185e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.0419 - loglik: -2.5022e+02 - logprior: -9.8220e+00
Epoch 4/10
10/10 - 1s - loss: 245.4427 - loglik: -2.4208e+02 - logprior: -3.3632e+00
Epoch 5/10
10/10 - 1s - loss: 237.8282 - loglik: -2.3760e+02 - logprior: -2.2552e-01
Epoch 6/10
10/10 - 1s - loss: 232.6716 - loglik: -2.3419e+02 - logprior: 1.5219
Epoch 7/10
10/10 - 1s - loss: 229.5091 - loglik: -2.3192e+02 - logprior: 2.4090
Epoch 8/10
10/10 - 1s - loss: 227.7231 - loglik: -2.3079e+02 - logprior: 3.0623
Epoch 9/10
10/10 - 1s - loss: 226.6306 - loglik: -2.3030e+02 - logprior: 3.6679
Epoch 10/10
10/10 - 1s - loss: 225.8971 - loglik: -2.3009e+02 - logprior: 4.1958
Fitted a model with MAP estimate = -225.5880
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 377.7707 - loglik: -2.2882e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 268.2882 - loglik: -2.2644e+02 - logprior: -4.1853e+01
Fitted a model with MAP estimate = -248.1874
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 343.2954 - loglik: -2.2535e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 254.6228 - loglik: -2.2473e+02 - logprior: -2.9892e+01
Fitted a model with MAP estimate = -239.0165
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1235 - loglik: -2.2368e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 258.3513 - loglik: -2.2308e+02 - logprior: -3.5266e+01
Epoch 3/10
10/10 - 1s - loss: 233.8346 - loglik: -2.2352e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 223.3760 - loglik: -2.2388e+02 - logprior: 0.4994
Epoch 5/10
10/10 - 1s - loss: 218.6405 - loglik: -2.2392e+02 - logprior: 5.2832
Epoch 6/10
10/10 - 1s - loss: 216.0812 - loglik: -2.2391e+02 - logprior: 7.8306
Epoch 7/10
10/10 - 1s - loss: 214.4764 - loglik: -2.2378e+02 - logprior: 9.3053
Epoch 8/10
10/10 - 1s - loss: 213.3593 - loglik: -2.2366e+02 - logprior: 10.2983
Epoch 9/10
10/10 - 1s - loss: 212.5155 - loglik: -2.2361e+02 - logprior: 11.0982
Epoch 10/10
10/10 - 1s - loss: 211.8161 - loglik: -2.2366e+02 - logprior: 11.8399
Fitted a model with MAP estimate = -211.4677
Time for alignment: 37.3095
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0499 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1454 - loglik: -2.6185e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.0420 - loglik: -2.5022e+02 - logprior: -9.8220e+00
Epoch 4/10
10/10 - 1s - loss: 245.4427 - loglik: -2.4208e+02 - logprior: -3.3632e+00
Epoch 5/10
10/10 - 1s - loss: 237.8282 - loglik: -2.3760e+02 - logprior: -2.2552e-01
Epoch 6/10
10/10 - 1s - loss: 232.6716 - loglik: -2.3419e+02 - logprior: 1.5219
Epoch 7/10
10/10 - 1s - loss: 229.5091 - loglik: -2.3192e+02 - logprior: 2.4090
Epoch 8/10
10/10 - 1s - loss: 227.7231 - loglik: -2.3079e+02 - logprior: 3.0623
Epoch 9/10
10/10 - 1s - loss: 226.6306 - loglik: -2.3030e+02 - logprior: 3.6679
Epoch 10/10
10/10 - 1s - loss: 225.8972 - loglik: -2.3009e+02 - logprior: 4.1958
Fitted a model with MAP estimate = -225.5879
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 377.7707 - loglik: -2.2882e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 268.2881 - loglik: -2.2644e+02 - logprior: -4.1853e+01
Fitted a model with MAP estimate = -248.1874
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 343.2954 - loglik: -2.2535e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 254.6229 - loglik: -2.2473e+02 - logprior: -2.9892e+01
Fitted a model with MAP estimate = -239.0165
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1233 - loglik: -2.2368e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 258.3515 - loglik: -2.2308e+02 - logprior: -3.5267e+01
Epoch 3/10
10/10 - 1s - loss: 233.8369 - loglik: -2.2352e+02 - logprior: -1.0316e+01
Epoch 4/10
10/10 - 1s - loss: 223.3771 - loglik: -2.2388e+02 - logprior: 0.4997
Epoch 5/10
10/10 - 1s - loss: 218.6419 - loglik: -2.2393e+02 - logprior: 5.2831
Epoch 6/10
10/10 - 1s - loss: 216.0822 - loglik: -2.2391e+02 - logprior: 7.8301
Epoch 7/10
10/10 - 1s - loss: 214.4767 - loglik: -2.2378e+02 - logprior: 9.3053
Epoch 8/10
10/10 - 1s - loss: 213.3598 - loglik: -2.2366e+02 - logprior: 10.2982
Epoch 9/10
10/10 - 1s - loss: 212.5157 - loglik: -2.2361e+02 - logprior: 11.0981
Epoch 10/10
10/10 - 1s - loss: 211.8163 - loglik: -2.2366e+02 - logprior: 11.8397
Fitted a model with MAP estimate = -211.4679
Time for alignment: 35.7521
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0499 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1454 - loglik: -2.6185e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.0419 - loglik: -2.5022e+02 - logprior: -9.8220e+00
Epoch 4/10
10/10 - 1s - loss: 245.4427 - loglik: -2.4208e+02 - logprior: -3.3632e+00
Epoch 5/10
10/10 - 1s - loss: 237.8281 - loglik: -2.3760e+02 - logprior: -2.2553e-01
Epoch 6/10
10/10 - 1s - loss: 232.6715 - loglik: -2.3419e+02 - logprior: 1.5219
Epoch 7/10
10/10 - 1s - loss: 229.5091 - loglik: -2.3192e+02 - logprior: 2.4090
Epoch 8/10
10/10 - 1s - loss: 227.7231 - loglik: -2.3079e+02 - logprior: 3.0623
Epoch 9/10
10/10 - 1s - loss: 226.6306 - loglik: -2.3030e+02 - logprior: 3.6678
Epoch 10/10
10/10 - 1s - loss: 225.8971 - loglik: -2.3009e+02 - logprior: 4.1958
Fitted a model with MAP estimate = -225.5881
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 377.7706 - loglik: -2.2882e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 268.2881 - loglik: -2.2644e+02 - logprior: -4.1853e+01
Fitted a model with MAP estimate = -248.1874
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 343.2954 - loglik: -2.2535e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 254.6229 - loglik: -2.2473e+02 - logprior: -2.9892e+01
Fitted a model with MAP estimate = -239.0166
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1234 - loglik: -2.2368e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 258.3513 - loglik: -2.2308e+02 - logprior: -3.5266e+01
Epoch 3/10
10/10 - 1s - loss: 233.8351 - loglik: -2.2352e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 223.3763 - loglik: -2.2388e+02 - logprior: 0.4994
Epoch 5/10
10/10 - 1s - loss: 218.6409 - loglik: -2.2392e+02 - logprior: 5.2831
Epoch 6/10
10/10 - 1s - loss: 216.0814 - loglik: -2.2391e+02 - logprior: 7.8305
Epoch 7/10
10/10 - 1s - loss: 214.4765 - loglik: -2.2378e+02 - logprior: 9.3052
Epoch 8/10
10/10 - 1s - loss: 213.3594 - loglik: -2.2366e+02 - logprior: 10.2983
Epoch 9/10
10/10 - 1s - loss: 212.5155 - loglik: -2.2361e+02 - logprior: 11.0981
Epoch 10/10
10/10 - 1s - loss: 211.8162 - loglik: -2.2366e+02 - logprior: 11.8398
Fitted a model with MAP estimate = -211.4677
Time for alignment: 36.4664
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0499 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1454 - loglik: -2.6185e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.0419 - loglik: -2.5022e+02 - logprior: -9.8220e+00
Epoch 4/10
10/10 - 1s - loss: 245.4426 - loglik: -2.4208e+02 - logprior: -3.3632e+00
Epoch 5/10
10/10 - 1s - loss: 237.8282 - loglik: -2.3760e+02 - logprior: -2.2552e-01
Epoch 6/10
10/10 - 1s - loss: 232.6716 - loglik: -2.3419e+02 - logprior: 1.5219
Epoch 7/10
10/10 - 1s - loss: 229.5091 - loglik: -2.3192e+02 - logprior: 2.4090
Epoch 8/10
10/10 - 1s - loss: 227.7231 - loglik: -2.3079e+02 - logprior: 3.0623
Epoch 9/10
10/10 - 1s - loss: 226.6306 - loglik: -2.3030e+02 - logprior: 3.6679
Epoch 10/10
10/10 - 1s - loss: 225.8971 - loglik: -2.3009e+02 - logprior: 4.1958
Fitted a model with MAP estimate = -225.5881
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 377.7706 - loglik: -2.2882e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 268.2881 - loglik: -2.2644e+02 - logprior: -4.1853e+01
Fitted a model with MAP estimate = -248.1874
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 343.2954 - loglik: -2.2535e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 254.6228 - loglik: -2.2473e+02 - logprior: -2.9892e+01
Fitted a model with MAP estimate = -239.0164
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1234 - loglik: -2.2368e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 258.3513 - loglik: -2.2308e+02 - logprior: -3.5266e+01
Epoch 3/10
10/10 - 1s - loss: 233.8351 - loglik: -2.2352e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 223.3762 - loglik: -2.2388e+02 - logprior: 0.4996
Epoch 5/10
10/10 - 1s - loss: 218.6408 - loglik: -2.2392e+02 - logprior: 5.2832
Epoch 6/10
10/10 - 1s - loss: 216.0813 - loglik: -2.2391e+02 - logprior: 7.8306
Epoch 7/10
10/10 - 1s - loss: 214.4763 - loglik: -2.2378e+02 - logprior: 9.3053
Epoch 8/10
10/10 - 1s - loss: 213.3594 - loglik: -2.2366e+02 - logprior: 10.2984
Epoch 9/10
10/10 - 1s - loss: 212.5154 - loglik: -2.2361e+02 - logprior: 11.0982
Epoch 10/10
10/10 - 1s - loss: 211.8160 - loglik: -2.2366e+02 - logprior: 11.8399
Fitted a model with MAP estimate = -211.4678
Time for alignment: 35.8683
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 386.0499 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1454 - loglik: -2.6185e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.0419 - loglik: -2.5022e+02 - logprior: -9.8220e+00
Epoch 4/10
10/10 - 1s - loss: 245.4427 - loglik: -2.4208e+02 - logprior: -3.3632e+00
Epoch 5/10
10/10 - 1s - loss: 237.8282 - loglik: -2.3760e+02 - logprior: -2.2552e-01
Epoch 6/10
10/10 - 1s - loss: 232.6716 - loglik: -2.3419e+02 - logprior: 1.5219
Epoch 7/10
10/10 - 1s - loss: 229.5091 - loglik: -2.3192e+02 - logprior: 2.4090
Epoch 8/10
10/10 - 1s - loss: 227.7230 - loglik: -2.3079e+02 - logprior: 3.0623
Epoch 9/10
10/10 - 1s - loss: 226.6306 - loglik: -2.3030e+02 - logprior: 3.6679
Epoch 10/10
10/10 - 1s - loss: 225.8971 - loglik: -2.3009e+02 - logprior: 4.1958
Fitted a model with MAP estimate = -225.5882
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 377.7707 - loglik: -2.2882e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 268.2882 - loglik: -2.2644e+02 - logprior: -4.1853e+01
Fitted a model with MAP estimate = -248.1874
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 343.2954 - loglik: -2.2535e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 254.6228 - loglik: -2.2473e+02 - logprior: -2.9892e+01
Fitted a model with MAP estimate = -239.0166
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1235 - loglik: -2.2368e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 258.3513 - loglik: -2.2308e+02 - logprior: -3.5266e+01
Epoch 3/10
10/10 - 1s - loss: 233.8344 - loglik: -2.2352e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 223.3759 - loglik: -2.2388e+02 - logprior: 0.4994
Epoch 5/10
10/10 - 1s - loss: 218.6405 - loglik: -2.2392e+02 - logprior: 5.2831
Epoch 6/10
10/10 - 1s - loss: 216.0811 - loglik: -2.2391e+02 - logprior: 7.8306
Epoch 7/10
10/10 - 1s - loss: 214.4764 - loglik: -2.2378e+02 - logprior: 9.3052
Epoch 8/10
10/10 - 1s - loss: 213.3594 - loglik: -2.2366e+02 - logprior: 10.2983
Epoch 9/10
10/10 - 1s - loss: 212.5155 - loglik: -2.2361e+02 - logprior: 11.0981
Epoch 10/10
10/10 - 1s - loss: 211.8162 - loglik: -2.2366e+02 - logprior: 11.8398
Fitted a model with MAP estimate = -211.4678
Time for alignment: 36.0826
Computed alignments with likelihoods: ['-211.4677', '-211.4679', '-211.4677', '-211.4678', '-211.4678']
Best model has likelihood: -211.4677  (prior= 12.2325 )
time for generating output: 0.1441
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7485226526592252
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 998.3197 - loglik: -9.9081e+02 - logprior: -7.5133e+00
Epoch 2/10
19/19 - 17s - loss: 871.4076 - loglik: -8.7245e+02 - logprior: 1.0458
Epoch 3/10
19/19 - 17s - loss: 816.2206 - loglik: -8.1635e+02 - logprior: 0.1325
Epoch 4/10
19/19 - 17s - loss: 796.0661 - loglik: -7.9605e+02 - logprior: -1.8865e-02
Epoch 5/10
19/19 - 17s - loss: 791.2465 - loglik: -7.9123e+02 - logprior: -1.4394e-02
Epoch 6/10
19/19 - 17s - loss: 787.6835 - loglik: -7.8752e+02 - logprior: -1.6842e-01
Epoch 7/10
19/19 - 17s - loss: 789.0039 - loglik: -7.8879e+02 - logprior: -2.1020e-01
Fitted a model with MAP estimate = -786.3825
expansions: [(27, 1), (32, 1), (65, 2), (96, 1), (97, 1), (98, 2), (106, 1), (112, 1), (113, 1), (116, 1), (117, 1), (118, 1), (120, 6), (121, 2), (145, 1), (149, 1), (156, 1), (162, 2), (165, 1), (166, 1), (167, 1), (168, 1), (176, 1), (177, 2), (178, 1), (189, 4), (201, 1), (204, 1), (211, 1), (219, 1), (220, 2), (221, 2), (223, 1), (225, 1), (236, 3), (264, 7), (280, 1), (283, 1), (300, 2), (301, 1), (302, 2), (303, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 797.8036 - loglik: -7.8697e+02 - logprior: -1.0830e+01
Epoch 2/2
19/19 - 23s - loss: 763.4901 - loglik: -7.6148e+02 - logprior: -2.0093e+00
Fitted a model with MAP estimate = -756.9364
expansions: [(0, 2)]
discards: [  0 137 138 186 208 263 366]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 764.8670 - loglik: -7.5897e+02 - logprior: -5.9005e+00
Epoch 2/2
19/19 - 23s - loss: 752.2007 - loglik: -7.5490e+02 - logprior: 2.7024
Fitted a model with MAP estimate = -746.6993
expansions: []
discards: [  0 105]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 768.2573 - loglik: -7.5913e+02 - logprior: -9.1271e+00
Epoch 2/10
19/19 - 23s - loss: 753.8903 - loglik: -7.5489e+02 - logprior: 1.0030
Epoch 3/10
19/19 - 22s - loss: 743.6797 - loglik: -7.4802e+02 - logprior: 4.3376
Epoch 4/10
19/19 - 22s - loss: 740.6558 - loglik: -7.4549e+02 - logprior: 4.8373
Epoch 5/10
19/19 - 22s - loss: 743.8966 - loglik: -7.4904e+02 - logprior: 5.1440
Fitted a model with MAP estimate = -738.8650
Time for alignment: 400.8119
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 998.9487 - loglik: -9.9145e+02 - logprior: -7.4969e+00
Epoch 2/10
19/19 - 17s - loss: 869.9090 - loglik: -8.7104e+02 - logprior: 1.1264
Epoch 3/10
19/19 - 17s - loss: 811.4985 - loglik: -8.1179e+02 - logprior: 0.2895
Epoch 4/10
19/19 - 17s - loss: 798.4086 - loglik: -7.9861e+02 - logprior: 0.2020
Epoch 5/10
19/19 - 17s - loss: 793.4810 - loglik: -7.9367e+02 - logprior: 0.1860
Epoch 6/10
19/19 - 17s - loss: 786.9592 - loglik: -7.8695e+02 - logprior: -8.5487e-03
Epoch 7/10
19/19 - 17s - loss: 788.0923 - loglik: -7.8812e+02 - logprior: 0.0240
Fitted a model with MAP estimate = -786.7045
expansions: [(14, 1), (32, 1), (68, 1), (69, 1), (94, 1), (95, 1), (96, 1), (97, 3), (111, 1), (112, 3), (115, 1), (116, 1), (117, 1), (120, 6), (121, 2), (157, 1), (163, 2), (165, 1), (166, 2), (169, 1), (172, 1), (175, 2), (176, 1), (177, 1), (189, 2), (190, 2), (198, 1), (204, 1), (212, 1), (214, 1), (221, 1), (222, 2), (223, 1), (226, 1), (239, 1), (240, 1), (258, 1), (265, 4), (301, 6), (303, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 797.9616 - loglik: -7.8738e+02 - logprior: -1.0577e+01
Epoch 2/2
19/19 - 23s - loss: 771.5546 - loglik: -7.6928e+02 - logprior: -2.2735e+00
Fitted a model with MAP estimate = -762.9907
expansions: [(0, 2), (318, 1), (352, 1)]
discards: [  0 104 122 139 140 188 207 228 309]
Re-initialized the encoder parameters.
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 775.1637 - loglik: -7.6903e+02 - logprior: -6.1301e+00
Epoch 2/2
19/19 - 22s - loss: 758.1005 - loglik: -7.6047e+02 - logprior: 2.3704
Fitted a model with MAP estimate = -755.8710
expansions: [(303, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 775.4124 - loglik: -7.6596e+02 - logprior: -9.4569e+00
Epoch 2/10
19/19 - 22s - loss: 760.8033 - loglik: -7.6139e+02 - logprior: 0.5892
Epoch 3/10
19/19 - 22s - loss: 756.5079 - loglik: -7.6058e+02 - logprior: 4.0762
Epoch 4/10
19/19 - 22s - loss: 750.8621 - loglik: -7.5550e+02 - logprior: 4.6398
Epoch 5/10
19/19 - 22s - loss: 745.1423 - loglik: -7.4995e+02 - logprior: 4.8069
Epoch 6/10
19/19 - 23s - loss: 746.9042 - loglik: -7.5204e+02 - logprior: 5.1320
Fitted a model with MAP estimate = -745.4182
Time for alignment: 418.9570
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 998.9456 - loglik: -9.9146e+02 - logprior: -7.4830e+00
Epoch 2/10
19/19 - 17s - loss: 875.1764 - loglik: -8.7645e+02 - logprior: 1.2722
Epoch 3/10
19/19 - 17s - loss: 811.0101 - loglik: -8.1154e+02 - logprior: 0.5292
Epoch 4/10
19/19 - 17s - loss: 801.0238 - loglik: -8.0124e+02 - logprior: 0.2175
Epoch 5/10
19/19 - 17s - loss: 792.0010 - loglik: -7.9203e+02 - logprior: 0.0256
Epoch 6/10
19/19 - 17s - loss: 791.2291 - loglik: -7.9122e+02 - logprior: -1.2233e-02
Epoch 7/10
19/19 - 17s - loss: 791.4728 - loglik: -7.9138e+02 - logprior: -9.4268e-02
Fitted a model with MAP estimate = -789.5058
expansions: [(30, 1), (48, 1), (67, 1), (98, 5), (112, 1), (113, 1), (114, 1), (119, 3), (120, 8), (121, 1), (154, 1), (163, 2), (166, 1), (167, 1), (168, 1), (169, 1), (172, 1), (175, 2), (176, 1), (190, 4), (198, 1), (204, 1), (208, 1), (211, 1), (221, 1), (222, 3), (223, 1), (226, 1), (236, 2), (237, 2), (264, 5), (279, 1), (282, 1), (283, 1), (294, 2), (301, 1), (302, 2), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 802.4051 - loglik: -7.9135e+02 - logprior: -1.1052e+01
Epoch 2/2
19/19 - 23s - loss: 767.5313 - loglik: -7.6487e+02 - logprior: -2.6628e+00
Fitted a model with MAP estimate = -762.4480
expansions: [(0, 2), (318, 1)]
discards: [  0 100 101 186 205 353 354]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 773.2179 - loglik: -7.6702e+02 - logprior: -6.2001e+00
Epoch 2/2
19/19 - 23s - loss: 757.4617 - loglik: -7.5976e+02 - logprior: 2.3024
Fitted a model with MAP estimate = -754.1764
expansions: [(346, 6)]
discards: [  0  30 245 326 327]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 776.3234 - loglik: -7.6657e+02 - logprior: -9.7549e+00
Epoch 2/10
19/19 - 23s - loss: 759.8896 - loglik: -7.6007e+02 - logprior: 0.1829
Epoch 3/10
19/19 - 23s - loss: 750.1190 - loglik: -7.5405e+02 - logprior: 3.9264
Epoch 4/10
19/19 - 23s - loss: 749.7241 - loglik: -7.5422e+02 - logprior: 4.4937
Epoch 5/10
19/19 - 23s - loss: 746.6589 - loglik: -7.5141e+02 - logprior: 4.7515
Epoch 6/10
19/19 - 23s - loss: 743.4497 - loglik: -7.4845e+02 - logprior: 5.0041
Epoch 7/10
19/19 - 23s - loss: 743.9448 - loglik: -7.4918e+02 - logprior: 5.2338
Fitted a model with MAP estimate = -743.0733
Time for alignment: 449.5892
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 998.7050 - loglik: -9.9119e+02 - logprior: -7.5125e+00
Epoch 2/10
19/19 - 17s - loss: 875.8582 - loglik: -8.7709e+02 - logprior: 1.2308
Epoch 3/10
19/19 - 17s - loss: 815.2411 - loglik: -8.1571e+02 - logprior: 0.4725
Epoch 4/10
19/19 - 17s - loss: 800.9089 - loglik: -8.0129e+02 - logprior: 0.3787
Epoch 5/10
19/19 - 17s - loss: 799.7775 - loglik: -8.0019e+02 - logprior: 0.4147
Epoch 6/10
19/19 - 17s - loss: 793.9984 - loglik: -7.9435e+02 - logprior: 0.3474
Epoch 7/10
19/19 - 17s - loss: 793.3570 - loglik: -7.9372e+02 - logprior: 0.3672
Epoch 8/10
19/19 - 17s - loss: 792.4292 - loglik: -7.9278e+02 - logprior: 0.3526
Epoch 9/10
19/19 - 17s - loss: 792.6902 - loglik: -7.9299e+02 - logprior: 0.2950
Fitted a model with MAP estimate = -791.6541
expansions: [(33, 1), (58, 1), (68, 1), (109, 1), (110, 1), (115, 1), (116, 1), (119, 1), (120, 1), (121, 1), (122, 9), (124, 2), (142, 1), (164, 1), (167, 1), (168, 1), (169, 1), (170, 1), (173, 1), (176, 2), (177, 1), (178, 1), (190, 4), (198, 1), (204, 1), (212, 2), (221, 2), (222, 2), (224, 1), (237, 1), (238, 2), (265, 4), (301, 1), (302, 3), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 801.4304 - loglik: -7.9105e+02 - logprior: -1.0382e+01
Epoch 2/2
19/19 - 22s - loss: 774.3824 - loglik: -7.7260e+02 - logprior: -1.7837e+00
Fitted a model with MAP estimate = -765.4740
expansions: [(0, 2), (283, 1), (313, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 772.6096 - loglik: -7.6679e+02 - logprior: -5.8226e+00
Epoch 2/2
19/19 - 22s - loss: 756.8895 - loglik: -7.5973e+02 - logprior: 2.8417
Fitted a model with MAP estimate = -753.7706
expansions: []
discards: [  0 145]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 776.5513 - loglik: -7.6744e+02 - logprior: -9.1141e+00
Epoch 2/10
19/19 - 23s - loss: 759.7180 - loglik: -7.6050e+02 - logprior: 0.7858
Epoch 3/10
19/19 - 22s - loss: 751.6450 - loglik: -7.5600e+02 - logprior: 4.3544
Epoch 4/10
19/19 - 22s - loss: 749.8855 - loglik: -7.5479e+02 - logprior: 4.9090
Epoch 5/10
19/19 - 22s - loss: 749.1666 - loglik: -7.5424e+02 - logprior: 5.0776
Epoch 6/10
19/19 - 23s - loss: 747.8203 - loglik: -7.5314e+02 - logprior: 5.3215
Epoch 7/10
19/19 - 22s - loss: 744.5604 - loglik: -7.5020e+02 - logprior: 5.6354
Epoch 8/10
19/19 - 23s - loss: 743.1979 - loglik: -7.4930e+02 - logprior: 6.1044
Epoch 9/10
19/19 - 23s - loss: 743.7869 - loglik: -7.5015e+02 - logprior: 6.3606
Fitted a model with MAP estimate = -743.1619
Time for alignment: 521.6783
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 998.3014 - loglik: -9.9081e+02 - logprior: -7.4883e+00
Epoch 2/10
19/19 - 17s - loss: 870.6230 - loglik: -8.7174e+02 - logprior: 1.1183
Epoch 3/10
19/19 - 17s - loss: 810.8484 - loglik: -8.1108e+02 - logprior: 0.2271
Epoch 4/10
19/19 - 17s - loss: 801.4156 - loglik: -8.0146e+02 - logprior: 0.0400
Epoch 5/10
19/19 - 17s - loss: 793.2390 - loglik: -7.9334e+02 - logprior: 0.1019
Epoch 6/10
19/19 - 17s - loss: 792.9958 - loglik: -7.9307e+02 - logprior: 0.0699
Epoch 7/10
19/19 - 17s - loss: 788.5005 - loglik: -7.8856e+02 - logprior: 0.0553
Epoch 8/10
19/19 - 17s - loss: 790.9573 - loglik: -7.9102e+02 - logprior: 0.0607
Fitted a model with MAP estimate = -789.5319
expansions: [(14, 1), (30, 1), (61, 1), (66, 2), (97, 1), (98, 3), (105, 1), (111, 1), (112, 2), (116, 1), (117, 3), (118, 1), (119, 2), (120, 3), (122, 3), (157, 1), (164, 1), (166, 3), (169, 1), (176, 1), (177, 1), (178, 1), (180, 2), (190, 3), (198, 1), (204, 1), (212, 2), (222, 2), (223, 1), (226, 1), (239, 1), (240, 1), (260, 1), (264, 7), (280, 1), (294, 3), (301, 1), (302, 3), (309, 1), (310, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 801.6649 - loglik: -7.9084e+02 - logprior: -1.0828e+01
Epoch 2/2
19/19 - 23s - loss: 767.5406 - loglik: -7.6531e+02 - logprior: -2.2290e+00
Fitted a model with MAP estimate = -761.8436
expansions: [(0, 2), (266, 1)]
discards: [  0 103 104 122 134 140 215 352 353]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 771.5646 - loglik: -7.6561e+02 - logprior: -5.9503e+00
Epoch 2/2
19/19 - 23s - loss: 759.1100 - loglik: -7.6174e+02 - logprior: 2.6306
Fitted a model with MAP estimate = -753.9880
expansions: [(138, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 775.0784 - loglik: -7.6553e+02 - logprior: -9.5438e+00
Epoch 2/10
19/19 - 23s - loss: 761.3605 - loglik: -7.6199e+02 - logprior: 0.6257
Epoch 3/10
19/19 - 23s - loss: 750.9301 - loglik: -7.5512e+02 - logprior: 4.1921
Epoch 4/10
19/19 - 23s - loss: 745.9568 - loglik: -7.5081e+02 - logprior: 4.8505
Epoch 5/10
19/19 - 23s - loss: 745.7181 - loglik: -7.5083e+02 - logprior: 5.1084
Epoch 6/10
19/19 - 23s - loss: 743.1887 - loglik: -7.4853e+02 - logprior: 5.3423
Epoch 7/10
19/19 - 23s - loss: 745.5015 - loglik: -7.5114e+02 - logprior: 5.6369
Fitted a model with MAP estimate = -742.4044
Time for alignment: 467.3272
Computed alignments with likelihoods: ['-738.8650', '-745.4182', '-743.0733', '-743.1619', '-742.4044']
Best model has likelihood: -738.8650  (prior= 5.1232 )
time for generating output: 0.4475
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7163461538461539
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.6354 - loglik: -3.1238e+02 - logprior: -5.6258e+01
Epoch 2/10
10/10 - 1s - loss: 289.4079 - loglik: -2.7605e+02 - logprior: -1.3360e+01
Epoch 3/10
10/10 - 1s - loss: 238.9058 - loglik: -2.3299e+02 - logprior: -5.9175e+00
Epoch 4/10
10/10 - 1s - loss: 210.5318 - loglik: -2.0672e+02 - logprior: -3.8125e+00
Epoch 5/10
10/10 - 1s - loss: 200.4061 - loglik: -1.9771e+02 - logprior: -2.6952e+00
Epoch 6/10
10/10 - 1s - loss: 197.5474 - loglik: -1.9551e+02 - logprior: -2.0367e+00
Epoch 7/10
10/10 - 1s - loss: 195.0916 - loglik: -1.9351e+02 - logprior: -1.5795e+00
Epoch 8/10
10/10 - 1s - loss: 194.1318 - loglik: -1.9291e+02 - logprior: -1.2235e+00
Epoch 9/10
10/10 - 1s - loss: 193.4598 - loglik: -1.9249e+02 - logprior: -9.7245e-01
Epoch 10/10
10/10 - 1s - loss: 193.6318 - loglik: -1.9282e+02 - logprior: -8.0820e-01
Fitted a model with MAP estimate = -193.2100
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.0288 - loglik: -1.8574e+02 - logprior: -6.3287e+01
Epoch 2/2
10/10 - 1s - loss: 197.6128 - loglik: -1.7313e+02 - logprior: -2.4478e+01
Fitted a model with MAP estimate = -189.0966
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.6324 - loglik: -1.7085e+02 - logprior: -4.9781e+01
Epoch 2/2
10/10 - 1s - loss: 178.7052 - loglik: -1.6785e+02 - logprior: -1.0857e+01
Fitted a model with MAP estimate = -172.8174
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.4100 - loglik: -1.7019e+02 - logprior: -6.0225e+01
Epoch 2/10
10/10 - 1s - loss: 186.2452 - loglik: -1.6931e+02 - logprior: -1.6936e+01
Epoch 3/10
10/10 - 1s - loss: 172.7159 - loglik: -1.6867e+02 - logprior: -4.0460e+00
Epoch 4/10
10/10 - 1s - loss: 168.0007 - loglik: -1.6840e+02 - logprior: 0.3964
Epoch 5/10
10/10 - 1s - loss: 165.9189 - loglik: -1.6817e+02 - logprior: 2.2479
Epoch 6/10
10/10 - 1s - loss: 164.7669 - loglik: -1.6813e+02 - logprior: 3.3586
Epoch 7/10
10/10 - 1s - loss: 164.1091 - loglik: -1.6835e+02 - logprior: 4.2422
Epoch 8/10
10/10 - 1s - loss: 163.9043 - loglik: -1.6880e+02 - logprior: 4.9001
Epoch 9/10
10/10 - 1s - loss: 163.2218 - loglik: -1.6856e+02 - logprior: 5.3394
Epoch 10/10
10/10 - 1s - loss: 163.1119 - loglik: -1.6879e+02 - logprior: 5.6745
Fitted a model with MAP estimate = -162.8248
Time for alignment: 40.6126
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 368.5677 - loglik: -3.1231e+02 - logprior: -5.6259e+01
Epoch 2/10
10/10 - 1s - loss: 290.6496 - loglik: -2.7728e+02 - logprior: -1.3367e+01
Epoch 3/10
10/10 - 1s - loss: 241.7063 - loglik: -2.3570e+02 - logprior: -6.0087e+00
Epoch 4/10
10/10 - 1s - loss: 213.2464 - loglik: -2.0929e+02 - logprior: -3.9541e+00
Epoch 5/10
10/10 - 1s - loss: 202.0836 - loglik: -1.9913e+02 - logprior: -2.9543e+00
Epoch 6/10
10/10 - 1s - loss: 198.3786 - loglik: -1.9618e+02 - logprior: -2.1942e+00
Epoch 7/10
10/10 - 1s - loss: 195.7296 - loglik: -1.9400e+02 - logprior: -1.7336e+00
Epoch 8/10
10/10 - 1s - loss: 194.2881 - loglik: -1.9285e+02 - logprior: -1.4341e+00
Epoch 9/10
10/10 - 1s - loss: 194.0049 - loglik: -1.9281e+02 - logprior: -1.1910e+00
Epoch 10/10
10/10 - 1s - loss: 193.0568 - loglik: -1.9202e+02 - logprior: -1.0366e+00
Fitted a model with MAP estimate = -193.1260
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 1), (62, 1), (78, 4), (81, 1), (83, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.6959 - loglik: -1.8634e+02 - logprior: -6.3354e+01
Epoch 2/2
10/10 - 1s - loss: 199.2211 - loglik: -1.7456e+02 - logprior: -2.4665e+01
Fitted a model with MAP estimate = -190.2326
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.3593 - loglik: -1.7060e+02 - logprior: -4.9763e+01
Epoch 2/2
10/10 - 1s - loss: 179.1397 - loglik: -1.6831e+02 - logprior: -1.0831e+01
Fitted a model with MAP estimate = -172.7342
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 230.2955 - loglik: -1.7010e+02 - logprior: -6.0198e+01
Epoch 2/10
10/10 - 1s - loss: 186.1345 - loglik: -1.6929e+02 - logprior: -1.6847e+01
Epoch 3/10
10/10 - 1s - loss: 172.7753 - loglik: -1.6877e+02 - logprior: -4.0079e+00
Epoch 4/10
10/10 - 1s - loss: 167.6819 - loglik: -1.6810e+02 - logprior: 0.4139
Epoch 5/10
10/10 - 1s - loss: 166.0131 - loglik: -1.6828e+02 - logprior: 2.2715
Epoch 6/10
10/10 - 1s - loss: 164.5787 - loglik: -1.6794e+02 - logprior: 3.3610
Epoch 7/10
10/10 - 1s - loss: 164.2731 - loglik: -1.6852e+02 - logprior: 4.2441
Epoch 8/10
10/10 - 1s - loss: 163.7109 - loglik: -1.6861e+02 - logprior: 4.9005
Epoch 9/10
10/10 - 1s - loss: 163.2355 - loglik: -1.6857e+02 - logprior: 5.3371
Epoch 10/10
10/10 - 1s - loss: 163.0425 - loglik: -1.6873e+02 - logprior: 5.6833
Fitted a model with MAP estimate = -162.7994
Time for alignment: 45.2044
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.4741 - loglik: -3.1222e+02 - logprior: -5.6257e+01
Epoch 2/10
10/10 - 1s - loss: 290.9299 - loglik: -2.7757e+02 - logprior: -1.3364e+01
Epoch 3/10
10/10 - 1s - loss: 240.4027 - loglik: -2.3443e+02 - logprior: -5.9740e+00
Epoch 4/10
10/10 - 1s - loss: 212.3898 - loglik: -2.0850e+02 - logprior: -3.8850e+00
Epoch 5/10
10/10 - 1s - loss: 202.7243 - loglik: -2.0008e+02 - logprior: -2.6477e+00
Epoch 6/10
10/10 - 1s - loss: 198.8142 - loglik: -1.9689e+02 - logprior: -1.9263e+00
Epoch 7/10
10/10 - 1s - loss: 197.0136 - loglik: -1.9550e+02 - logprior: -1.5148e+00
Epoch 8/10
10/10 - 1s - loss: 195.7150 - loglik: -1.9440e+02 - logprior: -1.3115e+00
Epoch 9/10
10/10 - 1s - loss: 195.0198 - loglik: -1.9395e+02 - logprior: -1.0730e+00
Epoch 10/10
10/10 - 1s - loss: 194.3594 - loglik: -1.9341e+02 - logprior: -9.5315e-01
Fitted a model with MAP estimate = -194.2131
expansions: [(13, 4), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 1), (62, 1), (78, 4), (81, 1), (87, 1), (89, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.8798 - loglik: -1.8854e+02 - logprior: -6.3343e+01
Epoch 2/2
10/10 - 1s - loss: 200.4221 - loglik: -1.7582e+02 - logprior: -2.4602e+01
Fitted a model with MAP estimate = -191.5978
expansions: [(0, 2)]
discards: [ 0 20 71 99]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.0653 - loglik: -1.7125e+02 - logprior: -4.9819e+01
Epoch 2/2
10/10 - 1s - loss: 179.4745 - loglik: -1.6855e+02 - logprior: -1.0921e+01
Fitted a model with MAP estimate = -173.1817
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 230.5898 - loglik: -1.7035e+02 - logprior: -6.0244e+01
Epoch 2/10
10/10 - 1s - loss: 186.0852 - loglik: -1.6931e+02 - logprior: -1.6780e+01
Epoch 3/10
10/10 - 1s - loss: 173.0290 - loglik: -1.6903e+02 - logprior: -3.9958e+00
Epoch 4/10
10/10 - 1s - loss: 168.0550 - loglik: -1.6846e+02 - logprior: 0.4034
Epoch 5/10
10/10 - 1s - loss: 166.3099 - loglik: -1.6858e+02 - logprior: 2.2729
Epoch 6/10
10/10 - 1s - loss: 164.8999 - loglik: -1.6828e+02 - logprior: 3.3839
Epoch 7/10
10/10 - 1s - loss: 164.2558 - loglik: -1.6853e+02 - logprior: 4.2715
Epoch 8/10
10/10 - 1s - loss: 164.0782 - loglik: -1.6901e+02 - logprior: 4.9320
Epoch 9/10
10/10 - 1s - loss: 163.3947 - loglik: -1.6877e+02 - logprior: 5.3754
Epoch 10/10
10/10 - 1s - loss: 163.2928 - loglik: -1.6901e+02 - logprior: 5.7168
Fitted a model with MAP estimate = -163.0641
Time for alignment: 42.4552
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.8720 - loglik: -3.1262e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 290.5372 - loglik: -2.7717e+02 - logprior: -1.3371e+01
Epoch 3/10
10/10 - 1s - loss: 241.5505 - loglik: -2.3558e+02 - logprior: -5.9670e+00
Epoch 4/10
10/10 - 1s - loss: 213.1259 - loglik: -2.0930e+02 - logprior: -3.8252e+00
Epoch 5/10
10/10 - 1s - loss: 202.3136 - loglik: -1.9961e+02 - logprior: -2.6999e+00
Epoch 6/10
10/10 - 1s - loss: 197.5347 - loglik: -1.9554e+02 - logprior: -1.9947e+00
Epoch 7/10
10/10 - 1s - loss: 195.1006 - loglik: -1.9345e+02 - logprior: -1.6457e+00
Epoch 8/10
10/10 - 1s - loss: 194.0439 - loglik: -1.9270e+02 - logprior: -1.3437e+00
Epoch 9/10
10/10 - 1s - loss: 193.2976 - loglik: -1.9220e+02 - logprior: -1.0994e+00
Epoch 10/10
10/10 - 1s - loss: 192.6384 - loglik: -1.9172e+02 - logprior: -9.2199e-01
Fitted a model with MAP estimate = -192.7729
expansions: [(13, 3), (17, 3), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.1166 - loglik: -1.8576e+02 - logprior: -6.3358e+01
Epoch 2/2
10/10 - 1s - loss: 198.6559 - loglik: -1.7402e+02 - logprior: -2.4636e+01
Fitted a model with MAP estimate = -189.9319
expansions: [(0, 2)]
discards: [ 0 20 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.2782 - loglik: -1.7039e+02 - logprior: -4.9889e+01
Epoch 2/2
10/10 - 1s - loss: 179.7553 - loglik: -1.6876e+02 - logprior: -1.0995e+01
Fitted a model with MAP estimate = -173.4623
expansions: [(69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.5157 - loglik: -1.7074e+02 - logprior: -6.0779e+01
Epoch 2/10
10/10 - 1s - loss: 188.3980 - loglik: -1.7017e+02 - logprior: -1.8227e+01
Epoch 3/10
10/10 - 1s - loss: 173.5285 - loglik: -1.6894e+02 - logprior: -4.5856e+00
Epoch 4/10
10/10 - 1s - loss: 168.1972 - loglik: -1.6857e+02 - logprior: 0.3716
Epoch 5/10
10/10 - 1s - loss: 165.8151 - loglik: -1.6812e+02 - logprior: 2.3023
Epoch 6/10
10/10 - 1s - loss: 165.2049 - loglik: -1.6862e+02 - logprior: 3.4104
Epoch 7/10
10/10 - 1s - loss: 164.4317 - loglik: -1.6874e+02 - logprior: 4.3069
Epoch 8/10
10/10 - 1s - loss: 163.9683 - loglik: -1.6894e+02 - logprior: 4.9724
Epoch 9/10
10/10 - 1s - loss: 163.4504 - loglik: -1.6886e+02 - logprior: 5.4114
Epoch 10/10
10/10 - 1s - loss: 163.1855 - loglik: -1.6894e+02 - logprior: 5.7507
Fitted a model with MAP estimate = -163.0264
Time for alignment: 42.4710
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.0310 - loglik: -3.1278e+02 - logprior: -5.6255e+01
Epoch 2/10
10/10 - 1s - loss: 289.4074 - loglik: -2.7604e+02 - logprior: -1.3368e+01
Epoch 3/10
10/10 - 1s - loss: 242.8326 - loglik: -2.3685e+02 - logprior: -5.9780e+00
Epoch 4/10
10/10 - 1s - loss: 216.0316 - loglik: -2.1213e+02 - logprior: -3.8990e+00
Epoch 5/10
10/10 - 1s - loss: 204.8590 - loglik: -2.0206e+02 - logprior: -2.7974e+00
Epoch 6/10
10/10 - 1s - loss: 199.4415 - loglik: -1.9725e+02 - logprior: -2.1943e+00
Epoch 7/10
10/10 - 1s - loss: 196.6458 - loglik: -1.9491e+02 - logprior: -1.7364e+00
Epoch 8/10
10/10 - 1s - loss: 194.8213 - loglik: -1.9338e+02 - logprior: -1.4450e+00
Epoch 9/10
10/10 - 1s - loss: 194.3683 - loglik: -1.9315e+02 - logprior: -1.2198e+00
Epoch 10/10
10/10 - 1s - loss: 193.3371 - loglik: -1.9231e+02 - logprior: -1.0235e+00
Fitted a model with MAP estimate = -193.5219
expansions: [(13, 3), (16, 1), (17, 3), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (58, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.8588 - loglik: -1.8657e+02 - logprior: -6.3286e+01
Epoch 2/2
10/10 - 1s - loss: 198.4771 - loglik: -1.7398e+02 - logprior: -2.4493e+01
Fitted a model with MAP estimate = -189.8307
expansions: [(0, 2), (70, 1)]
discards: [ 0 18 99]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 218.7693 - loglik: -1.6907e+02 - logprior: -4.9698e+01
Epoch 2/2
10/10 - 1s - loss: 176.9724 - loglik: -1.6627e+02 - logprior: -1.0705e+01
Fitted a model with MAP estimate = -170.8777
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 228.5551 - loglik: -1.6845e+02 - logprior: -6.0108e+01
Epoch 2/10
10/10 - 1s - loss: 184.5427 - loglik: -1.6773e+02 - logprior: -1.6817e+01
Epoch 3/10
10/10 - 1s - loss: 170.9765 - loglik: -1.6707e+02 - logprior: -3.9015e+00
Epoch 4/10
10/10 - 1s - loss: 166.1070 - loglik: -1.6665e+02 - logprior: 0.5419
Epoch 5/10
10/10 - 1s - loss: 164.0268 - loglik: -1.6646e+02 - logprior: 2.4312
Epoch 6/10
10/10 - 1s - loss: 163.2693 - loglik: -1.6681e+02 - logprior: 3.5415
Epoch 7/10
10/10 - 1s - loss: 162.5356 - loglik: -1.6697e+02 - logprior: 4.4304
Epoch 8/10
10/10 - 1s - loss: 162.0032 - loglik: -1.6709e+02 - logprior: 5.0868
Epoch 9/10
10/10 - 1s - loss: 161.6954 - loglik: -1.6723e+02 - logprior: 5.5325
Epoch 10/10
10/10 - 1s - loss: 161.4062 - loglik: -1.6729e+02 - logprior: 5.8825
Fitted a model with MAP estimate = -161.1241
Time for alignment: 42.1567
Computed alignments with likelihoods: ['-162.8248', '-162.7994', '-163.0641', '-163.0264', '-161.1241']
Best model has likelihood: -161.1241  (prior= 6.0497 )
time for generating output: 0.1671
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9350982066609735
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.9161 - loglik: -3.8885e+02 - logprior: -8.0621e+00
Epoch 2/10
13/13 - 2s - loss: 355.1332 - loglik: -3.5332e+02 - logprior: -1.8145e+00
Epoch 3/10
13/13 - 2s - loss: 328.6737 - loglik: -3.2717e+02 - logprior: -1.4993e+00
Epoch 4/10
13/13 - 2s - loss: 313.0722 - loglik: -3.1132e+02 - logprior: -1.7480e+00
Epoch 5/10
13/13 - 2s - loss: 308.1715 - loglik: -3.0655e+02 - logprior: -1.6204e+00
Epoch 6/10
13/13 - 2s - loss: 306.0162 - loglik: -3.0450e+02 - logprior: -1.5129e+00
Epoch 7/10
13/13 - 2s - loss: 305.5656 - loglik: -3.0404e+02 - logprior: -1.5295e+00
Epoch 8/10
13/13 - 2s - loss: 304.6555 - loglik: -3.0313e+02 - logprior: -1.5220e+00
Epoch 9/10
13/13 - 2s - loss: 304.8834 - loglik: -3.0338e+02 - logprior: -1.5022e+00
Fitted a model with MAP estimate = -304.5389
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 2), (73, 2), (80, 1), (82, 1), (92, 8), (99, 1), (100, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 317.0560 - loglik: -3.0762e+02 - logprior: -9.4395e+00
Epoch 2/2
13/13 - 3s - loss: 300.2091 - loglik: -2.9608e+02 - logprior: -4.1326e+00
Fitted a model with MAP estimate = -296.8853
expansions: [(0, 2)]
discards: [  0  36  38  91 117 118 119]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 301.0906 - loglik: -2.9402e+02 - logprior: -7.0658e+00
Epoch 2/2
13/13 - 3s - loss: 293.1841 - loglik: -2.9140e+02 - logprior: -1.7820e+00
Fitted a model with MAP estimate = -291.3557
expansions: []
discards: [  0  73 114]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 303.6699 - loglik: -2.9483e+02 - logprior: -8.8420e+00
Epoch 2/10
13/13 - 2s - loss: 294.6013 - loglik: -2.9222e+02 - logprior: -2.3815e+00
Epoch 3/10
13/13 - 2s - loss: 291.5076 - loglik: -2.9055e+02 - logprior: -9.5761e-01
Epoch 4/10
13/13 - 2s - loss: 289.9347 - loglik: -2.8940e+02 - logprior: -5.3891e-01
Epoch 5/10
13/13 - 2s - loss: 289.7608 - loglik: -2.8935e+02 - logprior: -4.1379e-01
Epoch 6/10
13/13 - 2s - loss: 288.3613 - loglik: -2.8800e+02 - logprior: -3.6220e-01
Epoch 7/10
13/13 - 2s - loss: 288.8205 - loglik: -2.8849e+02 - logprior: -3.3155e-01
Fitted a model with MAP estimate = -288.1634
Time for alignment: 72.1090
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.5103 - loglik: -3.8846e+02 - logprior: -8.0517e+00
Epoch 2/10
13/13 - 2s - loss: 355.6346 - loglik: -3.5383e+02 - logprior: -1.8047e+00
Epoch 3/10
13/13 - 2s - loss: 328.6678 - loglik: -3.2718e+02 - logprior: -1.4897e+00
Epoch 4/10
13/13 - 2s - loss: 315.2326 - loglik: -3.1357e+02 - logprior: -1.6626e+00
Epoch 5/10
13/13 - 2s - loss: 310.3710 - loglik: -3.0883e+02 - logprior: -1.5407e+00
Epoch 6/10
13/13 - 2s - loss: 308.3710 - loglik: -3.0691e+02 - logprior: -1.4629e+00
Epoch 7/10
13/13 - 2s - loss: 306.4976 - loglik: -3.0500e+02 - logprior: -1.4991e+00
Epoch 8/10
13/13 - 2s - loss: 306.6685 - loglik: -3.0518e+02 - logprior: -1.4914e+00
Fitted a model with MAP estimate = -306.3864
expansions: [(6, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (82, 4), (83, 1), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 318.6223 - loglik: -3.0918e+02 - logprior: -9.4469e+00
Epoch 2/2
13/13 - 3s - loss: 301.1577 - loglik: -2.9707e+02 - logprior: -4.0891e+00
Fitted a model with MAP estimate = -298.3672
expansions: [(0, 2), (126, 1)]
discards: [  0  35  38  75  82 104]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 302.6984 - loglik: -2.9565e+02 - logprior: -7.0529e+00
Epoch 2/2
13/13 - 2s - loss: 294.5623 - loglik: -2.9280e+02 - logprior: -1.7667e+00
Fitted a model with MAP estimate = -292.8561
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.9718 - loglik: -2.9608e+02 - logprior: -8.8925e+00
Epoch 2/10
13/13 - 2s - loss: 295.9606 - loglik: -2.9355e+02 - logprior: -2.4058e+00
Epoch 3/10
13/13 - 2s - loss: 292.0300 - loglik: -2.9105e+02 - logprior: -9.7896e-01
Epoch 4/10
13/13 - 2s - loss: 291.5016 - loglik: -2.9095e+02 - logprior: -5.5474e-01
Epoch 5/10
13/13 - 2s - loss: 290.1664 - loglik: -2.8977e+02 - logprior: -4.0050e-01
Epoch 6/10
13/13 - 2s - loss: 290.0607 - loglik: -2.8971e+02 - logprior: -3.4752e-01
Epoch 7/10
13/13 - 2s - loss: 289.3997 - loglik: -2.8907e+02 - logprior: -3.2958e-01
Epoch 8/10
13/13 - 2s - loss: 288.9843 - loglik: -2.8867e+02 - logprior: -3.0937e-01
Epoch 9/10
13/13 - 2s - loss: 289.2149 - loglik: -2.8893e+02 - logprior: -2.8707e-01
Fitted a model with MAP estimate = -288.8980
Time for alignment: 74.6945
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8766 - loglik: -3.8881e+02 - logprior: -8.0634e+00
Epoch 2/10
13/13 - 2s - loss: 355.3078 - loglik: -3.5350e+02 - logprior: -1.8085e+00
Epoch 3/10
13/13 - 2s - loss: 327.7120 - loglik: -3.2621e+02 - logprior: -1.5006e+00
Epoch 4/10
13/13 - 2s - loss: 314.4231 - loglik: -3.1273e+02 - logprior: -1.6937e+00
Epoch 5/10
13/13 - 2s - loss: 310.3251 - loglik: -3.0879e+02 - logprior: -1.5360e+00
Epoch 6/10
13/13 - 2s - loss: 307.4163 - loglik: -3.0597e+02 - logprior: -1.4424e+00
Epoch 7/10
13/13 - 2s - loss: 306.7812 - loglik: -3.0529e+02 - logprior: -1.4909e+00
Epoch 8/10
13/13 - 2s - loss: 306.4347 - loglik: -3.0492e+02 - logprior: -1.5167e+00
Epoch 9/10
13/13 - 2s - loss: 305.2281 - loglik: -3.0372e+02 - logprior: -1.5055e+00
Epoch 10/10
13/13 - 2s - loss: 305.6644 - loglik: -3.0416e+02 - logprior: -1.5055e+00
Fitted a model with MAP estimate = -305.1967
expansions: [(6, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (48, 1), (52, 1), (55, 1), (58, 2), (74, 2), (80, 1), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 316.2271 - loglik: -3.0684e+02 - logprior: -9.3917e+00
Epoch 2/2
13/13 - 3s - loss: 300.4828 - loglik: -2.9645e+02 - logprior: -4.0309e+00
Fitted a model with MAP estimate = -297.5200
expansions: [(0, 2), (125, 1)]
discards: [ 0 36 38]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 301.1041 - loglik: -2.9409e+02 - logprior: -7.0185e+00
Epoch 2/2
13/13 - 3s - loss: 292.4011 - loglik: -2.9073e+02 - logprior: -1.6678e+00
Fitted a model with MAP estimate = -291.0089
expansions: []
discards: [ 0 73 90]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 303.2021 - loglik: -2.9440e+02 - logprior: -8.7986e+00
Epoch 2/10
13/13 - 2s - loss: 295.0668 - loglik: -2.9276e+02 - logprior: -2.3076e+00
Epoch 3/10
13/13 - 2s - loss: 291.0673 - loglik: -2.9013e+02 - logprior: -9.4048e-01
Epoch 4/10
13/13 - 2s - loss: 289.7599 - loglik: -2.8926e+02 - logprior: -4.9883e-01
Epoch 5/10
13/13 - 2s - loss: 289.0078 - loglik: -2.8867e+02 - logprior: -3.3421e-01
Epoch 6/10
13/13 - 2s - loss: 288.9200 - loglik: -2.8864e+02 - logprior: -2.8272e-01
Epoch 7/10
13/13 - 2s - loss: 287.9008 - loglik: -2.8765e+02 - logprior: -2.5527e-01
Epoch 8/10
13/13 - 2s - loss: 288.2787 - loglik: -2.8803e+02 - logprior: -2.4461e-01
Fitted a model with MAP estimate = -287.7982
Time for alignment: 75.5114
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8394 - loglik: -3.8878e+02 - logprior: -8.0613e+00
Epoch 2/10
13/13 - 2s - loss: 355.0285 - loglik: -3.5322e+02 - logprior: -1.8134e+00
Epoch 3/10
13/13 - 2s - loss: 329.1898 - loglik: -3.2769e+02 - logprior: -1.4977e+00
Epoch 4/10
13/13 - 2s - loss: 315.1225 - loglik: -3.1340e+02 - logprior: -1.7251e+00
Epoch 5/10
13/13 - 2s - loss: 309.1188 - loglik: -3.0742e+02 - logprior: -1.6963e+00
Epoch 6/10
13/13 - 2s - loss: 306.2823 - loglik: -3.0461e+02 - logprior: -1.6720e+00
Epoch 7/10
13/13 - 2s - loss: 305.8197 - loglik: -3.0409e+02 - logprior: -1.7254e+00
Epoch 8/10
13/13 - 2s - loss: 304.2064 - loglik: -3.0247e+02 - logprior: -1.7392e+00
Epoch 9/10
13/13 - 2s - loss: 304.3283 - loglik: -3.0262e+02 - logprior: -1.7129e+00
Fitted a model with MAP estimate = -303.9323
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 1), (28, 1), (29, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (80, 1), (81, 1), (82, 1), (83, 2), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 316.3000 - loglik: -3.0685e+02 - logprior: -9.4530e+00
Epoch 2/2
13/13 - 3s - loss: 300.6505 - loglik: -2.9660e+02 - logprior: -4.0516e+00
Fitted a model with MAP estimate = -297.9467
expansions: [(0, 2), (124, 1)]
discards: [ 0 72 73 80]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 301.9582 - loglik: -2.9493e+02 - logprior: -7.0248e+00
Epoch 2/2
13/13 - 3s - loss: 294.6052 - loglik: -2.9291e+02 - logprior: -1.6908e+00
Fitted a model with MAP estimate = -292.8199
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.7306 - loglik: -2.9591e+02 - logprior: -8.8158e+00
Epoch 2/10
13/13 - 2s - loss: 295.3430 - loglik: -2.9298e+02 - logprior: -2.3586e+00
Epoch 3/10
13/13 - 2s - loss: 292.6440 - loglik: -2.9171e+02 - logprior: -9.2908e-01
Epoch 4/10
13/13 - 3s - loss: 290.5363 - loglik: -2.9003e+02 - logprior: -5.0589e-01
Epoch 5/10
13/13 - 2s - loss: 290.8067 - loglik: -2.9045e+02 - logprior: -3.5930e-01
Fitted a model with MAP estimate = -289.8355
Time for alignment: 65.7963
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8522 - loglik: -3.8879e+02 - logprior: -8.0634e+00
Epoch 2/10
13/13 - 2s - loss: 354.5639 - loglik: -3.5275e+02 - logprior: -1.8173e+00
Epoch 3/10
13/13 - 2s - loss: 325.8200 - loglik: -3.2432e+02 - logprior: -1.5047e+00
Epoch 4/10
13/13 - 2s - loss: 313.1984 - loglik: -3.1149e+02 - logprior: -1.7100e+00
Epoch 5/10
13/13 - 2s - loss: 308.3280 - loglik: -3.0673e+02 - logprior: -1.5946e+00
Epoch 6/10
13/13 - 2s - loss: 306.1505 - loglik: -3.0464e+02 - logprior: -1.5096e+00
Epoch 7/10
13/13 - 2s - loss: 305.8821 - loglik: -3.0435e+02 - logprior: -1.5337e+00
Epoch 8/10
13/13 - 2s - loss: 304.9157 - loglik: -3.0339e+02 - logprior: -1.5258e+00
Epoch 9/10
13/13 - 2s - loss: 303.8399 - loglik: -3.0233e+02 - logprior: -1.5077e+00
Epoch 10/10
13/13 - 2s - loss: 303.9952 - loglik: -3.0248e+02 - logprior: -1.5117e+00
Fitted a model with MAP estimate = -303.9220
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 315.3653 - loglik: -3.0592e+02 - logprior: -9.4408e+00
Epoch 2/2
13/13 - 3s - loss: 299.3514 - loglik: -2.9532e+02 - logprior: -4.0300e+00
Fitted a model with MAP estimate = -296.7035
expansions: [(0, 2), (124, 1)]
discards: [ 0 35 38 81]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 300.4114 - loglik: -2.9342e+02 - logprior: -6.9947e+00
Epoch 2/2
13/13 - 2s - loss: 292.3143 - loglik: -2.9066e+02 - logprior: -1.6540e+00
Fitted a model with MAP estimate = -290.8664
expansions: []
discards: [ 0 73]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 303.1519 - loglik: -2.9439e+02 - logprior: -8.7662e+00
Epoch 2/10
13/13 - 2s - loss: 294.2488 - loglik: -2.9195e+02 - logprior: -2.2964e+00
Epoch 3/10
13/13 - 2s - loss: 290.8806 - loglik: -2.8999e+02 - logprior: -8.9397e-01
Epoch 4/10
13/13 - 2s - loss: 290.2542 - loglik: -2.8979e+02 - logprior: -4.5998e-01
Epoch 5/10
13/13 - 2s - loss: 288.6532 - loglik: -2.8834e+02 - logprior: -3.1766e-01
Epoch 6/10
13/13 - 2s - loss: 288.4958 - loglik: -2.8824e+02 - logprior: -2.5816e-01
Epoch 7/10
13/13 - 2s - loss: 287.9915 - loglik: -2.8777e+02 - logprior: -2.2284e-01
Epoch 8/10
13/13 - 2s - loss: 288.0671 - loglik: -2.8786e+02 - logprior: -2.0326e-01
Fitted a model with MAP estimate = -287.6797
Time for alignment: 74.8022
Computed alignments with likelihoods: ['-288.1634', '-288.8980', '-287.7982', '-289.8355', '-287.6797']
Best model has likelihood: -287.6797  (prior= -0.1834 )
time for generating output: 0.1809
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8985313751668892
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.8123 - loglik: -2.1423e+02 - logprior: -8.5859e+00
Epoch 2/10
13/13 - 1s - loss: 174.9651 - loglik: -1.7272e+02 - logprior: -2.2472e+00
Epoch 3/10
13/13 - 1s - loss: 145.9987 - loglik: -1.4418e+02 - logprior: -1.8234e+00
Epoch 4/10
13/13 - 1s - loss: 136.6813 - loglik: -1.3500e+02 - logprior: -1.6802e+00
Epoch 5/10
13/13 - 1s - loss: 133.7744 - loglik: -1.3220e+02 - logprior: -1.5734e+00
Epoch 6/10
13/13 - 1s - loss: 133.0480 - loglik: -1.3154e+02 - logprior: -1.5058e+00
Epoch 7/10
13/13 - 1s - loss: 132.2451 - loglik: -1.3076e+02 - logprior: -1.4805e+00
Epoch 8/10
13/13 - 1s - loss: 132.1830 - loglik: -1.3071e+02 - logprior: -1.4780e+00
Epoch 9/10
13/13 - 1s - loss: 132.0341 - loglik: -1.3058e+02 - logprior: -1.4541e+00
Epoch 10/10
13/13 - 1s - loss: 132.0562 - loglik: -1.3061e+02 - logprior: -1.4493e+00
Fitted a model with MAP estimate = -131.8621
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.2529 - loglik: -1.2805e+02 - logprior: -1.0199e+01
Epoch 2/2
13/13 - 1s - loss: 121.9007 - loglik: -1.1874e+02 - logprior: -3.1568e+00
Fitted a model with MAP estimate = -118.9246
expansions: [(0, 2)]
discards: [43 47 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 127.2286 - loglik: -1.1718e+02 - logprior: -1.0050e+01
Epoch 2/2
13/13 - 1s - loss: 118.4804 - loglik: -1.1521e+02 - logprior: -3.2708e+00
Fitted a model with MAP estimate = -116.6654
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 122.5177 - loglik: -1.1460e+02 - logprior: -7.9195e+00
Epoch 2/10
13/13 - 1s - loss: 116.9350 - loglik: -1.1471e+02 - logprior: -2.2221e+00
Epoch 3/10
13/13 - 1s - loss: 115.2363 - loglik: -1.1355e+02 - logprior: -1.6868e+00
Epoch 4/10
13/13 - 1s - loss: 115.1213 - loglik: -1.1378e+02 - logprior: -1.3376e+00
Epoch 5/10
13/13 - 1s - loss: 114.5407 - loglik: -1.1330e+02 - logprior: -1.2379e+00
Epoch 6/10
13/13 - 1s - loss: 114.8568 - loglik: -1.1367e+02 - logprior: -1.1899e+00
Fitted a model with MAP estimate = -114.4043
Time for alignment: 48.2138
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.6471 - loglik: -2.1405e+02 - logprior: -8.5960e+00
Epoch 2/10
13/13 - 1s - loss: 173.2793 - loglik: -1.7102e+02 - logprior: -2.2590e+00
Epoch 3/10
13/13 - 1s - loss: 145.0536 - loglik: -1.4327e+02 - logprior: -1.7793e+00
Epoch 4/10
13/13 - 1s - loss: 136.6123 - loglik: -1.3505e+02 - logprior: -1.5661e+00
Epoch 5/10
13/13 - 1s - loss: 134.3499 - loglik: -1.3287e+02 - logprior: -1.4825e+00
Epoch 6/10
13/13 - 1s - loss: 132.9354 - loglik: -1.3151e+02 - logprior: -1.4271e+00
Epoch 7/10
13/13 - 1s - loss: 132.8448 - loglik: -1.3143e+02 - logprior: -1.4134e+00
Epoch 8/10
13/13 - 1s - loss: 131.7932 - loglik: -1.3041e+02 - logprior: -1.3881e+00
Epoch 9/10
13/13 - 1s - loss: 132.3523 - loglik: -1.3098e+02 - logprior: -1.3736e+00
Fitted a model with MAP estimate = -132.0107
expansions: [(0, 4), (13, 1), (16, 1), (36, 2), (37, 2), (38, 2), (43, 2), (44, 3), (45, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 137.3768 - loglik: -1.2718e+02 - logprior: -1.0193e+01
Epoch 2/2
13/13 - 1s - loss: 120.4360 - loglik: -1.1724e+02 - logprior: -3.1967e+00
Fitted a model with MAP estimate = -117.2891
expansions: [(0, 2)]
discards: [46 60 62 63]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 128.8454 - loglik: -1.1877e+02 - logprior: -1.0077e+01
Epoch 2/2
13/13 - 1s - loss: 118.4040 - loglik: -1.1508e+02 - logprior: -3.3196e+00
Fitted a model with MAP estimate = -116.8610
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 123.0372 - loglik: -1.1504e+02 - logprior: -7.9979e+00
Epoch 2/10
13/13 - 1s - loss: 116.6798 - loglik: -1.1436e+02 - logprior: -2.3180e+00
Epoch 3/10
13/13 - 1s - loss: 115.6294 - loglik: -1.1384e+02 - logprior: -1.7900e+00
Epoch 4/10
13/13 - 1s - loss: 115.0376 - loglik: -1.1361e+02 - logprior: -1.4276e+00
Epoch 5/10
13/13 - 1s - loss: 114.9794 - loglik: -1.1365e+02 - logprior: -1.3340e+00
Epoch 6/10
13/13 - 1s - loss: 114.5284 - loglik: -1.1325e+02 - logprior: -1.2812e+00
Epoch 7/10
13/13 - 1s - loss: 114.7149 - loglik: -1.1347e+02 - logprior: -1.2442e+00
Fitted a model with MAP estimate = -114.4350
Time for alignment: 47.4947
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.7090 - loglik: -2.1412e+02 - logprior: -8.5851e+00
Epoch 2/10
13/13 - 1s - loss: 174.1003 - loglik: -1.7185e+02 - logprior: -2.2461e+00
Epoch 3/10
13/13 - 1s - loss: 145.5942 - loglik: -1.4379e+02 - logprior: -1.8075e+00
Epoch 4/10
13/13 - 1s - loss: 138.2127 - loglik: -1.3656e+02 - logprior: -1.6507e+00
Epoch 5/10
13/13 - 1s - loss: 134.6390 - loglik: -1.3306e+02 - logprior: -1.5796e+00
Epoch 6/10
13/13 - 2s - loss: 133.7133 - loglik: -1.3216e+02 - logprior: -1.5519e+00
Epoch 7/10
13/13 - 1s - loss: 133.5289 - loglik: -1.3203e+02 - logprior: -1.5008e+00
Epoch 8/10
13/13 - 1s - loss: 132.7914 - loglik: -1.3131e+02 - logprior: -1.4807e+00
Epoch 9/10
13/13 - 1s - loss: 132.8173 - loglik: -1.3136e+02 - logprior: -1.4570e+00
Fitted a model with MAP estimate = -132.7816
expansions: [(0, 4), (13, 1), (15, 1), (33, 1), (34, 1), (35, 2), (36, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 139.1333 - loglik: -1.2899e+02 - logprior: -1.0148e+01
Epoch 2/2
13/13 - 1s - loss: 121.8747 - loglik: -1.1878e+02 - logprior: -3.0947e+00
Fitted a model with MAP estimate = -119.0530
expansions: [(0, 2)]
discards: [44 47 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 128.1736 - loglik: -1.1812e+02 - logprior: -1.0053e+01
Epoch 2/2
13/13 - 1s - loss: 118.4485 - loglik: -1.1516e+02 - logprior: -3.2860e+00
Fitted a model with MAP estimate = -116.7385
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 122.7902 - loglik: -1.1486e+02 - logprior: -7.9294e+00
Epoch 2/10
13/13 - 1s - loss: 116.6296 - loglik: -1.1441e+02 - logprior: -2.2157e+00
Epoch 3/10
13/13 - 1s - loss: 115.4293 - loglik: -1.1374e+02 - logprior: -1.6919e+00
Epoch 4/10
13/13 - 1s - loss: 115.1013 - loglik: -1.1376e+02 - logprior: -1.3388e+00
Epoch 5/10
13/13 - 1s - loss: 115.0282 - loglik: -1.1378e+02 - logprior: -1.2440e+00
Epoch 6/10
13/13 - 1s - loss: 114.2984 - loglik: -1.1310e+02 - logprior: -1.1938e+00
Epoch 7/10
13/13 - 1s - loss: 114.4505 - loglik: -1.1329e+02 - logprior: -1.1642e+00
Fitted a model with MAP estimate = -114.3608
Time for alignment: 47.5317
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.8781 - loglik: -2.1429e+02 - logprior: -8.5874e+00
Epoch 2/10
13/13 - 1s - loss: 173.6078 - loglik: -1.7137e+02 - logprior: -2.2343e+00
Epoch 3/10
13/13 - 1s - loss: 144.4636 - loglik: -1.4268e+02 - logprior: -1.7865e+00
Epoch 4/10
13/13 - 1s - loss: 136.5346 - loglik: -1.3490e+02 - logprior: -1.6390e+00
Epoch 5/10
13/13 - 1s - loss: 133.8834 - loglik: -1.3232e+02 - logprior: -1.5646e+00
Epoch 6/10
13/13 - 1s - loss: 133.3013 - loglik: -1.3180e+02 - logprior: -1.4968e+00
Epoch 7/10
13/13 - 1s - loss: 132.8371 - loglik: -1.3137e+02 - logprior: -1.4706e+00
Epoch 8/10
13/13 - 1s - loss: 132.3957 - loglik: -1.3094e+02 - logprior: -1.4587e+00
Epoch 9/10
13/13 - 1s - loss: 131.7487 - loglik: -1.3030e+02 - logprior: -1.4508e+00
Epoch 10/10
13/13 - 1s - loss: 132.3310 - loglik: -1.3090e+02 - logprior: -1.4353e+00
Fitted a model with MAP estimate = -131.8721
expansions: [(0, 4), (13, 1), (16, 1), (34, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 138.2645 - loglik: -1.2808e+02 - logprior: -1.0181e+01
Epoch 2/2
13/13 - 1s - loss: 121.9811 - loglik: -1.1885e+02 - logprior: -3.1347e+00
Fitted a model with MAP estimate = -118.9389
expansions: [(0, 2)]
discards: [43 47 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 127.6980 - loglik: -1.1764e+02 - logprior: -1.0058e+01
Epoch 2/2
13/13 - 1s - loss: 118.1543 - loglik: -1.1489e+02 - logprior: -3.2655e+00
Fitted a model with MAP estimate = -116.7083
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 122.6727 - loglik: -1.1475e+02 - logprior: -7.9271e+00
Epoch 2/10
13/13 - 1s - loss: 116.8406 - loglik: -1.1462e+02 - logprior: -2.2203e+00
Epoch 3/10
13/13 - 1s - loss: 115.7585 - loglik: -1.1407e+02 - logprior: -1.6852e+00
Epoch 4/10
13/13 - 1s - loss: 114.7445 - loglik: -1.1341e+02 - logprior: -1.3373e+00
Epoch 5/10
13/13 - 1s - loss: 114.6835 - loglik: -1.1344e+02 - logprior: -1.2463e+00
Epoch 6/10
13/13 - 1s - loss: 114.6876 - loglik: -1.1350e+02 - logprior: -1.1862e+00
Fitted a model with MAP estimate = -114.4319
Time for alignment: 46.6372
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.9611 - loglik: -2.1437e+02 - logprior: -8.5888e+00
Epoch 2/10
13/13 - 1s - loss: 174.1861 - loglik: -1.7193e+02 - logprior: -2.2559e+00
Epoch 3/10
13/13 - 2s - loss: 144.8321 - loglik: -1.4302e+02 - logprior: -1.8113e+00
Epoch 4/10
13/13 - 1s - loss: 136.5502 - loglik: -1.3491e+02 - logprior: -1.6379e+00
Epoch 5/10
13/13 - 1s - loss: 134.1380 - loglik: -1.3259e+02 - logprior: -1.5497e+00
Epoch 6/10
13/13 - 1s - loss: 133.1562 - loglik: -1.3166e+02 - logprior: -1.4963e+00
Epoch 7/10
13/13 - 1s - loss: 132.8082 - loglik: -1.3133e+02 - logprior: -1.4770e+00
Epoch 8/10
13/13 - 1s - loss: 131.8693 - loglik: -1.3041e+02 - logprior: -1.4633e+00
Epoch 9/10
13/13 - 1s - loss: 132.1817 - loglik: -1.3073e+02 - logprior: -1.4555e+00
Fitted a model with MAP estimate = -131.9339
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 137.9917 - loglik: -1.2782e+02 - logprior: -1.0168e+01
Epoch 2/2
13/13 - 1s - loss: 122.1117 - loglik: -1.1898e+02 - logprior: -3.1308e+00
Fitted a model with MAP estimate = -118.9250
expansions: [(0, 2)]
discards: [43 47 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 127.3145 - loglik: -1.1725e+02 - logprior: -1.0064e+01
Epoch 2/2
13/13 - 1s - loss: 118.5151 - loglik: -1.1525e+02 - logprior: -3.2693e+00
Fitted a model with MAP estimate = -116.6992
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 122.6311 - loglik: -1.1470e+02 - logprior: -7.9265e+00
Epoch 2/10
13/13 - 1s - loss: 116.7518 - loglik: -1.1453e+02 - logprior: -2.2192e+00
Epoch 3/10
13/13 - 1s - loss: 115.6121 - loglik: -1.1392e+02 - logprior: -1.6912e+00
Epoch 4/10
13/13 - 1s - loss: 114.9310 - loglik: -1.1359e+02 - logprior: -1.3403e+00
Epoch 5/10
13/13 - 1s - loss: 114.6709 - loglik: -1.1343e+02 - logprior: -1.2395e+00
Epoch 6/10
13/13 - 1s - loss: 114.5803 - loglik: -1.1339e+02 - logprior: -1.1902e+00
Epoch 7/10
13/13 - 1s - loss: 114.5625 - loglik: -1.1341e+02 - logprior: -1.1530e+00
Epoch 8/10
13/13 - 1s - loss: 114.2681 - loglik: -1.1315e+02 - logprior: -1.1205e+00
Epoch 9/10
13/13 - 1s - loss: 114.0160 - loglik: -1.1293e+02 - logprior: -1.0840e+00
Epoch 10/10
13/13 - 1s - loss: 114.4593 - loglik: -1.1342e+02 - logprior: -1.0350e+00
Fitted a model with MAP estimate = -114.1168
Time for alignment: 51.4823
Computed alignments with likelihoods: ['-114.4043', '-114.4350', '-114.3608', '-114.4319', '-114.1168']
Best model has likelihood: -114.1168  (prior= -1.0193 )
time for generating output: 0.1798
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.4986827248776816
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.2782 - loglik: -5.8449e+01 - logprior: -8.2930e-01
Epoch 2/10
41/41 - 1s - loss: 46.1925 - loglik: -4.5387e+01 - logprior: -8.0588e-01
Epoch 3/10
41/41 - 1s - loss: 45.3577 - loglik: -4.4566e+01 - logprior: -7.9123e-01
Epoch 4/10
41/41 - 1s - loss: 45.4972 - loglik: -4.4710e+01 - logprior: -7.8762e-01
Fitted a model with MAP estimate = -44.7568
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.3471 - loglik: -4.4343e+01 - logprior: -1.0045e+00
Epoch 2/2
41/41 - 1s - loss: 43.4920 - loglik: -4.2724e+01 - logprior: -7.6772e-01
Fitted a model with MAP estimate = -42.8463
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 44.0623 - loglik: -4.3090e+01 - logprior: -9.7193e-01
Epoch 2/2
41/41 - 1s - loss: 43.4330 - loglik: -4.2692e+01 - logprior: -7.4112e-01
Fitted a model with MAP estimate = -42.9038
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7021 - loglik: -4.2095e+01 - logprior: -6.0714e-01
Epoch 2/10
58/58 - 2s - loss: 42.3346 - loglik: -4.1831e+01 - logprior: -5.0406e-01
Epoch 3/10
58/58 - 2s - loss: 42.2495 - loglik: -4.1749e+01 - logprior: -5.0048e-01
Epoch 4/10
58/58 - 2s - loss: 42.0265 - loglik: -4.1529e+01 - logprior: -4.9719e-01
Epoch 5/10
58/58 - 2s - loss: 42.0090 - loglik: -4.1514e+01 - logprior: -4.9531e-01
Epoch 6/10
58/58 - 2s - loss: 42.0237 - loglik: -4.1531e+01 - logprior: -4.9249e-01
Fitted a model with MAP estimate = -41.9209
Time for alignment: 57.1033
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.4571 - loglik: -5.8637e+01 - logprior: -8.1979e-01
Epoch 2/10
41/41 - 1s - loss: 46.1452 - loglik: -4.5335e+01 - logprior: -8.0986e-01
Epoch 3/10
41/41 - 1s - loss: 45.5183 - loglik: -4.4727e+01 - logprior: -7.9119e-01
Epoch 4/10
41/41 - 1s - loss: 45.3878 - loglik: -4.4603e+01 - logprior: -7.8444e-01
Epoch 5/10
41/41 - 1s - loss: 45.1969 - loglik: -4.4416e+01 - logprior: -7.8119e-01
Epoch 6/10
41/41 - 1s - loss: 45.2240 - loglik: -4.4446e+01 - logprior: -7.7814e-01
Fitted a model with MAP estimate = -44.7415
expansions: [(8, 2), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.6498 - loglik: -4.4641e+01 - logprior: -1.0084e+00
Epoch 2/2
41/41 - 1s - loss: 43.4218 - loglik: -4.2648e+01 - logprior: -7.7371e-01
Fitted a model with MAP estimate = -42.8760
expansions: []
discards: [ 8 12 14]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.1678 - loglik: -4.3195e+01 - logprior: -9.7303e-01
Epoch 2/2
41/41 - 1s - loss: 43.3978 - loglik: -4.2657e+01 - logprior: -7.4119e-01
Fitted a model with MAP estimate = -42.8895
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.6841 - loglik: -4.2076e+01 - logprior: -6.0817e-01
Epoch 2/10
58/58 - 2s - loss: 42.3392 - loglik: -4.1835e+01 - logprior: -5.0372e-01
Epoch 3/10
58/58 - 1s - loss: 42.2726 - loglik: -4.1772e+01 - logprior: -5.0024e-01
Epoch 4/10
58/58 - 2s - loss: 42.0484 - loglik: -4.1552e+01 - logprior: -4.9676e-01
Epoch 5/10
58/58 - 2s - loss: 42.0197 - loglik: -4.1524e+01 - logprior: -4.9531e-01
Epoch 6/10
58/58 - 2s - loss: 41.9875 - loglik: -4.1495e+01 - logprior: -4.9233e-01
Epoch 7/10
58/58 - 2s - loss: 41.8347 - loglik: -4.1343e+01 - logprior: -4.9178e-01
Epoch 8/10
58/58 - 2s - loss: 41.9048 - loglik: -4.1415e+01 - logprior: -4.8986e-01
Fitted a model with MAP estimate = -41.8977
Time for alignment: 61.8674
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.1390 - loglik: -5.8307e+01 - logprior: -8.3242e-01
Epoch 2/10
41/41 - 1s - loss: 46.1533 - loglik: -4.5348e+01 - logprior: -8.0483e-01
Epoch 3/10
41/41 - 1s - loss: 45.4894 - loglik: -4.4695e+01 - logprior: -7.9430e-01
Epoch 4/10
41/41 - 1s - loss: 45.4034 - loglik: -4.4619e+01 - logprior: -7.8480e-01
Epoch 5/10
41/41 - 1s - loss: 45.2907 - loglik: -4.4507e+01 - logprior: -7.8353e-01
Epoch 6/10
41/41 - 1s - loss: 45.1748 - loglik: -4.4394e+01 - logprior: -7.8055e-01
Epoch 7/10
41/41 - 1s - loss: 45.1807 - loglik: -4.4405e+01 - logprior: -7.7517e-01
Fitted a model with MAP estimate = -44.8293
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 44.9273 - loglik: -4.3934e+01 - logprior: -9.9368e-01
Epoch 2/2
41/41 - 1s - loss: 43.5302 - loglik: -4.2773e+01 - logprior: -7.5688e-01
Fitted a model with MAP estimate = -42.8588
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.0292 - loglik: -4.3055e+01 - logprior: -9.7422e-01
Epoch 2/2
41/41 - 1s - loss: 43.4317 - loglik: -4.2692e+01 - logprior: -7.3933e-01
Fitted a model with MAP estimate = -42.9085
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7161 - loglik: -4.2109e+01 - logprior: -6.0742e-01
Epoch 2/10
58/58 - 2s - loss: 42.2920 - loglik: -4.1789e+01 - logprior: -5.0345e-01
Epoch 3/10
58/58 - 2s - loss: 42.2689 - loglik: -4.1768e+01 - logprior: -5.0078e-01
Epoch 4/10
58/58 - 2s - loss: 42.0980 - loglik: -4.1600e+01 - logprior: -4.9768e-01
Epoch 5/10
58/58 - 2s - loss: 42.0196 - loglik: -4.1526e+01 - logprior: -4.9397e-01
Epoch 6/10
58/58 - 2s - loss: 41.9450 - loglik: -4.1452e+01 - logprior: -4.9314e-01
Epoch 7/10
58/58 - 2s - loss: 41.8560 - loglik: -4.1365e+01 - logprior: -4.9145e-01
Epoch 8/10
58/58 - 1s - loss: 41.9349 - loglik: -4.1446e+01 - logprior: -4.8937e-01
Fitted a model with MAP estimate = -41.9130
Time for alignment: 63.3050
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.3008 - loglik: -5.8471e+01 - logprior: -8.2985e-01
Epoch 2/10
41/41 - 1s - loss: 46.0857 - loglik: -4.5278e+01 - logprior: -8.0748e-01
Epoch 3/10
41/41 - 1s - loss: 45.4370 - loglik: -4.4644e+01 - logprior: -7.9298e-01
Epoch 4/10
41/41 - 1s - loss: 45.4685 - loglik: -4.4682e+01 - logprior: -7.8608e-01
Fitted a model with MAP estimate = -44.8221
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.0342 - loglik: -4.4038e+01 - logprior: -9.9663e-01
Epoch 2/2
41/41 - 1s - loss: 43.4349 - loglik: -4.2680e+01 - logprior: -7.5538e-01
Fitted a model with MAP estimate = -42.8753
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.0880 - loglik: -4.3113e+01 - logprior: -9.7494e-01
Epoch 2/2
41/41 - 1s - loss: 43.3989 - loglik: -4.2660e+01 - logprior: -7.3908e-01
Fitted a model with MAP estimate = -42.8392
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7618 - loglik: -4.2155e+01 - logprior: -6.0665e-01
Epoch 2/10
58/58 - 2s - loss: 42.2472 - loglik: -4.1742e+01 - logprior: -5.0476e-01
Epoch 3/10
58/58 - 2s - loss: 42.2546 - loglik: -4.1755e+01 - logprior: -4.9999e-01
Fitted a model with MAP estimate = -42.0509
Time for alignment: 51.8109
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.4866 - loglik: -5.8666e+01 - logprior: -8.2082e-01
Epoch 2/10
41/41 - 1s - loss: 46.2174 - loglik: -4.5411e+01 - logprior: -8.0647e-01
Epoch 3/10
41/41 - 1s - loss: 45.4011 - loglik: -4.4608e+01 - logprior: -7.9289e-01
Epoch 4/10
41/41 - 1s - loss: 45.5080 - loglik: -4.4721e+01 - logprior: -7.8721e-01
Fitted a model with MAP estimate = -44.7674
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.3274 - loglik: -4.4325e+01 - logprior: -1.0025e+00
Epoch 2/2
41/41 - 1s - loss: 43.4934 - loglik: -4.2725e+01 - logprior: -7.6826e-01
Fitted a model with MAP estimate = -42.8810
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.1166 - loglik: -4.3142e+01 - logprior: -9.7494e-01
Epoch 2/2
41/41 - 1s - loss: 43.3932 - loglik: -4.2654e+01 - logprior: -7.3948e-01
Fitted a model with MAP estimate = -42.9224
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7186 - loglik: -4.2111e+01 - logprior: -6.0804e-01
Epoch 2/10
58/58 - 1s - loss: 42.2470 - loglik: -4.1743e+01 - logprior: -5.0379e-01
Epoch 3/10
58/58 - 2s - loss: 42.3098 - loglik: -4.1810e+01 - logprior: -4.9968e-01
Fitted a model with MAP estimate = -42.0474
Time for alignment: 52.3477
Computed alignments with likelihoods: ['-41.9209', '-41.8977', '-41.9130', '-42.0509', '-42.0474']
Best model has likelihood: -41.8977  (prior= -0.4879 )
time for generating output: 0.0844
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0221 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 168.7343 - loglik: -1.3494e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 133.8668 - loglik: -1.1783e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 118.0523 - loglik: -1.0843e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 1s - loss: 110.1251 - loglik: -1.0406e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 106.4277 - loglik: -1.0244e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 0s - loss: 104.6535 - loglik: -1.0201e+02 - logprior: -2.6386e+00
Epoch 8/10
10/10 - 0s - loss: 103.5448 - loglik: -1.0180e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 102.4220 - loglik: -1.0128e+02 - logprior: -1.1392e+00
Epoch 10/10
10/10 - 0s - loss: 101.3378 - loglik: -1.0064e+02 - logprior: -7.0207e-01
Fitted a model with MAP estimate = -101.0354
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 239.2161 - loglik: -1.0020e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 0s - loss: 152.1716 - loglik: -9.4674e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.9168
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 203.7162 - loglik: -9.2179e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 120.8386 - loglik: -9.0844e+01 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -108.7097
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 228.4208 - loglik: -9.2702e+01 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 0s - loss: 139.4945 - loglik: -9.1691e+01 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 0s - loss: 109.9485 - loglik: -9.1452e+01 - logprior: -1.8497e+01
Epoch 4/10
10/10 - 0s - loss: 98.0830 - loglik: -9.1393e+01 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 1s - loss: 93.4441 - loglik: -9.1386e+01 - logprior: -2.0582e+00
Epoch 6/10
10/10 - 0s - loss: 91.1123 - loglik: -9.1417e+01 - logprior: 0.3049
Epoch 7/10
10/10 - 0s - loss: 89.8008 - loglik: -9.1502e+01 - logprior: 1.7012
Epoch 8/10
10/10 - 0s - loss: 88.9649 - loglik: -9.1621e+01 - logprior: 2.6560
Epoch 9/10
10/10 - 0s - loss: 88.3647 - loglik: -9.1761e+01 - logprior: 3.3966
Epoch 10/10
10/10 - 0s - loss: 87.8931 - loglik: -9.1910e+01 - logprior: 4.0164
Fitted a model with MAP estimate = -87.6654
Time for alignment: 27.8591
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.0221 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7343 - loglik: -1.3494e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 1s - loss: 133.8668 - loglik: -1.1783e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 1s - loss: 118.0523 - loglik: -1.0843e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 1s - loss: 110.1251 - loglik: -1.0406e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 106.4277 - loglik: -1.0244e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 1s - loss: 104.6535 - loglik: -1.0201e+02 - logprior: -2.6386e+00
Epoch 8/10
10/10 - 0s - loss: 103.5448 - loglik: -1.0180e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 102.4221 - loglik: -1.0128e+02 - logprior: -1.1392e+00
Epoch 10/10
10/10 - 0s - loss: 101.3378 - loglik: -1.0064e+02 - logprior: -7.0207e-01
Fitted a model with MAP estimate = -101.0356
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.2161 - loglik: -1.0020e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.1716 - loglik: -9.4674e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.9168
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7162 - loglik: -9.2179e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 120.8387 - loglik: -9.0844e+01 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -108.7097
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.4208 - loglik: -9.2702e+01 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 0s - loss: 139.4946 - loglik: -9.1691e+01 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 109.9485 - loglik: -9.1452e+01 - logprior: -1.8497e+01
Epoch 4/10
10/10 - 1s - loss: 98.0830 - loglik: -9.1393e+01 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 1s - loss: 93.4441 - loglik: -9.1386e+01 - logprior: -2.0582e+00
Epoch 6/10
10/10 - 1s - loss: 91.1123 - loglik: -9.1417e+01 - logprior: 0.3049
Epoch 7/10
10/10 - 0s - loss: 89.8008 - loglik: -9.1502e+01 - logprior: 1.7012
Epoch 8/10
10/10 - 1s - loss: 88.9649 - loglik: -9.1621e+01 - logprior: 2.6560
Epoch 9/10
10/10 - 0s - loss: 88.3647 - loglik: -9.1761e+01 - logprior: 3.3966
Epoch 10/10
10/10 - 1s - loss: 87.8932 - loglik: -9.1910e+01 - logprior: 4.0164
Fitted a model with MAP estimate = -87.6656
Time for alignment: 28.0476
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0221 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7343 - loglik: -1.3494e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 133.8668 - loglik: -1.1783e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 118.0523 - loglik: -1.0843e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 110.1251 - loglik: -1.0406e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 106.4277 - loglik: -1.0244e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 1s - loss: 104.6535 - loglik: -1.0201e+02 - logprior: -2.6386e+00
Epoch 8/10
10/10 - 1s - loss: 103.5447 - loglik: -1.0180e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 102.4220 - loglik: -1.0128e+02 - logprior: -1.1392e+00
Epoch 10/10
10/10 - 0s - loss: 101.3378 - loglik: -1.0064e+02 - logprior: -7.0207e-01
Fitted a model with MAP estimate = -101.0355
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 239.2161 - loglik: -1.0020e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.1716 - loglik: -9.4674e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.9168
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7162 - loglik: -9.2179e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 120.8387 - loglik: -9.0844e+01 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -108.7097
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.4208 - loglik: -9.2702e+01 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 1s - loss: 139.4945 - loglik: -9.1691e+01 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 109.9485 - loglik: -9.1452e+01 - logprior: -1.8497e+01
Epoch 4/10
10/10 - 0s - loss: 98.0830 - loglik: -9.1393e+01 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 93.4441 - loglik: -9.1386e+01 - logprior: -2.0582e+00
Epoch 6/10
10/10 - 1s - loss: 91.1124 - loglik: -9.1417e+01 - logprior: 0.3049
Epoch 7/10
10/10 - 0s - loss: 89.8008 - loglik: -9.1502e+01 - logprior: 1.7012
Epoch 8/10
10/10 - 1s - loss: 88.9649 - loglik: -9.1621e+01 - logprior: 2.6560
Epoch 9/10
10/10 - 0s - loss: 88.3647 - loglik: -9.1761e+01 - logprior: 3.3966
Epoch 10/10
10/10 - 1s - loss: 87.8931 - loglik: -9.1910e+01 - logprior: 4.0164
Fitted a model with MAP estimate = -87.6654
Time for alignment: 28.1736
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0221 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 168.7343 - loglik: -1.3494e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 133.8668 - loglik: -1.1783e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 118.0522 - loglik: -1.0843e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 110.1251 - loglik: -1.0406e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 106.4277 - loglik: -1.0244e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 0s - loss: 104.6535 - loglik: -1.0201e+02 - logprior: -2.6386e+00
Epoch 8/10
10/10 - 0s - loss: 103.5447 - loglik: -1.0180e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 102.4220 - loglik: -1.0128e+02 - logprior: -1.1392e+00
Epoch 10/10
10/10 - 0s - loss: 101.3379 - loglik: -1.0064e+02 - logprior: -7.0207e-01
Fitted a model with MAP estimate = -101.0355
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.2161 - loglik: -1.0020e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.1716 - loglik: -9.4674e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.9168
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7161 - loglik: -9.2179e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 120.8387 - loglik: -9.0844e+01 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -108.7097
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.4208 - loglik: -9.2702e+01 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 1s - loss: 139.4945 - loglik: -9.1691e+01 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 109.9485 - loglik: -9.1452e+01 - logprior: -1.8497e+01
Epoch 4/10
10/10 - 0s - loss: 98.0830 - loglik: -9.1393e+01 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 93.4442 - loglik: -9.1386e+01 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 0s - loss: 91.1124 - loglik: -9.1417e+01 - logprior: 0.3049
Epoch 7/10
10/10 - 0s - loss: 89.8009 - loglik: -9.1502e+01 - logprior: 1.7012
Epoch 8/10
10/10 - 0s - loss: 88.9649 - loglik: -9.1621e+01 - logprior: 2.6560
Epoch 9/10
10/10 - 0s - loss: 88.3647 - loglik: -9.1761e+01 - logprior: 3.3966
Epoch 10/10
10/10 - 1s - loss: 87.8931 - loglik: -9.1910e+01 - logprior: 4.0164
Fitted a model with MAP estimate = -87.6655
Time for alignment: 27.3415
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0221 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7343 - loglik: -1.3494e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 133.8668 - loglik: -1.1783e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 118.0522 - loglik: -1.0843e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 110.1251 - loglik: -1.0406e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 106.4277 - loglik: -1.0244e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 1s - loss: 104.6535 - loglik: -1.0201e+02 - logprior: -2.6386e+00
Epoch 8/10
10/10 - 0s - loss: 103.5447 - loglik: -1.0180e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 102.4220 - loglik: -1.0128e+02 - logprior: -1.1392e+00
Epoch 10/10
10/10 - 1s - loss: 101.3378 - loglik: -1.0064e+02 - logprior: -7.0207e-01
Fitted a model with MAP estimate = -101.0355
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.2161 - loglik: -1.0020e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.1716 - loglik: -9.4674e+01 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -137.9168
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7162 - loglik: -9.2179e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 120.8386 - loglik: -9.0844e+01 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -108.7097
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.4208 - loglik: -9.2702e+01 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 0s - loss: 139.4945 - loglik: -9.1691e+01 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 109.9485 - loglik: -9.1452e+01 - logprior: -1.8497e+01
Epoch 4/10
10/10 - 1s - loss: 98.0830 - loglik: -9.1393e+01 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 93.4441 - loglik: -9.1386e+01 - logprior: -2.0582e+00
Epoch 6/10
10/10 - 1s - loss: 91.1124 - loglik: -9.1417e+01 - logprior: 0.3049
Epoch 7/10
10/10 - 1s - loss: 89.8009 - loglik: -9.1502e+01 - logprior: 1.7012
Epoch 8/10
10/10 - 1s - loss: 88.9649 - loglik: -9.1621e+01 - logprior: 2.6560
Epoch 9/10
10/10 - 1s - loss: 88.3647 - loglik: -9.1761e+01 - logprior: 3.3966
Epoch 10/10
10/10 - 1s - loss: 87.8931 - loglik: -9.1910e+01 - logprior: 4.0164
Fitted a model with MAP estimate = -87.6654
Time for alignment: 27.7915
Computed alignments with likelihoods: ['-87.6654', '-87.6656', '-87.6654', '-87.6655', '-87.6654']
Best model has likelihood: -87.6654  (prior= 4.3184 )
time for generating output: 0.0994
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8919239904988123
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.1919 - loglik: -1.9208e+02 - logprior: -3.1128e+00
Epoch 2/10
19/19 - 1s - loss: 155.0161 - loglik: -1.5385e+02 - logprior: -1.1661e+00
Epoch 3/10
19/19 - 1s - loss: 141.5084 - loglik: -1.4026e+02 - logprior: -1.2528e+00
Epoch 4/10
19/19 - 1s - loss: 139.5290 - loglik: -1.3832e+02 - logprior: -1.2057e+00
Epoch 5/10
19/19 - 1s - loss: 139.2116 - loglik: -1.3806e+02 - logprior: -1.1549e+00
Epoch 6/10
19/19 - 1s - loss: 138.7781 - loglik: -1.3765e+02 - logprior: -1.1315e+00
Epoch 7/10
19/19 - 1s - loss: 138.8032 - loglik: -1.3770e+02 - logprior: -1.1059e+00
Fitted a model with MAP estimate = -138.9438
expansions: [(0, 3), (13, 2), (14, 2), (15, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.7066 - loglik: -1.3749e+02 - logprior: -4.2128e+00
Epoch 2/2
19/19 - 1s - loss: 131.9727 - loglik: -1.3053e+02 - logprior: -1.4438e+00
Fitted a model with MAP estimate = -132.0003
expansions: [(0, 2)]
discards: [19 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 134.1499 - loglik: -1.3014e+02 - logprior: -4.0080e+00
Epoch 2/2
19/19 - 1s - loss: 128.8175 - loglik: -1.2759e+02 - logprior: -1.2289e+00
Fitted a model with MAP estimate = -128.9458
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.7929 - loglik: -1.2944e+02 - logprior: -3.3545e+00
Epoch 2/10
21/21 - 1s - loss: 129.2220 - loglik: -1.2780e+02 - logprior: -1.4245e+00
Epoch 3/10
21/21 - 1s - loss: 128.2322 - loglik: -1.2717e+02 - logprior: -1.0605e+00
Epoch 4/10
21/21 - 1s - loss: 127.6316 - loglik: -1.2661e+02 - logprior: -1.0168e+00
Epoch 5/10
21/21 - 1s - loss: 127.5079 - loglik: -1.2650e+02 - logprior: -1.0126e+00
Epoch 6/10
21/21 - 1s - loss: 127.2684 - loglik: -1.2628e+02 - logprior: -9.8907e-01
Epoch 7/10
21/21 - 1s - loss: 127.1042 - loglik: -1.2613e+02 - logprior: -9.6988e-01
Epoch 8/10
21/21 - 1s - loss: 127.1292 - loglik: -1.2618e+02 - logprior: -9.4700e-01
Fitted a model with MAP estimate = -127.0707
Time for alignment: 46.9124
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.5102 - loglik: -1.9240e+02 - logprior: -3.1148e+00
Epoch 2/10
19/19 - 1s - loss: 156.9343 - loglik: -1.5577e+02 - logprior: -1.1670e+00
Epoch 3/10
19/19 - 1s - loss: 142.3617 - loglik: -1.4111e+02 - logprior: -1.2550e+00
Epoch 4/10
19/19 - 1s - loss: 139.9406 - loglik: -1.3872e+02 - logprior: -1.2228e+00
Epoch 5/10
19/19 - 1s - loss: 139.2567 - loglik: -1.3808e+02 - logprior: -1.1721e+00
Epoch 6/10
19/19 - 1s - loss: 138.7548 - loglik: -1.3760e+02 - logprior: -1.1513e+00
Epoch 7/10
19/19 - 1s - loss: 138.7221 - loglik: -1.3759e+02 - logprior: -1.1340e+00
Epoch 8/10
19/19 - 1s - loss: 138.5291 - loglik: -1.3741e+02 - logprior: -1.1145e+00
Epoch 9/10
19/19 - 1s - loss: 138.4720 - loglik: -1.3737e+02 - logprior: -1.1046e+00
Epoch 10/10
19/19 - 1s - loss: 138.5316 - loglik: -1.3743e+02 - logprior: -1.1056e+00
Fitted a model with MAP estimate = -138.9696
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.4755 - loglik: -1.3713e+02 - logprior: -4.3406e+00
Epoch 2/2
19/19 - 1s - loss: 131.7052 - loglik: -1.3028e+02 - logprior: -1.4255e+00
Fitted a model with MAP estimate = -131.8677
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.0220 - loglik: -1.3005e+02 - logprior: -3.9694e+00
Epoch 2/2
19/19 - 1s - loss: 129.2546 - loglik: -1.2796e+02 - logprior: -1.2960e+00
Fitted a model with MAP estimate = -129.8892
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.6241 - loglik: -1.3025e+02 - logprior: -3.3781e+00
Epoch 2/10
21/21 - 1s - loss: 130.3253 - loglik: -1.2876e+02 - logprior: -1.5619e+00
Epoch 3/10
21/21 - 1s - loss: 129.0986 - loglik: -1.2804e+02 - logprior: -1.0606e+00
Epoch 4/10
21/21 - 1s - loss: 128.4679 - loglik: -1.2746e+02 - logprior: -1.0088e+00
Epoch 5/10
21/21 - 1s - loss: 128.0842 - loglik: -1.2708e+02 - logprior: -1.0073e+00
Epoch 6/10
21/21 - 1s - loss: 128.2897 - loglik: -1.2731e+02 - logprior: -9.8045e-01
Fitted a model with MAP estimate = -127.9514
Time for alignment: 46.6678
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.5570 - loglik: -1.9244e+02 - logprior: -3.1160e+00
Epoch 2/10
19/19 - 1s - loss: 158.1689 - loglik: -1.5699e+02 - logprior: -1.1764e+00
Epoch 3/10
19/19 - 1s - loss: 142.6424 - loglik: -1.4138e+02 - logprior: -1.2619e+00
Epoch 4/10
19/19 - 1s - loss: 139.7458 - loglik: -1.3852e+02 - logprior: -1.2306e+00
Epoch 5/10
19/19 - 1s - loss: 139.0865 - loglik: -1.3791e+02 - logprior: -1.1778e+00
Epoch 6/10
19/19 - 1s - loss: 138.7893 - loglik: -1.3764e+02 - logprior: -1.1519e+00
Epoch 7/10
19/19 - 1s - loss: 138.7398 - loglik: -1.3761e+02 - logprior: -1.1304e+00
Epoch 8/10
19/19 - 1s - loss: 138.5508 - loglik: -1.3743e+02 - logprior: -1.1180e+00
Epoch 9/10
19/19 - 1s - loss: 138.5043 - loglik: -1.3740e+02 - logprior: -1.1070e+00
Epoch 10/10
19/19 - 1s - loss: 138.5501 - loglik: -1.3745e+02 - logprior: -1.1035e+00
Fitted a model with MAP estimate = -138.8539
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (24, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 141.4919 - loglik: -1.3714e+02 - logprior: -4.3505e+00
Epoch 2/2
19/19 - 1s - loss: 131.7531 - loglik: -1.3033e+02 - logprior: -1.4232e+00
Fitted a model with MAP estimate = -131.8548
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.9056 - loglik: -1.2994e+02 - logprior: -3.9662e+00
Epoch 2/2
19/19 - 1s - loss: 129.1778 - loglik: -1.2787e+02 - logprior: -1.3033e+00
Fitted a model with MAP estimate = -129.6669
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.5226 - loglik: -1.3014e+02 - logprior: -3.3778e+00
Epoch 2/10
21/21 - 1s - loss: 130.2074 - loglik: -1.2862e+02 - logprior: -1.5862e+00
Epoch 3/10
21/21 - 1s - loss: 129.0658 - loglik: -1.2800e+02 - logprior: -1.0663e+00
Epoch 4/10
21/21 - 1s - loss: 128.5593 - loglik: -1.2755e+02 - logprior: -1.0048e+00
Epoch 5/10
21/21 - 1s - loss: 128.1741 - loglik: -1.2716e+02 - logprior: -1.0099e+00
Epoch 6/10
21/21 - 1s - loss: 127.9717 - loglik: -1.2698e+02 - logprior: -9.8781e-01
Epoch 7/10
21/21 - 1s - loss: 127.9936 - loglik: -1.2703e+02 - logprior: -9.6291e-01
Fitted a model with MAP estimate = -127.8722
Time for alignment: 48.5183
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.2943 - loglik: -1.9218e+02 - logprior: -3.1138e+00
Epoch 2/10
19/19 - 1s - loss: 156.1356 - loglik: -1.5499e+02 - logprior: -1.1489e+00
Epoch 3/10
19/19 - 1s - loss: 142.9962 - loglik: -1.4178e+02 - logprior: -1.2134e+00
Epoch 4/10
19/19 - 1s - loss: 140.9266 - loglik: -1.3976e+02 - logprior: -1.1647e+00
Epoch 5/10
19/19 - 1s - loss: 140.2203 - loglik: -1.3911e+02 - logprior: -1.1064e+00
Epoch 6/10
19/19 - 1s - loss: 140.0660 - loglik: -1.3897e+02 - logprior: -1.0960e+00
Epoch 7/10
19/19 - 1s - loss: 139.6968 - loglik: -1.3862e+02 - logprior: -1.0784e+00
Epoch 8/10
19/19 - 1s - loss: 139.6994 - loglik: -1.3863e+02 - logprior: -1.0658e+00
Fitted a model with MAP estimate = -139.9908
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 140.6303 - loglik: -1.3639e+02 - logprior: -4.2389e+00
Epoch 2/2
19/19 - 1s - loss: 131.8167 - loglik: -1.3043e+02 - logprior: -1.3882e+00
Fitted a model with MAP estimate = -131.9699
expansions: [(0, 2)]
discards: [19 49]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.1397 - loglik: -1.3013e+02 - logprior: -4.0054e+00
Epoch 2/2
19/19 - 1s - loss: 129.1152 - loglik: -1.2788e+02 - logprior: -1.2393e+00
Fitted a model with MAP estimate = -129.0777
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.7747 - loglik: -1.2943e+02 - logprior: -3.3490e+00
Epoch 2/10
21/21 - 1s - loss: 129.2069 - loglik: -1.2782e+02 - logprior: -1.3830e+00
Epoch 3/10
21/21 - 1s - loss: 128.3404 - loglik: -1.2727e+02 - logprior: -1.0677e+00
Epoch 4/10
21/21 - 1s - loss: 127.5728 - loglik: -1.2655e+02 - logprior: -1.0237e+00
Epoch 5/10
21/21 - 1s - loss: 127.6180 - loglik: -1.2661e+02 - logprior: -1.0099e+00
Fitted a model with MAP estimate = -127.2687
Time for alignment: 43.5428
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.2758 - loglik: -1.9216e+02 - logprior: -3.1134e+00
Epoch 2/10
19/19 - 1s - loss: 155.1638 - loglik: -1.5401e+02 - logprior: -1.1552e+00
Epoch 3/10
19/19 - 1s - loss: 141.6233 - loglik: -1.4039e+02 - logprior: -1.2374e+00
Epoch 4/10
19/19 - 1s - loss: 139.7096 - loglik: -1.3851e+02 - logprior: -1.2039e+00
Epoch 5/10
19/19 - 1s - loss: 139.1811 - loglik: -1.3803e+02 - logprior: -1.1511e+00
Epoch 6/10
19/19 - 1s - loss: 138.8587 - loglik: -1.3773e+02 - logprior: -1.1284e+00
Epoch 7/10
19/19 - 1s - loss: 138.5070 - loglik: -1.3740e+02 - logprior: -1.1041e+00
Epoch 8/10
19/19 - 1s - loss: 138.6780 - loglik: -1.3759e+02 - logprior: -1.0911e+00
Fitted a model with MAP estimate = -138.9511
expansions: [(0, 3), (13, 2), (14, 2), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 142.5396 - loglik: -1.3827e+02 - logprior: -4.2703e+00
Epoch 2/2
19/19 - 1s - loss: 132.2195 - loglik: -1.3074e+02 - logprior: -1.4773e+00
Fitted a model with MAP estimate = -131.9407
expansions: [(0, 2)]
discards: [19 28 50 61 66]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.0731 - loglik: -1.3005e+02 - logprior: -4.0209e+00
Epoch 2/2
19/19 - 1s - loss: 128.9150 - loglik: -1.2766e+02 - logprior: -1.2527e+00
Fitted a model with MAP estimate = -128.9986
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.4017 - loglik: -1.3002e+02 - logprior: -3.3777e+00
Epoch 2/10
21/21 - 1s - loss: 130.0792 - loglik: -1.2860e+02 - logprior: -1.4748e+00
Epoch 3/10
21/21 - 1s - loss: 129.0098 - loglik: -1.2794e+02 - logprior: -1.0654e+00
Epoch 4/10
21/21 - 1s - loss: 128.4677 - loglik: -1.2745e+02 - logprior: -1.0218e+00
Epoch 5/10
21/21 - 1s - loss: 128.1922 - loglik: -1.2717e+02 - logprior: -1.0205e+00
Epoch 6/10
21/21 - 1s - loss: 128.0812 - loglik: -1.2708e+02 - logprior: -9.9624e-01
Epoch 7/10
21/21 - 1s - loss: 127.8271 - loglik: -1.2685e+02 - logprior: -9.7667e-01
Epoch 8/10
21/21 - 1s - loss: 128.0449 - loglik: -1.2709e+02 - logprior: -9.5897e-01
Fitted a model with MAP estimate = -127.8344
Time for alignment: 46.6874
Computed alignments with likelihoods: ['-127.0707', '-127.9514', '-127.8722', '-127.2687', '-127.8344']
Best model has likelihood: -127.0707  (prior= -0.9564 )
time for generating output: 0.1233
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9451029320024953
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.7143 - loglik: -3.8261e+02 - logprior: -8.1080e+00
Epoch 2/10
13/13 - 2s - loss: 339.3952 - loglik: -3.3754e+02 - logprior: -1.8583e+00
Epoch 3/10
13/13 - 2s - loss: 298.8612 - loglik: -2.9718e+02 - logprior: -1.6818e+00
Epoch 4/10
13/13 - 2s - loss: 285.9442 - loglik: -2.8404e+02 - logprior: -1.9053e+00
Epoch 5/10
13/13 - 2s - loss: 280.4106 - loglik: -2.7846e+02 - logprior: -1.9548e+00
Epoch 6/10
13/13 - 2s - loss: 277.6676 - loglik: -2.7573e+02 - logprior: -1.9384e+00
Epoch 7/10
13/13 - 2s - loss: 277.5528 - loglik: -2.7562e+02 - logprior: -1.9347e+00
Epoch 8/10
13/13 - 2s - loss: 276.3754 - loglik: -2.7445e+02 - logprior: -1.9263e+00
Epoch 9/10
13/13 - 2s - loss: 277.0538 - loglik: -2.7513e+02 - logprior: -1.9243e+00
Fitted a model with MAP estimate = -276.5315
expansions: [(7, 2), (8, 3), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 283.8730 - loglik: -2.7439e+02 - logprior: -9.4798e+00
Epoch 2/2
13/13 - 2s - loss: 265.4780 - loglik: -2.6153e+02 - logprior: -3.9450e+00
Fitted a model with MAP estimate = -262.3551
expansions: [(0, 3)]
discards: [ 0  9 94]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 266.9153 - loglik: -2.5958e+02 - logprior: -7.3316e+00
Epoch 2/2
13/13 - 2s - loss: 258.0309 - loglik: -2.5629e+02 - logprior: -1.7398e+00
Fitted a model with MAP estimate = -256.5572
expansions: []
discards: [ 0  2 89]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.5163 - loglik: -2.6034e+02 - logprior: -9.1735e+00
Epoch 2/10
13/13 - 2s - loss: 260.5721 - loglik: -2.5780e+02 - logprior: -2.7730e+00
Epoch 3/10
13/13 - 2s - loss: 257.1197 - loglik: -2.5618e+02 - logprior: -9.3603e-01
Epoch 4/10
13/13 - 2s - loss: 255.2837 - loglik: -2.5489e+02 - logprior: -3.9694e-01
Epoch 5/10
13/13 - 2s - loss: 255.3434 - loglik: -2.5506e+02 - logprior: -2.8691e-01
Fitted a model with MAP estimate = -254.4407
Time for alignment: 65.5694
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.9086 - loglik: -3.8280e+02 - logprior: -8.1042e+00
Epoch 2/10
13/13 - 2s - loss: 338.0285 - loglik: -3.3616e+02 - logprior: -1.8665e+00
Epoch 3/10
13/13 - 2s - loss: 299.4412 - loglik: -2.9771e+02 - logprior: -1.7272e+00
Epoch 4/10
13/13 - 2s - loss: 284.3731 - loglik: -2.8239e+02 - logprior: -1.9806e+00
Epoch 5/10
13/13 - 2s - loss: 280.2222 - loglik: -2.7820e+02 - logprior: -2.0181e+00
Epoch 6/10
13/13 - 2s - loss: 277.5617 - loglik: -2.7562e+02 - logprior: -1.9464e+00
Epoch 7/10
13/13 - 2s - loss: 277.0187 - loglik: -2.7509e+02 - logprior: -1.9303e+00
Epoch 8/10
13/13 - 2s - loss: 276.6991 - loglik: -2.7476e+02 - logprior: -1.9421e+00
Epoch 9/10
13/13 - 2s - loss: 276.5431 - loglik: -2.7461e+02 - logprior: -1.9315e+00
Epoch 10/10
13/13 - 2s - loss: 276.0006 - loglik: -2.7407e+02 - logprior: -1.9282e+00
Fitted a model with MAP estimate = -276.1453
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (63, 1), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 282.4951 - loglik: -2.7305e+02 - logprior: -9.4480e+00
Epoch 2/2
13/13 - 2s - loss: 265.8211 - loglik: -2.6207e+02 - logprior: -3.7465e+00
Fitted a model with MAP estimate = -262.7626
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 267.2811 - loglik: -2.5998e+02 - logprior: -7.2987e+00
Epoch 2/2
13/13 - 2s - loss: 258.0899 - loglik: -2.5639e+02 - logprior: -1.6965e+00
Fitted a model with MAP estimate = -256.8933
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 268.7513 - loglik: -2.5957e+02 - logprior: -9.1766e+00
Epoch 2/10
13/13 - 2s - loss: 260.9002 - loglik: -2.5816e+02 - logprior: -2.7374e+00
Epoch 3/10
13/13 - 2s - loss: 256.8155 - loglik: -2.5592e+02 - logprior: -8.9949e-01
Epoch 4/10
13/13 - 2s - loss: 256.4873 - loglik: -2.5615e+02 - logprior: -3.3945e-01
Epoch 5/10
13/13 - 2s - loss: 255.2148 - loglik: -2.5499e+02 - logprior: -2.2813e-01
Epoch 6/10
13/13 - 2s - loss: 254.4208 - loglik: -2.5424e+02 - logprior: -1.8365e-01
Epoch 7/10
13/13 - 2s - loss: 254.8738 - loglik: -2.5470e+02 - logprior: -1.7065e-01
Fitted a model with MAP estimate = -253.8522
Time for alignment: 71.1021
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.2088 - loglik: -3.8310e+02 - logprior: -8.1045e+00
Epoch 2/10
13/13 - 2s - loss: 337.4365 - loglik: -3.3557e+02 - logprior: -1.8701e+00
Epoch 3/10
13/13 - 2s - loss: 296.3260 - loglik: -2.9459e+02 - logprior: -1.7381e+00
Epoch 4/10
13/13 - 2s - loss: 284.8283 - loglik: -2.8285e+02 - logprior: -1.9744e+00
Epoch 5/10
13/13 - 2s - loss: 279.7346 - loglik: -2.7772e+02 - logprior: -2.0122e+00
Epoch 6/10
13/13 - 2s - loss: 278.6664 - loglik: -2.7673e+02 - logprior: -1.9414e+00
Epoch 7/10
13/13 - 2s - loss: 277.6359 - loglik: -2.7571e+02 - logprior: -1.9231e+00
Epoch 8/10
13/13 - 2s - loss: 277.2538 - loglik: -2.7531e+02 - logprior: -1.9394e+00
Epoch 9/10
13/13 - 2s - loss: 276.8883 - loglik: -2.7496e+02 - logprior: -1.9291e+00
Epoch 10/10
13/13 - 2s - loss: 276.6327 - loglik: -2.7470e+02 - logprior: -1.9360e+00
Fitted a model with MAP estimate = -276.8654
expansions: [(7, 2), (8, 3), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (52, 1), (53, 2), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 284.9330 - loglik: -2.7541e+02 - logprior: -9.5266e+00
Epoch 2/2
13/13 - 2s - loss: 265.3928 - loglik: -2.6133e+02 - logprior: -4.0587e+00
Fitted a model with MAP estimate = -262.0985
expansions: [(0, 2)]
discards: [ 0  9 69 89 90 96]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.8672 - loglik: -2.5960e+02 - logprior: -7.2704e+00
Epoch 2/2
13/13 - 2s - loss: 258.4135 - loglik: -2.5669e+02 - logprior: -1.7282e+00
Fitted a model with MAP estimate = -256.7287
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 269.9165 - loglik: -2.6069e+02 - logprior: -9.2265e+00
Epoch 2/10
13/13 - 2s - loss: 260.4916 - loglik: -2.5756e+02 - logprior: -2.9308e+00
Epoch 3/10
13/13 - 2s - loss: 257.5688 - loglik: -2.5660e+02 - logprior: -9.6675e-01
Epoch 4/10
13/13 - 2s - loss: 255.9141 - loglik: -2.5554e+02 - logprior: -3.7281e-01
Epoch 5/10
13/13 - 2s - loss: 255.0752 - loglik: -2.5481e+02 - logprior: -2.6218e-01
Epoch 6/10
13/13 - 2s - loss: 254.4832 - loglik: -2.5423e+02 - logprior: -2.5481e-01
Epoch 7/10
13/13 - 2s - loss: 254.0215 - loglik: -2.5378e+02 - logprior: -2.4375e-01
Epoch 8/10
13/13 - 2s - loss: 253.6946 - loglik: -2.5344e+02 - logprior: -2.5087e-01
Epoch 9/10
13/13 - 2s - loss: 253.7410 - loglik: -2.5351e+02 - logprior: -2.2596e-01
Fitted a model with MAP estimate = -253.4239
Time for alignment: 76.7316
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.3240 - loglik: -3.8322e+02 - logprior: -8.1008e+00
Epoch 2/10
13/13 - 2s - loss: 338.3997 - loglik: -3.3654e+02 - logprior: -1.8628e+00
Epoch 3/10
13/13 - 2s - loss: 296.3481 - loglik: -2.9465e+02 - logprior: -1.6965e+00
Epoch 4/10
13/13 - 2s - loss: 282.9398 - loglik: -2.8103e+02 - logprior: -1.9143e+00
Epoch 5/10
13/13 - 2s - loss: 279.9741 - loglik: -2.7803e+02 - logprior: -1.9411e+00
Epoch 6/10
13/13 - 2s - loss: 278.5240 - loglik: -2.7669e+02 - logprior: -1.8356e+00
Epoch 7/10
13/13 - 2s - loss: 278.0534 - loglik: -2.7621e+02 - logprior: -1.8410e+00
Epoch 8/10
13/13 - 2s - loss: 276.7222 - loglik: -2.7487e+02 - logprior: -1.8535e+00
Epoch 9/10
13/13 - 2s - loss: 276.8767 - loglik: -2.7503e+02 - logprior: -1.8463e+00
Fitted a model with MAP estimate = -276.9560
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 3), (75, 2), (76, 4), (99, 5), (100, 2), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 285.3317 - loglik: -2.7579e+02 - logprior: -9.5442e+00
Epoch 2/2
13/13 - 3s - loss: 265.2166 - loglik: -2.6108e+02 - logprior: -4.1335e+00
Fitted a model with MAP estimate = -262.1996
expansions: [(0, 2)]
discards: [  0   9  68  88  89  95 133]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 266.8633 - loglik: -2.5961e+02 - logprior: -7.2543e+00
Epoch 2/2
13/13 - 2s - loss: 258.8275 - loglik: -2.5711e+02 - logprior: -1.7200e+00
Fitted a model with MAP estimate = -256.9078
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 269.3261 - loglik: -2.6010e+02 - logprior: -9.2296e+00
Epoch 2/10
13/13 - 2s - loss: 260.7439 - loglik: -2.5780e+02 - logprior: -2.9415e+00
Epoch 3/10
13/13 - 2s - loss: 256.8501 - loglik: -2.5586e+02 - logprior: -9.8531e-01
Epoch 4/10
13/13 - 2s - loss: 256.4613 - loglik: -2.5606e+02 - logprior: -4.0471e-01
Epoch 5/10
13/13 - 2s - loss: 255.1037 - loglik: -2.5482e+02 - logprior: -2.8580e-01
Epoch 6/10
13/13 - 2s - loss: 254.2164 - loglik: -2.5396e+02 - logprior: -2.5620e-01
Epoch 7/10
13/13 - 2s - loss: 254.0307 - loglik: -2.5376e+02 - logprior: -2.6638e-01
Epoch 8/10
13/13 - 2s - loss: 253.7937 - loglik: -2.5353e+02 - logprior: -2.6823e-01
Epoch 9/10
13/13 - 2s - loss: 254.1548 - loglik: -2.5393e+02 - logprior: -2.2422e-01
Fitted a model with MAP estimate = -253.4250
Time for alignment: 74.0759
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.0404 - loglik: -3.8293e+02 - logprior: -8.1090e+00
Epoch 2/10
13/13 - 2s - loss: 338.9308 - loglik: -3.3706e+02 - logprior: -1.8750e+00
Epoch 3/10
13/13 - 2s - loss: 297.7333 - loglik: -2.9598e+02 - logprior: -1.7489e+00
Epoch 4/10
13/13 - 2s - loss: 283.3829 - loglik: -2.8136e+02 - logprior: -2.0231e+00
Epoch 5/10
13/13 - 2s - loss: 277.4924 - loglik: -2.7540e+02 - logprior: -2.0904e+00
Epoch 6/10
13/13 - 2s - loss: 277.8189 - loglik: -2.7580e+02 - logprior: -2.0177e+00
Fitted a model with MAP estimate = -276.2271
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 283.1452 - loglik: -2.7370e+02 - logprior: -9.4430e+00
Epoch 2/2
13/13 - 2s - loss: 265.1988 - loglik: -2.6137e+02 - logprior: -3.8281e+00
Fitted a model with MAP estimate = -261.9828
expansions: [(0, 2)]
discards: [ 0  9 88 89]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.8898 - loglik: -2.5970e+02 - logprior: -7.1947e+00
Epoch 2/2
13/13 - 2s - loss: 258.2379 - loglik: -2.5655e+02 - logprior: -1.6866e+00
Fitted a model with MAP estimate = -256.8251
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 268.9370 - loglik: -2.5974e+02 - logprior: -9.1995e+00
Epoch 2/10
13/13 - 2s - loss: 261.6838 - loglik: -2.5878e+02 - logprior: -2.8998e+00
Epoch 3/10
13/13 - 2s - loss: 256.8117 - loglik: -2.5587e+02 - logprior: -9.4662e-01
Epoch 4/10
13/13 - 2s - loss: 256.1725 - loglik: -2.5582e+02 - logprior: -3.5359e-01
Epoch 5/10
13/13 - 2s - loss: 255.1454 - loglik: -2.5488e+02 - logprior: -2.6114e-01
Epoch 6/10
13/13 - 2s - loss: 254.0405 - loglik: -2.5380e+02 - logprior: -2.4051e-01
Epoch 7/10
13/13 - 2s - loss: 254.4724 - loglik: -2.5422e+02 - logprior: -2.5346e-01
Fitted a model with MAP estimate = -253.7052
Time for alignment: 64.2711
Computed alignments with likelihoods: ['-254.4407', '-253.8522', '-253.4239', '-253.4250', '-253.7052']
Best model has likelihood: -253.4239  (prior= -0.1943 )
time for generating output: 0.1611
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9445362718089991
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 31s - loss: 1079.6335 - loglik: -1.0762e+03 - logprior: -3.4056e+00
Epoch 2/10
25/25 - 29s - loss: 821.5679 - loglik: -8.2064e+02 - logprior: -9.3104e-01
Epoch 3/10
25/25 - 28s - loss: 771.3113 - loglik: -7.6941e+02 - logprior: -1.9037e+00
Epoch 4/10
25/25 - 29s - loss: 756.2422 - loglik: -7.5423e+02 - logprior: -2.0083e+00
Epoch 5/10
25/25 - 29s - loss: 757.0611 - loglik: -7.5512e+02 - logprior: -1.9379e+00
Fitted a model with MAP estimate = -755.3912
expansions: [(0, 2), (126, 1), (142, 1), (144, 1), (145, 1), (146, 1), (163, 1), (164, 1), (170, 1), (174, 4), (175, 2), (176, 2), (177, 1), (191, 1), (193, 4), (194, 1), (195, 1), (198, 1), (199, 3), (200, 2), (202, 1), (204, 1), (205, 1), (207, 1), (209, 1), (210, 1), (213, 1), (214, 1), (216, 1), (218, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (236, 1), (238, 1), (239, 1), (250, 1), (252, 1), (256, 3), (257, 3), (259, 1), (260, 1), (267, 2), (281, 1), (283, 1), (300, 2), (302, 2), (305, 1), (315, 1), (318, 1), (319, 1), (320, 1), (322, 2), (327, 1), (340, 2), (341, 1), (354, 3), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 740.5657 - loglik: -7.3485e+02 - logprior: -5.7130e+00
Epoch 2/2
25/25 - 41s - loss: 705.9180 - loglik: -7.0509e+02 - logprior: -8.2375e-01
Fitted a model with MAP estimate = -699.6396
expansions: [(421, 1), (437, 2)]
discards: [  2 187 188 215 230 315 316 400 438 439]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 710.8631 - loglik: -7.0789e+02 - logprior: -2.9766e+00
Epoch 2/2
25/25 - 40s - loss: 696.0820 - loglik: -6.9699e+02 - logprior: 0.9076
Fitted a model with MAP estimate = -695.8381
expansions: [(308, 4)]
discards: [430 431]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 704.8176 - loglik: -7.0251e+02 - logprior: -2.3035e+00
Epoch 2/10
25/25 - 40s - loss: 700.7534 - loglik: -7.0254e+02 - logprior: 1.7900
Epoch 3/10
25/25 - 40s - loss: 697.0541 - loglik: -6.9941e+02 - logprior: 2.3586
Epoch 4/10
25/25 - 40s - loss: 693.2140 - loglik: -6.9575e+02 - logprior: 2.5357
Epoch 5/10
25/25 - 40s - loss: 692.4543 - loglik: -6.9525e+02 - logprior: 2.8006
Epoch 6/10
25/25 - 40s - loss: 692.7752 - loglik: -6.9583e+02 - logprior: 3.0503
Fitted a model with MAP estimate = -691.7106
Time for alignment: 706.0485
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1076.9487 - loglik: -1.0736e+03 - logprior: -3.3154e+00
Epoch 2/10
25/25 - 29s - loss: 826.3041 - loglik: -8.2539e+02 - logprior: -9.1235e-01
Epoch 3/10
25/25 - 29s - loss: 767.6273 - loglik: -7.6579e+02 - logprior: -1.8363e+00
Epoch 4/10
25/25 - 29s - loss: 757.5637 - loglik: -7.5567e+02 - logprior: -1.8954e+00
Epoch 5/10
25/25 - 28s - loss: 757.7057 - loglik: -7.5584e+02 - logprior: -1.8622e+00
Fitted a model with MAP estimate = -754.8137
expansions: [(39, 2), (126, 1), (136, 1), (144, 1), (145, 1), (164, 1), (165, 1), (171, 1), (175, 4), (176, 2), (177, 2), (178, 1), (191, 1), (192, 1), (193, 4), (194, 1), (195, 1), (198, 1), (199, 3), (200, 1), (202, 1), (204, 1), (205, 1), (207, 1), (209, 1), (210, 1), (213, 1), (214, 1), (219, 1), (225, 1), (226, 4), (227, 1), (228, 2), (230, 1), (231, 1), (232, 1), (237, 1), (239, 1), (240, 1), (251, 1), (252, 1), (253, 1), (255, 2), (256, 2), (257, 1), (259, 4), (267, 1), (281, 1), (282, 1), (283, 1), (299, 2), (300, 1), (301, 2), (303, 1), (313, 1), (316, 1), (317, 1), (318, 1), (325, 2), (327, 2), (354, 2), (358, 1), (360, 1), (364, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 733.3171 - loglik: -7.2946e+02 - logprior: -3.8547e+00
Epoch 2/2
25/25 - 41s - loss: 702.4012 - loglik: -7.0242e+02 - logprior: 0.0190
Fitted a model with MAP estimate = -698.6930
expansions: [(437, 1)]
discards: [ 40 187 215 274 321 438]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 708.9504 - loglik: -7.0568e+02 - logprior: -3.2684e+00
Epoch 2/2
25/25 - 40s - loss: 698.7634 - loglik: -6.9983e+02 - logprior: 1.0658
Fitted a model with MAP estimate = -697.1168
expansions: []
discards: [314 432]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 709.3120 - loglik: -7.0721e+02 - logprior: -2.1032e+00
Epoch 2/10
25/25 - 40s - loss: 697.3174 - loglik: -6.9917e+02 - logprior: 1.8569
Epoch 3/10
25/25 - 40s - loss: 699.1003 - loglik: -7.0149e+02 - logprior: 2.3879
Fitted a model with MAP estimate = -695.7115
Time for alignment: 582.8315
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 1075.3812 - loglik: -1.0719e+03 - logprior: -3.4952e+00
Epoch 2/10
25/25 - 29s - loss: 823.5626 - loglik: -8.2263e+02 - logprior: -9.3434e-01
Epoch 3/10
25/25 - 29s - loss: 768.0013 - loglik: -7.6603e+02 - logprior: -1.9737e+00
Epoch 4/10
25/25 - 28s - loss: 759.8321 - loglik: -7.5775e+02 - logprior: -2.0797e+00
Epoch 5/10
25/25 - 29s - loss: 751.2289 - loglik: -7.4905e+02 - logprior: -2.1786e+00
Epoch 6/10
25/25 - 29s - loss: 752.3770 - loglik: -7.5029e+02 - logprior: -2.0909e+00
Fitted a model with MAP estimate = -750.3870
expansions: [(0, 2), (52, 1), (141, 1), (145, 1), (164, 1), (165, 2), (171, 1), (175, 5), (176, 2), (177, 2), (180, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 2), (196, 1), (199, 1), (200, 3), (201, 2), (203, 1), (205, 1), (208, 1), (210, 1), (211, 1), (213, 1), (214, 1), (215, 1), (217, 1), (219, 1), (225, 1), (227, 3), (228, 2), (229, 2), (230, 1), (231, 1), (238, 2), (239, 1), (250, 1), (252, 1), (253, 1), (254, 2), (255, 4), (256, 2), (258, 4), (266, 1), (280, 1), (282, 1), (283, 1), (299, 1), (300, 1), (301, 2), (303, 1), (313, 1), (316, 1), (317, 1), (318, 1), (326, 1), (328, 2), (336, 1), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 731.5795 - loglik: -7.2640e+02 - logprior: -5.1755e+00
Epoch 2/2
25/25 - 41s - loss: 706.9274 - loglik: -7.0739e+02 - logprior: 0.4617
Fitted a model with MAP estimate = -700.5092
expansions: [(269, 1)]
discards: [  2 186 187 188 218 231 312 324]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 708.8712 - loglik: -7.0638e+02 - logprior: -2.4866e+00
Epoch 2/2
25/25 - 40s - loss: 702.9611 - loglik: -7.0436e+02 - logprior: 1.3957
Fitted a model with MAP estimate = -699.0333
expansions: []
discards: [187 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 709.1420 - loglik: -7.0676e+02 - logprior: -2.3816e+00
Epoch 2/10
25/25 - 39s - loss: 701.4428 - loglik: -7.0316e+02 - logprior: 1.7186
Epoch 3/10
25/25 - 40s - loss: 699.5429 - loglik: -7.0181e+02 - logprior: 2.2667
Epoch 4/10
25/25 - 40s - loss: 696.6093 - loglik: -6.9915e+02 - logprior: 2.5419
Epoch 5/10
25/25 - 40s - loss: 694.5783 - loglik: -6.9734e+02 - logprior: 2.7569
Epoch 6/10
25/25 - 40s - loss: 696.3080 - loglik: -6.9927e+02 - logprior: 2.9593
Fitted a model with MAP estimate = -694.7024
Time for alignment: 730.6089
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 1078.8638 - loglik: -1.0755e+03 - logprior: -3.3617e+00
Epoch 2/10
25/25 - 29s - loss: 822.4346 - loglik: -8.2155e+02 - logprior: -8.8047e-01
Epoch 3/10
25/25 - 29s - loss: 771.0455 - loglik: -7.6950e+02 - logprior: -1.5407e+00
Epoch 4/10
25/25 - 29s - loss: 762.8597 - loglik: -7.6104e+02 - logprior: -1.8203e+00
Epoch 5/10
25/25 - 29s - loss: 757.9788 - loglik: -7.5618e+02 - logprior: -1.8011e+00
Epoch 6/10
25/25 - 29s - loss: 757.6797 - loglik: -7.5596e+02 - logprior: -1.7231e+00
Epoch 7/10
25/25 - 29s - loss: 757.8921 - loglik: -7.5615e+02 - logprior: -1.7438e+00
Fitted a model with MAP estimate = -756.8066
expansions: [(0, 2), (126, 1), (135, 1), (145, 1), (164, 1), (165, 3), (171, 1), (175, 5), (176, 3), (179, 1), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (198, 1), (199, 1), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (207, 1), (209, 1), (210, 1), (213, 1), (214, 1), (216, 1), (218, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (230, 1), (231, 1), (232, 1), (235, 1), (238, 2), (239, 1), (250, 1), (252, 1), (253, 1), (255, 2), (256, 5), (258, 3), (280, 2), (282, 1), (283, 1), (299, 1), (300, 1), (301, 2), (318, 2), (319, 1), (326, 3), (340, 2), (354, 2), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 745.9617 - loglik: -7.4004e+02 - logprior: -5.9183e+00
Epoch 2/2
25/25 - 40s - loss: 712.7391 - loglik: -7.1188e+02 - logprior: -8.6248e-01
Fitted a model with MAP estimate = -707.5648
expansions: [(248, 1), (435, 1)]
discards: [  3 172 189 190 215 315 316 404 436]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 719.4490 - loglik: -7.1635e+02 - logprior: -3.1037e+00
Epoch 2/2
25/25 - 39s - loss: 708.0131 - loglik: -7.0917e+02 - logprior: 1.1525
Fitted a model with MAP estimate = -705.9487
expansions: [(187, 1)]
discards: [428]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 45s - loss: 716.6979 - loglik: -7.1394e+02 - logprior: -2.7625e+00
Epoch 2/10
25/25 - 39s - loss: 706.7108 - loglik: -7.0829e+02 - logprior: 1.5774
Epoch 3/10
25/25 - 40s - loss: 706.4099 - loglik: -7.0870e+02 - logprior: 2.2879
Epoch 4/10
25/25 - 39s - loss: 704.6309 - loglik: -7.0714e+02 - logprior: 2.5081
Epoch 5/10
25/25 - 39s - loss: 702.4189 - loglik: -7.0517e+02 - logprior: 2.7517
Epoch 6/10
25/25 - 39s - loss: 703.5229 - loglik: -7.0652e+02 - logprior: 2.9957
Fitted a model with MAP estimate = -701.4365
Time for alignment: 757.6221
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1082.3724 - loglik: -1.0789e+03 - logprior: -3.4348e+00
Epoch 2/10
25/25 - 29s - loss: 824.2729 - loglik: -8.2328e+02 - logprior: -9.9276e-01
Epoch 3/10
25/25 - 29s - loss: 767.9171 - loglik: -7.6586e+02 - logprior: -2.0571e+00
Epoch 4/10
25/25 - 29s - loss: 757.1154 - loglik: -7.5513e+02 - logprior: -1.9823e+00
Epoch 5/10
25/25 - 29s - loss: 756.3301 - loglik: -7.5428e+02 - logprior: -2.0461e+00
Epoch 6/10
25/25 - 29s - loss: 752.2037 - loglik: -7.5021e+02 - logprior: -1.9900e+00
Epoch 7/10
25/25 - 28s - loss: 755.7748 - loglik: -7.5376e+02 - logprior: -2.0189e+00
Fitted a model with MAP estimate = -753.0514
expansions: [(0, 2), (43, 1), (106, 1), (124, 1), (130, 1), (132, 1), (137, 1), (141, 1), (160, 2), (161, 2), (167, 2), (171, 4), (172, 2), (173, 2), (188, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (196, 1), (197, 3), (198, 2), (200, 1), (201, 1), (203, 1), (205, 2), (206, 1), (207, 1), (210, 1), (211, 1), (213, 1), (215, 1), (221, 1), (222, 1), (223, 2), (224, 1), (225, 1), (227, 1), (228, 1), (229, 1), (232, 1), (233, 1), (235, 1), (236, 1), (247, 1), (249, 1), (250, 1), (253, 4), (254, 2), (256, 4), (280, 1), (281, 1), (298, 1), (299, 1), (300, 2), (302, 1), (303, 1), (315, 1), (318, 1), (319, 2), (324, 1), (326, 2), (358, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 735.7385 - loglik: -7.3069e+02 - logprior: -5.0472e+00
Epoch 2/2
25/25 - 41s - loss: 706.4946 - loglik: -7.0703e+02 - logprior: 0.5366
Fitted a model with MAP estimate = -703.1000
expansions: [(252, 1), (440, 3)]
discards: [  2   3 170 180 188 189 191 217 232 245 312 321 322 326]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 715.1477 - loglik: -7.1197e+02 - logprior: -3.1754e+00
Epoch 2/2
25/25 - 39s - loss: 702.4033 - loglik: -7.0357e+02 - logprior: 1.1663
Fitted a model with MAP estimate = -700.9478
expansions: []
discards: [427 428 429]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 44s - loss: 712.8088 - loglik: -7.1065e+02 - logprior: -2.1584e+00
Epoch 2/10
25/25 - 39s - loss: 705.3691 - loglik: -7.0739e+02 - logprior: 2.0168
Epoch 3/10
25/25 - 39s - loss: 703.1741 - loglik: -7.0571e+02 - logprior: 2.5348
Epoch 4/10
25/25 - 39s - loss: 700.2126 - loglik: -7.0295e+02 - logprior: 2.7402
Epoch 5/10
25/25 - 39s - loss: 701.3056 - loglik: -7.0429e+02 - logprior: 2.9826
Fitted a model with MAP estimate = -699.3182
Time for alignment: 715.6232
Computed alignments with likelihoods: ['-691.7106', '-695.7115', '-694.7024', '-701.4365', '-699.3182']
Best model has likelihood: -691.7106  (prior= 3.2120 )
time for generating output: 0.4422
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9119762632620032
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.9581 - loglik: -3.2247e+02 - logprior: -4.4900e+00
Epoch 2/10
16/16 - 4s - loss: 255.4874 - loglik: -2.5399e+02 - logprior: -1.4970e+00
Epoch 3/10
16/16 - 3s - loss: 225.4594 - loglik: -2.2380e+02 - logprior: -1.6573e+00
Epoch 4/10
16/16 - 3s - loss: 216.6713 - loglik: -2.1521e+02 - logprior: -1.4652e+00
Epoch 5/10
16/16 - 3s - loss: 211.5926 - loglik: -2.1009e+02 - logprior: -1.5034e+00
Epoch 6/10
16/16 - 3s - loss: 209.2492 - loglik: -2.0780e+02 - logprior: -1.4538e+00
Epoch 7/10
16/16 - 4s - loss: 205.9923 - loglik: -2.0454e+02 - logprior: -1.4556e+00
Epoch 8/10
16/16 - 3s - loss: 204.7459 - loglik: -2.0329e+02 - logprior: -1.4572e+00
Epoch 9/10
16/16 - 3s - loss: 202.3907 - loglik: -2.0095e+02 - logprior: -1.4452e+00
Epoch 10/10
16/16 - 4s - loss: 201.0499 - loglik: -1.9960e+02 - logprior: -1.4549e+00
Fitted a model with MAP estimate = -200.5465
expansions: [(0, 1), (12, 1), (15, 1), (16, 3), (71, 1), (97, 1), (98, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 205.7502 - loglik: -2.0180e+02 - logprior: -3.9481e+00
Epoch 2/2
33/33 - 5s - loss: 196.7657 - loglik: -1.9523e+02 - logprior: -1.5383e+00
Fitted a model with MAP estimate = -194.9173
expansions: [(22, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 198.9228 - loglik: -1.9572e+02 - logprior: -3.2018e+00
Epoch 2/2
33/33 - 5s - loss: 194.9193 - loglik: -1.9351e+02 - logprior: -1.4084e+00
Fitted a model with MAP estimate = -194.1096
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 198.6460 - loglik: -1.9557e+02 - logprior: -3.0806e+00
Epoch 2/10
33/33 - 5s - loss: 195.6967 - loglik: -1.9442e+02 - logprior: -1.2763e+00
Epoch 3/10
33/33 - 4s - loss: 192.9400 - loglik: -1.9180e+02 - logprior: -1.1435e+00
Epoch 4/10
33/33 - 5s - loss: 191.0058 - loglik: -1.8995e+02 - logprior: -1.0536e+00
Epoch 5/10
33/33 - 5s - loss: 193.6574 - loglik: -1.9274e+02 - logprior: -9.1297e-01
Fitted a model with MAP estimate = -191.3033
Time for alignment: 107.4593
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 325.6276 - loglik: -3.2115e+02 - logprior: -4.4784e+00
Epoch 2/10
16/16 - 3s - loss: 255.2880 - loglik: -2.5380e+02 - logprior: -1.4889e+00
Epoch 3/10
16/16 - 3s - loss: 226.1823 - loglik: -2.2450e+02 - logprior: -1.6783e+00
Epoch 4/10
16/16 - 3s - loss: 217.8959 - loglik: -2.1640e+02 - logprior: -1.4988e+00
Epoch 5/10
16/16 - 3s - loss: 211.2281 - loglik: -2.0972e+02 - logprior: -1.5075e+00
Epoch 6/10
16/16 - 4s - loss: 210.0456 - loglik: -2.0859e+02 - logprior: -1.4599e+00
Epoch 7/10
16/16 - 4s - loss: 206.1278 - loglik: -2.0466e+02 - logprior: -1.4675e+00
Epoch 8/10
16/16 - 3s - loss: 203.7086 - loglik: -2.0224e+02 - logprior: -1.4693e+00
Epoch 9/10
16/16 - 3s - loss: 202.2283 - loglik: -2.0078e+02 - logprior: -1.4506e+00
Epoch 10/10
16/16 - 3s - loss: 202.2584 - loglik: -2.0081e+02 - logprior: -1.4489e+00
Fitted a model with MAP estimate = -201.2209
expansions: [(0, 1), (12, 1), (13, 1), (15, 1), (16, 1), (69, 1), (94, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 213.6205 - loglik: -2.0782e+02 - logprior: -5.7967e+00
Epoch 2/2
16/16 - 4s - loss: 200.3227 - loglik: -1.9811e+02 - logprior: -2.2145e+00
Fitted a model with MAP estimate = -198.1618
expansions: [(19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 203.0486 - loglik: -1.9803e+02 - logprior: -5.0144e+00
Epoch 2/2
16/16 - 3s - loss: 197.1530 - loglik: -1.9556e+02 - logprior: -1.5881e+00
Fitted a model with MAP estimate = -196.8603
expansions: [(0, 1), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 200.7306 - loglik: -1.9730e+02 - logprior: -3.4344e+00
Epoch 2/10
33/33 - 5s - loss: 195.7763 - loglik: -1.9442e+02 - logprior: -1.3557e+00
Epoch 3/10
33/33 - 5s - loss: 194.5179 - loglik: -1.9330e+02 - logprior: -1.2137e+00
Epoch 4/10
33/33 - 5s - loss: 191.5171 - loglik: -1.9039e+02 - logprior: -1.1307e+00
Epoch 5/10
33/33 - 4s - loss: 193.7275 - loglik: -1.9272e+02 - logprior: -1.0091e+00
Fitted a model with MAP estimate = -191.8293
Time for alignment: 106.7110
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.7648 - loglik: -3.2227e+02 - logprior: -4.4901e+00
Epoch 2/10
16/16 - 3s - loss: 254.0092 - loglik: -2.5252e+02 - logprior: -1.4920e+00
Epoch 3/10
16/16 - 3s - loss: 225.6081 - loglik: -2.2395e+02 - logprior: -1.6604e+00
Epoch 4/10
16/16 - 3s - loss: 216.3825 - loglik: -2.1492e+02 - logprior: -1.4635e+00
Epoch 5/10
16/16 - 3s - loss: 212.7397 - loglik: -2.1125e+02 - logprior: -1.4941e+00
Epoch 6/10
16/16 - 3s - loss: 209.4613 - loglik: -2.0803e+02 - logprior: -1.4309e+00
Epoch 7/10
16/16 - 3s - loss: 207.7741 - loglik: -2.0634e+02 - logprior: -1.4376e+00
Epoch 8/10
16/16 - 3s - loss: 206.0982 - loglik: -2.0465e+02 - logprior: -1.4456e+00
Epoch 9/10
16/16 - 3s - loss: 203.5822 - loglik: -2.0213e+02 - logprior: -1.4484e+00
Epoch 10/10
16/16 - 3s - loss: 203.1244 - loglik: -2.0166e+02 - logprior: -1.4621e+00
Fitted a model with MAP estimate = -201.7057
expansions: [(0, 1), (12, 1), (16, 2), (17, 1), (18, 1), (71, 1), (96, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 207.5160 - loglik: -2.0359e+02 - logprior: -3.9284e+00
Epoch 2/2
33/33 - 5s - loss: 197.5239 - loglik: -1.9603e+02 - logprior: -1.4947e+00
Fitted a model with MAP estimate = -195.8000
expansions: [(19, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 199.4951 - loglik: -1.9632e+02 - logprior: -3.1776e+00
Epoch 2/2
33/33 - 4s - loss: 194.9218 - loglik: -1.9352e+02 - logprior: -1.3978e+00
Fitted a model with MAP estimate = -194.5369
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 200.7349 - loglik: -1.9767e+02 - logprior: -3.0644e+00
Epoch 2/10
33/33 - 4s - loss: 195.7086 - loglik: -1.9444e+02 - logprior: -1.2725e+00
Epoch 3/10
33/33 - 4s - loss: 194.4912 - loglik: -1.9335e+02 - logprior: -1.1384e+00
Epoch 4/10
33/33 - 4s - loss: 195.0262 - loglik: -1.9398e+02 - logprior: -1.0415e+00
Fitted a model with MAP estimate = -193.1228
Time for alignment: 103.8110
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 327.5515 - loglik: -3.2305e+02 - logprior: -4.5039e+00
Epoch 2/10
16/16 - 3s - loss: 254.8507 - loglik: -2.5334e+02 - logprior: -1.5110e+00
Epoch 3/10
16/16 - 3s - loss: 225.7911 - loglik: -2.2405e+02 - logprior: -1.7383e+00
Epoch 4/10
16/16 - 3s - loss: 215.5275 - loglik: -2.1397e+02 - logprior: -1.5573e+00
Epoch 5/10
16/16 - 3s - loss: 211.9373 - loglik: -2.1041e+02 - logprior: -1.5254e+00
Epoch 6/10
16/16 - 4s - loss: 209.7834 - loglik: -2.0828e+02 - logprior: -1.4998e+00
Epoch 7/10
16/16 - 3s - loss: 206.9052 - loglik: -2.0540e+02 - logprior: -1.5039e+00
Epoch 8/10
16/16 - 3s - loss: 204.7300 - loglik: -2.0322e+02 - logprior: -1.5144e+00
Epoch 9/10
16/16 - 3s - loss: 203.8143 - loglik: -2.0229e+02 - logprior: -1.5199e+00
Epoch 10/10
16/16 - 3s - loss: 202.6238 - loglik: -2.0111e+02 - logprior: -1.5161e+00
Fitted a model with MAP estimate = -202.7798
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (70, 1), (73, 1), (98, 1), (99, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 207.8215 - loglik: -2.0389e+02 - logprior: -3.9265e+00
Epoch 2/2
33/33 - 5s - loss: 196.9980 - loglik: -1.9547e+02 - logprior: -1.5298e+00
Fitted a model with MAP estimate = -195.8310
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 199.9120 - loglik: -1.9672e+02 - logprior: -3.1896e+00
Epoch 2/10
33/33 - 4s - loss: 195.2770 - loglik: -1.9388e+02 - logprior: -1.3984e+00
Epoch 3/10
33/33 - 4s - loss: 192.8620 - loglik: -1.9159e+02 - logprior: -1.2754e+00
Epoch 4/10
33/33 - 4s - loss: 192.2740 - loglik: -1.9109e+02 - logprior: -1.1839e+00
Epoch 5/10
33/33 - 5s - loss: 192.2937 - loglik: -1.9122e+02 - logprior: -1.0764e+00
Fitted a model with MAP estimate = -191.2531
Time for alignment: 86.6916
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.2510 - loglik: -3.2177e+02 - logprior: -4.4796e+00
Epoch 2/10
16/16 - 3s - loss: 253.3196 - loglik: -2.5185e+02 - logprior: -1.4734e+00
Epoch 3/10
16/16 - 3s - loss: 225.8258 - loglik: -2.2409e+02 - logprior: -1.7320e+00
Epoch 4/10
16/16 - 4s - loss: 214.3526 - loglik: -2.1259e+02 - logprior: -1.7587e+00
Epoch 5/10
16/16 - 3s - loss: 206.1270 - loglik: -2.0451e+02 - logprior: -1.6217e+00
Epoch 6/10
16/16 - 4s - loss: 202.8925 - loglik: -2.0127e+02 - logprior: -1.6271e+00
Epoch 7/10
16/16 - 3s - loss: 201.7212 - loglik: -2.0011e+02 - logprior: -1.6129e+00
Epoch 8/10
16/16 - 3s - loss: 202.7505 - loglik: -2.0115e+02 - logprior: -1.6029e+00
Fitted a model with MAP estimate = -201.3560
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 213.0802 - loglik: -2.0746e+02 - logprior: -5.6222e+00
Epoch 2/2
16/16 - 4s - loss: 202.1040 - loglik: -1.9930e+02 - logprior: -2.8005e+00
Fitted a model with MAP estimate = -200.2886
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 200.6926 - loglik: -1.9767e+02 - logprior: -3.0221e+00
Epoch 2/2
33/33 - 4s - loss: 195.2370 - loglik: -1.9379e+02 - logprior: -1.4429e+00
Fitted a model with MAP estimate = -194.8119
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 198.2789 - loglik: -1.9530e+02 - logprior: -2.9773e+00
Epoch 2/10
33/33 - 5s - loss: 193.5318 - loglik: -1.9225e+02 - logprior: -1.2777e+00
Epoch 3/10
33/33 - 4s - loss: 193.0326 - loglik: -1.9186e+02 - logprior: -1.1739e+00
Epoch 4/10
33/33 - 4s - loss: 192.2320 - loglik: -1.9116e+02 - logprior: -1.0759e+00
Epoch 5/10
33/33 - 4s - loss: 190.5441 - loglik: -1.8958e+02 - logprior: -9.6025e-01
Epoch 6/10
33/33 - 5s - loss: 189.9669 - loglik: -1.8908e+02 - logprior: -8.8644e-01
Epoch 7/10
33/33 - 4s - loss: 190.4043 - loglik: -1.8958e+02 - logprior: -8.2233e-01
Fitted a model with MAP estimate = -189.9091
Time for alignment: 106.6722
Computed alignments with likelihoods: ['-191.3033', '-191.8293', '-193.1228', '-191.2531', '-189.9091']
Best model has likelihood: -189.9091  (prior= -0.7745 )
time for generating output: 0.3218
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5400789877861479
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 308.6424 - loglik: -3.0553e+02 - logprior: -3.1156e+00
Epoch 2/10
19/19 - 2s - loss: 277.8498 - loglik: -2.7673e+02 - logprior: -1.1238e+00
Epoch 3/10
19/19 - 2s - loss: 262.1749 - loglik: -2.6075e+02 - logprior: -1.4234e+00
Epoch 4/10
19/19 - 2s - loss: 258.8814 - loglik: -2.5750e+02 - logprior: -1.3832e+00
Epoch 5/10
19/19 - 2s - loss: 257.2586 - loglik: -2.5591e+02 - logprior: -1.3509e+00
Epoch 6/10
19/19 - 2s - loss: 256.8132 - loglik: -2.5547e+02 - logprior: -1.3465e+00
Epoch 7/10
19/19 - 2s - loss: 256.3656 - loglik: -2.5504e+02 - logprior: -1.3286e+00
Epoch 8/10
19/19 - 2s - loss: 256.1651 - loglik: -2.5484e+02 - logprior: -1.3235e+00
Epoch 9/10
19/19 - 2s - loss: 256.5457 - loglik: -2.5523e+02 - logprior: -1.3157e+00
Fitted a model with MAP estimate = -242.5215
expansions: [(6, 3), (7, 2), (8, 1), (10, 1), (34, 9), (39, 2), (40, 1), (43, 2), (58, 1), (59, 1), (60, 1), (62, 1), (63, 2), (65, 1), (67, 2), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.2328 - loglik: -2.5732e+02 - logprior: -3.9094e+00
Epoch 2/2
19/19 - 2s - loss: 251.3287 - loglik: -2.4930e+02 - logprior: -2.0323e+00
Fitted a model with MAP estimate = -235.8833
expansions: [(0, 2), (46, 2)]
discards: [ 0  8 54 87 95]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 251.5312 - loglik: -2.4863e+02 - logprior: -2.9014e+00
Epoch 2/2
19/19 - 2s - loss: 247.8557 - loglik: -2.4676e+02 - logprior: -1.0989e+00
Fitted a model with MAP estimate = -233.7266
expansions: []
discards: [ 0 11 47 48 49]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 236.8064 - loglik: -2.3428e+02 - logprior: -2.5269e+00
Epoch 2/10
23/23 - 3s - loss: 232.7787 - loglik: -2.3177e+02 - logprior: -1.0104e+00
Epoch 3/10
23/23 - 3s - loss: 232.7039 - loglik: -2.3175e+02 - logprior: -9.5224e-01
Epoch 4/10
23/23 - 3s - loss: 231.6697 - loglik: -2.3074e+02 - logprior: -9.2809e-01
Epoch 5/10
23/23 - 3s - loss: 229.7813 - loglik: -2.2887e+02 - logprior: -9.0786e-01
Epoch 6/10
23/23 - 3s - loss: 230.7895 - loglik: -2.2988e+02 - logprior: -9.0719e-01
Fitted a model with MAP estimate = -229.9681
Time for alignment: 77.3005
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.6332 - loglik: -3.0552e+02 - logprior: -3.1162e+00
Epoch 2/10
19/19 - 2s - loss: 277.2483 - loglik: -2.7612e+02 - logprior: -1.1233e+00
Epoch 3/10
19/19 - 2s - loss: 262.4179 - loglik: -2.6099e+02 - logprior: -1.4311e+00
Epoch 4/10
19/19 - 2s - loss: 259.1717 - loglik: -2.5780e+02 - logprior: -1.3723e+00
Epoch 5/10
19/19 - 2s - loss: 257.7032 - loglik: -2.5639e+02 - logprior: -1.3180e+00
Epoch 6/10
19/19 - 2s - loss: 257.1931 - loglik: -2.5589e+02 - logprior: -1.3036e+00
Epoch 7/10
19/19 - 2s - loss: 257.0762 - loglik: -2.5579e+02 - logprior: -1.2878e+00
Epoch 8/10
19/19 - 2s - loss: 256.7003 - loglik: -2.5542e+02 - logprior: -1.2792e+00
Epoch 9/10
19/19 - 2s - loss: 255.8486 - loglik: -2.5458e+02 - logprior: -1.2698e+00
Epoch 10/10
19/19 - 2s - loss: 256.8322 - loglik: -2.5557e+02 - logprior: -1.2646e+00
Fitted a model with MAP estimate = -242.8671
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (39, 3), (40, 1), (42, 5), (58, 1), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 262.2790 - loglik: -2.5837e+02 - logprior: -3.9070e+00
Epoch 2/2
19/19 - 2s - loss: 253.2601 - loglik: -2.5127e+02 - logprior: -1.9891e+00
Fitted a model with MAP estimate = -237.3505
expansions: [(0, 2)]
discards: [ 0  8 15 55 56 82 87]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.1276 - loglik: -2.5127e+02 - logprior: -2.8620e+00
Epoch 2/2
19/19 - 2s - loss: 250.6937 - loglik: -2.4960e+02 - logprior: -1.0910e+00
Fitted a model with MAP estimate = -235.7944
expansions: [(55, 2)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 237.8226 - loglik: -2.3531e+02 - logprior: -2.5168e+00
Epoch 2/10
23/23 - 3s - loss: 234.1288 - loglik: -2.3315e+02 - logprior: -9.7456e-01
Epoch 3/10
23/23 - 3s - loss: 233.3447 - loglik: -2.3244e+02 - logprior: -9.0647e-01
Epoch 4/10
23/23 - 3s - loss: 232.3818 - loglik: -2.3151e+02 - logprior: -8.7668e-01
Epoch 5/10
23/23 - 3s - loss: 232.4562 - loglik: -2.3159e+02 - logprior: -8.6566e-01
Fitted a model with MAP estimate = -231.6152
Time for alignment: 75.5854
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.8778 - loglik: -3.0576e+02 - logprior: -3.1154e+00
Epoch 2/10
19/19 - 2s - loss: 276.7103 - loglik: -2.7560e+02 - logprior: -1.1113e+00
Epoch 3/10
19/19 - 2s - loss: 262.3592 - loglik: -2.6092e+02 - logprior: -1.4360e+00
Epoch 4/10
19/19 - 2s - loss: 258.2310 - loglik: -2.5682e+02 - logprior: -1.4102e+00
Epoch 5/10
19/19 - 2s - loss: 256.8499 - loglik: -2.5548e+02 - logprior: -1.3673e+00
Epoch 6/10
19/19 - 2s - loss: 256.2839 - loglik: -2.5494e+02 - logprior: -1.3484e+00
Epoch 7/10
19/19 - 2s - loss: 256.6450 - loglik: -2.5531e+02 - logprior: -1.3350e+00
Fitted a model with MAP estimate = -242.1129
expansions: [(6, 3), (7, 2), (10, 2), (22, 4), (33, 9), (38, 2), (39, 2), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 261.7298 - loglik: -2.5780e+02 - logprior: -3.9262e+00
Epoch 2/2
19/19 - 3s - loss: 251.0614 - loglik: -2.4898e+02 - logprior: -2.0786e+00
Fitted a model with MAP estimate = -235.6017
expansions: [(0, 2), (49, 2)]
discards: [ 0  9 14 29 30 31 58 60 84 91]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.7694 - loglik: -2.4887e+02 - logprior: -2.9039e+00
Epoch 2/2
19/19 - 2s - loss: 248.0587 - loglik: -2.4695e+02 - logprior: -1.1092e+00
Fitted a model with MAP estimate = -233.8092
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 236.8878 - loglik: -2.3437e+02 - logprior: -2.5169e+00
Epoch 2/10
23/23 - 3s - loss: 233.1414 - loglik: -2.3212e+02 - logprior: -1.0171e+00
Epoch 3/10
23/23 - 3s - loss: 231.6097 - loglik: -2.3065e+02 - logprior: -9.5502e-01
Epoch 4/10
23/23 - 3s - loss: 231.3924 - loglik: -2.3048e+02 - logprior: -9.1459e-01
Epoch 5/10
23/23 - 3s - loss: 230.7789 - loglik: -2.2987e+02 - logprior: -9.1173e-01
Epoch 6/10
23/23 - 3s - loss: 230.2599 - loglik: -2.2937e+02 - logprior: -8.9385e-01
Epoch 7/10
23/23 - 3s - loss: 229.9753 - loglik: -2.2908e+02 - logprior: -8.9484e-01
Epoch 8/10
23/23 - 3s - loss: 229.7821 - loglik: -2.2890e+02 - logprior: -8.8321e-01
Epoch 9/10
23/23 - 3s - loss: 230.1181 - loglik: -2.2925e+02 - logprior: -8.7294e-01
Fitted a model with MAP estimate = -229.5470
Time for alignment: 80.2138
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.6385 - loglik: -3.0552e+02 - logprior: -3.1168e+00
Epoch 2/10
19/19 - 2s - loss: 276.7236 - loglik: -2.7560e+02 - logprior: -1.1284e+00
Epoch 3/10
19/19 - 2s - loss: 261.6359 - loglik: -2.6019e+02 - logprior: -1.4436e+00
Epoch 4/10
19/19 - 2s - loss: 257.9346 - loglik: -2.5656e+02 - logprior: -1.3777e+00
Epoch 5/10
19/19 - 2s - loss: 257.0229 - loglik: -2.5571e+02 - logprior: -1.3153e+00
Epoch 6/10
19/19 - 2s - loss: 256.6315 - loglik: -2.5533e+02 - logprior: -1.2989e+00
Epoch 7/10
19/19 - 2s - loss: 256.3152 - loglik: -2.5504e+02 - logprior: -1.2784e+00
Epoch 8/10
19/19 - 2s - loss: 256.1601 - loglik: -2.5490e+02 - logprior: -1.2642e+00
Epoch 9/10
19/19 - 2s - loss: 256.2213 - loglik: -2.5496e+02 - logprior: -1.2572e+00
Fitted a model with MAP estimate = -242.0422
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (39, 7), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 261.6627 - loglik: -2.5777e+02 - logprior: -3.8928e+00
Epoch 2/2
19/19 - 2s - loss: 253.4849 - loglik: -2.5150e+02 - logprior: -1.9838e+00
Fitted a model with MAP estimate = -237.2737
expansions: [(0, 2), (51, 1)]
discards: [ 0  8 10 15 53 54 73 80]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.8659 - loglik: -2.5201e+02 - logprior: -2.8598e+00
Epoch 2/2
19/19 - 2s - loss: 251.3149 - loglik: -2.5023e+02 - logprior: -1.0840e+00
Fitted a model with MAP estimate = -236.3418
expansions: [(51, 1), (53, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 237.8020 - loglik: -2.3530e+02 - logprior: -2.4983e+00
Epoch 2/10
23/23 - 3s - loss: 234.1915 - loglik: -2.3323e+02 - logprior: -9.6545e-01
Epoch 3/10
23/23 - 3s - loss: 233.5235 - loglik: -2.3262e+02 - logprior: -9.0648e-01
Epoch 4/10
23/23 - 3s - loss: 232.1273 - loglik: -2.3126e+02 - logprior: -8.7168e-01
Epoch 5/10
23/23 - 3s - loss: 232.3493 - loglik: -2.3148e+02 - logprior: -8.6697e-01
Fitted a model with MAP estimate = -231.5205
Time for alignment: 70.9008
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.6427 - loglik: -3.0553e+02 - logprior: -3.1160e+00
Epoch 2/10
19/19 - 2s - loss: 279.6274 - loglik: -2.7851e+02 - logprior: -1.1214e+00
Epoch 3/10
19/19 - 2s - loss: 264.5759 - loglik: -2.6315e+02 - logprior: -1.4257e+00
Epoch 4/10
19/19 - 2s - loss: 259.1369 - loglik: -2.5775e+02 - logprior: -1.3853e+00
Epoch 5/10
19/19 - 2s - loss: 257.4648 - loglik: -2.5611e+02 - logprior: -1.3518e+00
Epoch 6/10
19/19 - 2s - loss: 256.6358 - loglik: -2.5530e+02 - logprior: -1.3379e+00
Epoch 7/10
19/19 - 2s - loss: 256.3527 - loglik: -2.5503e+02 - logprior: -1.3183e+00
Epoch 8/10
19/19 - 2s - loss: 255.8406 - loglik: -2.5452e+02 - logprior: -1.3164e+00
Epoch 9/10
19/19 - 2s - loss: 256.6248 - loglik: -2.5531e+02 - logprior: -1.3109e+00
Fitted a model with MAP estimate = -242.5998
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 9), (38, 2), (39, 2), (42, 2), (54, 2), (59, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 262.0193 - loglik: -2.5810e+02 - logprior: -3.9182e+00
Epoch 2/2
19/19 - 2s - loss: 251.6848 - loglik: -2.4963e+02 - logprior: -2.0554e+00
Fitted a model with MAP estimate = -235.9797
expansions: [(0, 2)]
discards: [ 0  8 14 28 45 46 47 48 56 58 77 89]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 253.2561 - loglik: -2.5038e+02 - logprior: -2.8748e+00
Epoch 2/2
19/19 - 2s - loss: 249.5267 - loglik: -2.4842e+02 - logprior: -1.1033e+00
Fitted a model with MAP estimate = -235.0252
expansions: [(43, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 237.5046 - loglik: -2.3493e+02 - logprior: -2.5716e+00
Epoch 2/10
23/23 - 3s - loss: 232.5400 - loglik: -2.3149e+02 - logprior: -1.0454e+00
Epoch 3/10
23/23 - 3s - loss: 232.0240 - loglik: -2.3106e+02 - logprior: -9.6167e-01
Epoch 4/10
23/23 - 3s - loss: 231.8245 - loglik: -2.3090e+02 - logprior: -9.2127e-01
Epoch 5/10
23/23 - 3s - loss: 230.0054 - loglik: -2.2909e+02 - logprior: -9.1336e-01
Epoch 6/10
23/23 - 3s - loss: 229.8803 - loglik: -2.2897e+02 - logprior: -9.0991e-01
Epoch 7/10
23/23 - 3s - loss: 229.6647 - loglik: -2.2877e+02 - logprior: -8.9648e-01
Epoch 8/10
23/23 - 3s - loss: 229.3464 - loglik: -2.2845e+02 - logprior: -8.9706e-01
Epoch 9/10
23/23 - 3s - loss: 229.1286 - loglik: -2.2825e+02 - logprior: -8.7889e-01
Epoch 10/10
23/23 - 3s - loss: 229.7357 - loglik: -2.2886e+02 - logprior: -8.7303e-01
Fitted a model with MAP estimate = -229.1438
Time for alignment: 85.4655
Computed alignments with likelihoods: ['-229.9681', '-231.6152', '-229.5470', '-231.5205', '-229.1438']
Best model has likelihood: -229.1438  (prior= -0.8907 )
time for generating output: 0.1729
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.8210412147505423
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.4949 - loglik: -3.8701e+02 - logprior: -4.5482e+01
Epoch 2/10
10/10 - 2s - loss: 370.9406 - loglik: -3.6064e+02 - logprior: -1.0296e+01
Epoch 3/10
10/10 - 2s - loss: 336.5019 - loglik: -3.3239e+02 - logprior: -4.1158e+00
Epoch 4/10
10/10 - 2s - loss: 312.4241 - loglik: -3.1028e+02 - logprior: -2.1413e+00
Epoch 5/10
10/10 - 2s - loss: 302.7831 - loglik: -3.0157e+02 - logprior: -1.2129e+00
Epoch 6/10
10/10 - 2s - loss: 298.5185 - loglik: -2.9781e+02 - logprior: -7.0489e-01
Epoch 7/10
10/10 - 2s - loss: 297.1745 - loglik: -2.9690e+02 - logprior: -2.7762e-01
Epoch 8/10
10/10 - 2s - loss: 296.4592 - loglik: -2.9649e+02 - logprior: 0.0335
Epoch 9/10
10/10 - 2s - loss: 295.6055 - loglik: -2.9579e+02 - logprior: 0.1861
Epoch 10/10
10/10 - 2s - loss: 295.5814 - loglik: -2.9586e+02 - logprior: 0.2762
Fitted a model with MAP estimate = -295.1285
expansions: [(7, 2), (9, 1), (17, 1), (18, 1), (26, 1), (28, 3), (42, 1), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.9776 - loglik: -2.9631e+02 - logprior: -5.0669e+01
Epoch 2/2
10/10 - 2s - loss: 305.2649 - loglik: -2.8624e+02 - logprior: -1.9022e+01
Fitted a model with MAP estimate = -298.8379
expansions: [(9, 3)]
discards: [ 68  74 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 331.3384 - loglik: -2.8293e+02 - logprior: -4.8409e+01
Epoch 2/2
10/10 - 2s - loss: 293.6243 - loglik: -2.8046e+02 - logprior: -1.3167e+01
Fitted a model with MAP estimate = -284.5827
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.2173 - loglik: -2.7933e+02 - logprior: -3.9889e+01
Epoch 2/10
10/10 - 2s - loss: 286.4965 - loglik: -2.7880e+02 - logprior: -7.6935e+00
Epoch 3/10
10/10 - 2s - loss: 278.7876 - loglik: -2.7765e+02 - logprior: -1.1338e+00
Epoch 4/10
10/10 - 2s - loss: 276.2652 - loglik: -2.7767e+02 - logprior: 1.4096
Epoch 5/10
10/10 - 2s - loss: 273.8206 - loglik: -2.7649e+02 - logprior: 2.6711
Epoch 6/10
10/10 - 2s - loss: 273.4651 - loglik: -2.7699e+02 - logprior: 3.5223
Epoch 7/10
10/10 - 2s - loss: 273.4161 - loglik: -2.7763e+02 - logprior: 4.2166
Epoch 8/10
10/10 - 2s - loss: 271.8178 - loglik: -2.7651e+02 - logprior: 4.6926
Epoch 9/10
10/10 - 2s - loss: 272.2061 - loglik: -2.7722e+02 - logprior: 5.0099
Fitted a model with MAP estimate = -272.0159
Time for alignment: 59.3638
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 432.3810 - loglik: -3.8690e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 369.7322 - loglik: -3.5943e+02 - logprior: -1.0306e+01
Epoch 3/10
10/10 - 2s - loss: 334.3911 - loglik: -3.3026e+02 - logprior: -4.1337e+00
Epoch 4/10
10/10 - 2s - loss: 311.9698 - loglik: -3.0989e+02 - logprior: -2.0813e+00
Epoch 5/10
10/10 - 2s - loss: 302.2564 - loglik: -3.0115e+02 - logprior: -1.1052e+00
Epoch 6/10
10/10 - 2s - loss: 298.7509 - loglik: -2.9808e+02 - logprior: -6.6632e-01
Epoch 7/10
10/10 - 2s - loss: 297.0855 - loglik: -2.9682e+02 - logprior: -2.6088e-01
Epoch 8/10
10/10 - 2s - loss: 296.3043 - loglik: -2.9631e+02 - logprior: 0.0096
Epoch 9/10
10/10 - 2s - loss: 295.8627 - loglik: -2.9600e+02 - logprior: 0.1392
Epoch 10/10
10/10 - 2s - loss: 295.3314 - loglik: -2.9554e+02 - logprior: 0.2109
Fitted a model with MAP estimate = -295.1111
expansions: [(10, 3), (17, 1), (18, 1), (26, 1), (28, 2), (42, 1), (48, 2), (58, 2), (59, 2), (61, 2), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.2980 - loglik: -2.9752e+02 - logprior: -5.0782e+01
Epoch 2/2
10/10 - 2s - loss: 306.6040 - loglik: -2.8733e+02 - logprior: -1.9275e+01
Fitted a model with MAP estimate = -299.5258
expansions: [(10, 2), (11, 1)]
discards: [ 0 56 68 69 75]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.4311 - loglik: -2.8481e+02 - logprior: -4.9625e+01
Epoch 2/2
10/10 - 2s - loss: 299.2220 - loglik: -2.8217e+02 - logprior: -1.7054e+01
Fitted a model with MAP estimate = -291.5849
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 319.3078 - loglik: -2.8055e+02 - logprior: -3.8762e+01
Epoch 2/10
10/10 - 2s - loss: 285.3022 - loglik: -2.7776e+02 - logprior: -7.5424e+00
Epoch 3/10
10/10 - 2s - loss: 278.7159 - loglik: -2.7753e+02 - logprior: -1.1878e+00
Epoch 4/10
10/10 - 2s - loss: 275.7076 - loglik: -2.7715e+02 - logprior: 1.4446
Epoch 5/10
10/10 - 2s - loss: 273.9367 - loglik: -2.7684e+02 - logprior: 2.9045
Epoch 6/10
10/10 - 2s - loss: 273.7165 - loglik: -2.7746e+02 - logprior: 3.7440
Epoch 7/10
10/10 - 2s - loss: 273.5862 - loglik: -2.7784e+02 - logprior: 4.2557
Epoch 8/10
10/10 - 2s - loss: 272.4320 - loglik: -2.7704e+02 - logprior: 4.6079
Epoch 9/10
10/10 - 2s - loss: 272.8021 - loglik: -2.7769e+02 - logprior: 4.8869
Fitted a model with MAP estimate = -272.3451
Time for alignment: 64.3996
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.2624 - loglik: -3.8678e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 370.5702 - loglik: -3.6026e+02 - logprior: -1.0306e+01
Epoch 3/10
10/10 - 2s - loss: 335.2515 - loglik: -3.3110e+02 - logprior: -4.1525e+00
Epoch 4/10
10/10 - 2s - loss: 311.3766 - loglik: -3.0929e+02 - logprior: -2.0902e+00
Epoch 5/10
10/10 - 2s - loss: 302.1882 - loglik: -3.0117e+02 - logprior: -1.0217e+00
Epoch 6/10
10/10 - 2s - loss: 299.6772 - loglik: -2.9919e+02 - logprior: -4.8528e-01
Epoch 7/10
10/10 - 2s - loss: 297.1913 - loglik: -2.9704e+02 - logprior: -1.5046e-01
Epoch 8/10
10/10 - 2s - loss: 296.4711 - loglik: -2.9659e+02 - logprior: 0.1154
Epoch 9/10
10/10 - 2s - loss: 295.7532 - loglik: -2.9604e+02 - logprior: 0.2834
Epoch 10/10
10/10 - 2s - loss: 295.7834 - loglik: -2.9617e+02 - logprior: 0.3845
Fitted a model with MAP estimate = -295.4309
expansions: [(11, 4), (16, 1), (27, 1), (28, 4), (42, 1), (58, 2), (59, 1), (62, 1), (71, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 347.3437 - loglik: -2.9674e+02 - logprior: -5.0606e+01
Epoch 2/2
10/10 - 2s - loss: 305.6609 - loglik: -2.8670e+02 - logprior: -1.8959e+01
Fitted a model with MAP estimate = -299.0037
expansions: [(10, 1), (12, 1)]
discards: [ 69 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.2450 - loglik: -2.8484e+02 - logprior: -4.8406e+01
Epoch 2/2
10/10 - 2s - loss: 294.4256 - loglik: -2.8129e+02 - logprior: -1.3140e+01
Fitted a model with MAP estimate = -285.6975
expansions: [(13, 1)]
discards: [35]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 320.8212 - loglik: -2.8023e+02 - logprior: -4.0591e+01
Epoch 2/10
10/10 - 2s - loss: 286.9931 - loglik: -2.7918e+02 - logprior: -7.8112e+00
Epoch 3/10
10/10 - 2s - loss: 278.9307 - loglik: -2.7776e+02 - logprior: -1.1714e+00
Epoch 4/10
10/10 - 2s - loss: 276.4024 - loglik: -2.7780e+02 - logprior: 1.3978
Epoch 5/10
10/10 - 2s - loss: 274.2826 - loglik: -2.7695e+02 - logprior: 2.6641
Epoch 6/10
10/10 - 2s - loss: 273.2708 - loglik: -2.7679e+02 - logprior: 3.5155
Epoch 7/10
10/10 - 2s - loss: 273.3637 - loglik: -2.7758e+02 - logprior: 4.2150
Fitted a model with MAP estimate = -272.6499
Time for alignment: 57.6316
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.2347 - loglik: -3.8675e+02 - logprior: -4.5485e+01
Epoch 2/10
10/10 - 2s - loss: 370.2652 - loglik: -3.5996e+02 - logprior: -1.0306e+01
Epoch 3/10
10/10 - 2s - loss: 335.2679 - loglik: -3.3123e+02 - logprior: -4.0383e+00
Epoch 4/10
10/10 - 2s - loss: 312.0042 - loglik: -3.1013e+02 - logprior: -1.8731e+00
Epoch 5/10
10/10 - 2s - loss: 303.6356 - loglik: -3.0275e+02 - logprior: -8.8102e-01
Epoch 6/10
10/10 - 2s - loss: 299.6496 - loglik: -2.9918e+02 - logprior: -4.6743e-01
Epoch 7/10
10/10 - 2s - loss: 298.5178 - loglik: -2.9838e+02 - logprior: -1.3581e-01
Epoch 8/10
10/10 - 2s - loss: 296.6971 - loglik: -2.9680e+02 - logprior: 0.1048
Epoch 9/10
10/10 - 2s - loss: 296.7574 - loglik: -2.9697e+02 - logprior: 0.2097
Fitted a model with MAP estimate = -295.9108
expansions: [(10, 2), (17, 1), (18, 1), (26, 1), (28, 2), (42, 1), (58, 2), (59, 2), (71, 1), (73, 1), (82, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 347.2189 - loglik: -2.9645e+02 - logprior: -5.0774e+01
Epoch 2/2
10/10 - 2s - loss: 306.2128 - loglik: -2.8716e+02 - logprior: -1.9056e+01
Fitted a model with MAP estimate = -299.9780
expansions: [(6, 2)]
discards: [ 0  9 66]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 336.9887 - loglik: -2.8724e+02 - logprior: -4.9750e+01
Epoch 2/2
10/10 - 2s - loss: 302.0963 - loglik: -2.8519e+02 - logprior: -1.6905e+01
Fitted a model with MAP estimate = -294.2499
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 322.9143 - loglik: -2.8379e+02 - logprior: -3.9122e+01
Epoch 2/10
10/10 - 2s - loss: 288.9716 - loglik: -2.8123e+02 - logprior: -7.7438e+00
Epoch 3/10
10/10 - 2s - loss: 281.0558 - loglik: -2.7964e+02 - logprior: -1.4161e+00
Epoch 4/10
10/10 - 2s - loss: 277.1750 - loglik: -2.7823e+02 - logprior: 1.0591
Epoch 5/10
10/10 - 2s - loss: 275.6291 - loglik: -2.7812e+02 - logprior: 2.4883
Epoch 6/10
10/10 - 2s - loss: 275.1239 - loglik: -2.7848e+02 - logprior: 3.3610
Epoch 7/10
10/10 - 2s - loss: 274.7018 - loglik: -2.7857e+02 - logprior: 3.8668
Epoch 8/10
10/10 - 2s - loss: 273.8457 - loglik: -2.7806e+02 - logprior: 4.2171
Epoch 9/10
10/10 - 2s - loss: 273.8485 - loglik: -2.7835e+02 - logprior: 4.4997
Fitted a model with MAP estimate = -273.6326
Time for alignment: 58.9296
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 432.6337 - loglik: -3.8715e+02 - logprior: -4.5482e+01
Epoch 2/10
10/10 - 2s - loss: 370.1827 - loglik: -3.5988e+02 - logprior: -1.0305e+01
Epoch 3/10
10/10 - 2s - loss: 334.9053 - loglik: -3.3080e+02 - logprior: -4.1050e+00
Epoch 4/10
10/10 - 2s - loss: 310.9537 - loglik: -3.0881e+02 - logprior: -2.1413e+00
Epoch 5/10
10/10 - 2s - loss: 302.2457 - loglik: -3.0100e+02 - logprior: -1.2436e+00
Epoch 6/10
10/10 - 2s - loss: 298.2175 - loglik: -2.9753e+02 - logprior: -6.8503e-01
Epoch 7/10
10/10 - 2s - loss: 296.4996 - loglik: -2.9627e+02 - logprior: -2.3335e-01
Epoch 8/10
10/10 - 2s - loss: 296.1963 - loglik: -2.9621e+02 - logprior: 0.0170
Epoch 9/10
10/10 - 2s - loss: 295.0850 - loglik: -2.9526e+02 - logprior: 0.1731
Epoch 10/10
10/10 - 2s - loss: 295.3063 - loglik: -2.9560e+02 - logprior: 0.2971
Fitted a model with MAP estimate = -294.9433
expansions: [(10, 4), (17, 1), (18, 1), (27, 1), (28, 3), (48, 2), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.8151 - loglik: -2.9616e+02 - logprior: -5.0656e+01
Epoch 2/2
10/10 - 2s - loss: 306.1473 - loglik: -2.8695e+02 - logprior: -1.9202e+01
Fitted a model with MAP estimate = -298.8898
expansions: [(11, 1)]
discards: [58 70 76]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.8541 - loglik: -2.8450e+02 - logprior: -4.8357e+01
Epoch 2/2
10/10 - 2s - loss: 294.1104 - loglik: -2.8123e+02 - logprior: -1.2876e+01
Fitted a model with MAP estimate = -285.1649
expansions: [(10, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 320.1756 - loglik: -2.7966e+02 - logprior: -4.0520e+01
Epoch 2/10
10/10 - 2s - loss: 286.4881 - loglik: -2.7869e+02 - logprior: -7.7979e+00
Epoch 3/10
10/10 - 2s - loss: 278.5005 - loglik: -2.7733e+02 - logprior: -1.1717e+00
Epoch 4/10
10/10 - 2s - loss: 276.4376 - loglik: -2.7781e+02 - logprior: 1.3769
Epoch 5/10
10/10 - 2s - loss: 273.9822 - loglik: -2.7661e+02 - logprior: 2.6262
Epoch 6/10
10/10 - 2s - loss: 273.3416 - loglik: -2.7681e+02 - logprior: 3.4640
Epoch 7/10
10/10 - 2s - loss: 273.0848 - loglik: -2.7725e+02 - logprior: 4.1677
Epoch 8/10
10/10 - 2s - loss: 271.7913 - loglik: -2.7644e+02 - logprior: 4.6512
Epoch 9/10
10/10 - 2s - loss: 271.9917 - loglik: -2.7696e+02 - logprior: 4.9694
Fitted a model with MAP estimate = -271.8787
Time for alignment: 63.2921
Computed alignments with likelihoods: ['-272.0159', '-272.3451', '-272.6499', '-273.6326', '-271.8787']
Best model has likelihood: -271.8787  (prior= 5.1280 )
time for generating output: 0.1610
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.8878576952822892
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 453.0035 - loglik: -1.8609e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3064 - loglik: -1.6037e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 170.5251 - loglik: -1.3978e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 142.5050 - loglik: -1.2582e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 128.9663 - loglik: -1.1995e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 121.5786 - loglik: -1.1785e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 116.6985 - loglik: -1.1633e+02 - logprior: -3.7085e-01
Epoch 8/10
10/10 - 1s - loss: 113.9294 - loglik: -1.1572e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 112.3552 - loglik: -1.1573e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 111.2829 - loglik: -1.1585e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -110.8160
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.0465 - loglik: -1.0958e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 203.7114 - loglik: -9.7383e+01 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -155.3794
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.6631 - loglik: -9.7307e+01 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 209.9552 - loglik: -9.5676e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -180.0823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 369.6450 - loglik: -9.4816e+01 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 167.1075 - loglik: -9.4055e+01 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 116.3991 - loglik: -9.4285e+01 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 98.2742 - loglik: -9.4763e+01 - logprior: -3.5114e+00
Epoch 5/10
10/10 - 1s - loss: 89.1204 - loglik: -9.5168e+01 - logprior: 6.0473
Epoch 6/10
10/10 - 1s - loss: 83.8036 - loglik: -9.5462e+01 - logprior: 11.6582
Epoch 7/10
10/10 - 1s - loss: 80.4257 - loglik: -9.5653e+01 - logprior: 15.2269
Epoch 8/10
10/10 - 1s - loss: 78.0409 - loglik: -9.5765e+01 - logprior: 17.7245
Epoch 9/10
10/10 - 1s - loss: 76.2171 - loglik: -9.5866e+01 - logprior: 19.6489
Epoch 10/10
10/10 - 1s - loss: 74.7076 - loglik: -9.5968e+01 - logprior: 21.2606
Fitted a model with MAP estimate = -73.9585
Time for alignment: 32.0938
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.0035 - loglik: -1.8609e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3064 - loglik: -1.6037e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 170.5251 - loglik: -1.3978e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 142.5049 - loglik: -1.2582e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 128.9663 - loglik: -1.1995e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 121.5786 - loglik: -1.1785e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 116.6985 - loglik: -1.1633e+02 - logprior: -3.7085e-01
Epoch 8/10
10/10 - 1s - loss: 113.9293 - loglik: -1.1572e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 112.3552 - loglik: -1.1573e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 111.2829 - loglik: -1.1585e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -110.8158
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 464.0465 - loglik: -1.0958e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 203.7114 - loglik: -9.7383e+01 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -155.3795
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.6631 - loglik: -9.7307e+01 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 209.9552 - loglik: -9.5676e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -180.0823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 369.6451 - loglik: -9.4816e+01 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 167.1077 - loglik: -9.4055e+01 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 116.3996 - loglik: -9.4285e+01 - logprior: -2.2115e+01
Epoch 4/10
10/10 - 1s - loss: 98.2749 - loglik: -9.4763e+01 - logprior: -3.5119e+00
Epoch 5/10
10/10 - 1s - loss: 89.1213 - loglik: -9.5168e+01 - logprior: 6.0465
Epoch 6/10
10/10 - 1s - loss: 83.8047 - loglik: -9.5462e+01 - logprior: 11.6572
Epoch 7/10
10/10 - 1s - loss: 80.4270 - loglik: -9.5653e+01 - logprior: 15.2258
Epoch 8/10
10/10 - 1s - loss: 78.0425 - loglik: -9.5766e+01 - logprior: 17.7232
Epoch 9/10
10/10 - 1s - loss: 76.2187 - loglik: -9.5866e+01 - logprior: 19.6474
Epoch 10/10
10/10 - 1s - loss: 74.7094 - loglik: -9.5968e+01 - logprior: 21.2590
Fitted a model with MAP estimate = -73.9602
Time for alignment: 29.4914
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.0035 - loglik: -1.8609e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3064 - loglik: -1.6037e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 170.5251 - loglik: -1.3978e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 142.5049 - loglik: -1.2582e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 128.9663 - loglik: -1.1995e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 121.5786 - loglik: -1.1785e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 116.6986 - loglik: -1.1633e+02 - logprior: -3.7085e-01
Epoch 8/10
10/10 - 1s - loss: 113.9293 - loglik: -1.1572e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 112.3552 - loglik: -1.1573e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 111.2830 - loglik: -1.1585e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -110.8158
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 464.0465 - loglik: -1.0958e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 203.7115 - loglik: -9.7383e+01 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -155.3795
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.6631 - loglik: -9.7307e+01 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 209.9552 - loglik: -9.5676e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -180.0823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6450 - loglik: -9.4816e+01 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 167.1074 - loglik: -9.4055e+01 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 116.3989 - loglik: -9.4285e+01 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 98.2740 - loglik: -9.4763e+01 - logprior: -3.5112e+00
Epoch 5/10
10/10 - 1s - loss: 89.1200 - loglik: -9.5168e+01 - logprior: 6.0476
Epoch 6/10
10/10 - 1s - loss: 83.8033 - loglik: -9.5462e+01 - logprior: 11.6586
Epoch 7/10
10/10 - 1s - loss: 80.4253 - loglik: -9.5653e+01 - logprior: 15.2273
Epoch 8/10
10/10 - 1s - loss: 78.0405 - loglik: -9.5765e+01 - logprior: 17.7250
Epoch 9/10
10/10 - 1s - loss: 76.2165 - loglik: -9.5866e+01 - logprior: 19.6494
Epoch 10/10
10/10 - 1s - loss: 74.7070 - loglik: -9.5968e+01 - logprior: 21.2612
Fitted a model with MAP estimate = -73.9580
Time for alignment: 27.1725
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.0035 - loglik: -1.8609e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3064 - loglik: -1.6037e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 170.5251 - loglik: -1.3978e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 142.5049 - loglik: -1.2582e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 128.9663 - loglik: -1.1995e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 121.5786 - loglik: -1.1785e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 116.6985 - loglik: -1.1633e+02 - logprior: -3.7085e-01
Epoch 8/10
10/10 - 1s - loss: 113.9293 - loglik: -1.1572e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 112.3552 - loglik: -1.1573e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 111.2829 - loglik: -1.1585e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -110.8160
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.0465 - loglik: -1.0958e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 203.7114 - loglik: -9.7383e+01 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -155.3795
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.6631 - loglik: -9.7307e+01 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 209.9552 - loglik: -9.5676e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -180.0823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6450 - loglik: -9.4816e+01 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 167.1075 - loglik: -9.4055e+01 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 116.3990 - loglik: -9.4285e+01 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 98.2742 - loglik: -9.4763e+01 - logprior: -3.5113e+00
Epoch 5/10
10/10 - 1s - loss: 89.1204 - loglik: -9.5168e+01 - logprior: 6.0474
Epoch 6/10
10/10 - 1s - loss: 83.8036 - loglik: -9.5462e+01 - logprior: 11.6582
Epoch 7/10
10/10 - 1s - loss: 80.4256 - loglik: -9.5653e+01 - logprior: 15.2270
Epoch 8/10
10/10 - 1s - loss: 78.0409 - loglik: -9.5765e+01 - logprior: 17.7246
Epoch 9/10
10/10 - 1s - loss: 76.2170 - loglik: -9.5866e+01 - logprior: 19.6490
Epoch 10/10
10/10 - 1s - loss: 74.7075 - loglik: -9.5968e+01 - logprior: 21.2607
Fitted a model with MAP estimate = -73.9583
Time for alignment: 30.3800
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 453.0035 - loglik: -1.8609e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3064 - loglik: -1.6037e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 170.5251 - loglik: -1.3978e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 142.5049 - loglik: -1.2582e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 128.9662 - loglik: -1.1995e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 121.5787 - loglik: -1.1785e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 116.6986 - loglik: -1.1633e+02 - logprior: -3.7085e-01
Epoch 8/10
10/10 - 1s - loss: 113.9292 - loglik: -1.1572e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 112.3552 - loglik: -1.1573e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 111.2830 - loglik: -1.1585e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -110.8159
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.0465 - loglik: -1.0958e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 203.7114 - loglik: -9.7383e+01 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -155.3794
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.6631 - loglik: -9.7307e+01 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 209.9552 - loglik: -9.5676e+01 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -180.0823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6450 - loglik: -9.4816e+01 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 167.1075 - loglik: -9.4055e+01 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 116.3990 - loglik: -9.4285e+01 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 98.2741 - loglik: -9.4763e+01 - logprior: -3.5113e+00
Epoch 5/10
10/10 - 1s - loss: 89.1203 - loglik: -9.5168e+01 - logprior: 6.0474
Epoch 6/10
10/10 - 1s - loss: 83.8035 - loglik: -9.5462e+01 - logprior: 11.6583
Epoch 7/10
10/10 - 1s - loss: 80.4255 - loglik: -9.5653e+01 - logprior: 15.2271
Epoch 8/10
10/10 - 1s - loss: 78.0408 - loglik: -9.5766e+01 - logprior: 17.7247
Epoch 9/10
10/10 - 1s - loss: 76.2169 - loglik: -9.5866e+01 - logprior: 19.6491
Epoch 10/10
10/10 - 1s - loss: 74.7073 - loglik: -9.5968e+01 - logprior: 21.2609
Fitted a model with MAP estimate = -73.9582
Time for alignment: 29.5145
Computed alignments with likelihoods: ['-73.9585', '-73.9602', '-73.9580', '-73.9583', '-73.9582']
Best model has likelihood: -73.9580  (prior= 22.0669 )
time for generating output: 0.1140
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.823262839879154
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 696.8038 - loglik: -6.9274e+02 - logprior: -4.0595e+00
Epoch 2/10
26/26 - 9s - loss: 598.4590 - loglik: -5.9727e+02 - logprior: -1.1902e+00
Epoch 3/10
26/26 - 9s - loss: 582.0804 - loglik: -5.8087e+02 - logprior: -1.2067e+00
Epoch 4/10
26/26 - 9s - loss: 578.4601 - loglik: -5.7731e+02 - logprior: -1.1519e+00
Epoch 5/10
26/26 - 9s - loss: 576.5753 - loglik: -5.7546e+02 - logprior: -1.1154e+00
Epoch 6/10
26/26 - 9s - loss: 576.1944 - loglik: -5.7510e+02 - logprior: -1.0930e+00
Epoch 7/10
26/26 - 9s - loss: 576.3391 - loglik: -5.7527e+02 - logprior: -1.0700e+00
Fitted a model with MAP estimate = -575.7344
expansions: [(11, 1), (55, 1), (83, 1), (84, 1), (92, 1), (93, 1), (109, 4), (160, 1), (173, 1), (175, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 578.3511 - loglik: -5.7396e+02 - logprior: -4.3893e+00
Epoch 2/2
26/26 - 9s - loss: 571.5754 - loglik: -5.7058e+02 - logprior: -9.9630e-01
Fitted a model with MAP estimate = -568.7120
expansions: [(116, 1)]
discards: [  0 188]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 578.5635 - loglik: -5.7226e+02 - logprior: -6.3075e+00
Epoch 2/2
26/26 - 9s - loss: 573.1993 - loglik: -5.7063e+02 - logprior: -2.5716e+00
Fitted a model with MAP estimate = -571.4586
expansions: [(0, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 574.9348 - loglik: -5.7087e+02 - logprior: -4.0646e+00
Epoch 2/10
26/26 - 10s - loss: 568.3625 - loglik: -5.6782e+02 - logprior: -5.4750e-01
Epoch 3/10
26/26 - 10s - loss: 567.6896 - loglik: -5.6742e+02 - logprior: -2.6715e-01
Epoch 4/10
26/26 - 10s - loss: 565.3032 - loglik: -5.6513e+02 - logprior: -1.6873e-01
Epoch 5/10
26/26 - 10s - loss: 566.3259 - loglik: -5.6630e+02 - logprior: -2.6467e-02
Fitted a model with MAP estimate = -564.3895
Time for alignment: 200.4517
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 691.1318 - loglik: -6.8709e+02 - logprior: -4.0463e+00
Epoch 2/10
26/26 - 9s - loss: 586.8287 - loglik: -5.8557e+02 - logprior: -1.2573e+00
Epoch 3/10
26/26 - 9s - loss: 572.3692 - loglik: -5.7098e+02 - logprior: -1.3902e+00
Epoch 4/10
26/26 - 9s - loss: 569.7537 - loglik: -5.6834e+02 - logprior: -1.4184e+00
Epoch 5/10
26/26 - 9s - loss: 568.3765 - loglik: -5.6696e+02 - logprior: -1.4191e+00
Epoch 6/10
26/26 - 9s - loss: 567.3355 - loglik: -5.6592e+02 - logprior: -1.4167e+00
Epoch 7/10
26/26 - 9s - loss: 568.0635 - loglik: -5.6664e+02 - logprior: -1.4269e+00
Fitted a model with MAP estimate = -567.3146
expansions: [(0, 3), (11, 1), (50, 1), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 575.0458 - loglik: -5.7017e+02 - logprior: -4.8755e+00
Epoch 2/2
26/26 - 9s - loss: 569.1068 - loglik: -5.6779e+02 - logprior: -1.3166e+00
Fitted a model with MAP estimate = -567.4978
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 576.3397 - loglik: -5.6973e+02 - logprior: -6.6063e+00
Epoch 2/2
26/26 - 9s - loss: 571.9257 - loglik: -5.6910e+02 - logprior: -2.8266e+00
Fitted a model with MAP estimate = -570.3246
expansions: [(0, 5), (108, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 573.6610 - loglik: -5.6934e+02 - logprior: -4.3183e+00
Epoch 2/10
26/26 - 9s - loss: 566.1300 - loglik: -5.6522e+02 - logprior: -9.0502e-01
Epoch 3/10
26/26 - 9s - loss: 564.8357 - loglik: -5.6411e+02 - logprior: -7.2798e-01
Epoch 4/10
26/26 - 9s - loss: 564.4044 - loglik: -5.6370e+02 - logprior: -7.0224e-01
Epoch 5/10
26/26 - 9s - loss: 562.0795 - loglik: -5.6148e+02 - logprior: -6.0163e-01
Epoch 6/10
26/26 - 9s - loss: 561.7162 - loglik: -5.6124e+02 - logprior: -4.8082e-01
Epoch 7/10
26/26 - 9s - loss: 560.4573 - loglik: -5.6013e+02 - logprior: -3.2701e-01
Epoch 8/10
26/26 - 9s - loss: 562.8051 - loglik: -5.6260e+02 - logprior: -2.0536e-01
Fitted a model with MAP estimate = -561.1220
Time for alignment: 218.4851
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 691.3109 - loglik: -6.8730e+02 - logprior: -4.0153e+00
Epoch 2/10
26/26 - 9s - loss: 590.5670 - loglik: -5.8955e+02 - logprior: -1.0181e+00
Epoch 3/10
26/26 - 9s - loss: 577.0562 - loglik: -5.7608e+02 - logprior: -9.7522e-01
Epoch 4/10
26/26 - 9s - loss: 574.1029 - loglik: -5.7315e+02 - logprior: -9.4860e-01
Epoch 5/10
26/26 - 9s - loss: 574.1238 - loglik: -5.7319e+02 - logprior: -9.3558e-01
Fitted a model with MAP estimate = -572.6722
expansions: [(11, 1), (55, 2), (56, 1), (77, 1), (162, 1), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 575.9232 - loglik: -5.7163e+02 - logprior: -4.2950e+00
Epoch 2/2
26/26 - 9s - loss: 572.7218 - loglik: -5.7183e+02 - logprior: -8.9636e-01
Fitted a model with MAP estimate = -569.3862
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 579.4535 - loglik: -5.7319e+02 - logprior: -6.2669e+00
Epoch 2/2
26/26 - 9s - loss: 572.5016 - loglik: -5.6999e+02 - logprior: -2.5153e+00
Fitted a model with MAP estimate = -571.8189
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 574.3974 - loglik: -5.7044e+02 - logprior: -3.9592e+00
Epoch 2/10
26/26 - 9s - loss: 569.5283 - loglik: -5.6902e+02 - logprior: -5.0665e-01
Epoch 3/10
26/26 - 9s - loss: 567.2522 - loglik: -5.6700e+02 - logprior: -2.5171e-01
Epoch 4/10
26/26 - 9s - loss: 565.8535 - loglik: -5.6566e+02 - logprior: -1.9175e-01
Epoch 5/10
26/26 - 9s - loss: 565.2077 - loglik: -5.6514e+02 - logprior: -6.8015e-02
Epoch 6/10
26/26 - 9s - loss: 565.5056 - loglik: -5.6556e+02 - logprior: 0.0506
Fitted a model with MAP estimate = -564.7757
Time for alignment: 184.4946
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 689.4030 - loglik: -6.8535e+02 - logprior: -4.0488e+00
Epoch 2/10
26/26 - 9s - loss: 585.6986 - loglik: -5.8442e+02 - logprior: -1.2801e+00
Epoch 3/10
26/26 - 9s - loss: 572.8624 - loglik: -5.7163e+02 - logprior: -1.2340e+00
Epoch 4/10
26/26 - 9s - loss: 570.4249 - loglik: -5.6918e+02 - logprior: -1.2426e+00
Epoch 5/10
26/26 - 9s - loss: 569.2916 - loglik: -5.6799e+02 - logprior: -1.2997e+00
Epoch 6/10
26/26 - 9s - loss: 569.8287 - loglik: -5.6852e+02 - logprior: -1.3073e+00
Fitted a model with MAP estimate = -568.6556
expansions: [(57, 2), (107, 3), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 575.4484 - loglik: -5.7080e+02 - logprior: -4.6514e+00
Epoch 2/2
26/26 - 9s - loss: 569.4523 - loglik: -5.6825e+02 - logprior: -1.2051e+00
Fitted a model with MAP estimate = -567.6460
expansions: []
discards: [ 57  58 109 110 111]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 11s - loss: 575.0948 - loglik: -5.7065e+02 - logprior: -4.4478e+00
Epoch 2/2
26/26 - 9s - loss: 571.6530 - loglik: -5.7074e+02 - logprior: -9.1140e-01
Fitted a model with MAP estimate = -569.7026
expansions: [(107, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 574.7628 - loglik: -5.7057e+02 - logprior: -4.1919e+00
Epoch 2/10
26/26 - 9s - loss: 569.5405 - loglik: -5.6890e+02 - logprior: -6.3668e-01
Epoch 3/10
26/26 - 9s - loss: 567.8824 - loglik: -5.6743e+02 - logprior: -4.5269e-01
Epoch 4/10
26/26 - 9s - loss: 566.6303 - loglik: -5.6620e+02 - logprior: -4.3094e-01
Epoch 5/10
26/26 - 9s - loss: 567.0509 - loglik: -5.6671e+02 - logprior: -3.3655e-01
Fitted a model with MAP estimate = -565.8243
Time for alignment: 178.2438
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 695.1230 - loglik: -6.9103e+02 - logprior: -4.0967e+00
Epoch 2/10
26/26 - 9s - loss: 592.1970 - loglik: -5.9067e+02 - logprior: -1.5305e+00
Epoch 3/10
26/26 - 9s - loss: 575.8724 - loglik: -5.7434e+02 - logprior: -1.5327e+00
Epoch 4/10
26/26 - 9s - loss: 570.9001 - loglik: -5.6924e+02 - logprior: -1.6606e+00
Epoch 5/10
26/26 - 9s - loss: 571.3509 - loglik: -5.6966e+02 - logprior: -1.6923e+00
Fitted a model with MAP estimate = -569.5257
expansions: [(11, 1), (58, 1), (78, 1), (84, 1), (93, 1), (106, 1), (107, 1), (108, 1), (110, 1), (111, 1), (159, 1), (176, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 578.4933 - loglik: -5.7189e+02 - logprior: -6.6071e+00
Epoch 2/2
26/26 - 9s - loss: 569.0562 - loglik: -5.6629e+02 - logprior: -2.7646e+00
Fitted a model with MAP estimate = -567.0368
expansions: [(0, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 571.1058 - loglik: -5.6662e+02 - logprior: -4.4905e+00
Epoch 2/2
26/26 - 9s - loss: 564.8609 - loglik: -5.6375e+02 - logprior: -1.1081e+00
Fitted a model with MAP estimate = -562.9819
expansions: []
discards: [0 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 574.1804 - loglik: -5.6800e+02 - logprior: -6.1832e+00
Epoch 2/10
26/26 - 9s - loss: 568.1228 - loglik: -5.6624e+02 - logprior: -1.8826e+00
Epoch 3/10
26/26 - 9s - loss: 562.9480 - loglik: -5.6268e+02 - logprior: -2.7254e-01
Epoch 4/10
26/26 - 9s - loss: 564.0784 - loglik: -5.6379e+02 - logprior: -2.8657e-01
Fitted a model with MAP estimate = -561.9584
Time for alignment: 168.7651
Computed alignments with likelihoods: ['-564.3895', '-561.1220', '-564.7757', '-565.8243', '-561.9584']
Best model has likelihood: -561.1220  (prior= -0.1066 )
time for generating output: 0.2750
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.4587096774193548
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 421.9977 - loglik: -3.4526e+02 - logprior: -7.6738e+01
Epoch 2/10
10/10 - 1s - loss: 336.6615 - loglik: -3.1906e+02 - logprior: -1.7606e+01
Epoch 3/10
10/10 - 1s - loss: 297.9438 - loglik: -2.9159e+02 - logprior: -6.3539e+00
Epoch 4/10
10/10 - 1s - loss: 278.5808 - loglik: -2.7634e+02 - logprior: -2.2362e+00
Epoch 5/10
10/10 - 1s - loss: 271.9860 - loglik: -2.7187e+02 - logprior: -1.1821e-01
Epoch 6/10
10/10 - 1s - loss: 268.1699 - loglik: -2.6910e+02 - logprior: 0.9311
Epoch 7/10
10/10 - 1s - loss: 265.3873 - loglik: -2.6697e+02 - logprior: 1.5785
Epoch 8/10
10/10 - 1s - loss: 264.3495 - loglik: -2.6648e+02 - logprior: 2.1271
Epoch 9/10
10/10 - 1s - loss: 262.8849 - loglik: -2.6535e+02 - logprior: 2.4649
Epoch 10/10
10/10 - 1s - loss: 262.4380 - loglik: -2.6516e+02 - logprior: 2.7183
Fitted a model with MAP estimate = -262.0622
expansions: [(5, 3), (6, 2), (12, 4), (18, 2), (19, 1), (36, 4), (50, 1), (55, 2), (63, 4), (84, 5), (86, 2), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 350.4622 - loglik: -2.6500e+02 - logprior: -8.5462e+01
Epoch 2/2
10/10 - 2s - loss: 284.0991 - loglik: -2.5145e+02 - logprior: -3.2649e+01
Fitted a model with MAP estimate = -271.7525
expansions: [(0, 3), (110, 2)]
discards: [ 0  7  8 49 71]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 316.0714 - loglik: -2.4815e+02 - logprior: -6.7917e+01
Epoch 2/2
10/10 - 1s - loss: 257.3812 - loglik: -2.4253e+02 - logprior: -1.4852e+01
Fitted a model with MAP estimate = -248.0205
expansions: [(9, 1), (108, 1)]
discards: [ 0  1 28 64 79]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 328.4012 - loglik: -2.4565e+02 - logprior: -8.2753e+01
Epoch 2/10
10/10 - 1s - loss: 267.8506 - loglik: -2.4229e+02 - logprior: -2.5565e+01
Epoch 3/10
10/10 - 1s - loss: 246.5633 - loglik: -2.4041e+02 - logprior: -6.1524e+00
Epoch 4/10
10/10 - 1s - loss: 237.5987 - loglik: -2.3924e+02 - logprior: 1.6409
Epoch 5/10
10/10 - 1s - loss: 233.9534 - loglik: -2.3862e+02 - logprior: 4.6622
Epoch 6/10
10/10 - 1s - loss: 232.5634 - loglik: -2.3878e+02 - logprior: 6.2163
Epoch 7/10
10/10 - 1s - loss: 231.1554 - loglik: -2.3838e+02 - logprior: 7.2239
Epoch 8/10
10/10 - 1s - loss: 230.8560 - loglik: -2.3888e+02 - logprior: 8.0233
Epoch 9/10
10/10 - 1s - loss: 230.3738 - loglik: -2.3907e+02 - logprior: 8.6991
Epoch 10/10
10/10 - 1s - loss: 229.5948 - loglik: -2.3886e+02 - logprior: 9.2667
Fitted a model with MAP estimate = -229.5597
Time for alignment: 52.1923
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.8634 - loglik: -3.4513e+02 - logprior: -7.6738e+01
Epoch 2/10
10/10 - 1s - loss: 336.6084 - loglik: -3.1900e+02 - logprior: -1.7605e+01
Epoch 3/10
10/10 - 1s - loss: 297.7639 - loglik: -2.9143e+02 - logprior: -6.3309e+00
Epoch 4/10
10/10 - 1s - loss: 278.9789 - loglik: -2.7676e+02 - logprior: -2.2202e+00
Epoch 5/10
10/10 - 1s - loss: 272.0842 - loglik: -2.7193e+02 - logprior: -1.4949e-01
Epoch 6/10
10/10 - 1s - loss: 268.4839 - loglik: -2.6954e+02 - logprior: 1.0603
Epoch 7/10
10/10 - 1s - loss: 266.4161 - loglik: -2.6816e+02 - logprior: 1.7407
Epoch 8/10
10/10 - 1s - loss: 264.6114 - loglik: -2.6691e+02 - logprior: 2.3023
Epoch 9/10
10/10 - 1s - loss: 263.7581 - loglik: -2.6640e+02 - logprior: 2.6427
Epoch 10/10
10/10 - 1s - loss: 262.8954 - loglik: -2.6579e+02 - logprior: 2.8941
Fitted a model with MAP estimate = -262.6304
expansions: [(5, 1), (6, 1), (11, 4), (36, 4), (45, 2), (48, 2), (63, 4), (83, 4), (86, 1), (87, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 351.8140 - loglik: -2.6623e+02 - logprior: -8.5587e+01
Epoch 2/2
10/10 - 1s - loss: 287.7916 - loglik: -2.5500e+02 - logprior: -3.2790e+01
Fitted a model with MAP estimate = -275.7934
expansions: [(0, 3), (12, 1), (56, 1), (113, 1), (114, 1)]
discards: [  0  43  76 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.0258 - loglik: -2.5056e+02 - logprior: -6.7469e+01
Epoch 2/2
10/10 - 1s - loss: 258.2595 - loglik: -2.4411e+02 - logprior: -1.4151e+01
Fitted a model with MAP estimate = -248.9279
expansions: [(63, 1)]
discards: [ 0  1 45]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 327.9533 - loglik: -2.4599e+02 - logprior: -8.1964e+01
Epoch 2/10
10/10 - 1s - loss: 265.5424 - loglik: -2.4191e+02 - logprior: -2.3630e+01
Epoch 3/10
10/10 - 1s - loss: 245.0577 - loglik: -2.4006e+02 - logprior: -4.9945e+00
Epoch 4/10
10/10 - 1s - loss: 236.7355 - loglik: -2.3850e+02 - logprior: 1.7649
Epoch 5/10
10/10 - 1s - loss: 233.4847 - loglik: -2.3804e+02 - logprior: 4.5587
Epoch 6/10
10/10 - 1s - loss: 231.6930 - loglik: -2.3777e+02 - logprior: 6.0785
Epoch 7/10
10/10 - 1s - loss: 230.7477 - loglik: -2.3780e+02 - logprior: 7.0513
Epoch 8/10
10/10 - 1s - loss: 229.9250 - loglik: -2.3776e+02 - logprior: 7.8329
Epoch 9/10
10/10 - 1s - loss: 229.1367 - loglik: -2.3762e+02 - logprior: 8.4793
Epoch 10/10
10/10 - 1s - loss: 228.9881 - loglik: -2.3801e+02 - logprior: 9.0224
Fitted a model with MAP estimate = -228.5819
Time for alignment: 49.4362
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.8925 - loglik: -3.4515e+02 - logprior: -7.6738e+01
Epoch 2/10
10/10 - 1s - loss: 336.7373 - loglik: -3.1912e+02 - logprior: -1.7614e+01
Epoch 3/10
10/10 - 1s - loss: 296.9932 - loglik: -2.9055e+02 - logprior: -6.4471e+00
Epoch 4/10
10/10 - 1s - loss: 277.7217 - loglik: -2.7529e+02 - logprior: -2.4356e+00
Epoch 5/10
10/10 - 1s - loss: 270.3867 - loglik: -2.7006e+02 - logprior: -3.2752e-01
Epoch 6/10
10/10 - 1s - loss: 266.6262 - loglik: -2.6738e+02 - logprior: 0.7569
Epoch 7/10
10/10 - 1s - loss: 264.2877 - loglik: -2.6579e+02 - logprior: 1.5054
Epoch 8/10
10/10 - 1s - loss: 262.7970 - loglik: -2.6487e+02 - logprior: 2.0694
Epoch 9/10
10/10 - 1s - loss: 262.2075 - loglik: -2.6465e+02 - logprior: 2.4392
Epoch 10/10
10/10 - 1s - loss: 261.1247 - loglik: -2.6382e+02 - logprior: 2.6971
Fitted a model with MAP estimate = -260.9848
expansions: [(9, 2), (10, 2), (12, 4), (19, 2), (36, 4), (45, 3), (63, 4), (83, 4), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 348.2979 - loglik: -2.6273e+02 - logprior: -8.5566e+01
Epoch 2/2
10/10 - 1s - loss: 282.7813 - loglik: -2.5019e+02 - logprior: -3.2587e+01
Fitted a model with MAP estimate = -271.5675
expansions: [(0, 3), (112, 3)]
discards: [  0  47  48 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 314.8069 - loglik: -2.4740e+02 - logprior: -6.7403e+01
Epoch 2/2
10/10 - 1s - loss: 256.2711 - loglik: -2.4211e+02 - logprior: -1.4157e+01
Fitted a model with MAP estimate = -247.0385
expansions: [(114, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 325.6230 - loglik: -2.4294e+02 - logprior: -8.2679e+01
Epoch 2/10
10/10 - 1s - loss: 265.7803 - loglik: -2.3983e+02 - logprior: -2.5954e+01
Epoch 3/10
10/10 - 1s - loss: 245.2258 - loglik: -2.3880e+02 - logprior: -6.4231e+00
Epoch 4/10
10/10 - 1s - loss: 235.5547 - loglik: -2.3745e+02 - logprior: 1.8943
Epoch 5/10
10/10 - 1s - loss: 232.0248 - loglik: -2.3702e+02 - logprior: 4.9934
Epoch 6/10
10/10 - 1s - loss: 230.7926 - loglik: -2.3734e+02 - logprior: 6.5519
Epoch 7/10
10/10 - 1s - loss: 229.2136 - loglik: -2.3675e+02 - logprior: 7.5322
Epoch 8/10
10/10 - 1s - loss: 228.6167 - loglik: -2.3692e+02 - logprior: 8.3046
Epoch 9/10
10/10 - 1s - loss: 228.5781 - loglik: -2.3756e+02 - logprior: 8.9817
Epoch 10/10
10/10 - 1s - loss: 227.8842 - loglik: -2.3743e+02 - logprior: 9.5447
Fitted a model with MAP estimate = -227.5793
Time for alignment: 48.9213
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.8406 - loglik: -3.4510e+02 - logprior: -7.6737e+01
Epoch 2/10
10/10 - 1s - loss: 336.6641 - loglik: -3.1905e+02 - logprior: -1.7612e+01
Epoch 3/10
10/10 - 1s - loss: 297.4086 - loglik: -2.9087e+02 - logprior: -6.5340e+00
Epoch 4/10
10/10 - 1s - loss: 277.2057 - loglik: -2.7447e+02 - logprior: -2.7356e+00
Epoch 5/10
10/10 - 1s - loss: 269.4641 - loglik: -2.6880e+02 - logprior: -6.6580e-01
Epoch 6/10
10/10 - 1s - loss: 266.2742 - loglik: -2.6676e+02 - logprior: 0.4898
Epoch 7/10
10/10 - 1s - loss: 264.6189 - loglik: -2.6585e+02 - logprior: 1.2303
Epoch 8/10
10/10 - 1s - loss: 263.9676 - loglik: -2.6573e+02 - logprior: 1.7633
Epoch 9/10
10/10 - 1s - loss: 263.2227 - loglik: -2.6537e+02 - logprior: 2.1444
Epoch 10/10
10/10 - 1s - loss: 262.5472 - loglik: -2.6497e+02 - logprior: 2.4268
Fitted a model with MAP estimate = -262.6135
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 3), (19, 2), (36, 4), (45, 2), (48, 2), (78, 2), (83, 3), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 352.3303 - loglik: -2.6642e+02 - logprior: -8.5912e+01
Epoch 2/2
10/10 - 1s - loss: 287.3118 - loglik: -2.5420e+02 - logprior: -3.3115e+01
Fitted a model with MAP estimate = -276.0867
expansions: [(0, 4), (60, 1), (80, 3), (110, 3), (111, 2)]
discards: [  0   5  95 103]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 316.8259 - loglik: -2.4899e+02 - logprior: -6.7831e+01
Epoch 2/2
10/10 - 2s - loss: 254.4555 - loglik: -2.4032e+02 - logprior: -1.4131e+01
Fitted a model with MAP estimate = -244.3422
expansions: [(19, 1), (118, 2)]
discards: [ 1  2  3 47]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 305.5414 - loglik: -2.4015e+02 - logprior: -6.5394e+01
Epoch 2/10
10/10 - 2s - loss: 249.6089 - loglik: -2.3660e+02 - logprior: -1.3004e+01
Epoch 3/10
10/10 - 2s - loss: 237.9860 - loglik: -2.3588e+02 - logprior: -2.1048e+00
Epoch 4/10
10/10 - 2s - loss: 232.2705 - loglik: -2.3485e+02 - logprior: 2.5828
Epoch 5/10
10/10 - 2s - loss: 229.7461 - loglik: -2.3501e+02 - logprior: 5.2666
Epoch 6/10
10/10 - 2s - loss: 228.0586 - loglik: -2.3490e+02 - logprior: 6.8373
Epoch 7/10
10/10 - 2s - loss: 226.8149 - loglik: -2.3464e+02 - logprior: 7.8269
Epoch 8/10
10/10 - 2s - loss: 226.2297 - loglik: -2.3471e+02 - logprior: 8.4766
Epoch 9/10
10/10 - 2s - loss: 225.4933 - loglik: -2.3447e+02 - logprior: 8.9801
Epoch 10/10
10/10 - 2s - loss: 225.1103 - loglik: -2.3461e+02 - logprior: 9.4975
Fitted a model with MAP estimate = -224.7940
Time for alignment: 48.6537
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 422.0939 - loglik: -3.4535e+02 - logprior: -7.6739e+01
Epoch 2/10
10/10 - 1s - loss: 336.4893 - loglik: -3.1888e+02 - logprior: -1.7605e+01
Epoch 3/10
10/10 - 1s - loss: 298.4215 - loglik: -2.9200e+02 - logprior: -6.4192e+00
Epoch 4/10
10/10 - 1s - loss: 278.8248 - loglik: -2.7639e+02 - logprior: -2.4373e+00
Epoch 5/10
10/10 - 1s - loss: 271.7601 - loglik: -2.7144e+02 - logprior: -3.2298e-01
Epoch 6/10
10/10 - 1s - loss: 268.3074 - loglik: -2.6912e+02 - logprior: 0.8142
Epoch 7/10
10/10 - 1s - loss: 265.8302 - loglik: -2.6732e+02 - logprior: 1.4849
Epoch 8/10
10/10 - 1s - loss: 264.2090 - loglik: -2.6618e+02 - logprior: 1.9743
Epoch 9/10
10/10 - 1s - loss: 263.2030 - loglik: -2.6554e+02 - logprior: 2.3376
Epoch 10/10
10/10 - 1s - loss: 262.7780 - loglik: -2.6538e+02 - logprior: 2.6060
Fitted a model with MAP estimate = -262.4501
expansions: [(5, 1), (6, 2), (7, 1), (10, 1), (12, 3), (13, 1), (36, 4), (45, 3), (64, 3), (83, 4), (86, 1), (88, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 351.3931 - loglik: -2.6594e+02 - logprior: -8.5454e+01
Epoch 2/2
10/10 - 1s - loss: 286.9434 - loglik: -2.5451e+02 - logprior: -3.2430e+01
Fitted a model with MAP estimate = -276.1667
expansions: [(0, 3), (26, 2)]
discards: [  0   7   9  17  44  45 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 320.4906 - loglik: -2.5301e+02 - logprior: -6.7477e+01
Epoch 2/2
10/10 - 1s - loss: 261.3127 - loglik: -2.4682e+02 - logprior: -1.4488e+01
Fitted a model with MAP estimate = -251.2352
expansions: [(10, 1), (22, 1), (110, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.1878 - loglik: -2.4627e+02 - logprior: -8.2916e+01
Epoch 2/10
10/10 - 1s - loss: 268.9773 - loglik: -2.4208e+02 - logprior: -2.6898e+01
Epoch 3/10
10/10 - 1s - loss: 246.9979 - loglik: -2.3967e+02 - logprior: -7.3299e+00
Epoch 4/10
10/10 - 1s - loss: 236.8272 - loglik: -2.3860e+02 - logprior: 1.7705
Epoch 5/10
10/10 - 1s - loss: 233.0624 - loglik: -2.3803e+02 - logprior: 4.9650
Epoch 6/10
10/10 - 1s - loss: 230.9315 - loglik: -2.3745e+02 - logprior: 6.5219
Epoch 7/10
10/10 - 1s - loss: 230.5411 - loglik: -2.3804e+02 - logprior: 7.5034
Epoch 8/10
10/10 - 1s - loss: 229.5389 - loglik: -2.3783e+02 - logprior: 8.2930
Epoch 9/10
10/10 - 1s - loss: 229.0645 - loglik: -2.3801e+02 - logprior: 8.9458
Epoch 10/10
10/10 - 1s - loss: 228.4078 - loglik: -2.3790e+02 - logprior: 9.4935
Fitted a model with MAP estimate = -228.2563
Time for alignment: 47.9554
Computed alignments with likelihoods: ['-229.5597', '-228.5819', '-227.5793', '-224.7940', '-228.2563']
Best model has likelihood: -224.7940  (prior= 9.8232 )
time for generating output: 0.1664
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7568345323741007
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.1447 - loglik: -9.2711e+01 - logprior: -4.4336e+00
Epoch 2/10
17/17 - 1s - loss: 75.7002 - loglik: -7.4133e+01 - logprior: -1.5671e+00
Epoch 3/10
17/17 - 1s - loss: 65.7062 - loglik: -6.4106e+01 - logprior: -1.6007e+00
Epoch 4/10
17/17 - 1s - loss: 63.1160 - loglik: -6.1540e+01 - logprior: -1.5758e+00
Epoch 5/10
17/17 - 1s - loss: 62.3392 - loglik: -6.0827e+01 - logprior: -1.5121e+00
Epoch 6/10
17/17 - 1s - loss: 62.1494 - loglik: -6.0669e+01 - logprior: -1.4801e+00
Epoch 7/10
17/17 - 1s - loss: 61.7974 - loglik: -6.0320e+01 - logprior: -1.4773e+00
Epoch 8/10
17/17 - 1s - loss: 61.8309 - loglik: -6.0373e+01 - logprior: -1.4576e+00
Fitted a model with MAP estimate = -61.7286
expansions: [(1, 1), (2, 2), (10, 2), (11, 2), (13, 2), (16, 1), (18, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 35 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 65.6712 - loglik: -6.1205e+01 - logprior: -4.4666e+00
Epoch 2/2
17/17 - 1s - loss: 58.9291 - loglik: -5.7189e+01 - logprior: -1.7398e+00
Fitted a model with MAP estimate = -58.3596
expansions: []
discards: [ 3 14 17 20]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 61.9981 - loglik: -5.7724e+01 - logprior: -4.2743e+00
Epoch 2/2
17/17 - 1s - loss: 58.3616 - loglik: -5.6772e+01 - logprior: -1.5892e+00
Fitted a model with MAP estimate = -58.0528
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4184 - loglik: -5.7157e+01 - logprior: -4.2617e+00
Epoch 2/10
17/17 - 1s - loss: 58.4584 - loglik: -5.6889e+01 - logprior: -1.5690e+00
Epoch 3/10
17/17 - 1s - loss: 57.7941 - loglik: -5.6456e+01 - logprior: -1.3377e+00
Epoch 4/10
17/17 - 1s - loss: 57.8321 - loglik: -5.6552e+01 - logprior: -1.2800e+00
Fitted a model with MAP estimate = -57.7238
Time for alignment: 26.1734
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 97.1169 - loglik: -9.2687e+01 - logprior: -4.4301e+00
Epoch 2/10
17/17 - 1s - loss: 75.7672 - loglik: -7.4222e+01 - logprior: -1.5457e+00
Epoch 3/10
17/17 - 1s - loss: 66.2925 - loglik: -6.4698e+01 - logprior: -1.5942e+00
Epoch 4/10
17/17 - 1s - loss: 64.0081 - loglik: -6.2354e+01 - logprior: -1.6546e+00
Epoch 5/10
17/17 - 1s - loss: 63.3153 - loglik: -6.1781e+01 - logprior: -1.5340e+00
Epoch 6/10
17/17 - 1s - loss: 63.0064 - loglik: -6.1455e+01 - logprior: -1.5511e+00
Epoch 7/10
17/17 - 1s - loss: 62.9802 - loglik: -6.1448e+01 - logprior: -1.5323e+00
Epoch 8/10
17/17 - 1s - loss: 63.0247 - loglik: -6.1512e+01 - logprior: -1.5126e+00
Fitted a model with MAP estimate = -62.8608
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.0777 - loglik: -6.4570e+01 - logprior: -5.5076e+00
Epoch 2/2
17/17 - 1s - loss: 61.5912 - loglik: -5.9120e+01 - logprior: -2.4716e+00
Fitted a model with MAP estimate = -59.3131
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.1095 - loglik: -5.7735e+01 - logprior: -4.3749e+00
Epoch 2/2
17/17 - 1s - loss: 58.5155 - loglik: -5.6911e+01 - logprior: -1.6049e+00
Fitted a model with MAP estimate = -58.0507
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.5319 - loglik: -5.7265e+01 - logprior: -4.2666e+00
Epoch 2/10
17/17 - 1s - loss: 58.2535 - loglik: -5.6675e+01 - logprior: -1.5780e+00
Epoch 3/10
17/17 - 1s - loss: 57.9540 - loglik: -5.6609e+01 - logprior: -1.3455e+00
Epoch 4/10
17/17 - 1s - loss: 57.8943 - loglik: -5.6627e+01 - logprior: -1.2676e+00
Epoch 5/10
17/17 - 1s - loss: 57.6890 - loglik: -5.6442e+01 - logprior: -1.2468e+00
Epoch 6/10
17/17 - 1s - loss: 57.5987 - loglik: -5.6373e+01 - logprior: -1.2253e+00
Epoch 7/10
17/17 - 1s - loss: 57.6750 - loglik: -5.6482e+01 - logprior: -1.1927e+00
Fitted a model with MAP estimate = -57.5861
Time for alignment: 33.3050
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 97.1563 - loglik: -9.2726e+01 - logprior: -4.4300e+00
Epoch 2/10
17/17 - 1s - loss: 75.4075 - loglik: -7.3844e+01 - logprior: -1.5634e+00
Epoch 3/10
17/17 - 1s - loss: 65.8911 - loglik: -6.4279e+01 - logprior: -1.6121e+00
Epoch 4/10
17/17 - 1s - loss: 63.4255 - loglik: -6.1783e+01 - logprior: -1.6425e+00
Epoch 5/10
17/17 - 1s - loss: 62.9592 - loglik: -6.1425e+01 - logprior: -1.5340e+00
Epoch 6/10
17/17 - 1s - loss: 62.7224 - loglik: -6.1169e+01 - logprior: -1.5536e+00
Epoch 7/10
17/17 - 1s - loss: 62.7114 - loglik: -6.1180e+01 - logprior: -1.5318e+00
Epoch 8/10
17/17 - 1s - loss: 62.4679 - loglik: -6.0958e+01 - logprior: -1.5100e+00
Epoch 9/10
17/17 - 1s - loss: 62.5107 - loglik: -6.1009e+01 - logprior: -1.5018e+00
Fitted a model with MAP estimate = -62.4618
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.7452 - loglik: -6.4171e+01 - logprior: -5.5738e+00
Epoch 2/2
17/17 - 1s - loss: 61.3873 - loglik: -5.8658e+01 - logprior: -2.7290e+00
Fitted a model with MAP estimate = -59.4438
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 63.9132 - loglik: -5.8987e+01 - logprior: -4.9261e+00
Epoch 2/2
17/17 - 1s - loss: 58.6376 - loglik: -5.6966e+01 - logprior: -1.6713e+00
Fitted a model with MAP estimate = -58.1025
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4405 - loglik: -5.7164e+01 - logprior: -4.2762e+00
Epoch 2/10
17/17 - 1s - loss: 58.4140 - loglik: -5.6847e+01 - logprior: -1.5666e+00
Epoch 3/10
17/17 - 1s - loss: 57.7507 - loglik: -5.6406e+01 - logprior: -1.3442e+00
Epoch 4/10
17/17 - 1s - loss: 57.8999 - loglik: -5.6625e+01 - logprior: -1.2750e+00
Fitted a model with MAP estimate = -57.7169
Time for alignment: 30.6750
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 97.0301 - loglik: -9.2600e+01 - logprior: -4.4301e+00
Epoch 2/10
17/17 - 1s - loss: 75.3228 - loglik: -7.3765e+01 - logprior: -1.5576e+00
Epoch 3/10
17/17 - 1s - loss: 66.5408 - loglik: -6.4910e+01 - logprior: -1.6303e+00
Epoch 4/10
17/17 - 1s - loss: 63.3987 - loglik: -6.1756e+01 - logprior: -1.6426e+00
Epoch 5/10
17/17 - 1s - loss: 62.9556 - loglik: -6.1411e+01 - logprior: -1.5449e+00
Epoch 6/10
17/17 - 1s - loss: 62.6081 - loglik: -6.1043e+01 - logprior: -1.5646e+00
Epoch 7/10
17/17 - 1s - loss: 62.3962 - loglik: -6.0859e+01 - logprior: -1.5377e+00
Epoch 8/10
17/17 - 1s - loss: 62.4060 - loglik: -6.0890e+01 - logprior: -1.5160e+00
Fitted a model with MAP estimate = -62.3578
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2119 - loglik: -6.4625e+01 - logprior: -5.5868e+00
Epoch 2/2
17/17 - 1s - loss: 61.4089 - loglik: -5.8667e+01 - logprior: -2.7416e+00
Fitted a model with MAP estimate = -59.4669
expansions: [(2, 1)]
discards: [ 0 13 16 26]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 63.9143 - loglik: -5.9005e+01 - logprior: -4.9090e+00
Epoch 2/2
17/17 - 1s - loss: 58.5543 - loglik: -5.6883e+01 - logprior: -1.6710e+00
Fitted a model with MAP estimate = -58.1083
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4780 - loglik: -5.7199e+01 - logprior: -4.2786e+00
Epoch 2/10
17/17 - 1s - loss: 58.2743 - loglik: -5.6701e+01 - logprior: -1.5731e+00
Epoch 3/10
17/17 - 1s - loss: 58.0211 - loglik: -5.6680e+01 - logprior: -1.3409e+00
Epoch 4/10
17/17 - 1s - loss: 57.8145 - loglik: -5.6541e+01 - logprior: -1.2737e+00
Epoch 5/10
17/17 - 1s - loss: 57.7349 - loglik: -5.6491e+01 - logprior: -1.2439e+00
Epoch 6/10
17/17 - 1s - loss: 57.6624 - loglik: -5.6445e+01 - logprior: -1.2176e+00
Epoch 7/10
17/17 - 1s - loss: 57.6617 - loglik: -5.6469e+01 - logprior: -1.1929e+00
Epoch 8/10
17/17 - 1s - loss: 57.5193 - loglik: -5.6334e+01 - logprior: -1.1854e+00
Epoch 9/10
17/17 - 1s - loss: 57.6062 - loglik: -5.6440e+01 - logprior: -1.1663e+00
Fitted a model with MAP estimate = -57.5439
Time for alignment: 33.6653
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.0114 - loglik: -9.2581e+01 - logprior: -4.4304e+00
Epoch 2/10
17/17 - 1s - loss: 74.9674 - loglik: -7.3408e+01 - logprior: -1.5598e+00
Epoch 3/10
17/17 - 1s - loss: 65.1642 - loglik: -6.3547e+01 - logprior: -1.6171e+00
Epoch 4/10
17/17 - 1s - loss: 63.3674 - loglik: -6.1729e+01 - logprior: -1.6386e+00
Epoch 5/10
17/17 - 1s - loss: 62.8761 - loglik: -6.1347e+01 - logprior: -1.5294e+00
Epoch 6/10
17/17 - 1s - loss: 62.7995 - loglik: -6.1251e+01 - logprior: -1.5489e+00
Epoch 7/10
17/17 - 1s - loss: 62.5321 - loglik: -6.1007e+01 - logprior: -1.5250e+00
Epoch 8/10
17/17 - 1s - loss: 62.5238 - loglik: -6.1017e+01 - logprior: -1.5071e+00
Epoch 9/10
17/17 - 1s - loss: 62.5207 - loglik: -6.1024e+01 - logprior: -1.4970e+00
Epoch 10/10
17/17 - 1s - loss: 62.4180 - loglik: -6.0930e+01 - logprior: -1.4877e+00
Fitted a model with MAP estimate = -62.4332
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 69.7430 - loglik: -6.4166e+01 - logprior: -5.5771e+00
Epoch 2/2
17/17 - 1s - loss: 61.2049 - loglik: -5.8448e+01 - logprior: -2.7569e+00
Fitted a model with MAP estimate = -59.4710
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 63.8143 - loglik: -5.8893e+01 - logprior: -4.9208e+00
Epoch 2/2
17/17 - 1s - loss: 58.5764 - loglik: -5.6907e+01 - logprior: -1.6691e+00
Fitted a model with MAP estimate = -58.1146
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4625 - loglik: -5.7188e+01 - logprior: -4.2744e+00
Epoch 2/10
17/17 - 1s - loss: 58.2491 - loglik: -5.6681e+01 - logprior: -1.5684e+00
Epoch 3/10
17/17 - 1s - loss: 58.0203 - loglik: -5.6675e+01 - logprior: -1.3452e+00
Epoch 4/10
17/17 - 1s - loss: 57.6652 - loglik: -5.6394e+01 - logprior: -1.2713e+00
Epoch 5/10
17/17 - 1s - loss: 57.8470 - loglik: -5.6601e+01 - logprior: -1.2456e+00
Fitted a model with MAP estimate = -57.6578
Time for alignment: 31.1837
Computed alignments with likelihoods: ['-57.7238', '-57.5861', '-57.7169', '-57.5439', '-57.6578']
Best model has likelihood: -57.5439  (prior= -1.1375 )
time for generating output: 0.0952
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7672713238131995
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.6470 - loglik: -1.8555e+02 - logprior: -8.0979e+00
Epoch 2/10
13/13 - 1s - loss: 163.4932 - loglik: -1.6125e+02 - logprior: -2.2464e+00
Epoch 3/10
13/13 - 1s - loss: 146.4994 - loglik: -1.4457e+02 - logprior: -1.9268e+00
Epoch 4/10
13/13 - 1s - loss: 139.6205 - loglik: -1.3759e+02 - logprior: -2.0272e+00
Epoch 5/10
13/13 - 1s - loss: 136.7762 - loglik: -1.3488e+02 - logprior: -1.8999e+00
Epoch 6/10
13/13 - 1s - loss: 135.7307 - loglik: -1.3390e+02 - logprior: -1.8334e+00
Epoch 7/10
13/13 - 1s - loss: 134.1793 - loglik: -1.3227e+02 - logprior: -1.9066e+00
Epoch 8/10
13/13 - 1s - loss: 133.9363 - loglik: -1.3203e+02 - logprior: -1.9105e+00
Epoch 9/10
13/13 - 1s - loss: 133.2444 - loglik: -1.3134e+02 - logprior: -1.9060e+00
Epoch 10/10
13/13 - 1s - loss: 133.5507 - loglik: -1.3165e+02 - logprior: -1.8975e+00
Fitted a model with MAP estimate = -133.2712
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 142.9947 - loglik: -1.3341e+02 - logprior: -9.5818e+00
Epoch 2/2
13/13 - 1s - loss: 129.4275 - loglik: -1.2500e+02 - logprior: -4.4309e+00
Fitted a model with MAP estimate = -127.3748
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.7494 - loglik: -1.2345e+02 - logprior: -7.3031e+00
Epoch 2/2
13/13 - 1s - loss: 124.1129 - loglik: -1.2194e+02 - logprior: -2.1740e+00
Fitted a model with MAP estimate = -123.1892
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.7764 - loglik: -1.2443e+02 - logprior: -9.3460e+00
Epoch 2/10
13/13 - 1s - loss: 125.4049 - loglik: -1.2198e+02 - logprior: -3.4253e+00
Epoch 3/10
13/13 - 1s - loss: 122.0326 - loglik: -1.2037e+02 - logprior: -1.6642e+00
Epoch 4/10
13/13 - 1s - loss: 121.2372 - loglik: -1.2001e+02 - logprior: -1.2320e+00
Epoch 5/10
13/13 - 1s - loss: 120.6710 - loglik: -1.1953e+02 - logprior: -1.1395e+00
Epoch 6/10
13/13 - 1s - loss: 120.0146 - loglik: -1.1886e+02 - logprior: -1.1584e+00
Epoch 7/10
13/13 - 1s - loss: 119.2910 - loglik: -1.1814e+02 - logprior: -1.1529e+00
Epoch 8/10
13/13 - 1s - loss: 119.4389 - loglik: -1.1830e+02 - logprior: -1.1390e+00
Fitted a model with MAP estimate = -119.2446
Time for alignment: 36.6796
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.6617 - loglik: -1.8556e+02 - logprior: -8.0992e+00
Epoch 2/10
13/13 - 1s - loss: 163.9991 - loglik: -1.6174e+02 - logprior: -2.2550e+00
Epoch 3/10
13/13 - 1s - loss: 147.9107 - loglik: -1.4599e+02 - logprior: -1.9203e+00
Epoch 4/10
13/13 - 1s - loss: 141.1318 - loglik: -1.3913e+02 - logprior: -2.0021e+00
Epoch 5/10
13/13 - 1s - loss: 136.7955 - loglik: -1.3488e+02 - logprior: -1.9122e+00
Epoch 6/10
13/13 - 1s - loss: 134.9011 - loglik: -1.3307e+02 - logprior: -1.8309e+00
Epoch 7/10
13/13 - 1s - loss: 134.2799 - loglik: -1.3242e+02 - logprior: -1.8603e+00
Epoch 8/10
13/13 - 1s - loss: 134.0902 - loglik: -1.3225e+02 - logprior: -1.8412e+00
Epoch 9/10
13/13 - 1s - loss: 134.0735 - loglik: -1.3226e+02 - logprior: -1.8174e+00
Epoch 10/10
13/13 - 1s - loss: 133.9010 - loglik: -1.3209e+02 - logprior: -1.8082e+00
Fitted a model with MAP estimate = -133.7940
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (32, 1), (38, 1), (41, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 142.5908 - loglik: -1.3298e+02 - logprior: -9.6073e+00
Epoch 2/2
13/13 - 1s - loss: 129.7222 - loglik: -1.2529e+02 - logprior: -4.4359e+00
Fitted a model with MAP estimate = -127.4463
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6052 - loglik: -1.2330e+02 - logprior: -7.3005e+00
Epoch 2/2
13/13 - 1s - loss: 124.5026 - loglik: -1.2233e+02 - logprior: -2.1728e+00
Fitted a model with MAP estimate = -123.2225
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.6745 - loglik: -1.2435e+02 - logprior: -9.3284e+00
Epoch 2/10
13/13 - 1s - loss: 125.4797 - loglik: -1.2210e+02 - logprior: -3.3829e+00
Epoch 3/10
13/13 - 1s - loss: 122.4201 - loglik: -1.2076e+02 - logprior: -1.6651e+00
Epoch 4/10
13/13 - 1s - loss: 121.1014 - loglik: -1.1987e+02 - logprior: -1.2280e+00
Epoch 5/10
13/13 - 1s - loss: 120.3769 - loglik: -1.1923e+02 - logprior: -1.1447e+00
Epoch 6/10
13/13 - 1s - loss: 119.8549 - loglik: -1.1869e+02 - logprior: -1.1601e+00
Epoch 7/10
13/13 - 1s - loss: 119.6396 - loglik: -1.1848e+02 - logprior: -1.1583e+00
Epoch 8/10
13/13 - 1s - loss: 119.0496 - loglik: -1.1791e+02 - logprior: -1.1435e+00
Epoch 9/10
13/13 - 1s - loss: 119.4660 - loglik: -1.1834e+02 - logprior: -1.1228e+00
Fitted a model with MAP estimate = -119.1899
Time for alignment: 36.7124
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7426 - loglik: -1.8564e+02 - logprior: -8.0985e+00
Epoch 2/10
13/13 - 1s - loss: 164.9151 - loglik: -1.6265e+02 - logprior: -2.2629e+00
Epoch 3/10
13/13 - 1s - loss: 147.7110 - loglik: -1.4576e+02 - logprior: -1.9467e+00
Epoch 4/10
13/13 - 1s - loss: 139.5972 - loglik: -1.3753e+02 - logprior: -2.0692e+00
Epoch 5/10
13/13 - 1s - loss: 136.1607 - loglik: -1.3416e+02 - logprior: -1.9960e+00
Epoch 6/10
13/13 - 1s - loss: 134.4689 - loglik: -1.3256e+02 - logprior: -1.9091e+00
Epoch 7/10
13/13 - 1s - loss: 133.8730 - loglik: -1.3195e+02 - logprior: -1.9263e+00
Epoch 8/10
13/13 - 1s - loss: 133.5274 - loglik: -1.3161e+02 - logprior: -1.9129e+00
Epoch 9/10
13/13 - 1s - loss: 133.5697 - loglik: -1.3168e+02 - logprior: -1.8869e+00
Fitted a model with MAP estimate = -133.3899
expansions: [(12, 1), (17, 3), (20, 4), (21, 1), (32, 1), (35, 1), (41, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 143.0225 - loglik: -1.3342e+02 - logprior: -9.6007e+00
Epoch 2/2
13/13 - 1s - loss: 129.1803 - loglik: -1.2475e+02 - logprior: -4.4319e+00
Fitted a model with MAP estimate = -127.4706
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6107 - loglik: -1.2331e+02 - logprior: -7.3000e+00
Epoch 2/2
13/13 - 1s - loss: 124.3513 - loglik: -1.2217e+02 - logprior: -2.1770e+00
Fitted a model with MAP estimate = -123.2090
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.7689 - loglik: -1.2443e+02 - logprior: -9.3351e+00
Epoch 2/10
13/13 - 1s - loss: 125.2661 - loglik: -1.2186e+02 - logprior: -3.4061e+00
Epoch 3/10
13/13 - 1s - loss: 122.2726 - loglik: -1.2061e+02 - logprior: -1.6670e+00
Epoch 4/10
13/13 - 1s - loss: 121.1464 - loglik: -1.1991e+02 - logprior: -1.2328e+00
Epoch 5/10
13/13 - 1s - loss: 120.3850 - loglik: -1.1923e+02 - logprior: -1.1526e+00
Epoch 6/10
13/13 - 1s - loss: 120.0013 - loglik: -1.1884e+02 - logprior: -1.1611e+00
Epoch 7/10
13/13 - 1s - loss: 119.7341 - loglik: -1.1857e+02 - logprior: -1.1599e+00
Epoch 8/10
13/13 - 1s - loss: 119.2280 - loglik: -1.1808e+02 - logprior: -1.1442e+00
Epoch 9/10
13/13 - 1s - loss: 119.4074 - loglik: -1.1829e+02 - logprior: -1.1182e+00
Fitted a model with MAP estimate = -119.1947
Time for alignment: 36.7234
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.5259 - loglik: -1.8543e+02 - logprior: -8.0992e+00
Epoch 2/10
13/13 - 1s - loss: 163.8920 - loglik: -1.6164e+02 - logprior: -2.2554e+00
Epoch 3/10
13/13 - 1s - loss: 146.5052 - loglik: -1.4462e+02 - logprior: -1.8892e+00
Epoch 4/10
13/13 - 1s - loss: 139.4212 - loglik: -1.3743e+02 - logprior: -1.9905e+00
Epoch 5/10
13/13 - 1s - loss: 136.1529 - loglik: -1.3425e+02 - logprior: -1.9039e+00
Epoch 6/10
13/13 - 1s - loss: 135.3286 - loglik: -1.3353e+02 - logprior: -1.8002e+00
Epoch 7/10
13/13 - 1s - loss: 134.5184 - loglik: -1.3270e+02 - logprior: -1.8185e+00
Epoch 8/10
13/13 - 1s - loss: 134.4285 - loglik: -1.3260e+02 - logprior: -1.8237e+00
Epoch 9/10
13/13 - 1s - loss: 134.4563 - loglik: -1.3266e+02 - logprior: -1.7943e+00
Fitted a model with MAP estimate = -134.2500
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 142.7315 - loglik: -1.3316e+02 - logprior: -9.5665e+00
Epoch 2/2
13/13 - 1s - loss: 129.2076 - loglik: -1.2480e+02 - logprior: -4.4113e+00
Fitted a model with MAP estimate = -127.3237
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.8571 - loglik: -1.2355e+02 - logprior: -7.3072e+00
Epoch 2/2
13/13 - 1s - loss: 123.9041 - loglik: -1.2173e+02 - logprior: -2.1716e+00
Fitted a model with MAP estimate = -123.2059
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 133.9320 - loglik: -1.2458e+02 - logprior: -9.3549e+00
Epoch 2/10
13/13 - 1s - loss: 125.3284 - loglik: -1.2188e+02 - logprior: -3.4451e+00
Epoch 3/10
13/13 - 1s - loss: 122.3840 - loglik: -1.2070e+02 - logprior: -1.6790e+00
Epoch 4/10
13/13 - 1s - loss: 120.8902 - loglik: -1.1966e+02 - logprior: -1.2300e+00
Epoch 5/10
13/13 - 1s - loss: 120.7073 - loglik: -1.1956e+02 - logprior: -1.1477e+00
Epoch 6/10
13/13 - 1s - loss: 119.7262 - loglik: -1.1856e+02 - logprior: -1.1657e+00
Epoch 7/10
13/13 - 1s - loss: 119.7924 - loglik: -1.1863e+02 - logprior: -1.1634e+00
Fitted a model with MAP estimate = -119.4279
Time for alignment: 34.0883
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.8340 - loglik: -1.8574e+02 - logprior: -8.0971e+00
Epoch 2/10
13/13 - 1s - loss: 163.8365 - loglik: -1.6158e+02 - logprior: -2.2521e+00
Epoch 3/10
13/13 - 1s - loss: 146.4634 - loglik: -1.4454e+02 - logprior: -1.9242e+00
Epoch 4/10
13/13 - 1s - loss: 139.6414 - loglik: -1.3762e+02 - logprior: -2.0189e+00
Epoch 5/10
13/13 - 1s - loss: 136.6749 - loglik: -1.3477e+02 - logprior: -1.9072e+00
Epoch 6/10
13/13 - 1s - loss: 135.3375 - loglik: -1.3352e+02 - logprior: -1.8174e+00
Epoch 7/10
13/13 - 1s - loss: 134.4814 - loglik: -1.3264e+02 - logprior: -1.8369e+00
Epoch 8/10
13/13 - 1s - loss: 134.3739 - loglik: -1.3254e+02 - logprior: -1.8315e+00
Epoch 9/10
13/13 - 1s - loss: 134.3938 - loglik: -1.3258e+02 - logprior: -1.8096e+00
Fitted a model with MAP estimate = -134.2304
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (36, 1), (41, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 143.2405 - loglik: -1.3365e+02 - logprior: -9.5896e+00
Epoch 2/2
13/13 - 1s - loss: 128.7773 - loglik: -1.2436e+02 - logprior: -4.4180e+00
Fitted a model with MAP estimate = -127.3716
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 130.5487 - loglik: -1.2325e+02 - logprior: -7.3000e+00
Epoch 2/2
13/13 - 1s - loss: 124.3844 - loglik: -1.2221e+02 - logprior: -2.1767e+00
Fitted a model with MAP estimate = -123.2130
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.0231 - loglik: -1.2467e+02 - logprior: -9.3482e+00
Epoch 2/10
13/13 - 1s - loss: 125.2573 - loglik: -1.2182e+02 - logprior: -3.4392e+00
Epoch 3/10
13/13 - 1s - loss: 122.3009 - loglik: -1.2063e+02 - logprior: -1.6726e+00
Epoch 4/10
13/13 - 1s - loss: 121.1307 - loglik: -1.1990e+02 - logprior: -1.2330e+00
Epoch 5/10
13/13 - 1s - loss: 120.4449 - loglik: -1.1930e+02 - logprior: -1.1455e+00
Epoch 6/10
13/13 - 1s - loss: 120.0660 - loglik: -1.1890e+02 - logprior: -1.1685e+00
Epoch 7/10
13/13 - 1s - loss: 119.6142 - loglik: -1.1846e+02 - logprior: -1.1555e+00
Epoch 8/10
13/13 - 1s - loss: 119.4071 - loglik: -1.1826e+02 - logprior: -1.1439e+00
Epoch 9/10
13/13 - 1s - loss: 119.4115 - loglik: -1.1829e+02 - logprior: -1.1258e+00
Fitted a model with MAP estimate = -119.2000
Time for alignment: 35.5623
Computed alignments with likelihoods: ['-119.2446', '-119.1899', '-119.1947', '-119.4279', '-119.2000']
Best model has likelihood: -119.1899  (prior= -1.0992 )
time for generating output: 0.1269
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.8824362606232294
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.9581 - loglik: -2.2842e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 207.7257 - loglik: -2.0444e+02 - logprior: -3.2847e+00
Epoch 3/10
11/11 - 1s - loss: 181.4697 - loglik: -1.7939e+02 - logprior: -2.0810e+00
Epoch 4/10
11/11 - 1s - loss: 165.1808 - loglik: -1.6358e+02 - logprior: -1.6038e+00
Epoch 5/10
11/11 - 1s - loss: 160.5281 - loglik: -1.5913e+02 - logprior: -1.3938e+00
Epoch 6/10
11/11 - 1s - loss: 158.1836 - loglik: -1.5697e+02 - logprior: -1.2146e+00
Epoch 7/10
11/11 - 1s - loss: 156.7755 - loglik: -1.5577e+02 - logprior: -1.0038e+00
Epoch 8/10
11/11 - 1s - loss: 157.1243 - loglik: -1.5627e+02 - logprior: -8.5823e-01
Fitted a model with MAP estimate = -156.5272
expansions: [(0, 6), (22, 1), (23, 2), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 171.1952 - loglik: -1.5563e+02 - logprior: -1.5563e+01
Epoch 2/2
11/11 - 1s - loss: 150.7480 - loglik: -1.4640e+02 - logprior: -4.3469e+00
Fitted a model with MAP estimate = -147.4634
expansions: []
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.1360 - loglik: -1.4804e+02 - logprior: -1.4097e+01
Epoch 2/2
11/11 - 1s - loss: 151.1704 - loglik: -1.4561e+02 - logprior: -5.5624e+00
Fitted a model with MAP estimate = -148.7061
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.0932 - loglik: -1.4581e+02 - logprior: -1.2282e+01
Epoch 2/10
11/11 - 1s - loss: 147.8208 - loglik: -1.4457e+02 - logprior: -3.2512e+00
Epoch 3/10
11/11 - 1s - loss: 145.3599 - loglik: -1.4365e+02 - logprior: -1.7066e+00
Epoch 4/10
11/11 - 1s - loss: 144.6527 - loglik: -1.4363e+02 - logprior: -1.0241e+00
Epoch 5/10
11/11 - 1s - loss: 143.8969 - loglik: -1.4307e+02 - logprior: -8.2716e-01
Epoch 6/10
11/11 - 1s - loss: 143.3030 - loglik: -1.4265e+02 - logprior: -6.5663e-01
Epoch 7/10
11/11 - 1s - loss: 142.9283 - loglik: -1.4236e+02 - logprior: -5.7159e-01
Epoch 8/10
11/11 - 1s - loss: 143.1012 - loglik: -1.4256e+02 - logprior: -5.3788e-01
Fitted a model with MAP estimate = -142.9353
Time for alignment: 34.0459
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0143 - loglik: -2.2847e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 207.4757 - loglik: -2.0420e+02 - logprior: -3.2770e+00
Epoch 3/10
11/11 - 1s - loss: 181.8931 - loglik: -1.7985e+02 - logprior: -2.0475e+00
Epoch 4/10
11/11 - 1s - loss: 166.2814 - loglik: -1.6476e+02 - logprior: -1.5251e+00
Epoch 5/10
11/11 - 1s - loss: 160.9092 - loglik: -1.5965e+02 - logprior: -1.2607e+00
Epoch 6/10
11/11 - 1s - loss: 158.4642 - loglik: -1.5739e+02 - logprior: -1.0696e+00
Epoch 7/10
11/11 - 1s - loss: 157.6188 - loglik: -1.5671e+02 - logprior: -9.1097e-01
Epoch 8/10
11/11 - 1s - loss: 157.2444 - loglik: -1.5645e+02 - logprior: -7.9480e-01
Epoch 9/10
11/11 - 1s - loss: 156.8233 - loglik: -1.5608e+02 - logprior: -7.3867e-01
Epoch 10/10
11/11 - 1s - loss: 156.7088 - loglik: -1.5603e+02 - logprior: -6.7915e-01
Fitted a model with MAP estimate = -156.4463
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.4353 - loglik: -1.5575e+02 - logprior: -1.5687e+01
Epoch 2/2
11/11 - 1s - loss: 150.5463 - loglik: -1.4610e+02 - logprior: -4.4436e+00
Fitted a model with MAP estimate = -147.6636
expansions: []
discards: [ 0 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.1728 - loglik: -1.4807e+02 - logprior: -1.4103e+01
Epoch 2/2
11/11 - 1s - loss: 151.1404 - loglik: -1.4558e+02 - logprior: -5.5618e+00
Fitted a model with MAP estimate = -148.7517
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 157.9754 - loglik: -1.4570e+02 - logprior: -1.2278e+01
Epoch 2/10
11/11 - 1s - loss: 147.6819 - loglik: -1.4443e+02 - logprior: -3.2514e+00
Epoch 3/10
11/11 - 1s - loss: 145.5977 - loglik: -1.4389e+02 - logprior: -1.7053e+00
Epoch 4/10
11/11 - 1s - loss: 144.4502 - loglik: -1.4343e+02 - logprior: -1.0181e+00
Epoch 5/10
11/11 - 1s - loss: 144.0529 - loglik: -1.4323e+02 - logprior: -8.2445e-01
Epoch 6/10
11/11 - 1s - loss: 143.3808 - loglik: -1.4273e+02 - logprior: -6.5224e-01
Epoch 7/10
11/11 - 1s - loss: 143.1799 - loglik: -1.4261e+02 - logprior: -5.7332e-01
Epoch 8/10
11/11 - 1s - loss: 142.7264 - loglik: -1.4220e+02 - logprior: -5.2835e-01
Epoch 9/10
11/11 - 1s - loss: 143.3043 - loglik: -1.4281e+02 - logprior: -4.9138e-01
Fitted a model with MAP estimate = -142.8460
Time for alignment: 35.9747
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.7621 - loglik: -2.2822e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 206.8506 - loglik: -2.0356e+02 - logprior: -3.2865e+00
Epoch 3/10
11/11 - 1s - loss: 179.1450 - loglik: -1.7707e+02 - logprior: -2.0798e+00
Epoch 4/10
11/11 - 1s - loss: 164.6513 - loglik: -1.6307e+02 - logprior: -1.5841e+00
Epoch 5/10
11/11 - 1s - loss: 160.2042 - loglik: -1.5884e+02 - logprior: -1.3617e+00
Epoch 6/10
11/11 - 1s - loss: 158.3050 - loglik: -1.5713e+02 - logprior: -1.1734e+00
Epoch 7/10
11/11 - 1s - loss: 157.5589 - loglik: -1.5657e+02 - logprior: -9.8416e-01
Epoch 8/10
11/11 - 1s - loss: 156.8078 - loglik: -1.5596e+02 - logprior: -8.4443e-01
Epoch 9/10
11/11 - 1s - loss: 156.6003 - loglik: -1.5583e+02 - logprior: -7.7235e-01
Epoch 10/10
11/11 - 1s - loss: 156.6644 - loglik: -1.5595e+02 - logprior: -7.1194e-01
Fitted a model with MAP estimate = -156.3289
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.4068 - loglik: -1.5573e+02 - logprior: -1.5678e+01
Epoch 2/2
11/11 - 1s - loss: 150.8568 - loglik: -1.4641e+02 - logprior: -4.4465e+00
Fitted a model with MAP estimate = -147.6642
expansions: []
discards: [ 0 37 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.3862 - loglik: -1.4827e+02 - logprior: -1.4112e+01
Epoch 2/2
11/11 - 1s - loss: 151.5916 - loglik: -1.4603e+02 - logprior: -5.5619e+00
Fitted a model with MAP estimate = -148.9875
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 158.2985 - loglik: -1.4601e+02 - logprior: -1.2293e+01
Epoch 2/10
11/11 - 1s - loss: 147.8778 - loglik: -1.4462e+02 - logprior: -3.2568e+00
Epoch 3/10
11/11 - 1s - loss: 145.5937 - loglik: -1.4390e+02 - logprior: -1.6981e+00
Epoch 4/10
11/11 - 1s - loss: 144.1615 - loglik: -1.4314e+02 - logprior: -1.0208e+00
Epoch 5/10
11/11 - 1s - loss: 144.0252 - loglik: -1.4321e+02 - logprior: -8.1270e-01
Epoch 6/10
11/11 - 1s - loss: 143.5856 - loglik: -1.4293e+02 - logprior: -6.5629e-01
Epoch 7/10
11/11 - 1s - loss: 143.1868 - loglik: -1.4261e+02 - logprior: -5.7345e-01
Epoch 8/10
11/11 - 1s - loss: 142.9991 - loglik: -1.4247e+02 - logprior: -5.2628e-01
Epoch 9/10
11/11 - 1s - loss: 142.9092 - loglik: -1.4242e+02 - logprior: -4.9098e-01
Epoch 10/10
11/11 - 1s - loss: 142.7931 - loglik: -1.4234e+02 - logprior: -4.5355e-01
Fitted a model with MAP estimate = -142.7858
Time for alignment: 35.7921
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.7099 - loglik: -2.2817e+02 - logprior: -1.2542e+01
Epoch 2/10
11/11 - 1s - loss: 207.5047 - loglik: -2.0422e+02 - logprior: -3.2815e+00
Epoch 3/10
11/11 - 1s - loss: 180.8232 - loglik: -1.7874e+02 - logprior: -2.0784e+00
Epoch 4/10
11/11 - 1s - loss: 165.8178 - loglik: -1.6422e+02 - logprior: -1.5982e+00
Epoch 5/10
11/11 - 1s - loss: 160.6611 - loglik: -1.5926e+02 - logprior: -1.3965e+00
Epoch 6/10
11/11 - 1s - loss: 158.5230 - loglik: -1.5730e+02 - logprior: -1.2261e+00
Epoch 7/10
11/11 - 1s - loss: 157.5277 - loglik: -1.5651e+02 - logprior: -1.0162e+00
Epoch 8/10
11/11 - 1s - loss: 157.0920 - loglik: -1.5623e+02 - logprior: -8.6440e-01
Epoch 9/10
11/11 - 1s - loss: 156.9133 - loglik: -1.5613e+02 - logprior: -7.8291e-01
Epoch 10/10
11/11 - 1s - loss: 156.4943 - loglik: -1.5577e+02 - logprior: -7.2266e-01
Fitted a model with MAP estimate = -156.3541
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 170.9903 - loglik: -1.5534e+02 - logprior: -1.5654e+01
Epoch 2/2
11/11 - 1s - loss: 150.5156 - loglik: -1.4613e+02 - logprior: -4.3861e+00
Fitted a model with MAP estimate = -147.3867
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.1856 - loglik: -1.4809e+02 - logprior: -1.4100e+01
Epoch 2/2
11/11 - 1s - loss: 151.1269 - loglik: -1.4557e+02 - logprior: -5.5618e+00
Fitted a model with MAP estimate = -148.9645
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 158.2078 - loglik: -1.4592e+02 - logprior: -1.2289e+01
Epoch 2/10
11/11 - 1s - loss: 147.9174 - loglik: -1.4467e+02 - logprior: -3.2519e+00
Epoch 3/10
11/11 - 1s - loss: 145.4804 - loglik: -1.4378e+02 - logprior: -1.7008e+00
Epoch 4/10
11/11 - 1s - loss: 144.3949 - loglik: -1.4338e+02 - logprior: -1.0164e+00
Epoch 5/10
11/11 - 1s - loss: 143.5215 - loglik: -1.4270e+02 - logprior: -8.2028e-01
Epoch 6/10
11/11 - 1s - loss: 143.8383 - loglik: -1.4319e+02 - logprior: -6.4800e-01
Fitted a model with MAP estimate = -143.2187
Time for alignment: 33.1729
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.8079 - loglik: -2.2827e+02 - logprior: -1.2542e+01
Epoch 2/10
11/11 - 1s - loss: 207.8014 - loglik: -2.0452e+02 - logprior: -3.2801e+00
Epoch 3/10
11/11 - 1s - loss: 181.8922 - loglik: -1.7985e+02 - logprior: -2.0413e+00
Epoch 4/10
11/11 - 1s - loss: 167.1956 - loglik: -1.6571e+02 - logprior: -1.4879e+00
Epoch 5/10
11/11 - 1s - loss: 160.6123 - loglik: -1.5933e+02 - logprior: -1.2843e+00
Epoch 6/10
11/11 - 1s - loss: 158.1943 - loglik: -1.5706e+02 - logprior: -1.1298e+00
Epoch 7/10
11/11 - 1s - loss: 157.5485 - loglik: -1.5657e+02 - logprior: -9.7580e-01
Epoch 8/10
11/11 - 1s - loss: 156.8915 - loglik: -1.5605e+02 - logprior: -8.4292e-01
Epoch 9/10
11/11 - 1s - loss: 156.6979 - loglik: -1.5593e+02 - logprior: -7.6965e-01
Epoch 10/10
11/11 - 1s - loss: 156.6083 - loglik: -1.5591e+02 - logprior: -7.0321e-01
Fitted a model with MAP estimate = -156.3240
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 171.8000 - loglik: -1.5613e+02 - logprior: -1.5671e+01
Epoch 2/2
11/11 - 1s - loss: 151.0818 - loglik: -1.4664e+02 - logprior: -4.4436e+00
Fitted a model with MAP estimate = -147.5341
expansions: []
discards: [ 0 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.5434 - loglik: -1.4843e+02 - logprior: -1.4112e+01
Epoch 2/2
11/11 - 1s - loss: 151.4185 - loglik: -1.4586e+02 - logprior: -5.5592e+00
Fitted a model with MAP estimate = -149.0171
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 158.0800 - loglik: -1.4579e+02 - logprior: -1.2293e+01
Epoch 2/10
11/11 - 1s - loss: 147.9757 - loglik: -1.4474e+02 - logprior: -3.2381e+00
Epoch 3/10
11/11 - 1s - loss: 145.5213 - loglik: -1.4381e+02 - logprior: -1.7093e+00
Epoch 4/10
11/11 - 1s - loss: 144.3515 - loglik: -1.4333e+02 - logprior: -1.0170e+00
Epoch 5/10
11/11 - 1s - loss: 144.1012 - loglik: -1.4328e+02 - logprior: -8.2106e-01
Epoch 6/10
11/11 - 1s - loss: 143.3488 - loglik: -1.4270e+02 - logprior: -6.4857e-01
Epoch 7/10
11/11 - 1s - loss: 143.1813 - loglik: -1.4261e+02 - logprior: -5.7569e-01
Epoch 8/10
11/11 - 1s - loss: 142.9049 - loglik: -1.4238e+02 - logprior: -5.2721e-01
Epoch 9/10
11/11 - 1s - loss: 143.0072 - loglik: -1.4252e+02 - logprior: -4.8415e-01
Fitted a model with MAP estimate = -142.8476
Time for alignment: 33.3652
Computed alignments with likelihoods: ['-142.9353', '-142.8460', '-142.7858', '-143.2187', '-142.8476']
Best model has likelihood: -142.7858  (prior= -0.4306 )
time for generating output: 0.1376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 525.0316 - loglik: -5.1973e+02 - logprior: -5.3007e+00
Epoch 2/10
24/24 - 6s - loss: 376.5027 - loglik: -3.7432e+02 - logprior: -2.1830e+00
Epoch 3/10
24/24 - 6s - loss: 348.3219 - loglik: -3.4579e+02 - logprior: -2.5297e+00
Epoch 4/10
24/24 - 6s - loss: 345.5822 - loglik: -3.4318e+02 - logprior: -2.4066e+00
Epoch 5/10
24/24 - 6s - loss: 344.3907 - loglik: -3.4201e+02 - logprior: -2.3813e+00
Epoch 6/10
24/24 - 6s - loss: 342.5653 - loglik: -3.4017e+02 - logprior: -2.3911e+00
Epoch 7/10
24/24 - 6s - loss: 344.0907 - loglik: -3.4168e+02 - logprior: -2.4109e+00
Fitted a model with MAP estimate = -342.8508
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (83, 1), (88, 2), (91, 1), (111, 1), (113, 1), (114, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (153, 1), (154, 4), (155, 1), (158, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.1011 - loglik: -3.2782e+02 - logprior: -7.2833e+00
Epoch 2/2
24/24 - 8s - loss: 317.2885 - loglik: -3.1477e+02 - logprior: -2.5191e+00
Fitted a model with MAP estimate = -313.5061
expansions: [(0, 3), (191, 1), (193, 1)]
discards: [ 0 12 25]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 317.0747 - loglik: -3.1262e+02 - logprior: -4.4526e+00
Epoch 2/2
24/24 - 8s - loss: 308.0351 - loglik: -3.0804e+02 - logprior: 0.0078
Fitted a model with MAP estimate = -307.0216
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 320.3051 - loglik: -3.1392e+02 - logprior: -6.3805e+00
Epoch 2/10
24/24 - 8s - loss: 310.4482 - loglik: -3.0969e+02 - logprior: -7.5689e-01
Epoch 3/10
24/24 - 8s - loss: 308.7298 - loglik: -3.0997e+02 - logprior: 1.2452
Epoch 4/10
24/24 - 8s - loss: 306.6399 - loglik: -3.0793e+02 - logprior: 1.2901
Epoch 5/10
24/24 - 8s - loss: 304.8266 - loglik: -3.0609e+02 - logprior: 1.2647
Epoch 6/10
24/24 - 8s - loss: 305.8740 - loglik: -3.0728e+02 - logprior: 1.4071
Fitted a model with MAP estimate = -304.5933
Time for alignment: 163.4895
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 525.3719 - loglik: -5.2003e+02 - logprior: -5.3434e+00
Epoch 2/10
24/24 - 6s - loss: 379.2338 - loglik: -3.7696e+02 - logprior: -2.2689e+00
Epoch 3/10
24/24 - 6s - loss: 350.6728 - loglik: -3.4805e+02 - logprior: -2.6214e+00
Epoch 4/10
24/24 - 6s - loss: 345.3797 - loglik: -3.4287e+02 - logprior: -2.5104e+00
Epoch 5/10
24/24 - 6s - loss: 344.9737 - loglik: -3.4246e+02 - logprior: -2.5177e+00
Epoch 6/10
24/24 - 6s - loss: 345.2234 - loglik: -3.4269e+02 - logprior: -2.5324e+00
Fitted a model with MAP estimate = -343.6732
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (34, 1), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (59, 1), (63, 1), (65, 1), (83, 1), (87, 1), (88, 1), (91, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.3512 - loglik: -3.2810e+02 - logprior: -7.2473e+00
Epoch 2/2
24/24 - 8s - loss: 316.5493 - loglik: -3.1405e+02 - logprior: -2.5006e+00
Fitted a model with MAP estimate = -312.6543
expansions: [(0, 2), (194, 1)]
discards: [ 0 12 25]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 316.1781 - loglik: -3.1174e+02 - logprior: -4.4364e+00
Epoch 2/2
24/24 - 8s - loss: 310.1559 - loglik: -3.1013e+02 - logprior: -2.4092e-02
Fitted a model with MAP estimate = -308.0635
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 315.0286 - loglik: -3.1090e+02 - logprior: -4.1251e+00
Epoch 2/10
24/24 - 8s - loss: 309.6227 - loglik: -3.0994e+02 - logprior: 0.3206
Epoch 3/10
24/24 - 8s - loss: 305.8913 - loglik: -3.0670e+02 - logprior: 0.8127
Epoch 4/10
24/24 - 8s - loss: 305.5124 - loglik: -3.0653e+02 - logprior: 1.0166
Epoch 5/10
24/24 - 8s - loss: 306.2705 - loglik: -3.0743e+02 - logprior: 1.1623
Fitted a model with MAP estimate = -304.4866
Time for alignment: 147.3351
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 526.9362 - loglik: -5.2161e+02 - logprior: -5.3306e+00
Epoch 2/10
24/24 - 6s - loss: 379.6700 - loglik: -3.7743e+02 - logprior: -2.2376e+00
Epoch 3/10
24/24 - 6s - loss: 349.3369 - loglik: -3.4673e+02 - logprior: -2.6116e+00
Epoch 4/10
24/24 - 6s - loss: 345.1938 - loglik: -3.4276e+02 - logprior: -2.4295e+00
Epoch 5/10
24/24 - 6s - loss: 343.9917 - loglik: -3.4157e+02 - logprior: -2.4173e+00
Epoch 6/10
24/24 - 6s - loss: 343.8483 - loglik: -3.4142e+02 - logprior: -2.4236e+00
Epoch 7/10
24/24 - 6s - loss: 342.6101 - loglik: -3.4014e+02 - logprior: -2.4662e+00
Epoch 8/10
24/24 - 6s - loss: 342.3568 - loglik: -3.3986e+02 - logprior: -2.4937e+00
Epoch 9/10
24/24 - 6s - loss: 342.8354 - loglik: -3.4035e+02 - logprior: -2.4837e+00
Fitted a model with MAP estimate = -342.1971
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (59, 1), (63, 1), (65, 1), (83, 1), (85, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (155, 1), (158, 1), (171, 1), (172, 1), (173, 2), (174, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 334.6946 - loglik: -3.2743e+02 - logprior: -7.2676e+00
Epoch 2/2
24/24 - 8s - loss: 316.5761 - loglik: -3.1411e+02 - logprior: -2.4686e+00
Fitted a model with MAP estimate = -313.3635
expansions: [(0, 3), (194, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 315.1950 - loglik: -3.1074e+02 - logprior: -4.4501e+00
Epoch 2/2
24/24 - 8s - loss: 309.5574 - loglik: -3.0959e+02 - logprior: 0.0338
Fitted a model with MAP estimate = -306.6098
expansions: [(152, 1)]
discards: [ 0  1 26]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 319.7804 - loglik: -3.1340e+02 - logprior: -6.3773e+00
Epoch 2/10
24/24 - 8s - loss: 311.1047 - loglik: -3.1025e+02 - logprior: -8.5163e-01
Epoch 3/10
24/24 - 8s - loss: 308.7119 - loglik: -3.0982e+02 - logprior: 1.1093
Epoch 4/10
24/24 - 8s - loss: 304.9683 - loglik: -3.0613e+02 - logprior: 1.1614
Epoch 5/10
24/24 - 8s - loss: 306.3130 - loglik: -3.0760e+02 - logprior: 1.2885
Fitted a model with MAP estimate = -304.7936
Time for alignment: 164.7993
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 528.3893 - loglik: -5.2306e+02 - logprior: -5.3337e+00
Epoch 2/10
24/24 - 6s - loss: 376.1992 - loglik: -3.7397e+02 - logprior: -2.2272e+00
Epoch 3/10
24/24 - 6s - loss: 349.7473 - loglik: -3.4712e+02 - logprior: -2.6235e+00
Epoch 4/10
24/24 - 6s - loss: 343.7588 - loglik: -3.4129e+02 - logprior: -2.4688e+00
Epoch 5/10
24/24 - 6s - loss: 343.4088 - loglik: -3.4096e+02 - logprior: -2.4536e+00
Epoch 6/10
24/24 - 6s - loss: 342.6151 - loglik: -3.4014e+02 - logprior: -2.4786e+00
Epoch 7/10
24/24 - 6s - loss: 341.7018 - loglik: -3.3917e+02 - logprior: -2.5350e+00
Epoch 8/10
24/24 - 6s - loss: 341.9462 - loglik: -3.3941e+02 - logprior: -2.5319e+00
Fitted a model with MAP estimate = -341.4773
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (78, 1), (87, 1), (88, 1), (91, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 4), (154, 1), (155, 1), (157, 1), (171, 1), (173, 3), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 334.7637 - loglik: -3.2752e+02 - logprior: -7.2467e+00
Epoch 2/2
24/24 - 8s - loss: 317.8396 - loglik: -3.1540e+02 - logprior: -2.4393e+00
Fitted a model with MAP estimate = -314.4685
expansions: [(0, 3), (215, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 315.6866 - loglik: -3.1123e+02 - logprior: -4.4576e+00
Epoch 2/2
24/24 - 8s - loss: 309.5335 - loglik: -3.0956e+02 - logprior: 0.0286
Fitted a model with MAP estimate = -306.9441
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 318.8388 - loglik: -3.1247e+02 - logprior: -6.3719e+00
Epoch 2/10
24/24 - 8s - loss: 311.4860 - loglik: -3.1062e+02 - logprior: -8.6392e-01
Epoch 3/10
24/24 - 8s - loss: 308.3676 - loglik: -3.0943e+02 - logprior: 1.0589
Epoch 4/10
24/24 - 8s - loss: 306.8473 - loglik: -3.0798e+02 - logprior: 1.1298
Epoch 5/10
24/24 - 8s - loss: 305.1413 - loglik: -3.0644e+02 - logprior: 1.3017
Epoch 6/10
24/24 - 8s - loss: 305.0487 - loglik: -3.0648e+02 - logprior: 1.4298
Epoch 7/10
24/24 - 8s - loss: 304.6007 - loglik: -3.0623e+02 - logprior: 1.6282
Epoch 8/10
24/24 - 8s - loss: 304.3264 - loglik: -3.0615e+02 - logprior: 1.8240
Epoch 9/10
24/24 - 8s - loss: 304.8890 - loglik: -3.0690e+02 - logprior: 2.0082
Fitted a model with MAP estimate = -303.7336
Time for alignment: 188.4537
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 528.5563 - loglik: -5.2322e+02 - logprior: -5.3403e+00
Epoch 2/10
24/24 - 6s - loss: 379.0786 - loglik: -3.7681e+02 - logprior: -2.2709e+00
Epoch 3/10
24/24 - 6s - loss: 347.3631 - loglik: -3.4475e+02 - logprior: -2.6085e+00
Epoch 4/10
24/24 - 6s - loss: 344.3773 - loglik: -3.4198e+02 - logprior: -2.3943e+00
Epoch 5/10
24/24 - 6s - loss: 342.1078 - loglik: -3.3970e+02 - logprior: -2.4123e+00
Epoch 6/10
24/24 - 6s - loss: 343.1083 - loglik: -3.4066e+02 - logprior: -2.4497e+00
Fitted a model with MAP estimate = -341.9050
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (47, 1), (48, 1), (61, 1), (65, 1), (75, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 336.1494 - loglik: -3.2888e+02 - logprior: -7.2659e+00
Epoch 2/2
24/24 - 8s - loss: 315.0666 - loglik: -3.1262e+02 - logprior: -2.4458e+00
Fitted a model with MAP estimate = -313.5326
expansions: [(0, 2), (190, 1), (192, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 317.0796 - loglik: -3.1263e+02 - logprior: -4.4493e+00
Epoch 2/2
24/24 - 8s - loss: 309.7200 - loglik: -3.0970e+02 - logprior: -2.2601e-02
Fitted a model with MAP estimate = -307.9725
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 314.7826 - loglik: -3.1065e+02 - logprior: -4.1283e+00
Epoch 2/10
24/24 - 8s - loss: 309.4604 - loglik: -3.0979e+02 - logprior: 0.3250
Epoch 3/10
24/24 - 8s - loss: 306.0447 - loglik: -3.0686e+02 - logprior: 0.8120
Epoch 4/10
24/24 - 8s - loss: 306.8001 - loglik: -3.0781e+02 - logprior: 1.0074
Fitted a model with MAP estimate = -305.1613
Time for alignment: 137.5005
Computed alignments with likelihoods: ['-304.5933', '-304.4866', '-304.7936', '-303.7336', '-305.1613']
Best model has likelihood: -303.7336  (prior= 2.1336 )
time for generating output: 0.2688
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9795549122489596
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.5414 - loglik: -2.7647e+02 - logprior: -2.7072e+01
Epoch 2/10
10/10 - 1s - loss: 246.2412 - loglik: -2.3935e+02 - logprior: -6.8938e+00
Epoch 3/10
10/10 - 1s - loss: 208.5406 - loglik: -2.0492e+02 - logprior: -3.6220e+00
Epoch 4/10
10/10 - 1s - loss: 189.6184 - loglik: -1.8702e+02 - logprior: -2.6031e+00
Epoch 5/10
10/10 - 1s - loss: 180.8662 - loglik: -1.7876e+02 - logprior: -2.1069e+00
Epoch 6/10
10/10 - 1s - loss: 177.7900 - loglik: -1.7612e+02 - logprior: -1.6739e+00
Epoch 7/10
10/10 - 1s - loss: 175.6019 - loglik: -1.7413e+02 - logprior: -1.4763e+00
Epoch 8/10
10/10 - 1s - loss: 175.0844 - loglik: -1.7365e+02 - logprior: -1.4335e+00
Epoch 9/10
10/10 - 1s - loss: 174.2268 - loglik: -1.7290e+02 - logprior: -1.3233e+00
Epoch 10/10
10/10 - 1s - loss: 173.9661 - loglik: -1.7273e+02 - logprior: -1.2381e+00
Fitted a model with MAP estimate = -173.7345
expansions: [(0, 3), (10, 2), (34, 1), (36, 4), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.7280 - loglik: -1.7027e+02 - logprior: -3.4457e+01
Epoch 2/2
10/10 - 1s - loss: 171.8472 - loglik: -1.6156e+02 - logprior: -1.0292e+01
Fitted a model with MAP estimate = -164.7580
expansions: []
discards: [14 91]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.5928 - loglik: -1.5823e+02 - logprior: -2.5366e+01
Epoch 2/2
10/10 - 1s - loss: 163.3877 - loglik: -1.5693e+02 - logprior: -6.4586e+00
Fitted a model with MAP estimate = -160.1238
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 180.8498 - loglik: -1.5672e+02 - logprior: -2.4130e+01
Epoch 2/10
10/10 - 1s - loss: 161.6794 - loglik: -1.5565e+02 - logprior: -6.0287e+00
Epoch 3/10
10/10 - 1s - loss: 157.3293 - loglik: -1.5487e+02 - logprior: -2.4564e+00
Epoch 4/10
10/10 - 1s - loss: 155.6766 - loglik: -1.5459e+02 - logprior: -1.0907e+00
Epoch 5/10
10/10 - 1s - loss: 154.1320 - loglik: -1.5368e+02 - logprior: -4.5222e-01
Epoch 6/10
10/10 - 1s - loss: 153.9727 - loglik: -1.5383e+02 - logprior: -1.3831e-01
Epoch 7/10
10/10 - 1s - loss: 153.3086 - loglik: -1.5340e+02 - logprior: 0.0916
Epoch 8/10
10/10 - 1s - loss: 153.2451 - loglik: -1.5355e+02 - logprior: 0.3075
Epoch 9/10
10/10 - 1s - loss: 152.9158 - loglik: -1.5340e+02 - logprior: 0.4803
Epoch 10/10
10/10 - 1s - loss: 152.6058 - loglik: -1.5322e+02 - logprior: 0.6107
Fitted a model with MAP estimate = -152.8302
Time for alignment: 36.1284
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 303.7527 - loglik: -2.7668e+02 - logprior: -2.7071e+01
Epoch 2/10
10/10 - 1s - loss: 246.4150 - loglik: -2.3952e+02 - logprior: -6.8992e+00
Epoch 3/10
10/10 - 1s - loss: 207.2029 - loglik: -2.0359e+02 - logprior: -3.6176e+00
Epoch 4/10
10/10 - 1s - loss: 186.2675 - loglik: -1.8368e+02 - logprior: -2.5831e+00
Epoch 5/10
10/10 - 1s - loss: 179.4831 - loglik: -1.7744e+02 - logprior: -2.0386e+00
Epoch 6/10
10/10 - 1s - loss: 176.4925 - loglik: -1.7490e+02 - logprior: -1.5910e+00
Epoch 7/10
10/10 - 1s - loss: 175.8274 - loglik: -1.7446e+02 - logprior: -1.3720e+00
Epoch 8/10
10/10 - 1s - loss: 175.2730 - loglik: -1.7399e+02 - logprior: -1.2853e+00
Epoch 9/10
10/10 - 1s - loss: 174.4487 - loglik: -1.7324e+02 - logprior: -1.2044e+00
Epoch 10/10
10/10 - 1s - loss: 174.1818 - loglik: -1.7301e+02 - logprior: -1.1680e+00
Fitted a model with MAP estimate = -173.5647
expansions: [(0, 3), (10, 2), (34, 1), (36, 5), (48, 2), (49, 2), (69, 2), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.0239 - loglik: -1.7057e+02 - logprior: -3.4450e+01
Epoch 2/2
10/10 - 1s - loss: 171.0237 - loglik: -1.6065e+02 - logprior: -1.0372e+01
Fitted a model with MAP estimate = -164.2748
expansions: [(60, 1)]
discards: [13]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.1926 - loglik: -1.5683e+02 - logprior: -2.5365e+01
Epoch 2/2
10/10 - 1s - loss: 160.7649 - loglik: -1.5434e+02 - logprior: -6.4261e+00
Fitted a model with MAP estimate = -157.5402
expansions: []
discards: [87]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.4908 - loglik: -1.5442e+02 - logprior: -2.4075e+01
Epoch 2/10
10/10 - 1s - loss: 160.0425 - loglik: -1.5402e+02 - logprior: -6.0209e+00
Epoch 3/10
10/10 - 1s - loss: 156.0033 - loglik: -1.5356e+02 - logprior: -2.4463e+00
Epoch 4/10
10/10 - 1s - loss: 153.7183 - loglik: -1.5265e+02 - logprior: -1.0637e+00
Epoch 5/10
10/10 - 1s - loss: 152.8263 - loglik: -1.5241e+02 - logprior: -4.2120e-01
Epoch 6/10
10/10 - 1s - loss: 151.5435 - loglik: -1.5141e+02 - logprior: -1.3750e-01
Epoch 7/10
10/10 - 1s - loss: 151.6302 - loglik: -1.5169e+02 - logprior: 0.0611
Fitted a model with MAP estimate = -151.3250
Time for alignment: 37.9949
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 303.7022 - loglik: -2.7663e+02 - logprior: -2.7071e+01
Epoch 2/10
10/10 - 1s - loss: 246.8567 - loglik: -2.3996e+02 - logprior: -6.9001e+00
Epoch 3/10
10/10 - 1s - loss: 206.9460 - loglik: -2.0330e+02 - logprior: -3.6477e+00
Epoch 4/10
10/10 - 1s - loss: 185.5227 - loglik: -1.8282e+02 - logprior: -2.7057e+00
Epoch 5/10
10/10 - 1s - loss: 178.9629 - loglik: -1.7671e+02 - logprior: -2.2551e+00
Epoch 6/10
10/10 - 1s - loss: 176.8699 - loglik: -1.7504e+02 - logprior: -1.8255e+00
Epoch 7/10
10/10 - 1s - loss: 175.7461 - loglik: -1.7418e+02 - logprior: -1.5644e+00
Epoch 8/10
10/10 - 1s - loss: 175.5329 - loglik: -1.7405e+02 - logprior: -1.4789e+00
Epoch 9/10
10/10 - 1s - loss: 174.7066 - loglik: -1.7327e+02 - logprior: -1.4360e+00
Epoch 10/10
10/10 - 1s - loss: 174.6772 - loglik: -1.7330e+02 - logprior: -1.3816e+00
Fitted a model with MAP estimate = -174.5008
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (36, 4), (37, 1), (48, 2), (50, 2), (69, 2), (70, 2), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.6160 - loglik: -1.7010e+02 - logprior: -3.4512e+01
Epoch 2/2
10/10 - 1s - loss: 171.0721 - loglik: -1.6080e+02 - logprior: -1.0270e+01
Fitted a model with MAP estimate = -163.7389
expansions: []
discards: [86 87]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.1837 - loglik: -1.5784e+02 - logprior: -2.5349e+01
Epoch 2/2
10/10 - 1s - loss: 161.5802 - loglik: -1.5514e+02 - logprior: -6.4376e+00
Fitted a model with MAP estimate = -158.8588
expansions: [(59, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.6811 - loglik: -1.5561e+02 - logprior: -2.4074e+01
Epoch 2/10
10/10 - 1s - loss: 160.3496 - loglik: -1.5431e+02 - logprior: -6.0406e+00
Epoch 3/10
10/10 - 1s - loss: 155.7529 - loglik: -1.5325e+02 - logprior: -2.5046e+00
Epoch 4/10
10/10 - 1s - loss: 154.0004 - loglik: -1.5287e+02 - logprior: -1.1345e+00
Epoch 5/10
10/10 - 1s - loss: 153.0350 - loglik: -1.5256e+02 - logprior: -4.7686e-01
Epoch 6/10
10/10 - 1s - loss: 151.9520 - loglik: -1.5181e+02 - logprior: -1.4505e-01
Epoch 7/10
10/10 - 1s - loss: 151.8475 - loglik: -1.5196e+02 - logprior: 0.1160
Epoch 8/10
10/10 - 1s - loss: 151.5289 - loglik: -1.5186e+02 - logprior: 0.3300
Epoch 9/10
10/10 - 1s - loss: 151.3608 - loglik: -1.5185e+02 - logprior: 0.4865
Epoch 10/10
10/10 - 1s - loss: 151.7170 - loglik: -1.5232e+02 - logprior: 0.6002
Fitted a model with MAP estimate = -151.2441
Time for alignment: 39.3458
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 303.7903 - loglik: -2.7672e+02 - logprior: -2.7071e+01
Epoch 2/10
10/10 - 1s - loss: 246.4289 - loglik: -2.3953e+02 - logprior: -6.9004e+00
Epoch 3/10
10/10 - 1s - loss: 209.1012 - loglik: -2.0548e+02 - logprior: -3.6188e+00
Epoch 4/10
10/10 - 1s - loss: 190.3189 - loglik: -1.8774e+02 - logprior: -2.5800e+00
Epoch 5/10
10/10 - 1s - loss: 181.4097 - loglik: -1.7937e+02 - logprior: -2.0409e+00
Epoch 6/10
10/10 - 1s - loss: 177.3738 - loglik: -1.7576e+02 - logprior: -1.6118e+00
Epoch 7/10
10/10 - 1s - loss: 176.3344 - loglik: -1.7497e+02 - logprior: -1.3631e+00
Epoch 8/10
10/10 - 1s - loss: 175.2788 - loglik: -1.7402e+02 - logprior: -1.2555e+00
Epoch 9/10
10/10 - 1s - loss: 174.7804 - loglik: -1.7359e+02 - logprior: -1.1882e+00
Epoch 10/10
10/10 - 1s - loss: 174.4228 - loglik: -1.7328e+02 - logprior: -1.1390e+00
Fitted a model with MAP estimate = -174.2338
expansions: [(0, 3), (10, 1), (34, 1), (36, 4), (48, 3), (49, 2), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.8130 - loglik: -1.6945e+02 - logprior: -3.4359e+01
Epoch 2/2
10/10 - 1s - loss: 169.9745 - loglik: -1.5972e+02 - logprior: -1.0257e+01
Fitted a model with MAP estimate = -163.2536
expansions: []
discards: [87]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.8197 - loglik: -1.5647e+02 - logprior: -2.5349e+01
Epoch 2/2
10/10 - 1s - loss: 161.5420 - loglik: -1.5509e+02 - logprior: -6.4529e+00
Fitted a model with MAP estimate = -157.8861
expansions: [(59, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.0270 - loglik: -1.5394e+02 - logprior: -2.4090e+01
Epoch 2/10
10/10 - 1s - loss: 158.6221 - loglik: -1.5257e+02 - logprior: -6.0570e+00
Epoch 3/10
10/10 - 1s - loss: 154.8590 - loglik: -1.5235e+02 - logprior: -2.5108e+00
Epoch 4/10
10/10 - 1s - loss: 152.1971 - loglik: -1.5105e+02 - logprior: -1.1509e+00
Epoch 5/10
10/10 - 1s - loss: 151.9134 - loglik: -1.5143e+02 - logprior: -4.8814e-01
Epoch 6/10
10/10 - 1s - loss: 151.3907 - loglik: -1.5124e+02 - logprior: -1.5560e-01
Epoch 7/10
10/10 - 1s - loss: 151.1355 - loglik: -1.5124e+02 - logprior: 0.1077
Epoch 8/10
10/10 - 1s - loss: 150.2168 - loglik: -1.5054e+02 - logprior: 0.3247
Epoch 9/10
10/10 - 1s - loss: 150.8426 - loglik: -1.5132e+02 - logprior: 0.4799
Fitted a model with MAP estimate = -150.4505
Time for alignment: 38.7089
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8319 - loglik: -2.7676e+02 - logprior: -2.7068e+01
Epoch 2/10
10/10 - 1s - loss: 247.3313 - loglik: -2.4043e+02 - logprior: -6.8979e+00
Epoch 3/10
10/10 - 1s - loss: 207.5800 - loglik: -2.0395e+02 - logprior: -3.6252e+00
Epoch 4/10
10/10 - 1s - loss: 187.1816 - loglik: -1.8454e+02 - logprior: -2.6388e+00
Epoch 5/10
10/10 - 1s - loss: 178.9888 - loglik: -1.7682e+02 - logprior: -2.1659e+00
Epoch 6/10
10/10 - 1s - loss: 176.9481 - loglik: -1.7518e+02 - logprior: -1.7668e+00
Epoch 7/10
10/10 - 1s - loss: 176.0765 - loglik: -1.7457e+02 - logprior: -1.5073e+00
Epoch 8/10
10/10 - 1s - loss: 175.1227 - loglik: -1.7369e+02 - logprior: -1.4350e+00
Epoch 9/10
10/10 - 1s - loss: 175.2832 - loglik: -1.7390e+02 - logprior: -1.3834e+00
Fitted a model with MAP estimate = -174.9592
expansions: [(0, 2), (11, 1), (12, 1), (33, 1), (36, 4), (50, 3), (70, 1), (71, 1), (72, 2), (73, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 205.6738 - loglik: -1.7126e+02 - logprior: -3.4415e+01
Epoch 2/2
10/10 - 1s - loss: 172.7918 - loglik: -1.6267e+02 - logprior: -1.0123e+01
Fitted a model with MAP estimate = -166.4305
expansions: [(58, 2), (89, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.9501 - loglik: -1.5868e+02 - logprior: -2.5274e+01
Epoch 2/2
10/10 - 1s - loss: 162.5978 - loglik: -1.5618e+02 - logprior: -6.4147e+00
Fitted a model with MAP estimate = -158.5662
expansions: [(59, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.8334 - loglik: -1.5479e+02 - logprior: -2.4045e+01
Epoch 2/10
10/10 - 1s - loss: 159.5261 - loglik: -1.5352e+02 - logprior: -6.0100e+00
Epoch 3/10
10/10 - 1s - loss: 155.1070 - loglik: -1.5267e+02 - logprior: -2.4393e+00
Epoch 4/10
10/10 - 1s - loss: 154.3008 - loglik: -1.5320e+02 - logprior: -1.1058e+00
Epoch 5/10
10/10 - 1s - loss: 152.7569 - loglik: -1.5229e+02 - logprior: -4.6879e-01
Epoch 6/10
10/10 - 1s - loss: 152.3644 - loglik: -1.5223e+02 - logprior: -1.3888e-01
Epoch 7/10
10/10 - 1s - loss: 152.0984 - loglik: -1.5223e+02 - logprior: 0.1291
Epoch 8/10
10/10 - 1s - loss: 151.4694 - loglik: -1.5182e+02 - logprior: 0.3532
Epoch 9/10
10/10 - 1s - loss: 151.8743 - loglik: -1.5239e+02 - logprior: 0.5134
Fitted a model with MAP estimate = -151.5549
Time for alignment: 37.6817
Computed alignments with likelihoods: ['-152.8302', '-151.3250', '-151.2441', '-150.4505', '-151.5549']
Best model has likelihood: -150.4505  (prior= 0.5425 )
time for generating output: 0.1305
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.8236286919831224
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.1387 - loglik: -1.4605e+02 - logprior: -5.0909e+00
Epoch 2/10
16/16 - 1s - loss: 126.9374 - loglik: -1.2533e+02 - logprior: -1.6111e+00
Epoch 3/10
16/16 - 1s - loss: 116.2152 - loglik: -1.1464e+02 - logprior: -1.5786e+00
Epoch 4/10
16/16 - 1s - loss: 111.5441 - loglik: -1.1003e+02 - logprior: -1.5140e+00
Epoch 5/10
16/16 - 1s - loss: 109.7257 - loglik: -1.0817e+02 - logprior: -1.5599e+00
Epoch 6/10
16/16 - 1s - loss: 109.1994 - loglik: -1.0768e+02 - logprior: -1.5243e+00
Epoch 7/10
16/16 - 1s - loss: 109.0898 - loglik: -1.0756e+02 - logprior: -1.5269e+00
Epoch 8/10
16/16 - 1s - loss: 109.0766 - loglik: -1.0755e+02 - logprior: -1.5251e+00
Epoch 9/10
16/16 - 1s - loss: 108.5915 - loglik: -1.0709e+02 - logprior: -1.5054e+00
Epoch 10/10
16/16 - 1s - loss: 108.8734 - loglik: -1.0736e+02 - logprior: -1.5097e+00
Fitted a model with MAP estimate = -108.7205
expansions: [(3, 1), (6, 1), (12, 2), (17, 1), (20, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.9630 - loglik: -1.0761e+02 - logprior: -6.3503e+00
Epoch 2/2
16/16 - 1s - loss: 105.4772 - loglik: -1.0242e+02 - logprior: -3.0564e+00
Fitted a model with MAP estimate = -104.3227
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 106.6818 - loglik: -1.0207e+02 - logprior: -4.6112e+00
Epoch 2/2
16/16 - 1s - loss: 102.9858 - loglik: -1.0136e+02 - logprior: -1.6241e+00
Fitted a model with MAP estimate = -102.4231
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 106.0214 - loglik: -1.0143e+02 - logprior: -4.5870e+00
Epoch 2/10
16/16 - 1s - loss: 102.3674 - loglik: -1.0079e+02 - logprior: -1.5760e+00
Epoch 3/10
16/16 - 1s - loss: 101.6359 - loglik: -1.0034e+02 - logprior: -1.2949e+00
Epoch 4/10
16/16 - 1s - loss: 101.7646 - loglik: -1.0054e+02 - logprior: -1.2294e+00
Fitted a model with MAP estimate = -101.1401
Time for alignment: 44.4949
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.0533 - loglik: -1.4596e+02 - logprior: -5.0898e+00
Epoch 2/10
16/16 - 1s - loss: 128.3333 - loglik: -1.2675e+02 - logprior: -1.5839e+00
Epoch 3/10
16/16 - 1s - loss: 117.4097 - loglik: -1.1586e+02 - logprior: -1.5458e+00
Epoch 4/10
16/16 - 1s - loss: 111.4195 - loglik: -1.0981e+02 - logprior: -1.6120e+00
Epoch 5/10
16/16 - 1s - loss: 109.0275 - loglik: -1.0736e+02 - logprior: -1.6679e+00
Epoch 6/10
16/16 - 1s - loss: 108.6777 - loglik: -1.0702e+02 - logprior: -1.6554e+00
Epoch 7/10
16/16 - 1s - loss: 108.4472 - loglik: -1.0682e+02 - logprior: -1.6245e+00
Epoch 8/10
16/16 - 1s - loss: 108.4921 - loglik: -1.0690e+02 - logprior: -1.5963e+00
Fitted a model with MAP estimate = -108.2548
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (17, 1), (23, 5), (24, 2), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.9657 - loglik: -1.0763e+02 - logprior: -6.3307e+00
Epoch 2/2
16/16 - 1s - loss: 105.5235 - loglik: -1.0252e+02 - logprior: -3.0039e+00
Fitted a model with MAP estimate = -104.2230
expansions: [(0, 1)]
discards: [ 0 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.7786 - loglik: -1.0217e+02 - logprior: -4.6067e+00
Epoch 2/2
16/16 - 1s - loss: 103.1381 - loglik: -1.0151e+02 - logprior: -1.6260e+00
Fitted a model with MAP estimate = -102.4253
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9355 - loglik: -1.0267e+02 - logprior: -6.2688e+00
Epoch 2/10
16/16 - 1s - loss: 104.4447 - loglik: -1.0171e+02 - logprior: -2.7356e+00
Epoch 3/10
16/16 - 1s - loss: 102.7244 - loglik: -1.0122e+02 - logprior: -1.5090e+00
Epoch 4/10
16/16 - 1s - loss: 102.1897 - loglik: -1.0094e+02 - logprior: -1.2531e+00
Epoch 5/10
16/16 - 1s - loss: 101.6434 - loglik: -1.0042e+02 - logprior: -1.2214e+00
Epoch 6/10
16/16 - 1s - loss: 101.4086 - loglik: -1.0021e+02 - logprior: -1.1962e+00
Epoch 7/10
16/16 - 1s - loss: 100.9713 - loglik: -9.9788e+01 - logprior: -1.1838e+00
Epoch 8/10
16/16 - 1s - loss: 100.9812 - loglik: -9.9815e+01 - logprior: -1.1660e+00
Fitted a model with MAP estimate = -100.8969
Time for alignment: 46.1591
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.1389 - loglik: -1.4605e+02 - logprior: -5.0891e+00
Epoch 2/10
16/16 - 1s - loss: 128.1852 - loglik: -1.2658e+02 - logprior: -1.6102e+00
Epoch 3/10
16/16 - 1s - loss: 117.4433 - loglik: -1.1589e+02 - logprior: -1.5491e+00
Epoch 4/10
16/16 - 1s - loss: 112.5206 - loglik: -1.1105e+02 - logprior: -1.4677e+00
Epoch 5/10
16/16 - 1s - loss: 110.8258 - loglik: -1.0932e+02 - logprior: -1.5065e+00
Epoch 6/10
16/16 - 1s - loss: 109.9190 - loglik: -1.0845e+02 - logprior: -1.4732e+00
Epoch 7/10
16/16 - 1s - loss: 109.2597 - loglik: -1.0778e+02 - logprior: -1.4810e+00
Epoch 8/10
16/16 - 1s - loss: 108.9268 - loglik: -1.0746e+02 - logprior: -1.4667e+00
Epoch 9/10
16/16 - 1s - loss: 108.9552 - loglik: -1.0750e+02 - logprior: -1.4555e+00
Fitted a model with MAP estimate = -108.8849
expansions: [(3, 1), (6, 1), (12, 2), (17, 1), (24, 8), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.6245 - loglik: -1.0726e+02 - logprior: -6.3618e+00
Epoch 2/2
16/16 - 1s - loss: 105.4077 - loglik: -1.0234e+02 - logprior: -3.0676e+00
Fitted a model with MAP estimate = -104.2910
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 106.6895 - loglik: -1.0208e+02 - logprior: -4.6118e+00
Epoch 2/2
16/16 - 1s - loss: 103.1894 - loglik: -1.0155e+02 - logprior: -1.6380e+00
Fitted a model with MAP estimate = -102.4285
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9902 - loglik: -1.0272e+02 - logprior: -6.2703e+00
Epoch 2/10
16/16 - 1s - loss: 104.2740 - loglik: -1.0158e+02 - logprior: -2.6943e+00
Epoch 3/10
16/16 - 1s - loss: 102.9318 - loglik: -1.0144e+02 - logprior: -1.4878e+00
Epoch 4/10
16/16 - 1s - loss: 102.0748 - loglik: -1.0082e+02 - logprior: -1.2499e+00
Epoch 5/10
16/16 - 1s - loss: 101.6578 - loglik: -1.0043e+02 - logprior: -1.2247e+00
Epoch 6/10
16/16 - 1s - loss: 101.2515 - loglik: -1.0005e+02 - logprior: -1.2011e+00
Epoch 7/10
16/16 - 1s - loss: 101.0237 - loglik: -9.9847e+01 - logprior: -1.1764e+00
Epoch 8/10
16/16 - 1s - loss: 100.7815 - loglik: -9.9619e+01 - logprior: -1.1621e+00
Epoch 9/10
16/16 - 1s - loss: 101.0253 - loglik: -9.9880e+01 - logprior: -1.1455e+00
Fitted a model with MAP estimate = -100.8208
Time for alignment: 48.5305
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 150.9929 - loglik: -1.4590e+02 - logprior: -5.0921e+00
Epoch 2/10
16/16 - 1s - loss: 127.2611 - loglik: -1.2564e+02 - logprior: -1.6173e+00
Epoch 3/10
16/16 - 1s - loss: 115.0536 - loglik: -1.1340e+02 - logprior: -1.6550e+00
Epoch 4/10
16/16 - 1s - loss: 110.4641 - loglik: -1.0881e+02 - logprior: -1.6538e+00
Epoch 5/10
16/16 - 1s - loss: 109.1098 - loglik: -1.0745e+02 - logprior: -1.6586e+00
Epoch 6/10
16/16 - 1s - loss: 109.1006 - loglik: -1.0747e+02 - logprior: -1.6279e+00
Epoch 7/10
16/16 - 1s - loss: 108.2522 - loglik: -1.0664e+02 - logprior: -1.6141e+00
Epoch 8/10
16/16 - 1s - loss: 108.6777 - loglik: -1.0710e+02 - logprior: -1.5789e+00
Fitted a model with MAP estimate = -108.4755
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.2451 - loglik: -1.0791e+02 - logprior: -6.3348e+00
Epoch 2/2
16/16 - 1s - loss: 105.4790 - loglik: -1.0245e+02 - logprior: -3.0265e+00
Fitted a model with MAP estimate = -104.2642
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 106.7079 - loglik: -1.0210e+02 - logprior: -4.6071e+00
Epoch 2/2
16/16 - 1s - loss: 102.8567 - loglik: -1.0122e+02 - logprior: -1.6348e+00
Fitted a model with MAP estimate = -102.4183
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9643 - loglik: -1.0269e+02 - logprior: -6.2758e+00
Epoch 2/10
16/16 - 1s - loss: 104.5137 - loglik: -1.0178e+02 - logprior: -2.7335e+00
Epoch 3/10
16/16 - 1s - loss: 102.4289 - loglik: -1.0091e+02 - logprior: -1.5152e+00
Epoch 4/10
16/16 - 1s - loss: 102.2710 - loglik: -1.0102e+02 - logprior: -1.2469e+00
Epoch 5/10
16/16 - 1s - loss: 101.6501 - loglik: -1.0043e+02 - logprior: -1.2161e+00
Epoch 6/10
16/16 - 1s - loss: 101.1575 - loglik: -9.9952e+01 - logprior: -1.2054e+00
Epoch 7/10
16/16 - 1s - loss: 101.1504 - loglik: -9.9969e+01 - logprior: -1.1812e+00
Epoch 8/10
16/16 - 1s - loss: 101.0017 - loglik: -9.9841e+01 - logprior: -1.1612e+00
Epoch 9/10
16/16 - 1s - loss: 101.0906 - loglik: -9.9951e+01 - logprior: -1.1399e+00
Fitted a model with MAP estimate = -100.8295
Time for alignment: 47.5516
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 150.9224 - loglik: -1.4583e+02 - logprior: -5.0954e+00
Epoch 2/10
16/16 - 1s - loss: 127.9435 - loglik: -1.2632e+02 - logprior: -1.6206e+00
Epoch 3/10
16/16 - 1s - loss: 115.6023 - loglik: -1.1394e+02 - logprior: -1.6618e+00
Epoch 4/10
16/16 - 1s - loss: 110.3506 - loglik: -1.0868e+02 - logprior: -1.6735e+00
Epoch 5/10
16/16 - 1s - loss: 109.1550 - loglik: -1.0748e+02 - logprior: -1.6701e+00
Epoch 6/10
16/16 - 1s - loss: 108.4281 - loglik: -1.0678e+02 - logprior: -1.6502e+00
Epoch 7/10
16/16 - 1s - loss: 108.7859 - loglik: -1.0717e+02 - logprior: -1.6132e+00
Fitted a model with MAP estimate = -108.3373
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (23, 5), (24, 2), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.9593 - loglik: -1.0763e+02 - logprior: -6.3274e+00
Epoch 2/2
16/16 - 1s - loss: 105.7746 - loglik: -1.0278e+02 - logprior: -2.9947e+00
Fitted a model with MAP estimate = -104.1913
expansions: [(0, 1)]
discards: [ 0 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 106.5933 - loglik: -1.0198e+02 - logprior: -4.6087e+00
Epoch 2/2
16/16 - 1s - loss: 103.1706 - loglik: -1.0153e+02 - logprior: -1.6370e+00
Fitted a model with MAP estimate = -102.3780
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9393 - loglik: -1.0267e+02 - logprior: -6.2672e+00
Epoch 2/10
16/16 - 1s - loss: 104.6230 - loglik: -1.0192e+02 - logprior: -2.7024e+00
Epoch 3/10
16/16 - 1s - loss: 102.4138 - loglik: -1.0092e+02 - logprior: -1.4978e+00
Epoch 4/10
16/16 - 1s - loss: 102.1066 - loglik: -1.0086e+02 - logprior: -1.2484e+00
Epoch 5/10
16/16 - 1s - loss: 101.6022 - loglik: -1.0038e+02 - logprior: -1.2204e+00
Epoch 6/10
16/16 - 1s - loss: 100.9184 - loglik: -9.9710e+01 - logprior: -1.2081e+00
Epoch 7/10
16/16 - 1s - loss: 100.9734 - loglik: -9.9790e+01 - logprior: -1.1839e+00
Fitted a model with MAP estimate = -100.9366
Time for alignment: 43.3508
Computed alignments with likelihoods: ['-101.1401', '-100.8969', '-100.8208', '-100.8295', '-100.9366']
Best model has likelihood: -100.8208  (prior= -1.1408 )
time for generating output: 0.1058
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.8901453957996769
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 588.2720 - loglik: -5.8662e+02 - logprior: -1.6569e+00
Epoch 2/10
39/39 - 9s - loss: 539.6404 - loglik: -5.3873e+02 - logprior: -9.1058e-01
Epoch 3/10
39/39 - 9s - loss: 534.0125 - loglik: -5.3315e+02 - logprior: -8.6428e-01
Epoch 4/10
39/39 - 9s - loss: 531.8763 - loglik: -5.3102e+02 - logprior: -8.5186e-01
Epoch 5/10
39/39 - 9s - loss: 530.4811 - loglik: -5.2962e+02 - logprior: -8.5862e-01
Epoch 6/10
39/39 - 9s - loss: 529.9650 - loglik: -5.2909e+02 - logprior: -8.7044e-01
Epoch 7/10
39/39 - 9s - loss: 529.5125 - loglik: -5.2865e+02 - logprior: -8.6679e-01
Epoch 8/10
39/39 - 9s - loss: 529.5688 - loglik: -5.2869e+02 - logprior: -8.7526e-01
Fitted a model with MAP estimate = -492.3134
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 2), (23, 6), (56, 1), (71, 2), (77, 2), (86, 1), (91, 2), (105, 1), (106, 1), (121, 2), (124, 3), (125, 3), (146, 3)]
discards: [  0 131 132 133 134 135]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 536.5786 - loglik: -5.3394e+02 - logprior: -2.6413e+00
Epoch 2/2
39/39 - 11s - loss: 525.4901 - loglik: -5.2480e+02 - logprior: -6.8671e-01
Fitted a model with MAP estimate = -479.2358
expansions: [(0, 2), (38, 1), (39, 1), (93, 1), (149, 1), (153, 1)]
discards: [  0  13  26 155 161 162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 527.0960 - loglik: -5.2544e+02 - logprior: -1.6551e+00
Epoch 2/2
39/39 - 11s - loss: 522.9454 - loglik: -5.2246e+02 - logprior: -4.8294e-01
Fitted a model with MAP estimate = -477.1308
expansions: [(164, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 20s - loss: 472.8204 - loglik: -4.7168e+02 - logprior: -1.1448e+00
Epoch 2/10
51/51 - 14s - loss: 469.1578 - loglik: -4.6881e+02 - logprior: -3.4700e-01
Epoch 3/10
51/51 - 14s - loss: 467.9283 - loglik: -4.6761e+02 - logprior: -3.1593e-01
Epoch 4/10
51/51 - 14s - loss: 466.8572 - loglik: -4.6657e+02 - logprior: -2.8724e-01
Epoch 5/10
51/51 - 14s - loss: 464.8294 - loglik: -4.6451e+02 - logprior: -3.2406e-01
Epoch 6/10
51/51 - 13s - loss: 462.8274 - loglik: -4.6251e+02 - logprior: -3.1357e-01
Epoch 7/10
51/51 - 14s - loss: 465.1071 - loglik: -4.6482e+02 - logprior: -2.8662e-01
Fitted a model with MAP estimate = -463.6736
Time for alignment: 289.4722
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 588.6333 - loglik: -5.8697e+02 - logprior: -1.6649e+00
Epoch 2/10
39/39 - 9s - loss: 540.5101 - loglik: -5.3953e+02 - logprior: -9.7650e-01
Epoch 3/10
39/39 - 9s - loss: 534.7474 - loglik: -5.3378e+02 - logprior: -9.6739e-01
Epoch 4/10
39/39 - 9s - loss: 532.4094 - loglik: -5.3144e+02 - logprior: -9.7061e-01
Epoch 5/10
39/39 - 9s - loss: 530.9685 - loglik: -5.2998e+02 - logprior: -9.9316e-01
Epoch 6/10
39/39 - 9s - loss: 530.4081 - loglik: -5.2940e+02 - logprior: -1.0056e+00
Epoch 7/10
39/39 - 9s - loss: 530.3392 - loglik: -5.2933e+02 - logprior: -1.0114e+00
Epoch 8/10
39/39 - 9s - loss: 530.2218 - loglik: -5.2921e+02 - logprior: -1.0131e+00
Epoch 9/10
39/39 - 9s - loss: 530.2537 - loglik: -5.2923e+02 - logprior: -1.0195e+00
Fitted a model with MAP estimate = -493.1426
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (13, 2), (20, 3), (24, 1), (25, 1), (26, 1), (27, 1), (55, 2), (56, 1), (70, 1), (79, 8), (94, 2), (112, 2), (114, 1), (123, 1), (124, 2), (125, 2), (126, 4), (127, 2), (140, 7), (146, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 534.1031 - loglik: -5.3136e+02 - logprior: -2.7455e+00
Epoch 2/2
39/39 - 12s - loss: 521.4579 - loglik: -5.2056e+02 - logprior: -8.9550e-01
Fitted a model with MAP estimate = -476.1532
expansions: [(0, 2), (32, 5)]
discards: [  0  13  17  70 100 101 102 103 104 105 167 174 175 176 177 178 179 184
 185]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 524.1428 - loglik: -5.2241e+02 - logprior: -1.7321e+00
Epoch 2/2
39/39 - 11s - loss: 519.0972 - loglik: -5.1848e+02 - logprior: -6.1587e-01
Fitted a model with MAP estimate = -475.1633
expansions: [(168, 2), (176, 6)]
discards: [  0  31  32  33  34  35  97 170 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 20s - loss: 472.9534 - loglik: -4.7177e+02 - logprior: -1.1864e+00
Epoch 2/10
51/51 - 14s - loss: 467.7218 - loglik: -4.6734e+02 - logprior: -3.8561e-01
Epoch 3/10
51/51 - 14s - loss: 466.2272 - loglik: -4.6588e+02 - logprior: -3.4687e-01
Epoch 4/10
51/51 - 14s - loss: 465.2150 - loglik: -4.6490e+02 - logprior: -3.1057e-01
Epoch 5/10
51/51 - 14s - loss: 463.6516 - loglik: -4.6336e+02 - logprior: -2.8990e-01
Epoch 6/10
51/51 - 14s - loss: 463.6588 - loglik: -4.6340e+02 - logprior: -2.5416e-01
Fitted a model with MAP estimate = -462.7731
Time for alignment: 296.3058
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 588.3954 - loglik: -5.8675e+02 - logprior: -1.6461e+00
Epoch 2/10
39/39 - 9s - loss: 540.3241 - loglik: -5.3942e+02 - logprior: -9.0256e-01
Epoch 3/10
39/39 - 9s - loss: 532.8230 - loglik: -5.3187e+02 - logprior: -9.5253e-01
Epoch 4/10
39/39 - 9s - loss: 529.9885 - loglik: -5.2903e+02 - logprior: -9.6167e-01
Epoch 5/10
39/39 - 9s - loss: 528.4395 - loglik: -5.2747e+02 - logprior: -9.6618e-01
Epoch 6/10
39/39 - 9s - loss: 527.9092 - loglik: -5.2694e+02 - logprior: -9.6530e-01
Epoch 7/10
39/39 - 9s - loss: 527.7952 - loglik: -5.2682e+02 - logprior: -9.7534e-01
Epoch 8/10
39/39 - 9s - loss: 527.8528 - loglik: -5.2688e+02 - logprior: -9.7560e-01
Fitted a model with MAP estimate = -490.1792
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (20, 2), (23, 6), (25, 1), (30, 1), (55, 1), (56, 1), (71, 3), (77, 8), (78, 2), (112, 1), (122, 3), (124, 2), (125, 4), (126, 2), (127, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 533.0482 - loglik: -5.3042e+02 - logprior: -2.6241e+00
Epoch 2/2
39/39 - 12s - loss: 522.5341 - loglik: -5.2184e+02 - logprior: -6.8934e-01
Fitted a model with MAP estimate = -477.5728
expansions: [(0, 2), (159, 1)]
discards: [  0  24  29  87  95 104 118 119 120 153 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 526.8854 - loglik: -5.2522e+02 - logprior: -1.6632e+00
Epoch 2/2
39/39 - 11s - loss: 523.1562 - loglik: -5.2265e+02 - logprior: -5.0749e-01
Fitted a model with MAP estimate = -478.0364
expansions: [(29, 1)]
discards: [ 0 26]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 473.3715 - loglik: -4.7226e+02 - logprior: -1.1155e+00
Epoch 2/10
51/51 - 14s - loss: 470.3032 - loglik: -4.6998e+02 - logprior: -3.2310e-01
Epoch 3/10
51/51 - 13s - loss: 467.4745 - loglik: -4.6718e+02 - logprior: -2.9167e-01
Epoch 4/10
51/51 - 14s - loss: 467.4131 - loglik: -4.6716e+02 - logprior: -2.5111e-01
Epoch 5/10
51/51 - 14s - loss: 464.6033 - loglik: -4.6437e+02 - logprior: -2.3385e-01
Epoch 6/10
51/51 - 14s - loss: 463.8379 - loglik: -4.6364e+02 - logprior: -1.9906e-01
Epoch 7/10
51/51 - 14s - loss: 465.1344 - loglik: -4.6497e+02 - logprior: -1.6828e-01
Fitted a model with MAP estimate = -464.1461
Time for alignment: 291.4062
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 588.4931 - loglik: -5.8684e+02 - logprior: -1.6488e+00
Epoch 2/10
39/39 - 9s - loss: 540.7758 - loglik: -5.3983e+02 - logprior: -9.4575e-01
Epoch 3/10
39/39 - 9s - loss: 534.8062 - loglik: -5.3383e+02 - logprior: -9.7257e-01
Epoch 4/10
39/39 - 9s - loss: 532.4761 - loglik: -5.3151e+02 - logprior: -9.6550e-01
Epoch 5/10
39/39 - 9s - loss: 530.9404 - loglik: -5.2997e+02 - logprior: -9.7398e-01
Epoch 6/10
39/39 - 9s - loss: 530.5031 - loglik: -5.2952e+02 - logprior: -9.8154e-01
Epoch 7/10
39/39 - 9s - loss: 530.2964 - loglik: -5.2931e+02 - logprior: -9.8541e-01
Epoch 8/10
39/39 - 9s - loss: 530.1870 - loglik: -5.2920e+02 - logprior: -9.8757e-01
Epoch 9/10
39/39 - 9s - loss: 530.3869 - loglik: -5.2940e+02 - logprior: -9.8619e-01
Fitted a model with MAP estimate = -492.8462
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 3), (23, 6), (55, 2), (56, 1), (70, 1), (78, 6), (81, 3), (93, 1), (107, 1), (111, 2), (122, 3), (125, 3), (126, 2), (128, 1), (146, 4)]
discards: [  0 135 136 137 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 537.3340 - loglik: -5.3467e+02 - logprior: -2.6670e+00
Epoch 2/2
39/39 - 11s - loss: 526.4818 - loglik: -5.2575e+02 - logprior: -7.3032e-01
Fitted a model with MAP estimate = -480.7314
expansions: [(0, 2), (39, 1), (40, 1), (174, 1), (176, 11)]
discards: [  0  13  25  26  27  70 162 165 183]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 526.0688 - loglik: -5.2433e+02 - logprior: -1.7366e+00
Epoch 2/2
39/39 - 12s - loss: 520.5426 - loglik: -5.1986e+02 - logprior: -6.8632e-01
Fitted a model with MAP estimate = -477.4578
expansions: []
discards: [  0  98  99 100 101 163 164 165 166 167 168 169 170 171 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 475.8616 - loglik: -4.7473e+02 - logprior: -1.1325e+00
Epoch 2/10
51/51 - 14s - loss: 469.3776 - loglik: -4.6902e+02 - logprior: -3.5586e-01
Epoch 3/10
51/51 - 13s - loss: 469.1989 - loglik: -4.6889e+02 - logprior: -3.0912e-01
Epoch 4/10
51/51 - 14s - loss: 467.0254 - loglik: -4.6676e+02 - logprior: -2.6091e-01
Epoch 5/10
51/51 - 14s - loss: 465.5852 - loglik: -4.6535e+02 - logprior: -2.3219e-01
Epoch 6/10
51/51 - 14s - loss: 465.8507 - loglik: -4.6561e+02 - logprior: -2.3664e-01
Fitted a model with MAP estimate = -464.6172
Time for alignment: 288.3376
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 589.0782 - loglik: -5.8743e+02 - logprior: -1.6457e+00
Epoch 2/10
39/39 - 9s - loss: 540.9592 - loglik: -5.4010e+02 - logprior: -8.5452e-01
Epoch 3/10
39/39 - 9s - loss: 533.1796 - loglik: -5.3222e+02 - logprior: -9.6015e-01
Epoch 4/10
39/39 - 9s - loss: 530.6686 - loglik: -5.2971e+02 - logprior: -9.5640e-01
Epoch 5/10
39/39 - 9s - loss: 529.1224 - loglik: -5.2817e+02 - logprior: -9.5636e-01
Epoch 6/10
39/39 - 9s - loss: 528.5295 - loglik: -5.2758e+02 - logprior: -9.4968e-01
Epoch 7/10
39/39 - 9s - loss: 528.1760 - loglik: -5.2723e+02 - logprior: -9.4594e-01
Epoch 8/10
39/39 - 9s - loss: 528.1670 - loglik: -5.2722e+02 - logprior: -9.4739e-01
Epoch 9/10
39/39 - 9s - loss: 528.0170 - loglik: -5.2707e+02 - logprior: -9.4630e-01
Epoch 10/10
39/39 - 9s - loss: 527.8567 - loglik: -5.2691e+02 - logprior: -9.4696e-01
Fitted a model with MAP estimate = -490.8775
expansions: [(4, 1), (5, 1), (10, 2), (11, 1), (20, 2), (23, 5), (24, 1), (25, 1), (26, 1), (56, 2), (57, 1), (71, 2), (78, 6), (80, 2), (83, 2), (84, 1), (109, 1), (122, 2), (125, 2), (126, 2)]
discards: [ 0 94 95 96 97]
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 536.3116 - loglik: -5.3371e+02 - logprior: -2.5974e+00
Epoch 2/2
39/39 - 11s - loss: 525.1003 - loglik: -5.2447e+02 - logprior: -6.2897e-01
Fitted a model with MAP estimate = -478.5928
expansions: [(0, 2), (137, 2)]
discards: [  0  13  25  30  31  32  72 105 110]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 525.3902 - loglik: -5.2379e+02 - logprior: -1.6035e+00
Epoch 2/2
39/39 - 11s - loss: 521.8947 - loglik: -5.2146e+02 - logprior: -4.3789e-01
Fitted a model with MAP estimate = -478.3870
expansions: [(146, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 472.5945 - loglik: -4.7160e+02 - logprior: -9.9853e-01
Epoch 2/10
51/51 - 14s - loss: 469.0726 - loglik: -4.6849e+02 - logprior: -5.7927e-01
Epoch 3/10
51/51 - 14s - loss: 465.8786 - loglik: -4.6533e+02 - logprior: -5.4442e-01
Epoch 4/10
51/51 - 14s - loss: 467.0214 - loglik: -4.6650e+02 - logprior: -5.1816e-01
Fitted a model with MAP estimate = -464.4243
Time for alignment: 265.5834
Computed alignments with likelihoods: ['-463.6736', '-462.7731', '-464.1461', '-464.6172', '-464.4243']
Best model has likelihood: -462.7731  (prior= -0.2573 )
time for generating output: 0.2596
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6636245110821382
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f03d038ce80>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f03d038c2e0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b310>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b7f0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b6d0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f03d031b070>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b7c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d031b520>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382cd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382af0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382f10>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382c70>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03826a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f03d0382a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f03d0382820>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d03821f0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f03d02fde50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f03d0382e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : True , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function matvec at 0x7f04f6ba7700> , emission_matrix_generator : <function make_default_emission_matrix at 0x7f03d0302160> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f03d0382190>
 , kernel_dim : alphabet_size , num_rate_matrices : 1 , per_matrix_rate : False
 , matrix_rate_l2 : 0.0 , shared_rate_matrix : False
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 491.5573 - loglik: -4.8563e+02 - logprior: -5.9301e+00
Epoch 2/10
14/14 - 3s - loss: 436.0869 - loglik: -4.3485e+02 - logprior: -1.2323e+00
Epoch 3/10
14/14 - 3s - loss: 401.9286 - loglik: -4.0074e+02 - logprior: -1.1900e+00
Epoch 4/10
14/14 - 3s - loss: 389.2767 - loglik: -3.8816e+02 - logprior: -1.1136e+00
Epoch 5/10
14/14 - 3s - loss: 384.5816 - loglik: -3.8347e+02 - logprior: -1.1116e+00
Epoch 6/10
14/14 - 3s - loss: 382.7251 - loglik: -3.8163e+02 - logprior: -1.0952e+00
Epoch 7/10
14/14 - 3s - loss: 381.5364 - loglik: -3.8044e+02 - logprior: -1.0933e+00
Epoch 8/10
14/14 - 3s - loss: 380.9106 - loglik: -3.7982e+02 - logprior: -1.0930e+00
Epoch 9/10
14/14 - 3s - loss: 380.9131 - loglik: -3.7982e+02 - logprior: -1.0953e+00
Fitted a model with MAP estimate = -380.5534
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (30, 1), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 2), (69, 1), (71, 1), (74, 1), (76, 1), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 381.7711 - loglik: -3.7751e+02 - logprior: -4.2568e+00
Epoch 2/2
29/29 - 5s - loss: 368.7383 - loglik: -3.6783e+02 - logprior: -9.1135e-01
Fitted a model with MAP estimate = -366.4958
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [  0  76 146 163]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 374.8952 - loglik: -3.7026e+02 - logprior: -4.6322e+00
Epoch 2/2
29/29 - 5s - loss: 367.1876 - loglik: -3.6654e+02 - logprior: -6.4509e-01
Fitted a model with MAP estimate = -365.7500
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 371.3308 - loglik: -3.6857e+02 - logprior: -2.7591e+00
Epoch 2/10
29/29 - 5s - loss: 365.4113 - loglik: -3.6502e+02 - logprior: -3.8808e-01
Epoch 3/10
29/29 - 5s - loss: 365.9254 - loglik: -3.6575e+02 - logprior: -1.7129e-01
Fitted a model with MAP estimate = -363.4847
Time for alignment: 107.3014
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.5814 - loglik: -4.8565e+02 - logprior: -5.9286e+00
Epoch 2/10
14/14 - 3s - loss: 436.6999 - loglik: -4.3546e+02 - logprior: -1.2400e+00
Epoch 3/10
14/14 - 3s - loss: 404.7730 - loglik: -4.0365e+02 - logprior: -1.1220e+00
Epoch 4/10
14/14 - 3s - loss: 394.5539 - loglik: -3.9356e+02 - logprior: -9.9425e-01
Epoch 5/10
14/14 - 3s - loss: 389.7271 - loglik: -3.8869e+02 - logprior: -1.0336e+00
Epoch 6/10
14/14 - 3s - loss: 388.4870 - loglik: -3.8743e+02 - logprior: -1.0551e+00
Epoch 7/10
14/14 - 3s - loss: 384.5305 - loglik: -3.8346e+02 - logprior: -1.0720e+00
Epoch 8/10
14/14 - 3s - loss: 385.6897 - loglik: -3.8463e+02 - logprior: -1.0629e+00
Fitted a model with MAP estimate = -384.8149
expansions: [(0, 3), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (33, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (65, 1), (70, 4), (71, 1), (96, 2), (102, 4), (103, 1), (117, 1), (120, 2), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 382.9790 - loglik: -3.7866e+02 - logprior: -4.3167e+00
Epoch 2/2
29/29 - 6s - loss: 368.1228 - loglik: -3.6711e+02 - logprior: -1.0114e+00
Fitted a model with MAP estimate = -366.1637
expansions: [(23, 1)]
discards: [  0  30 150 163 171]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 374.3336 - loglik: -3.6954e+02 - logprior: -4.7980e+00
Epoch 2/2
29/29 - 5s - loss: 368.1560 - loglik: -3.6739e+02 - logprior: -7.6977e-01
Fitted a model with MAP estimate = -365.2930
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 372.9693 - loglik: -3.6913e+02 - logprior: -3.8403e+00
Epoch 2/10
29/29 - 5s - loss: 366.5866 - loglik: -3.6638e+02 - logprior: -2.0758e-01
Epoch 3/10
29/29 - 5s - loss: 365.2060 - loglik: -3.6520e+02 - logprior: -2.5094e-03
Epoch 4/10
29/29 - 5s - loss: 363.5670 - loglik: -3.6362e+02 - logprior: 0.0535
Epoch 5/10
29/29 - 5s - loss: 361.3632 - loglik: -3.6150e+02 - logprior: 0.1371
Epoch 6/10
29/29 - 5s - loss: 362.3553 - loglik: -3.6257e+02 - logprior: 0.2172
Fitted a model with MAP estimate = -360.9140
Time for alignment: 122.0600
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 490.9900 - loglik: -4.8505e+02 - logprior: -5.9379e+00
Epoch 2/10
14/14 - 3s - loss: 435.0477 - loglik: -4.3380e+02 - logprior: -1.2448e+00
Epoch 3/10
14/14 - 3s - loss: 399.7730 - loglik: -3.9859e+02 - logprior: -1.1827e+00
Epoch 4/10
14/14 - 3s - loss: 388.3468 - loglik: -3.8728e+02 - logprior: -1.0692e+00
Epoch 5/10
14/14 - 3s - loss: 384.2079 - loglik: -3.8315e+02 - logprior: -1.0558e+00
Epoch 6/10
14/14 - 3s - loss: 382.8463 - loglik: -3.8182e+02 - logprior: -1.0214e+00
Epoch 7/10
14/14 - 3s - loss: 381.2810 - loglik: -3.8027e+02 - logprior: -1.0149e+00
Epoch 8/10
14/14 - 3s - loss: 380.8162 - loglik: -3.7982e+02 - logprior: -1.0006e+00
Epoch 9/10
14/14 - 3s - loss: 380.7074 - loglik: -3.7971e+02 - logprior: -1.0010e+00
Epoch 10/10
14/14 - 3s - loss: 380.5020 - loglik: -3.7950e+02 - logprior: -1.0003e+00
Fitted a model with MAP estimate = -380.4488
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (70, 4), (89, 1), (96, 1), (103, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (140, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 381.6098 - loglik: -3.7727e+02 - logprior: -4.3415e+00
Epoch 2/2
29/29 - 5s - loss: 366.3985 - loglik: -3.6541e+02 - logprior: -9.9276e-01
Fitted a model with MAP estimate = -364.7236
expansions: [(127, 1), (129, 1)]
discards: [  0  41 149]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 371.7487 - loglik: -3.6697e+02 - logprior: -4.7793e+00
Epoch 2/2
29/29 - 5s - loss: 365.8334 - loglik: -3.6504e+02 - logprior: -7.9469e-01
Fitted a model with MAP estimate = -362.8233
expansions: [(166, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 370.0762 - loglik: -3.6610e+02 - logprior: -3.9714e+00
Epoch 2/10
29/29 - 5s - loss: 364.8388 - loglik: -3.6467e+02 - logprior: -1.7127e-01
Epoch 3/10
29/29 - 5s - loss: 362.4010 - loglik: -3.6244e+02 - logprior: 0.0389
Epoch 4/10
29/29 - 5s - loss: 361.1592 - loglik: -3.6126e+02 - logprior: 0.1016
Epoch 5/10
29/29 - 5s - loss: 359.8347 - loglik: -3.6002e+02 - logprior: 0.1819
Epoch 6/10
29/29 - 6s - loss: 358.0579 - loglik: -3.5832e+02 - logprior: 0.2670
Epoch 7/10
29/29 - 5s - loss: 358.4532 - loglik: -3.5880e+02 - logprior: 0.3438
Fitted a model with MAP estimate = -358.1422
Time for alignment: 132.8208
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.7524 - loglik: -4.8583e+02 - logprior: -5.9268e+00
Epoch 2/10
14/14 - 3s - loss: 436.9182 - loglik: -4.3567e+02 - logprior: -1.2485e+00
Epoch 3/10
14/14 - 3s - loss: 400.9844 - loglik: -3.9980e+02 - logprior: -1.1856e+00
Epoch 4/10
14/14 - 3s - loss: 389.4196 - loglik: -3.8836e+02 - logprior: -1.0624e+00
Epoch 5/10
14/14 - 3s - loss: 385.2693 - loglik: -3.8424e+02 - logprior: -1.0276e+00
Epoch 6/10
14/14 - 3s - loss: 382.6618 - loglik: -3.8166e+02 - logprior: -1.0049e+00
Epoch 7/10
14/14 - 3s - loss: 381.6233 - loglik: -3.8061e+02 - logprior: -1.0121e+00
Epoch 8/10
14/14 - 3s - loss: 381.2830 - loglik: -3.8027e+02 - logprior: -1.0085e+00
Epoch 9/10
14/14 - 3s - loss: 380.8077 - loglik: -3.7980e+02 - logprior: -1.0115e+00
Epoch 10/10
14/14 - 3s - loss: 381.5329 - loglik: -3.8052e+02 - logprior: -1.0114e+00
Fitted a model with MAP estimate = -380.6354
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (46, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (70, 3), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 381.3908 - loglik: -3.7710e+02 - logprior: -4.2869e+00
Epoch 2/2
29/29 - 5s - loss: 366.0401 - loglik: -3.6510e+02 - logprior: -9.4497e-01
Fitted a model with MAP estimate = -365.0609
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [ 41 146 163]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 370.2249 - loglik: -3.6715e+02 - logprior: -3.0748e+00
Epoch 2/2
29/29 - 5s - loss: 365.6835 - loglik: -3.6511e+02 - logprior: -5.7517e-01
Fitted a model with MAP estimate = -364.3367
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 369.4845 - loglik: -3.6655e+02 - logprior: -2.9394e+00
Epoch 2/10
29/29 - 5s - loss: 365.2715 - loglik: -3.6488e+02 - logprior: -3.8729e-01
Epoch 3/10
29/29 - 5s - loss: 363.6501 - loglik: -3.6343e+02 - logprior: -2.2030e-01
Epoch 4/10
29/29 - 5s - loss: 361.9922 - loglik: -3.6186e+02 - logprior: -1.2817e-01
Epoch 5/10
29/29 - 5s - loss: 361.3330 - loglik: -3.6128e+02 - logprior: -5.7848e-02
Epoch 6/10
29/29 - 5s - loss: 360.7483 - loglik: -3.6077e+02 - logprior: 0.0264
Epoch 7/10
29/29 - 5s - loss: 359.4501 - loglik: -3.5956e+02 - logprior: 0.1105
Epoch 8/10
29/29 - 5s - loss: 360.3344 - loglik: -3.6054e+02 - logprior: 0.2015
Fitted a model with MAP estimate = -359.5425
Time for alignment: 134.6057
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 490.9312 - loglik: -4.8500e+02 - logprior: -5.9294e+00
Epoch 2/10
14/14 - 3s - loss: 439.1191 - loglik: -4.3788e+02 - logprior: -1.2349e+00
Epoch 3/10
14/14 - 3s - loss: 402.4169 - loglik: -4.0114e+02 - logprior: -1.2795e+00
Epoch 4/10
14/14 - 3s - loss: 391.4153 - loglik: -3.9014e+02 - logprior: -1.2707e+00
Epoch 5/10
14/14 - 3s - loss: 386.9452 - loglik: -3.8569e+02 - logprior: -1.2587e+00
Epoch 6/10
14/14 - 3s - loss: 384.1440 - loglik: -3.8293e+02 - logprior: -1.2104e+00
Epoch 7/10
14/14 - 3s - loss: 383.2240 - loglik: -3.8205e+02 - logprior: -1.1748e+00
Epoch 8/10
14/14 - 3s - loss: 382.7243 - loglik: -3.8157e+02 - logprior: -1.1500e+00
Epoch 9/10
14/14 - 3s - loss: 381.6051 - loglik: -3.8046e+02 - logprior: -1.1405e+00
Epoch 10/10
14/14 - 3s - loss: 382.8192 - loglik: -3.8168e+02 - logprior: -1.1418e+00
Fitted a model with MAP estimate = -381.7866
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 1), (74, 1), (82, 1), (102, 4), (118, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 382.3683 - loglik: -3.7810e+02 - logprior: -4.2713e+00
Epoch 2/2
29/29 - 5s - loss: 369.8713 - loglik: -3.6901e+02 - logprior: -8.6064e-01
Fitted a model with MAP estimate = -367.0857
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [ 41 146 163]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 372.2541 - loglik: -3.6925e+02 - logprior: -3.0013e+00
Epoch 2/2
29/29 - 5s - loss: 367.1465 - loglik: -3.6661e+02 - logprior: -5.3187e-01
Fitted a model with MAP estimate = -366.1773
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 371.4489 - loglik: -3.6856e+02 - logprior: -2.8906e+00
Epoch 2/10
29/29 - 5s - loss: 367.6343 - loglik: -3.6728e+02 - logprior: -3.5793e-01
Epoch 3/10
29/29 - 5s - loss: 365.0027 - loglik: -3.6480e+02 - logprior: -2.0169e-01
Epoch 4/10
29/29 - 5s - loss: 363.2270 - loglik: -3.6310e+02 - logprior: -1.2268e-01
Epoch 5/10
29/29 - 5s - loss: 363.1390 - loglik: -3.6310e+02 - logprior: -4.2887e-02
Epoch 6/10
29/29 - 5s - loss: 361.9052 - loglik: -3.6194e+02 - logprior: 0.0304
Epoch 7/10
29/29 - 5s - loss: 361.7925 - loglik: -3.6190e+02 - logprior: 0.1086
Epoch 8/10
29/29 - 5s - loss: 361.0834 - loglik: -3.6127e+02 - logprior: 0.1876
Epoch 9/10
29/29 - 5s - loss: 361.3434 - loglik: -3.6163e+02 - logprior: 0.2862
Fitted a model with MAP estimate = -360.9090
Time for alignment: 142.2582
Computed alignments with likelihoods: ['-363.4847', '-360.9140', '-358.1422', '-359.5425', '-360.9090']
Best model has likelihood: -358.1422  (prior= 0.4096 )
time for generating output: 0.2289
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.82950432430373
