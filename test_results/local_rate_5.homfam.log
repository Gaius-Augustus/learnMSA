Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b65dd6f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b65dd6340>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b65dd6280>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b65dd6eb0>, <__main__.SimpleDirichletPrior object at 0x7f3b65dd68e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 575.5208 - loglik: -5.6639e+02 - logprior: -9.1028e+00
Epoch 2/10
12/12 - 6s - loss: 508.2286 - loglik: -5.0593e+02 - logprior: -2.2482e+00
Epoch 3/10
12/12 - 6s - loss: 443.8129 - loglik: -4.4150e+02 - logprior: -2.2010e+00
Epoch 4/10
12/12 - 6s - loss: 415.0294 - loglik: -4.1168e+02 - logprior: -3.0085e+00
Epoch 5/10
12/12 - 6s - loss: 405.8714 - loglik: -4.0186e+02 - logprior: -3.3111e+00
Epoch 6/10
12/12 - 6s - loss: 403.1162 - loglik: -3.9908e+02 - logprior: -3.2039e+00
Epoch 7/10
12/12 - 6s - loss: 402.8361 - loglik: -3.9897e+02 - logprior: -3.0724e+00
Epoch 8/10
12/12 - 6s - loss: 401.4364 - loglik: -3.9765e+02 - logprior: -3.0494e+00
Epoch 9/10
12/12 - 6s - loss: 401.2445 - loglik: -3.9747e+02 - logprior: -3.0627e+00
Epoch 10/10
12/12 - 6s - loss: 398.6936 - loglik: -3.9493e+02 - logprior: -3.0838e+00
Fitted a model with MAP estimate = -399.0075
expansions: [(9, 3), (10, 3), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (66, 1), (75, 1), (80, 1), (82, 1), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (102, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (163, 1), (167, 1), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 417.5135 - loglik: -4.0646e+02 - logprior: -1.1010e+01
Epoch 2/2
12/12 - 9s - loss: 393.2572 - loglik: -3.8811e+02 - logprior: -4.8972e+00
Fitted a model with MAP estimate = -389.2983
expansions: [(0, 3), (12, 1)]
discards: [  0   9  79 104 141 175]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 397.9170 - loglik: -3.8937e+02 - logprior: -8.5152e+00
Epoch 2/2
12/12 - 9s - loss: 385.3295 - loglik: -3.8279e+02 - logprior: -2.3149e+00
Fitted a model with MAP estimate = -383.9116
expansions: []
discards: [ 0  2 61]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 398.5024 - loglik: -3.8795e+02 - logprior: -1.0520e+01
Epoch 2/10
12/12 - 8s - loss: 390.3857 - loglik: -3.8588e+02 - logprior: -4.2799e+00
Epoch 3/10
12/12 - 8s - loss: 384.9205 - loglik: -3.8157e+02 - logprior: -2.7944e+00
Epoch 4/10
12/12 - 8s - loss: 381.8487 - loglik: -3.7968e+02 - logprior: -1.3175e+00
Epoch 5/10
12/12 - 8s - loss: 380.4592 - loglik: -3.7842e+02 - logprior: -1.0841e+00
Epoch 6/10
12/12 - 8s - loss: 379.7041 - loglik: -3.7769e+02 - logprior: -1.0893e+00
Epoch 7/10
12/12 - 8s - loss: 376.3669 - loglik: -3.7439e+02 - logprior: -1.1122e+00
Epoch 8/10
12/12 - 8s - loss: 376.7218 - loglik: -3.7480e+02 - logprior: -1.1187e+00
Fitted a model with MAP estimate = -376.3231
Time for alignment: 210.5263
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 576.8411 - loglik: -5.6771e+02 - logprior: -9.1040e+00
Epoch 2/10
12/12 - 6s - loss: 506.2207 - loglik: -5.0394e+02 - logprior: -2.2352e+00
Epoch 3/10
12/12 - 6s - loss: 443.2600 - loglik: -4.4103e+02 - logprior: -2.0862e+00
Epoch 4/10
12/12 - 6s - loss: 415.0824 - loglik: -4.1187e+02 - logprior: -2.6325e+00
Epoch 5/10
12/12 - 6s - loss: 410.0194 - loglik: -4.0626e+02 - logprior: -2.7881e+00
Epoch 6/10
12/12 - 6s - loss: 405.7876 - loglik: -4.0218e+02 - logprior: -2.6446e+00
Epoch 7/10
12/12 - 6s - loss: 404.4496 - loglik: -4.0100e+02 - logprior: -2.6094e+00
Epoch 8/10
12/12 - 6s - loss: 403.3328 - loglik: -3.9986e+02 - logprior: -2.6692e+00
Epoch 9/10
12/12 - 6s - loss: 401.5700 - loglik: -3.9805e+02 - logprior: -2.7264e+00
Epoch 10/10
12/12 - 6s - loss: 402.0845 - loglik: -3.9851e+02 - logprior: -2.7982e+00
Fitted a model with MAP estimate = -400.2958
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (102, 1), (112, 2), (122, 1), (124, 1), (126, 3), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 415.8939 - loglik: -4.0481e+02 - logprior: -1.1051e+01
Epoch 2/2
12/12 - 9s - loss: 393.2767 - loglik: -3.8801e+02 - logprior: -5.0325e+00
Fitted a model with MAP estimate = -388.5361
expansions: [(0, 3)]
discards: [  0   9  42  46  62  82 107 205]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 397.1117 - loglik: -3.8848e+02 - logprior: -8.5945e+00
Epoch 2/2
12/12 - 9s - loss: 387.6021 - loglik: -3.8502e+02 - logprior: -2.3508e+00
Fitted a model with MAP estimate = -384.1850
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 397.5362 - loglik: -3.8697e+02 - logprior: -1.0530e+01
Epoch 2/10
12/12 - 8s - loss: 390.4917 - loglik: -3.8598e+02 - logprior: -4.2815e+00
Epoch 3/10
12/12 - 8s - loss: 384.5530 - loglik: -3.8119e+02 - logprior: -2.8249e+00
Epoch 4/10
12/12 - 8s - loss: 383.0167 - loglik: -3.8090e+02 - logprior: -1.3177e+00
Epoch 5/10
12/12 - 8s - loss: 379.1296 - loglik: -3.7712e+02 - logprior: -1.0825e+00
Epoch 6/10
12/12 - 8s - loss: 378.1865 - loglik: -3.7620e+02 - logprior: -1.0606e+00
Epoch 7/10
12/12 - 8s - loss: 378.7469 - loglik: -3.7677e+02 - logprior: -1.0969e+00
Fitted a model with MAP estimate = -376.5676
Time for alignment: 202.0420
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 576.5403 - loglik: -5.6743e+02 - logprior: -9.0822e+00
Epoch 2/10
12/12 - 6s - loss: 505.7836 - loglik: -5.0351e+02 - logprior: -2.2277e+00
Epoch 3/10
12/12 - 6s - loss: 442.4879 - loglik: -4.4015e+02 - logprior: -2.2311e+00
Epoch 4/10
12/12 - 6s - loss: 414.9535 - loglik: -4.1163e+02 - logprior: -2.9769e+00
Epoch 5/10
12/12 - 6s - loss: 406.8814 - loglik: -4.0307e+02 - logprior: -3.0992e+00
Epoch 6/10
12/12 - 6s - loss: 405.3267 - loglik: -4.0156e+02 - logprior: -2.9350e+00
Epoch 7/10
12/12 - 6s - loss: 403.3446 - loglik: -3.9969e+02 - logprior: -2.8591e+00
Epoch 8/10
12/12 - 6s - loss: 402.1221 - loglik: -3.9848e+02 - logprior: -2.8689e+00
Epoch 9/10
12/12 - 6s - loss: 402.4444 - loglik: -3.9880e+02 - logprior: -2.8775e+00
Fitted a model with MAP estimate = -400.7259
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (48, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (102, 1), (112, 1), (113, 2), (122, 1), (123, 2), (126, 2), (137, 1), (138, 1), (140, 2), (160, 1), (162, 2), (163, 1), (167, 1), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 415.4104 - loglik: -4.0442e+02 - logprior: -1.0954e+01
Epoch 2/2
12/12 - 9s - loss: 392.6456 - loglik: -3.8740e+02 - logprior: -5.0019e+00
Fitted a model with MAP estimate = -388.1873
expansions: [(0, 3)]
discards: [  0   9  44  61  81 106 143 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 398.2218 - loglik: -3.8961e+02 - logprior: -8.5748e+00
Epoch 2/2
12/12 - 9s - loss: 386.7443 - loglik: -3.8416e+02 - logprior: -2.3603e+00
Fitted a model with MAP estimate = -384.2205
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 397.6890 - loglik: -3.8711e+02 - logprior: -1.0542e+01
Epoch 2/10
12/12 - 8s - loss: 389.4325 - loglik: -3.8488e+02 - logprior: -4.3218e+00
Epoch 3/10
12/12 - 8s - loss: 386.0439 - loglik: -3.8266e+02 - logprior: -2.8381e+00
Epoch 4/10
12/12 - 8s - loss: 382.8376 - loglik: -3.8069e+02 - logprior: -1.3518e+00
Epoch 5/10
12/12 - 8s - loss: 379.2293 - loglik: -3.7720e+02 - logprior: -1.1201e+00
Epoch 6/10
12/12 - 8s - loss: 379.8267 - loglik: -3.7782e+02 - logprior: -1.1008e+00
Fitted a model with MAP estimate = -377.1225
Time for alignment: 187.4815
Computed alignments with likelihoods: ['-376.3231', '-376.5676', '-377.1225']
Best model has likelihood: -376.3231
time for generating output: 0.3036
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9273818270879255
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3af74eb400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af7e3c0d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e9bc70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af7e34e50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b2268f2e0>, <__main__.SimpleDirichletPrior object at 0x7f3b00619cd0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.9378 - loglik: -3.8595e+02 - logprior: -2.0987e+01
Epoch 2/10
10/10 - 2s - loss: 342.0931 - loglik: -3.3661e+02 - logprior: -5.4790e+00
Epoch 3/10
10/10 - 2s - loss: 290.8578 - loglik: -2.8758e+02 - logprior: -3.2658e+00
Epoch 4/10
10/10 - 2s - loss: 261.3235 - loglik: -2.5829e+02 - logprior: -2.8908e+00
Epoch 5/10
10/10 - 2s - loss: 248.5513 - loglik: -2.4546e+02 - logprior: -2.6889e+00
Epoch 6/10
10/10 - 2s - loss: 243.9760 - loglik: -2.4101e+02 - logprior: -2.3256e+00
Epoch 7/10
10/10 - 2s - loss: 241.6638 - loglik: -2.3899e+02 - logprior: -2.1003e+00
Epoch 8/10
10/10 - 2s - loss: 239.9840 - loglik: -2.3742e+02 - logprior: -2.1035e+00
Epoch 9/10
10/10 - 2s - loss: 239.1604 - loglik: -2.3661e+02 - logprior: -2.1334e+00
Epoch 10/10
10/10 - 2s - loss: 238.7184 - loglik: -2.3616e+02 - logprior: -2.1268e+00
Fitted a model with MAP estimate = -237.9595
expansions: [(0, 4), (13, 3), (18, 1), (26, 1), (30, 1), (31, 2), (46, 3), (47, 2), (72, 1), (76, 1), (77, 4), (78, 2), (98, 1), (104, 3), (106, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 262.8022 - loglik: -2.3493e+02 - logprior: -2.7852e+01
Epoch 2/2
10/10 - 2s - loss: 223.9117 - loglik: -2.1485e+02 - logprior: -8.9162e+00
Fitted a model with MAP estimate = -214.8198
expansions: [(135, 1)]
discards: [ 59  62 101 131]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 229.3488 - loglik: -2.0955e+02 - logprior: -1.9777e+01
Epoch 2/2
10/10 - 2s - loss: 210.6906 - loglik: -2.0534e+02 - logprior: -5.1945e+00
Fitted a model with MAP estimate = -207.6589
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 225.4799 - loglik: -2.0683e+02 - logprior: -1.8625e+01
Epoch 2/10
10/10 - 2s - loss: 209.6429 - loglik: -2.0468e+02 - logprior: -4.8095e+00
Epoch 3/10
10/10 - 2s - loss: 206.4392 - loglik: -2.0382e+02 - logprior: -2.2864e+00
Epoch 4/10
10/10 - 2s - loss: 203.6881 - loglik: -2.0177e+02 - logprior: -1.4595e+00
Epoch 5/10
10/10 - 2s - loss: 202.5001 - loglik: -2.0092e+02 - logprior: -1.0853e+00
Epoch 6/10
10/10 - 2s - loss: 202.0212 - loglik: -2.0073e+02 - logprior: -7.8548e-01
Epoch 7/10
10/10 - 2s - loss: 201.3693 - loglik: -2.0034e+02 - logprior: -5.3102e-01
Epoch 8/10
10/10 - 2s - loss: 200.6718 - loglik: -1.9983e+02 - logprior: -3.6652e-01
Epoch 9/10
10/10 - 2s - loss: 201.1452 - loglik: -2.0040e+02 - logprior: -2.8315e-01
Fitted a model with MAP estimate = -199.9905
Time for alignment: 73.2431
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 407.2125 - loglik: -3.8622e+02 - logprior: -2.0988e+01
Epoch 2/10
10/10 - 2s - loss: 342.5569 - loglik: -3.3707e+02 - logprior: -5.4872e+00
Epoch 3/10
10/10 - 2s - loss: 292.2193 - loglik: -2.8884e+02 - logprior: -3.3666e+00
Epoch 4/10
10/10 - 2s - loss: 258.2370 - loglik: -2.5489e+02 - logprior: -3.2042e+00
Epoch 5/10
10/10 - 2s - loss: 245.9734 - loglik: -2.4253e+02 - logprior: -3.1872e+00
Epoch 6/10
10/10 - 2s - loss: 241.4151 - loglik: -2.3812e+02 - logprior: -2.9149e+00
Epoch 7/10
10/10 - 2s - loss: 238.8269 - loglik: -2.3575e+02 - logprior: -2.6317e+00
Epoch 8/10
10/10 - 2s - loss: 238.3688 - loglik: -2.3538e+02 - logprior: -2.5302e+00
Epoch 9/10
10/10 - 2s - loss: 237.0776 - loglik: -2.3413e+02 - logprior: -2.5021e+00
Epoch 10/10
10/10 - 2s - loss: 236.8697 - loglik: -2.3397e+02 - logprior: -2.4759e+00
Fitted a model with MAP estimate = -236.2089
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 2), (46, 3), (47, 2), (62, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 1), (106, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 260.7043 - loglik: -2.3292e+02 - logprior: -2.7765e+01
Epoch 2/2
10/10 - 2s - loss: 221.4312 - loglik: -2.1273e+02 - logprior: -8.5902e+00
Fitted a model with MAP estimate = -213.0360
expansions: []
discards: [59 62]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.1923 - loglik: -2.0849e+02 - logprior: -1.9684e+01
Epoch 2/2
10/10 - 2s - loss: 210.9836 - loglik: -2.0571e+02 - logprior: -5.1503e+00
Fitted a model with MAP estimate = -207.6153
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 225.0220 - loglik: -2.0641e+02 - logprior: -1.8593e+01
Epoch 2/10
10/10 - 2s - loss: 210.1236 - loglik: -2.0524e+02 - logprior: -4.7682e+00
Epoch 3/10
10/10 - 2s - loss: 206.4501 - loglik: -2.0391e+02 - logprior: -2.2615e+00
Epoch 4/10
10/10 - 2s - loss: 203.8855 - loglik: -2.0201e+02 - logprior: -1.4440e+00
Epoch 5/10
10/10 - 2s - loss: 202.0257 - loglik: -2.0047e+02 - logprior: -1.0739e+00
Epoch 6/10
10/10 - 2s - loss: 201.9813 - loglik: -2.0071e+02 - logprior: -7.8922e-01
Epoch 7/10
10/10 - 2s - loss: 201.2251 - loglik: -2.0026e+02 - logprior: -5.1616e-01
Epoch 8/10
10/10 - 2s - loss: 200.7204 - loglik: -1.9996e+02 - logprior: -3.4408e-01
Epoch 9/10
10/10 - 2s - loss: 200.3568 - loglik: -1.9969e+02 - logprior: -2.4479e-01
Epoch 10/10
10/10 - 2s - loss: 200.2220 - loglik: -1.9961e+02 - logprior: -1.7887e-01
Fitted a model with MAP estimate = -199.3688
Time for alignment: 75.5319
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.9842 - loglik: -3.8599e+02 - logprior: -2.0987e+01
Epoch 2/10
10/10 - 2s - loss: 342.8402 - loglik: -3.3736e+02 - logprior: -5.4750e+00
Epoch 3/10
10/10 - 2s - loss: 290.0301 - loglik: -2.8674e+02 - logprior: -3.2822e+00
Epoch 4/10
10/10 - 2s - loss: 257.8278 - loglik: -2.5474e+02 - logprior: -2.9639e+00
Epoch 5/10
10/10 - 2s - loss: 245.4921 - loglik: -2.4239e+02 - logprior: -2.9107e+00
Epoch 6/10
10/10 - 2s - loss: 241.4639 - loglik: -2.3839e+02 - logprior: -2.7410e+00
Epoch 7/10
10/10 - 2s - loss: 238.9630 - loglik: -2.3600e+02 - logprior: -2.5005e+00
Epoch 8/10
10/10 - 2s - loss: 237.9191 - loglik: -2.3496e+02 - logprior: -2.4685e+00
Epoch 9/10
10/10 - 2s - loss: 238.1363 - loglik: -2.3517e+02 - logprior: -2.4866e+00
Fitted a model with MAP estimate = -236.7878
expansions: [(0, 4), (13, 3), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.2162 - loglik: -2.3155e+02 - logprior: -2.7649e+01
Epoch 2/2
10/10 - 2s - loss: 220.3462 - loglik: -2.1159e+02 - logprior: -8.6424e+00
Fitted a model with MAP estimate = -212.8819
expansions: []
discards: [ 59  62 136]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 227.9821 - loglik: -2.0828e+02 - logprior: -1.9687e+01
Epoch 2/2
10/10 - 2s - loss: 210.4982 - loglik: -2.0523e+02 - logprior: -5.1503e+00
Fitted a model with MAP estimate = -207.5389
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 225.0171 - loglik: -2.0640e+02 - logprior: -1.8592e+01
Epoch 2/10
10/10 - 2s - loss: 209.5149 - loglik: -2.0460e+02 - logprior: -4.7810e+00
Epoch 3/10
10/10 - 2s - loss: 206.7411 - loglik: -2.0417e+02 - logprior: -2.2562e+00
Epoch 4/10
10/10 - 2s - loss: 204.0117 - loglik: -2.0212e+02 - logprior: -1.4484e+00
Epoch 5/10
10/10 - 2s - loss: 202.3864 - loglik: -2.0083e+02 - logprior: -1.0781e+00
Epoch 6/10
10/10 - 2s - loss: 201.4897 - loglik: -2.0022e+02 - logprior: -7.8783e-01
Epoch 7/10
10/10 - 2s - loss: 201.6224 - loglik: -2.0064e+02 - logprior: -5.2260e-01
Fitted a model with MAP estimate = -200.4749
Time for alignment: 65.9125
Computed alignments with likelihoods: ['-199.9905', '-199.3688', '-200.4749']
Best model has likelihood: -199.3688
time for generating output: 0.1910
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9174061433447099
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b5d788a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b3bcf6730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3bcf6d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b3328ec70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b445b4100>, <__main__.SimpleDirichletPrior object at 0x7f3b3346b880>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 318.7690 - loglik: -2.4848e+02 - logprior: -7.0280e+01
Epoch 2/10
10/10 - 2s - loss: 237.9238 - loglik: -2.1889e+02 - logprior: -1.9012e+01
Epoch 3/10
10/10 - 2s - loss: 204.6811 - loglik: -1.9543e+02 - logprior: -9.1692e+00
Epoch 4/10
10/10 - 2s - loss: 188.2549 - loglik: -1.8252e+02 - logprior: -5.6989e+00
Epoch 5/10
10/10 - 2s - loss: 180.9789 - loglik: -1.7718e+02 - logprior: -3.7991e+00
Epoch 6/10
10/10 - 2s - loss: 178.1261 - loglik: -1.7540e+02 - logprior: -2.7114e+00
Epoch 7/10
10/10 - 2s - loss: 176.5442 - loglik: -1.7442e+02 - logprior: -2.1105e+00
Epoch 8/10
10/10 - 2s - loss: 175.7417 - loglik: -1.7390e+02 - logprior: -1.8220e+00
Epoch 9/10
10/10 - 2s - loss: 175.1832 - loglik: -1.7341e+02 - logprior: -1.6601e+00
Epoch 10/10
10/10 - 2s - loss: 174.8422 - loglik: -1.7310e+02 - logprior: -1.5367e+00
Fitted a model with MAP estimate = -174.4704
expansions: [(9, 3), (14, 1), (15, 1), (25, 1), (26, 2), (27, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.5063 - loglik: -1.7253e+02 - logprior: -7.8956e+01
Epoch 2/2
10/10 - 2s - loss: 197.5018 - loglik: -1.6460e+02 - logprior: -3.2814e+01
Fitted a model with MAP estimate = -188.3342
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 237.2302 - loglik: -1.6200e+02 - logprior: -7.5202e+01
Epoch 2/10
10/10 - 2s - loss: 183.2990 - loglik: -1.6016e+02 - logprior: -2.3020e+01
Epoch 3/10
10/10 - 2s - loss: 168.8875 - loglik: -1.5994e+02 - logprior: -8.8333e+00
Epoch 4/10
10/10 - 2s - loss: 164.1649 - loglik: -1.6019e+02 - logprior: -3.9155e+00
Epoch 5/10
10/10 - 2s - loss: 162.0082 - loglik: -1.6042e+02 - logprior: -1.5571e+00
Epoch 6/10
10/10 - 2s - loss: 160.8246 - loglik: -1.6055e+02 - logprior: -2.4414e-01
Epoch 7/10
10/10 - 2s - loss: 159.9583 - loglik: -1.6050e+02 - logprior: 0.5528
Epoch 8/10
10/10 - 2s - loss: 159.2127 - loglik: -1.6010e+02 - logprior: 0.9500
Epoch 9/10
10/10 - 2s - loss: 158.7019 - loglik: -1.5978e+02 - logprior: 1.2784
Epoch 10/10
10/10 - 2s - loss: 158.3493 - loglik: -1.5972e+02 - logprior: 1.6077
Fitted a model with MAP estimate = -157.9247
Time for alignment: 61.2217
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 318.7662 - loglik: -2.4848e+02 - logprior: -7.0278e+01
Epoch 2/10
10/10 - 2s - loss: 237.9492 - loglik: -2.1892e+02 - logprior: -1.9016e+01
Epoch 3/10
10/10 - 2s - loss: 205.0600 - loglik: -1.9581e+02 - logprior: -9.1586e+00
Epoch 4/10
10/10 - 2s - loss: 191.6352 - loglik: -1.8601e+02 - logprior: -5.5768e+00
Epoch 5/10
10/10 - 2s - loss: 186.6268 - loglik: -1.8312e+02 - logprior: -3.5038e+00
Epoch 6/10
10/10 - 2s - loss: 183.1962 - loglik: -1.8076e+02 - logprior: -2.4310e+00
Epoch 7/10
10/10 - 2s - loss: 180.6308 - loglik: -1.7875e+02 - logprior: -1.8703e+00
Epoch 8/10
10/10 - 2s - loss: 178.9358 - loglik: -1.7739e+02 - logprior: -1.5394e+00
Epoch 9/10
10/10 - 2s - loss: 177.6157 - loglik: -1.7614e+02 - logprior: -1.4040e+00
Epoch 10/10
10/10 - 2s - loss: 177.0309 - loglik: -1.7552e+02 - logprior: -1.3196e+00
Fitted a model with MAP estimate = -176.5590
expansions: [(9, 3), (14, 2), (26, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.7940 - loglik: -1.7575e+02 - logprior: -7.9024e+01
Epoch 2/2
10/10 - 2s - loss: 201.2774 - loglik: -1.6836e+02 - logprior: -3.2855e+01
Fitted a model with MAP estimate = -191.9146
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 240.2124 - loglik: -1.6538e+02 - logprior: -7.4811e+01
Epoch 2/10
10/10 - 2s - loss: 186.0075 - loglik: -1.6342e+02 - logprior: -2.2477e+01
Epoch 3/10
10/10 - 2s - loss: 171.8412 - loglik: -1.6302e+02 - logprior: -8.7262e+00
Epoch 4/10
10/10 - 2s - loss: 167.1247 - loglik: -1.6317e+02 - logprior: -3.9157e+00
Epoch 5/10
10/10 - 2s - loss: 164.8835 - loglik: -1.6326e+02 - logprior: -1.6009e+00
Epoch 6/10
10/10 - 2s - loss: 163.4220 - loglik: -1.6299e+02 - logprior: -3.8533e-01
Epoch 7/10
10/10 - 2s - loss: 162.4258 - loglik: -1.6256e+02 - logprior: 0.2891
Epoch 8/10
10/10 - 2s - loss: 161.8643 - loglik: -1.6240e+02 - logprior: 0.7675
Epoch 9/10
10/10 - 2s - loss: 161.4677 - loglik: -1.6242e+02 - logprior: 1.1716
Epoch 10/10
10/10 - 2s - loss: 161.1475 - loglik: -1.6246e+02 - logprior: 1.5351
Fitted a model with MAP estimate = -160.7358
Time for alignment: 61.2446
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 318.7570 - loglik: -2.4847e+02 - logprior: -7.0279e+01
Epoch 2/10
10/10 - 2s - loss: 237.8040 - loglik: -2.1877e+02 - logprior: -1.9015e+01
Epoch 3/10
10/10 - 2s - loss: 205.0990 - loglik: -1.9585e+02 - logprior: -9.1590e+00
Epoch 4/10
10/10 - 2s - loss: 189.8618 - loglik: -1.8419e+02 - logprior: -5.6373e+00
Epoch 5/10
10/10 - 2s - loss: 182.1486 - loglik: -1.7839e+02 - logprior: -3.7628e+00
Epoch 6/10
10/10 - 2s - loss: 178.5777 - loglik: -1.7578e+02 - logprior: -2.7961e+00
Epoch 7/10
10/10 - 2s - loss: 176.9383 - loglik: -1.7473e+02 - logprior: -2.1896e+00
Epoch 8/10
10/10 - 2s - loss: 176.0006 - loglik: -1.7404e+02 - logprior: -1.8931e+00
Epoch 9/10
10/10 - 2s - loss: 175.5172 - loglik: -1.7359e+02 - logprior: -1.7002e+00
Epoch 10/10
10/10 - 2s - loss: 175.2563 - loglik: -1.7346e+02 - logprior: -1.5355e+00
Fitted a model with MAP estimate = -174.8960
expansions: [(9, 3), (14, 1), (15, 1), (26, 2), (27, 3), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.2486 - loglik: -1.7421e+02 - logprior: -7.9021e+01
Epoch 2/2
10/10 - 2s - loss: 199.5790 - loglik: -1.6647e+02 - logprior: -3.3053e+01
Fitted a model with MAP estimate = -190.4029
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 239.1323 - loglik: -1.6384e+02 - logprior: -7.5273e+01
Epoch 2/10
10/10 - 2s - loss: 184.7848 - loglik: -1.6163e+02 - logprior: -2.3046e+01
Epoch 3/10
10/10 - 2s - loss: 170.2587 - loglik: -1.6114e+02 - logprior: -9.0152e+00
Epoch 4/10
10/10 - 2s - loss: 165.4751 - loglik: -1.6129e+02 - logprior: -4.1388e+00
Epoch 5/10
10/10 - 2s - loss: 163.0643 - loglik: -1.6125e+02 - logprior: -1.7883e+00
Epoch 6/10
10/10 - 2s - loss: 161.6753 - loglik: -1.6115e+02 - logprior: -4.9608e-01
Epoch 7/10
10/10 - 2s - loss: 160.7472 - loglik: -1.6089e+02 - logprior: 0.1933
Epoch 8/10
10/10 - 2s - loss: 159.9420 - loglik: -1.6035e+02 - logprior: 0.5790
Epoch 9/10
10/10 - 2s - loss: 159.5300 - loglik: -1.6019e+02 - logprior: 0.9514
Epoch 10/10
10/10 - 2s - loss: 159.1745 - loglik: -1.6020e+02 - logprior: 1.3060
Fitted a model with MAP estimate = -158.7124
Time for alignment: 61.2859
Computed alignments with likelihoods: ['-157.9247', '-160.7358', '-158.7124']
Best model has likelihood: -157.9247
time for generating output: 0.2629
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3833939794905723
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b08c5eaf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af7578eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b228a0850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af8041340>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b8613e280>, <__main__.SimpleDirichletPrior object at 0x7f3b002630a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 779.3104 - loglik: -7.6409e+02 - logprior: -1.5192e+01
Epoch 2/10
10/10 - 11s - loss: 735.2552 - loglik: -7.3182e+02 - logprior: -3.2230e+00
Epoch 3/10
10/10 - 11s - loss: 699.8790 - loglik: -6.9810e+02 - logprior: -1.1757e+00
Epoch 4/10
10/10 - 10s - loss: 669.2025 - loglik: -6.6740e+02 - logprior: -9.5492e-01
Epoch 5/10
10/10 - 11s - loss: 655.6173 - loglik: -6.5327e+02 - logprior: -1.5925e+00
Epoch 6/10
10/10 - 11s - loss: 648.2577 - loglik: -6.4529e+02 - logprior: -2.2084e+00
Epoch 7/10
10/10 - 11s - loss: 641.8793 - loglik: -6.3863e+02 - logprior: -2.3197e+00
Epoch 8/10
10/10 - 11s - loss: 640.3580 - loglik: -6.3705e+02 - logprior: -2.2338e+00
Epoch 9/10
10/10 - 11s - loss: 638.4398 - loglik: -6.3510e+02 - logprior: -2.2333e+00
Epoch 10/10
10/10 - 11s - loss: 635.5096 - loglik: -6.3217e+02 - logprior: -2.2659e+00
Fitted a model with MAP estimate = -634.9875
expansions: [(0, 3), (24, 3), (44, 1), (49, 2), (50, 1), (51, 1), (82, 9), (92, 1), (94, 1), (100, 1), (111, 5), (116, 1), (117, 1), (147, 1), (153, 1), (155, 1), (171, 11), (199, 5), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 20s - loss: 695.7149 - loglik: -6.8129e+02 - logprior: -1.4314e+01
Epoch 2/2
20/20 - 16s - loss: 651.2438 - loglik: -6.4785e+02 - logprior: -2.8936e+00
Fitted a model with MAP estimate = -639.8920
expansions: [(61, 1), (268, 1)]
discards: [  0   2   3  27  56  57  59 136 137 138 177 204 205 206 207 208 214]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 18s - loss: 659.6740 - loglik: -6.4725e+02 - logprior: -1.2326e+01
Epoch 2/2
20/20 - 15s - loss: 641.0741 - loglik: -6.3793e+02 - logprior: -2.6828e+00
Fitted a model with MAP estimate = -634.0928
expansions: [(0, 3), (18, 3), (94, 3), (98, 2), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 19s - loss: 650.6655 - loglik: -6.4196e+02 - logprior: -8.6269e+00
Epoch 2/10
20/20 - 15s - loss: 634.5604 - loglik: -6.3321e+02 - logprior: -8.9927e-01
Epoch 3/10
20/20 - 15s - loss: 626.4293 - loglik: -6.2496e+02 - logprior: -5.4196e-01
Epoch 4/10
20/20 - 15s - loss: 625.3660 - loglik: -6.2385e+02 - logprior: -3.3272e-01
Epoch 5/10
20/20 - 15s - loss: 619.3859 - loglik: -6.1790e+02 - logprior: -2.3509e-01
Epoch 6/10
20/20 - 15s - loss: 620.6630 - loglik: -6.1930e+02 - logprior: -1.2663e-01
Fitted a model with MAP estimate = -616.2060
Time for alignment: 311.1983
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 779.1764 - loglik: -7.6396e+02 - logprior: -1.5186e+01
Epoch 2/10
10/10 - 11s - loss: 735.5324 - loglik: -7.3208e+02 - logprior: -3.2319e+00
Epoch 3/10
10/10 - 11s - loss: 697.5240 - loglik: -6.9574e+02 - logprior: -1.1800e+00
Epoch 4/10
10/10 - 11s - loss: 670.3906 - loglik: -6.6859e+02 - logprior: -9.5778e-01
Epoch 5/10
10/10 - 11s - loss: 654.7227 - loglik: -6.5230e+02 - logprior: -1.4471e+00
Epoch 6/10
10/10 - 11s - loss: 646.6458 - loglik: -6.4332e+02 - logprior: -1.9259e+00
Epoch 7/10
10/10 - 11s - loss: 640.5443 - loglik: -6.3692e+02 - logprior: -1.9483e+00
Epoch 8/10
10/10 - 11s - loss: 640.4550 - loglik: -6.3687e+02 - logprior: -1.9670e+00
Epoch 9/10
10/10 - 11s - loss: 635.8825 - loglik: -6.3231e+02 - logprior: -2.1029e+00
Epoch 10/10
10/10 - 11s - loss: 635.6407 - loglik: -6.3203e+02 - logprior: -2.2805e+00
Fitted a model with MAP estimate = -632.5680
expansions: [(0, 4), (43, 2), (49, 2), (51, 1), (53, 2), (81, 8), (83, 5), (91, 1), (93, 1), (99, 1), (110, 5), (115, 2), (117, 1), (150, 1), (153, 1), (171, 12), (176, 1), (192, 2), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 281 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 19s - loss: 692.9304 - loglik: -6.7882e+02 - logprior: -1.4008e+01
Epoch 2/2
20/20 - 16s - loss: 646.7049 - loglik: -6.4352e+02 - logprior: -2.7112e+00
Fitted a model with MAP estimate = -637.5817
expansions: [(0, 4), (253, 1), (275, 1)]
discards: [  2   3   4   5   6  56  60 104 139 140 141 147 208 209 244]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 19s - loss: 658.8297 - loglik: -6.4494e+02 - logprior: -1.3800e+01
Epoch 2/2
20/20 - 16s - loss: 638.4219 - loglik: -6.3574e+02 - logprior: -2.2563e+00
Fitted a model with MAP estimate = -633.2656
expansions: [(24, 5)]
discards: [  0   5 101 200 201 202 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 654.6556 - loglik: -6.4230e+02 - logprior: -1.2275e+01
Epoch 2/10
20/20 - 15s - loss: 637.3351 - loglik: -6.3318e+02 - logprior: -3.6652e+00
Epoch 3/10
20/20 - 15s - loss: 630.8860 - loglik: -6.2927e+02 - logprior: -5.1409e-01
Epoch 4/10
20/20 - 15s - loss: 625.6614 - loglik: -6.2406e+02 - logprior: -1.8495e-01
Epoch 5/10
20/20 - 15s - loss: 619.8114 - loglik: -6.1823e+02 - logprior: -1.2122e-01
Epoch 6/10
20/20 - 15s - loss: 618.5822 - loglik: -6.1708e+02 - logprior: -8.4757e-02
Epoch 7/10
20/20 - 15s - loss: 614.7867 - loglik: -6.1348e+02 - logprior: 0.0399
Epoch 8/10
20/20 - 15s - loss: 614.6163 - loglik: -6.1348e+02 - logprior: 0.1744
Epoch 9/10
20/20 - 15s - loss: 613.2086 - loglik: -6.1217e+02 - logprior: 0.2860
Epoch 10/10
20/20 - 15s - loss: 613.1643 - loglik: -6.1219e+02 - logprior: 0.3804
Fitted a model with MAP estimate = -609.9637
Time for alignment: 372.7152
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 778.1566 - loglik: -7.6294e+02 - logprior: -1.5188e+01
Epoch 2/10
10/10 - 11s - loss: 736.5835 - loglik: -7.3314e+02 - logprior: -3.2296e+00
Epoch 3/10
10/10 - 11s - loss: 700.1242 - loglik: -6.9836e+02 - logprior: -1.1616e+00
Epoch 4/10
10/10 - 11s - loss: 666.6198 - loglik: -6.6487e+02 - logprior: -9.1951e-01
Epoch 5/10
10/10 - 10s - loss: 656.3318 - loglik: -6.5416e+02 - logprior: -1.4496e+00
Epoch 6/10
10/10 - 11s - loss: 648.6149 - loglik: -6.4582e+02 - logprior: -2.1177e+00
Epoch 7/10
10/10 - 11s - loss: 643.7655 - loglik: -6.4060e+02 - logprior: -2.3271e+00
Epoch 8/10
10/10 - 11s - loss: 640.9680 - loglik: -6.3766e+02 - logprior: -2.2984e+00
Epoch 9/10
10/10 - 11s - loss: 638.4501 - loglik: -6.3506e+02 - logprior: -2.2947e+00
Epoch 10/10
10/10 - 11s - loss: 636.2007 - loglik: -6.3277e+02 - logprior: -2.3184e+00
Fitted a model with MAP estimate = -635.1646
expansions: [(0, 4), (50, 2), (52, 1), (54, 2), (82, 4), (92, 1), (94, 1), (100, 1), (111, 5), (112, 2), (115, 2), (117, 1), (154, 1), (155, 2), (171, 11), (176, 1), (193, 1), (201, 1), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 19s - loss: 693.5930 - loglik: -6.7946e+02 - logprior: -1.4026e+01
Epoch 2/2
20/20 - 15s - loss: 651.3707 - loglik: -6.4824e+02 - logprior: -2.5475e+00
Fitted a model with MAP estimate = -642.3078
expansions: [(14, 2), (262, 1)]
discards: [  2   3   4   5   6  55  59 128 129 130 131 132 133 138 182]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 18s - loss: 657.7443 - loglik: -6.4875e+02 - logprior: -8.8986e+00
Epoch 2/2
20/20 - 15s - loss: 640.8182 - loglik: -6.3914e+02 - logprior: -1.2446e+00
Fitted a model with MAP estimate = -637.0525
expansions: [(0, 4), (46, 1), (89, 4), (92, 3), (227, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 656.8522 - loglik: -6.4253e+02 - logprior: -1.4241e+01
Epoch 2/10
20/20 - 15s - loss: 637.6510 - loglik: -6.3483e+02 - logprior: -2.3763e+00
Epoch 3/10
20/20 - 15s - loss: 633.2883 - loglik: -6.3149e+02 - logprior: -7.7106e-01
Epoch 4/10
20/20 - 15s - loss: 624.6056 - loglik: -6.2294e+02 - logprior: -3.6489e-01
Epoch 5/10
20/20 - 15s - loss: 621.9769 - loglik: -6.2034e+02 - logprior: -2.7615e-01
Epoch 6/10
20/20 - 15s - loss: 619.8197 - loglik: -6.1829e+02 - logprior: -2.0842e-01
Epoch 7/10
20/20 - 15s - loss: 617.4269 - loglik: -6.1603e+02 - logprior: -1.0833e-01
Epoch 8/10
20/20 - 15s - loss: 616.1927 - loglik: -6.1491e+02 - logprior: -2.1730e-02
Epoch 9/10
20/20 - 15s - loss: 615.9333 - loglik: -6.1476e+02 - logprior: 0.0952
Epoch 10/10
20/20 - 15s - loss: 615.2991 - loglik: -6.1424e+02 - logprior: 0.2227
Fitted a model with MAP estimate = -611.9021
Time for alignment: 369.5394
Computed alignments with likelihoods: ['-616.2060', '-609.9637', '-611.9021']
Best model has likelihood: -609.9637
time for generating output: 0.3840
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6487613441255825
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b2aec9790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b445b97f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af71ce0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b44664250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b448aeb20>, <__main__.SimpleDirichletPrior object at 0x7f3b19b53d90>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8656 - loglik: -1.3156e+02 - logprior: -3.2988e+00
Epoch 2/10
19/19 - 2s - loss: 113.6221 - loglik: -1.1215e+02 - logprior: -1.4188e+00
Epoch 3/10
19/19 - 2s - loss: 106.2977 - loglik: -1.0443e+02 - logprior: -1.5821e+00
Epoch 4/10
19/19 - 2s - loss: 103.9666 - loglik: -1.0213e+02 - logprior: -1.5568e+00
Epoch 5/10
19/19 - 2s - loss: 103.0731 - loglik: -1.0130e+02 - logprior: -1.5379e+00
Epoch 6/10
19/19 - 2s - loss: 102.5324 - loglik: -1.0079e+02 - logprior: -1.5449e+00
Epoch 7/10
19/19 - 1s - loss: 101.6377 - loglik: -9.9920e+01 - logprior: -1.5492e+00
Epoch 8/10
19/19 - 2s - loss: 101.6440 - loglik: -9.9934e+01 - logprior: -1.5471e+00
Fitted a model with MAP estimate = -100.0020
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 1), (29, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.2643 - loglik: -9.9906e+01 - logprior: -4.3071e+00
Epoch 2/2
19/19 - 2s - loss: 95.4816 - loglik: -9.3176e+01 - logprior: -2.1794e+00
Fitted a model with MAP estimate = -92.4482
expansions: [(0, 1)]
discards: [ 0  9 30]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.7769 - loglik: -9.3572e+01 - logprior: -3.1720e+00
Epoch 2/2
19/19 - 2s - loss: 93.4076 - loglik: -9.1753e+01 - logprior: -1.5486e+00
Fitted a model with MAP estimate = -91.4359
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.6061 - loglik: -9.1350e+01 - logprior: -3.2263e+00
Epoch 2/10
19/19 - 2s - loss: 91.8435 - loglik: -9.0208e+01 - logprior: -1.5356e+00
Epoch 3/10
19/19 - 2s - loss: 90.9162 - loglik: -8.9269e+01 - logprior: -1.4498e+00
Epoch 4/10
19/19 - 2s - loss: 90.7158 - loglik: -8.9029e+01 - logprior: -1.4214e+00
Epoch 5/10
19/19 - 2s - loss: 90.0434 - loglik: -8.8385e+01 - logprior: -1.4055e+00
Epoch 6/10
19/19 - 2s - loss: 89.9029 - loglik: -8.8290e+01 - logprior: -1.3906e+00
Epoch 7/10
19/19 - 2s - loss: 90.1746 - loglik: -8.8606e+01 - logprior: -1.3731e+00
Fitted a model with MAP estimate = -89.6467
Time for alignment: 54.6147
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.8414 - loglik: -1.3153e+02 - logprior: -3.2984e+00
Epoch 2/10
19/19 - 2s - loss: 114.4982 - loglik: -1.1303e+02 - logprior: -1.4205e+00
Epoch 3/10
19/19 - 2s - loss: 107.7373 - loglik: -1.0593e+02 - logprior: -1.5394e+00
Epoch 4/10
19/19 - 2s - loss: 105.8712 - loglik: -1.0414e+02 - logprior: -1.4723e+00
Epoch 5/10
19/19 - 2s - loss: 105.2800 - loglik: -1.0361e+02 - logprior: -1.4463e+00
Epoch 6/10
19/19 - 2s - loss: 104.7626 - loglik: -1.0313e+02 - logprior: -1.4521e+00
Epoch 7/10
19/19 - 2s - loss: 104.6831 - loglik: -1.0308e+02 - logprior: -1.4384e+00
Epoch 8/10
19/19 - 2s - loss: 104.4842 - loglik: -1.0290e+02 - logprior: -1.4253e+00
Epoch 9/10
19/19 - 2s - loss: 104.3476 - loglik: -1.0278e+02 - logprior: -1.4132e+00
Epoch 10/10
19/19 - 2s - loss: 104.2401 - loglik: -1.0268e+02 - logprior: -1.4049e+00
Fitted a model with MAP estimate = -102.6822
expansions: [(10, 2), (12, 3), (13, 3), (23, 2), (28, 3), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1703 - loglik: -1.0377e+02 - logprior: -4.3451e+00
Epoch 2/2
19/19 - 2s - loss: 98.4830 - loglik: -9.6006e+01 - logprior: -2.3440e+00
Fitted a model with MAP estimate = -94.8815
expansions: [(0, 1)]
discards: [ 0  9 14 31 39]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.0817 - loglik: -9.5821e+01 - logprior: -3.2098e+00
Epoch 2/2
19/19 - 2s - loss: 94.0273 - loglik: -9.2349e+01 - logprior: -1.5433e+00
Fitted a model with MAP estimate = -91.5945
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.6129 - loglik: -9.1346e+01 - logprior: -3.2331e+00
Epoch 2/10
19/19 - 2s - loss: 91.9112 - loglik: -9.0273e+01 - logprior: -1.5386e+00
Epoch 3/10
19/19 - 1s - loss: 90.7571 - loglik: -8.9120e+01 - logprior: -1.4447e+00
Epoch 4/10
19/19 - 2s - loss: 90.7180 - loglik: -8.9035e+01 - logprior: -1.4218e+00
Epoch 5/10
19/19 - 2s - loss: 90.1509 - loglik: -8.8496e+01 - logprior: -1.4014e+00
Epoch 6/10
19/19 - 2s - loss: 89.8928 - loglik: -8.8283e+01 - logprior: -1.3852e+00
Epoch 7/10
19/19 - 2s - loss: 90.1271 - loglik: -8.8557e+01 - logprior: -1.3734e+00
Fitted a model with MAP estimate = -89.6439
Time for alignment: 56.7830
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8566 - loglik: -1.3155e+02 - logprior: -3.2974e+00
Epoch 2/10
19/19 - 2s - loss: 114.5338 - loglik: -1.1307e+02 - logprior: -1.4158e+00
Epoch 3/10
19/19 - 2s - loss: 106.9989 - loglik: -1.0513e+02 - logprior: -1.5704e+00
Epoch 4/10
19/19 - 1s - loss: 104.6078 - loglik: -1.0282e+02 - logprior: -1.5526e+00
Epoch 5/10
19/19 - 2s - loss: 103.4886 - loglik: -1.0177e+02 - logprior: -1.5447e+00
Epoch 6/10
19/19 - 2s - loss: 102.4916 - loglik: -1.0080e+02 - logprior: -1.5346e+00
Epoch 7/10
19/19 - 2s - loss: 102.0999 - loglik: -1.0043e+02 - logprior: -1.5365e+00
Epoch 8/10
19/19 - 2s - loss: 102.1339 - loglik: -1.0046e+02 - logprior: -1.5387e+00
Fitted a model with MAP estimate = -100.4566
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (28, 2), (29, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.7382 - loglik: -1.0040e+02 - logprior: -4.2905e+00
Epoch 2/2
19/19 - 2s - loss: 95.6468 - loglik: -9.3334e+01 - logprior: -2.1936e+00
Fitted a model with MAP estimate = -92.6765
expansions: [(0, 1)]
discards: [ 0  9 30]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.6101 - loglik: -9.3460e+01 - logprior: -3.1212e+00
Epoch 2/2
19/19 - 2s - loss: 93.4241 - loglik: -9.1797e+01 - logprior: -1.5236e+00
Fitted a model with MAP estimate = -91.4316
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.3889 - loglik: -9.1166e+01 - logprior: -3.1945e+00
Epoch 2/10
19/19 - 2s - loss: 91.8635 - loglik: -9.0242e+01 - logprior: -1.5176e+00
Epoch 3/10
19/19 - 2s - loss: 90.8819 - loglik: -8.9234e+01 - logprior: -1.4417e+00
Epoch 4/10
19/19 - 2s - loss: 90.4886 - loglik: -8.8796e+01 - logprior: -1.4227e+00
Epoch 5/10
19/19 - 2s - loss: 90.0755 - loglik: -8.8412e+01 - logprior: -1.4052e+00
Epoch 6/10
19/19 - 2s - loss: 89.5728 - loglik: -8.7952e+01 - logprior: -1.3901e+00
Epoch 7/10
19/19 - 2s - loss: 89.6897 - loglik: -8.8111e+01 - logprior: -1.3764e+00
Fitted a model with MAP estimate = -89.4730
Time for alignment: 53.3896
Computed alignments with likelihoods: ['-89.6467', '-89.6439', '-89.4730']
Best model has likelihood: -89.4730
time for generating output: 0.1169
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9599198396793587
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b11752910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b2252b910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b2252b8b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b08ff1910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b1175f460>, <__main__.SimpleDirichletPrior object at 0x7f3b088bbd60>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 13s - loss: 668.6575 - loglik: -6.4123e+02 - logprior: -2.7400e+01
Epoch 2/10
10/10 - 10s - loss: 613.8176 - loglik: -6.0764e+02 - logprior: -5.9781e+00
Epoch 3/10
10/10 - 10s - loss: 560.8970 - loglik: -5.5853e+02 - logprior: -2.1096e+00
Epoch 4/10
10/10 - 10s - loss: 520.4124 - loglik: -5.1832e+02 - logprior: -1.9359e+00
Epoch 5/10
10/10 - 10s - loss: 494.8013 - loglik: -4.9195e+02 - logprior: -2.5252e+00
Epoch 6/10
10/10 - 10s - loss: 480.9104 - loglik: -4.7740e+02 - logprior: -2.8756e+00
Epoch 7/10
10/10 - 10s - loss: 476.2025 - loglik: -4.7227e+02 - logprior: -3.2087e+00
Epoch 8/10
10/10 - 10s - loss: 469.4974 - loglik: -4.6542e+02 - logprior: -3.4400e+00
Epoch 9/10
10/10 - 10s - loss: 471.8366 - loglik: -4.6794e+02 - logprior: -3.3726e+00
Fitted a model with MAP estimate = -468.8331
expansions: [(25, 4), (39, 1), (42, 1), (52, 2), (53, 1), (55, 1), (68, 3), (69, 1), (70, 4), (71, 3), (72, 2), (73, 1), (74, 1), (76, 2), (85, 1), (101, 1), (102, 2), (103, 3), (133, 4), (134, 4), (135, 3), (137, 1), (139, 1), (156, 1), (157, 4), (158, 3), (176, 4), (198, 1)]
discards: [  1   2 203 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 484.4391 - loglik: -4.6443e+02 - logprior: -1.9963e+01
Epoch 2/2
15/15 - 11s - loss: 440.5143 - loglik: -4.3740e+02 - logprior: -2.8869e+00
Fitted a model with MAP estimate = -435.3598
expansions: [(204, 1), (226, 7), (230, 1), (261, 2)]
discards: [ 23 129 175]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 455.2418 - loglik: -4.3660e+02 - logprior: -1.8584e+01
Epoch 2/2
15/15 - 11s - loss: 428.9724 - loglik: -4.2678e+02 - logprior: -1.9074e+00
Fitted a model with MAP estimate = -427.3298
expansions: [(0, 3), (25, 1), (78, 1)]
discards: [163 231 268]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 15s - loss: 457.6543 - loglik: -4.3052e+02 - logprior: -2.7081e+01
Epoch 2/10
15/15 - 12s - loss: 431.0540 - loglik: -4.2627e+02 - logprior: -4.5067e+00
Epoch 3/10
15/15 - 11s - loss: 425.0177 - loglik: -4.2516e+02 - logprior: 0.6645
Epoch 4/10
15/15 - 12s - loss: 423.8710 - loglik: -4.2556e+02 - logprior: 2.2733
Epoch 5/10
15/15 - 12s - loss: 419.6996 - loglik: -4.2199e+02 - logprior: 2.8208
Epoch 6/10
15/15 - 12s - loss: 421.2812 - loglik: -4.2407e+02 - logprior: 3.2551
Fitted a model with MAP estimate = -419.0933
Time for alignment: 238.1796
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 670.9933 - loglik: -6.4356e+02 - logprior: -2.7409e+01
Epoch 2/10
10/10 - 10s - loss: 611.0431 - loglik: -6.0485e+02 - logprior: -6.0011e+00
Epoch 3/10
10/10 - 10s - loss: 561.2286 - loglik: -5.5870e+02 - logprior: -2.2796e+00
Epoch 4/10
10/10 - 10s - loss: 518.8023 - loglik: -5.1644e+02 - logprior: -2.2192e+00
Epoch 5/10
10/10 - 10s - loss: 489.2553 - loglik: -4.8595e+02 - logprior: -3.0215e+00
Epoch 6/10
10/10 - 10s - loss: 476.3056 - loglik: -4.7209e+02 - logprior: -3.6436e+00
Epoch 7/10
10/10 - 10s - loss: 470.2939 - loglik: -4.6558e+02 - logprior: -4.0274e+00
Epoch 8/10
10/10 - 10s - loss: 467.7704 - loglik: -4.6327e+02 - logprior: -3.8442e+00
Epoch 9/10
10/10 - 10s - loss: 465.0934 - loglik: -4.6072e+02 - logprior: -3.7702e+00
Epoch 10/10
10/10 - 10s - loss: 464.2197 - loglik: -4.6002e+02 - logprior: -3.6283e+00
Fitted a model with MAP estimate = -464.0608
expansions: [(23, 3), (24, 1), (25, 1), (37, 3), (39, 1), (41, 1), (48, 2), (49, 1), (51, 2), (53, 2), (62, 1), (65, 1), (67, 2), (68, 4), (71, 1), (72, 1), (73, 1), (76, 1), (78, 1), (93, 1), (100, 1), (101, 1), (103, 2), (131, 1), (132, 2), (133, 2), (134, 4), (135, 2), (137, 2), (157, 1), (158, 7), (173, 11)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 476.0052 - loglik: -4.5608e+02 - logprior: -1.9875e+01
Epoch 2/2
15/15 - 11s - loss: 435.4126 - loglik: -4.3251e+02 - logprior: -2.7138e+00
Fitted a model with MAP estimate = -429.7219
expansions: [(0, 3), (206, 2), (207, 1)]
discards: [ 84 169 177 233]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 458.2794 - loglik: -4.3047e+02 - logprior: -2.7752e+01
Epoch 2/2
15/15 - 11s - loss: 432.9388 - loglik: -4.2744e+02 - logprior: -5.1955e+00
Fitted a model with MAP estimate = -426.4477
expansions: []
discards: [  0   1   2  23 207]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 15s - loss: 451.1617 - loglik: -4.3090e+02 - logprior: -2.0202e+01
Epoch 2/10
15/15 - 11s - loss: 428.6608 - loglik: -4.2717e+02 - logprior: -1.1961e+00
Epoch 3/10
15/15 - 11s - loss: 423.9323 - loglik: -4.2497e+02 - logprior: 1.5417
Epoch 4/10
15/15 - 11s - loss: 423.8412 - loglik: -4.2562e+02 - logprior: 2.3767
Epoch 5/10
15/15 - 11s - loss: 420.7819 - loglik: -4.2296e+02 - logprior: 2.7598
Epoch 6/10
15/15 - 11s - loss: 419.7206 - loglik: -4.2238e+02 - logprior: 3.1833
Epoch 7/10
15/15 - 11s - loss: 419.7788 - loglik: -4.2298e+02 - logprior: 3.6698
Fitted a model with MAP estimate = -418.8126
Time for alignment: 258.8233
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 669.4725 - loglik: -6.4204e+02 - logprior: -2.7404e+01
Epoch 2/10
10/10 - 10s - loss: 611.9977 - loglik: -6.0579e+02 - logprior: -6.0122e+00
Epoch 3/10
10/10 - 10s - loss: 560.1434 - loglik: -5.5763e+02 - logprior: -2.2750e+00
Epoch 4/10
10/10 - 10s - loss: 520.6913 - loglik: -5.1841e+02 - logprior: -2.1641e+00
Epoch 5/10
10/10 - 10s - loss: 498.0713 - loglik: -4.9512e+02 - logprior: -2.7405e+00
Epoch 6/10
10/10 - 10s - loss: 479.9313 - loglik: -4.7606e+02 - logprior: -3.3357e+00
Epoch 7/10
10/10 - 10s - loss: 474.1497 - loglik: -4.6960e+02 - logprior: -3.8313e+00
Epoch 8/10
10/10 - 10s - loss: 471.2458 - loglik: -4.6688e+02 - logprior: -3.7144e+00
Epoch 9/10
10/10 - 10s - loss: 467.9795 - loglik: -4.6386e+02 - logprior: -3.5862e+00
Epoch 10/10
10/10 - 10s - loss: 469.5228 - loglik: -4.6560e+02 - logprior: -3.4510e+00
Fitted a model with MAP estimate = -467.6302
expansions: [(23, 1), (24, 2), (25, 1), (26, 1), (40, 1), (42, 1), (49, 1), (52, 1), (53, 1), (55, 2), (56, 3), (68, 1), (69, 5), (70, 2), (71, 1), (72, 2), (73, 1), (74, 1), (76, 1), (78, 1), (85, 1), (101, 1), (102, 2), (103, 4), (132, 2), (133, 2), (134, 4), (135, 3), (137, 1), (139, 1), (156, 1), (157, 4), (158, 3), (173, 7), (181, 4)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 480.0336 - loglik: -4.6008e+02 - logprior: -1.9906e+01
Epoch 2/2
15/15 - 12s - loss: 438.8963 - loglik: -4.3569e+02 - logprior: -2.9858e+00
Fitted a model with MAP estimate = -432.2886
expansions: [(208, 1), (245, 1), (247, 1)]
discards: [ 89 132 171 179 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 15s - loss: 454.5023 - loglik: -4.3593e+02 - logprior: -1.8517e+01
Epoch 2/2
15/15 - 11s - loss: 429.3695 - loglik: -4.2714e+02 - logprior: -1.9310e+00
Fitted a model with MAP estimate = -426.3665
expansions: [(63, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 15s - loss: 448.3190 - loglik: -4.3038e+02 - logprior: -1.7885e+01
Epoch 2/10
15/15 - 11s - loss: 425.5792 - loglik: -4.2421e+02 - logprior: -1.0836e+00
Epoch 3/10
15/15 - 11s - loss: 424.6002 - loglik: -4.2550e+02 - logprior: 1.4157
Epoch 4/10
15/15 - 11s - loss: 423.6754 - loglik: -4.2543e+02 - logprior: 2.3364
Epoch 5/10
15/15 - 11s - loss: 421.3336 - loglik: -4.2367e+02 - logprior: 2.8647
Epoch 6/10
15/15 - 11s - loss: 417.4881 - loglik: -4.2039e+02 - logprior: 3.3711
Epoch 7/10
15/15 - 12s - loss: 420.9225 - loglik: -4.2424e+02 - logprior: 3.7583
Fitted a model with MAP estimate = -418.6207
Time for alignment: 260.4766
Computed alignments with likelihoods: ['-419.0933', '-418.8126', '-418.6207']
Best model has likelihood: -418.6207
time for generating output: 0.3570
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9066397034041119
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b004c9760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b00543700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7f64670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b19ffbbe0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b08a32f10>, <__main__.SimpleDirichletPrior object at 0x7f3b08ae2070>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.4628 - loglik: -1.8498e+02 - logprior: -4.2458e+01
Epoch 2/10
10/10 - 1s - loss: 180.9095 - loglik: -1.6891e+02 - logprior: -1.1977e+01
Epoch 3/10
10/10 - 1s - loss: 161.3573 - loglik: -1.5509e+02 - logprior: -6.2601e+00
Epoch 4/10
10/10 - 1s - loss: 152.2276 - loglik: -1.4796e+02 - logprior: -4.2303e+00
Epoch 5/10
10/10 - 1s - loss: 147.9867 - loglik: -1.4456e+02 - logprior: -3.2890e+00
Epoch 6/10
10/10 - 1s - loss: 146.1114 - loglik: -1.4288e+02 - logprior: -2.9134e+00
Epoch 7/10
10/10 - 1s - loss: 145.0004 - loglik: -1.4196e+02 - logprior: -2.7338e+00
Epoch 8/10
10/10 - 1s - loss: 144.0211 - loglik: -1.4121e+02 - logprior: -2.5537e+00
Epoch 9/10
10/10 - 1s - loss: 143.9203 - loglik: -1.4135e+02 - logprior: -2.3342e+00
Epoch 10/10
10/10 - 1s - loss: 143.4181 - loglik: -1.4100e+02 - logprior: -2.1907e+00
Fitted a model with MAP estimate = -143.2439
expansions: [(0, 2), (1, 1), (17, 1), (19, 2), (21, 1), (24, 1), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.3852 - loglik: -1.4031e+02 - logprior: -5.6053e+01
Epoch 2/2
10/10 - 1s - loss: 152.9051 - loglik: -1.3516e+02 - logprior: -1.7608e+01
Fitted a model with MAP estimate = -144.9192
expansions: [(26, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 183.8041 - loglik: -1.3541e+02 - logprior: -4.8369e+01
Epoch 2/2
10/10 - 1s - loss: 153.8607 - loglik: -1.3441e+02 - logprior: -1.9324e+01
Fitted a model with MAP estimate = -148.4606
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.8623 - loglik: -1.3431e+02 - logprior: -4.3530e+01
Epoch 2/10
10/10 - 1s - loss: 145.7192 - loglik: -1.3308e+02 - logprior: -1.2512e+01
Epoch 3/10
10/10 - 1s - loss: 138.2433 - loglik: -1.3247e+02 - logprior: -5.5162e+00
Epoch 4/10
10/10 - 1s - loss: 135.2399 - loglik: -1.3187e+02 - logprior: -3.0706e+00
Epoch 5/10
10/10 - 1s - loss: 133.6770 - loglik: -1.3152e+02 - logprior: -1.8812e+00
Epoch 6/10
10/10 - 1s - loss: 132.5446 - loglik: -1.3096e+02 - logprior: -1.3239e+00
Epoch 7/10
10/10 - 1s - loss: 132.1967 - loglik: -1.3086e+02 - logprior: -1.0733e+00
Epoch 8/10
10/10 - 1s - loss: 131.8789 - loglik: -1.3079e+02 - logprior: -8.2596e-01
Epoch 9/10
10/10 - 1s - loss: 131.5527 - loglik: -1.3075e+02 - logprior: -5.4420e-01
Epoch 10/10
10/10 - 1s - loss: 131.3950 - loglik: -1.3080e+02 - logprior: -3.3262e-01
Fitted a model with MAP estimate = -131.0253
Time for alignment: 36.6042
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.4952 - loglik: -1.8502e+02 - logprior: -4.2456e+01
Epoch 2/10
10/10 - 1s - loss: 180.9121 - loglik: -1.6891e+02 - logprior: -1.1978e+01
Epoch 3/10
10/10 - 1s - loss: 161.7644 - loglik: -1.5548e+02 - logprior: -6.2811e+00
Epoch 4/10
10/10 - 1s - loss: 152.3696 - loglik: -1.4805e+02 - logprior: -4.2774e+00
Epoch 5/10
10/10 - 1s - loss: 148.1053 - loglik: -1.4454e+02 - logprior: -3.4020e+00
Epoch 6/10
10/10 - 1s - loss: 145.9346 - loglik: -1.4265e+02 - logprior: -2.9429e+00
Epoch 7/10
10/10 - 1s - loss: 144.9774 - loglik: -1.4194e+02 - logprior: -2.7069e+00
Epoch 8/10
10/10 - 1s - loss: 144.3650 - loglik: -1.4161e+02 - logprior: -2.4892e+00
Epoch 9/10
10/10 - 1s - loss: 144.1861 - loglik: -1.4163e+02 - logprior: -2.2920e+00
Epoch 10/10
10/10 - 1s - loss: 143.9387 - loglik: -1.4153e+02 - logprior: -2.1512e+00
Fitted a model with MAP estimate = -143.5285
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.0709 - loglik: -1.4006e+02 - logprior: -5.5993e+01
Epoch 2/2
10/10 - 1s - loss: 152.2044 - loglik: -1.3434e+02 - logprior: -1.7723e+01
Fitted a model with MAP estimate = -143.9505
expansions: [(19, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.9684 - loglik: -1.3473e+02 - logprior: -4.8211e+01
Epoch 2/2
10/10 - 1s - loss: 153.2448 - loglik: -1.3379e+02 - logprior: -1.9318e+01
Fitted a model with MAP estimate = -148.0238
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.5888 - loglik: -1.3404e+02 - logprior: -4.3529e+01
Epoch 2/10
10/10 - 1s - loss: 145.5663 - loglik: -1.3295e+02 - logprior: -1.2496e+01
Epoch 3/10
10/10 - 1s - loss: 138.0166 - loglik: -1.3228e+02 - logprior: -5.4961e+00
Epoch 4/10
10/10 - 1s - loss: 135.1918 - loglik: -1.3186e+02 - logprior: -3.0430e+00
Epoch 5/10
10/10 - 1s - loss: 133.5519 - loglik: -1.3141e+02 - logprior: -1.8703e+00
Epoch 6/10
10/10 - 1s - loss: 132.6491 - loglik: -1.3106e+02 - logprior: -1.3332e+00
Epoch 7/10
10/10 - 1s - loss: 132.0719 - loglik: -1.3074e+02 - logprior: -1.0773e+00
Epoch 8/10
10/10 - 1s - loss: 131.7880 - loglik: -1.3074e+02 - logprior: -7.9131e-01
Epoch 9/10
10/10 - 1s - loss: 131.4833 - loglik: -1.3074e+02 - logprior: -4.9105e-01
Epoch 10/10
10/10 - 1s - loss: 131.3423 - loglik: -1.3080e+02 - logprior: -2.8036e-01
Fitted a model with MAP estimate = -130.9670
Time for alignment: 36.6355
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.6054 - loglik: -1.8513e+02 - logprior: -4.2456e+01
Epoch 2/10
10/10 - 1s - loss: 180.9971 - loglik: -1.6899e+02 - logprior: -1.1978e+01
Epoch 3/10
10/10 - 1s - loss: 161.5925 - loglik: -1.5530e+02 - logprior: -6.2884e+00
Epoch 4/10
10/10 - 1s - loss: 152.0646 - loglik: -1.4775e+02 - logprior: -4.2681e+00
Epoch 5/10
10/10 - 1s - loss: 147.7500 - loglik: -1.4421e+02 - logprior: -3.3545e+00
Epoch 6/10
10/10 - 1s - loss: 145.6761 - loglik: -1.4246e+02 - logprior: -2.8897e+00
Epoch 7/10
10/10 - 1s - loss: 144.6081 - loglik: -1.4160e+02 - logprior: -2.7124e+00
Epoch 8/10
10/10 - 1s - loss: 143.8752 - loglik: -1.4110e+02 - logprior: -2.5180e+00
Epoch 9/10
10/10 - 1s - loss: 143.7460 - loglik: -1.4122e+02 - logprior: -2.2694e+00
Epoch 10/10
10/10 - 1s - loss: 143.3963 - loglik: -1.4099e+02 - logprior: -2.1337e+00
Fitted a model with MAP estimate = -143.0736
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.1578 - loglik: -1.4018e+02 - logprior: -5.5954e+01
Epoch 2/2
10/10 - 1s - loss: 152.3319 - loglik: -1.3454e+02 - logprior: -1.7671e+01
Fitted a model with MAP estimate = -144.0853
expansions: [(19, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.9549 - loglik: -1.3473e+02 - logprior: -4.8199e+01
Epoch 2/2
10/10 - 1s - loss: 153.3277 - loglik: -1.3387e+02 - logprior: -1.9321e+01
Fitted a model with MAP estimate = -147.9019
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.3676 - loglik: -1.3379e+02 - logprior: -4.3555e+01
Epoch 2/10
10/10 - 1s - loss: 145.7248 - loglik: -1.3308e+02 - logprior: -1.2519e+01
Epoch 3/10
10/10 - 1s - loss: 137.8263 - loglik: -1.3207e+02 - logprior: -5.5104e+00
Epoch 4/10
10/10 - 1s - loss: 134.9005 - loglik: -1.3153e+02 - logprior: -3.0660e+00
Epoch 5/10
10/10 - 1s - loss: 133.2913 - loglik: -1.3111e+02 - logprior: -1.9022e+00
Epoch 6/10
10/10 - 1s - loss: 132.5254 - loglik: -1.3092e+02 - logprior: -1.3498e+00
Epoch 7/10
10/10 - 1s - loss: 132.0595 - loglik: -1.3071e+02 - logprior: -1.0974e+00
Epoch 8/10
10/10 - 1s - loss: 131.6486 - loglik: -1.3057e+02 - logprior: -8.2657e-01
Epoch 9/10
10/10 - 1s - loss: 131.5198 - loglik: -1.3073e+02 - logprior: -5.3119e-01
Epoch 10/10
10/10 - 1s - loss: 131.2953 - loglik: -1.3072e+02 - logprior: -3.2154e-01
Fitted a model with MAP estimate = -130.9842
Time for alignment: 36.1418
Computed alignments with likelihoods: ['-131.0253', '-130.9670', '-130.9842']
Best model has likelihood: -130.9670
time for generating output: 0.1296
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.8954506922859565
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3ade09d310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b19f25df0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b19f25f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b08e30880>, <__main__.SimpleDirichletPrior object at 0x7f3af7687340>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.1646 - loglik: -2.3765e+02 - logprior: -1.3498e+01
Epoch 2/10
11/11 - 2s - loss: 220.9465 - loglik: -2.1715e+02 - logprior: -3.7301e+00
Epoch 3/10
11/11 - 2s - loss: 200.5673 - loglik: -1.9785e+02 - logprior: -2.4686e+00
Epoch 4/10
11/11 - 2s - loss: 188.7128 - loglik: -1.8571e+02 - logprior: -2.4622e+00
Epoch 5/10
11/11 - 2s - loss: 186.0807 - loglik: -1.8310e+02 - logprior: -2.4800e+00
Epoch 6/10
11/11 - 2s - loss: 185.7301 - loglik: -1.8309e+02 - logprior: -2.3050e+00
Epoch 7/10
11/11 - 2s - loss: 184.4999 - loglik: -1.8201e+02 - logprior: -2.1780e+00
Epoch 8/10
11/11 - 2s - loss: 184.0338 - loglik: -1.8156e+02 - logprior: -2.1639e+00
Epoch 9/10
11/11 - 2s - loss: 183.3401 - loglik: -1.8079e+02 - logprior: -2.2292e+00
Epoch 10/10
11/11 - 2s - loss: 183.3134 - loglik: -1.8071e+02 - logprior: -2.2657e+00
Fitted a model with MAP estimate = -182.6668
expansions: [(8, 2), (9, 1), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 201.6471 - loglik: -1.8609e+02 - logprior: -1.5525e+01
Epoch 2/2
11/11 - 2s - loss: 185.1100 - loglik: -1.7819e+02 - logprior: -6.7590e+00
Fitted a model with MAP estimate = -181.9022
expansions: [(0, 2)]
discards: [ 0 10]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.7568 - loglik: -1.7652e+02 - logprior: -1.2215e+01
Epoch 2/2
11/11 - 2s - loss: 178.3522 - loglik: -1.7469e+02 - logprior: -3.5041e+00
Fitted a model with MAP estimate = -176.3559
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.9840 - loglik: -1.7729e+02 - logprior: -1.4667e+01
Epoch 2/10
11/11 - 2s - loss: 179.8320 - loglik: -1.7500e+02 - logprior: -4.6552e+00
Epoch 3/10
11/11 - 2s - loss: 176.6654 - loglik: -1.7366e+02 - logprior: -2.6379e+00
Epoch 4/10
11/11 - 2s - loss: 175.9292 - loglik: -1.7343e+02 - logprior: -1.9721e+00
Epoch 5/10
11/11 - 2s - loss: 174.5171 - loglik: -1.7236e+02 - logprior: -1.5976e+00
Epoch 6/10
11/11 - 2s - loss: 173.7513 - loglik: -1.7170e+02 - logprior: -1.5454e+00
Epoch 7/10
11/11 - 2s - loss: 172.3865 - loglik: -1.7051e+02 - logprior: -1.4368e+00
Epoch 8/10
11/11 - 2s - loss: 173.8791 - loglik: -1.7204e+02 - logprior: -1.4321e+00
Fitted a model with MAP estimate = -172.3287
Time for alignment: 61.2322
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.6428 - loglik: -2.3712e+02 - logprior: -1.3500e+01
Epoch 2/10
11/11 - 2s - loss: 221.3764 - loglik: -2.1758e+02 - logprior: -3.7305e+00
Epoch 3/10
11/11 - 2s - loss: 199.3638 - loglik: -1.9667e+02 - logprior: -2.4533e+00
Epoch 4/10
11/11 - 2s - loss: 189.8026 - loglik: -1.8686e+02 - logprior: -2.4249e+00
Epoch 5/10
11/11 - 2s - loss: 186.3118 - loglik: -1.8331e+02 - logprior: -2.4141e+00
Epoch 6/10
11/11 - 2s - loss: 185.4904 - loglik: -1.8283e+02 - logprior: -2.2285e+00
Epoch 7/10
11/11 - 2s - loss: 184.3403 - loglik: -1.8189e+02 - logprior: -2.0695e+00
Epoch 8/10
11/11 - 2s - loss: 184.2204 - loglik: -1.8180e+02 - logprior: -2.0405e+00
Epoch 9/10
11/11 - 2s - loss: 184.1062 - loglik: -1.8164e+02 - logprior: -2.0838e+00
Epoch 10/10
11/11 - 2s - loss: 182.4542 - loglik: -1.7993e+02 - logprior: -2.1319e+00
Fitted a model with MAP estimate = -182.6133
expansions: [(11, 4), (12, 2), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 201.6656 - loglik: -1.8613e+02 - logprior: -1.5510e+01
Epoch 2/2
11/11 - 2s - loss: 184.6562 - loglik: -1.7779e+02 - logprior: -6.7212e+00
Fitted a model with MAP estimate = -181.1535
expansions: [(0, 2)]
discards: [ 0 15]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.2722 - loglik: -1.7605e+02 - logprior: -1.2201e+01
Epoch 2/2
11/11 - 2s - loss: 178.2906 - loglik: -1.7463e+02 - logprior: -3.4990e+00
Fitted a model with MAP estimate = -176.3199
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.7372 - loglik: -1.7612e+02 - logprior: -1.4584e+01
Epoch 2/10
11/11 - 2s - loss: 180.9537 - loglik: -1.7620e+02 - logprior: -4.5779e+00
Epoch 3/10
11/11 - 2s - loss: 177.0561 - loglik: -1.7407e+02 - logprior: -2.6190e+00
Epoch 4/10
11/11 - 2s - loss: 175.4651 - loglik: -1.7297e+02 - logprior: -2.0075e+00
Epoch 5/10
11/11 - 2s - loss: 173.8039 - loglik: -1.7165e+02 - logprior: -1.6366e+00
Epoch 6/10
11/11 - 2s - loss: 173.8160 - loglik: -1.7177e+02 - logprior: -1.5542e+00
Fitted a model with MAP estimate = -172.9555
Time for alignment: 57.7420
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.5795 - loglik: -2.3806e+02 - logprior: -1.3498e+01
Epoch 2/10
11/11 - 2s - loss: 221.1335 - loglik: -2.1733e+02 - logprior: -3.7281e+00
Epoch 3/10
11/11 - 2s - loss: 203.0440 - loglik: -2.0035e+02 - logprior: -2.4789e+00
Epoch 4/10
11/11 - 2s - loss: 191.7389 - loglik: -1.8881e+02 - logprior: -2.5127e+00
Epoch 5/10
11/11 - 2s - loss: 187.6470 - loglik: -1.8449e+02 - logprior: -2.5692e+00
Epoch 6/10
11/11 - 2s - loss: 185.7764 - loglik: -1.8287e+02 - logprior: -2.4250e+00
Epoch 7/10
11/11 - 2s - loss: 185.1936 - loglik: -1.8254e+02 - logprior: -2.2739e+00
Epoch 8/10
11/11 - 2s - loss: 185.2884 - loglik: -1.8273e+02 - logprior: -2.2290e+00
Fitted a model with MAP estimate = -184.5090
expansions: [(8, 2), (9, 1), (10, 2), (12, 1), (23, 2), (25, 1), (31, 1), (32, 1), (34, 1), (45, 1), (48, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 202.4905 - loglik: -1.8699e+02 - logprior: -1.5473e+01
Epoch 2/2
11/11 - 2s - loss: 185.3152 - loglik: -1.7840e+02 - logprior: -6.7576e+00
Fitted a model with MAP estimate = -182.0434
expansions: [(0, 2)]
discards: [ 0 10 29]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.4332 - loglik: -1.7622e+02 - logprior: -1.2186e+01
Epoch 2/2
11/11 - 2s - loss: 178.3110 - loglik: -1.7470e+02 - logprior: -3.4653e+00
Fitted a model with MAP estimate = -176.4259
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 191.5433 - loglik: -1.7688e+02 - logprior: -1.4636e+01
Epoch 2/10
11/11 - 2s - loss: 180.3020 - loglik: -1.7550e+02 - logprior: -4.6387e+00
Epoch 3/10
11/11 - 2s - loss: 176.5764 - loglik: -1.7361e+02 - logprior: -2.6037e+00
Epoch 4/10
11/11 - 2s - loss: 175.6794 - loglik: -1.7323e+02 - logprior: -1.9316e+00
Epoch 5/10
11/11 - 2s - loss: 174.5742 - loglik: -1.7244e+02 - logprior: -1.5935e+00
Epoch 6/10
11/11 - 2s - loss: 173.3062 - loglik: -1.7128e+02 - logprior: -1.5352e+00
Epoch 7/10
11/11 - 2s - loss: 173.0832 - loglik: -1.7118e+02 - logprior: -1.4500e+00
Epoch 8/10
11/11 - 2s - loss: 172.4891 - loglik: -1.7066e+02 - logprior: -1.4141e+00
Epoch 9/10
11/11 - 2s - loss: 172.5184 - loglik: -1.7071e+02 - logprior: -1.4105e+00
Fitted a model with MAP estimate = -171.8017
Time for alignment: 59.1651
Computed alignments with likelihoods: ['-172.3287', '-172.9555', '-171.8017']
Best model has likelihood: -171.8017
time for generating output: 0.1469
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8193172356369692
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b3bf63670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b11523e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11523dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b00505b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3ade09d310>, <__main__.SimpleDirichletPrior object at 0x7f3b4cb903d0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.2928 - loglik: -2.2370e+02 - logprior: -4.1573e+01
Epoch 2/10
10/10 - 1s - loss: 209.7878 - loglik: -1.9809e+02 - logprior: -1.1683e+01
Epoch 3/10
10/10 - 1s - loss: 179.2260 - loglik: -1.7285e+02 - logprior: -6.3655e+00
Epoch 4/10
10/10 - 1s - loss: 157.7778 - loglik: -1.5307e+02 - logprior: -4.6984e+00
Epoch 5/10
10/10 - 1s - loss: 148.5635 - loglik: -1.4437e+02 - logprior: -4.1238e+00
Epoch 6/10
10/10 - 1s - loss: 145.3698 - loglik: -1.4128e+02 - logprior: -3.7744e+00
Epoch 7/10
10/10 - 1s - loss: 144.0914 - loglik: -1.4013e+02 - logprior: -3.4667e+00
Epoch 8/10
10/10 - 1s - loss: 143.4593 - loglik: -1.3972e+02 - logprior: -3.2784e+00
Epoch 9/10
10/10 - 1s - loss: 143.0316 - loglik: -1.3949e+02 - logprior: -3.1644e+00
Epoch 10/10
10/10 - 1s - loss: 142.5268 - loglik: -1.3908e+02 - logprior: -3.0743e+00
Fitted a model with MAP estimate = -142.1680
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 171.8956 - loglik: -1.3370e+02 - logprior: -3.8174e+01
Epoch 2/2
10/10 - 1s - loss: 137.9506 - loglik: -1.2692e+02 - logprior: -1.0894e+01
Fitted a model with MAP estimate = -133.0114
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 178.0821 - loglik: -1.3164e+02 - logprior: -4.6423e+01
Epoch 2/2
10/10 - 1s - loss: 150.2240 - loglik: -1.3045e+02 - logprior: -1.9670e+01
Fitted a model with MAP estimate = -145.9558
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 166.2360 - loglik: -1.2864e+02 - logprior: -3.7580e+01
Epoch 2/10
10/10 - 1s - loss: 136.6508 - loglik: -1.2631e+02 - logprior: -1.0260e+01
Epoch 3/10
10/10 - 1s - loss: 131.0041 - loglik: -1.2604e+02 - logprior: -4.7918e+00
Epoch 4/10
10/10 - 1s - loss: 128.6673 - loglik: -1.2581e+02 - logprior: -2.6253e+00
Epoch 5/10
10/10 - 1s - loss: 127.5500 - loglik: -1.2578e+02 - logprior: -1.5019e+00
Epoch 6/10
10/10 - 1s - loss: 126.7853 - loglik: -1.2556e+02 - logprior: -9.2022e-01
Epoch 7/10
10/10 - 1s - loss: 126.5932 - loglik: -1.2570e+02 - logprior: -5.7022e-01
Epoch 8/10
10/10 - 1s - loss: 126.2366 - loglik: -1.2556e+02 - logprior: -3.4123e-01
Epoch 9/10
10/10 - 1s - loss: 126.0415 - loglik: -1.2554e+02 - logprior: -1.6708e-01
Epoch 10/10
10/10 - 1s - loss: 125.7982 - loglik: -1.2548e+02 - logprior: -1.1641e-02
Fitted a model with MAP estimate = -125.3982
Time for alignment: 40.9981
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 265.4265 - loglik: -2.2384e+02 - logprior: -4.1572e+01
Epoch 2/10
10/10 - 1s - loss: 209.1854 - loglik: -1.9749e+02 - logprior: -1.1683e+01
Epoch 3/10
10/10 - 1s - loss: 179.5082 - loglik: -1.7312e+02 - logprior: -6.3698e+00
Epoch 4/10
10/10 - 1s - loss: 157.6504 - loglik: -1.5295e+02 - logprior: -4.6949e+00
Epoch 5/10
10/10 - 1s - loss: 148.9800 - loglik: -1.4498e+02 - logprior: -3.9996e+00
Epoch 6/10
10/10 - 1s - loss: 145.6145 - loglik: -1.4188e+02 - logprior: -3.6738e+00
Epoch 7/10
10/10 - 1s - loss: 144.2009 - loglik: -1.4050e+02 - logprior: -3.4616e+00
Epoch 8/10
10/10 - 1s - loss: 143.2997 - loglik: -1.3966e+02 - logprior: -3.2986e+00
Epoch 9/10
10/10 - 1s - loss: 142.7589 - loglik: -1.3926e+02 - logprior: -3.1886e+00
Epoch 10/10
10/10 - 1s - loss: 142.4042 - loglik: -1.3899e+02 - logprior: -3.1326e+00
Fitted a model with MAP estimate = -142.1160
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.7256 - loglik: -1.3349e+02 - logprior: -3.8214e+01
Epoch 2/2
10/10 - 1s - loss: 138.1764 - loglik: -1.2721e+02 - logprior: -1.0888e+01
Fitted a model with MAP estimate = -133.0635
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 177.9680 - loglik: -1.3154e+02 - logprior: -4.6416e+01
Epoch 2/2
10/10 - 1s - loss: 150.3659 - loglik: -1.3064e+02 - logprior: -1.9665e+01
Fitted a model with MAP estimate = -145.9564
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 165.7858 - loglik: -1.2820e+02 - logprior: -3.7573e+01
Epoch 2/10
10/10 - 1s - loss: 137.1101 - loglik: -1.2680e+02 - logprior: -1.0246e+01
Epoch 3/10
10/10 - 1s - loss: 130.9347 - loglik: -1.2598e+02 - logprior: -4.8020e+00
Epoch 4/10
10/10 - 1s - loss: 128.2601 - loglik: -1.2537e+02 - logprior: -2.6794e+00
Epoch 5/10
10/10 - 1s - loss: 127.7714 - loglik: -1.2599e+02 - logprior: -1.5436e+00
Epoch 6/10
10/10 - 1s - loss: 126.6593 - loglik: -1.2546e+02 - logprior: -9.4104e-01
Epoch 7/10
10/10 - 1s - loss: 126.4802 - loglik: -1.2561e+02 - logprior: -6.0759e-01
Epoch 8/10
10/10 - 1s - loss: 126.0442 - loglik: -1.2540e+02 - logprior: -3.7673e-01
Epoch 9/10
10/10 - 1s - loss: 126.1102 - loglik: -1.2565e+02 - logprior: -1.8444e-01
Fitted a model with MAP estimate = -125.5474
Time for alignment: 40.2332
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.2483 - loglik: -2.2366e+02 - logprior: -4.1573e+01
Epoch 2/10
10/10 - 1s - loss: 209.6919 - loglik: -1.9800e+02 - logprior: -1.1682e+01
Epoch 3/10
10/10 - 1s - loss: 178.3602 - loglik: -1.7199e+02 - logprior: -6.3589e+00
Epoch 4/10
10/10 - 1s - loss: 156.6667 - loglik: -1.5194e+02 - logprior: -4.7132e+00
Epoch 5/10
10/10 - 1s - loss: 148.3655 - loglik: -1.4419e+02 - logprior: -4.1018e+00
Epoch 6/10
10/10 - 1s - loss: 145.3870 - loglik: -1.4134e+02 - logprior: -3.7168e+00
Epoch 7/10
10/10 - 1s - loss: 144.1726 - loglik: -1.4023e+02 - logprior: -3.4362e+00
Epoch 8/10
10/10 - 1s - loss: 143.5048 - loglik: -1.3976e+02 - logprior: -3.2591e+00
Epoch 9/10
10/10 - 1s - loss: 142.9798 - loglik: -1.3940e+02 - logprior: -3.1453e+00
Epoch 10/10
10/10 - 1s - loss: 143.0159 - loglik: -1.3952e+02 - logprior: -3.0800e+00
Fitted a model with MAP estimate = -142.1879
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 171.9964 - loglik: -1.3379e+02 - logprior: -3.8188e+01
Epoch 2/2
10/10 - 1s - loss: 137.6748 - loglik: -1.2668e+02 - logprior: -1.0892e+01
Fitted a model with MAP estimate = -133.0024
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 178.2565 - loglik: -1.3179e+02 - logprior: -4.6452e+01
Epoch 2/2
10/10 - 1s - loss: 150.1756 - loglik: -1.3040e+02 - logprior: -1.9673e+01
Fitted a model with MAP estimate = -145.9639
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 166.0026 - loglik: -1.2831e+02 - logprior: -3.7671e+01
Epoch 2/10
10/10 - 1s - loss: 137.0473 - loglik: -1.2670e+02 - logprior: -1.0267e+01
Epoch 3/10
10/10 - 1s - loss: 130.8670 - loglik: -1.2589e+02 - logprior: -4.7975e+00
Epoch 4/10
10/10 - 1s - loss: 128.5560 - loglik: -1.2567e+02 - logprior: -2.6375e+00
Epoch 5/10
10/10 - 1s - loss: 127.6511 - loglik: -1.2587e+02 - logprior: -1.5080e+00
Epoch 6/10
10/10 - 1s - loss: 126.7037 - loglik: -1.2550e+02 - logprior: -9.1725e-01
Epoch 7/10
10/10 - 1s - loss: 126.5096 - loglik: -1.2564e+02 - logprior: -5.9428e-01
Epoch 8/10
10/10 - 1s - loss: 126.1248 - loglik: -1.2548e+02 - logprior: -3.6971e-01
Epoch 9/10
10/10 - 1s - loss: 125.8364 - loglik: -1.2537e+02 - logprior: -1.8045e-01
Epoch 10/10
10/10 - 1s - loss: 125.7508 - loglik: -1.2546e+02 - logprior: -8.6451e-03
Fitted a model with MAP estimate = -125.3839
Time for alignment: 39.9006
Computed alignments with likelihoods: ['-125.3982', '-125.5474', '-125.3839']
Best model has likelihood: -125.3839
time for generating output: 0.1390
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.8845470692717584
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3aeeb3c520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad406b520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad406b6a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3addb75550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3ac1c23f10>, <__main__.SimpleDirichletPrior object at 0x7f3b08f704f0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5075 - loglik: -1.6674e+02 - logprior: -5.7352e+00
Epoch 2/10
15/15 - 1s - loss: 145.5043 - loglik: -1.4359e+02 - logprior: -1.8837e+00
Epoch 3/10
15/15 - 1s - loss: 130.6859 - loglik: -1.2866e+02 - logprior: -1.9480e+00
Epoch 4/10
15/15 - 1s - loss: 126.8640 - loglik: -1.2450e+02 - logprior: -1.9348e+00
Epoch 5/10
15/15 - 1s - loss: 125.7308 - loglik: -1.2343e+02 - logprior: -1.8394e+00
Epoch 6/10
15/15 - 1s - loss: 125.1634 - loglik: -1.2290e+02 - logprior: -1.8539e+00
Epoch 7/10
15/15 - 1s - loss: 124.5202 - loglik: -1.2225e+02 - logprior: -1.8409e+00
Epoch 8/10
15/15 - 1s - loss: 124.1978 - loglik: -1.2190e+02 - logprior: -1.8430e+00
Epoch 9/10
15/15 - 1s - loss: 123.9294 - loglik: -1.2159e+02 - logprior: -1.8614e+00
Epoch 10/10
15/15 - 1s - loss: 123.5684 - loglik: -1.2126e+02 - logprior: -1.8714e+00
Fitted a model with MAP estimate = -122.9763
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 137.3030 - loglik: -1.3014e+02 - logprior: -7.1034e+00
Epoch 2/2
15/15 - 1s - loss: 127.1615 - loglik: -1.2322e+02 - logprior: -3.6444e+00
Fitted a model with MAP estimate = -124.5023
expansions: [(0, 1)]
discards: [ 0 13 15 45]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 129.1122 - loglik: -1.2364e+02 - logprior: -5.4137e+00
Epoch 2/2
15/15 - 1s - loss: 121.9518 - loglik: -1.1955e+02 - logprior: -2.1548e+00
Fitted a model with MAP estimate = -120.5265
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.7542 - loglik: -1.2016e+02 - logprior: -5.5407e+00
Epoch 2/10
15/15 - 1s - loss: 121.4308 - loglik: -1.1910e+02 - logprior: -2.1046e+00
Epoch 3/10
15/15 - 1s - loss: 120.4394 - loglik: -1.1836e+02 - logprior: -1.6966e+00
Epoch 4/10
15/15 - 1s - loss: 119.5663 - loglik: -1.1753e+02 - logprior: -1.5902e+00
Epoch 5/10
15/15 - 1s - loss: 119.1251 - loglik: -1.1714e+02 - logprior: -1.5423e+00
Epoch 6/10
15/15 - 1s - loss: 118.7502 - loglik: -1.1681e+02 - logprior: -1.5250e+00
Epoch 7/10
15/15 - 1s - loss: 118.4224 - loglik: -1.1651e+02 - logprior: -1.5091e+00
Epoch 8/10
15/15 - 1s - loss: 118.5055 - loglik: -1.1662e+02 - logprior: -1.4942e+00
Fitted a model with MAP estimate = -117.8039
Time for alignment: 46.4472
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 172.4990 - loglik: -1.6673e+02 - logprior: -5.7331e+00
Epoch 2/10
15/15 - 1s - loss: 144.6811 - loglik: -1.4278e+02 - logprior: -1.8752e+00
Epoch 3/10
15/15 - 1s - loss: 130.7285 - loglik: -1.2874e+02 - logprior: -1.9274e+00
Epoch 4/10
15/15 - 1s - loss: 126.8133 - loglik: -1.2447e+02 - logprior: -1.9260e+00
Epoch 5/10
15/15 - 1s - loss: 125.4318 - loglik: -1.2306e+02 - logprior: -1.8604e+00
Epoch 6/10
15/15 - 1s - loss: 124.5850 - loglik: -1.2227e+02 - logprior: -1.8830e+00
Epoch 7/10
15/15 - 1s - loss: 124.4364 - loglik: -1.2209e+02 - logprior: -1.8843e+00
Epoch 8/10
15/15 - 1s - loss: 123.9639 - loglik: -1.2162e+02 - logprior: -1.8812e+00
Epoch 9/10
15/15 - 1s - loss: 123.8400 - loglik: -1.2149e+02 - logprior: -1.8774e+00
Epoch 10/10
15/15 - 1s - loss: 123.6590 - loglik: -1.2131e+02 - logprior: -1.8677e+00
Fitted a model with MAP estimate = -123.0747
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 138.3244 - loglik: -1.3116e+02 - logprior: -7.1065e+00
Epoch 2/2
15/15 - 1s - loss: 127.3696 - loglik: -1.2338e+02 - logprior: -3.7006e+00
Fitted a model with MAP estimate = -124.6333
expansions: [(0, 1)]
discards: [ 0 13 15 31 46]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 129.0936 - loglik: -1.2360e+02 - logprior: -5.4317e+00
Epoch 2/2
15/15 - 1s - loss: 122.2211 - loglik: -1.1982e+02 - logprior: -2.1512e+00
Fitted a model with MAP estimate = -120.5902
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 125.8757 - loglik: -1.2028e+02 - logprior: -5.5423e+00
Epoch 2/10
15/15 - 1s - loss: 121.5167 - loglik: -1.1914e+02 - logprior: -2.1105e+00
Epoch 3/10
15/15 - 1s - loss: 120.2220 - loglik: -1.1812e+02 - logprior: -1.6948e+00
Epoch 4/10
15/15 - 1s - loss: 119.5418 - loglik: -1.1748e+02 - logprior: -1.5919e+00
Epoch 5/10
15/15 - 1s - loss: 119.1641 - loglik: -1.1717e+02 - logprior: -1.5462e+00
Epoch 6/10
15/15 - 1s - loss: 118.8041 - loglik: -1.1686e+02 - logprior: -1.5285e+00
Epoch 7/10
15/15 - 1s - loss: 118.5726 - loglik: -1.1665e+02 - logprior: -1.5102e+00
Epoch 8/10
15/15 - 1s - loss: 118.2675 - loglik: -1.1638e+02 - logprior: -1.4957e+00
Epoch 9/10
15/15 - 1s - loss: 118.2602 - loglik: -1.1639e+02 - logprior: -1.4769e+00
Epoch 10/10
15/15 - 1s - loss: 117.9529 - loglik: -1.1610e+02 - logprior: -1.4615e+00
Fitted a model with MAP estimate = -117.4793
Time for alignment: 49.4796
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5226 - loglik: -1.6675e+02 - logprior: -5.7337e+00
Epoch 2/10
15/15 - 1s - loss: 145.2874 - loglik: -1.4339e+02 - logprior: -1.8752e+00
Epoch 3/10
15/15 - 1s - loss: 130.7179 - loglik: -1.2876e+02 - logprior: -1.9238e+00
Epoch 4/10
15/15 - 1s - loss: 126.5305 - loglik: -1.2431e+02 - logprior: -1.9122e+00
Epoch 5/10
15/15 - 1s - loss: 125.8367 - loglik: -1.2353e+02 - logprior: -1.8394e+00
Epoch 6/10
15/15 - 1s - loss: 124.9771 - loglik: -1.2270e+02 - logprior: -1.8784e+00
Epoch 7/10
15/15 - 1s - loss: 124.6677 - loglik: -1.2240e+02 - logprior: -1.8739e+00
Epoch 8/10
15/15 - 1s - loss: 123.8206 - loglik: -1.2153e+02 - logprior: -1.8805e+00
Epoch 9/10
15/15 - 1s - loss: 123.7156 - loglik: -1.2145e+02 - logprior: -1.8787e+00
Epoch 10/10
15/15 - 1s - loss: 123.4069 - loglik: -1.2115e+02 - logprior: -1.8663e+00
Fitted a model with MAP estimate = -122.9025
expansions: [(9, 2), (10, 1), (11, 2), (12, 2), (13, 1), (15, 1), (24, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 137.5082 - loglik: -1.3036e+02 - logprior: -7.0913e+00
Epoch 2/2
15/15 - 1s - loss: 127.4177 - loglik: -1.2347e+02 - logprior: -3.6420e+00
Fitted a model with MAP estimate = -124.7227
expansions: [(0, 2)]
discards: [ 0  9 16 46]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.8551 - loglik: -1.2249e+02 - logprior: -5.3052e+00
Epoch 2/2
15/15 - 1s - loss: 121.2216 - loglik: -1.1907e+02 - logprior: -1.8807e+00
Fitted a model with MAP estimate = -119.7239
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 129.3473 - loglik: -1.2258e+02 - logprior: -6.7185e+00
Epoch 2/10
15/15 - 1s - loss: 122.0954 - loglik: -1.1951e+02 - logprior: -2.3875e+00
Epoch 3/10
15/15 - 1s - loss: 120.4851 - loglik: -1.1839e+02 - logprior: -1.7295e+00
Epoch 4/10
15/15 - 1s - loss: 119.5106 - loglik: -1.1748e+02 - logprior: -1.5894e+00
Epoch 5/10
15/15 - 1s - loss: 119.1018 - loglik: -1.1712e+02 - logprior: -1.5461e+00
Epoch 6/10
15/15 - 1s - loss: 118.7271 - loglik: -1.1678e+02 - logprior: -1.5340e+00
Epoch 7/10
15/15 - 1s - loss: 118.6351 - loglik: -1.1671e+02 - logprior: -1.5188e+00
Epoch 8/10
15/15 - 1s - loss: 118.3093 - loglik: -1.1641e+02 - logprior: -1.5038e+00
Epoch 9/10
15/15 - 1s - loss: 118.0354 - loglik: -1.1614e+02 - logprior: -1.4939e+00
Epoch 10/10
15/15 - 1s - loss: 117.9850 - loglik: -1.1611e+02 - logprior: -1.4755e+00
Fitted a model with MAP estimate = -117.4241
Time for alignment: 47.9734
Computed alignments with likelihoods: ['-117.8039', '-117.4793', '-117.4241']
Best model has likelihood: -117.4241
time for generating output: 0.1417
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b1126c700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b3bfae760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3bfaebb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad3887970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3ad3887610>, <__main__.SimpleDirichletPrior object at 0x7f3b658f6340>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.9465 - loglik: -1.8334e+02 - logprior: -1.4600e+01
Epoch 2/10
10/10 - 2s - loss: 170.8938 - loglik: -1.6652e+02 - logprior: -4.3474e+00
Epoch 3/10
10/10 - 2s - loss: 153.4437 - loglik: -1.5054e+02 - logprior: -2.7327e+00
Epoch 4/10
10/10 - 2s - loss: 141.9454 - loglik: -1.3908e+02 - logprior: -2.5272e+00
Epoch 5/10
10/10 - 2s - loss: 138.3241 - loglik: -1.3543e+02 - logprior: -2.5655e+00
Epoch 6/10
10/10 - 2s - loss: 136.1033 - loglik: -1.3333e+02 - logprior: -2.5776e+00
Epoch 7/10
10/10 - 2s - loss: 134.8116 - loglik: -1.3212e+02 - logprior: -2.5253e+00
Epoch 8/10
10/10 - 2s - loss: 133.5737 - loglik: -1.3100e+02 - logprior: -2.4592e+00
Epoch 9/10
10/10 - 2s - loss: 133.3782 - loglik: -1.3086e+02 - logprior: -2.4197e+00
Epoch 10/10
10/10 - 2s - loss: 133.3954 - loglik: -1.3089e+02 - logprior: -2.4000e+00
Fitted a model with MAP estimate = -133.1155
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (33, 3), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 148.5873 - loglik: -1.3216e+02 - logprior: -1.6404e+01
Epoch 2/2
10/10 - 2s - loss: 129.1064 - loglik: -1.2179e+02 - logprior: -7.1948e+00
Fitted a model with MAP estimate = -125.7723
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 132.2055 - loglik: -1.1916e+02 - logprior: -1.3021e+01
Epoch 2/2
10/10 - 2s - loss: 122.1635 - loglik: -1.1823e+02 - logprior: -3.8282e+00
Fitted a model with MAP estimate = -120.1386
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.6167 - loglik: -1.2018e+02 - logprior: -1.5412e+01
Epoch 2/10
10/10 - 2s - loss: 124.3153 - loglik: -1.1911e+02 - logprior: -5.0824e+00
Epoch 3/10
10/10 - 2s - loss: 121.1445 - loglik: -1.1803e+02 - logprior: -2.8802e+00
Epoch 4/10
10/10 - 2s - loss: 119.5167 - loglik: -1.1707e+02 - logprior: -2.1695e+00
Epoch 5/10
10/10 - 2s - loss: 118.8223 - loglik: -1.1688e+02 - logprior: -1.6903e+00
Epoch 6/10
10/10 - 2s - loss: 118.5970 - loglik: -1.1682e+02 - logprior: -1.5719e+00
Epoch 7/10
10/10 - 2s - loss: 118.4031 - loglik: -1.1677e+02 - logprior: -1.4565e+00
Epoch 8/10
10/10 - 2s - loss: 118.0665 - loglik: -1.1652e+02 - logprior: -1.3817e+00
Epoch 9/10
10/10 - 2s - loss: 117.7914 - loglik: -1.1627e+02 - logprior: -1.3707e+00
Epoch 10/10
10/10 - 2s - loss: 117.9352 - loglik: -1.1645e+02 - logprior: -1.3337e+00
Fitted a model with MAP estimate = -117.7319
Time for alignment: 71.1767
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.9130 - loglik: -1.8331e+02 - logprior: -1.4599e+01
Epoch 2/10
10/10 - 2s - loss: 170.4935 - loglik: -1.6612e+02 - logprior: -4.3411e+00
Epoch 3/10
10/10 - 2s - loss: 153.1620 - loglik: -1.5032e+02 - logprior: -2.6920e+00
Epoch 4/10
10/10 - 2s - loss: 142.3333 - loglik: -1.3963e+02 - logprior: -2.4434e+00
Epoch 5/10
10/10 - 2s - loss: 138.1278 - loglik: -1.3528e+02 - logprior: -2.5311e+00
Epoch 6/10
10/10 - 2s - loss: 135.6403 - loglik: -1.3282e+02 - logprior: -2.6098e+00
Epoch 7/10
10/10 - 2s - loss: 134.0312 - loglik: -1.3128e+02 - logprior: -2.5681e+00
Epoch 8/10
10/10 - 2s - loss: 133.4470 - loglik: -1.3078e+02 - logprior: -2.5018e+00
Epoch 9/10
10/10 - 2s - loss: 133.1773 - loglik: -1.3055e+02 - logprior: -2.4726e+00
Epoch 10/10
10/10 - 2s - loss: 131.7710 - loglik: -1.2917e+02 - logprior: -2.4725e+00
Fitted a model with MAP estimate = -132.1949
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 148.5888 - loglik: -1.3215e+02 - logprior: -1.6416e+01
Epoch 2/2
10/10 - 2s - loss: 129.2954 - loglik: -1.2196e+02 - logprior: -7.2057e+00
Fitted a model with MAP estimate = -125.8188
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.4309 - loglik: -1.1939e+02 - logprior: -1.3023e+01
Epoch 2/2
10/10 - 2s - loss: 121.7912 - loglik: -1.1786e+02 - logprior: -3.8279e+00
Fitted a model with MAP estimate = -120.1503
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.7443 - loglik: -1.2028e+02 - logprior: -1.5437e+01
Epoch 2/10
10/10 - 2s - loss: 124.0571 - loglik: -1.1883e+02 - logprior: -5.1079e+00
Epoch 3/10
10/10 - 2s - loss: 121.3588 - loglik: -1.1825e+02 - logprior: -2.8851e+00
Epoch 4/10
10/10 - 2s - loss: 119.5676 - loglik: -1.1712e+02 - logprior: -2.1724e+00
Epoch 5/10
10/10 - 2s - loss: 118.8928 - loglik: -1.1694e+02 - logprior: -1.6846e+00
Epoch 6/10
10/10 - 2s - loss: 118.4053 - loglik: -1.1662e+02 - logprior: -1.5700e+00
Epoch 7/10
10/10 - 2s - loss: 118.2407 - loglik: -1.1660e+02 - logprior: -1.4563e+00
Epoch 8/10
10/10 - 2s - loss: 118.2805 - loglik: -1.1673e+02 - logprior: -1.3871e+00
Fitted a model with MAP estimate = -117.9092
Time for alignment: 66.4291
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 197.8162 - loglik: -1.8321e+02 - logprior: -1.4600e+01
Epoch 2/10
10/10 - 2s - loss: 170.6696 - loglik: -1.6630e+02 - logprior: -4.3406e+00
Epoch 3/10
10/10 - 2s - loss: 153.0532 - loglik: -1.5019e+02 - logprior: -2.6993e+00
Epoch 4/10
10/10 - 2s - loss: 141.8963 - loglik: -1.3912e+02 - logprior: -2.5151e+00
Epoch 5/10
10/10 - 2s - loss: 137.1642 - loglik: -1.3414e+02 - logprior: -2.6705e+00
Epoch 6/10
10/10 - 2s - loss: 135.2972 - loglik: -1.3226e+02 - logprior: -2.7861e+00
Epoch 7/10
10/10 - 2s - loss: 134.1081 - loglik: -1.3119e+02 - logprior: -2.7127e+00
Epoch 8/10
10/10 - 2s - loss: 133.1870 - loglik: -1.3042e+02 - logprior: -2.5973e+00
Epoch 9/10
10/10 - 2s - loss: 132.6183 - loglik: -1.2988e+02 - logprior: -2.5834e+00
Epoch 10/10
10/10 - 2s - loss: 131.5067 - loglik: -1.2877e+02 - logprior: -2.5992e+00
Fitted a model with MAP estimate = -131.7498
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 149.2336 - loglik: -1.3278e+02 - logprior: -1.6433e+01
Epoch 2/2
10/10 - 2s - loss: 129.5517 - loglik: -1.2225e+02 - logprior: -7.2016e+00
Fitted a model with MAP estimate = -125.9316
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 132.3925 - loglik: -1.1935e+02 - logprior: -1.3016e+01
Epoch 2/2
10/10 - 2s - loss: 121.8069 - loglik: -1.1789e+02 - logprior: -3.8183e+00
Fitted a model with MAP estimate = -120.2058
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.6316 - loglik: -1.2019e+02 - logprior: -1.5419e+01
Epoch 2/10
10/10 - 2s - loss: 124.5558 - loglik: -1.1940e+02 - logprior: -5.0770e+00
Epoch 3/10
10/10 - 2s - loss: 120.8615 - loglik: -1.1781e+02 - logprior: -2.8824e+00
Epoch 4/10
10/10 - 2s - loss: 119.4818 - loglik: -1.1704e+02 - logprior: -2.1714e+00
Epoch 5/10
10/10 - 2s - loss: 119.0481 - loglik: -1.1710e+02 - logprior: -1.6848e+00
Epoch 6/10
10/10 - 2s - loss: 118.6535 - loglik: -1.1686e+02 - logprior: -1.5758e+00
Epoch 7/10
10/10 - 2s - loss: 118.0935 - loglik: -1.1646e+02 - logprior: -1.4574e+00
Epoch 8/10
10/10 - 2s - loss: 118.4469 - loglik: -1.1690e+02 - logprior: -1.3863e+00
Fitted a model with MAP estimate = -117.9171
Time for alignment: 68.5933
Computed alignments with likelihoods: ['-117.7319', '-117.9092', '-117.9171']
Best model has likelihood: -117.7319
time for generating output: 0.3238
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b08bab0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b002d27f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ab96dcdc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b088ea9d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3aa82a02e0>, <__main__.SimpleDirichletPrior object at 0x7f3b5d383820>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 552.0912 - loglik: -5.4031e+02 - logprior: -1.1751e+01
Epoch 2/10
11/11 - 5s - loss: 506.8218 - loglik: -5.0393e+02 - logprior: -2.6124e+00
Epoch 3/10
11/11 - 5s - loss: 465.9116 - loglik: -4.6364e+02 - logprior: -1.4985e+00
Epoch 4/10
11/11 - 5s - loss: 443.9222 - loglik: -4.4116e+02 - logprior: -1.5842e+00
Epoch 5/10
11/11 - 5s - loss: 433.4849 - loglik: -4.3034e+02 - logprior: -1.7774e+00
Epoch 6/10
11/11 - 5s - loss: 429.2151 - loglik: -4.2591e+02 - logprior: -1.7551e+00
Epoch 7/10
11/11 - 5s - loss: 426.1877 - loglik: -4.2292e+02 - logprior: -1.7101e+00
Epoch 8/10
11/11 - 5s - loss: 424.9538 - loglik: -4.2197e+02 - logprior: -1.6752e+00
Epoch 9/10
11/11 - 5s - loss: 424.9797 - loglik: -4.2223e+02 - logprior: -1.6933e+00
Fitted a model with MAP estimate = -422.5963
expansions: [(0, 8), (10, 3), (12, 1), (19, 1), (43, 1), (57, 2), (58, 2), (59, 1), (66, 1), (70, 1), (71, 3), (77, 1), (78, 1), (83, 1), (91, 1), (120, 3), (121, 2), (124, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 465.8312 - loglik: -4.5098e+02 - logprior: -1.4822e+01
Epoch 2/2
11/11 - 6s - loss: 432.8320 - loglik: -4.2820e+02 - logprior: -4.4108e+00
Fitted a model with MAP estimate = -424.9384
expansions: [(0, 16)]
discards: [  0   1   2   3   4   5   6   7  72 150 198 199 200 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 439.4169 - loglik: -4.2730e+02 - logprior: -1.2084e+01
Epoch 2/2
11/11 - 7s - loss: 425.4120 - loglik: -4.2230e+02 - logprior: -2.9134e+00
Fitted a model with MAP estimate = -420.3811
expansions: [(0, 15), (204, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 98]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 440.0189 - loglik: -4.2625e+02 - logprior: -1.3734e+01
Epoch 2/10
11/11 - 7s - loss: 423.1198 - loglik: -4.1919e+02 - logprior: -3.7236e+00
Epoch 3/10
11/11 - 7s - loss: 416.0567 - loglik: -4.1380e+02 - logprior: -1.7491e+00
Epoch 4/10
11/11 - 7s - loss: 411.2773 - loglik: -4.0922e+02 - logprior: -1.2334e+00
Epoch 5/10
11/11 - 7s - loss: 408.3594 - loglik: -4.0618e+02 - logprior: -1.1068e+00
Epoch 6/10
11/11 - 7s - loss: 406.2671 - loglik: -4.0419e+02 - logprior: -8.6152e-01
Epoch 7/10
11/11 - 7s - loss: 405.4868 - loglik: -4.0357e+02 - logprior: -6.9659e-01
Epoch 8/10
11/11 - 7s - loss: 404.1129 - loglik: -4.0231e+02 - logprior: -6.7764e-01
Epoch 9/10
11/11 - 7s - loss: 403.7916 - loglik: -4.0215e+02 - logprior: -6.3745e-01
Epoch 10/10
11/11 - 7s - loss: 403.4206 - loglik: -4.0196e+02 - logprior: -5.6396e-01
Fitted a model with MAP estimate = -401.9828
Time for alignment: 178.2386
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 552.0110 - loglik: -5.4023e+02 - logprior: -1.1744e+01
Epoch 2/10
11/11 - 5s - loss: 506.5659 - loglik: -5.0367e+02 - logprior: -2.6183e+00
Epoch 3/10
11/11 - 5s - loss: 467.9531 - loglik: -4.6557e+02 - logprior: -1.6153e+00
Epoch 4/10
11/11 - 5s - loss: 444.2178 - loglik: -4.4117e+02 - logprior: -1.8320e+00
Epoch 5/10
11/11 - 5s - loss: 434.0368 - loglik: -4.3027e+02 - logprior: -2.0676e+00
Epoch 6/10
11/11 - 5s - loss: 428.8665 - loglik: -4.2489e+02 - logprior: -2.0341e+00
Epoch 7/10
11/11 - 5s - loss: 424.9861 - loglik: -4.2132e+02 - logprior: -2.0063e+00
Epoch 8/10
11/11 - 5s - loss: 423.8339 - loglik: -4.2056e+02 - logprior: -2.0022e+00
Epoch 9/10
11/11 - 5s - loss: 421.1422 - loglik: -4.1817e+02 - logprior: -1.9883e+00
Epoch 10/10
11/11 - 5s - loss: 422.4099 - loglik: -4.1960e+02 - logprior: -2.0095e+00
Fitted a model with MAP estimate = -420.4751
expansions: [(0, 8), (9, 4), (15, 1), (33, 1), (45, 1), (53, 1), (55, 2), (56, 1), (58, 2), (65, 1), (70, 1), (71, 3), (76, 1), (77, 1), (82, 1), (107, 1), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (130, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 467.0766 - loglik: -4.5214e+02 - logprior: -1.4900e+01
Epoch 2/2
11/11 - 7s - loss: 431.6983 - loglik: -4.2698e+02 - logprior: -4.4926e+00
Fitted a model with MAP estimate = -424.0984
expansions: [(0, 18)]
discards: [  0   1   2   3   4   5   6   7  72  95 143 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 441.0359 - loglik: -4.2882e+02 - logprior: -1.2181e+01
Epoch 2/2
11/11 - 7s - loss: 424.7932 - loglik: -4.2152e+02 - logprior: -3.0665e+00
Fitted a model with MAP estimate = -419.8721
expansions: [(0, 15)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 87]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 10s - loss: 439.7915 - loglik: -4.2601e+02 - logprior: -1.3749e+01
Epoch 2/10
11/11 - 6s - loss: 424.8944 - loglik: -4.2119e+02 - logprior: -3.4984e+00
Epoch 3/10
11/11 - 6s - loss: 417.6973 - loglik: -4.1576e+02 - logprior: -1.4020e+00
Epoch 4/10
11/11 - 6s - loss: 413.2954 - loglik: -4.1153e+02 - logprior: -8.1440e-01
Epoch 5/10
11/11 - 6s - loss: 409.0440 - loglik: -4.0712e+02 - logprior: -6.2032e-01
Epoch 6/10
11/11 - 6s - loss: 408.1294 - loglik: -4.0635e+02 - logprior: -3.2103e-01
Epoch 7/10
11/11 - 6s - loss: 405.6120 - loglik: -4.0399e+02 - logprior: -2.2325e-01
Epoch 8/10
11/11 - 6s - loss: 405.9690 - loglik: -4.0446e+02 - logprior: -2.6825e-01
Fitted a model with MAP estimate = -403.7966
Time for alignment: 164.7469
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 551.6141 - loglik: -5.3983e+02 - logprior: -1.1754e+01
Epoch 2/10
11/11 - 5s - loss: 506.4347 - loglik: -5.0354e+02 - logprior: -2.6162e+00
Epoch 3/10
11/11 - 5s - loss: 465.3401 - loglik: -4.6307e+02 - logprior: -1.5085e+00
Epoch 4/10
11/11 - 5s - loss: 444.4390 - loglik: -4.4172e+02 - logprior: -1.5569e+00
Epoch 5/10
11/11 - 5s - loss: 435.0311 - loglik: -4.3221e+02 - logprior: -1.6755e+00
Epoch 6/10
11/11 - 5s - loss: 429.4074 - loglik: -4.2669e+02 - logprior: -1.6823e+00
Epoch 7/10
11/11 - 5s - loss: 427.0417 - loglik: -4.2428e+02 - logprior: -1.6973e+00
Epoch 8/10
11/11 - 5s - loss: 425.4663 - loglik: -4.2278e+02 - logprior: -1.6464e+00
Epoch 9/10
11/11 - 5s - loss: 425.4426 - loglik: -4.2293e+02 - logprior: -1.5701e+00
Epoch 10/10
11/11 - 5s - loss: 424.5110 - loglik: -4.2211e+02 - logprior: -1.5659e+00
Fitted a model with MAP estimate = -423.3163
expansions: [(0, 9), (9, 3), (15, 1), (30, 1), (45, 1), (56, 2), (57, 1), (59, 1), (66, 1), (71, 1), (72, 2), (77, 4), (86, 1), (112, 1), (120, 2), (121, 3), (127, 1), (150, 3), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 466.6899 - loglik: -4.5178e+02 - logprior: -1.4876e+01
Epoch 2/2
11/11 - 7s - loss: 433.5245 - loglik: -4.2880e+02 - logprior: -4.5043e+00
Fitted a model with MAP estimate = -425.5476
expansions: [(0, 17), (187, 1)]
discards: [  0   1   2   3   4   5   6   7   8  72  93 150 151 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 10s - loss: 442.3477 - loglik: -4.3018e+02 - logprior: -1.2138e+01
Epoch 2/2
11/11 - 7s - loss: 428.9264 - loglik: -4.2572e+02 - logprior: -3.0147e+00
Fitted a model with MAP estimate = -421.7246
expansions: [(0, 15), (26, 2), (156, 1), (202, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 440.8069 - loglik: -4.2707e+02 - logprior: -1.3708e+01
Epoch 2/10
11/11 - 7s - loss: 423.6696 - loglik: -4.1983e+02 - logprior: -3.6180e+00
Epoch 3/10
11/11 - 7s - loss: 417.0894 - loglik: -4.1491e+02 - logprior: -1.5776e+00
Epoch 4/10
11/11 - 7s - loss: 410.0581 - loglik: -4.0800e+02 - logprior: -9.7948e-01
Epoch 5/10
11/11 - 7s - loss: 409.6669 - loglik: -4.0734e+02 - logprior: -8.9962e-01
Epoch 6/10
11/11 - 7s - loss: 405.3164 - loglik: -4.0308e+02 - logprior: -7.1594e-01
Epoch 7/10
11/11 - 7s - loss: 406.5110 - loglik: -4.0458e+02 - logprior: -5.3676e-01
Fitted a model with MAP estimate = -403.4904
Time for alignment: 159.6615
Computed alignments with likelihoods: ['-401.9828', '-403.7966', '-403.4904']
Best model has likelihood: -401.9828
time for generating output: 0.2939
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.697121973503883
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3af731ce20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b3bf3f0d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b5d5f2580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3add9bcb50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3af73badc0>, <__main__.SimpleDirichletPrior object at 0x7f3b330dbe20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.1697 - loglik: -2.4869e+02 - logprior: -1.2060e+00
Epoch 2/10
29/29 - 4s - loss: 224.6443 - loglik: -2.2273e+02 - logprior: -8.8978e-01
Epoch 3/10
29/29 - 4s - loss: 219.3395 - loglik: -2.1743e+02 - logprior: -8.8916e-01
Epoch 4/10
29/29 - 4s - loss: 217.1460 - loglik: -2.1541e+02 - logprior: -9.3073e-01
Epoch 5/10
29/29 - 4s - loss: 216.1668 - loglik: -2.1443e+02 - logprior: -9.2629e-01
Epoch 6/10
29/29 - 4s - loss: 215.2069 - loglik: -2.1349e+02 - logprior: -9.2073e-01
Epoch 7/10
29/29 - 4s - loss: 215.1893 - loglik: -2.1342e+02 - logprior: -9.3356e-01
Epoch 8/10
29/29 - 4s - loss: 213.6993 - loglik: -2.1191e+02 - logprior: -9.3724e-01
Epoch 9/10
29/29 - 4s - loss: 213.9212 - loglik: -2.1228e+02 - logprior: -9.3749e-01
Fitted a model with MAP estimate = -202.4104
expansions: [(2, 1), (3, 1), (14, 3), (15, 2), (22, 1), (39, 2), (41, 1), (42, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 229.8721 - loglik: -2.2837e+02 - logprior: -1.2970e+00
Epoch 2/2
29/29 - 4s - loss: 215.6659 - loglik: -2.1396e+02 - logprior: -9.4104e-01
Fitted a model with MAP estimate = -195.2254
expansions: [(17, 1), (18, 2), (61, 1)]
discards: [ 3 15 19 21 22 48 62]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 217.8992 - loglik: -2.1660e+02 - logprior: -1.1926e+00
Epoch 2/2
29/29 - 4s - loss: 213.8112 - loglik: -2.1250e+02 - logprior: -8.4575e-01
Fitted a model with MAP estimate = -194.3887
expansions: [(17, 1)]
discards: [55]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 197.1662 - loglik: -1.9634e+02 - logprior: -7.3548e-01
Epoch 2/10
42/42 - 6s - loss: 194.6893 - loglik: -1.9380e+02 - logprior: -5.7164e-01
Epoch 3/10
42/42 - 6s - loss: 192.9101 - loglik: -1.9176e+02 - logprior: -5.6717e-01
Epoch 4/10
42/42 - 6s - loss: 192.2965 - loglik: -1.9091e+02 - logprior: -5.6177e-01
Epoch 5/10
42/42 - 6s - loss: 191.3879 - loglik: -1.8986e+02 - logprior: -5.5440e-01
Epoch 6/10
42/42 - 6s - loss: 190.9418 - loglik: -1.8935e+02 - logprior: -5.5187e-01
Epoch 7/10
42/42 - 6s - loss: 190.4263 - loglik: -1.8878e+02 - logprior: -5.4358e-01
Epoch 8/10
42/42 - 6s - loss: 189.1019 - loglik: -1.8734e+02 - logprior: -5.4607e-01
Epoch 9/10
42/42 - 6s - loss: 188.7323 - loglik: -1.8682e+02 - logprior: -5.4332e-01
Epoch 10/10
42/42 - 6s - loss: 187.7648 - loglik: -1.8606e+02 - logprior: -5.4054e-01
Fitted a model with MAP estimate = -186.1707
Time for alignment: 177.9616
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.3919 - loglik: -2.4792e+02 - logprior: -1.2055e+00
Epoch 2/10
29/29 - 4s - loss: 224.4042 - loglik: -2.2251e+02 - logprior: -9.0718e-01
Epoch 3/10
29/29 - 4s - loss: 219.3160 - loglik: -2.1730e+02 - logprior: -9.1808e-01
Epoch 4/10
29/29 - 4s - loss: 217.1597 - loglik: -2.1529e+02 - logprior: -9.3063e-01
Epoch 5/10
29/29 - 4s - loss: 216.4753 - loglik: -2.1461e+02 - logprior: -9.3670e-01
Epoch 6/10
29/29 - 4s - loss: 215.6388 - loglik: -2.1383e+02 - logprior: -9.3086e-01
Epoch 7/10
29/29 - 4s - loss: 214.3248 - loglik: -2.1254e+02 - logprior: -9.3269e-01
Epoch 8/10
29/29 - 4s - loss: 214.1645 - loglik: -2.1237e+02 - logprior: -9.3469e-01
Epoch 9/10
29/29 - 4s - loss: 213.4253 - loglik: -2.1176e+02 - logprior: -9.3125e-01
Epoch 10/10
29/29 - 4s - loss: 213.3753 - loglik: -2.1171e+02 - logprior: -9.3305e-01
Fitted a model with MAP estimate = -204.2835
expansions: [(2, 1), (3, 1), (14, 3), (25, 2), (38, 2), (41, 2), (43, 2), (44, 2), (45, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 232.7149 - loglik: -2.3119e+02 - logprior: -1.3068e+00
Epoch 2/2
29/29 - 4s - loss: 216.1645 - loglik: -2.1426e+02 - logprior: -9.6419e-01
Fitted a model with MAP estimate = -195.0199
expansions: [(14, 3), (15, 1), (18, 1)]
discards: [ 3 19 30 46 51 54 57]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 218.3949 - loglik: -2.1709e+02 - logprior: -1.1793e+00
Epoch 2/2
29/29 - 4s - loss: 214.1680 - loglik: -2.1297e+02 - logprior: -8.3100e-01
Fitted a model with MAP estimate = -194.8713
expansions: [(2, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 196.3992 - loglik: -1.9558e+02 - logprior: -7.3228e-01
Epoch 2/10
42/42 - 6s - loss: 194.2989 - loglik: -1.9338e+02 - logprior: -5.6725e-01
Epoch 3/10
42/42 - 6s - loss: 192.5078 - loglik: -1.9132e+02 - logprior: -5.6053e-01
Epoch 4/10
42/42 - 6s - loss: 192.0095 - loglik: -1.9059e+02 - logprior: -5.5403e-01
Epoch 5/10
42/42 - 6s - loss: 191.1721 - loglik: -1.8961e+02 - logprior: -5.4551e-01
Epoch 6/10
42/42 - 6s - loss: 190.6851 - loglik: -1.8908e+02 - logprior: -5.4366e-01
Epoch 7/10
42/42 - 6s - loss: 189.7972 - loglik: -1.8813e+02 - logprior: -5.3856e-01
Epoch 8/10
42/42 - 6s - loss: 188.7542 - loglik: -1.8694e+02 - logprior: -5.4549e-01
Epoch 9/10
42/42 - 6s - loss: 188.2372 - loglik: -1.8627e+02 - logprior: -5.3840e-01
Epoch 10/10
42/42 - 6s - loss: 187.4504 - loglik: -1.8573e+02 - logprior: -5.3615e-01
Fitted a model with MAP estimate = -185.6226
Time for alignment: 182.1607
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.0139 - loglik: -2.4854e+02 - logprior: -1.2049e+00
Epoch 2/10
29/29 - 4s - loss: 224.1823 - loglik: -2.2229e+02 - logprior: -8.8826e-01
Epoch 3/10
29/29 - 4s - loss: 219.2709 - loglik: -2.1735e+02 - logprior: -8.9287e-01
Epoch 4/10
29/29 - 4s - loss: 217.1336 - loglik: -2.1538e+02 - logprior: -9.2251e-01
Epoch 5/10
29/29 - 4s - loss: 216.5544 - loglik: -2.1484e+02 - logprior: -9.1999e-01
Epoch 6/10
29/29 - 4s - loss: 215.3170 - loglik: -2.1359e+02 - logprior: -9.1322e-01
Epoch 7/10
29/29 - 4s - loss: 215.0654 - loglik: -2.1338e+02 - logprior: -9.0846e-01
Epoch 8/10
29/29 - 4s - loss: 214.4284 - loglik: -2.1267e+02 - logprior: -9.1752e-01
Epoch 9/10
29/29 - 4s - loss: 213.6688 - loglik: -2.1199e+02 - logprior: -9.3158e-01
Epoch 10/10
29/29 - 4s - loss: 213.5781 - loglik: -2.1192e+02 - logprior: -9.3530e-01
Fitted a model with MAP estimate = -204.0991
expansions: [(2, 1), (15, 2), (22, 1), (27, 2), (38, 2), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 232.2581 - loglik: -2.3075e+02 - logprior: -1.2991e+00
Epoch 2/2
29/29 - 4s - loss: 216.4987 - loglik: -2.1473e+02 - logprior: -9.5963e-01
Fitted a model with MAP estimate = -195.5449
expansions: [(4, 1), (13, 2), (14, 2), (17, 1), (58, 1)]
discards: [18 31 45 54 59]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 218.3588 - loglik: -2.1706e+02 - logprior: -1.1973e+00
Epoch 2/2
29/29 - 4s - loss: 214.2065 - loglik: -2.1317e+02 - logprior: -8.2612e-01
Fitted a model with MAP estimate = -194.9954
expansions: [(18, 1)]
discards: [20]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 10s - loss: 196.2891 - loglik: -1.9547e+02 - logprior: -7.3670e-01
Epoch 2/10
42/42 - 6s - loss: 194.4392 - loglik: -1.9350e+02 - logprior: -5.6794e-01
Epoch 3/10
42/42 - 6s - loss: 192.2653 - loglik: -1.9104e+02 - logprior: -5.5994e-01
Epoch 4/10
42/42 - 6s - loss: 192.1053 - loglik: -1.9065e+02 - logprior: -5.5596e-01
Epoch 5/10
42/42 - 6s - loss: 191.4839 - loglik: -1.8990e+02 - logprior: -5.4967e-01
Epoch 6/10
42/42 - 6s - loss: 190.4374 - loglik: -1.8881e+02 - logprior: -5.4299e-01
Epoch 7/10
42/42 - 6s - loss: 189.7235 - loglik: -1.8804e+02 - logprior: -5.4040e-01
Epoch 8/10
42/42 - 6s - loss: 188.8573 - loglik: -1.8703e+02 - logprior: -5.3890e-01
Epoch 9/10
42/42 - 6s - loss: 188.2875 - loglik: -1.8630e+02 - logprior: -5.3649e-01
Epoch 10/10
42/42 - 6s - loss: 187.4006 - loglik: -1.8569e+02 - logprior: -5.3348e-01
Fitted a model with MAP estimate = -185.6455
Time for alignment: 180.8919
Computed alignments with likelihoods: ['-186.1707', '-185.6226', '-185.6455']
Best model has likelihood: -185.6226
time for generating output: 0.2121
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6055363321799307
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3af7909610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af7545430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7545700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b4ce629d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b08dd70d0>, <__main__.SimpleDirichletPrior object at 0x7f3b4cf74e20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.8887 - loglik: -3.2996e+02 - logprior: -4.6907e+01
Epoch 2/10
10/10 - 2s - loss: 300.8394 - loglik: -2.8836e+02 - logprior: -1.2465e+01
Epoch 3/10
10/10 - 1s - loss: 254.4788 - loglik: -2.4789e+02 - logprior: -6.5721e+00
Epoch 4/10
10/10 - 1s - loss: 228.0892 - loglik: -2.2344e+02 - logprior: -4.6412e+00
Epoch 5/10
10/10 - 1s - loss: 214.0143 - loglik: -2.1030e+02 - logprior: -3.6862e+00
Epoch 6/10
10/10 - 1s - loss: 208.1261 - loglik: -2.0445e+02 - logprior: -3.4579e+00
Epoch 7/10
10/10 - 1s - loss: 205.9171 - loglik: -2.0216e+02 - logprior: -3.2744e+00
Epoch 8/10
10/10 - 1s - loss: 204.8297 - loglik: -2.0116e+02 - logprior: -3.1153e+00
Epoch 9/10
10/10 - 1s - loss: 204.4988 - loglik: -2.0104e+02 - logprior: -2.9875e+00
Epoch 10/10
10/10 - 1s - loss: 203.5080 - loglik: -2.0021e+02 - logprior: -2.8901e+00
Fitted a model with MAP estimate = -203.0585
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (37, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (67, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 253.3712 - loglik: -1.9980e+02 - logprior: -5.3546e+01
Epoch 2/2
10/10 - 2s - loss: 210.1615 - loglik: -1.8749e+02 - logprior: -2.2505e+01
Fitted a model with MAP estimate = -202.1480
expansions: [(0, 3)]
discards: [ 0 10 20]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.9149 - loglik: -1.8429e+02 - logprior: -4.2604e+01
Epoch 2/2
10/10 - 2s - loss: 192.6717 - loglik: -1.8104e+02 - logprior: -1.1476e+01
Fitted a model with MAP estimate = -186.5930
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 234.9844 - loglik: -1.8362e+02 - logprior: -5.1342e+01
Epoch 2/10
10/10 - 2s - loss: 199.2332 - loglik: -1.8220e+02 - logprior: -1.6885e+01
Epoch 3/10
10/10 - 2s - loss: 187.1869 - loglik: -1.8038e+02 - logprior: -6.4425e+00
Epoch 4/10
10/10 - 2s - loss: 182.1514 - loglik: -1.7884e+02 - logprior: -2.8523e+00
Epoch 5/10
10/10 - 2s - loss: 180.0069 - loglik: -1.7818e+02 - logprior: -1.4108e+00
Epoch 6/10
10/10 - 2s - loss: 178.6830 - loglik: -1.7765e+02 - logprior: -6.4704e-01
Epoch 7/10
10/10 - 2s - loss: 178.5738 - loglik: -1.7816e+02 - logprior: -1.6898e-02
Epoch 8/10
10/10 - 2s - loss: 177.5838 - loglik: -1.7765e+02 - logprior: 0.4887
Epoch 9/10
10/10 - 2s - loss: 177.1973 - loglik: -1.7756e+02 - logprior: 0.7998
Epoch 10/10
10/10 - 2s - loss: 176.8490 - loglik: -1.7744e+02 - logprior: 1.0278
Fitted a model with MAP estimate = -176.0964
Time for alignment: 56.9237
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 376.8317 - loglik: -3.2990e+02 - logprior: -4.6908e+01
Epoch 2/10
10/10 - 1s - loss: 300.2762 - loglik: -2.8780e+02 - logprior: -1.2461e+01
Epoch 3/10
10/10 - 1s - loss: 252.5983 - loglik: -2.4596e+02 - logprior: -6.6272e+00
Epoch 4/10
10/10 - 1s - loss: 225.4472 - loglik: -2.2057e+02 - logprior: -4.8635e+00
Epoch 5/10
10/10 - 1s - loss: 213.3018 - loglik: -2.0928e+02 - logprior: -4.0169e+00
Epoch 6/10
10/10 - 1s - loss: 208.2790 - loglik: -2.0445e+02 - logprior: -3.7194e+00
Epoch 7/10
10/10 - 1s - loss: 204.7921 - loglik: -2.0082e+02 - logprior: -3.6051e+00
Epoch 8/10
10/10 - 1s - loss: 202.6921 - loglik: -1.9852e+02 - logprior: -3.6499e+00
Epoch 9/10
10/10 - 2s - loss: 201.6735 - loglik: -1.9763e+02 - logprior: -3.5545e+00
Epoch 10/10
10/10 - 1s - loss: 201.0530 - loglik: -1.9723e+02 - logprior: -3.4176e+00
Fitted a model with MAP estimate = -200.5846
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 1), (55, 1), (56, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.2177 - loglik: -1.9870e+02 - logprior: -5.3494e+01
Epoch 2/2
10/10 - 2s - loss: 207.9590 - loglik: -1.8535e+02 - logprior: -2.2442e+01
Fitted a model with MAP estimate = -200.2257
expansions: [(0, 3)]
discards: [  0   9  20 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.8822 - loglik: -1.8530e+02 - logprior: -4.2556e+01
Epoch 2/2
10/10 - 2s - loss: 193.4785 - loglik: -1.8184e+02 - logprior: -1.1495e+01
Fitted a model with MAP estimate = -187.6047
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.8103 - loglik: -1.8447e+02 - logprior: -5.1322e+01
Epoch 2/10
10/10 - 2s - loss: 199.5088 - loglik: -1.8262e+02 - logprior: -1.6773e+01
Epoch 3/10
10/10 - 2s - loss: 187.7710 - loglik: -1.8107e+02 - logprior: -6.4012e+00
Epoch 4/10
10/10 - 2s - loss: 182.8944 - loglik: -1.7962e+02 - logprior: -2.8508e+00
Epoch 5/10
10/10 - 2s - loss: 180.7897 - loglik: -1.7897e+02 - logprior: -1.4154e+00
Epoch 6/10
10/10 - 2s - loss: 179.6072 - loglik: -1.7857e+02 - logprior: -6.4180e-01
Epoch 7/10
10/10 - 2s - loss: 179.0186 - loglik: -1.7861e+02 - logprior: -6.2511e-03
Epoch 8/10
10/10 - 2s - loss: 178.2568 - loglik: -1.7833e+02 - logprior: 0.4848
Epoch 9/10
10/10 - 2s - loss: 177.9908 - loglik: -1.7836e+02 - logprior: 0.8022
Epoch 10/10
10/10 - 2s - loss: 177.3999 - loglik: -1.7798e+02 - logprior: 1.0333
Fitted a model with MAP estimate = -176.8830
Time for alignment: 56.0943
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 377.2655 - loglik: -3.3034e+02 - logprior: -4.6907e+01
Epoch 2/10
10/10 - 1s - loss: 299.7877 - loglik: -2.8731e+02 - logprior: -1.2464e+01
Epoch 3/10
10/10 - 1s - loss: 253.2594 - loglik: -2.4661e+02 - logprior: -6.6360e+00
Epoch 4/10
10/10 - 1s - loss: 225.4198 - loglik: -2.2046e+02 - logprior: -4.9477e+00
Epoch 5/10
10/10 - 1s - loss: 212.1521 - loglik: -2.0795e+02 - logprior: -4.1983e+00
Epoch 6/10
10/10 - 1s - loss: 206.4454 - loglik: -2.0245e+02 - logprior: -3.9038e+00
Epoch 7/10
10/10 - 2s - loss: 205.0359 - loglik: -2.0107e+02 - logprior: -3.6889e+00
Epoch 8/10
10/10 - 1s - loss: 203.6100 - loglik: -1.9981e+02 - logprior: -3.4285e+00
Epoch 9/10
10/10 - 1s - loss: 203.2269 - loglik: -1.9961e+02 - logprior: -3.2699e+00
Epoch 10/10
10/10 - 1s - loss: 202.7280 - loglik: -1.9924e+02 - logprior: -3.1881e+00
Fitted a model with MAP estimate = -202.3035
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.8036 - loglik: -1.9926e+02 - logprior: -5.3521e+01
Epoch 2/2
10/10 - 2s - loss: 208.8912 - loglik: -1.8635e+02 - logprior: -2.2386e+01
Fitted a model with MAP estimate = -201.0276
expansions: [(0, 3)]
discards: [  0   9 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.7651 - loglik: -1.8618e+02 - logprior: -4.2559e+01
Epoch 2/2
10/10 - 2s - loss: 193.5483 - loglik: -1.8190e+02 - logprior: -1.1516e+01
Fitted a model with MAP estimate = -187.8587
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.4637 - loglik: -1.8490e+02 - logprior: -5.1549e+01
Epoch 2/10
10/10 - 2s - loss: 200.2488 - loglik: -1.8265e+02 - logprior: -1.7507e+01
Epoch 3/10
10/10 - 2s - loss: 187.9246 - loglik: -1.8095e+02 - logprior: -6.7064e+00
Epoch 4/10
10/10 - 2s - loss: 182.6483 - loglik: -1.7944e+02 - logprior: -2.8405e+00
Epoch 5/10
10/10 - 2s - loss: 180.2478 - loglik: -1.7850e+02 - logprior: -1.3995e+00
Epoch 6/10
10/10 - 2s - loss: 179.2659 - loglik: -1.7822e+02 - logprior: -6.9357e-01
Epoch 7/10
10/10 - 2s - loss: 178.2780 - loglik: -1.7785e+02 - logprior: -6.5864e-02
Epoch 8/10
10/10 - 2s - loss: 178.0701 - loglik: -1.7816e+02 - logprior: 0.4558
Epoch 9/10
10/10 - 2s - loss: 177.2580 - loglik: -1.7769e+02 - logprior: 0.8011
Epoch 10/10
10/10 - 2s - loss: 177.3642 - loglik: -1.7805e+02 - logprior: 1.0508
Fitted a model with MAP estimate = -176.5609
Time for alignment: 55.9882
Computed alignments with likelihoods: ['-176.0964', '-176.8830', '-176.5609']
Best model has likelihood: -176.0964
time for generating output: 0.1728
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9204298680327413
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3ad3b582b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b089aa670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3be9fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad38801c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b19d04e50>, <__main__.SimpleDirichletPrior object at 0x7f3aca67c9a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 645.6048 - loglik: -6.1498e+02 - logprior: -3.0604e+01
Epoch 2/10
10/10 - 5s - loss: 578.2804 - loglik: -5.7121e+02 - logprior: -6.9598e+00
Epoch 3/10
10/10 - 5s - loss: 523.6554 - loglik: -5.2027e+02 - logprior: -3.3103e+00
Epoch 4/10
10/10 - 5s - loss: 482.0316 - loglik: -4.7865e+02 - logprior: -3.2883e+00
Epoch 5/10
10/10 - 5s - loss: 466.2296 - loglik: -4.6228e+02 - logprior: -3.5923e+00
Epoch 6/10
10/10 - 5s - loss: 459.6589 - loglik: -4.5550e+02 - logprior: -3.5055e+00
Epoch 7/10
10/10 - 5s - loss: 457.3503 - loglik: -4.5337e+02 - logprior: -3.2591e+00
Epoch 8/10
10/10 - 5s - loss: 456.5975 - loglik: -4.5274e+02 - logprior: -3.1804e+00
Epoch 9/10
10/10 - 5s - loss: 455.2238 - loglik: -4.5139e+02 - logprior: -3.1669e+00
Epoch 10/10
10/10 - 5s - loss: 454.6984 - loglik: -4.5092e+02 - logprior: -3.1227e+00
Fitted a model with MAP estimate = -453.6376
expansions: [(14, 1), (16, 1), (28, 1), (30, 1), (31, 2), (32, 1), (36, 1), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (54, 3), (61, 1), (62, 1), (80, 1), (90, 1), (110, 1), (114, 3), (115, 2), (116, 2), (118, 2), (119, 2), (120, 3), (130, 2), (160, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 495.5312 - loglik: -4.6026e+02 - logprior: -3.5239e+01
Epoch 2/2
10/10 - 7s - loss: 455.8154 - loglik: -4.4188e+02 - logprior: -1.3743e+01
Fitted a model with MAP estimate = -449.0871
expansions: [(0, 3), (35, 1)]
discards: [  0  49 149 166]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 469.4182 - loglik: -4.4205e+02 - logprior: -2.7343e+01
Epoch 2/2
10/10 - 7s - loss: 442.3780 - loglik: -4.3635e+02 - logprior: -5.8551e+00
Fitted a model with MAP estimate = -437.2176
expansions: [(70, 2)]
discards: [  0   1 153 206]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 472.7784 - loglik: -4.3928e+02 - logprior: -3.3476e+01
Epoch 2/10
10/10 - 7s - loss: 447.3223 - loglik: -4.3537e+02 - logprior: -1.1783e+01
Epoch 3/10
10/10 - 7s - loss: 437.3339 - loglik: -4.3211e+02 - logprior: -4.8291e+00
Epoch 4/10
10/10 - 7s - loss: 431.8685 - loglik: -4.3085e+02 - logprior: -4.2924e-01
Epoch 5/10
10/10 - 7s - loss: 430.1758 - loglik: -4.3039e+02 - logprior: 0.8693
Epoch 6/10
10/10 - 7s - loss: 428.3654 - loglik: -4.2909e+02 - logprior: 1.3503
Epoch 7/10
10/10 - 7s - loss: 428.4896 - loglik: -4.2971e+02 - logprior: 1.7905
Fitted a model with MAP estimate = -426.8240
Time for alignment: 162.4643
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 646.7967 - loglik: -6.1616e+02 - logprior: -3.0611e+01
Epoch 2/10
10/10 - 5s - loss: 576.3934 - loglik: -5.6932e+02 - logprior: -6.9630e+00
Epoch 3/10
10/10 - 5s - loss: 517.5134 - loglik: -5.1414e+02 - logprior: -3.2901e+00
Epoch 4/10
10/10 - 5s - loss: 477.0830 - loglik: -4.7371e+02 - logprior: -3.1865e+00
Epoch 5/10
10/10 - 5s - loss: 462.5498 - loglik: -4.5891e+02 - logprior: -3.0963e+00
Epoch 6/10
10/10 - 5s - loss: 458.8233 - loglik: -4.5510e+02 - logprior: -2.9161e+00
Epoch 7/10
10/10 - 5s - loss: 455.2572 - loglik: -4.5170e+02 - logprior: -2.7014e+00
Epoch 8/10
10/10 - 5s - loss: 454.2636 - loglik: -4.5084e+02 - logprior: -2.6337e+00
Epoch 9/10
10/10 - 5s - loss: 454.3992 - loglik: -4.5105e+02 - logprior: -2.6172e+00
Fitted a model with MAP estimate = -452.6620
expansions: [(14, 1), (15, 1), (28, 1), (29, 2), (30, 2), (31, 1), (40, 2), (41, 2), (42, 1), (52, 1), (53, 1), (80, 1), (90, 1), (110, 1), (112, 1), (114, 2), (115, 3), (116, 2), (118, 1), (119, 1), (120, 3), (130, 2), (160, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 494.9772 - loglik: -4.5987e+02 - logprior: -3.5083e+01
Epoch 2/2
10/10 - 7s - loss: 457.6851 - loglik: -4.4408e+02 - logprior: -1.3525e+01
Fitted a model with MAP estimate = -450.9153
expansions: [(35, 1)]
discards: [  0  48 161]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 477.8442 - loglik: -4.4393e+02 - logprior: -3.3889e+01
Epoch 2/2
10/10 - 7s - loss: 451.1898 - loglik: -4.3852e+02 - logprior: -1.2477e+01
Fitted a model with MAP estimate = -445.7611
expansions: [(0, 4), (68, 4)]
discards: [  0 146 199]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 466.4625 - loglik: -4.3971e+02 - logprior: -2.6726e+01
Epoch 2/10
10/10 - 7s - loss: 440.3875 - loglik: -4.3462e+02 - logprior: -5.5921e+00
Epoch 3/10
10/10 - 7s - loss: 434.2903 - loglik: -4.3239e+02 - logprior: -1.4841e+00
Epoch 4/10
10/10 - 7s - loss: 430.3791 - loglik: -4.2990e+02 - logprior: 0.1615
Epoch 5/10
10/10 - 7s - loss: 429.5074 - loglik: -4.2985e+02 - logprior: 1.0775
Epoch 6/10
10/10 - 7s - loss: 428.5005 - loglik: -4.2932e+02 - logprior: 1.5396
Epoch 7/10
10/10 - 7s - loss: 427.6747 - loglik: -4.2879e+02 - logprior: 1.7946
Epoch 8/10
10/10 - 7s - loss: 426.8282 - loglik: -4.2817e+02 - logprior: 1.9756
Epoch 9/10
10/10 - 7s - loss: 424.8587 - loglik: -4.2641e+02 - logprior: 2.1406
Epoch 10/10
10/10 - 7s - loss: 427.2324 - loglik: -4.2900e+02 - logprior: 2.3326
Fitted a model with MAP estimate = -425.0230
Time for alignment: 177.4713
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 647.2742 - loglik: -6.1665e+02 - logprior: -3.0602e+01
Epoch 2/10
10/10 - 5s - loss: 578.1364 - loglik: -5.7106e+02 - logprior: -6.9589e+00
Epoch 3/10
10/10 - 5s - loss: 521.2441 - loglik: -5.1788e+02 - logprior: -3.2852e+00
Epoch 4/10
10/10 - 5s - loss: 480.9845 - loglik: -4.7755e+02 - logprior: -3.3280e+00
Epoch 5/10
10/10 - 5s - loss: 465.9506 - loglik: -4.6213e+02 - logprior: -3.4365e+00
Epoch 6/10
10/10 - 5s - loss: 462.0850 - loglik: -4.5815e+02 - logprior: -3.2647e+00
Epoch 7/10
10/10 - 5s - loss: 459.1231 - loglik: -4.5541e+02 - logprior: -2.9767e+00
Epoch 8/10
10/10 - 5s - loss: 457.8318 - loglik: -4.5429e+02 - logprior: -2.8422e+00
Epoch 9/10
10/10 - 5s - loss: 457.2157 - loglik: -4.5369e+02 - logprior: -2.8595e+00
Epoch 10/10
10/10 - 5s - loss: 455.2922 - loglik: -4.5182e+02 - logprior: -2.8350e+00
Fitted a model with MAP estimate = -455.1778
expansions: [(14, 1), (16, 1), (28, 1), (30, 1), (31, 2), (32, 1), (37, 1), (42, 2), (43, 1), (52, 1), (53, 3), (54, 3), (65, 1), (80, 1), (90, 1), (110, 1), (114, 2), (115, 2), (116, 2), (118, 2), (119, 2), (120, 3), (130, 2), (154, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 497.5911 - loglik: -4.6228e+02 - logprior: -3.5279e+01
Epoch 2/2
10/10 - 7s - loss: 457.3720 - loglik: -4.4328e+02 - logprior: -1.3900e+01
Fitted a model with MAP estimate = -449.3008
expansions: [(0, 3), (69, 2), (137, 1)]
discards: [  0  50  89 148 149]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 470.4092 - loglik: -4.4291e+02 - logprior: -2.7476e+01
Epoch 2/2
10/10 - 7s - loss: 442.8927 - loglik: -4.3664e+02 - logprior: -6.0755e+00
Fitted a model with MAP estimate = -437.1721
expansions: []
discards: [  0   1  74  75  76 166 206]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 474.5753 - loglik: -4.4101e+02 - logprior: -3.3543e+01
Epoch 2/10
10/10 - 7s - loss: 451.3329 - loglik: -4.3950e+02 - logprior: -1.1664e+01
Epoch 3/10
10/10 - 7s - loss: 440.1370 - loglik: -4.3531e+02 - logprior: -4.4287e+00
Epoch 4/10
10/10 - 7s - loss: 433.7223 - loglik: -4.3278e+02 - logprior: -3.4698e-01
Epoch 5/10
10/10 - 7s - loss: 431.7265 - loglik: -4.3185e+02 - logprior: 0.7926
Epoch 6/10
10/10 - 7s - loss: 430.2272 - loglik: -4.3083e+02 - logprior: 1.2445
Epoch 7/10
10/10 - 7s - loss: 428.9229 - loglik: -4.3001e+02 - logprior: 1.6706
Epoch 8/10
10/10 - 7s - loss: 427.8156 - loglik: -4.2925e+02 - logprior: 1.9788
Epoch 9/10
10/10 - 7s - loss: 427.6039 - loglik: -4.2924e+02 - logprior: 2.1650
Epoch 10/10
10/10 - 7s - loss: 428.1496 - loglik: -4.2994e+02 - logprior: 2.3161
Fitted a model with MAP estimate = -426.5523
Time for alignment: 180.8382
Computed alignments with likelihoods: ['-426.8240', '-425.0230', '-426.5523']
Best model has likelihood: -425.0230
time for generating output: 0.3020
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8898963730569949
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b117be160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b4caa77c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4caa71c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ab97b28e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b65bc6940>, <__main__.SimpleDirichletPrior object at 0x7f3b3bb1b370>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 740.8940 - loglik: -6.9559e+02 - logprior: -4.5269e+01
Epoch 2/10
11/11 - 10s - loss: 634.8018 - loglik: -6.2715e+02 - logprior: -7.5396e+00
Epoch 3/10
11/11 - 11s - loss: 543.4676 - loglik: -5.3994e+02 - logprior: -3.4672e+00
Epoch 4/10
11/11 - 11s - loss: 493.6144 - loglik: -4.8994e+02 - logprior: -3.5505e+00
Epoch 5/10
11/11 - 11s - loss: 476.6911 - loglik: -4.7235e+02 - logprior: -3.9147e+00
Epoch 6/10
11/11 - 11s - loss: 470.0151 - loglik: -4.6587e+02 - logprior: -3.5299e+00
Epoch 7/10
11/11 - 10s - loss: 469.2329 - loglik: -4.6558e+02 - logprior: -3.1328e+00
Epoch 8/10
11/11 - 11s - loss: 467.4253 - loglik: -4.6415e+02 - logprior: -2.8748e+00
Epoch 9/10
11/11 - 12s - loss: 465.8701 - loglik: -4.6293e+02 - logprior: -2.5830e+00
Epoch 10/10
11/11 - 11s - loss: 465.8187 - loglik: -4.6315e+02 - logprior: -2.3073e+00
Fitted a model with MAP estimate = -465.1704
expansions: [(20, 1), (21, 2), (22, 2), (25, 1), (37, 1), (48, 1), (50, 1), (52, 2), (61, 1), (63, 1), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (93, 1), (103, 3), (104, 1), (105, 2), (106, 1), (108, 4), (129, 1), (161, 2), (163, 4), (182, 1), (183, 1), (184, 2), (198, 1), (199, 6), (201, 2), (202, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 19s - loss: 518.0124 - loglik: -4.6489e+02 - logprior: -5.3095e+01
Epoch 2/2
11/11 - 14s - loss: 459.7222 - loglik: -4.4088e+02 - logprior: -1.8643e+01
Fitted a model with MAP estimate = -448.8295
expansions: [(0, 3), (242, 1), (247, 1)]
discards: [  0  20  60 121 122 132 133]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 479.6894 - loglik: -4.3893e+02 - logprior: -4.0732e+01
Epoch 2/2
11/11 - 14s - loss: 439.1884 - loglik: -4.3325e+02 - logprior: -5.8110e+00
Fitted a model with MAP estimate = -431.0795
expansions: [(196, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 19s - loss: 485.8422 - loglik: -4.3543e+02 - logprior: -5.0399e+01
Epoch 2/10
11/11 - 15s - loss: 445.9541 - loglik: -4.3100e+02 - logprior: -1.4899e+01
Epoch 3/10
11/11 - 13s - loss: 433.8316 - loglik: -4.3110e+02 - logprior: -2.5544e+00
Epoch 4/10
11/11 - 13s - loss: 423.2421 - loglik: -4.2692e+02 - logprior: 4.0907
Epoch 5/10
11/11 - 15s - loss: 418.3663 - loglik: -4.2388e+02 - logprior: 6.0486
Epoch 6/10
11/11 - 14s - loss: 417.1676 - loglik: -4.2368e+02 - logprior: 6.9830
Epoch 7/10
11/11 - 14s - loss: 416.4013 - loglik: -4.2380e+02 - logprior: 7.8238
Epoch 8/10
11/11 - 14s - loss: 417.1485 - loglik: -4.2527e+02 - logprior: 8.5798
Fitted a model with MAP estimate = -414.5808
Time for alignment: 314.5798
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 740.3558 - loglik: -6.9507e+02 - logprior: -4.5256e+01
Epoch 2/10
11/11 - 11s - loss: 636.0253 - loglik: -6.2832e+02 - logprior: -7.5917e+00
Epoch 3/10
11/11 - 9s - loss: 544.3100 - loglik: -5.4082e+02 - logprior: -3.4327e+00
Epoch 4/10
11/11 - 11s - loss: 495.9514 - loglik: -4.9274e+02 - logprior: -3.1309e+00
Epoch 5/10
11/11 - 11s - loss: 480.9059 - loglik: -4.7708e+02 - logprior: -3.6396e+00
Epoch 6/10
11/11 - 11s - loss: 475.8080 - loglik: -4.7195e+02 - logprior: -3.4523e+00
Epoch 7/10
11/11 - 11s - loss: 469.1559 - loglik: -4.6547e+02 - logprior: -3.2046e+00
Epoch 8/10
11/11 - 11s - loss: 470.6758 - loglik: -4.6724e+02 - logprior: -2.9595e+00
Fitted a model with MAP estimate = -469.0771
expansions: [(19, 4), (21, 1), (22, 1), (24, 1), (28, 1), (35, 1), (46, 1), (48, 1), (51, 2), (62, 4), (63, 1), (76, 1), (77, 2), (79, 1), (89, 1), (102, 3), (103, 1), (104, 1), (106, 1), (107, 1), (161, 2), (163, 5), (180, 1), (181, 1), (182, 3), (183, 1), (197, 3), (198, 1), (199, 2), (201, 2), (202, 5), (223, 1), (224, 1), (225, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 524.7833 - loglik: -4.7099e+02 - logprior: -5.3760e+01
Epoch 2/2
11/11 - 15s - loss: 462.6840 - loglik: -4.4345e+02 - logprior: -1.9028e+01
Fitted a model with MAP estimate = -454.2459
expansions: [(0, 3), (203, 2), (219, 1)]
discards: [  0  20  60  76  94 124 125 197 299 300 301]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 19s - loss: 490.1511 - loglik: -4.4919e+02 - logprior: -4.0939e+01
Epoch 2/2
11/11 - 14s - loss: 442.9838 - loglik: -4.3673e+02 - logprior: -6.1173e+00
Fitted a model with MAP estimate = -438.4788
expansions: [(246, 1), (297, 6)]
discards: [  0  76 198]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 19s - loss: 494.7117 - loglik: -4.4316e+02 - logprior: -5.1541e+01
Epoch 2/10
11/11 - 13s - loss: 453.5139 - loglik: -4.3767e+02 - logprior: -1.5785e+01
Epoch 3/10
11/11 - 14s - loss: 436.9761 - loglik: -4.3327e+02 - logprior: -3.5240e+00
Epoch 4/10
11/11 - 15s - loss: 430.0564 - loglik: -4.3266e+02 - logprior: 3.0341
Epoch 5/10
11/11 - 14s - loss: 423.7511 - loglik: -4.2875e+02 - logprior: 5.5392
Epoch 6/10
11/11 - 14s - loss: 424.0931 - loglik: -4.2958e+02 - logprior: 5.9564
Fitted a model with MAP estimate = -422.1792
Time for alignment: 262.3812
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 741.6793 - loglik: -6.9637e+02 - logprior: -4.5276e+01
Epoch 2/10
11/11 - 11s - loss: 637.1091 - loglik: -6.2947e+02 - logprior: -7.5113e+00
Epoch 3/10
11/11 - 12s - loss: 545.6412 - loglik: -5.4218e+02 - logprior: -3.3993e+00
Epoch 4/10
11/11 - 11s - loss: 496.5544 - loglik: -4.9319e+02 - logprior: -3.2821e+00
Epoch 5/10
11/11 - 10s - loss: 478.2267 - loglik: -4.7420e+02 - logprior: -3.7411e+00
Epoch 6/10
11/11 - 11s - loss: 473.4285 - loglik: -4.6948e+02 - logprior: -3.3882e+00
Epoch 7/10
11/11 - 10s - loss: 471.0564 - loglik: -4.6734e+02 - logprior: -3.1612e+00
Epoch 8/10
11/11 - 11s - loss: 469.7395 - loglik: -4.6647e+02 - logprior: -2.8430e+00
Epoch 9/10
11/11 - 12s - loss: 468.2091 - loglik: -4.6530e+02 - logprior: -2.5486e+00
Epoch 10/10
11/11 - 12s - loss: 466.2747 - loglik: -4.6356e+02 - logprior: -2.3437e+00
Fitted a model with MAP estimate = -466.1027
expansions: [(20, 1), (21, 3), (36, 1), (37, 1), (47, 1), (52, 1), (61, 1), (63, 1), (65, 1), (78, 1), (79, 1), (81, 1), (91, 1), (93, 1), (103, 5), (104, 1), (106, 2), (129, 1), (134, 1), (136, 1), (160, 1), (161, 1), (162, 5), (181, 1), (182, 1), (183, 2), (197, 1), (198, 6), (200, 2), (201, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0   1 209]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 519.1826 - loglik: -4.6605e+02 - logprior: -5.3103e+01
Epoch 2/2
11/11 - 13s - loss: 460.1900 - loglik: -4.4136e+02 - logprior: -1.8646e+01
Fitted a model with MAP estimate = -450.6672
expansions: [(0, 3), (238, 1), (243, 1)]
discards: [  0  21 119 120 126]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 479.7133 - loglik: -4.3891e+02 - logprior: -4.0790e+01
Epoch 2/2
11/11 - 15s - loss: 442.0256 - loglik: -4.3638e+02 - logprior: -5.5648e+00
Fitted a model with MAP estimate = -432.6850
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 17s - loss: 485.8827 - loglik: -4.3527e+02 - logprior: -5.0606e+01
Epoch 2/10
11/11 - 14s - loss: 450.5234 - loglik: -4.3514e+02 - logprior: -1.5333e+01
Epoch 3/10
11/11 - 13s - loss: 434.9805 - loglik: -4.3080e+02 - logprior: -4.0253e+00
Epoch 4/10
11/11 - 13s - loss: 423.0438 - loglik: -4.2641e+02 - logprior: 3.7241
Epoch 5/10
11/11 - 14s - loss: 423.4023 - loglik: -4.2893e+02 - logprior: 5.9937
Fitted a model with MAP estimate = -418.8085
Time for alignment: 268.2035
Computed alignments with likelihoods: ['-414.5808', '-422.1792', '-418.8085']
Best model has likelihood: -414.5808
time for generating output: 0.6299
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9227472782024554
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3ac2088a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af7a51cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b2ad83490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b3bf2c490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b110d5cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b19e43700>, <__main__.SimpleDirichletPrior object at 0x7f3ad32b7c10>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 22s - loss: 816.3643 - loglik: -8.0885e+02 - logprior: -7.3751e+00
Epoch 2/10
21/21 - 19s - loss: 695.7045 - loglik: -6.9282e+02 - logprior: -2.2585e+00
Epoch 3/10
21/21 - 19s - loss: 640.9441 - loglik: -6.3503e+02 - logprior: -4.6456e+00
Epoch 4/10
21/21 - 19s - loss: 635.7397 - loglik: -6.2979e+02 - logprior: -4.5179e+00
Epoch 5/10
21/21 - 19s - loss: 633.4393 - loglik: -6.2792e+02 - logprior: -4.4463e+00
Epoch 6/10
21/21 - 19s - loss: 628.6109 - loglik: -6.2340e+02 - logprior: -4.3618e+00
Epoch 7/10
21/21 - 19s - loss: 630.2300 - loglik: -6.2498e+02 - logprior: -4.5064e+00
Fitted a model with MAP estimate = -628.1481
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (49, 1), (51, 2), (53, 3), (54, 2), (59, 2), (60, 2), (61, 1), (63, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (79, 1), (80, 1), (81, 1), (83, 1), (85, 1), (86, 1), (91, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (117, 1), (130, 1), (134, 1), (135, 1), (138, 1), (141, 1), (144, 1), (153, 1), (154, 1), (157, 1), (158, 1), (161, 1), (162, 1), (171, 1), (181, 1), (184, 2), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (211, 1), (212, 1), (219, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (256, 1), (257, 3), (258, 2), (260, 2), (261, 1), (272, 2), (273, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 32s - loss: 640.6438 - loglik: -6.2999e+02 - logprior: -1.0557e+01
Epoch 2/2
21/21 - 27s - loss: 608.3224 - loglik: -6.0412e+02 - logprior: -3.7850e+00
Fitted a model with MAP estimate = -601.9419
expansions: [(0, 3), (148, 1)]
discards: [  0  57  80 335]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 31s - loss: 616.7319 - loglik: -6.0975e+02 - logprior: -6.8893e+00
Epoch 2/2
21/21 - 28s - loss: 599.5367 - loglik: -5.9887e+02 - logprior: -1.1135e-01
Fitted a model with MAP estimate = -596.7653
expansions: []
discards: [  1   2 355]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 31s - loss: 613.7843 - loglik: -6.0767e+02 - logprior: -6.0021e+00
Epoch 2/10
21/21 - 27s - loss: 601.8948 - loglik: -6.0169e+02 - logprior: 0.3220
Epoch 3/10
21/21 - 27s - loss: 594.3391 - loglik: -5.9414e+02 - logprior: 0.6965
Epoch 4/10
21/21 - 27s - loss: 595.6036 - loglik: -5.9572e+02 - logprior: 1.1440
Fitted a model with MAP estimate = -591.8513
Time for alignment: 434.1494
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 23s - loss: 816.6835 - loglik: -8.0915e+02 - logprior: -7.4027e+00
Epoch 2/10
21/21 - 19s - loss: 697.9833 - loglik: -6.9515e+02 - logprior: -2.1963e+00
Epoch 3/10
21/21 - 19s - loss: 642.1980 - loglik: -6.3621e+02 - logprior: -4.5904e+00
Epoch 4/10
21/21 - 19s - loss: 634.6091 - loglik: -6.2867e+02 - logprior: -4.5415e+00
Epoch 5/10
21/21 - 19s - loss: 632.6508 - loglik: -6.2713e+02 - logprior: -4.4716e+00
Epoch 6/10
21/21 - 19s - loss: 629.4038 - loglik: -6.2386e+02 - logprior: -4.6980e+00
Epoch 7/10
21/21 - 19s - loss: 627.2065 - loglik: -6.2170e+02 - logprior: -4.7592e+00
Epoch 8/10
21/21 - 19s - loss: 630.0609 - loglik: -6.2452e+02 - logprior: -4.8416e+00
Fitted a model with MAP estimate = -627.0114
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (54, 2), (56, 1), (61, 2), (62, 2), (64, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (111, 1), (113, 1), (115, 1), (117, 1), (131, 1), (136, 1), (139, 1), (142, 2), (144, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 2), (178, 1), (179, 1), (182, 1), (189, 1), (190, 1), (191, 1), (193, 1), (194, 1), (195, 1), (196, 1), (198, 1), (209, 1), (212, 1), (214, 1), (218, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (233, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 1), (261, 1), (272, 2), (273, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 31s - loss: 640.7432 - loglik: -6.3279e+02 - logprior: -7.8522e+00
Epoch 2/2
21/21 - 28s - loss: 604.9009 - loglik: -6.0339e+02 - logprior: -1.0473e+00
Fitted a model with MAP estimate = -599.4196
expansions: []
discards: [ 59 181 182]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 30s - loss: 615.4862 - loglik: -6.0862e+02 - logprior: -6.7697e+00
Epoch 2/2
21/21 - 27s - loss: 603.1300 - loglik: -6.0245e+02 - logprior: -1.3414e-01
Fitted a model with MAP estimate = -597.8887
expansions: [(180, 1)]
discards: [59]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 31s - loss: 612.7625 - loglik: -6.0660e+02 - logprior: -6.0505e+00
Epoch 2/10
21/21 - 27s - loss: 602.6345 - loglik: -6.0241e+02 - logprior: 0.2984
Epoch 3/10
21/21 - 27s - loss: 594.6387 - loglik: -5.9455e+02 - logprior: 0.8064
Epoch 4/10
21/21 - 27s - loss: 594.7845 - loglik: -5.9488e+02 - logprior: 1.1299
Fitted a model with MAP estimate = -591.7914
Time for alignment: 450.8754
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 22s - loss: 820.0859 - loglik: -8.1259e+02 - logprior: -7.3590e+00
Epoch 2/10
21/21 - 19s - loss: 695.1722 - loglik: -6.9241e+02 - logprior: -2.1500e+00
Epoch 3/10
21/21 - 19s - loss: 641.6505 - loglik: -6.3597e+02 - logprior: -4.7005e+00
Epoch 4/10
21/21 - 19s - loss: 634.9667 - loglik: -6.2905e+02 - logprior: -4.6839e+00
Epoch 5/10
21/21 - 19s - loss: 632.2015 - loglik: -6.2660e+02 - logprior: -4.5296e+00
Epoch 6/10
21/21 - 19s - loss: 628.8636 - loglik: -6.2347e+02 - logprior: -4.5053e+00
Epoch 7/10
21/21 - 19s - loss: 631.1541 - loglik: -6.2589e+02 - logprior: -4.4991e+00
Fitted a model with MAP estimate = -628.5851
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (55, 1), (56, 1), (61, 2), (62, 2), (63, 2), (64, 2), (74, 1), (75, 1), (76, 1), (77, 3), (83, 1), (85, 1), (87, 1), (88, 1), (91, 1), (94, 1), (95, 1), (96, 1), (102, 1), (104, 1), (111, 1), (113, 1), (115, 1), (117, 1), (136, 1), (138, 1), (139, 1), (145, 1), (153, 1), (154, 1), (155, 1), (159, 2), (161, 1), (162, 1), (171, 1), (181, 1), (184, 1), (187, 1), (190, 1), (191, 1), (192, 3), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (211, 1), (212, 1), (218, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (233, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 1), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 31s - loss: 639.0947 - loglik: -6.3115e+02 - logprior: -7.8291e+00
Epoch 2/2
21/21 - 27s - loss: 607.9678 - loglik: -6.0655e+02 - logprior: -9.7226e-01
Fitted a model with MAP estimate = -600.3863
expansions: [(205, 1)]
discards: [ 77 246]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 30s - loss: 614.6068 - loglik: -6.0801e+02 - logprior: -6.5120e+00
Epoch 2/2
21/21 - 27s - loss: 602.8727 - loglik: -6.0226e+02 - logprior: -9.5006e-02
Fitted a model with MAP estimate = -597.4527
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 31s - loss: 611.2205 - loglik: -6.0506e+02 - logprior: -6.0522e+00
Epoch 2/10
21/21 - 27s - loss: 603.1802 - loglik: -6.0297e+02 - logprior: 0.3360
Epoch 3/10
21/21 - 27s - loss: 594.7820 - loglik: -5.9472e+02 - logprior: 0.8737
Epoch 4/10
21/21 - 27s - loss: 594.4484 - loglik: -5.9453e+02 - logprior: 1.1231
Epoch 5/10
21/21 - 27s - loss: 593.5005 - loglik: -5.9389e+02 - logprior: 1.3721
Epoch 6/10
21/21 - 27s - loss: 590.4159 - loglik: -5.9113e+02 - logprior: 1.5899
Epoch 7/10
21/21 - 27s - loss: 591.7250 - loglik: -5.9276e+02 - logprior: 1.8385
Fitted a model with MAP estimate = -589.2396
Time for alignment: 513.7545
Computed alignments with likelihoods: ['-591.8513', '-591.7914', '-589.2396']
Best model has likelihood: -589.2396
time for generating output: 0.4042
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9667260843731432
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b2223ea30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3aeeca60d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ac19d8eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad3a241f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b110d5cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3a24910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b4cd94cd0>, <__main__.SimpleDirichletPrior object at 0x7f3aca380220>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 838.6872 - loglik: -8.3624e+02 - logprior: -2.0834e+00
Epoch 2/10
39/39 - 27s - loss: 737.1733 - loglik: -7.3348e+02 - logprior: -2.0330e+00
Epoch 3/10
39/39 - 27s - loss: 721.3009 - loglik: -7.1670e+02 - logprior: -2.4071e+00
Epoch 4/10
39/39 - 26s - loss: 715.0215 - loglik: -7.1054e+02 - logprior: -2.5569e+00
Epoch 5/10
39/39 - 26s - loss: 712.5417 - loglik: -7.0828e+02 - logprior: -2.5689e+00
Epoch 6/10
39/39 - 27s - loss: 711.6716 - loglik: -7.0762e+02 - logprior: -2.5710e+00
Epoch 7/10
39/39 - 27s - loss: 710.3418 - loglik: -7.0641e+02 - logprior: -2.5956e+00
Epoch 8/10
39/39 - 27s - loss: 710.0012 - loglik: -7.0620e+02 - logprior: -2.6050e+00
Epoch 9/10
39/39 - 27s - loss: 708.8994 - loglik: -7.0511e+02 - logprior: -2.6185e+00
Epoch 10/10
39/39 - 27s - loss: 709.0283 - loglik: -7.0529e+02 - logprior: -2.6353e+00
Fitted a model with MAP estimate = -612.7238
expansions: [(14, 1), (19, 1), (41, 1), (53, 1), (55, 1), (80, 2), (81, 3), (82, 1), (84, 1), (94, 4), (105, 2), (106, 1), (109, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (148, 1), (160, 2), (161, 5), (162, 2), (168, 6), (169, 2), (171, 3), (172, 2), (174, 1), (175, 1), (178, 1), (180, 6), (186, 1), (187, 1), (188, 2), (189, 3), (190, 1), (206, 1), (212, 4), (218, 1), (244, 3)]
discards: [  0 103 150 151 152 153]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 731.2992 - loglik: -7.2729e+02 - logprior: -3.6816e+00
Epoch 2/2
39/39 - 35s - loss: 700.6927 - loglik: -6.9698e+02 - logprior: -2.8037e+00
Fitted a model with MAP estimate = -598.5242
expansions: [(0, 2), (120, 2), (313, 2)]
discards: [  0  86 109 118 121 122 123 124 125 126 183 192 207 310 311 312]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 709.8668 - loglik: -7.0692e+02 - logprior: -2.6368e+00
Epoch 2/2
39/39 - 34s - loss: 697.6621 - loglik: -6.9483e+02 - logprior: -1.6233e+00
Fitted a model with MAP estimate = -595.9782
expansions: [(108, 1), (117, 1), (118, 4), (121, 2), (122, 3), (183, 1), (237, 1), (238, 1)]
discards: [  0 109 110 301 302]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 42s - loss: 604.7572 - loglik: -6.0181e+02 - logprior: -2.6329e+00
Epoch 2/10
43/43 - 39s - loss: 593.0532 - loglik: -5.9068e+02 - logprior: -1.4386e+00
Epoch 3/10
43/43 - 38s - loss: 587.2517 - loglik: -5.8457e+02 - logprior: -1.2599e+00
Epoch 4/10
43/43 - 39s - loss: 587.4766 - loglik: -5.8471e+02 - logprior: -1.2006e+00
Fitted a model with MAP estimate = -582.9038
Time for alignment: 739.6442
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 837.9399 - loglik: -8.3550e+02 - logprior: -2.0829e+00
Epoch 2/10
39/39 - 27s - loss: 738.8849 - loglik: -7.3536e+02 - logprior: -2.0911e+00
Epoch 3/10
39/39 - 27s - loss: 725.0531 - loglik: -7.2060e+02 - logprior: -2.2727e+00
Epoch 4/10
39/39 - 26s - loss: 719.9443 - loglik: -7.1553e+02 - logprior: -2.4147e+00
Epoch 5/10
39/39 - 27s - loss: 717.4571 - loglik: -7.1325e+02 - logprior: -2.4648e+00
Epoch 6/10
39/39 - 27s - loss: 716.0972 - loglik: -7.1212e+02 - logprior: -2.4622e+00
Epoch 7/10
39/39 - 27s - loss: 715.3715 - loglik: -7.1152e+02 - logprior: -2.4678e+00
Epoch 8/10
39/39 - 27s - loss: 714.3714 - loglik: -7.1059e+02 - logprior: -2.4947e+00
Epoch 9/10
39/39 - 27s - loss: 714.2519 - loglik: -7.1054e+02 - logprior: -2.5037e+00
Epoch 10/10
39/39 - 27s - loss: 713.3538 - loglik: -7.0967e+02 - logprior: -2.5047e+00
Fitted a model with MAP estimate = -616.7527
expansions: [(14, 1), (41, 1), (45, 1), (53, 1), (81, 2), (82, 3), (83, 1), (93, 2), (94, 3), (96, 3), (103, 1), (116, 1), (118, 1), (120, 2), (121, 4), (122, 1), (144, 6), (145, 13), (156, 4), (159, 1), (163, 2), (173, 1), (174, 1), (175, 1), (179, 3), (180, 6), (181, 2), (184, 2), (186, 1), (187, 1), (188, 1), (190, 2), (191, 2), (207, 1), (208, 1), (212, 4), (244, 3)]
discards: [  0  39 104 105 106 107 108 109 164 165 166 167 168 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 742.8790 - loglik: -7.3884e+02 - logprior: -3.7232e+00
Epoch 2/2
39/39 - 36s - loss: 706.2957 - loglik: -7.0220e+02 - logprior: -2.9001e+00
Fitted a model with MAP estimate = -600.2655
expansions: [(0, 2), (121, 11), (168, 1), (277, 1), (287, 1), (315, 2)]
discards: [  0  85  95  96 102 106 110 135 177 178 198 203 223 231 232 233 237 250
 251 312 313 314]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 710.0599 - loglik: -7.0707e+02 - logprior: -2.6796e+00
Epoch 2/2
39/39 - 35s - loss: 693.9999 - loglik: -6.9121e+02 - logprior: -1.5995e+00
Fitted a model with MAP estimate = -593.2281
expansions: [(95, 2), (127, 2), (203, 1), (227, 1), (247, 1)]
discards: [  0 173 309 310]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 44s - loss: 605.4296 - loglik: -6.0246e+02 - logprior: -2.6165e+00
Epoch 2/10
43/43 - 40s - loss: 595.9507 - loglik: -5.9342e+02 - logprior: -1.3224e+00
Epoch 3/10
43/43 - 39s - loss: 585.7119 - loglik: -5.8278e+02 - logprior: -1.2458e+00
Epoch 4/10
43/43 - 39s - loss: 584.1478 - loglik: -5.8130e+02 - logprior: -1.1390e+00
Epoch 5/10
43/43 - 39s - loss: 585.1526 - loglik: -5.8228e+02 - logprior: -1.2951e+00
Fitted a model with MAP estimate = -582.2824
Time for alignment: 786.5919
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 837.9166 - loglik: -8.3545e+02 - logprior: -2.1017e+00
Epoch 2/10
39/39 - 27s - loss: 735.2912 - loglik: -7.3212e+02 - logprior: -2.2166e+00
Epoch 3/10
39/39 - 27s - loss: 720.8409 - loglik: -7.1671e+02 - logprior: -2.4764e+00
Epoch 4/10
39/39 - 26s - loss: 717.1584 - loglik: -7.1285e+02 - logprior: -2.5127e+00
Epoch 5/10
39/39 - 27s - loss: 715.1675 - loglik: -7.1097e+02 - logprior: -2.5469e+00
Epoch 6/10
39/39 - 27s - loss: 712.9774 - loglik: -7.0894e+02 - logprior: -2.5775e+00
Epoch 7/10
39/39 - 27s - loss: 712.6985 - loglik: -7.0871e+02 - logprior: -2.6209e+00
Epoch 8/10
39/39 - 27s - loss: 711.6686 - loglik: -7.0775e+02 - logprior: -2.6405e+00
Epoch 9/10
39/39 - 26s - loss: 711.2643 - loglik: -7.0740e+02 - logprior: -2.6505e+00
Epoch 10/10
39/39 - 27s - loss: 710.6126 - loglik: -7.0679e+02 - logprior: -2.6609e+00
Fitted a model with MAP estimate = -613.8715
expansions: [(14, 1), (30, 1), (41, 1), (53, 1), (81, 2), (82, 3), (83, 1), (85, 1), (92, 1), (93, 3), (95, 2), (99, 1), (103, 1), (116, 1), (118, 1), (121, 1), (122, 4), (123, 1), (141, 1), (142, 7), (146, 6), (157, 1), (168, 3), (169, 2), (172, 2), (173, 3), (174, 1), (180, 7), (181, 2), (182, 2), (185, 2), (186, 1), (187, 1), (188, 1), (191, 2), (192, 1), (193, 1), (207, 3), (208, 1), (213, 1), (214, 1), (244, 3)]
discards: [  0 104 105 106 107 108 109 158 159 160 161 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 741.4642 - loglik: -7.3746e+02 - logprior: -3.7036e+00
Epoch 2/2
39/39 - 35s - loss: 706.5942 - loglik: -7.0264e+02 - logprior: -2.8280e+00
Fitted a model with MAP estimate = -600.8171
expansions: [(0, 2), (122, 4), (123, 7), (167, 1), (168, 8), (175, 1), (216, 2), (310, 2)]
discards: [  0  86 104 105 106 136 162 193 204 224 229 230 234 250 251 265 307 308
 309]
Re-initialized the encoder parameters.
Fitting a model of length 318 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 707.8395 - loglik: -7.0484e+02 - logprior: -2.7028e+00
Epoch 2/2
39/39 - 37s - loss: 690.4335 - loglik: -6.8754e+02 - logprior: -1.7352e+00
Fitted a model with MAP estimate = -590.0785
expansions: [(208, 1), (219, 1), (232, 1), (261, 1)]
discards: [  0 119 120 121 122 123 124 125 126 169 170 171 172 173 174 176 240 241
 242 316 317]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 40s - loss: 608.4094 - loglik: -6.0552e+02 - logprior: -2.5551e+00
Epoch 2/10
43/43 - 37s - loss: 598.2923 - loglik: -5.9598e+02 - logprior: -1.2864e+00
Epoch 3/10
43/43 - 37s - loss: 591.6359 - loglik: -5.8878e+02 - logprior: -1.3272e+00
Epoch 4/10
43/43 - 38s - loss: 589.8013 - loglik: -5.8707e+02 - logprior: -1.1386e+00
Epoch 5/10
43/43 - 37s - loss: 588.6134 - loglik: -5.8612e+02 - logprior: -1.0237e+00
Epoch 6/10
43/43 - 37s - loss: 589.1090 - loglik: -5.8685e+02 - logprior: -9.5007e-01
Fitted a model with MAP estimate = -586.0124
Time for alignment: 811.5914
Computed alignments with likelihoods: ['-582.9038', '-582.2824', '-586.0124']
Best model has likelihood: -582.2824
time for generating output: 0.5117
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5400294293966974
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3ae63e70d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b333f74f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3bee4bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad2bf9910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b110d5cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3a24910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b00171cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b333d1ee0>, <__main__.SimpleDirichletPrior object at 0x7f3ad3875760>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.6654 - loglik: -2.3660e+02 - logprior: -3.9050e+01
Epoch 2/10
10/10 - 2s - loss: 224.4314 - loglik: -2.1344e+02 - logprior: -1.0970e+01
Epoch 3/10
10/10 - 2s - loss: 198.4014 - loglik: -1.9239e+02 - logprior: -5.9974e+00
Epoch 4/10
10/10 - 2s - loss: 183.4400 - loglik: -1.7902e+02 - logprior: -4.3440e+00
Epoch 5/10
10/10 - 2s - loss: 175.4401 - loglik: -1.7155e+02 - logprior: -3.6092e+00
Epoch 6/10
10/10 - 2s - loss: 171.7792 - loglik: -1.6811e+02 - logprior: -3.2768e+00
Epoch 7/10
10/10 - 2s - loss: 170.4276 - loglik: -1.6697e+02 - logprior: -3.1658e+00
Epoch 8/10
10/10 - 2s - loss: 169.3584 - loglik: -1.6600e+02 - logprior: -3.1084e+00
Epoch 9/10
10/10 - 2s - loss: 167.9955 - loglik: -1.6467e+02 - logprior: -3.0595e+00
Epoch 10/10
10/10 - 2s - loss: 166.9838 - loglik: -1.6371e+02 - logprior: -2.9988e+00
Fitted a model with MAP estimate = -166.5322
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 1), (23, 2), (25, 1), (32, 1), (43, 1), (55, 3), (56, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 215.6669 - loglik: -1.6411e+02 - logprior: -5.1535e+01
Epoch 2/2
10/10 - 2s - loss: 171.3234 - loglik: -1.5478e+02 - logprior: -1.6406e+01
Fitted a model with MAP estimate = -162.5827
expansions: []
discards: [ 0 18 67]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 200.0960 - loglik: -1.5545e+02 - logprior: -4.4619e+01
Epoch 2/2
10/10 - 2s - loss: 172.2688 - loglik: -1.5405e+02 - logprior: -1.8092e+01
Fitted a model with MAP estimate = -167.0586
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 194.2966 - loglik: -1.5346e+02 - logprior: -4.0817e+01
Epoch 2/10
10/10 - 2s - loss: 164.4159 - loglik: -1.5234e+02 - logprior: -1.1975e+01
Epoch 3/10
10/10 - 2s - loss: 156.9296 - loglik: -1.5145e+02 - logprior: -5.2496e+00
Epoch 4/10
10/10 - 2s - loss: 154.0998 - loglik: -1.5071e+02 - logprior: -3.1086e+00
Epoch 5/10
10/10 - 2s - loss: 152.1875 - loglik: -1.4973e+02 - logprior: -2.2053e+00
Epoch 6/10
10/10 - 2s - loss: 151.8748 - loglik: -1.4999e+02 - logprior: -1.6415e+00
Epoch 7/10
10/10 - 2s - loss: 151.2784 - loglik: -1.4994e+02 - logprior: -1.1247e+00
Epoch 8/10
10/10 - 2s - loss: 150.7839 - loglik: -1.4981e+02 - logprior: -7.7313e-01
Epoch 9/10
10/10 - 2s - loss: 151.0543 - loglik: -1.5026e+02 - logprior: -5.9105e-01
Fitted a model with MAP estimate = -150.5661
Time for alignment: 57.3744
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.5831 - loglik: -2.3651e+02 - logprior: -3.9051e+01
Epoch 2/10
10/10 - 2s - loss: 224.1143 - loglik: -2.1312e+02 - logprior: -1.0972e+01
Epoch 3/10
10/10 - 2s - loss: 197.8609 - loglik: -1.9186e+02 - logprior: -5.9871e+00
Epoch 4/10
10/10 - 2s - loss: 181.5514 - loglik: -1.7708e+02 - logprior: -4.4108e+00
Epoch 5/10
10/10 - 2s - loss: 174.2976 - loglik: -1.7032e+02 - logprior: -3.7408e+00
Epoch 6/10
10/10 - 2s - loss: 171.7960 - loglik: -1.6801e+02 - logprior: -3.3641e+00
Epoch 7/10
10/10 - 2s - loss: 170.4785 - loglik: -1.6693e+02 - logprior: -3.1561e+00
Epoch 8/10
10/10 - 2s - loss: 169.4422 - loglik: -1.6607e+02 - logprior: -3.0591e+00
Epoch 9/10
10/10 - 2s - loss: 168.7276 - loglik: -1.6542e+02 - logprior: -3.0076e+00
Epoch 10/10
10/10 - 2s - loss: 168.7020 - loglik: -1.6545e+02 - logprior: -2.9354e+00
Fitted a model with MAP estimate = -167.8491
expansions: [(7, 2), (13, 1), (15, 2), (20, 1), (21, 2), (24, 3), (30, 1), (32, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.5586 - loglik: -1.6635e+02 - logprior: -4.4185e+01
Epoch 2/2
10/10 - 2s - loss: 175.0927 - loglik: -1.5618e+02 - logprior: -1.8755e+01
Fitted a model with MAP estimate = -168.5508
expansions: [(0, 2)]
discards: [ 0 18 26 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.9289 - loglik: -1.5290e+02 - logprior: -3.5006e+01
Epoch 2/2
10/10 - 2s - loss: 160.1912 - loglik: -1.5036e+02 - logprior: -9.7140e+00
Fitted a model with MAP estimate = -156.0222
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 193.7226 - loglik: -1.5318e+02 - logprior: -4.0520e+01
Epoch 2/10
10/10 - 2s - loss: 163.9837 - loglik: -1.5212e+02 - logprior: -1.1774e+01
Epoch 3/10
10/10 - 2s - loss: 156.3497 - loglik: -1.5099e+02 - logprior: -5.1456e+00
Epoch 4/10
10/10 - 2s - loss: 153.1733 - loglik: -1.4987e+02 - logprior: -3.0231e+00
Epoch 5/10
10/10 - 2s - loss: 151.8727 - loglik: -1.4949e+02 - logprior: -2.1219e+00
Epoch 6/10
10/10 - 2s - loss: 151.3090 - loglik: -1.4951e+02 - logprior: -1.5605e+00
Epoch 7/10
10/10 - 2s - loss: 150.4753 - loglik: -1.4921e+02 - logprior: -1.0436e+00
Epoch 8/10
10/10 - 2s - loss: 150.3801 - loglik: -1.4947e+02 - logprior: -6.9549e-01
Epoch 9/10
10/10 - 2s - loss: 150.1333 - loglik: -1.4943e+02 - logprior: -4.9716e-01
Epoch 10/10
10/10 - 2s - loss: 149.7648 - loglik: -1.4919e+02 - logprior: -3.6460e-01
Fitted a model with MAP estimate = -149.7192
Time for alignment: 57.8440
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.4321 - loglik: -2.3636e+02 - logprior: -3.9052e+01
Epoch 2/10
10/10 - 2s - loss: 224.4222 - loglik: -2.1343e+02 - logprior: -1.0970e+01
Epoch 3/10
10/10 - 2s - loss: 198.7495 - loglik: -1.9277e+02 - logprior: -5.9729e+00
Epoch 4/10
10/10 - 2s - loss: 184.0109 - loglik: -1.7956e+02 - logprior: -4.3735e+00
Epoch 5/10
10/10 - 2s - loss: 176.5590 - loglik: -1.7266e+02 - logprior: -3.6156e+00
Epoch 6/10
10/10 - 2s - loss: 173.2094 - loglik: -1.6960e+02 - logprior: -3.2398e+00
Epoch 7/10
10/10 - 2s - loss: 171.3750 - loglik: -1.6805e+02 - logprior: -3.0631e+00
Epoch 8/10
10/10 - 2s - loss: 170.2921 - loglik: -1.6712e+02 - logprior: -2.9641e+00
Epoch 9/10
10/10 - 2s - loss: 169.4740 - loglik: -1.6639e+02 - logprior: -2.8753e+00
Epoch 10/10
10/10 - 2s - loss: 168.9015 - loglik: -1.6585e+02 - logprior: -2.8343e+00
Fitted a model with MAP estimate = -168.2091
expansions: [(7, 2), (13, 1), (15, 2), (20, 1), (21, 2), (23, 3), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.5939 - loglik: -1.6738e+02 - logprior: -4.4189e+01
Epoch 2/2
10/10 - 2s - loss: 175.6293 - loglik: -1.5671e+02 - logprior: -1.8782e+01
Fitted a model with MAP estimate = -169.1321
expansions: [(0, 2)]
discards: [ 0 18 30 46 70]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 188.3177 - loglik: -1.5327e+02 - logprior: -3.5024e+01
Epoch 2/2
10/10 - 2s - loss: 161.1131 - loglik: -1.5121e+02 - logprior: -9.7894e+00
Fitted a model with MAP estimate = -156.4819
expansions: []
discards: [ 0 28]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 194.6371 - loglik: -1.5396e+02 - logprior: -4.0658e+01
Epoch 2/10
10/10 - 2s - loss: 165.0791 - loglik: -1.5305e+02 - logprior: -1.1941e+01
Epoch 3/10
10/10 - 2s - loss: 157.3788 - loglik: -1.5193e+02 - logprior: -5.2406e+00
Epoch 4/10
10/10 - 2s - loss: 154.0266 - loglik: -1.5067e+02 - logprior: -3.0819e+00
Epoch 5/10
10/10 - 2s - loss: 152.3001 - loglik: -1.4986e+02 - logprior: -2.1819e+00
Epoch 6/10
10/10 - 2s - loss: 151.4834 - loglik: -1.4964e+02 - logprior: -1.6132e+00
Epoch 7/10
10/10 - 2s - loss: 151.5631 - loglik: -1.5025e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -150.8270
Time for alignment: 52.5424
Computed alignments with likelihoods: ['-150.5661', '-149.7192', '-150.8270']
Best model has likelihood: -149.7192
time for generating output: 0.2205
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9222944334557784
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b44731820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3ad40822b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4d00e3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3adda20bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b110d5cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3a24910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b00171cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4d015130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b5d26bfa0>, <__main__.SimpleDirichletPrior object at 0x7f3aeeb3eb20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 323.2582 - loglik: -3.0588e+02 - logprior: -1.7373e+01
Epoch 2/10
10/10 - 5s - loss: 279.2093 - loglik: -2.7439e+02 - logprior: -4.8201e+00
Epoch 3/10
10/10 - 6s - loss: 245.1787 - loglik: -2.4240e+02 - logprior: -2.7706e+00
Epoch 4/10
10/10 - 4s - loss: 226.5058 - loglik: -2.2396e+02 - logprior: -2.4498e+00
Epoch 5/10
10/10 - 5s - loss: 221.4908 - loglik: -2.1878e+02 - logprior: -2.4714e+00
Epoch 6/10
10/10 - 5s - loss: 219.0409 - loglik: -2.1631e+02 - logprior: -2.4097e+00
Epoch 7/10
10/10 - 5s - loss: 218.1233 - loglik: -2.1555e+02 - logprior: -2.2697e+00
Epoch 8/10
10/10 - 5s - loss: 217.7424 - loglik: -2.1530e+02 - logprior: -2.1463e+00
Epoch 9/10
10/10 - 5s - loss: 217.3822 - loglik: -2.1496e+02 - logprior: -2.1181e+00
Epoch 10/10
10/10 - 6s - loss: 217.1557 - loglik: -2.1472e+02 - logprior: -2.1354e+00
Fitted a model with MAP estimate = -216.4284
expansions: [(10, 5), (11, 2), (26, 2), (43, 1), (47, 4), (48, 4), (49, 1), (57, 1), (60, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 234.7478 - loglik: -2.1508e+02 - logprior: -1.9643e+01
Epoch 2/2
10/10 - 5s - loss: 211.3646 - loglik: -2.0278e+02 - logprior: -8.4544e+00
Fitted a model with MAP estimate = -207.6398
expansions: [(0, 2)]
discards: [ 0 15 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 215.4239 - loglik: -1.9987e+02 - logprior: -1.5532e+01
Epoch 2/2
10/10 - 6s - loss: 202.2792 - loglik: -1.9781e+02 - logprior: -4.3261e+00
Fitted a model with MAP estimate = -199.7554
expansions: []
discards: [ 0 11 58]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 219.8896 - loglik: -2.0171e+02 - logprior: -1.8156e+01
Epoch 2/10
10/10 - 7s - loss: 205.5907 - loglik: -1.9998e+02 - logprior: -5.4809e+00
Epoch 3/10
10/10 - 7s - loss: 201.3132 - loglik: -1.9829e+02 - logprior: -2.7950e+00
Epoch 4/10
10/10 - 7s - loss: 199.5156 - loglik: -1.9719e+02 - logprior: -2.0680e+00
Epoch 5/10
10/10 - 7s - loss: 199.5326 - loglik: -1.9781e+02 - logprior: -1.4540e+00
Fitted a model with MAP estimate = -198.3484
Time for alignment: 143.6506
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 323.6636 - loglik: -3.0628e+02 - logprior: -1.7377e+01
Epoch 2/10
10/10 - 5s - loss: 278.9365 - loglik: -2.7410e+02 - logprior: -4.8289e+00
Epoch 3/10
10/10 - 5s - loss: 248.0490 - loglik: -2.4522e+02 - logprior: -2.8152e+00
Epoch 4/10
10/10 - 5s - loss: 230.2712 - loglik: -2.2769e+02 - logprior: -2.4739e+00
Epoch 5/10
10/10 - 5s - loss: 223.2310 - loglik: -2.2050e+02 - logprior: -2.4806e+00
Epoch 6/10
10/10 - 6s - loss: 220.0768 - loglik: -2.1726e+02 - logprior: -2.4534e+00
Epoch 7/10
10/10 - 5s - loss: 217.9183 - loglik: -2.1529e+02 - logprior: -2.3146e+00
Epoch 8/10
10/10 - 5s - loss: 218.1848 - loglik: -2.1573e+02 - logprior: -2.1883e+00
Fitted a model with MAP estimate = -217.3391
expansions: [(10, 3), (11, 2), (15, 1), (26, 2), (43, 1), (47, 4), (48, 4), (49, 1), (50, 2), (60, 1), (72, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 234.5627 - loglik: -2.1503e+02 - logprior: -1.9518e+01
Epoch 2/2
10/10 - 7s - loss: 212.2869 - loglik: -2.0396e+02 - logprior: -8.2614e+00
Fitted a model with MAP estimate = -207.9073
expansions: [(0, 2)]
discards: [ 0 13 58 59 67]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 216.2701 - loglik: -2.0080e+02 - logprior: -1.5453e+01
Epoch 2/2
10/10 - 5s - loss: 202.3778 - loglik: -1.9797e+02 - logprior: -4.3077e+00
Fitted a model with MAP estimate = -200.4087
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 219.3040 - loglik: -2.0107e+02 - logprior: -1.8206e+01
Epoch 2/10
10/10 - 6s - loss: 205.1179 - loglik: -1.9948e+02 - logprior: -5.5159e+00
Epoch 3/10
10/10 - 8s - loss: 201.3236 - loglik: -1.9831e+02 - logprior: -2.8012e+00
Epoch 4/10
10/10 - 7s - loss: 199.5986 - loglik: -1.9732e+02 - logprior: -2.0438e+00
Epoch 5/10
10/10 - 7s - loss: 198.3993 - loglik: -1.9671e+02 - logprior: -1.4160e+00
Epoch 6/10
10/10 - 7s - loss: 198.9955 - loglik: -1.9758e+02 - logprior: -1.1292e+00
Fitted a model with MAP estimate = -197.8294
Time for alignment: 140.7626
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 323.7199 - loglik: -3.0634e+02 - logprior: -1.7375e+01
Epoch 2/10
10/10 - 5s - loss: 278.8768 - loglik: -2.7404e+02 - logprior: -4.8289e+00
Epoch 3/10
10/10 - 5s - loss: 250.2554 - loglik: -2.4740e+02 - logprior: -2.8448e+00
Epoch 4/10
10/10 - 5s - loss: 232.0655 - loglik: -2.2933e+02 - logprior: -2.6376e+00
Epoch 5/10
10/10 - 5s - loss: 223.8969 - loglik: -2.2081e+02 - logprior: -2.8019e+00
Epoch 6/10
10/10 - 5s - loss: 219.3837 - loglik: -2.1611e+02 - logprior: -2.8780e+00
Epoch 7/10
10/10 - 5s - loss: 218.1758 - loglik: -2.1504e+02 - logprior: -2.8010e+00
Epoch 8/10
10/10 - 5s - loss: 216.5000 - loglik: -2.1354e+02 - logprior: -2.6796e+00
Epoch 9/10
10/10 - 6s - loss: 216.9892 - loglik: -2.1408e+02 - logprior: -2.6375e+00
Fitted a model with MAP estimate = -215.8280
expansions: [(7, 1), (9, 3), (10, 2), (11, 1), (27, 1), (29, 1), (42, 1), (43, 1), (47, 2), (48, 3), (49, 1), (50, 1), (57, 1), (62, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 235.2985 - loglik: -2.1565e+02 - logprior: -1.9632e+01
Epoch 2/2
10/10 - 7s - loss: 212.3518 - loglik: -2.0387e+02 - logprior: -8.3668e+00
Fitted a model with MAP estimate = -207.8214
expansions: [(0, 2)]
discards: [ 0 12 14 61]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 216.3332 - loglik: -2.0083e+02 - logprior: -1.5478e+01
Epoch 2/2
10/10 - 6s - loss: 202.2912 - loglik: -1.9787e+02 - logprior: -4.2896e+00
Fitted a model with MAP estimate = -200.3201
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 219.4831 - loglik: -2.0131e+02 - logprior: -1.8145e+01
Epoch 2/10
10/10 - 7s - loss: 205.0233 - loglik: -1.9950e+02 - logprior: -5.3895e+00
Epoch 3/10
10/10 - 6s - loss: 201.1549 - loglik: -1.9830e+02 - logprior: -2.6225e+00
Epoch 4/10
10/10 - 7s - loss: 199.6120 - loglik: -1.9741e+02 - logprior: -1.9335e+00
Epoch 5/10
10/10 - 6s - loss: 198.1732 - loglik: -1.9653e+02 - logprior: -1.3747e+00
Epoch 6/10
10/10 - 7s - loss: 198.7239 - loglik: -1.9743e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -197.5876
Time for alignment: 145.3008
Computed alignments with likelihoods: ['-198.3484', '-197.8294', '-197.5876']
Best model has likelihood: -197.5876
time for generating output: 0.6494
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21169733383402178
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f390c3c6bb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f390c3c6d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c3c6d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c5e0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ca30>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c730>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f390c43c940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c4c0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c880>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c190>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cbe0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cac0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cc40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd30>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c280>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c430>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43ceb0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f390c43c610> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3ad2a36fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3af72c92b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af72c9d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b5d35a190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f390c32fe50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f390c43c700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65dd6d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3af7e34a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b3328ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b8613e340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4485b670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08fecf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b335a9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19f25dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b11528730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3add9cabb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3887880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b334c8be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b19c2f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b08903f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3880130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b65bc6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b110d5cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3ad3a24910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b00171cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b4d015130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b5d35ab80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3b742005e0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f390c449160>, <function make_default_emission_matrix at 0x7f390c449160>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3ab09710a0>, <__main__.SimpleDirichletPrior object at 0x7f3b116bb040>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 52s - loss: 1201.1937 - loglik: -1.1961e+03 - logprior: -4.9408e+00
Epoch 2/10
24/24 - 47s - loss: 1008.4653 - loglik: -1.0067e+03 - logprior: -1.2316e+00
Epoch 3/10
24/24 - 47s - loss: 950.9289 - loglik: -9.4717e+02 - logprior: -2.9630e+00
Epoch 4/10
24/24 - 48s - loss: 940.9451 - loglik: -9.3641e+02 - logprior: -3.1759e+00
Epoch 5/10
24/24 - 47s - loss: 933.8932 - loglik: -9.2911e+02 - logprior: -3.1764e+00
Epoch 6/10
24/24 - 47s - loss: 931.9554 - loglik: -9.2700e+02 - logprior: -3.2848e+00
Epoch 7/10
24/24 - 47s - loss: 930.0684 - loglik: -9.2490e+02 - logprior: -3.5116e+00
Epoch 8/10
24/24 - 47s - loss: 925.2117 - loglik: -9.2003e+02 - logprior: -3.4944e+00
Epoch 9/10
24/24 - 47s - loss: 931.1398 - loglik: -9.2577e+02 - logprior: -3.6368e+00
Fitted a model with MAP estimate = -923.2765
expansions: [(0, 3), (108, 1), (130, 1), (131, 1), (139, 1), (181, 1), (183, 1), (211, 1), (212, 1), (226, 1), (229, 4), (230, 3), (231, 1), (234, 3), (236, 3), (237, 2), (238, 1), (247, 1), (250, 1), (251, 3), (252, 5), (274, 1), (275, 1), (276, 1), (277, 1), (294, 2), (295, 7), (296, 4), (297, 1), (298, 3), (299, 7), (309, 3), (310, 1), (311, 1), (315, 1), (324, 1), (325, 1), (326, 1), (328, 1), (344, 1), (346, 5), (347, 1), (348, 1), (368, 1), (394, 1), (395, 4), (396, 1), (398, 1), (399, 12)]
discards: [35 36 89]
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
Out of memory. A resource was exhausted.
Try reducing the batch size (-b). The current batch size was: <function get_adaptive_batch_size at 0x7f390c32fe50>.
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3b100335e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3b10033340>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b10033250>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b10033310>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b100330d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033130>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b10033940>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033400>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033820>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b100339a0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033880>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033280>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b100331f0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033490>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033220>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033430>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b100334f0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b100338b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3d6ed56520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3d6ed56b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3d6ed56cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3d6ed56c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3aac4190d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b10033040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3d6ed56ac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f3d6edb5430> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f3c27a70b80>, <function make_default_emission_matrix at 0x7f3c27a70b80>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3d6ed56d00>, <__main__.SimpleDirichletPrior object at 0x7f3d6ed56ca0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd49c4387c0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd49c438670>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd49c498340>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd49c498070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c4987c0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498fd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd49c498610>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498dc0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498fa0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c4986d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c4982e0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498f40>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c498190>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42f1f0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42f970>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42fa60>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42fa30>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd49c42f880> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd774261a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd774261940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c46f0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd754e6aa90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd49c47cf70> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42f220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd49c42f6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd754e6aa30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fd77529f160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fd4fc0448b0>, <function make_default_emission_matrix at 0x7fd4fc0448b0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fd754e6af40>, <__main__.SimpleDirichletPrior object at 0x7fd77536b880>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 51s - loss: 1193.5387 - loglik: -1.1883e+03 - logprior: -5.0398e+00
Epoch 2/10
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fa6e0096ac0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fa6e00962b0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fa6e0096fd0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fa6e00960a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096e20>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096070>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fa6e0096580>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e00966a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096a00>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e00964f0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096430>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096df0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096cd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096d90>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096670>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fa6e0096760> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa937575580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fa937575250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa9375751c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fa937575f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fa6cc7e90d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e00965e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa6e0096970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa937575340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fa957da3160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fa7f4516b80>, <function make_default_emission_matrix at 0x7fa7f4516b80>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa937575b20>, <__main__.SimpleDirichletPrior object at 0x7fa937575d60>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b505c6700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68e1bc4310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b34783e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b346f2220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b346f21f0>, <__main__.SimpleDirichletPrior object at 0x7f6b3550c670>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 54s - loss: 1107.9127 - loglik: -1.1045e+03 - logprior: -2.9316e+00
Epoch 2/10
49/49 - 50s - loss: 950.9589 - loglik: -9.4725e+02 - logprior: -2.4986e+00
Epoch 3/10
49/49 - 50s - loss: 939.0437 - loglik: -9.3404e+02 - logprior: -2.8210e+00
Epoch 4/10
49/49 - 50s - loss: 933.1013 - loglik: -9.2760e+02 - logprior: -3.1664e+00
Epoch 5/10
49/49 - 50s - loss: 932.9118 - loglik: -9.2738e+02 - logprior: -3.3734e+00
Epoch 6/10
49/49 - 50s - loss: 928.4570 - loglik: -9.2310e+02 - logprior: -3.4015e+00
Epoch 7/10
49/49 - 50s - loss: 929.6823 - loglik: -9.2437e+02 - logprior: -3.5069e+00
Fitted a model with MAP estimate = -924.5998
expansions: [(0, 3), (83, 1), (84, 1), (85, 1), (122, 1), (138, 1), (190, 1), (211, 3), (222, 1), (224, 2), (226, 5), (227, 3), (228, 1), (231, 2), (235, 2), (236, 2), (237, 1), (242, 1), (244, 1), (248, 1), (249, 2), (250, 5), (268, 1), (269, 1), (270, 2), (271, 1), (272, 1), (290, 1), (292, 11), (293, 1), (294, 2), (295, 4), (297, 1), (307, 4), (308, 1), (309, 2), (311, 1), (312, 1), (321, 1), (322, 1), (323, 1), (330, 1), (344, 5), (378, 1), (396, 1), (398, 1), (399, 12)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 72s - loss: 963.4263 - loglik: -9.5726e+02 - logprior: -5.6757e+00
Epoch 2/2
49/49 - 69s - loss: 910.3922 - loglik: -9.0755e+02 - logprior: -1.1383e+00
Fitted a model with MAP estimate = -902.0948
expansions: [(0, 3), (96, 5), (284, 1), (344, 1), (352, 1), (354, 1), (480, 1), (492, 1), (497, 2), (498, 3)]
discards: [  2   3   4   5   6   7   8  37  38  39 238 377 383 426 429 430 431 499
 500 501 502 503]
Re-initialized the encoder parameters.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 72s - loss: 941.5334 - loglik: -9.3719e+02 - logprior: -3.8767e+00
Epoch 2/2
49/49 - 69s - loss: 909.2003 - loglik: -9.0830e+02 - logprior: 0.7705
Fitted a model with MAP estimate = -901.9061
expansions: [(0, 4), (33, 3), (353, 1), (361, 2), (425, 4), (426, 1), (497, 1), (498, 1), (501, 6)]
discards: [ 2  3  4  5  6 89 90 91 92 93]
Re-initialized the encoder parameters.
Fitting a model of length 514 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 76s - loss: 930.9805 - loglik: -9.2697e+02 - logprior: -3.5499e+00
Epoch 2/10
49/49 - 72s - loss: 907.1518 - loglik: -9.0703e+02 - logprior: 1.5500
Epoch 3/10
49/49 - 72s - loss: 898.2800 - loglik: -8.9749e+02 - logprior: 1.6293
Epoch 4/10
49/49 - 72s - loss: 889.4310 - loglik: -8.8871e+02 - logprior: 2.0089
Epoch 5/10
49/49 - 72s - loss: 885.7420 - loglik: -8.8525e+02 - logprior: 2.3035
Epoch 6/10
49/49 - 72s - loss: 884.4917 - loglik: -8.8449e+02 - logprior: 2.8300
Epoch 7/10
49/49 - 72s - loss: 877.9203 - loglik: -8.7839e+02 - logprior: 3.3613
Epoch 8/10
49/49 - 72s - loss: 874.7437 - loglik: -8.7564e+02 - logprior: 3.8095
Epoch 9/10
49/49 - 72s - loss: 871.7015 - loglik: -8.7354e+02 - logprior: 4.4467
Epoch 10/10
49/49 - 72s - loss: 870.0799 - loglik: -8.7284e+02 - logprior: 5.0693
Fitted a model with MAP estimate = -866.6031
Time for alignment: 1542.7061
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 54s - loss: 1103.1816 - loglik: -1.0998e+03 - logprior: -2.9904e+00
Epoch 2/10
49/49 - 50s - loss: 952.7833 - loglik: -9.4903e+02 - logprior: -2.2659e+00
Epoch 3/10
49/49 - 50s - loss: 942.0258 - loglik: -9.3675e+02 - logprior: -2.9425e+00
Epoch 4/10
49/49 - 50s - loss: 932.7985 - loglik: -9.2711e+02 - logprior: -3.2763e+00
Epoch 5/10
49/49 - 50s - loss: 934.7050 - loglik: -9.2916e+02 - logprior: -3.3453e+00
Fitted a model with MAP estimate = -928.2670
expansions: [(0, 3), (129, 1), (139, 1), (181, 1), (191, 1), (211, 1), (212, 1), (224, 1), (225, 1), (227, 5), (228, 2), (229, 3), (231, 2), (234, 3), (235, 2), (236, 1), (241, 1), (242, 1), (243, 1), (246, 1), (247, 2), (248, 5), (250, 1), (266, 1), (267, 1), (268, 2), (269, 1), (270, 2), (287, 2), (290, 7), (294, 1), (297, 6), (298, 2), (306, 3), (309, 2), (311, 1), (312, 1), (320, 2), (321, 1), (322, 1), (324, 1), (343, 1), (344, 1), (346, 1), (364, 4), (393, 2), (394, 3), (399, 11)]
discards: [33]
Re-initialized the encoder parameters.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 73s - loss: 959.7365 - loglik: -9.5384e+02 - logprior: -5.4203e+00
Epoch 2/2
49/49 - 69s - loss: 910.1622 - loglik: -9.0723e+02 - logprior: -1.1649e+00
Fitted a model with MAP estimate = -902.4127
expansions: [(0, 3), (244, 1), (280, 1), (349, 1), (492, 1), (494, 1)]
discards: [  2   3   4   5   6   7   8   9  10  36 246 335 379 446 478 503]
Re-initialized the encoder parameters.
Fitting a model of length 496 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 72s - loss: 936.1343 - loglik: -9.3135e+02 - logprior: -4.3429e+00
Epoch 2/2
49/49 - 67s - loss: 913.0443 - loglik: -9.1201e+02 - logprior: 0.5448
Fitted a model with MAP estimate = -902.6074
expansions: [(0, 4), (31, 3), (496, 4)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 73s - loss: 932.7593 - loglik: -9.2854e+02 - logprior: -3.7699e+00
Epoch 2/10
49/49 - 70s - loss: 910.6045 - loglik: -9.0967e+02 - logprior: 0.7165
Epoch 3/10
49/49 - 70s - loss: 899.1699 - loglik: -8.9766e+02 - logprior: 0.8984
Epoch 4/10
49/49 - 70s - loss: 895.5315 - loglik: -8.9429e+02 - logprior: 1.4796
Epoch 5/10
49/49 - 69s - loss: 890.7581 - loglik: -8.8976e+02 - logprior: 1.7096
Epoch 6/10
49/49 - 70s - loss: 886.5905 - loglik: -8.8592e+02 - logprior: 2.1059
Epoch 7/10
49/49 - 70s - loss: 881.4330 - loglik: -8.8106e+02 - logprior: 2.5726
Epoch 8/10
49/49 - 70s - loss: 878.5903 - loglik: -8.7866e+02 - logprior: 3.0638
Epoch 9/10
49/49 - 70s - loss: 871.9857 - loglik: -8.7304e+02 - logprior: 3.6935
Epoch 10/10
49/49 - 70s - loss: 876.5028 - loglik: -8.7881e+02 - logprior: 4.6440
Fitted a model with MAP estimate = -868.5545
Time for alignment: 1418.1846
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 54s - loss: 1103.5588 - loglik: -1.1001e+03 - logprior: -3.0288e+00
Epoch 2/10
49/49 - 50s - loss: 955.2227 - loglik: -9.5163e+02 - logprior: -2.6443e+00
Epoch 3/10
49/49 - 50s - loss: 942.9788 - loglik: -9.3828e+02 - logprior: -2.8596e+00
Epoch 4/10
49/49 - 50s - loss: 939.4632 - loglik: -9.3416e+02 - logprior: -3.2294e+00
Epoch 5/10
49/49 - 50s - loss: 932.4899 - loglik: -9.2695e+02 - logprior: -3.4700e+00
Epoch 6/10
49/49 - 50s - loss: 935.5953 - loglik: -9.2990e+02 - logprior: -3.7364e+00
Fitted a model with MAP estimate = -929.7455
expansions: [(0, 3), (143, 1), (196, 1), (216, 1), (217, 1), (228, 1), (230, 2), (233, 4), (234, 3), (235, 1), (238, 3), (241, 2), (243, 1), (244, 1), (249, 1), (250, 1), (251, 1), (254, 1), (255, 2), (256, 6), (257, 1), (271, 1), (272, 1), (273, 1), (274, 2), (275, 5), (291, 2), (292, 5), (293, 3), (294, 1), (295, 1), (297, 3), (298, 8), (308, 2), (309, 1), (310, 4), (312, 1), (313, 1), (322, 1), (323, 1), (324, 1), (345, 4), (347, 1), (364, 5), (365, 3), (366, 2), (367, 3), (368, 1), (404, 5)]
discards: [ 34  35  36  84 381]
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 73s - loss: 966.7137 - loglik: -9.6069e+02 - logprior: -5.5113e+00
Epoch 2/2
49/49 - 70s - loss: 910.8313 - loglik: -9.0753e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -904.2152
expansions: [(0, 3), (94, 5), (282, 1), (427, 1), (506, 4)]
discards: [  2   3   4   5   6   7   8  37 234 378 428 479 480 500 501 502 503 504
 505]
Re-initialized the encoder parameters.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 73s - loss: 938.7388 - loglik: -9.3398e+02 - logprior: -4.2987e+00
Epoch 2/2
49/49 - 69s - loss: 911.8854 - loglik: -9.1070e+02 - logprior: 0.5007
Fitted a model with MAP estimate = -901.6746
expansions: [(0, 4), (33, 3), (498, 1), (499, 4), (500, 3)]
discards: [ 2  3  4  5  6 89 90 91 92 93]
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 73s - loss: 934.9474 - loglik: -9.3109e+02 - logprior: -3.4069e+00
Epoch 2/10
49/49 - 70s - loss: 904.7119 - loglik: -9.0450e+02 - logprior: 1.3285
Epoch 3/10
49/49 - 70s - loss: 898.0065 - loglik: -8.9720e+02 - logprior: 1.4699
Epoch 4/10
49/49 - 70s - loss: 895.3062 - loglik: -8.9451e+02 - logprior: 1.7958
Epoch 5/10
49/49 - 70s - loss: 885.7442 - loglik: -8.8540e+02 - logprior: 2.2842
Epoch 6/10
49/49 - 69s - loss: 888.0961 - loglik: -8.8808e+02 - logprior: 2.6948
Fitted a model with MAP estimate = -878.6031
Time for alignment: 1194.5878
Computed alignments with likelihoods: ['-866.6031', '-868.5545', '-878.6031']
Best model has likelihood: -866.6031
time for generating output: 0.5568
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.7882101893838357
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b34783e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68e1bc4280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68e1bc4310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac84af310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b1f2ccee0>, <__main__.SimpleDirichletPrior object at 0x7f6ad2e23190>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 480.0935 - loglik: -4.7765e+02 - logprior: -2.1942e+00
Epoch 2/10
39/39 - 12s - loss: 412.3929 - loglik: -4.0952e+02 - logprior: -1.9117e+00
Epoch 3/10
39/39 - 12s - loss: 401.9341 - loglik: -3.9892e+02 - logprior: -1.9691e+00
Epoch 4/10
39/39 - 12s - loss: 399.0932 - loglik: -3.9616e+02 - logprior: -2.0204e+00
Epoch 5/10
39/39 - 12s - loss: 397.4674 - loglik: -3.9472e+02 - logprior: -2.0316e+00
Epoch 6/10
39/39 - 12s - loss: 397.5137 - loglik: -3.9484e+02 - logprior: -2.0434e+00
Fitted a model with MAP estimate = -396.6772
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (17, 1), (23, 1), (26, 1), (39, 2), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 2), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (118, 1), (122, 2), (126, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 399.1469 - loglik: -3.9550e+02 - logprior: -3.3654e+00
Epoch 2/2
39/39 - 15s - loss: 381.8874 - loglik: -3.7984e+02 - logprior: -1.5612e+00
Fitted a model with MAP estimate = -378.6781
expansions: [(49, 1)]
discards: [ 13  68  71  76  89  92 112 134 140]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 384.8271 - loglik: -3.8221e+02 - logprior: -2.3916e+00
Epoch 2/2
39/39 - 14s - loss: 379.2172 - loglik: -3.7731e+02 - logprior: -1.2122e+00
Fitted a model with MAP estimate = -377.2119
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 382.6559 - loglik: -3.8044e+02 - logprior: -1.9704e+00
Epoch 2/10
41/41 - 15s - loss: 377.3451 - loglik: -3.7566e+02 - logprior: -9.9091e-01
Epoch 3/10
41/41 - 15s - loss: 374.7401 - loglik: -3.7284e+02 - logprior: -9.6842e-01
Epoch 4/10
41/41 - 15s - loss: 373.7157 - loglik: -3.7190e+02 - logprior: -9.1421e-01
Epoch 5/10
41/41 - 15s - loss: 372.7932 - loglik: -3.7107e+02 - logprior: -8.6051e-01
Epoch 6/10
41/41 - 15s - loss: 372.1725 - loglik: -3.7062e+02 - logprior: -7.7685e-01
Epoch 7/10
41/41 - 15s - loss: 371.5170 - loglik: -3.7012e+02 - logprior: -7.0417e-01
Epoch 8/10
41/41 - 15s - loss: 372.0164 - loglik: -3.7073e+02 - logprior: -6.3863e-01
Fitted a model with MAP estimate = -370.3239
Time for alignment: 334.4822
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 478.8744 - loglik: -4.7644e+02 - logprior: -2.1765e+00
Epoch 2/10
39/39 - 12s - loss: 408.6278 - loglik: -4.0601e+02 - logprior: -2.0057e+00
Epoch 3/10
39/39 - 12s - loss: 400.7226 - loglik: -3.9789e+02 - logprior: -2.0292e+00
Epoch 4/10
39/39 - 12s - loss: 398.4612 - loglik: -3.9565e+02 - logprior: -2.0406e+00
Epoch 5/10
39/39 - 12s - loss: 397.6895 - loglik: -3.9496e+02 - logprior: -2.0435e+00
Epoch 6/10
39/39 - 12s - loss: 397.2471 - loglik: -3.9462e+02 - logprior: -2.0348e+00
Epoch 7/10
39/39 - 12s - loss: 396.9104 - loglik: -3.9435e+02 - logprior: -2.0246e+00
Epoch 8/10
39/39 - 12s - loss: 396.8673 - loglik: -3.9436e+02 - logprior: -2.0215e+00
Epoch 9/10
39/39 - 12s - loss: 396.5641 - loglik: -3.9410e+02 - logprior: -2.0134e+00
Epoch 10/10
39/39 - 12s - loss: 396.6551 - loglik: -3.9422e+02 - logprior: -2.0138e+00
Fitted a model with MAP estimate = -397.0844
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (27, 1), (40, 3), (41, 1), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 2), (71, 2), (89, 2), (92, 1), (93, 2), (95, 1), (102, 1), (103, 2), (104, 2), (105, 1), (107, 1), (109, 1), (118, 1), (122, 1), (126, 1), (127, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 400.5396 - loglik: -3.9682e+02 - logprior: -3.3637e+00
Epoch 2/2
39/39 - 15s - loss: 382.1548 - loglik: -3.7967e+02 - logprior: -1.5442e+00
Fitted a model with MAP estimate = -377.7305
expansions: [(165, 1)]
discards: [ 13  50  70  73  78  91  94 114 121 136 139 161 162 169]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 387.5475 - loglik: -3.8502e+02 - logprior: -2.2635e+00
Epoch 2/2
39/39 - 14s - loss: 381.8605 - loglik: -3.8007e+02 - logprior: -1.0519e+00
Fitted a model with MAP estimate = -380.0144
expansions: [(150, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 19s - loss: 383.7948 - loglik: -3.8166e+02 - logprior: -1.8627e+00
Epoch 2/10
41/41 - 15s - loss: 377.2851 - loglik: -3.7568e+02 - logprior: -8.6375e-01
Epoch 3/10
41/41 - 15s - loss: 375.4320 - loglik: -3.7366e+02 - logprior: -8.0140e-01
Epoch 4/10
41/41 - 15s - loss: 372.4416 - loglik: -3.7075e+02 - logprior: -7.3465e-01
Epoch 5/10
41/41 - 15s - loss: 373.4327 - loglik: -3.7184e+02 - logprior: -7.0110e-01
Fitted a model with MAP estimate = -371.1515
Time for alignment: 337.7362
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 477.6413 - loglik: -4.7521e+02 - logprior: -2.1777e+00
Epoch 2/10
39/39 - 12s - loss: 410.5587 - loglik: -4.0790e+02 - logprior: -1.9170e+00
Epoch 3/10
39/39 - 12s - loss: 402.8201 - loglik: -4.0003e+02 - logprior: -2.0243e+00
Epoch 4/10
39/39 - 12s - loss: 400.1267 - loglik: -3.9737e+02 - logprior: -2.1196e+00
Epoch 5/10
39/39 - 12s - loss: 398.9889 - loglik: -3.9629e+02 - logprior: -2.1198e+00
Epoch 6/10
39/39 - 12s - loss: 398.6034 - loglik: -3.9594e+02 - logprior: -2.1304e+00
Epoch 7/10
39/39 - 12s - loss: 398.2707 - loglik: -3.9563e+02 - logprior: -2.1303e+00
Epoch 8/10
39/39 - 12s - loss: 398.2487 - loglik: -3.9563e+02 - logprior: -2.1329e+00
Epoch 9/10
39/39 - 12s - loss: 397.6920 - loglik: -3.9509e+02 - logprior: -2.1303e+00
Epoch 10/10
39/39 - 12s - loss: 397.9921 - loglik: -3.9541e+02 - logprior: -2.1259e+00
Fitted a model with MAP estimate = -397.6801
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (27, 1), (40, 3), (41, 1), (44, 1), (45, 1), (54, 1), (55, 1), (57, 2), (58, 2), (69, 2), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 2), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 2), (126, 1), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 400.1288 - loglik: -3.9642e+02 - logprior: -3.3979e+00
Epoch 2/2
39/39 - 15s - loss: 383.7455 - loglik: -3.8134e+02 - logprior: -1.6278e+00
Fitted a model with MAP estimate = -379.9962
expansions: []
discards: [ 13  50  74  76  90  93 113 128]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 387.5479 - loglik: -3.8491e+02 - logprior: -2.3850e+00
Epoch 2/2
39/39 - 15s - loss: 382.3044 - loglik: -3.8042e+02 - logprior: -1.2230e+00
Fitted a model with MAP estimate = -380.1694
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 19s - loss: 385.4496 - loglik: -3.8318e+02 - logprior: -1.9987e+00
Epoch 2/10
41/41 - 15s - loss: 379.7204 - loglik: -3.7798e+02 - logprior: -1.0163e+00
Epoch 3/10
41/41 - 15s - loss: 377.4442 - loglik: -3.7561e+02 - logprior: -9.7892e-01
Epoch 4/10
41/41 - 15s - loss: 377.0181 - loglik: -3.7529e+02 - logprior: -9.0793e-01
Epoch 5/10
41/41 - 15s - loss: 374.4868 - loglik: -3.7286e+02 - logprior: -8.3496e-01
Epoch 6/10
41/41 - 15s - loss: 375.7399 - loglik: -3.7425e+02 - logprior: -7.7456e-01
Fitted a model with MAP estimate = -373.7885
Time for alignment: 351.3714
Computed alignments with likelihoods: ['-370.3239', '-371.1515', '-373.7885']
Best model has likelihood: -370.3239
time for generating output: 0.3593
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.5053235053235053
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ae3f520d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69ed580880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fa2cac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6af4f33880>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6af4f1b610>, <__main__.SimpleDirichletPrior object at 0x7f6b166bce20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 171.2993 - loglik: -1.1170e+02 - logprior: -5.9586e+01
Epoch 2/10
10/10 - 1s - loss: 106.2899 - loglik: -8.9027e+01 - logprior: -1.7255e+01
Epoch 3/10
10/10 - 1s - loss: 81.3959 - loglik: -7.2428e+01 - logprior: -8.9332e+00
Epoch 4/10
10/10 - 1s - loss: 71.0995 - loglik: -6.5197e+01 - logprior: -5.8536e+00
Epoch 5/10
10/10 - 1s - loss: 66.4943 - loglik: -6.2183e+01 - logprior: -4.2935e+00
Epoch 6/10
10/10 - 1s - loss: 64.0704 - loglik: -6.0495e+01 - logprior: -3.5506e+00
Epoch 7/10
10/10 - 1s - loss: 62.6287 - loglik: -5.9318e+01 - logprior: -3.1758e+00
Epoch 8/10
10/10 - 1s - loss: 62.2200 - loglik: -5.9041e+01 - logprior: -2.9059e+00
Epoch 9/10
10/10 - 1s - loss: 61.8404 - loglik: -5.8859e+01 - logprior: -2.7015e+00
Epoch 10/10
10/10 - 1s - loss: 61.8179 - loglik: -5.9023e+01 - logprior: -2.5463e+00
Fitted a model with MAP estimate = -61.3335
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.8923 - loglik: -5.7644e+01 - logprior: -8.0232e+01
Epoch 2/2
10/10 - 1s - loss: 80.0327 - loglik: -5.3605e+01 - logprior: -2.6347e+01
Fitted a model with MAP estimate = -68.7491
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.9789 - loglik: -5.1270e+01 - logprior: -6.6690e+01
Epoch 2/2
10/10 - 1s - loss: 74.5336 - loglik: -5.0528e+01 - logprior: -2.3934e+01
Fitted a model with MAP estimate = -65.4601
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.6771 - loglik: -4.9977e+01 - logprior: -5.5681e+01
Epoch 2/10
10/10 - 1s - loss: 67.1584 - loglik: -5.0845e+01 - logprior: -1.6254e+01
Epoch 3/10
10/10 - 1s - loss: 59.6025 - loglik: -5.1267e+01 - logprior: -8.2732e+00
Epoch 4/10
10/10 - 1s - loss: 56.5906 - loglik: -5.1229e+01 - logprior: -5.2060e+00
Epoch 5/10
10/10 - 1s - loss: 55.0596 - loglik: -5.1094e+01 - logprior: -3.7076e+00
Epoch 6/10
10/10 - 1s - loss: 53.9262 - loglik: -5.0904e+01 - logprior: -2.7547e+00
Epoch 7/10
10/10 - 1s - loss: 53.6186 - loglik: -5.1193e+01 - logprior: -2.1381e+00
Epoch 8/10
10/10 - 1s - loss: 53.1318 - loglik: -5.1048e+01 - logprior: -1.7696e+00
Epoch 9/10
10/10 - 1s - loss: 52.8810 - loglik: -5.1021e+01 - logprior: -1.5339e+00
Epoch 10/10
10/10 - 1s - loss: 52.6493 - loglik: -5.0952e+01 - logprior: -1.3597e+00
Fitted a model with MAP estimate = -52.3239
Time for alignment: 29.1327
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 171.3485 - loglik: -1.1175e+02 - logprior: -5.9585e+01
Epoch 2/10
10/10 - 1s - loss: 106.3557 - loglik: -8.9093e+01 - logprior: -1.7254e+01
Epoch 3/10
10/10 - 1s - loss: 81.4172 - loglik: -7.2450e+01 - logprior: -8.9330e+00
Epoch 4/10
10/10 - 1s - loss: 70.7663 - loglik: -6.4850e+01 - logprior: -5.8664e+00
Epoch 5/10
10/10 - 1s - loss: 66.4065 - loglik: -6.2082e+01 - logprior: -4.3070e+00
Epoch 6/10
10/10 - 1s - loss: 64.6377 - loglik: -6.1054e+01 - logprior: -3.5558e+00
Epoch 7/10
10/10 - 1s - loss: 63.7859 - loglik: -6.0467e+01 - logprior: -3.1818e+00
Epoch 8/10
10/10 - 1s - loss: 63.4170 - loglik: -6.0267e+01 - logprior: -2.8870e+00
Epoch 9/10
10/10 - 1s - loss: 63.1217 - loglik: -6.0136e+01 - logprior: -2.7153e+00
Epoch 10/10
10/10 - 1s - loss: 62.7372 - loglik: -5.9857e+01 - logprior: -2.6354e+00
Fitted a model with MAP estimate = -62.3481
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.9962 - loglik: -5.8942e+01 - logprior: -8.0042e+01
Epoch 2/2
10/10 - 1s - loss: 80.2937 - loglik: -5.3716e+01 - logprior: -2.6518e+01
Fitted a model with MAP estimate = -69.3145
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 118.1623 - loglik: -5.1553e+01 - logprior: -6.6591e+01
Epoch 2/2
10/10 - 1s - loss: 74.7994 - loglik: -5.0802e+01 - logprior: -2.3928e+01
Fitted a model with MAP estimate = -65.7004
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.8983 - loglik: -5.0209e+01 - logprior: -5.5670e+01
Epoch 2/10
10/10 - 1s - loss: 67.3551 - loglik: -5.1087e+01 - logprior: -1.6205e+01
Epoch 3/10
10/10 - 1s - loss: 59.7574 - loglik: -5.1454e+01 - logprior: -8.2269e+00
Epoch 4/10
10/10 - 1s - loss: 56.6084 - loglik: -5.1241e+01 - logprior: -5.1803e+00
Epoch 5/10
10/10 - 1s - loss: 55.0240 - loglik: -5.1043e+01 - logprior: -3.7052e+00
Epoch 6/10
10/10 - 1s - loss: 53.9577 - loglik: -5.0959e+01 - logprior: -2.7398e+00
Epoch 7/10
10/10 - 1s - loss: 53.4723 - loglik: -5.1098e+01 - logprior: -2.1285e+00
Epoch 8/10
10/10 - 1s - loss: 53.1188 - loglik: -5.1095e+01 - logprior: -1.7618e+00
Epoch 9/10
10/10 - 1s - loss: 52.8950 - loglik: -5.1095e+01 - logprior: -1.5294e+00
Epoch 10/10
10/10 - 1s - loss: 52.7309 - loglik: -5.1097e+01 - logprior: -1.3541e+00
Fitted a model with MAP estimate = -52.3539
Time for alignment: 29.2240
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 171.2726 - loglik: -1.1167e+02 - logprior: -5.9586e+01
Epoch 2/10
10/10 - 1s - loss: 106.3906 - loglik: -8.9125e+01 - logprior: -1.7257e+01
Epoch 3/10
10/10 - 1s - loss: 81.3386 - loglik: -7.2366e+01 - logprior: -8.9385e+00
Epoch 4/10
10/10 - 1s - loss: 70.4101 - loglik: -6.4469e+01 - logprior: -5.8912e+00
Epoch 5/10
10/10 - 1s - loss: 66.2197 - loglik: -6.1879e+01 - logprior: -4.3241e+00
Epoch 6/10
10/10 - 1s - loss: 64.6568 - loglik: -6.1107e+01 - logprior: -3.5339e+00
Epoch 7/10
10/10 - 1s - loss: 63.7519 - loglik: -6.0518e+01 - logprior: -3.1661e+00
Epoch 8/10
10/10 - 1s - loss: 63.2714 - loglik: -6.0174e+01 - logprior: -2.8976e+00
Epoch 9/10
10/10 - 1s - loss: 62.9925 - loglik: -6.0031e+01 - logprior: -2.7265e+00
Epoch 10/10
10/10 - 1s - loss: 62.6145 - loglik: -5.9777e+01 - logprior: -2.6384e+00
Fitted a model with MAP estimate = -62.3656
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.8558 - loglik: -5.8805e+01 - logprior: -8.0036e+01
Epoch 2/2
10/10 - 1s - loss: 80.4236 - loglik: -5.3864e+01 - logprior: -2.6521e+01
Fitted a model with MAP estimate = -69.3624
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 118.1709 - loglik: -5.1567e+01 - logprior: -6.6585e+01
Epoch 2/2
10/10 - 1s - loss: 74.8750 - loglik: -5.0929e+01 - logprior: -2.3909e+01
Fitted a model with MAP estimate = -65.8880
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.9764 - loglik: -5.0298e+01 - logprior: -5.5655e+01
Epoch 2/10
10/10 - 1s - loss: 67.2205 - loglik: -5.0966e+01 - logprior: -1.6199e+01
Epoch 3/10
10/10 - 1s - loss: 59.9681 - loglik: -5.1786e+01 - logprior: -8.1666e+00
Epoch 4/10
10/10 - 1s - loss: 56.9443 - loglik: -5.1844e+01 - logprior: -5.0741e+00
Epoch 5/10
10/10 - 1s - loss: 55.0456 - loglik: -5.1221e+01 - logprior: -3.6851e+00
Epoch 6/10
10/10 - 1s - loss: 54.1189 - loglik: -5.1165e+01 - logprior: -2.7050e+00
Epoch 7/10
10/10 - 1s - loss: 53.5433 - loglik: -5.1188e+01 - logprior: -2.1077e+00
Epoch 8/10
10/10 - 1s - loss: 53.1528 - loglik: -5.1162e+01 - logprior: -1.7516e+00
Epoch 9/10
10/10 - 1s - loss: 52.9162 - loglik: -5.1143e+01 - logprior: -1.5139e+00
Epoch 10/10
10/10 - 1s - loss: 52.8128 - loglik: -5.1193e+01 - logprior: -1.3491e+00
Fitted a model with MAP estimate = -52.3892
Time for alignment: 28.9070
Computed alignments with likelihoods: ['-52.3239', '-52.3539', '-52.3892']
Best model has likelihood: -52.3239
time for generating output: 0.1016
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9734299516908212
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac7f8db20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1603e940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f2d66a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b2fa153a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6abee6f760>, <__main__.SimpleDirichletPrior object at 0x7f6ac9402e20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 372.9062 - loglik: -3.6982e+02 - logprior: -2.9636e+00
Epoch 2/10
20/20 - 4s - loss: 328.7197 - loglik: -3.2656e+02 - logprior: -1.4380e+00
Epoch 3/10
20/20 - 4s - loss: 308.4503 - loglik: -3.0543e+02 - logprior: -1.7733e+00
Epoch 4/10
20/20 - 5s - loss: 303.1793 - loglik: -3.0026e+02 - logprior: -1.7247e+00
Epoch 5/10
20/20 - 4s - loss: 301.0151 - loglik: -2.9820e+02 - logprior: -1.7539e+00
Epoch 6/10
20/20 - 5s - loss: 298.8706 - loglik: -2.9621e+02 - logprior: -1.7617e+00
Epoch 7/10
20/20 - 4s - loss: 297.9379 - loglik: -2.9542e+02 - logprior: -1.7571e+00
Epoch 8/10
20/20 - 5s - loss: 297.7747 - loglik: -2.9535e+02 - logprior: -1.7439e+00
Epoch 9/10
20/20 - 5s - loss: 296.9615 - loglik: -2.9461e+02 - logprior: -1.7371e+00
Epoch 10/10
20/20 - 4s - loss: 296.9621 - loglik: -2.9466e+02 - logprior: -1.7344e+00
Fitted a model with MAP estimate = -290.2126
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (18, 1), (19, 1), (26, 2), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (56, 2), (57, 2), (58, 1), (59, 1), (60, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 3), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 309.6042 - loglik: -3.0625e+02 - logprior: -3.0110e+00
Epoch 2/2
40/40 - 7s - loss: 289.3111 - loglik: -2.8706e+02 - logprior: -1.3390e+00
Fitted a model with MAP estimate = -273.2619
expansions: []
discards: [  8  13  35  47  76 102 105 109 127]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 295.0537 - loglik: -2.9266e+02 - logprior: -2.1405e+00
Epoch 2/2
40/40 - 7s - loss: 287.7550 - loglik: -2.8580e+02 - logprior: -1.0927e+00
Fitted a model with MAP estimate = -272.9071
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 275.1397 - loglik: -2.7347e+02 - logprior: -1.2257e+00
Epoch 2/10
57/57 - 9s - loss: 268.7998 - loglik: -2.6654e+02 - logprior: -1.0097e+00
Epoch 3/10
57/57 - 10s - loss: 267.3072 - loglik: -2.6483e+02 - logprior: -1.0004e+00
Epoch 4/10
57/57 - 9s - loss: 265.6896 - loglik: -2.6343e+02 - logprior: -9.7673e-01
Epoch 5/10
57/57 - 9s - loss: 265.0444 - loglik: -2.6287e+02 - logprior: -9.4634e-01
Epoch 6/10
57/57 - 9s - loss: 264.3841 - loglik: -2.6222e+02 - logprior: -9.2449e-01
Epoch 7/10
57/57 - 9s - loss: 264.1488 - loglik: -2.6223e+02 - logprior: -9.0044e-01
Epoch 8/10
57/57 - 9s - loss: 262.9025 - loglik: -2.6106e+02 - logprior: -8.6606e-01
Epoch 9/10
57/57 - 9s - loss: 262.6044 - loglik: -2.6084e+02 - logprior: -8.4356e-01
Epoch 10/10
57/57 - 9s - loss: 262.1839 - loglik: -2.6059e+02 - logprior: -8.0620e-01
Fitted a model with MAP estimate = -261.1896
Time for alignment: 238.2910
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.6087 - loglik: -3.6952e+02 - logprior: -2.9663e+00
Epoch 2/10
20/20 - 4s - loss: 329.0591 - loglik: -3.2695e+02 - logprior: -1.4218e+00
Epoch 3/10
20/20 - 5s - loss: 307.5666 - loglik: -3.0499e+02 - logprior: -1.8107e+00
Epoch 4/10
20/20 - 4s - loss: 301.8471 - loglik: -2.9911e+02 - logprior: -1.8382e+00
Epoch 5/10
20/20 - 4s - loss: 300.4362 - loglik: -2.9768e+02 - logprior: -1.8445e+00
Epoch 6/10
20/20 - 4s - loss: 299.0918 - loglik: -2.9644e+02 - logprior: -1.7939e+00
Epoch 7/10
20/20 - 5s - loss: 299.0037 - loglik: -2.9645e+02 - logprior: -1.7798e+00
Epoch 8/10
20/20 - 4s - loss: 297.8264 - loglik: -2.9536e+02 - logprior: -1.7778e+00
Epoch 9/10
20/20 - 5s - loss: 298.0498 - loglik: -2.9566e+02 - logprior: -1.7642e+00
Fitted a model with MAP estimate = -288.9567
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (18, 1), (19, 1), (23, 2), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 2), (76, 2), (81, 1), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 309.5018 - loglik: -3.0614e+02 - logprior: -3.0090e+00
Epoch 2/2
40/40 - 7s - loss: 290.1697 - loglik: -2.8782e+02 - logprior: -1.2901e+00
Fitted a model with MAP estimate = -273.7839
expansions: []
discards: [ 8 13 31 47 82 86]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 294.8448 - loglik: -2.9250e+02 - logprior: -2.1382e+00
Epoch 2/2
40/40 - 7s - loss: 288.0623 - loglik: -2.8641e+02 - logprior: -1.0754e+00
Fitted a model with MAP estimate = -273.5460
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 274.8297 - loglik: -2.7316e+02 - logprior: -1.2245e+00
Epoch 2/10
57/57 - 9s - loss: 269.2159 - loglik: -2.6695e+02 - logprior: -1.0111e+00
Epoch 3/10
57/57 - 10s - loss: 266.6555 - loglik: -2.6421e+02 - logprior: -9.9038e-01
Epoch 4/10
57/57 - 9s - loss: 265.7960 - loglik: -2.6357e+02 - logprior: -9.7090e-01
Epoch 5/10
57/57 - 9s - loss: 265.2650 - loglik: -2.6313e+02 - logprior: -9.3777e-01
Epoch 6/10
57/57 - 10s - loss: 264.5100 - loglik: -2.6238e+02 - logprior: -9.1516e-01
Epoch 7/10
57/57 - 9s - loss: 263.3029 - loglik: -2.6140e+02 - logprior: -8.8845e-01
Epoch 8/10
57/57 - 10s - loss: 262.8581 - loglik: -2.6102e+02 - logprior: -8.6156e-01
Epoch 9/10
57/57 - 9s - loss: 262.9106 - loglik: -2.6116e+02 - logprior: -8.3256e-01
Fitted a model with MAP estimate = -261.3052
Time for alignment: 224.0332
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 373.0074 - loglik: -3.6992e+02 - logprior: -2.9656e+00
Epoch 2/10
20/20 - 5s - loss: 328.1875 - loglik: -3.2604e+02 - logprior: -1.4359e+00
Epoch 3/10
20/20 - 5s - loss: 308.2411 - loglik: -3.0524e+02 - logprior: -1.7584e+00
Epoch 4/10
20/20 - 5s - loss: 303.5723 - loglik: -3.0067e+02 - logprior: -1.7312e+00
Epoch 5/10
20/20 - 4s - loss: 301.2386 - loglik: -2.9847e+02 - logprior: -1.7341e+00
Epoch 6/10
20/20 - 5s - loss: 299.1252 - loglik: -2.9651e+02 - logprior: -1.7461e+00
Epoch 7/10
20/20 - 5s - loss: 298.7108 - loglik: -2.9624e+02 - logprior: -1.7273e+00
Epoch 8/10
20/20 - 5s - loss: 297.8006 - loglik: -2.9543e+02 - logprior: -1.7076e+00
Epoch 9/10
20/20 - 5s - loss: 297.7509 - loglik: -2.9543e+02 - logprior: -1.7103e+00
Epoch 10/10
20/20 - 4s - loss: 297.2888 - loglik: -2.9503e+02 - logprior: -1.7026e+00
Fitted a model with MAP estimate = -290.7397
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (26, 2), (35, 1), (36, 2), (38, 1), (47, 1), (50, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 310.0413 - loglik: -3.0667e+02 - logprior: -3.0168e+00
Epoch 2/2
40/40 - 7s - loss: 289.8799 - loglik: -2.8753e+02 - logprior: -1.2978e+00
Fitted a model with MAP estimate = -273.5374
expansions: [(99, 1), (100, 2)]
discards: [  8  13  35  47  85 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 295.3020 - loglik: -2.9290e+02 - logprior: -2.1502e+00
Epoch 2/2
40/40 - 7s - loss: 287.5706 - loglik: -2.8564e+02 - logprior: -1.0754e+00
Fitted a model with MAP estimate = -272.7161
expansions: []
discards: [78]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 274.9897 - loglik: -2.7336e+02 - logprior: -1.2146e+00
Epoch 2/10
57/57 - 10s - loss: 268.9262 - loglik: -2.6671e+02 - logprior: -9.8436e-01
Epoch 3/10
57/57 - 9s - loss: 266.8214 - loglik: -2.6434e+02 - logprior: -9.8503e-01
Epoch 4/10
57/57 - 9s - loss: 266.1678 - loglik: -2.6391e+02 - logprior: -9.6161e-01
Epoch 5/10
57/57 - 10s - loss: 264.4857 - loglik: -2.6233e+02 - logprior: -9.3035e-01
Epoch 6/10
57/57 - 9s - loss: 264.1106 - loglik: -2.6195e+02 - logprior: -9.1486e-01
Epoch 7/10
57/57 - 9s - loss: 263.6106 - loglik: -2.6170e+02 - logprior: -8.8413e-01
Epoch 8/10
57/57 - 9s - loss: 262.9402 - loglik: -2.6113e+02 - logprior: -8.5880e-01
Epoch 9/10
57/57 - 10s - loss: 262.8691 - loglik: -2.6113e+02 - logprior: -8.3047e-01
Epoch 10/10
57/57 - 9s - loss: 262.2318 - loglik: -2.6065e+02 - logprior: -7.9745e-01
Fitted a model with MAP estimate = -261.0363
Time for alignment: 237.1319
Computed alignments with likelihoods: ['-261.1896', '-261.3052', '-261.0363']
Best model has likelihood: -261.0363
time for generating output: 0.3689
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6088590604026846
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ad2a042b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b166bca00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bc310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b166bb5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ad2a12c40>, <__main__.SimpleDirichletPrior object at 0x7f6ac918fb20>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 687.4767 - loglik: -6.7654e+02 - logprior: -1.0816e+01
Epoch 2/10
19/19 - 9s - loss: 620.0529 - loglik: -6.1743e+02 - logprior: -1.8056e+00
Epoch 3/10
19/19 - 9s - loss: 588.8981 - loglik: -5.8569e+02 - logprior: -1.8511e+00
Epoch 4/10
19/19 - 9s - loss: 579.1531 - loglik: -5.7535e+02 - logprior: -1.9377e+00
Epoch 5/10
19/19 - 9s - loss: 574.8988 - loglik: -5.7087e+02 - logprior: -1.9180e+00
Epoch 6/10
19/19 - 9s - loss: 569.8323 - loglik: -5.6560e+02 - logprior: -2.2810e+00
Epoch 7/10
19/19 - 9s - loss: 566.4967 - loglik: -5.6208e+02 - logprior: -2.5723e+00
Epoch 8/10
19/19 - 9s - loss: 564.9357 - loglik: -5.6056e+02 - logprior: -2.7132e+00
Epoch 9/10
19/19 - 9s - loss: 562.9699 - loglik: -5.5879e+02 - logprior: -2.7356e+00
Epoch 10/10
19/19 - 9s - loss: 562.4871 - loglik: -5.5845e+02 - logprior: -2.7435e+00
Fitted a model with MAP estimate = -560.8635
expansions: [(25, 1), (27, 1), (28, 1), (30, 2), (43, 5), (64, 3), (65, 2), (91, 8), (92, 1), (103, 1), (118, 1), (120, 1), (136, 1), (154, 3), (163, 1), (166, 4), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [170 171 172 173 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 649.6823 - loglik: -6.3746e+02 - logprior: -1.2120e+01
Epoch 2/2
19/19 - 10s - loss: 594.7114 - loglik: -5.9068e+02 - logprior: -3.4526e+00
Fitted a model with MAP estimate = -583.6769
expansions: []
discards: [ 29  35  36  37  38  39  40  41  53  75  76  78 113 114 115 116 185 186
 187 188 195 196 197 198 199 200 201 202 203 204 205 206 207 208 210 211
 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 616.5443 - loglik: -6.0588e+02 - logprior: -1.0571e+01
Epoch 2/2
19/19 - 8s - loss: 598.7173 - loglik: -5.9676e+02 - logprior: -1.4649e+00
Fitted a model with MAP estimate = -592.7986
expansions: [(38, 3), (42, 1), (167, 5), (168, 1), (169, 2), (174, 9), (176, 25)]
discards: [33]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 604.5176 - loglik: -5.9460e+02 - logprior: -9.8268e+00
Epoch 2/10
19/19 - 10s - loss: 580.5023 - loglik: -5.7878e+02 - logprior: -1.2304e+00
Epoch 3/10
19/19 - 10s - loss: 570.1001 - loglik: -5.6820e+02 - logprior: -7.1768e-01
Epoch 4/10
19/19 - 10s - loss: 565.1887 - loglik: -5.6303e+02 - logprior: -5.0412e-01
Epoch 5/10
19/19 - 10s - loss: 560.4330 - loglik: -5.5803e+02 - logprior: -5.9594e-01
Epoch 6/10
19/19 - 10s - loss: 554.7404 - loglik: -5.5219e+02 - logprior: -7.0288e-01
Epoch 7/10
19/19 - 10s - loss: 554.0203 - loglik: -5.5151e+02 - logprior: -7.1433e-01
Epoch 8/10
19/19 - 10s - loss: 551.8745 - loglik: -5.4953e+02 - logprior: -6.4578e-01
Epoch 9/10
19/19 - 10s - loss: 550.8829 - loglik: -5.4872e+02 - logprior: -5.4505e-01
Epoch 10/10
19/19 - 10s - loss: 549.5747 - loglik: -5.4760e+02 - logprior: -4.2182e-01
Fitted a model with MAP estimate = -547.6927
Time for alignment: 258.0201
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 688.1308 - loglik: -6.7720e+02 - logprior: -1.0815e+01
Epoch 2/10
19/19 - 9s - loss: 619.0304 - loglik: -6.1642e+02 - logprior: -1.8054e+00
Epoch 3/10
19/19 - 9s - loss: 588.1855 - loglik: -5.8471e+02 - logprior: -2.1252e+00
Epoch 4/10
19/19 - 9s - loss: 579.1301 - loglik: -5.7479e+02 - logprior: -2.5277e+00
Epoch 5/10
19/19 - 9s - loss: 571.5220 - loglik: -5.6706e+02 - logprior: -2.5254e+00
Epoch 6/10
19/19 - 9s - loss: 568.0949 - loglik: -5.6349e+02 - logprior: -2.6880e+00
Epoch 7/10
19/19 - 9s - loss: 566.3235 - loglik: -5.6177e+02 - logprior: -2.7875e+00
Epoch 8/10
19/19 - 9s - loss: 565.3490 - loglik: -5.6097e+02 - logprior: -2.8297e+00
Epoch 9/10
19/19 - 9s - loss: 563.8732 - loglik: -5.5968e+02 - logprior: -2.8270e+00
Epoch 10/10
19/19 - 9s - loss: 562.4643 - loglik: -5.5835e+02 - logprior: -2.8510e+00
Fitted a model with MAP estimate = -561.3210
expansions: [(25, 1), (27, 1), (28, 1), (30, 2), (43, 1), (44, 7), (64, 3), (89, 7), (102, 1), (117, 1), (119, 1), (134, 1), (135, 1), (150, 4), (152, 1), (162, 1), (163, 1), (166, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [169 170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 645.7175 - loglik: -6.3373e+02 - logprior: -1.1885e+01
Epoch 2/2
19/19 - 10s - loss: 593.0214 - loglik: -5.8925e+02 - logprior: -3.2084e+00
Fitted a model with MAP estimate = -583.0222
expansions: []
discards: [ 29  35  36  37  38  39  40  41  48  57  58  59 112 185 186 187 188 189
 190 191 192 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226
 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 617.1437 - loglik: -6.0641e+02 - logprior: -1.0640e+01
Epoch 2/2
19/19 - 8s - loss: 597.9370 - loglik: -5.9592e+02 - logprior: -1.5017e+00
Fitted a model with MAP estimate = -592.8364
expansions: [(33, 2), (37, 3), (38, 2), (100, 1), (172, 19)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 609.1849 - loglik: -5.9898e+02 - logprior: -1.0114e+01
Epoch 2/10
19/19 - 9s - loss: 588.9662 - loglik: -5.8712e+02 - logprior: -1.3633e+00
Epoch 3/10
19/19 - 9s - loss: 579.9738 - loglik: -5.7803e+02 - logprior: -8.1024e-01
Epoch 4/10
19/19 - 9s - loss: 574.9740 - loglik: -5.7273e+02 - logprior: -6.9297e-01
Epoch 5/10
19/19 - 9s - loss: 569.0889 - loglik: -5.6677e+02 - logprior: -5.9917e-01
Epoch 6/10
19/19 - 9s - loss: 565.4702 - loglik: -5.6305e+02 - logprior: -6.2455e-01
Epoch 7/10
19/19 - 9s - loss: 563.4361 - loglik: -5.6108e+02 - logprior: -5.6571e-01
Epoch 8/10
19/19 - 9s - loss: 563.1407 - loglik: -5.6104e+02 - logprior: -3.7981e-01
Epoch 9/10
19/19 - 9s - loss: 557.1740 - loglik: -5.5532e+02 - logprior: -2.3312e-01
Epoch 10/10
19/19 - 9s - loss: 558.2900 - loglik: -5.5672e+02 - logprior: -4.2249e-02
Fitted a model with MAP estimate = -556.1230
Time for alignment: 248.0025
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 687.7690 - loglik: -6.7684e+02 - logprior: -1.0818e+01
Epoch 2/10
19/19 - 9s - loss: 620.6463 - loglik: -6.1810e+02 - logprior: -1.7330e+00
Epoch 3/10
19/19 - 9s - loss: 587.7582 - loglik: -5.8456e+02 - logprior: -1.9376e+00
Epoch 4/10
19/19 - 9s - loss: 579.1326 - loglik: -5.7549e+02 - logprior: -2.5164e+00
Epoch 5/10
19/19 - 9s - loss: 572.7889 - loglik: -5.6885e+02 - logprior: -2.4914e+00
Epoch 6/10
19/19 - 9s - loss: 568.9378 - loglik: -5.6471e+02 - logprior: -2.5746e+00
Epoch 7/10
19/19 - 9s - loss: 567.8979 - loglik: -5.6371e+02 - logprior: -2.5914e+00
Epoch 8/10
19/19 - 9s - loss: 566.6174 - loglik: -5.6253e+02 - logprior: -2.6205e+00
Epoch 9/10
19/19 - 9s - loss: 563.1106 - loglik: -5.5908e+02 - logprior: -2.6587e+00
Epoch 10/10
19/19 - 9s - loss: 562.1565 - loglik: -5.5815e+02 - logprior: -2.6929e+00
Fitted a model with MAP estimate = -561.1177
expansions: [(0, 2), (25, 1), (27, 1), (28, 1), (30, 2), (41, 1), (43, 5), (64, 2), (65, 2), (102, 1), (117, 1), (119, 1), (134, 1), (135, 1), (150, 4), (151, 1), (163, 1), (165, 4), (183, 1), (184, 2), (185, 1)]
discards: [  1   2 169 170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 654.0735 - loglik: -6.3778e+02 - logprior: -1.6190e+01
Epoch 2/2
19/19 - 10s - loss: 593.9907 - loglik: -5.8901e+02 - logprior: -4.4275e+00
Fitted a model with MAP estimate = -581.4432
expansions: [(52, 1), (102, 1), (104, 2), (105, 3), (106, 1)]
discards: [ 29  35  36  37  47  54 176 177 178 179 180 181 182 183 188 189 190 191
 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209
 210 211 212 213 214 215 216 217 218 219 220 221 222]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 616.1375 - loglik: -6.0527e+02 - logprior: -1.0771e+01
Epoch 2/2
19/19 - 8s - loss: 596.0123 - loglik: -5.9398e+02 - logprior: -1.5519e+00
Fitted a model with MAP estimate = -590.1909
expansions: [(30, 1), (32, 3), (73, 1), (102, 1), (182, 19)]
discards: [178]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 607.3853 - loglik: -5.9717e+02 - logprior: -1.0133e+01
Epoch 2/10
19/19 - 9s - loss: 587.1792 - loglik: -5.8513e+02 - logprior: -1.5622e+00
Epoch 3/10
19/19 - 9s - loss: 579.1291 - loglik: -5.7695e+02 - logprior: -9.7207e-01
Epoch 4/10
19/19 - 9s - loss: 571.8595 - loglik: -5.6955e+02 - logprior: -6.2489e-01
Epoch 5/10
19/19 - 9s - loss: 567.7048 - loglik: -5.6537e+02 - logprior: -4.9142e-01
Epoch 6/10
19/19 - 9s - loss: 562.5397 - loglik: -5.6007e+02 - logprior: -5.3610e-01
Epoch 7/10
19/19 - 9s - loss: 560.3443 - loglik: -5.5779e+02 - logprior: -5.8284e-01
Epoch 8/10
19/19 - 9s - loss: 557.3811 - loglik: -5.5503e+02 - logprior: -4.4163e-01
Epoch 9/10
19/19 - 9s - loss: 556.3958 - loglik: -5.5433e+02 - logprior: -2.9065e-01
Epoch 10/10
19/19 - 9s - loss: 554.7100 - loglik: -5.5301e+02 - logprior: -7.8462e-02
Fitted a model with MAP estimate = -553.2184
Time for alignment: 250.7083
Computed alignments with likelihoods: ['-547.6927', '-556.1230', '-553.2184']
Best model has likelihood: -547.6927
time for generating output: 0.3704
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.72901878914405
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac96cbfd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad6c3eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05b66550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1f52be80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb306820>, <__main__.SimpleDirichletPrior object at 0x7f6b1f0ba8b0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.4544 - loglik: -1.5114e+02 - logprior: -3.3061e+00
Epoch 2/10
19/19 - 1s - loss: 121.4869 - loglik: -1.1987e+02 - logprior: -1.5853e+00
Epoch 3/10
19/19 - 1s - loss: 106.2739 - loglik: -1.0441e+02 - logprior: -1.6283e+00
Epoch 4/10
19/19 - 1s - loss: 102.4248 - loglik: -1.0039e+02 - logprior: -1.8059e+00
Epoch 5/10
19/19 - 1s - loss: 101.8745 - loglik: -9.9991e+01 - logprior: -1.6821e+00
Epoch 6/10
19/19 - 1s - loss: 101.2627 - loglik: -9.9380e+01 - logprior: -1.6856e+00
Epoch 7/10
19/19 - 1s - loss: 100.9893 - loglik: -9.9138e+01 - logprior: -1.6537e+00
Epoch 8/10
19/19 - 1s - loss: 101.0203 - loglik: -9.9176e+01 - logprior: -1.6471e+00
Fitted a model with MAP estimate = -96.4448
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 102.4496 - loglik: -9.9221e+01 - logprior: -3.1810e+00
Epoch 2/2
19/19 - 1s - loss: 94.7323 - loglik: -9.3212e+01 - logprior: -1.4280e+00
Fitted a model with MAP estimate = -89.7916
expansions: []
discards: [21 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.8281 - loglik: -9.3712e+01 - logprior: -3.0846e+00
Epoch 2/2
19/19 - 1s - loss: 94.0583 - loglik: -9.2632e+01 - logprior: -1.3161e+00
Fitted a model with MAP estimate = -89.5312
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 95.2376 - loglik: -9.1739e+01 - logprior: -3.4606e+00
Epoch 2/10
21/21 - 2s - loss: 91.5322 - loglik: -8.9327e+01 - logprior: -2.0893e+00
Epoch 3/10
21/21 - 2s - loss: 90.4064 - loglik: -8.8150e+01 - logprior: -2.0650e+00
Epoch 4/10
21/21 - 2s - loss: 88.5404 - loglik: -8.6759e+01 - logprior: -1.5296e+00
Epoch 5/10
21/21 - 2s - loss: 88.1373 - loglik: -8.6441e+01 - logprior: -1.4283e+00
Epoch 6/10
21/21 - 2s - loss: 87.9304 - loglik: -8.6280e+01 - logprior: -1.3906e+00
Epoch 7/10
21/21 - 2s - loss: 87.4338 - loglik: -8.5810e+01 - logprior: -1.3676e+00
Epoch 8/10
21/21 - 2s - loss: 87.3304 - loglik: -8.5723e+01 - logprior: -1.3459e+00
Epoch 9/10
21/21 - 2s - loss: 86.7859 - loglik: -8.5198e+01 - logprior: -1.3295e+00
Epoch 10/10
21/21 - 2s - loss: 87.0111 - loglik: -8.5440e+01 - logprior: -1.3189e+00
Fitted a model with MAP estimate = -86.6205
Time for alignment: 59.2865
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3858 - loglik: -1.5107e+02 - logprior: -3.3053e+00
Epoch 2/10
19/19 - 1s - loss: 120.0142 - loglik: -1.1840e+02 - logprior: -1.5869e+00
Epoch 3/10
19/19 - 1s - loss: 106.1895 - loglik: -1.0422e+02 - logprior: -1.6734e+00
Epoch 4/10
19/19 - 1s - loss: 103.3736 - loglik: -1.0131e+02 - logprior: -1.7751e+00
Epoch 5/10
19/19 - 1s - loss: 102.1638 - loglik: -1.0028e+02 - logprior: -1.6649e+00
Epoch 6/10
19/19 - 1s - loss: 101.9894 - loglik: -1.0013e+02 - logprior: -1.6638e+00
Epoch 7/10
19/19 - 1s - loss: 101.4781 - loglik: -9.9640e+01 - logprior: -1.6479e+00
Epoch 8/10
19/19 - 1s - loss: 101.3162 - loglik: -9.9482e+01 - logprior: -1.6362e+00
Epoch 9/10
19/19 - 1s - loss: 101.0252 - loglik: -9.9193e+01 - logprior: -1.6351e+00
Epoch 10/10
19/19 - 2s - loss: 101.3487 - loglik: -9.9523e+01 - logprior: -1.6257e+00
Fitted a model with MAP estimate = -96.8987
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.4389 - loglik: -1.0017e+02 - logprior: -3.2137e+00
Epoch 2/2
19/19 - 2s - loss: 94.9520 - loglik: -9.3384e+01 - logprior: -1.4513e+00
Fitted a model with MAP estimate = -89.8526
expansions: []
discards: [ 1 21 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.8588 - loglik: -9.4564e+01 - logprior: -3.2582e+00
Epoch 2/2
19/19 - 2s - loss: 94.6609 - loglik: -9.3078e+01 - logprior: -1.4686e+00
Fitted a model with MAP estimate = -89.9772
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 92.2422 - loglik: -8.9511e+01 - logprior: -2.6945e+00
Epoch 2/10
21/21 - 2s - loss: 89.5309 - loglik: -8.7985e+01 - logprior: -1.4348e+00
Epoch 3/10
21/21 - 2s - loss: 89.1006 - loglik: -8.7534e+01 - logprior: -1.3788e+00
Epoch 4/10
21/21 - 2s - loss: 87.7793 - loglik: -8.6183e+01 - logprior: -1.3500e+00
Epoch 5/10
21/21 - 2s - loss: 87.5201 - loglik: -8.5918e+01 - logprior: -1.3371e+00
Epoch 6/10
21/21 - 2s - loss: 87.3025 - loglik: -8.5718e+01 - logprior: -1.3264e+00
Epoch 7/10
21/21 - 2s - loss: 87.0016 - loglik: -8.5439e+01 - logprior: -1.3086e+00
Epoch 8/10
21/21 - 2s - loss: 86.7309 - loglik: -8.5171e+01 - logprior: -1.3029e+00
Epoch 9/10
21/21 - 2s - loss: 86.8344 - loglik: -8.5274e+01 - logprior: -1.3014e+00
Fitted a model with MAP estimate = -86.3515
Time for alignment: 59.3734
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.3673 - loglik: -1.5106e+02 - logprior: -3.2988e+00
Epoch 2/10
19/19 - 1s - loss: 121.8775 - loglik: -1.2029e+02 - logprior: -1.5550e+00
Epoch 3/10
19/19 - 1s - loss: 108.4259 - loglik: -1.0647e+02 - logprior: -1.6514e+00
Epoch 4/10
19/19 - 1s - loss: 103.7666 - loglik: -1.0167e+02 - logprior: -1.7552e+00
Epoch 5/10
19/19 - 1s - loss: 102.2545 - loglik: -1.0038e+02 - logprior: -1.6588e+00
Epoch 6/10
19/19 - 1s - loss: 101.7728 - loglik: -9.9887e+01 - logprior: -1.6743e+00
Epoch 7/10
19/19 - 1s - loss: 101.5486 - loglik: -9.9704e+01 - logprior: -1.6499e+00
Epoch 8/10
19/19 - 1s - loss: 101.3424 - loglik: -9.9497e+01 - logprior: -1.6386e+00
Epoch 9/10
19/19 - 1s - loss: 101.2891 - loglik: -9.9448e+01 - logprior: -1.6316e+00
Epoch 10/10
19/19 - 1s - loss: 101.2586 - loglik: -9.9406e+01 - logprior: -1.6316e+00
Fitted a model with MAP estimate = -96.9092
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.4693 - loglik: -1.0020e+02 - logprior: -3.2191e+00
Epoch 2/2
19/19 - 1s - loss: 94.9153 - loglik: -9.3328e+01 - logprior: -1.4549e+00
Fitted a model with MAP estimate = -89.9123
expansions: []
discards: [21 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.8757 - loglik: -9.3744e+01 - logprior: -3.0889e+00
Epoch 2/2
19/19 - 1s - loss: 94.1163 - loglik: -9.2684e+01 - logprior: -1.3173e+00
Fitted a model with MAP estimate = -89.5790
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.9551 - loglik: -9.1484e+01 - logprior: -3.4346e+00
Epoch 2/10
21/21 - 2s - loss: 91.7355 - loglik: -8.9533e+01 - logprior: -2.0813e+00
Epoch 3/10
21/21 - 2s - loss: 90.5160 - loglik: -8.8309e+01 - logprior: -2.0114e+00
Epoch 4/10
21/21 - 2s - loss: 88.6078 - loglik: -8.6891e+01 - logprior: -1.4670e+00
Epoch 5/10
21/21 - 2s - loss: 88.0874 - loglik: -8.6403e+01 - logprior: -1.4167e+00
Epoch 6/10
21/21 - 2s - loss: 87.5910 - loglik: -8.5931e+01 - logprior: -1.4033e+00
Epoch 7/10
21/21 - 2s - loss: 87.4822 - loglik: -8.5865e+01 - logprior: -1.3661e+00
Epoch 8/10
21/21 - 2s - loss: 87.0509 - loglik: -8.5441e+01 - logprior: -1.3453e+00
Epoch 9/10
21/21 - 2s - loss: 87.1599 - loglik: -8.5572e+01 - logprior: -1.3305e+00
Fitted a model with MAP estimate = -86.7141
Time for alignment: 59.1699
Computed alignments with likelihoods: ['-86.6205', '-86.3515', '-86.7141']
Best model has likelihood: -86.3515
time for generating output: 0.1334
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9098518995492595
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6aad6328e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad63f6a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad63f580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abeaee310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16a8cbe0>, <__main__.SimpleDirichletPrior object at 0x7f6ac9544a30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 418.0431 - loglik: -4.1288e+02 - logprior: -5.1030e+00
Epoch 2/10
27/27 - 5s - loss: 332.0615 - loglik: -3.2932e+02 - logprior: -2.3788e+00
Epoch 3/10
27/27 - 5s - loss: 317.3214 - loglik: -3.1432e+02 - logprior: -2.4516e+00
Epoch 4/10
27/27 - 6s - loss: 313.7918 - loglik: -3.1082e+02 - logprior: -2.5227e+00
Epoch 5/10
27/27 - 5s - loss: 312.8641 - loglik: -3.0990e+02 - logprior: -2.5321e+00
Epoch 6/10
27/27 - 5s - loss: 312.1277 - loglik: -3.0920e+02 - logprior: -2.5102e+00
Epoch 7/10
27/27 - 5s - loss: 311.6447 - loglik: -3.0873e+02 - logprior: -2.5048e+00
Epoch 8/10
27/27 - 5s - loss: 311.7863 - loglik: -3.0889e+02 - logprior: -2.4968e+00
Fitted a model with MAP estimate = -311.1014
expansions: [(0, 2), (10, 1), (21, 1), (25, 2), (26, 1), (33, 1), (36, 1), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 2), (93, 2), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 309.3694 - loglik: -3.0221e+02 - logprior: -6.9924e+00
Epoch 2/2
27/27 - 6s - loss: 290.2926 - loglik: -2.8773e+02 - logprior: -2.0482e+00
Fitted a model with MAP estimate = -287.3854
expansions: [(42, 1)]
discards: [ 29  47  53  66  87 110 138]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 295.6674 - loglik: -2.9096e+02 - logprior: -4.5685e+00
Epoch 2/2
27/27 - 6s - loss: 289.1784 - loglik: -2.8738e+02 - logprior: -1.3635e+00
Fitted a model with MAP estimate = -287.1875
expansions: []
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 296.0463 - loglik: -2.9157e+02 - logprior: -4.3414e+00
Epoch 2/10
27/27 - 6s - loss: 289.9091 - loglik: -2.8816e+02 - logprior: -1.3150e+00
Epoch 3/10
27/27 - 6s - loss: 288.0653 - loglik: -2.8647e+02 - logprior: -1.0682e+00
Epoch 4/10
27/27 - 6s - loss: 287.2583 - loglik: -2.8578e+02 - logprior: -9.8065e-01
Epoch 5/10
27/27 - 6s - loss: 287.4537 - loglik: -2.8612e+02 - logprior: -8.7214e-01
Fitted a model with MAP estimate = -286.1641
Time for alignment: 139.5086
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.4387 - loglik: -4.1228e+02 - logprior: -5.0961e+00
Epoch 2/10
27/27 - 5s - loss: 328.1314 - loglik: -3.2524e+02 - logprior: -2.5113e+00
Epoch 3/10
27/27 - 5s - loss: 313.9975 - loglik: -3.1087e+02 - logprior: -2.5973e+00
Epoch 4/10
27/27 - 6s - loss: 311.6309 - loglik: -3.0859e+02 - logprior: -2.6192e+00
Epoch 5/10
27/27 - 5s - loss: 310.2000 - loglik: -3.0717e+02 - logprior: -2.6353e+00
Epoch 6/10
27/27 - 5s - loss: 309.8971 - loglik: -3.0690e+02 - logprior: -2.6163e+00
Epoch 7/10
27/27 - 5s - loss: 309.6917 - loglik: -3.0671e+02 - logprior: -2.6052e+00
Epoch 8/10
27/27 - 5s - loss: 309.2201 - loglik: -3.0625e+02 - logprior: -2.5995e+00
Epoch 9/10
27/27 - 5s - loss: 308.9923 - loglik: -3.0602e+02 - logprior: -2.6031e+00
Epoch 10/10
27/27 - 5s - loss: 309.2350 - loglik: -3.0626e+02 - logprior: -2.5955e+00
Fitted a model with MAP estimate = -308.5110
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (82, 2), (93, 2), (99, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 306.8912 - loglik: -2.9954e+02 - logprior: -7.1822e+00
Epoch 2/2
27/27 - 7s - loss: 286.9396 - loglik: -2.8428e+02 - logprior: -2.1072e+00
Fitted a model with MAP estimate = -284.2421
expansions: [(43, 1)]
discards: [  0  12  30  48  49  50  67  89 111 139]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 297.7740 - loglik: -2.9074e+02 - logprior: -6.8922e+00
Epoch 2/2
27/27 - 6s - loss: 289.6282 - loglik: -2.8731e+02 - logprior: -1.8943e+00
Fitted a model with MAP estimate = -286.7500
expansions: [(86, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 293.4222 - loglik: -2.8860e+02 - logprior: -4.6911e+00
Epoch 2/10
27/27 - 6s - loss: 288.0887 - loglik: -2.8633e+02 - logprior: -1.3682e+00
Epoch 3/10
27/27 - 6s - loss: 286.0572 - loglik: -2.8440e+02 - logprior: -1.1388e+00
Epoch 4/10
27/27 - 6s - loss: 285.1084 - loglik: -2.8357e+02 - logprior: -1.0445e+00
Epoch 5/10
27/27 - 6s - loss: 284.6881 - loglik: -2.8330e+02 - logprior: -9.2313e-01
Epoch 6/10
27/27 - 6s - loss: 284.6613 - loglik: -2.8340e+02 - logprior: -8.2574e-01
Epoch 7/10
27/27 - 6s - loss: 284.1147 - loglik: -2.8297e+02 - logprior: -7.2212e-01
Epoch 8/10
27/27 - 6s - loss: 283.3269 - loglik: -2.8227e+02 - logprior: -6.3431e-01
Epoch 9/10
27/27 - 6s - loss: 284.0030 - loglik: -2.8307e+02 - logprior: -5.1322e-01
Fitted a model with MAP estimate = -282.9420
Time for alignment: 173.7284
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.1238 - loglik: -4.1198e+02 - logprior: -5.0903e+00
Epoch 2/10
27/27 - 5s - loss: 329.0838 - loglik: -3.2637e+02 - logprior: -2.5401e+00
Epoch 3/10
27/27 - 6s - loss: 316.0270 - loglik: -3.1292e+02 - logprior: -2.7190e+00
Epoch 4/10
27/27 - 5s - loss: 313.4255 - loglik: -3.1039e+02 - logprior: -2.6145e+00
Epoch 5/10
27/27 - 6s - loss: 312.9725 - loglik: -3.1000e+02 - logprior: -2.5456e+00
Epoch 6/10
27/27 - 5s - loss: 312.2307 - loglik: -3.0923e+02 - logprior: -2.5620e+00
Epoch 7/10
27/27 - 6s - loss: 311.8852 - loglik: -3.0889e+02 - logprior: -2.5628e+00
Epoch 8/10
27/27 - 5s - loss: 311.6834 - loglik: -3.0869e+02 - logprior: -2.5712e+00
Epoch 9/10
27/27 - 5s - loss: 311.5205 - loglik: -3.0855e+02 - logprior: -2.5528e+00
Epoch 10/10
27/27 - 5s - loss: 311.4543 - loglik: -3.0849e+02 - logprior: -2.5552e+00
Fitted a model with MAP estimate = -310.8186
expansions: [(0, 2), (10, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (82, 2), (101, 1), (103, 1), (104, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 310.4492 - loglik: -3.0321e+02 - logprior: -7.0684e+00
Epoch 2/2
27/27 - 7s - loss: 289.9596 - loglik: -2.8738e+02 - logprior: -2.0168e+00
Fitted a model with MAP estimate = -286.7942
expansions: [(131, 1)]
discards: [  0  29  47  48  66 110 137]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 299.1113 - loglik: -2.9214e+02 - logprior: -6.8223e+00
Epoch 2/2
27/27 - 6s - loss: 289.9348 - loglik: -2.8770e+02 - logprior: -1.7867e+00
Fitted a model with MAP estimate = -287.1914
expansions: []
discards: [45]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 295.0538 - loglik: -2.9033e+02 - logprior: -4.5893e+00
Epoch 2/10
27/27 - 6s - loss: 289.0986 - loglik: -2.8740e+02 - logprior: -1.2649e+00
Epoch 3/10
27/27 - 6s - loss: 286.8775 - loglik: -2.8527e+02 - logprior: -1.0735e+00
Epoch 4/10
27/27 - 6s - loss: 286.1335 - loglik: -2.8469e+02 - logprior: -9.5497e-01
Epoch 5/10
27/27 - 6s - loss: 286.1402 - loglik: -2.8483e+02 - logprior: -8.4743e-01
Fitted a model with MAP estimate = -284.9182
Time for alignment: 150.2924
Computed alignments with likelihoods: ['-286.1641', '-282.9420', '-284.9182']
Best model has likelihood: -282.9420
time for generating output: 0.3424
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5611295681063123
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac97b05e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abe88e730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6abe88e1c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac950eca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6af4dacee0>, <__main__.SimpleDirichletPrior object at 0x7f6ab62a6af0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 768.3362 - loglik: -7.6544e+02 - logprior: -2.6269e+00
Epoch 2/10
34/34 - 19s - loss: 637.5803 - loglik: -6.3413e+02 - logprior: -2.4525e+00
Epoch 3/10
34/34 - 19s - loss: 617.6044 - loglik: -6.1392e+02 - logprior: -2.6760e+00
Epoch 4/10
34/34 - 19s - loss: 614.0022 - loglik: -6.1060e+02 - logprior: -2.6551e+00
Epoch 5/10
34/34 - 18s - loss: 611.3636 - loglik: -6.0806e+02 - logprior: -2.6470e+00
Epoch 6/10
34/34 - 19s - loss: 612.0672 - loglik: -6.0873e+02 - logprior: -2.6642e+00
Fitted a model with MAP estimate = -608.8043
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 3), (49, 1), (52, 2), (55, 1), (56, 3), (57, 1), (61, 1), (65, 1), (68, 1), (93, 1), (94, 1), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (111, 1), (135, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (172, 2), (173, 2), (175, 1), (178, 1), (185, 3), (186, 3), (189, 1), (191, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [  0 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 29s - loss: 600.1019 - loglik: -5.9568e+02 - logprior: -4.1396e+00
Epoch 2/2
34/34 - 25s - loss: 570.6949 - loglik: -5.6734e+02 - logprior: -2.3885e+00
Fitted a model with MAP estimate = -563.8710
expansions: [(74, 1), (188, 1), (264, 2)]
discards: [ 42  67  75  76 123 125]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 29s - loss: 575.7302 - loglik: -5.7268e+02 - logprior: -2.8200e+00
Epoch 2/2
34/34 - 25s - loss: 566.3167 - loglik: -5.6433e+02 - logprior: -1.2877e+00
Fitted a model with MAP estimate = -562.6490
expansions: [(262, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 29s - loss: 572.6090 - loglik: -5.6982e+02 - logprior: -2.5719e+00
Epoch 2/10
34/34 - 25s - loss: 565.1307 - loglik: -5.6345e+02 - logprior: -9.0810e-01
Epoch 3/10
34/34 - 25s - loss: 562.2340 - loglik: -5.6047e+02 - logprior: -8.2493e-01
Epoch 4/10
34/34 - 26s - loss: 560.5772 - loglik: -5.5904e+02 - logprior: -6.6990e-01
Epoch 5/10
34/34 - 25s - loss: 559.2915 - loglik: -5.5797e+02 - logprior: -5.2251e-01
Epoch 6/10
34/34 - 25s - loss: 559.6442 - loglik: -5.5848e+02 - logprior: -3.9267e-01
Fitted a model with MAP estimate = -558.1091
Time for alignment: 486.3905
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 767.1995 - loglik: -7.6432e+02 - logprior: -2.6181e+00
Epoch 2/10
34/34 - 19s - loss: 640.0955 - loglik: -6.3677e+02 - logprior: -2.3742e+00
Epoch 3/10
34/34 - 19s - loss: 622.4821 - loglik: -6.1873e+02 - logprior: -2.5179e+00
Epoch 4/10
34/34 - 19s - loss: 618.9590 - loglik: -6.1551e+02 - logprior: -2.4656e+00
Epoch 5/10
34/34 - 19s - loss: 616.1160 - loglik: -6.1279e+02 - logprior: -2.5160e+00
Epoch 6/10
34/34 - 19s - loss: 613.4574 - loglik: -6.1016e+02 - logprior: -2.5352e+00
Epoch 7/10
34/34 - 19s - loss: 614.8833 - loglik: -6.1162e+02 - logprior: -2.5508e+00
Fitted a model with MAP estimate = -612.5700
expansions: [(13, 2), (14, 1), (15, 2), (16, 1), (17, 5), (22, 1), (29, 1), (30, 2), (51, 1), (52, 1), (53, 1), (55, 5), (56, 2), (65, 1), (66, 2), (70, 1), (87, 2), (93, 1), (94, 1), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (130, 1), (136, 1), (138, 1), (141, 1), (142, 3), (143, 1), (164, 1), (171, 2), (173, 1), (175, 1), (178, 1), (188, 1), (190, 1), (192, 1), (202, 2), (203, 2), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 29s - loss: 604.6898 - loglik: -6.0032e+02 - logprior: -4.1340e+00
Epoch 2/2
34/34 - 25s - loss: 576.9153 - loglik: -5.7388e+02 - logprior: -2.4472e+00
Fitted a model with MAP estimate = -569.9867
expansions: [(73, 1), (191, 1), (263, 1)]
discards: [ 22  41  74  75  76  77  92 115 116]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 28s - loss: 585.7844 - loglik: -5.8271e+02 - logprior: -2.8570e+00
Epoch 2/2
34/34 - 25s - loss: 576.2675 - loglik: -5.7426e+02 - logprior: -1.2956e+00
Fitted a model with MAP estimate = -572.8620
expansions: [(25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 28s - loss: 583.8130 - loglik: -5.8097e+02 - logprior: -2.6259e+00
Epoch 2/10
34/34 - 25s - loss: 575.6602 - loglik: -5.7385e+02 - logprior: -9.9081e-01
Epoch 3/10
34/34 - 24s - loss: 572.1688 - loglik: -5.7031e+02 - logprior: -8.9120e-01
Epoch 4/10
34/34 - 25s - loss: 570.7060 - loglik: -5.6905e+02 - logprior: -7.6668e-01
Epoch 5/10
34/34 - 25s - loss: 569.9941 - loglik: -5.6855e+02 - logprior: -6.2669e-01
Epoch 6/10
34/34 - 25s - loss: 570.4409 - loglik: -5.6922e+02 - logprior: -4.4676e-01
Fitted a model with MAP estimate = -568.2856
Time for alignment: 502.5013
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 767.4937 - loglik: -7.6463e+02 - logprior: -2.6146e+00
Epoch 2/10
34/34 - 19s - loss: 637.7949 - loglik: -6.3488e+02 - logprior: -2.4912e+00
Epoch 3/10
34/34 - 19s - loss: 619.7299 - loglik: -6.1614e+02 - logprior: -2.7995e+00
Epoch 4/10
34/34 - 19s - loss: 615.5753 - loglik: -6.1212e+02 - logprior: -2.7242e+00
Epoch 5/10
34/34 - 18s - loss: 614.6505 - loglik: -6.1127e+02 - logprior: -2.7134e+00
Epoch 6/10
34/34 - 19s - loss: 613.9365 - loglik: -6.1050e+02 - logprior: -2.7795e+00
Epoch 7/10
34/34 - 19s - loss: 610.8231 - loglik: -6.0738e+02 - logprior: -2.8153e+00
Epoch 8/10
34/34 - 19s - loss: 611.7837 - loglik: -6.0833e+02 - logprior: -2.8294e+00
Fitted a model with MAP estimate = -610.1646
expansions: [(9, 1), (12, 1), (14, 3), (16, 1), (17, 5), (22, 1), (28, 1), (29, 2), (34, 1), (54, 1), (56, 1), (64, 1), (66, 1), (71, 1), (91, 1), (93, 2), (94, 2), (95, 2), (98, 1), (99, 1), (101, 1), (102, 1), (103, 1), (109, 1), (110, 1), (111, 1), (136, 1), (138, 1), (141, 1), (143, 2), (144, 1), (147, 1), (164, 4), (167, 1), (171, 1), (173, 1), (175, 1), (178, 1), (188, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 29s - loss: 605.8551 - loglik: -6.0138e+02 - logprior: -4.2073e+00
Epoch 2/2
34/34 - 24s - loss: 582.2481 - loglik: -5.7884e+02 - logprior: -2.4844e+00
Fitted a model with MAP estimate = -575.6547
expansions: [(15, 1), (17, 1), (66, 1), (183, 1), (208, 1)]
discards: [ 23  24  41 115 117 120 184]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 588.3354 - loglik: -5.8538e+02 - logprior: -2.7207e+00
Epoch 2/2
34/34 - 24s - loss: 577.6011 - loglik: -5.7572e+02 - logprior: -1.1734e+00
Fitted a model with MAP estimate = -574.5797
expansions: []
discards: [203 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 28s - loss: 587.0142 - loglik: -5.8432e+02 - logprior: -2.4612e+00
Epoch 2/10
34/34 - 24s - loss: 578.7829 - loglik: -5.7712e+02 - logprior: -8.2140e-01
Epoch 3/10
34/34 - 24s - loss: 577.5021 - loglik: -5.7577e+02 - logprior: -7.6728e-01
Epoch 4/10
34/34 - 24s - loss: 574.4896 - loglik: -5.7300e+02 - logprior: -6.2298e-01
Epoch 5/10
34/34 - 24s - loss: 574.6559 - loglik: -5.7340e+02 - logprior: -4.4334e-01
Fitted a model with MAP estimate = -572.9846
Time for alignment: 487.1505
Computed alignments with likelihoods: ['-558.1091', '-568.2856', '-572.9846']
Best model has likelihood: -558.1091
time for generating output: 0.4453
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.7609876543209877
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6adb3ac6a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abe81c3a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad261730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6adb3104f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16a027c0>, <__main__.SimpleDirichletPrior object at 0x7f6aca16fb80>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 272.8590 - loglik: -1.8302e+02 - logprior: -8.9816e+01
Epoch 2/10
10/10 - 1s - loss: 185.3848 - loglik: -1.6007e+02 - logprior: -2.5298e+01
Epoch 3/10
10/10 - 1s - loss: 154.6868 - loglik: -1.4189e+02 - logprior: -1.2781e+01
Epoch 4/10
10/10 - 1s - loss: 140.7070 - loglik: -1.3251e+02 - logprior: -8.1757e+00
Epoch 5/10
10/10 - 1s - loss: 133.2141 - loglik: -1.2734e+02 - logprior: -5.8576e+00
Epoch 6/10
10/10 - 1s - loss: 129.2989 - loglik: -1.2451e+02 - logprior: -4.7000e+00
Epoch 7/10
10/10 - 1s - loss: 127.5421 - loglik: -1.2341e+02 - logprior: -3.8412e+00
Epoch 8/10
10/10 - 1s - loss: 126.6552 - loglik: -1.2294e+02 - logprior: -3.3310e+00
Epoch 9/10
10/10 - 1s - loss: 126.0908 - loglik: -1.2270e+02 - logprior: -3.0121e+00
Epoch 10/10
10/10 - 1s - loss: 125.6748 - loglik: -1.2252e+02 - logprior: -2.8040e+00
Fitted a model with MAP estimate = -125.1542
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.7899 - loglik: -1.2288e+02 - logprior: -1.0088e+02
Epoch 2/2
10/10 - 1s - loss: 158.6658 - loglik: -1.1613e+02 - logprior: -4.2410e+01
Fitted a model with MAP estimate = -146.8902
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.4891 - loglik: -1.1265e+02 - logprior: -8.0825e+01
Epoch 2/2
10/10 - 1s - loss: 132.7707 - loglik: -1.0991e+02 - logprior: -2.2780e+01
Fitted a model with MAP estimate = -123.4119
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 188.8825 - loglik: -1.0955e+02 - logprior: -7.9320e+01
Epoch 2/10
10/10 - 1s - loss: 130.7141 - loglik: -1.0860e+02 - logprior: -2.2054e+01
Epoch 3/10
10/10 - 1s - loss: 118.2477 - loglik: -1.0768e+02 - logprior: -1.0374e+01
Epoch 4/10
10/10 - 1s - loss: 113.3236 - loglik: -1.0757e+02 - logprior: -5.4823e+00
Epoch 5/10
10/10 - 1s - loss: 110.7765 - loglik: -1.0773e+02 - logprior: -2.7284e+00
Epoch 6/10
10/10 - 1s - loss: 109.4091 - loglik: -1.0797e+02 - logprior: -1.1099e+00
Epoch 7/10
10/10 - 1s - loss: 108.6107 - loglik: -1.0814e+02 - logprior: -1.4600e-01
Epoch 8/10
10/10 - 1s - loss: 108.0870 - loglik: -1.0824e+02 - logprior: 0.4846
Epoch 9/10
10/10 - 1s - loss: 107.6942 - loglik: -1.0831e+02 - logprior: 0.9653
Epoch 10/10
10/10 - 1s - loss: 107.3743 - loglik: -1.0835e+02 - logprior: 1.3347
Fitted a model with MAP estimate = -106.8543
Time for alignment: 35.3128
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 272.8591 - loglik: -1.8302e+02 - logprior: -8.9816e+01
Epoch 2/10
10/10 - 1s - loss: 185.3848 - loglik: -1.6007e+02 - logprior: -2.5298e+01
Epoch 3/10
10/10 - 1s - loss: 154.6868 - loglik: -1.4189e+02 - logprior: -1.2781e+01
Epoch 4/10
10/10 - 1s - loss: 140.7062 - loglik: -1.3251e+02 - logprior: -8.1763e+00
Epoch 5/10
10/10 - 1s - loss: 133.0639 - loglik: -1.2715e+02 - logprior: -5.8949e+00
Epoch 6/10
10/10 - 1s - loss: 128.9529 - loglik: -1.2405e+02 - logprior: -4.7776e+00
Epoch 7/10
10/10 - 1s - loss: 127.3001 - loglik: -1.2310e+02 - logprior: -3.9178e+00
Epoch 8/10
10/10 - 1s - loss: 126.4811 - loglik: -1.2281e+02 - logprior: -3.3650e+00
Epoch 9/10
10/10 - 1s - loss: 125.9707 - loglik: -1.2266e+02 - logprior: -3.0385e+00
Epoch 10/10
10/10 - 1s - loss: 125.6207 - loglik: -1.2258e+02 - logprior: -2.8017e+00
Fitted a model with MAP estimate = -125.2421
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.8143 - loglik: -1.2291e+02 - logprior: -1.0088e+02
Epoch 2/2
10/10 - 1s - loss: 158.6299 - loglik: -1.1609e+02 - logprior: -4.2407e+01
Fitted a model with MAP estimate = -146.8183
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.4686 - loglik: -1.1264e+02 - logprior: -8.0813e+01
Epoch 2/2
10/10 - 1s - loss: 132.7559 - loglik: -1.0990e+02 - logprior: -2.2782e+01
Fitted a model with MAP estimate = -123.3986
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 188.8756 - loglik: -1.0954e+02 - logprior: -7.9323e+01
Epoch 2/10
10/10 - 1s - loss: 130.6525 - loglik: -1.0852e+02 - logprior: -2.2067e+01
Epoch 3/10
10/10 - 1s - loss: 118.2472 - loglik: -1.0770e+02 - logprior: -1.0364e+01
Epoch 4/10
10/10 - 1s - loss: 113.3406 - loglik: -1.0762e+02 - logprior: -5.4717e+00
Epoch 5/10
10/10 - 1s - loss: 110.7880 - loglik: -1.0775e+02 - logprior: -2.7293e+00
Epoch 6/10
10/10 - 1s - loss: 109.4206 - loglik: -1.0798e+02 - logprior: -1.1087e+00
Epoch 7/10
10/10 - 1s - loss: 108.6183 - loglik: -1.0814e+02 - logprior: -1.4566e-01
Epoch 8/10
10/10 - 1s - loss: 108.0932 - loglik: -1.0825e+02 - logprior: 0.4843
Epoch 9/10
10/10 - 1s - loss: 107.7008 - loglik: -1.0833e+02 - logprior: 0.9662
Epoch 10/10
10/10 - 1s - loss: 107.3809 - loglik: -1.0837e+02 - logprior: 1.3384
Fitted a model with MAP estimate = -106.8681
Time for alignment: 35.3164
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 272.8592 - loglik: -1.8302e+02 - logprior: -8.9816e+01
Epoch 2/10
10/10 - 1s - loss: 185.3849 - loglik: -1.6007e+02 - logprior: -2.5298e+01
Epoch 3/10
10/10 - 1s - loss: 154.6868 - loglik: -1.4189e+02 - logprior: -1.2781e+01
Epoch 4/10
10/10 - 1s - loss: 140.7052 - loglik: -1.3251e+02 - logprior: -8.1770e+00
Epoch 5/10
10/10 - 1s - loss: 133.0301 - loglik: -1.2708e+02 - logprior: -5.9200e+00
Epoch 6/10
10/10 - 1s - loss: 129.0124 - loglik: -1.2407e+02 - logprior: -4.7475e+00
Epoch 7/10
10/10 - 1s - loss: 127.4245 - loglik: -1.2319e+02 - logprior: -3.8459e+00
Epoch 8/10
10/10 - 1s - loss: 126.6149 - loglik: -1.2290e+02 - logprior: -3.3247e+00
Epoch 9/10
10/10 - 1s - loss: 126.1120 - loglik: -1.2280e+02 - logprior: -2.9801e+00
Epoch 10/10
10/10 - 1s - loss: 125.7668 - loglik: -1.2271e+02 - logprior: -2.7453e+00
Fitted a model with MAP estimate = -125.3053
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.7777 - loglik: -1.2291e+02 - logprior: -1.0084e+02
Epoch 2/2
10/10 - 1s - loss: 158.6333 - loglik: -1.1608e+02 - logprior: -4.2407e+01
Fitted a model with MAP estimate = -146.7871
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.4505 - loglik: -1.1262e+02 - logprior: -8.0813e+01
Epoch 2/2
10/10 - 1s - loss: 132.7660 - loglik: -1.0986e+02 - logprior: -2.2794e+01
Fitted a model with MAP estimate = -123.3648
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 188.8803 - loglik: -1.0953e+02 - logprior: -7.9327e+01
Epoch 2/10
10/10 - 1s - loss: 130.6534 - loglik: -1.0848e+02 - logprior: -2.2075e+01
Epoch 3/10
10/10 - 1s - loss: 118.2477 - loglik: -1.0761e+02 - logprior: -1.0395e+01
Epoch 4/10
10/10 - 1s - loss: 113.3968 - loglik: -1.0766e+02 - logprior: -5.4109e+00
Epoch 5/10
10/10 - 1s - loss: 110.9201 - loglik: -1.0795e+02 - logprior: -2.6040e+00
Epoch 6/10
10/10 - 1s - loss: 109.5751 - loglik: -1.0820e+02 - logprior: -1.0094e+00
Epoch 7/10
10/10 - 1s - loss: 108.7812 - loglik: -1.0835e+02 - logprior: -5.0468e-02
Epoch 8/10
10/10 - 1s - loss: 108.2024 - loglik: -1.0836e+02 - logprior: 0.5458
Epoch 9/10
10/10 - 1s - loss: 107.7229 - loglik: -1.0830e+02 - logprior: 0.9547
Epoch 10/10
10/10 - 1s - loss: 107.3870 - loglik: -1.0837e+02 - logprior: 1.3298
Fitted a model with MAP estimate = -106.8766
Time for alignment: 34.9423
Computed alignments with likelihoods: ['-106.8543', '-106.8681', '-106.8766']
Best model has likelihood: -106.8543
time for generating output: 0.1432
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.9188648353647444
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac8d54760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b161b7cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b161b7610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ae3bdc9d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16ea5d00>, <__main__.SimpleDirichletPrior object at 0x7f6adb795e50>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.8831 - loglik: -3.0886e+02 - logprior: -7.9798e+00
Epoch 2/10
13/13 - 2s - loss: 284.2832 - loglik: -2.8199e+02 - logprior: -2.1493e+00
Epoch 3/10
13/13 - 2s - loss: 259.4258 - loglik: -2.5730e+02 - logprior: -1.9403e+00
Epoch 4/10
13/13 - 2s - loss: 250.6929 - loglik: -2.4817e+02 - logprior: -2.2737e+00
Epoch 5/10
13/13 - 2s - loss: 248.2578 - loglik: -2.4556e+02 - logprior: -2.2348e+00
Epoch 6/10
13/13 - 2s - loss: 246.6798 - loglik: -2.4398e+02 - logprior: -2.1479e+00
Epoch 7/10
13/13 - 2s - loss: 246.2062 - loglik: -2.4345e+02 - logprior: -2.1843e+00
Epoch 8/10
13/13 - 2s - loss: 245.9459 - loglik: -2.4320e+02 - logprior: -2.2014e+00
Epoch 9/10
13/13 - 2s - loss: 245.0683 - loglik: -2.4239e+02 - logprior: -2.1818e+00
Epoch 10/10
13/13 - 2s - loss: 244.8511 - loglik: -2.4219e+02 - logprior: -2.1824e+00
Fitted a model with MAP estimate = -244.2367
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 3), (31, 2), (32, 2), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 265.5902 - loglik: -2.5599e+02 - logprior: -9.5589e+00
Epoch 2/2
13/13 - 2s - loss: 249.4565 - loglik: -2.4473e+02 - logprior: -4.5039e+00
Fitted a model with MAP estimate = -246.4662
expansions: [(0, 2)]
discards: [ 0 14 90 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.6245 - loglik: -2.4330e+02 - logprior: -7.2807e+00
Epoch 2/2
13/13 - 2s - loss: 243.7754 - loglik: -2.4126e+02 - logprior: -2.2779e+00
Fitted a model with MAP estimate = -242.2300
expansions: []
discards: [ 0 67]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 253.2433 - loglik: -2.4424e+02 - logprior: -8.9592e+00
Epoch 2/10
13/13 - 2s - loss: 244.9396 - loglik: -2.4166e+02 - logprior: -3.0492e+00
Epoch 3/10
13/13 - 2s - loss: 243.0530 - loglik: -2.4086e+02 - logprior: -1.7523e+00
Epoch 4/10
13/13 - 2s - loss: 241.6328 - loglik: -2.3954e+02 - logprior: -1.5353e+00
Epoch 5/10
13/13 - 2s - loss: 240.8061 - loglik: -2.3877e+02 - logprior: -1.4431e+00
Epoch 6/10
13/13 - 2s - loss: 239.9194 - loglik: -2.3787e+02 - logprior: -1.4383e+00
Epoch 7/10
13/13 - 2s - loss: 239.1785 - loglik: -2.3713e+02 - logprior: -1.4296e+00
Epoch 8/10
13/13 - 2s - loss: 238.4650 - loglik: -2.3640e+02 - logprior: -1.4306e+00
Epoch 9/10
13/13 - 2s - loss: 237.7836 - loglik: -2.3570e+02 - logprior: -1.4295e+00
Epoch 10/10
13/13 - 2s - loss: 237.0862 - loglik: -2.3500e+02 - logprior: -1.4277e+00
Fitted a model with MAP estimate = -236.1321
Time for alignment: 73.3145
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.6433 - loglik: -3.0862e+02 - logprior: -7.9821e+00
Epoch 2/10
13/13 - 2s - loss: 284.8860 - loglik: -2.8259e+02 - logprior: -2.1469e+00
Epoch 3/10
13/13 - 2s - loss: 261.8797 - loglik: -2.5971e+02 - logprior: -1.9297e+00
Epoch 4/10
13/13 - 2s - loss: 252.2869 - loglik: -2.4956e+02 - logprior: -2.1899e+00
Epoch 5/10
13/13 - 2s - loss: 248.6189 - loglik: -2.4579e+02 - logprior: -2.2303e+00
Epoch 6/10
13/13 - 2s - loss: 246.5598 - loglik: -2.4379e+02 - logprior: -2.1798e+00
Epoch 7/10
13/13 - 2s - loss: 246.4779 - loglik: -2.4373e+02 - logprior: -2.1971e+00
Epoch 8/10
13/13 - 2s - loss: 245.5933 - loglik: -2.4287e+02 - logprior: -2.2044e+00
Epoch 9/10
13/13 - 2s - loss: 245.5077 - loglik: -2.4283e+02 - logprior: -2.1986e+00
Epoch 10/10
13/13 - 2s - loss: 244.6587 - loglik: -2.4200e+02 - logprior: -2.1958e+00
Fitted a model with MAP estimate = -244.2411
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 2), (30, 2), (31, 3), (32, 3), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 266.6455 - loglik: -2.5703e+02 - logprior: -9.5757e+00
Epoch 2/2
13/13 - 2s - loss: 250.0852 - loglik: -2.4536e+02 - logprior: -4.4896e+00
Fitted a model with MAP estimate = -246.6853
expansions: [(0, 2)]
discards: [ 0 14 34 91 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 251.0406 - loglik: -2.4370e+02 - logprior: -7.2941e+00
Epoch 2/2
13/13 - 2s - loss: 243.9912 - loglik: -2.4145e+02 - logprior: -2.2969e+00
Fitted a model with MAP estimate = -242.2666
expansions: []
discards: [ 0 34 67]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 253.2636 - loglik: -2.4424e+02 - logprior: -8.9770e+00
Epoch 2/10
13/13 - 2s - loss: 245.3163 - loglik: -2.4200e+02 - logprior: -3.0718e+00
Epoch 3/10
13/13 - 2s - loss: 243.0179 - loglik: -2.4083e+02 - logprior: -1.7104e+00
Epoch 4/10
13/13 - 2s - loss: 241.7513 - loglik: -2.3968e+02 - logprior: -1.4970e+00
Epoch 5/10
13/13 - 2s - loss: 240.5551 - loglik: -2.3855e+02 - logprior: -1.4145e+00
Epoch 6/10
13/13 - 2s - loss: 239.9852 - loglik: -2.3799e+02 - logprior: -1.4012e+00
Epoch 7/10
13/13 - 2s - loss: 238.7619 - loglik: -2.3676e+02 - logprior: -1.4022e+00
Epoch 8/10
13/13 - 2s - loss: 238.6487 - loglik: -2.3663e+02 - logprior: -1.4021e+00
Epoch 9/10
13/13 - 2s - loss: 238.2404 - loglik: -2.3623e+02 - logprior: -1.3900e+00
Epoch 10/10
13/13 - 2s - loss: 236.9139 - loglik: -2.3488e+02 - logprior: -1.4035e+00
Fitted a model with MAP estimate = -236.1774
Time for alignment: 73.9979
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.8265 - loglik: -3.0880e+02 - logprior: -7.9847e+00
Epoch 2/10
13/13 - 2s - loss: 285.4532 - loglik: -2.8314e+02 - logprior: -2.1647e+00
Epoch 3/10
13/13 - 2s - loss: 261.9410 - loglik: -2.5966e+02 - logprior: -1.9951e+00
Epoch 4/10
13/13 - 2s - loss: 251.6077 - loglik: -2.4863e+02 - logprior: -2.3042e+00
Epoch 5/10
13/13 - 2s - loss: 248.7759 - loglik: -2.4582e+02 - logprior: -2.2244e+00
Epoch 6/10
13/13 - 2s - loss: 247.3355 - loglik: -2.4461e+02 - logprior: -2.1270e+00
Epoch 7/10
13/13 - 2s - loss: 246.2947 - loglik: -2.4354e+02 - logprior: -2.1676e+00
Epoch 8/10
13/13 - 2s - loss: 246.3688 - loglik: -2.4360e+02 - logprior: -2.2016e+00
Fitted a model with MAP estimate = -245.0161
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 4), (29, 2), (30, 2), (31, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 263.4927 - loglik: -2.5388e+02 - logprior: -9.5732e+00
Epoch 2/2
13/13 - 2s - loss: 248.9926 - loglik: -2.4427e+02 - logprior: -4.5176e+00
Fitted a model with MAP estimate = -246.1735
expansions: [(0, 2)]
discards: [ 0 14 34 35 68 91 97]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.9361 - loglik: -2.4364e+02 - logprior: -7.2483e+00
Epoch 2/2
13/13 - 2s - loss: 244.0941 - loglik: -2.4161e+02 - logprior: -2.2435e+00
Fitted a model with MAP estimate = -242.4596
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 252.9914 - loglik: -2.4400e+02 - logprior: -8.9531e+00
Epoch 2/10
13/13 - 2s - loss: 245.8277 - loglik: -2.4254e+02 - logprior: -3.0512e+00
Epoch 3/10
13/13 - 2s - loss: 242.4451 - loglik: -2.4025e+02 - logprior: -1.7373e+00
Epoch 4/10
13/13 - 2s - loss: 241.2793 - loglik: -2.3918e+02 - logprior: -1.5191e+00
Epoch 5/10
13/13 - 2s - loss: 240.9311 - loglik: -2.3889e+02 - logprior: -1.4124e+00
Epoch 6/10
13/13 - 2s - loss: 239.7892 - loglik: -2.3777e+02 - logprior: -1.3959e+00
Epoch 7/10
13/13 - 2s - loss: 238.7362 - loglik: -2.3671e+02 - logprior: -1.3982e+00
Epoch 8/10
13/13 - 2s - loss: 238.6404 - loglik: -2.3661e+02 - logprior: -1.3953e+00
Epoch 9/10
13/13 - 2s - loss: 237.3558 - loglik: -2.3533e+02 - logprior: -1.3878e+00
Epoch 10/10
13/13 - 2s - loss: 236.8705 - loglik: -2.3483e+02 - logprior: -1.3915e+00
Fitted a model with MAP estimate = -235.9756
Time for alignment: 69.1107
Computed alignments with likelihoods: ['-236.1321', '-236.1774', '-235.9756']
Best model has likelihood: -235.9756
time for generating output: 0.1943
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9036259541984732
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b16e242b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e882e0a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e882e610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6adb23a4f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb23a580>, <__main__.SimpleDirichletPrior object at 0x7f6b1f530a60>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 435.6982 - loglik: -3.4692e+02 - logprior: -8.8764e+01
Epoch 2/10
10/10 - 2s - loss: 331.2562 - loglik: -3.0824e+02 - logprior: -2.3012e+01
Epoch 3/10
10/10 - 1s - loss: 280.3232 - loglik: -2.6912e+02 - logprior: -1.1188e+01
Epoch 4/10
10/10 - 1s - loss: 249.0542 - loglik: -2.4139e+02 - logprior: -7.6648e+00
Epoch 5/10
10/10 - 1s - loss: 235.2962 - loglik: -2.2965e+02 - logprior: -5.6483e+00
Epoch 6/10
10/10 - 2s - loss: 228.9968 - loglik: -2.2437e+02 - logprior: -4.5746e+00
Epoch 7/10
10/10 - 2s - loss: 224.9755 - loglik: -2.2080e+02 - logprior: -3.8698e+00
Epoch 8/10
10/10 - 2s - loss: 223.3756 - loglik: -2.1956e+02 - logprior: -3.3292e+00
Epoch 9/10
10/10 - 2s - loss: 222.4104 - loglik: -2.1904e+02 - logprior: -2.9088e+00
Epoch 10/10
10/10 - 2s - loss: 221.7334 - loglik: -2.1880e+02 - logprior: -2.5495e+00
Fitted a model with MAP estimate = -221.0268
expansions: [(13, 3), (14, 1), (24, 1), (29, 2), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 318.3343 - loglik: -2.1830e+02 - logprior: -1.0001e+02
Epoch 2/2
10/10 - 2s - loss: 241.7319 - loglik: -2.0104e+02 - logprior: -4.0535e+01
Fitted a model with MAP estimate = -227.9423
expansions: [(0, 3)]
discards: [  0  34  47 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 277.8751 - loglik: -1.9877e+02 - logprior: -7.9097e+01
Epoch 2/2
10/10 - 2s - loss: 214.3767 - loglik: -1.9428e+02 - logprior: -2.0021e+01
Fitted a model with MAP estimate = -204.2268
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 290.9574 - loglik: -1.9727e+02 - logprior: -9.3669e+01
Epoch 2/10
10/10 - 2s - loss: 222.4057 - loglik: -1.9602e+02 - logprior: -2.6349e+01
Epoch 3/10
10/10 - 2s - loss: 202.8742 - loglik: -1.9442e+02 - logprior: -8.3934e+00
Epoch 4/10
10/10 - 2s - loss: 195.1893 - loglik: -1.9268e+02 - logprior: -2.2673e+00
Epoch 5/10
10/10 - 2s - loss: 191.4632 - loglik: -1.9180e+02 - logprior: 0.7785
Epoch 6/10
10/10 - 2s - loss: 189.3858 - loglik: -1.9155e+02 - logprior: 2.6277
Epoch 7/10
10/10 - 2s - loss: 188.0910 - loglik: -1.9147e+02 - logprior: 3.7804
Epoch 8/10
10/10 - 2s - loss: 187.2224 - loglik: -1.9150e+02 - logprior: 4.6283
Epoch 9/10
10/10 - 2s - loss: 186.5597 - loglik: -1.9156e+02 - logprior: 5.3387
Epoch 10/10
10/10 - 2s - loss: 185.9937 - loglik: -1.9163e+02 - logprior: 5.9769
Fitted a model with MAP estimate = -185.3593
Time for alignment: 56.5191
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 435.6985 - loglik: -3.4692e+02 - logprior: -8.8764e+01
Epoch 2/10
10/10 - 1s - loss: 331.2561 - loglik: -3.0824e+02 - logprior: -2.3012e+01
Epoch 3/10
10/10 - 2s - loss: 280.3228 - loglik: -2.6912e+02 - logprior: -1.1188e+01
Epoch 4/10
10/10 - 1s - loss: 249.0362 - loglik: -2.4136e+02 - logprior: -7.6724e+00
Epoch 5/10
10/10 - 1s - loss: 234.5765 - loglik: -2.2869e+02 - logprior: -5.8579e+00
Epoch 6/10
10/10 - 2s - loss: 228.0150 - loglik: -2.2292e+02 - logprior: -4.8936e+00
Epoch 7/10
10/10 - 2s - loss: 224.7479 - loglik: -2.2036e+02 - logprior: -4.0389e+00
Epoch 8/10
10/10 - 2s - loss: 223.1878 - loglik: -2.1948e+02 - logprior: -3.3847e+00
Epoch 9/10
10/10 - 1s - loss: 222.3247 - loglik: -2.1912e+02 - logprior: -2.9361e+00
Epoch 10/10
10/10 - 2s - loss: 221.7161 - loglik: -2.1888e+02 - logprior: -2.5814e+00
Fitted a model with MAP estimate = -221.1908
expansions: [(13, 3), (14, 1), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 317.6583 - loglik: -2.1768e+02 - logprior: -9.9955e+01
Epoch 2/2
10/10 - 2s - loss: 241.2846 - loglik: -2.0088e+02 - logprior: -4.0231e+01
Fitted a model with MAP estimate = -227.4408
expansions: [(0, 2)]
discards: [  0  34 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.7903 - loglik: -1.9793e+02 - logprior: -7.8847e+01
Epoch 2/2
10/10 - 2s - loss: 213.9348 - loglik: -1.9423e+02 - logprior: -1.9668e+01
Fitted a model with MAP estimate = -204.0255
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 290.5309 - loglik: -1.9709e+02 - logprior: -9.3421e+01
Epoch 2/10
10/10 - 2s - loss: 222.0792 - loglik: -1.9595e+02 - logprior: -2.6086e+01
Epoch 3/10
10/10 - 2s - loss: 203.1411 - loglik: -1.9503e+02 - logprior: -8.1020e+00
Epoch 4/10
10/10 - 2s - loss: 195.7756 - loglik: -1.9384e+02 - logprior: -1.8839e+00
Epoch 5/10
10/10 - 2s - loss: 191.5722 - loglik: -1.9225e+02 - logprior: 0.9022
Epoch 6/10
10/10 - 2s - loss: 189.3091 - loglik: -1.9162e+02 - logprior: 2.7025
Epoch 7/10
10/10 - 2s - loss: 187.9461 - loglik: -1.9139e+02 - logprior: 3.8380
Epoch 8/10
10/10 - 2s - loss: 187.0313 - loglik: -1.9131e+02 - logprior: 4.6444
Epoch 9/10
10/10 - 2s - loss: 186.3541 - loglik: -1.9134e+02 - logprior: 5.3445
Epoch 10/10
10/10 - 2s - loss: 185.7771 - loglik: -1.9142e+02 - logprior: 5.9933
Fitted a model with MAP estimate = -185.1347
Time for alignment: 57.7315
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 435.6982 - loglik: -3.4692e+02 - logprior: -8.8764e+01
Epoch 2/10
10/10 - 1s - loss: 331.2558 - loglik: -3.0824e+02 - logprior: -2.3012e+01
Epoch 3/10
10/10 - 1s - loss: 280.3233 - loglik: -2.6912e+02 - logprior: -1.1188e+01
Epoch 4/10
10/10 - 1s - loss: 249.0544 - loglik: -2.4139e+02 - logprior: -7.6652e+00
Epoch 5/10
10/10 - 2s - loss: 235.1406 - loglik: -2.2943e+02 - logprior: -5.7063e+00
Epoch 6/10
10/10 - 2s - loss: 228.4283 - loglik: -2.2355e+02 - logprior: -4.7316e+00
Epoch 7/10
10/10 - 2s - loss: 224.9515 - loglik: -2.2063e+02 - logprior: -3.9018e+00
Epoch 8/10
10/10 - 2s - loss: 223.4022 - loglik: -2.1958e+02 - logprior: -3.3452e+00
Epoch 9/10
10/10 - 1s - loss: 222.4467 - loglik: -2.1913e+02 - logprior: -2.9051e+00
Epoch 10/10
10/10 - 2s - loss: 221.8206 - loglik: -2.1891e+02 - logprior: -2.5295e+00
Fitted a model with MAP estimate = -221.1292
expansions: [(13, 3), (14, 1), (24, 1), (29, 2), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.3813 - loglik: -2.1836e+02 - logprior: -9.9992e+01
Epoch 2/2
10/10 - 2s - loss: 241.7447 - loglik: -2.0102e+02 - logprior: -4.0544e+01
Fitted a model with MAP estimate = -227.9058
expansions: [(0, 3)]
discards: [  0  34  47 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 277.8858 - loglik: -1.9875e+02 - logprior: -7.9123e+01
Epoch 2/2
10/10 - 2s - loss: 214.3244 - loglik: -1.9420e+02 - logprior: -2.0044e+01
Fitted a model with MAP estimate = -204.1860
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 290.9340 - loglik: -1.9723e+02 - logprior: -9.3685e+01
Epoch 2/10
10/10 - 2s - loss: 222.1306 - loglik: -1.9565e+02 - logprior: -2.6395e+01
Epoch 3/10
10/10 - 2s - loss: 202.3548 - loglik: -1.9365e+02 - logprior: -8.5491e+00
Epoch 4/10
10/10 - 2s - loss: 194.9119 - loglik: -1.9220e+02 - logprior: -2.4019e+00
Epoch 5/10
10/10 - 2s - loss: 191.3517 - loglik: -1.9166e+02 - logprior: 0.7400
Epoch 6/10
10/10 - 2s - loss: 189.3276 - loglik: -1.9151e+02 - logprior: 2.6158
Epoch 7/10
10/10 - 2s - loss: 188.0380 - loglik: -1.9144e+02 - logprior: 3.8044
Epoch 8/10
10/10 - 2s - loss: 187.1638 - loglik: -1.9141e+02 - logprior: 4.6493
Epoch 9/10
10/10 - 2s - loss: 186.4265 - loglik: -1.9132e+02 - logprior: 5.3033
Epoch 10/10
10/10 - 2s - loss: 185.8047 - loglik: -1.9136e+02 - logprior: 5.9411
Fitted a model with MAP estimate = -185.1368
Time for alignment: 56.6556
Computed alignments with likelihoods: ['-185.3593', '-185.1347', '-185.1368']
Best model has likelihood: -185.1347
time for generating output: 0.1793
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8626865671641791
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac75c1d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ab6770e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82702dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abed124f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ae3b5eeb0>, <__main__.SimpleDirichletPrior object at 0x7f6ac8928130>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.6342 - loglik: -1.3552e+02 - logprior: -3.2100e+01
Epoch 2/10
10/10 - 1s - loss: 124.6929 - loglik: -1.1531e+02 - logprior: -9.3801e+00
Epoch 3/10
10/10 - 1s - loss: 99.8804 - loglik: -9.4559e+01 - logprior: -5.2898e+00
Epoch 4/10
10/10 - 1s - loss: 84.9910 - loglik: -8.0802e+01 - logprior: -4.1719e+00
Epoch 5/10
10/10 - 1s - loss: 79.5851 - loglik: -7.5713e+01 - logprior: -3.8663e+00
Epoch 6/10
10/10 - 1s - loss: 77.5665 - loglik: -7.3846e+01 - logprior: -3.6670e+00
Epoch 7/10
10/10 - 1s - loss: 76.3045 - loglik: -7.2815e+01 - logprior: -3.2818e+00
Epoch 8/10
10/10 - 1s - loss: 75.8793 - loglik: -7.2610e+01 - logprior: -2.9966e+00
Epoch 9/10
10/10 - 1s - loss: 75.4963 - loglik: -7.2364e+01 - logprior: -2.8986e+00
Epoch 10/10
10/10 - 1s - loss: 75.4849 - loglik: -7.2408e+01 - logprior: -2.8618e+00
Fitted a model with MAP estimate = -75.0570
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 115.1415 - loglik: -7.2216e+01 - logprior: -4.2906e+01
Epoch 2/2
10/10 - 1s - loss: 78.7217 - loglik: -6.4262e+01 - logprior: -1.4374e+01
Fitted a model with MAP estimate = -71.3809
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 91.5638 - loglik: -6.1147e+01 - logprior: -3.0399e+01
Epoch 2/2
10/10 - 1s - loss: 69.7683 - loglik: -6.0710e+01 - logprior: -8.9900e+00
Fitted a model with MAP estimate = -66.5871
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 89.1755 - loglik: -6.0380e+01 - logprior: -2.8772e+01
Epoch 2/10
10/10 - 1s - loss: 69.1831 - loglik: -6.0552e+01 - logprior: -8.5447e+00
Epoch 3/10
10/10 - 1s - loss: 65.3827 - loglik: -6.0687e+01 - logprior: -4.5857e+00
Epoch 4/10
10/10 - 1s - loss: 63.9390 - loglik: -6.0690e+01 - logprior: -3.1152e+00
Epoch 5/10
10/10 - 1s - loss: 63.2406 - loglik: -6.0658e+01 - logprior: -2.3997e+00
Epoch 6/10
10/10 - 1s - loss: 62.7457 - loglik: -6.0474e+01 - logprior: -2.0626e+00
Epoch 7/10
10/10 - 1s - loss: 62.6001 - loglik: -6.0545e+01 - logprior: -1.8332e+00
Epoch 8/10
10/10 - 1s - loss: 62.3923 - loglik: -6.0508e+01 - logprior: -1.6493e+00
Epoch 9/10
10/10 - 1s - loss: 62.2574 - loglik: -6.0519e+01 - logprior: -1.5129e+00
Epoch 10/10
10/10 - 1s - loss: 62.2774 - loglik: -6.0641e+01 - logprior: -1.4265e+00
Fitted a model with MAP estimate = -61.9202
Time for alignment: 31.1132
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.6220 - loglik: -1.3551e+02 - logprior: -3.2099e+01
Epoch 2/10
10/10 - 1s - loss: 124.4003 - loglik: -1.1502e+02 - logprior: -9.3775e+00
Epoch 3/10
10/10 - 1s - loss: 98.6775 - loglik: -9.3337e+01 - logprior: -5.3071e+00
Epoch 4/10
10/10 - 1s - loss: 83.8577 - loglik: -7.9625e+01 - logprior: -4.2145e+00
Epoch 5/10
10/10 - 1s - loss: 78.8217 - loglik: -7.4942e+01 - logprior: -3.8757e+00
Epoch 6/10
10/10 - 1s - loss: 77.2126 - loglik: -7.3585e+01 - logprior: -3.6088e+00
Epoch 7/10
10/10 - 1s - loss: 76.2705 - loglik: -7.2895e+01 - logprior: -3.2391e+00
Epoch 8/10
10/10 - 1s - loss: 75.8473 - loglik: -7.2634e+01 - logprior: -2.9696e+00
Epoch 9/10
10/10 - 1s - loss: 75.4938 - loglik: -7.2372e+01 - logprior: -2.8837e+00
Epoch 10/10
10/10 - 1s - loss: 75.4428 - loglik: -7.2383e+01 - logprior: -2.8475e+00
Fitted a model with MAP estimate = -75.0289
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 115.1526 - loglik: -7.2220e+01 - logprior: -4.2917e+01
Epoch 2/2
10/10 - 1s - loss: 78.5850 - loglik: -6.4135e+01 - logprior: -1.4375e+01
Fitted a model with MAP estimate = -71.3726
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 91.5302 - loglik: -6.1111e+01 - logprior: -3.0400e+01
Epoch 2/2
10/10 - 1s - loss: 69.7563 - loglik: -6.0690e+01 - logprior: -8.9925e+00
Fitted a model with MAP estimate = -66.5859
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 89.1676 - loglik: -6.0367e+01 - logprior: -2.8777e+01
Epoch 2/10
10/10 - 1s - loss: 69.1714 - loglik: -6.0539e+01 - logprior: -8.5417e+00
Epoch 3/10
10/10 - 1s - loss: 65.4952 - loglik: -6.0793e+01 - logprior: -4.5874e+00
Epoch 4/10
10/10 - 1s - loss: 63.8141 - loglik: -6.0560e+01 - logprior: -3.1061e+00
Epoch 5/10
10/10 - 1s - loss: 63.2207 - loglik: -6.0619e+01 - logprior: -2.4051e+00
Epoch 6/10
10/10 - 1s - loss: 62.8603 - loglik: -6.0601e+01 - logprior: -2.0531e+00
Epoch 7/10
10/10 - 1s - loss: 62.5835 - loglik: -6.0525e+01 - logprior: -1.8345e+00
Epoch 8/10
10/10 - 1s - loss: 62.4153 - loglik: -6.0530e+01 - logprior: -1.6625e+00
Epoch 9/10
10/10 - 1s - loss: 62.2140 - loglik: -6.0497e+01 - logprior: -1.5108e+00
Epoch 10/10
10/10 - 1s - loss: 62.1569 - loglik: -6.0515e+01 - logprior: -1.4251e+00
Fitted a model with MAP estimate = -61.9154
Time for alignment: 31.4978
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.6396 - loglik: -1.3553e+02 - logprior: -3.2099e+01
Epoch 2/10
10/10 - 1s - loss: 124.4964 - loglik: -1.1512e+02 - logprior: -9.3746e+00
Epoch 3/10
10/10 - 1s - loss: 99.1194 - loglik: -9.3802e+01 - logprior: -5.2857e+00
Epoch 4/10
10/10 - 1s - loss: 84.1843 - loglik: -7.9982e+01 - logprior: -4.1852e+00
Epoch 5/10
10/10 - 1s - loss: 79.0698 - loglik: -7.5201e+01 - logprior: -3.8639e+00
Epoch 6/10
10/10 - 1s - loss: 77.3096 - loglik: -7.3688e+01 - logprior: -3.6071e+00
Epoch 7/10
10/10 - 1s - loss: 76.3109 - loglik: -7.2934e+01 - logprior: -3.2520e+00
Epoch 8/10
10/10 - 1s - loss: 75.8190 - loglik: -7.2608e+01 - logprior: -2.9685e+00
Epoch 9/10
10/10 - 1s - loss: 75.5788 - loglik: -7.2472e+01 - logprior: -2.8681e+00
Epoch 10/10
10/10 - 1s - loss: 75.4457 - loglik: -7.2386e+01 - logprior: -2.8496e+00
Fitted a model with MAP estimate = -75.0290
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 115.1233 - loglik: -7.2185e+01 - logprior: -4.2926e+01
Epoch 2/2
10/10 - 1s - loss: 78.6639 - loglik: -6.4223e+01 - logprior: -1.4375e+01
Fitted a model with MAP estimate = -71.3687
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 91.5422 - loglik: -6.1119e+01 - logprior: -3.0402e+01
Epoch 2/2
10/10 - 1s - loss: 69.7834 - loglik: -6.0725e+01 - logprior: -8.9872e+00
Fitted a model with MAP estimate = -66.5965
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 89.0506 - loglik: -6.0254e+01 - logprior: -2.8773e+01
Epoch 2/10
10/10 - 1s - loss: 69.2603 - loglik: -6.0624e+01 - logprior: -8.5463e+00
Epoch 3/10
10/10 - 1s - loss: 65.4294 - loglik: -6.0733e+01 - logprior: -4.5847e+00
Epoch 4/10
10/10 - 1s - loss: 63.9264 - loglik: -6.0664e+01 - logprior: -3.1181e+00
Epoch 5/10
10/10 - 1s - loss: 63.1518 - loglik: -6.0543e+01 - logprior: -2.4272e+00
Epoch 6/10
10/10 - 1s - loss: 62.7386 - loglik: -6.0487e+01 - logprior: -2.0750e+00
Epoch 7/10
10/10 - 1s - loss: 62.5155 - loglik: -6.0464e+01 - logprior: -1.8566e+00
Epoch 8/10
10/10 - 1s - loss: 62.4110 - loglik: -6.0545e+01 - logprior: -1.6620e+00
Epoch 9/10
10/10 - 1s - loss: 62.2789 - loglik: -6.0550e+01 - logprior: -1.5159e+00
Epoch 10/10
10/10 - 1s - loss: 62.1735 - loglik: -6.0531e+01 - logprior: -1.4262e+00
Fitted a model with MAP estimate = -61.9061
Time for alignment: 30.5407
Computed alignments with likelihoods: ['-61.9202', '-61.9154', '-61.9061']
Best model has likelihood: -61.9061
time for generating output: 0.1067
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ab66fc0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ab66fc7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab66fcd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a82260760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ab65fbf10>, <__main__.SimpleDirichletPrior object at 0x7f6b2fadaf40>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 445.3675 - loglik: -4.3970e+02 - logprior: -5.6578e+00
Epoch 2/10
15/15 - 5s - loss: 367.2763 - loglik: -3.6565e+02 - logprior: -1.6037e+00
Epoch 3/10
15/15 - 5s - loss: 319.0789 - loglik: -3.1717e+02 - logprior: -1.8880e+00
Epoch 4/10
15/15 - 5s - loss: 305.5382 - loglik: -3.0328e+02 - logprior: -2.1306e+00
Epoch 5/10
15/15 - 5s - loss: 302.1785 - loglik: -2.9989e+02 - logprior: -2.0419e+00
Epoch 6/10
15/15 - 5s - loss: 300.7080 - loglik: -2.9843e+02 - logprior: -2.0081e+00
Epoch 7/10
15/15 - 5s - loss: 299.3730 - loglik: -2.9717e+02 - logprior: -1.9700e+00
Epoch 8/10
15/15 - 5s - loss: 299.2290 - loglik: -2.9708e+02 - logprior: -1.9363e+00
Epoch 9/10
15/15 - 5s - loss: 298.1833 - loglik: -2.9602e+02 - logprior: -1.9439e+00
Epoch 10/10
15/15 - 5s - loss: 298.1650 - loglik: -2.9600e+02 - logprior: -1.9478e+00
Fitted a model with MAP estimate = -297.6992
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (55, 1), (60, 1), (66, 3), (69, 1), (91, 1), (92, 2), (105, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 304.0990 - loglik: -2.9712e+02 - logprior: -6.9481e+00
Epoch 2/2
15/15 - 6s - loss: 289.5775 - loglik: -2.8579e+02 - logprior: -3.6506e+00
Fitted a model with MAP estimate = -286.2890
expansions: [(0, 2)]
discards: [  0   7 107 134]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 288.5248 - loglik: -2.8314e+02 - logprior: -5.3553e+00
Epoch 2/2
15/15 - 6s - loss: 282.8380 - loglik: -2.8080e+02 - logprior: -1.8889e+00
Fitted a model with MAP estimate = -280.5149
expansions: []
discards: [ 0 82]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 290.2421 - loglik: -2.8342e+02 - logprior: -6.7812e+00
Epoch 2/10
15/15 - 6s - loss: 283.7089 - loglik: -2.8096e+02 - logprior: -2.5843e+00
Epoch 3/10
15/15 - 6s - loss: 281.9882 - loglik: -2.8019e+02 - logprior: -1.5350e+00
Epoch 4/10
15/15 - 6s - loss: 279.9900 - loglik: -2.7829e+02 - logprior: -1.3886e+00
Epoch 5/10
15/15 - 6s - loss: 277.9269 - loglik: -2.7625e+02 - logprior: -1.3530e+00
Epoch 6/10
15/15 - 6s - loss: 277.4126 - loglik: -2.7581e+02 - logprior: -1.3063e+00
Epoch 7/10
15/15 - 6s - loss: 277.8911 - loglik: -2.7636e+02 - logprior: -1.2562e+00
Fitted a model with MAP estimate = -277.0503
Time for alignment: 151.7715
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 446.0252 - loglik: -4.4036e+02 - logprior: -5.6563e+00
Epoch 2/10
15/15 - 5s - loss: 368.5762 - loglik: -3.6695e+02 - logprior: -1.6090e+00
Epoch 3/10
15/15 - 5s - loss: 320.8734 - loglik: -3.1899e+02 - logprior: -1.8573e+00
Epoch 4/10
15/15 - 5s - loss: 305.9431 - loglik: -3.0373e+02 - logprior: -2.0538e+00
Epoch 5/10
15/15 - 5s - loss: 301.6116 - loglik: -2.9938e+02 - logprior: -1.9709e+00
Epoch 6/10
15/15 - 5s - loss: 300.8431 - loglik: -2.9861e+02 - logprior: -1.9659e+00
Epoch 7/10
15/15 - 5s - loss: 298.8887 - loglik: -2.9671e+02 - logprior: -1.9300e+00
Epoch 8/10
15/15 - 5s - loss: 298.7606 - loglik: -2.9663e+02 - logprior: -1.8972e+00
Epoch 9/10
15/15 - 5s - loss: 299.1464 - loglik: -2.9701e+02 - logprior: -1.9025e+00
Fitted a model with MAP estimate = -298.1700
expansions: [(7, 3), (10, 1), (16, 1), (24, 3), (25, 1), (49, 2), (55, 1), (60, 1), (65, 2), (69, 1), (91, 1), (92, 2), (112, 1), (114, 4), (116, 3), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 303.7856 - loglik: -2.9677e+02 - logprior: -6.9866e+00
Epoch 2/2
15/15 - 6s - loss: 287.5276 - loglik: -2.8368e+02 - logprior: -3.7189e+00
Fitted a model with MAP estimate = -284.1480
expansions: [(0, 2)]
discards: [  0   7  29  58  83 108]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 289.1942 - loglik: -2.8376e+02 - logprior: -5.4097e+00
Epoch 2/2
15/15 - 6s - loss: 282.7514 - loglik: -2.8068e+02 - logprior: -1.9401e+00
Fitted a model with MAP estimate = -280.9115
expansions: [(81, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 290.0808 - loglik: -2.8323e+02 - logprior: -6.8197e+00
Epoch 2/10
15/15 - 6s - loss: 284.2241 - loglik: -2.8145e+02 - logprior: -2.6559e+00
Epoch 3/10
15/15 - 6s - loss: 280.4737 - loglik: -2.7867e+02 - logprior: -1.5735e+00
Epoch 4/10
15/15 - 6s - loss: 278.4893 - loglik: -2.7677e+02 - logprior: -1.4035e+00
Epoch 5/10
15/15 - 6s - loss: 279.1530 - loglik: -2.7745e+02 - logprior: -1.3706e+00
Fitted a model with MAP estimate = -277.5541
Time for alignment: 134.1667
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 445.2841 - loglik: -4.3962e+02 - logprior: -5.6552e+00
Epoch 2/10
15/15 - 5s - loss: 366.7451 - loglik: -3.6512e+02 - logprior: -1.5999e+00
Epoch 3/10
15/15 - 5s - loss: 319.7571 - loglik: -3.1786e+02 - logprior: -1.8746e+00
Epoch 4/10
15/15 - 5s - loss: 306.4913 - loglik: -3.0416e+02 - logprior: -2.0972e+00
Epoch 5/10
15/15 - 5s - loss: 302.7915 - loglik: -3.0046e+02 - logprior: -1.9808e+00
Epoch 6/10
15/15 - 5s - loss: 301.1215 - loglik: -2.9883e+02 - logprior: -1.9936e+00
Epoch 7/10
15/15 - 5s - loss: 298.3811 - loglik: -2.9615e+02 - logprior: -1.9865e+00
Epoch 8/10
15/15 - 5s - loss: 298.5854 - loglik: -2.9638e+02 - logprior: -1.9662e+00
Fitted a model with MAP estimate = -297.8769
expansions: [(7, 3), (10, 1), (14, 1), (24, 2), (25, 1), (26, 1), (55, 1), (59, 1), (65, 2), (69, 1), (91, 1), (92, 2), (103, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 304.9328 - loglik: -2.9792e+02 - logprior: -6.9673e+00
Epoch 2/2
15/15 - 6s - loss: 287.7670 - loglik: -2.8394e+02 - logprior: -3.6514e+00
Fitted a model with MAP estimate = -285.7110
expansions: [(0, 2)]
discards: [  0   7  81 106 133]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 289.7885 - loglik: -2.8438e+02 - logprior: -5.3642e+00
Epoch 2/2
15/15 - 6s - loss: 283.8195 - loglik: -2.8176e+02 - logprior: -1.8763e+00
Fitted a model with MAP estimate = -281.4349
expansions: [(77, 1)]
discards: [  0 135]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 290.9948 - loglik: -2.8416e+02 - logprior: -6.7876e+00
Epoch 2/10
15/15 - 6s - loss: 284.5947 - loglik: -2.8177e+02 - logprior: -2.6283e+00
Epoch 3/10
15/15 - 6s - loss: 281.6138 - loglik: -2.7981e+02 - logprior: -1.5381e+00
Epoch 4/10
15/15 - 6s - loss: 279.6776 - loglik: -2.7797e+02 - logprior: -1.3979e+00
Epoch 5/10
15/15 - 6s - loss: 279.1609 - loglik: -2.7748e+02 - logprior: -1.3460e+00
Epoch 6/10
15/15 - 6s - loss: 278.4714 - loglik: -2.7687e+02 - logprior: -1.2948e+00
Epoch 7/10
15/15 - 6s - loss: 277.8871 - loglik: -2.7635e+02 - logprior: -1.2530e+00
Epoch 8/10
15/15 - 6s - loss: 278.4873 - loglik: -2.7702e+02 - logprior: -1.2038e+00
Fitted a model with MAP estimate = -277.3893
Time for alignment: 147.0097
Computed alignments with likelihoods: ['-277.0503', '-277.5541', '-277.3893']
Best model has likelihood: -277.0503
time for generating output: 0.2211
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9214046822742475
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ad2997670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abedfe2e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aadc3fb20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac9d9ce20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac9d9c190>, <__main__.SimpleDirichletPrior object at 0x7f69e8de1cd0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 299.3660 - loglik: -2.9619e+02 - logprior: -3.0932e+00
Epoch 2/10
34/34 - 6s - loss: 208.2327 - loglik: -2.0603e+02 - logprior: -2.0993e+00
Epoch 3/10
34/34 - 7s - loss: 201.1198 - loglik: -1.9870e+02 - logprior: -2.1442e+00
Epoch 4/10
34/34 - 6s - loss: 198.2929 - loglik: -1.9591e+02 - logprior: -2.1480e+00
Epoch 5/10
34/34 - 6s - loss: 199.0084 - loglik: -1.9664e+02 - logprior: -2.1246e+00
Fitted a model with MAP estimate = -198.1183
expansions: [(0, 2), (15, 1), (16, 4), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 1), (56, 1), (60, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 12s - loss: 188.8749 - loglik: -1.8532e+02 - logprior: -3.4540e+00
Epoch 2/2
34/34 - 7s - loss: 177.3349 - loglik: -1.7542e+02 - logprior: -1.7055e+00
Fitted a model with MAP estimate = -176.1565
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 180.5907 - loglik: -1.7744e+02 - logprior: -3.0589e+00
Epoch 2/10
34/34 - 8s - loss: 177.0719 - loglik: -1.7533e+02 - logprior: -1.5256e+00
Epoch 3/10
34/34 - 7s - loss: 175.6446 - loglik: -1.7394e+02 - logprior: -1.4399e+00
Epoch 4/10
34/34 - 7s - loss: 174.7771 - loglik: -1.7314e+02 - logprior: -1.3656e+00
Epoch 5/10
34/34 - 7s - loss: 175.5450 - loglik: -1.7399e+02 - logprior: -1.2885e+00
Fitted a model with MAP estimate = -174.0601
Time for alignment: 118.3073
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 300.9037 - loglik: -2.9774e+02 - logprior: -3.0872e+00
Epoch 2/10
34/34 - 6s - loss: 208.8272 - loglik: -2.0666e+02 - logprior: -2.0813e+00
Epoch 3/10
34/34 - 7s - loss: 200.1517 - loglik: -1.9775e+02 - logprior: -2.1167e+00
Epoch 4/10
34/34 - 6s - loss: 200.3192 - loglik: -1.9793e+02 - logprior: -2.1271e+00
Fitted a model with MAP estimate = -198.5995
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (70, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 188.7173 - loglik: -1.8515e+02 - logprior: -3.4510e+00
Epoch 2/2
34/34 - 7s - loss: 177.4590 - loglik: -1.7549e+02 - logprior: -1.7453e+00
Fitted a model with MAP estimate = -176.1784
expansions: []
discards: [ 20 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 180.6223 - loglik: -1.7747e+02 - logprior: -3.0532e+00
Epoch 2/2
34/34 - 8s - loss: 177.4299 - loglik: -1.7569e+02 - logprior: -1.5218e+00
Fitted a model with MAP estimate = -175.8594
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 12s - loss: 180.0462 - loglik: -1.7702e+02 - logprior: -2.9321e+00
Epoch 2/10
34/34 - 7s - loss: 177.8076 - loglik: -1.7620e+02 - logprior: -1.3812e+00
Epoch 3/10
34/34 - 8s - loss: 175.5218 - loglik: -1.7396e+02 - logprior: -1.2918e+00
Epoch 4/10
34/34 - 7s - loss: 173.9093 - loglik: -1.7241e+02 - logprior: -1.2217e+00
Epoch 5/10
34/34 - 7s - loss: 175.1339 - loglik: -1.7374e+02 - logprior: -1.1266e+00
Fitted a model with MAP estimate = -173.9831
Time for alignment: 142.9411
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 301.2258 - loglik: -2.9808e+02 - logprior: -3.0627e+00
Epoch 2/10
34/34 - 7s - loss: 211.6292 - loglik: -2.0954e+02 - logprior: -2.0316e+00
Epoch 3/10
34/34 - 7s - loss: 202.4671 - loglik: -2.0011e+02 - logprior: -2.1274e+00
Epoch 4/10
34/34 - 7s - loss: 199.5733 - loglik: -1.9724e+02 - logprior: -2.1117e+00
Epoch 5/10
34/34 - 6s - loss: 199.8619 - loglik: -1.9753e+02 - logprior: -2.0903e+00
Fitted a model with MAP estimate = -199.1134
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 187.6480 - loglik: -1.8410e+02 - logprior: -3.4546e+00
Epoch 2/2
34/34 - 7s - loss: 176.2967 - loglik: -1.7437e+02 - logprior: -1.7110e+00
Fitted a model with MAP estimate = -175.2359
expansions: []
discards: [ 34 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 180.5494 - loglik: -1.7742e+02 - logprior: -3.0485e+00
Epoch 2/2
34/34 - 7s - loss: 176.4402 - loglik: -1.7470e+02 - logprior: -1.5161e+00
Fitted a model with MAP estimate = -175.4079
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 179.7308 - loglik: -1.7671e+02 - logprior: -2.9339e+00
Epoch 2/10
34/34 - 7s - loss: 177.0580 - loglik: -1.7546e+02 - logprior: -1.3789e+00
Epoch 3/10
34/34 - 7s - loss: 174.7485 - loglik: -1.7321e+02 - logprior: -1.2827e+00
Epoch 4/10
34/34 - 7s - loss: 174.8884 - loglik: -1.7341e+02 - logprior: -1.2091e+00
Fitted a model with MAP estimate = -173.8079
Time for alignment: 140.9215
Computed alignments with likelihoods: ['-174.0601', '-173.9831', '-173.8079']
Best model has likelihood: -173.8079
time for generating output: 0.3132
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6300615484288954
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6abe96e0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1677cee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b167590d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1686c9d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b1686c640>, <__main__.SimpleDirichletPrior object at 0x7f6a8279e490>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.5815 - loglik: -3.9894e+02 - logprior: -2.1627e+01
Epoch 2/10
10/10 - 2s - loss: 365.0446 - loglik: -3.5937e+02 - logprior: -5.6673e+00
Epoch 3/10
10/10 - 2s - loss: 323.1084 - loglik: -3.1961e+02 - logprior: -3.4976e+00
Epoch 4/10
10/10 - 2s - loss: 297.1470 - loglik: -2.9399e+02 - logprior: -3.0657e+00
Epoch 5/10
10/10 - 2s - loss: 285.7753 - loglik: -2.8246e+02 - logprior: -3.1396e+00
Epoch 6/10
10/10 - 2s - loss: 278.5123 - loglik: -2.7502e+02 - logprior: -3.2152e+00
Epoch 7/10
10/10 - 2s - loss: 274.7426 - loglik: -2.7112e+02 - logprior: -3.2076e+00
Epoch 8/10
10/10 - 2s - loss: 273.0838 - loglik: -2.6944e+02 - logprior: -3.1996e+00
Epoch 9/10
10/10 - 2s - loss: 271.6732 - loglik: -2.6812e+02 - logprior: -3.1561e+00
Epoch 10/10
10/10 - 2s - loss: 271.3723 - loglik: -2.6790e+02 - logprior: -3.1008e+00
Fitted a model with MAP estimate = -270.2829
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 1), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (109, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 289.8156 - loglik: -2.6955e+02 - logprior: -2.0247e+01
Epoch 2/2
10/10 - 3s - loss: 263.8938 - loglik: -2.5791e+02 - logprior: -5.8369e+00
Fitted a model with MAP estimate = -258.6280
expansions: []
discards: [  0 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 286.5100 - loglik: -2.6186e+02 - logprior: -2.4629e+01
Epoch 2/2
10/10 - 3s - loss: 269.9648 - loglik: -2.5941e+02 - logprior: -1.0406e+01
Fitted a model with MAP estimate = -265.7651
expansions: [(0, 5), (121, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 278.0427 - loglik: -2.5825e+02 - logprior: -1.9772e+01
Epoch 2/10
10/10 - 3s - loss: 259.3152 - loglik: -2.5379e+02 - logprior: -5.3732e+00
Epoch 3/10
10/10 - 3s - loss: 253.7484 - loglik: -2.5068e+02 - logprior: -2.7132e+00
Epoch 4/10
10/10 - 3s - loss: 250.5463 - loglik: -2.4822e+02 - logprior: -1.8124e+00
Epoch 5/10
10/10 - 3s - loss: 249.4846 - loglik: -2.4754e+02 - logprior: -1.4083e+00
Epoch 6/10
10/10 - 3s - loss: 247.8742 - loglik: -2.4622e+02 - logprior: -1.1699e+00
Epoch 7/10
10/10 - 3s - loss: 247.7389 - loglik: -2.4630e+02 - logprior: -1.0097e+00
Epoch 8/10
10/10 - 3s - loss: 247.5780 - loglik: -2.4634e+02 - logprior: -8.4601e-01
Epoch 9/10
10/10 - 3s - loss: 247.5756 - loglik: -2.4651e+02 - logprior: -6.9798e-01
Epoch 10/10
10/10 - 3s - loss: 247.0759 - loglik: -2.4612e+02 - logprior: -5.9555e-01
Fitted a model with MAP estimate = -246.4319
Time for alignment: 83.6115
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.9414 - loglik: -3.9930e+02 - logprior: -2.1626e+01
Epoch 2/10
10/10 - 2s - loss: 365.5947 - loglik: -3.5993e+02 - logprior: -5.6603e+00
Epoch 3/10
10/10 - 2s - loss: 323.9183 - loglik: -3.2047e+02 - logprior: -3.4447e+00
Epoch 4/10
10/10 - 2s - loss: 292.6664 - loglik: -2.8946e+02 - logprior: -3.1124e+00
Epoch 5/10
10/10 - 2s - loss: 281.9803 - loglik: -2.7861e+02 - logprior: -3.2011e+00
Epoch 6/10
10/10 - 2s - loss: 276.7675 - loglik: -2.7328e+02 - logprior: -3.2076e+00
Epoch 7/10
10/10 - 2s - loss: 274.9161 - loglik: -2.7140e+02 - logprior: -3.1159e+00
Epoch 8/10
10/10 - 2s - loss: 274.6633 - loglik: -2.7120e+02 - logprior: -3.0456e+00
Epoch 9/10
10/10 - 2s - loss: 273.1108 - loglik: -2.6975e+02 - logprior: -2.9775e+00
Epoch 10/10
10/10 - 2s - loss: 273.0764 - loglik: -2.6976e+02 - logprior: -2.9687e+00
Fitted a model with MAP estimate = -272.4396
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (37, 1), (53, 1), (55, 1), (62, 1), (79, 3), (80, 2), (81, 2), (84, 2), (86, 1), (102, 1), (107, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 293.6346 - loglik: -2.7328e+02 - logprior: -2.0325e+01
Epoch 2/2
10/10 - 3s - loss: 268.9584 - loglik: -2.6281e+02 - logprior: -5.9763e+00
Fitted a model with MAP estimate = -263.4934
expansions: [(128, 1)]
discards: [  0   7   8  97 105]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 288.3678 - loglik: -2.6373e+02 - logprior: -2.4611e+01
Epoch 2/2
10/10 - 3s - loss: 271.4926 - loglik: -2.6092e+02 - logprior: -1.0427e+01
Fitted a model with MAP estimate = -267.5176
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 280.5857 - loglik: -2.6083e+02 - logprior: -1.9730e+01
Epoch 2/10
10/10 - 3s - loss: 264.1711 - loglik: -2.5866e+02 - logprior: -5.3741e+00
Epoch 3/10
10/10 - 3s - loss: 259.2612 - loglik: -2.5627e+02 - logprior: -2.6364e+00
Epoch 4/10
10/10 - 3s - loss: 257.7241 - loglik: -2.5552e+02 - logprior: -1.6840e+00
Epoch 5/10
10/10 - 3s - loss: 255.9174 - loglik: -2.5410e+02 - logprior: -1.2873e+00
Epoch 6/10
10/10 - 3s - loss: 254.7727 - loglik: -2.5320e+02 - logprior: -1.0986e+00
Epoch 7/10
10/10 - 3s - loss: 254.3377 - loglik: -2.5300e+02 - logprior: -9.2806e-01
Epoch 8/10
10/10 - 3s - loss: 254.0724 - loglik: -2.5292e+02 - logprior: -7.6908e-01
Epoch 9/10
10/10 - 3s - loss: 254.2259 - loglik: -2.5323e+02 - logprior: -6.2556e-01
Fitted a model with MAP estimate = -253.2224
Time for alignment: 78.9944
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 420.8918 - loglik: -3.9925e+02 - logprior: -2.1627e+01
Epoch 2/10
10/10 - 2s - loss: 364.7790 - loglik: -3.5911e+02 - logprior: -5.6609e+00
Epoch 3/10
10/10 - 2s - loss: 322.8895 - loglik: -3.1943e+02 - logprior: -3.4580e+00
Epoch 4/10
10/10 - 2s - loss: 293.8384 - loglik: -2.9064e+02 - logprior: -3.1229e+00
Epoch 5/10
10/10 - 2s - loss: 281.8529 - loglik: -2.7846e+02 - logprior: -3.2440e+00
Epoch 6/10
10/10 - 2s - loss: 276.9794 - loglik: -2.7344e+02 - logprior: -3.2576e+00
Epoch 7/10
10/10 - 2s - loss: 274.4211 - loglik: -2.7087e+02 - logprior: -3.1482e+00
Epoch 8/10
10/10 - 2s - loss: 272.4458 - loglik: -2.6892e+02 - logprior: -3.0989e+00
Epoch 9/10
10/10 - 2s - loss: 271.8593 - loglik: -2.6835e+02 - logprior: -3.0912e+00
Epoch 10/10
10/10 - 2s - loss: 272.2193 - loglik: -2.6872e+02 - logprior: -3.1090e+00
Fitted a model with MAP estimate = -271.1206
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 290.4259 - loglik: -2.7016e+02 - logprior: -2.0238e+01
Epoch 2/2
10/10 - 3s - loss: 264.6630 - loglik: -2.5863e+02 - logprior: -5.8760e+00
Fitted a model with MAP estimate = -259.0547
expansions: []
discards: [  0 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 287.9621 - loglik: -2.6334e+02 - logprior: -2.4603e+01
Epoch 2/2
10/10 - 3s - loss: 269.6712 - loglik: -2.5914e+02 - logprior: -1.0387e+01
Fitted a model with MAP estimate = -266.3636
expansions: [(0, 5), (121, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 278.0735 - loglik: -2.5827e+02 - logprior: -1.9780e+01
Epoch 2/10
10/10 - 3s - loss: 260.0666 - loglik: -2.5458e+02 - logprior: -5.3447e+00
Epoch 3/10
10/10 - 3s - loss: 254.4600 - loglik: -2.5141e+02 - logprior: -2.6956e+00
Epoch 4/10
10/10 - 3s - loss: 250.9767 - loglik: -2.4867e+02 - logprior: -1.7978e+00
Epoch 5/10
10/10 - 3s - loss: 249.2654 - loglik: -2.4738e+02 - logprior: -1.3850e+00
Epoch 6/10
10/10 - 3s - loss: 249.0078 - loglik: -2.4745e+02 - logprior: -1.1325e+00
Epoch 7/10
10/10 - 3s - loss: 248.0310 - loglik: -2.4668e+02 - logprior: -9.6901e-01
Epoch 8/10
10/10 - 3s - loss: 247.7548 - loglik: -2.4661e+02 - logprior: -7.9386e-01
Epoch 9/10
10/10 - 3s - loss: 247.6794 - loglik: -2.4668e+02 - logprior: -6.4045e-01
Epoch 10/10
10/10 - 3s - loss: 247.4284 - loglik: -2.4653e+02 - logprior: -5.4122e-01
Fitted a model with MAP estimate = -246.8133
Time for alignment: 83.3899
Computed alignments with likelihoods: ['-246.4319', '-253.2224', '-246.8133']
Best model has likelihood: -246.4319
time for generating output: 0.2080
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.913372859025033
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac8a89760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac7228220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a8279e4c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b05bc9850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6aca2ba160>, <__main__.SimpleDirichletPrior object at 0x7f6ac8b5c490>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 270.9047 - loglik: -2.3268e+02 - logprior: -3.8201e+01
Epoch 2/10
10/10 - 1s - loss: 231.5289 - loglik: -2.2088e+02 - logprior: -1.0424e+01
Epoch 3/10
10/10 - 1s - loss: 214.5465 - loglik: -2.0886e+02 - logprior: -5.1999e+00
Epoch 4/10
10/10 - 1s - loss: 203.5322 - loglik: -1.9973e+02 - logprior: -3.4397e+00
Epoch 5/10
10/10 - 1s - loss: 197.3552 - loglik: -1.9400e+02 - logprior: -2.9615e+00
Epoch 6/10
10/10 - 1s - loss: 193.6269 - loglik: -1.9011e+02 - logprior: -2.9568e+00
Epoch 7/10
10/10 - 1s - loss: 191.7932 - loglik: -1.8875e+02 - logprior: -2.5550e+00
Epoch 8/10
10/10 - 1s - loss: 190.8508 - loglik: -1.8843e+02 - logprior: -2.0655e+00
Epoch 9/10
10/10 - 1s - loss: 190.2605 - loglik: -1.8806e+02 - logprior: -1.8997e+00
Epoch 10/10
10/10 - 1s - loss: 189.7709 - loglik: -1.8764e+02 - logprior: -1.8360e+00
Fitted a model with MAP estimate = -189.3752
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.9147 - loglik: -2.0639e+02 - logprior: -5.0501e+01
Epoch 2/2
10/10 - 1s - loss: 206.4964 - loglik: -1.9072e+02 - logprior: -1.5585e+01
Fitted a model with MAP estimate = -196.4091
expansions: [(38, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.7697 - loglik: -1.9065e+02 - logprior: -4.3093e+01
Epoch 2/2
10/10 - 1s - loss: 203.8822 - loglik: -1.8664e+02 - logprior: -1.7086e+01
Fitted a model with MAP estimate = -199.0218
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.8337 - loglik: -1.8636e+02 - logprior: -3.8450e+01
Epoch 2/10
10/10 - 1s - loss: 195.4517 - loglik: -1.8485e+02 - logprior: -1.0461e+01
Epoch 3/10
10/10 - 1s - loss: 188.7119 - loglik: -1.8417e+02 - logprior: -4.3101e+00
Epoch 4/10
10/10 - 1s - loss: 186.1644 - loglik: -1.8374e+02 - logprior: -2.1650e+00
Epoch 5/10
10/10 - 1s - loss: 184.4300 - loglik: -1.8296e+02 - logprior: -1.1530e+00
Epoch 6/10
10/10 - 1s - loss: 183.6288 - loglik: -1.8267e+02 - logprior: -6.0862e-01
Epoch 7/10
10/10 - 1s - loss: 182.8161 - loglik: -1.8213e+02 - logprior: -3.6296e-01
Epoch 8/10
10/10 - 1s - loss: 182.4099 - loglik: -1.8189e+02 - logprior: -2.4049e-01
Epoch 9/10
10/10 - 1s - loss: 181.7373 - loglik: -1.8134e+02 - logprior: -1.4294e-01
Epoch 10/10
10/10 - 1s - loss: 181.3817 - loglik: -1.8112e+02 - logprior: -1.0167e-02
Fitted a model with MAP estimate = -181.0684
Time for alignment: 49.1830
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.7425 - loglik: -2.3251e+02 - logprior: -3.8202e+01
Epoch 2/10
10/10 - 1s - loss: 231.5365 - loglik: -2.2088e+02 - logprior: -1.0425e+01
Epoch 3/10
10/10 - 1s - loss: 215.5116 - loglik: -2.0984e+02 - logprior: -5.1776e+00
Epoch 4/10
10/10 - 1s - loss: 204.5670 - loglik: -2.0081e+02 - logprior: -3.3864e+00
Epoch 5/10
10/10 - 1s - loss: 198.8559 - loglik: -1.9586e+02 - logprior: -2.7487e+00
Epoch 6/10
10/10 - 1s - loss: 194.4049 - loglik: -1.9132e+02 - logprior: -2.7412e+00
Epoch 7/10
10/10 - 1s - loss: 192.4485 - loglik: -1.8952e+02 - logprior: -2.4614e+00
Epoch 8/10
10/10 - 1s - loss: 191.4735 - loglik: -1.8915e+02 - logprior: -1.9773e+00
Epoch 9/10
10/10 - 1s - loss: 190.5868 - loglik: -1.8846e+02 - logprior: -1.8449e+00
Epoch 10/10
10/10 - 1s - loss: 190.0529 - loglik: -1.8799e+02 - logprior: -1.8249e+00
Fitted a model with MAP estimate = -189.5946
expansions: [(0, 3), (9, 2), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.8287 - loglik: -2.0638e+02 - logprior: -5.0419e+01
Epoch 2/2
10/10 - 1s - loss: 206.1063 - loglik: -1.9026e+02 - logprior: -1.5657e+01
Fitted a model with MAP estimate = -196.2706
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.2217 - loglik: -1.9015e+02 - logprior: -4.3042e+01
Epoch 2/2
10/10 - 1s - loss: 204.0480 - loglik: -1.8678e+02 - logprior: -1.7117e+01
Fitted a model with MAP estimate = -198.9228
expansions: []
discards: [ 0  1 11]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 225.7741 - loglik: -1.8716e+02 - logprior: -3.8593e+01
Epoch 2/10
10/10 - 1s - loss: 195.8458 - loglik: -1.8512e+02 - logprior: -1.0582e+01
Epoch 3/10
10/10 - 1s - loss: 189.2973 - loglik: -1.8474e+02 - logprior: -4.3285e+00
Epoch 4/10
10/10 - 1s - loss: 186.5013 - loglik: -1.8406e+02 - logprior: -2.1900e+00
Epoch 5/10
10/10 - 1s - loss: 184.9335 - loglik: -1.8341e+02 - logprior: -1.2120e+00
Epoch 6/10
10/10 - 1s - loss: 183.9624 - loglik: -1.8290e+02 - logprior: -7.2462e-01
Epoch 7/10
10/10 - 1s - loss: 183.2475 - loglik: -1.8249e+02 - logprior: -4.6039e-01
Epoch 8/10
10/10 - 1s - loss: 182.6435 - loglik: -1.8217e+02 - logprior: -2.1108e-01
Epoch 9/10
10/10 - 1s - loss: 182.4400 - loglik: -1.8220e+02 - logprior: 0.0096
Epoch 10/10
10/10 - 1s - loss: 182.1459 - loglik: -1.8207e+02 - logprior: 0.1639
Fitted a model with MAP estimate = -181.7031
Time for alignment: 48.4507
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.8763 - loglik: -2.3265e+02 - logprior: -3.8203e+01
Epoch 2/10
10/10 - 1s - loss: 231.5423 - loglik: -2.2089e+02 - logprior: -1.0428e+01
Epoch 3/10
10/10 - 1s - loss: 214.5562 - loglik: -2.0887e+02 - logprior: -5.2056e+00
Epoch 4/10
10/10 - 1s - loss: 204.1380 - loglik: -2.0036e+02 - logprior: -3.4288e+00
Epoch 5/10
10/10 - 1s - loss: 198.9756 - loglik: -1.9575e+02 - logprior: -2.9410e+00
Epoch 6/10
10/10 - 1s - loss: 195.2798 - loglik: -1.9203e+02 - logprior: -2.8455e+00
Epoch 7/10
10/10 - 1s - loss: 192.7645 - loglik: -1.8955e+02 - logprior: -2.7893e+00
Epoch 8/10
10/10 - 1s - loss: 191.4607 - loglik: -1.8860e+02 - logprior: -2.5201e+00
Epoch 9/10
10/10 - 1s - loss: 190.6693 - loglik: -1.8812e+02 - logprior: -2.2545e+00
Epoch 10/10
10/10 - 1s - loss: 190.0746 - loglik: -1.8766e+02 - logprior: -2.1522e+00
Fitted a model with MAP estimate = -189.5425
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (43, 1), (45, 1), (49, 1), (57, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.4691 - loglik: -2.0692e+02 - logprior: -5.0520e+01
Epoch 2/2
10/10 - 1s - loss: 206.2192 - loglik: -1.9051e+02 - logprior: -1.5525e+01
Fitted a model with MAP estimate = -196.0893
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.5753 - loglik: -1.9056e+02 - logprior: -4.2989e+01
Epoch 2/2
10/10 - 1s - loss: 203.8118 - loglik: -1.8658e+02 - logprior: -1.7079e+01
Fitted a model with MAP estimate = -198.9877
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.9920 - loglik: -1.8650e+02 - logprior: -3.8469e+01
Epoch 2/10
10/10 - 1s - loss: 195.3703 - loglik: -1.8476e+02 - logprior: -1.0470e+01
Epoch 3/10
10/10 - 1s - loss: 188.9215 - loglik: -1.8437e+02 - logprior: -4.3093e+00
Epoch 4/10
10/10 - 1s - loss: 186.1821 - loglik: -1.8373e+02 - logprior: -2.1803e+00
Epoch 5/10
10/10 - 1s - loss: 184.6890 - loglik: -1.8321e+02 - logprior: -1.1509e+00
Epoch 6/10
10/10 - 1s - loss: 183.6503 - loglik: -1.8268e+02 - logprior: -6.1156e-01
Epoch 7/10
10/10 - 1s - loss: 182.7932 - loglik: -1.8210e+02 - logprior: -3.7150e-01
Epoch 8/10
10/10 - 1s - loss: 182.3138 - loglik: -1.8176e+02 - logprior: -2.6201e-01
Epoch 9/10
10/10 - 1s - loss: 181.6757 - loglik: -1.8123e+02 - logprior: -1.7510e-01
Epoch 10/10
10/10 - 1s - loss: 181.4595 - loglik: -1.8117e+02 - logprior: -3.1321e-02
Fitted a model with MAP estimate = -181.0133
Time for alignment: 47.2870
Computed alignments with likelihoods: ['-181.0684', '-181.7031', '-181.0133']
Best model has likelihood: -181.0133
time for generating output: 0.1777
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.8073107049608355
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ad2cbaac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b16e06d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16ebc7c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aca506310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ae3aae1f0>, <__main__.SimpleDirichletPrior object at 0x7f6af4f25a90>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 951.3032 - loglik: -9.4906e+02 - logprior: -1.6204e+00
Epoch 2/10
43/43 - 34s - loss: 834.5249 - loglik: -8.3029e+02 - logprior: -2.3970e+00
Epoch 3/10
43/43 - 35s - loss: 818.9805 - loglik: -8.1474e+02 - logprior: -2.5096e+00
Epoch 4/10
43/43 - 34s - loss: 815.3633 - loglik: -8.1111e+02 - logprior: -2.5010e+00
Epoch 5/10
43/43 - 35s - loss: 814.7359 - loglik: -8.1055e+02 - logprior: -2.4974e+00
Epoch 6/10
43/43 - 35s - loss: 814.0339 - loglik: -8.0990e+02 - logprior: -2.5207e+00
Epoch 7/10
43/43 - 35s - loss: 813.3760 - loglik: -8.0933e+02 - logprior: -2.5261e+00
Epoch 8/10
43/43 - 35s - loss: 811.7093 - loglik: -8.0770e+02 - logprior: -2.5702e+00
Epoch 9/10
43/43 - 35s - loss: 812.0057 - loglik: -8.0814e+02 - logprior: -2.5390e+00
Fitted a model with MAP estimate = -817.8602
expansions: [(7, 2), (16, 1), (21, 3), (22, 1), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 2), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (88, 1), (91, 2), (93, 1), (95, 1), (96, 1), (101, 1), (104, 2), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 2), (153, 2), (156, 3), (157, 2), (168, 1), (181, 1), (183, 1), (185, 1), (186, 1), (187, 2), (188, 2), (204, 2), (206, 2), (207, 4), (208, 1), (210, 1), (221, 1), (222, 1), (223, 2), (225, 1), (226, 2), (236, 1), (239, 2), (242, 1), (244, 1), (245, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 56s - loss: 830.4368 - loglik: -8.2722e+02 - logprior: -2.7903e+00
Epoch 2/2
43/43 - 52s - loss: 787.3549 - loglik: -7.8389e+02 - logprior: -1.9058e+00
Fitted a model with MAP estimate = -779.9532
expansions: [(0, 2)]
discards: [  0  25  60 111 119 137 171 198 203 206 243 244 269 277 302 317 358 359
 361]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 801.6137 - loglik: -7.9950e+02 - logprior: -1.7666e+00
Epoch 2/2
43/43 - 48s - loss: 785.3057 - loglik: -7.8321e+02 - logprior: -1.0931e+00
Fitted a model with MAP estimate = -779.7829
expansions: [(344, 1)]
discards: [  0   1 284]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 70s - loss: 778.0836 - loglik: -7.7620e+02 - logprior: -1.2136e+00
Epoch 2/10
61/61 - 66s - loss: 761.7952 - loglik: -7.5872e+02 - logprior: -8.0308e-01
Epoch 3/10
61/61 - 66s - loss: 754.1447 - loglik: -7.5078e+02 - logprior: -7.5572e-01
Epoch 4/10
61/61 - 66s - loss: 752.7239 - loglik: -7.4924e+02 - logprior: -6.8551e-01
Epoch 5/10
61/61 - 66s - loss: 750.6494 - loglik: -7.4718e+02 - logprior: -6.2981e-01
Epoch 6/10
61/61 - 67s - loss: 751.5797 - loglik: -7.4832e+02 - logprior: -5.4816e-01
Fitted a model with MAP estimate = -746.8062
Time for alignment: 1247.0775
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 950.8836 - loglik: -9.4864e+02 - logprior: -1.6278e+00
Epoch 2/10
43/43 - 35s - loss: 831.7885 - loglik: -8.2708e+02 - logprior: -2.2267e+00
Epoch 3/10
43/43 - 34s - loss: 820.1661 - loglik: -8.1522e+02 - logprior: -2.3526e+00
Epoch 4/10
43/43 - 35s - loss: 812.6784 - loglik: -8.0833e+02 - logprior: -2.4158e+00
Epoch 5/10
43/43 - 35s - loss: 813.0988 - loglik: -8.0894e+02 - logprior: -2.4209e+00
Fitted a model with MAP estimate = -810.3165
expansions: [(0, 2), (16, 1), (21, 3), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 2), (46, 1), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (121, 1), (122, 1), (125, 1), (128, 2), (130, 2), (132, 2), (148, 2), (149, 1), (154, 1), (155, 3), (157, 2), (181, 1), (183, 2), (185, 3), (186, 1), (188, 1), (200, 1), (204, 1), (205, 4), (206, 1), (207, 1), (208, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 4), (242, 1), (244, 1), (245, 2), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 372 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 56s - loss: 821.0363 - loglik: -8.1808e+02 - logprior: -2.5692e+00
Epoch 2/2
43/43 - 52s - loss: 786.2029 - loglik: -7.8308e+02 - logprior: -1.7282e+00
Fitted a model with MAP estimate = -779.6572
expansions: []
discards: [  0  26  61 111 137 167 171 174 208 238 242 269 270 300 319 320 327 360
 363]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 53s - loss: 802.4944 - loglik: -7.9974e+02 - logprior: -2.4362e+00
Epoch 2/2
43/43 - 48s - loss: 784.4487 - loglik: -7.8240e+02 - logprior: -1.1981e+00
Fitted a model with MAP estimate = -780.6841
expansions: [(258, 2), (282, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 70s - loss: 776.8818 - loglik: -7.7512e+02 - logprior: -1.0512e+00
Epoch 2/10
61/61 - 67s - loss: 758.6432 - loglik: -7.5522e+02 - logprior: -9.3894e-01
Epoch 3/10
61/61 - 67s - loss: 753.6110 - loglik: -7.4995e+02 - logprior: -9.2551e-01
Epoch 4/10
61/61 - 68s - loss: 752.6987 - loglik: -7.4901e+02 - logprior: -8.4724e-01
Epoch 5/10
61/61 - 67s - loss: 751.7433 - loglik: -7.4809e+02 - logprior: -7.8903e-01
Epoch 6/10
61/61 - 67s - loss: 749.2434 - loglik: -7.4583e+02 - logprior: -7.2985e-01
Epoch 7/10
61/61 - 68s - loss: 749.2557 - loglik: -7.4593e+02 - logprior: -6.4459e-01
Fitted a model with MAP estimate = -745.0073
Time for alignment: 1185.7341
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 949.0801 - loglik: -9.4679e+02 - logprior: -1.6587e+00
Epoch 2/10
43/43 - 34s - loss: 830.1179 - loglik: -8.2575e+02 - logprior: -2.4581e+00
Epoch 3/10
43/43 - 35s - loss: 818.8063 - loglik: -8.1428e+02 - logprior: -2.6449e+00
Epoch 4/10
43/43 - 35s - loss: 812.9614 - loglik: -8.0847e+02 - logprior: -2.5994e+00
Epoch 5/10
43/43 - 34s - loss: 812.7369 - loglik: -8.0834e+02 - logprior: -2.6162e+00
Epoch 6/10
43/43 - 35s - loss: 810.1533 - loglik: -8.0581e+02 - logprior: -2.6687e+00
Epoch 7/10
43/43 - 34s - loss: 810.9641 - loglik: -8.0673e+02 - logprior: -2.6664e+00
Fitted a model with MAP estimate = -812.8964
expansions: [(8, 1), (13, 1), (16, 1), (21, 3), (22, 2), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (43, 1), (45, 2), (46, 1), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 3), (157, 2), (168, 1), (181, 1), (183, 2), (185, 2), (186, 2), (187, 2), (188, 1), (203, 1), (205, 3), (206, 1), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 4), (242, 1), (244, 1), (245, 2), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 55s - loss: 828.1265 - loglik: -8.2484e+02 - logprior: -2.8941e+00
Epoch 2/2
43/43 - 52s - loss: 786.4402 - loglik: -7.8316e+02 - logprior: -1.8751e+00
Fitted a model with MAP estimate = -779.2346
expansions: [(0, 2), (318, 1)]
discards: [  0  27  28  32  61 111 170 197 202 205 236 246 268 269 298 315 316 325
 356 358 361]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 51s - loss: 802.2929 - loglik: -8.0020e+02 - logprior: -1.7272e+00
Epoch 2/2
43/43 - 48s - loss: 785.6116 - loglik: -7.8319e+02 - logprior: -1.1210e+00
Fitted a model with MAP estimate = -778.7288
expansions: []
discards: [  0 234]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 70s - loss: 777.9102 - loglik: -7.7610e+02 - logprior: -1.1928e+00
Epoch 2/10
61/61 - 66s - loss: 760.8693 - loglik: -7.5789e+02 - logprior: -9.1280e-01
Epoch 3/10
61/61 - 66s - loss: 754.7059 - loglik: -7.5135e+02 - logprior: -8.8669e-01
Epoch 4/10
61/61 - 67s - loss: 755.2171 - loglik: -7.5168e+02 - logprior: -8.2122e-01
Fitted a model with MAP estimate = -749.5523
Time for alignment: 1042.9622
Computed alignments with likelihoods: ['-746.8062', '-745.0073', '-749.5523']
Best model has likelihood: -745.0073
time for generating output: 0.4536
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.7925625660909411
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a826e9670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad41a9d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c580e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a9c12e130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16ebc7c0>, <__main__.SimpleDirichletPrior object at 0x7f6a82927550>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 251.0545 - loglik: -1.9339e+02 - logprior: -5.7638e+01
Epoch 2/10
10/10 - 1s - loss: 189.2370 - loglik: -1.7266e+02 - logprior: -1.6547e+01
Epoch 3/10
10/10 - 2s - loss: 163.5276 - loglik: -1.5496e+02 - logprior: -8.5708e+00
Epoch 4/10
10/10 - 2s - loss: 154.6535 - loglik: -1.4910e+02 - logprior: -5.5250e+00
Epoch 5/10
10/10 - 1s - loss: 150.9587 - loglik: -1.4664e+02 - logprior: -4.1658e+00
Epoch 6/10
10/10 - 2s - loss: 149.7277 - loglik: -1.4614e+02 - logprior: -3.3098e+00
Epoch 7/10
10/10 - 1s - loss: 147.8040 - loglik: -1.4474e+02 - logprior: -2.7641e+00
Epoch 8/10
10/10 - 2s - loss: 148.3423 - loglik: -1.4554e+02 - logprior: -2.5081e+00
Fitted a model with MAP estimate = -147.7800
expansions: [(10, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.9325 - loglik: -1.4666e+02 - logprior: -6.5250e+01
Epoch 2/2
10/10 - 2s - loss: 172.8854 - loglik: -1.4466e+02 - logprior: -2.8061e+01
Fitted a model with MAP estimate = -166.4324
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.6888 - loglik: -1.4320e+02 - logprior: -5.3470e+01
Epoch 2/2
10/10 - 2s - loss: 158.5746 - loglik: -1.4293e+02 - logprior: -1.5539e+01
Fitted a model with MAP estimate = -152.7700
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 207.0443 - loglik: -1.4384e+02 - logprior: -6.3181e+01
Epoch 2/10
10/10 - 2s - loss: 165.3715 - loglik: -1.4332e+02 - logprior: -2.1961e+01
Epoch 3/10
10/10 - 2s - loss: 153.4141 - loglik: -1.4385e+02 - logprior: -9.3699e+00
Epoch 4/10
10/10 - 2s - loss: 148.2291 - loglik: -1.4296e+02 - logprior: -5.0318e+00
Epoch 5/10
10/10 - 2s - loss: 147.0709 - loglik: -1.4355e+02 - logprior: -3.2581e+00
Epoch 6/10
10/10 - 2s - loss: 145.5834 - loglik: -1.4287e+02 - logprior: -2.4362e+00
Epoch 7/10
10/10 - 2s - loss: 144.6597 - loglik: -1.4236e+02 - logprior: -2.0130e+00
Epoch 8/10
10/10 - 2s - loss: 144.5930 - loglik: -1.4259e+02 - logprior: -1.6985e+00
Epoch 9/10
10/10 - 2s - loss: 144.2533 - loglik: -1.4249e+02 - logprior: -1.4491e+00
Epoch 10/10
10/10 - 2s - loss: 143.7713 - loglik: -1.4220e+02 - logprior: -1.2484e+00
Fitted a model with MAP estimate = -143.5828
Time for alignment: 51.2232
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 251.1482 - loglik: -1.9349e+02 - logprior: -5.7638e+01
Epoch 2/10
10/10 - 2s - loss: 189.3945 - loglik: -1.7282e+02 - logprior: -1.6544e+01
Epoch 3/10
10/10 - 1s - loss: 163.1136 - loglik: -1.5456e+02 - logprior: -8.5548e+00
Epoch 4/10
10/10 - 2s - loss: 154.1861 - loglik: -1.4858e+02 - logprior: -5.5872e+00
Epoch 5/10
10/10 - 1s - loss: 149.7491 - loglik: -1.4524e+02 - logprior: -4.3691e+00
Epoch 6/10
10/10 - 2s - loss: 148.3438 - loglik: -1.4449e+02 - logprior: -3.5786e+00
Epoch 7/10
10/10 - 2s - loss: 147.8751 - loglik: -1.4454e+02 - logprior: -3.0453e+00
Epoch 8/10
10/10 - 1s - loss: 147.1726 - loglik: -1.4412e+02 - logprior: -2.7820e+00
Epoch 9/10
10/10 - 2s - loss: 146.0566 - loglik: -1.4307e+02 - logprior: -2.6819e+00
Epoch 10/10
10/10 - 2s - loss: 146.2488 - loglik: -1.4333e+02 - logprior: -2.5768e+00
Fitted a model with MAP estimate = -145.8316
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.8561 - loglik: -1.4550e+02 - logprior: -6.5331e+01
Epoch 2/2
10/10 - 2s - loss: 170.4385 - loglik: -1.4221e+02 - logprior: -2.8052e+01
Fitted a model with MAP estimate = -164.2802
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 194.9338 - loglik: -1.4150e+02 - logprior: -5.3414e+01
Epoch 2/2
10/10 - 2s - loss: 155.8263 - loglik: -1.4020e+02 - logprior: -1.5514e+01
Fitted a model with MAP estimate = -150.5907
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 204.8615 - loglik: -1.4166e+02 - logprior: -6.3178e+01
Epoch 2/10
10/10 - 2s - loss: 163.6559 - loglik: -1.4156e+02 - logprior: -2.2006e+01
Epoch 3/10
10/10 - 2s - loss: 150.8448 - loglik: -1.4125e+02 - logprior: -9.3809e+00
Epoch 4/10
10/10 - 2s - loss: 145.9049 - loglik: -1.4059e+02 - logprior: -5.0371e+00
Epoch 5/10
10/10 - 2s - loss: 144.8560 - loglik: -1.4129e+02 - logprior: -3.2736e+00
Epoch 6/10
10/10 - 1s - loss: 143.3085 - loglik: -1.4055e+02 - logprior: -2.4477e+00
Epoch 7/10
10/10 - 1s - loss: 142.4116 - loglik: -1.4006e+02 - logprior: -2.0206e+00
Epoch 8/10
10/10 - 2s - loss: 142.9841 - loglik: -1.4093e+02 - logprior: -1.7140e+00
Fitted a model with MAP estimate = -141.8634
Time for alignment: 50.8356
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 251.5421 - loglik: -1.9389e+02 - logprior: -5.7633e+01
Epoch 2/10
10/10 - 2s - loss: 188.1148 - loglik: -1.7155e+02 - logprior: -1.6537e+01
Epoch 3/10
10/10 - 2s - loss: 162.9962 - loglik: -1.5444e+02 - logprior: -8.5561e+00
Epoch 4/10
10/10 - 2s - loss: 154.4875 - loglik: -1.4893e+02 - logprior: -5.5295e+00
Epoch 5/10
10/10 - 2s - loss: 151.4892 - loglik: -1.4714e+02 - logprior: -4.1224e+00
Epoch 6/10
10/10 - 1s - loss: 149.8067 - loglik: -1.4622e+02 - logprior: -3.1889e+00
Epoch 7/10
10/10 - 2s - loss: 148.5887 - loglik: -1.4560e+02 - logprior: -2.6325e+00
Epoch 8/10
10/10 - 1s - loss: 148.4511 - loglik: -1.4577e+02 - logprior: -2.3723e+00
Epoch 9/10
10/10 - 2s - loss: 148.1602 - loglik: -1.4559e+02 - logprior: -2.2434e+00
Epoch 10/10
10/10 - 2s - loss: 148.2699 - loglik: -1.4577e+02 - logprior: -2.1581e+00
Fitted a model with MAP estimate = -147.5189
expansions: [(11, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 212.2201 - loglik: -1.4704e+02 - logprior: -6.5150e+01
Epoch 2/2
10/10 - 2s - loss: 172.9252 - loglik: -1.4476e+02 - logprior: -2.8014e+01
Fitted a model with MAP estimate = -166.7110
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.9478 - loglik: -1.4363e+02 - logprior: -5.3299e+01
Epoch 2/2
10/10 - 2s - loss: 158.5660 - loglik: -1.4295e+02 - logprior: -1.5501e+01
Fitted a model with MAP estimate = -152.8132
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 208.9511 - loglik: -1.4466e+02 - logprior: -6.4268e+01
Epoch 2/10
10/10 - 2s - loss: 171.4827 - loglik: -1.4481e+02 - logprior: -2.6576e+01
Epoch 3/10
10/10 - 2s - loss: 158.7345 - loglik: -1.4423e+02 - logprior: -1.4307e+01
Epoch 4/10
10/10 - 2s - loss: 150.3645 - loglik: -1.4406e+02 - logprior: -6.0397e+00
Epoch 5/10
10/10 - 2s - loss: 146.6283 - loglik: -1.4298e+02 - logprior: -3.3733e+00
Epoch 6/10
10/10 - 2s - loss: 145.5782 - loglik: -1.4289e+02 - logprior: -2.4176e+00
Epoch 7/10
10/10 - 2s - loss: 146.1560 - loglik: -1.4388e+02 - logprior: -1.9807e+00
Fitted a model with MAP estimate = -144.9222
Time for alignment: 49.9192
Computed alignments with likelihoods: ['-143.5828', '-141.8634', '-144.9222']
Best model has likelihood: -141.8634
time for generating output: 0.1298
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9472954230235784
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69b62b01c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e9141670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9141dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b2fca5df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6abebcae20>, <__main__.SimpleDirichletPrior object at 0x7f6afd48b430>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 542.2659 - loglik: -4.7191e+02 - logprior: -7.0337e+01
Epoch 2/10
10/10 - 3s - loss: 443.0158 - loglik: -4.2615e+02 - logprior: -1.6856e+01
Epoch 3/10
10/10 - 3s - loss: 389.4566 - loglik: -3.8152e+02 - logprior: -7.9264e+00
Epoch 4/10
10/10 - 3s - loss: 360.5214 - loglik: -3.5542e+02 - logprior: -5.1036e+00
Epoch 5/10
10/10 - 3s - loss: 348.1006 - loglik: -3.4392e+02 - logprior: -4.1143e+00
Epoch 6/10
10/10 - 3s - loss: 341.8648 - loglik: -3.3811e+02 - logprior: -3.5000e+00
Epoch 7/10
10/10 - 3s - loss: 339.2157 - loglik: -3.3601e+02 - logprior: -2.8570e+00
Epoch 8/10
10/10 - 3s - loss: 336.9986 - loglik: -3.3425e+02 - logprior: -2.4260e+00
Epoch 9/10
10/10 - 3s - loss: 336.2895 - loglik: -3.3382e+02 - logprior: -2.1887e+00
Epoch 10/10
10/10 - 3s - loss: 335.2356 - loglik: -3.3292e+02 - logprior: -2.0152e+00
Fitted a model with MAP estimate = -334.5075
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (69, 2), (71, 1), (77, 3), (79, 1), (80, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 416.9607 - loglik: -3.3660e+02 - logprior: -8.0338e+01
Epoch 2/2
10/10 - 3s - loss: 351.4660 - loglik: -3.1894e+02 - logprior: -3.2353e+01
Fitted a model with MAP estimate = -339.4383
expansions: [(0, 2), (125, 1)]
discards: [  0  22  43  48  50  82 159]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 379.3719 - loglik: -3.1631e+02 - logprior: -6.3047e+01
Epoch 2/2
10/10 - 3s - loss: 324.7432 - loglik: -3.0962e+02 - logprior: -1.5021e+01
Fitted a model with MAP estimate = -315.3087
expansions: [(108, 1)]
discards: [  0 135]
Re-initialized the encoder parameters.
Fitting a model of length 165 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 390.1699 - loglik: -3.1349e+02 - logprior: -7.6675e+01
Epoch 2/10
10/10 - 3s - loss: 334.1314 - loglik: -3.1049e+02 - logprior: -2.3579e+01
Epoch 3/10
10/10 - 3s - loss: 314.1333 - loglik: -3.0678e+02 - logprior: -7.1469e+00
Epoch 4/10
10/10 - 3s - loss: 305.4004 - loglik: -3.0375e+02 - logprior: -1.2888e+00
Epoch 5/10
10/10 - 3s - loss: 302.0263 - loglik: -3.0301e+02 - logprior: 1.3972
Epoch 6/10
10/10 - 3s - loss: 299.9347 - loglik: -3.0242e+02 - logprior: 2.8760
Epoch 7/10
10/10 - 3s - loss: 298.7682 - loglik: -3.0222e+02 - logprior: 3.8314
Epoch 8/10
10/10 - 3s - loss: 297.7272 - loglik: -3.0194e+02 - logprior: 4.5988
Epoch 9/10
10/10 - 3s - loss: 296.8165 - loglik: -3.0170e+02 - logprior: 5.2745
Epoch 10/10
10/10 - 3s - loss: 296.8384 - loglik: -3.0232e+02 - logprior: 5.8855
Fitted a model with MAP estimate = -295.8539
Time for alignment: 93.8209
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 542.3873 - loglik: -4.7203e+02 - logprior: -7.0339e+01
Epoch 2/10
10/10 - 3s - loss: 442.8482 - loglik: -4.2601e+02 - logprior: -1.6835e+01
Epoch 3/10
10/10 - 3s - loss: 389.4233 - loglik: -3.8165e+02 - logprior: -7.7623e+00
Epoch 4/10
10/10 - 3s - loss: 360.0090 - loglik: -3.5488e+02 - logprior: -5.1292e+00
Epoch 5/10
10/10 - 3s - loss: 346.9245 - loglik: -3.4258e+02 - logprior: -4.2341e+00
Epoch 6/10
10/10 - 3s - loss: 339.2735 - loglik: -3.3545e+02 - logprior: -3.4219e+00
Epoch 7/10
10/10 - 3s - loss: 336.1620 - loglik: -3.3271e+02 - logprior: -2.9162e+00
Epoch 8/10
10/10 - 3s - loss: 333.9589 - loglik: -3.3091e+02 - logprior: -2.5837e+00
Epoch 9/10
10/10 - 3s - loss: 332.4679 - loglik: -3.2969e+02 - logprior: -2.3647e+00
Epoch 10/10
10/10 - 3s - loss: 330.6971 - loglik: -3.2813e+02 - logprior: -2.1356e+00
Fitted a model with MAP estimate = -330.3311
expansions: [(11, 3), (12, 1), (16, 3), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (51, 1), (62, 1), (68, 1), (79, 4), (80, 2), (89, 1), (90, 3), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 411.4689 - loglik: -3.3137e+02 - logprior: -8.0070e+01
Epoch 2/2
10/10 - 4s - loss: 343.2211 - loglik: -3.1062e+02 - logprior: -3.2431e+01
Fitted a model with MAP estimate = -330.9087
expansions: [(0, 1), (19, 1), (25, 1), (130, 1)]
discards: [  0  11  45  50  52 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 371.9199 - loglik: -3.0898e+02 - logprior: -6.2917e+01
Epoch 2/2
10/10 - 4s - loss: 316.5054 - loglik: -3.0142e+02 - logprior: -1.4953e+01
Fitted a model with MAP estimate = -307.0264
expansions: [(73, 1)]
discards: [64]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 364.0589 - loglik: -3.0252e+02 - logprior: -6.1528e+01
Epoch 2/10
10/10 - 4s - loss: 313.7454 - loglik: -2.9974e+02 - logprior: -1.3916e+01
Epoch 3/10
10/10 - 4s - loss: 300.5675 - loglik: -2.9592e+02 - logprior: -4.3970e+00
Epoch 4/10
10/10 - 4s - loss: 295.1769 - loglik: -2.9448e+02 - logprior: -3.4463e-01
Epoch 5/10
10/10 - 4s - loss: 292.1111 - loglik: -2.9379e+02 - logprior: 2.0599
Epoch 6/10
10/10 - 4s - loss: 290.2964 - loglik: -2.9342e+02 - logprior: 3.5393
Epoch 7/10
10/10 - 4s - loss: 289.3165 - loglik: -2.9351e+02 - logprior: 4.5951
Epoch 8/10
10/10 - 4s - loss: 288.3072 - loglik: -2.9329e+02 - logprior: 5.3816
Epoch 9/10
10/10 - 4s - loss: 288.0128 - loglik: -2.9360e+02 - logprior: 5.9879
Epoch 10/10
10/10 - 4s - loss: 287.1592 - loglik: -2.9321e+02 - logprior: 6.4544
Fitted a model with MAP estimate = -286.3541
Time for alignment: 97.5376
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 542.4474 - loglik: -4.7210e+02 - logprior: -7.0338e+01
Epoch 2/10
10/10 - 3s - loss: 442.7414 - loglik: -4.2588e+02 - logprior: -1.6857e+01
Epoch 3/10
10/10 - 3s - loss: 390.2224 - loglik: -3.8233e+02 - logprior: -7.8895e+00
Epoch 4/10
10/10 - 3s - loss: 360.0441 - loglik: -3.5492e+02 - logprior: -5.1148e+00
Epoch 5/10
10/10 - 3s - loss: 345.2121 - loglik: -3.4099e+02 - logprior: -4.1038e+00
Epoch 6/10
10/10 - 3s - loss: 337.5857 - loglik: -3.3379e+02 - logprior: -3.4037e+00
Epoch 7/10
10/10 - 3s - loss: 333.3568 - loglik: -3.3008e+02 - logprior: -2.7979e+00
Epoch 8/10
10/10 - 3s - loss: 330.0880 - loglik: -3.2713e+02 - logprior: -2.5308e+00
Epoch 9/10
10/10 - 3s - loss: 329.1774 - loglik: -3.2638e+02 - logprior: -2.3747e+00
Epoch 10/10
10/10 - 3s - loss: 327.6115 - loglik: -3.2493e+02 - logprior: -2.2057e+00
Fitted a model with MAP estimate = -326.6819
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (39, 2), (55, 1), (66, 1), (69, 1), (71, 1), (77, 2), (79, 2), (80, 1), (89, 1), (90, 2), (101, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 407.4678 - loglik: -3.2718e+02 - logprior: -8.0266e+01
Epoch 2/2
10/10 - 3s - loss: 341.3911 - loglik: -3.0909e+02 - logprior: -3.2148e+01
Fitted a model with MAP estimate = -329.9158
expansions: [(0, 1)]
discards: [  0  22 111]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 370.6725 - loglik: -3.0758e+02 - logprior: -6.3082e+01
Epoch 2/2
10/10 - 3s - loss: 318.3065 - loglik: -3.0294e+02 - logprior: -1.5285e+01
Fitted a model with MAP estimate = -309.4626
expansions: [(122, 1)]
discards: [ 46 157]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 366.9374 - loglik: -3.0513e+02 - logprior: -6.1792e+01
Epoch 2/10
10/10 - 3s - loss: 316.1271 - loglik: -3.0187e+02 - logprior: -1.4194e+01
Epoch 3/10
10/10 - 3s - loss: 304.5793 - loglik: -2.9971e+02 - logprior: -4.6315e+00
Epoch 4/10
10/10 - 3s - loss: 299.8911 - loglik: -2.9894e+02 - logprior: -5.4911e-01
Epoch 5/10
10/10 - 3s - loss: 296.6893 - loglik: -2.9815e+02 - logprior: 1.8716
Epoch 6/10
10/10 - 3s - loss: 295.0696 - loglik: -2.9800e+02 - logprior: 3.3318
Epoch 7/10
10/10 - 3s - loss: 294.4382 - loglik: -2.9835e+02 - logprior: 4.3220
Epoch 8/10
10/10 - 3s - loss: 293.3874 - loglik: -2.9805e+02 - logprior: 5.0777
Epoch 9/10
10/10 - 3s - loss: 292.6157 - loglik: -2.9786e+02 - logprior: 5.6469
Epoch 10/10
10/10 - 3s - loss: 292.0607 - loglik: -2.9776e+02 - logprior: 6.1046
Fitted a model with MAP estimate = -291.2356
Time for alignment: 93.2527
Computed alignments with likelihoods: ['-295.8539', '-286.3541', '-291.2356']
Best model has likelihood: -286.3541
time for generating output: 0.2089
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.8028518288902666
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69e85835b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac8fa2eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fa2f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d814fa90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb196520>, <__main__.SimpleDirichletPrior object at 0x7f69a4cc6490>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.8946 - loglik: -2.8263e+02 - logprior: -1.0247e+01
Epoch 2/10
12/12 - 2s - loss: 254.2448 - loglik: -2.5146e+02 - logprior: -2.7367e+00
Epoch 3/10
12/12 - 2s - loss: 225.4007 - loglik: -2.2301e+02 - logprior: -2.0711e+00
Epoch 4/10
12/12 - 2s - loss: 213.5128 - loglik: -2.1063e+02 - logprior: -2.1864e+00
Epoch 5/10
12/12 - 2s - loss: 210.2922 - loglik: -2.0706e+02 - logprior: -2.3229e+00
Epoch 6/10
12/12 - 2s - loss: 207.5885 - loglik: -2.0464e+02 - logprior: -2.2717e+00
Epoch 7/10
12/12 - 2s - loss: 206.2601 - loglik: -2.0351e+02 - logprior: -2.2429e+00
Epoch 8/10
12/12 - 2s - loss: 205.9621 - loglik: -2.0329e+02 - logprior: -2.2684e+00
Epoch 9/10
12/12 - 2s - loss: 205.5697 - loglik: -2.0292e+02 - logprior: -2.2697e+00
Epoch 10/10
12/12 - 2s - loss: 204.9616 - loglik: -2.0235e+02 - logprior: -2.2613e+00
Fitted a model with MAP estimate = -204.5789
expansions: [(7, 2), (8, 1), (10, 2), (11, 1), (12, 1), (13, 1), (21, 1), (29, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 4), (59, 3), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 223.3289 - loglik: -2.1128e+02 - logprior: -1.2011e+01
Epoch 2/2
12/12 - 2s - loss: 199.8974 - loglik: -1.9435e+02 - logprior: -5.3240e+00
Fitted a model with MAP estimate = -195.0119
expansions: [(0, 4)]
discards: [ 0 47 62 77 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 204.7894 - loglik: -1.9521e+02 - logprior: -9.5400e+00
Epoch 2/2
12/12 - 2s - loss: 192.7614 - loglik: -1.8977e+02 - logprior: -2.7796e+00
Fitted a model with MAP estimate = -190.2203
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 201.0384 - loglik: -1.9170e+02 - logprior: -9.3091e+00
Epoch 2/10
12/12 - 2s - loss: 193.3845 - loglik: -1.9045e+02 - logprior: -2.7227e+00
Epoch 3/10
12/12 - 2s - loss: 189.4584 - loglik: -1.8692e+02 - logprior: -2.0779e+00
Epoch 4/10
12/12 - 2s - loss: 186.6412 - loglik: -1.8419e+02 - logprior: -1.8595e+00
Epoch 5/10
12/12 - 2s - loss: 184.5166 - loglik: -1.8224e+02 - logprior: -1.6496e+00
Epoch 6/10
12/12 - 2s - loss: 184.3145 - loglik: -1.8210e+02 - logprior: -1.6082e+00
Epoch 7/10
12/12 - 2s - loss: 183.6377 - loglik: -1.8157e+02 - logprior: -1.5445e+00
Epoch 8/10
12/12 - 2s - loss: 183.1192 - loglik: -1.8115e+02 - logprior: -1.5218e+00
Epoch 9/10
12/12 - 2s - loss: 182.6522 - loglik: -1.8078e+02 - logprior: -1.4735e+00
Epoch 10/10
12/12 - 2s - loss: 182.3834 - loglik: -1.8058e+02 - logprior: -1.4344e+00
Fitted a model with MAP estimate = -182.0354
Time for alignment: 70.5827
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.9980 - loglik: -2.8274e+02 - logprior: -1.0244e+01
Epoch 2/10
12/12 - 2s - loss: 254.4553 - loglik: -2.5168e+02 - logprior: -2.7286e+00
Epoch 3/10
12/12 - 2s - loss: 225.7839 - loglik: -2.2345e+02 - logprior: -2.0239e+00
Epoch 4/10
12/12 - 2s - loss: 214.2856 - loglik: -2.1158e+02 - logprior: -2.1537e+00
Epoch 5/10
12/12 - 2s - loss: 209.7485 - loglik: -2.0681e+02 - logprior: -2.3145e+00
Epoch 6/10
12/12 - 2s - loss: 207.8666 - loglik: -2.0502e+02 - logprior: -2.2368e+00
Epoch 7/10
12/12 - 2s - loss: 207.3170 - loglik: -2.0460e+02 - logprior: -2.1684e+00
Epoch 8/10
12/12 - 2s - loss: 206.2188 - loglik: -2.0355e+02 - logprior: -2.1771e+00
Epoch 9/10
12/12 - 2s - loss: 204.3331 - loglik: -2.0164e+02 - logprior: -2.2084e+00
Epoch 10/10
12/12 - 2s - loss: 203.9048 - loglik: -2.0124e+02 - logprior: -2.2010e+00
Fitted a model with MAP estimate = -203.0669
expansions: [(8, 1), (9, 3), (10, 4), (11, 2), (20, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 224.6482 - loglik: -2.1254e+02 - logprior: -1.2070e+01
Epoch 2/2
12/12 - 2s - loss: 202.3381 - loglik: -1.9673e+02 - logprior: -5.3798e+00
Fitted a model with MAP estimate = -195.8817
expansions: [(0, 3)]
discards: [ 0 11 63 78 81]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 202.3178 - loglik: -1.9275e+02 - logprior: -9.5316e+00
Epoch 2/2
12/12 - 2s - loss: 190.8068 - loglik: -1.8773e+02 - logprior: -2.8606e+00
Fitted a model with MAP estimate = -188.0430
expansions: []
discards: [ 0  2 47]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 204.5220 - loglik: -1.9274e+02 - logprior: -1.1747e+01
Epoch 2/10
12/12 - 2s - loss: 194.2665 - loglik: -1.8964e+02 - logprior: -4.4236e+00
Epoch 3/10
12/12 - 2s - loss: 189.0380 - loglik: -1.8608e+02 - logprior: -2.4678e+00
Epoch 4/10
12/12 - 2s - loss: 185.6487 - loglik: -1.8306e+02 - logprior: -1.8181e+00
Epoch 5/10
12/12 - 2s - loss: 183.3804 - loglik: -1.8093e+02 - logprior: -1.6458e+00
Epoch 6/10
12/12 - 2s - loss: 181.9420 - loglik: -1.7966e+02 - logprior: -1.6353e+00
Epoch 7/10
12/12 - 2s - loss: 182.0275 - loglik: -1.7994e+02 - logprior: -1.5737e+00
Fitted a model with MAP estimate = -181.1325
Time for alignment: 62.9748
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 292.6752 - loglik: -2.8242e+02 - logprior: -1.0243e+01
Epoch 2/10
12/12 - 2s - loss: 255.1818 - loglik: -2.5240e+02 - logprior: -2.7318e+00
Epoch 3/10
12/12 - 2s - loss: 227.0392 - loglik: -2.2470e+02 - logprior: -2.0326e+00
Epoch 4/10
12/12 - 2s - loss: 214.5791 - loglik: -2.1193e+02 - logprior: -2.1772e+00
Epoch 5/10
12/12 - 2s - loss: 209.4162 - loglik: -2.0657e+02 - logprior: -2.3284e+00
Epoch 6/10
12/12 - 2s - loss: 208.1812 - loglik: -2.0536e+02 - logprior: -2.2420e+00
Epoch 7/10
12/12 - 2s - loss: 207.2930 - loglik: -2.0457e+02 - logprior: -2.1667e+00
Epoch 8/10
12/12 - 2s - loss: 205.3506 - loglik: -2.0266e+02 - logprior: -2.1781e+00
Epoch 9/10
12/12 - 2s - loss: 204.5170 - loglik: -2.0184e+02 - logprior: -2.1833e+00
Epoch 10/10
12/12 - 2s - loss: 204.1138 - loglik: -2.0146e+02 - logprior: -2.1880e+00
Fitted a model with MAP estimate = -203.5079
expansions: [(6, 3), (10, 5), (13, 1), (21, 1), (36, 3), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 222.9732 - loglik: -2.1088e+02 - logprior: -1.2066e+01
Epoch 2/2
12/12 - 2s - loss: 201.4590 - loglik: -1.9598e+02 - logprior: -5.2857e+00
Fitted a model with MAP estimate = -195.7158
expansions: [(0, 4)]
discards: [ 0 14 80]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 202.7215 - loglik: -1.9297e+02 - logprior: -9.7201e+00
Epoch 2/2
12/12 - 2s - loss: 191.4919 - loglik: -1.8830e+02 - logprior: -2.9992e+00
Fitted a model with MAP estimate = -188.0555
expansions: []
discards: [ 1  2  3 47 65]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 201.0717 - loglik: -1.9163e+02 - logprior: -9.4029e+00
Epoch 2/10
12/12 - 2s - loss: 191.3586 - loglik: -1.8830e+02 - logprior: -2.8595e+00
Epoch 3/10
12/12 - 2s - loss: 189.0528 - loglik: -1.8643e+02 - logprior: -2.1793e+00
Epoch 4/10
12/12 - 2s - loss: 185.2017 - loglik: -1.8261e+02 - logprior: -1.9342e+00
Epoch 5/10
12/12 - 2s - loss: 183.6807 - loglik: -1.8123e+02 - logprior: -1.7509e+00
Epoch 6/10
12/12 - 2s - loss: 183.0092 - loglik: -1.8064e+02 - logprior: -1.7215e+00
Epoch 7/10
12/12 - 2s - loss: 181.3665 - loglik: -1.7910e+02 - logprior: -1.6933e+00
Epoch 8/10
12/12 - 2s - loss: 181.3596 - loglik: -1.7918e+02 - logprior: -1.6820e+00
Epoch 9/10
12/12 - 2s - loss: 180.6290 - loglik: -1.7854e+02 - logprior: -1.6398e+00
Epoch 10/10
12/12 - 2s - loss: 181.1422 - loglik: -1.7912e+02 - logprior: -1.5971e+00
Fitted a model with MAP estimate = -179.7432
Time for alignment: 69.3454
Computed alignments with likelihoods: ['-182.0354', '-181.1325', '-179.7432']
Best model has likelihood: -179.7432
time for generating output: 0.2144
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.889076935156025
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a9c474cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac9d00af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fa2c0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b2f8c9310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb7bfb20>, <__main__.SimpleDirichletPrior object at 0x7f6aada552e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 330.5538 - loglik: -2.8917e+02 - logprior: -4.1370e+01
Epoch 2/10
10/10 - 1s - loss: 277.8925 - loglik: -2.6667e+02 - logprior: -1.1211e+01
Epoch 3/10
10/10 - 1s - loss: 252.2947 - loglik: -2.4696e+02 - logprior: -5.3296e+00
Epoch 4/10
10/10 - 1s - loss: 239.2971 - loglik: -2.3609e+02 - logprior: -3.1808e+00
Epoch 5/10
10/10 - 1s - loss: 233.1507 - loglik: -2.3070e+02 - logprior: -2.2919e+00
Epoch 6/10
10/10 - 1s - loss: 230.5649 - loglik: -2.2832e+02 - logprior: -1.8741e+00
Epoch 7/10
10/10 - 1s - loss: 228.6908 - loglik: -2.2658e+02 - logprior: -1.6974e+00
Epoch 8/10
10/10 - 1s - loss: 227.1709 - loglik: -2.2520e+02 - logprior: -1.5913e+00
Epoch 9/10
10/10 - 1s - loss: 226.4757 - loglik: -2.2466e+02 - logprior: -1.4600e+00
Epoch 10/10
10/10 - 1s - loss: 226.1166 - loglik: -2.2442e+02 - logprior: -1.3863e+00
Fitted a model with MAP estimate = -225.6114
expansions: [(0, 3), (8, 4), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 277.9691 - loglik: -2.2446e+02 - logprior: -5.3481e+01
Epoch 2/2
10/10 - 2s - loss: 233.1796 - loglik: -2.1606e+02 - logprior: -1.6957e+01
Fitted a model with MAP estimate = -224.6751
expansions: []
discards: [ 0  1  2 11 58 59 60 76]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 256.6191 - loglik: -2.1786e+02 - logprior: -3.8740e+01
Epoch 2/2
10/10 - 2s - loss: 226.1138 - loglik: -2.1546e+02 - logprior: -1.0534e+01
Fitted a model with MAP estimate = -221.6237
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 267.0610 - loglik: -2.1540e+02 - logprior: -5.1635e+01
Epoch 2/10
10/10 - 2s - loss: 230.6286 - loglik: -2.1499e+02 - logprior: -1.5529e+01
Epoch 3/10
10/10 - 2s - loss: 220.8058 - loglik: -2.1383e+02 - logprior: -6.7360e+00
Epoch 4/10
10/10 - 2s - loss: 217.4050 - loglik: -2.1371e+02 - logprior: -3.3869e+00
Epoch 5/10
10/10 - 2s - loss: 215.8491 - loglik: -2.1374e+02 - logprior: -1.7878e+00
Epoch 6/10
10/10 - 2s - loss: 214.3079 - loglik: -2.1302e+02 - logprior: -9.6522e-01
Epoch 7/10
10/10 - 2s - loss: 213.9898 - loglik: -2.1314e+02 - logprior: -5.1439e-01
Epoch 8/10
10/10 - 2s - loss: 213.3929 - loglik: -2.1279e+02 - logprior: -2.5711e-01
Epoch 9/10
10/10 - 2s - loss: 213.0467 - loglik: -2.1265e+02 - logprior: -6.0274e-02
Epoch 10/10
10/10 - 2s - loss: 212.9834 - loglik: -2.1275e+02 - logprior: 0.1111
Fitted a model with MAP estimate = -212.3418
Time for alignment: 56.5649
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 330.7043 - loglik: -2.8932e+02 - logprior: -4.1373e+01
Epoch 2/10
10/10 - 1s - loss: 277.7040 - loglik: -2.6649e+02 - logprior: -1.1209e+01
Epoch 3/10
10/10 - 1s - loss: 252.7831 - loglik: -2.4744e+02 - logprior: -5.3338e+00
Epoch 4/10
10/10 - 1s - loss: 239.2455 - loglik: -2.3589e+02 - logprior: -3.2995e+00
Epoch 5/10
10/10 - 1s - loss: 233.7451 - loglik: -2.3097e+02 - logprior: -2.5021e+00
Epoch 6/10
10/10 - 1s - loss: 229.7130 - loglik: -2.2725e+02 - logprior: -2.0595e+00
Epoch 7/10
10/10 - 1s - loss: 228.0323 - loglik: -2.2575e+02 - logprior: -1.9155e+00
Epoch 8/10
10/10 - 1s - loss: 227.0131 - loglik: -2.2484e+02 - logprior: -1.8287e+00
Epoch 9/10
10/10 - 1s - loss: 226.3184 - loglik: -2.2424e+02 - logprior: -1.7363e+00
Epoch 10/10
10/10 - 1s - loss: 225.8157 - loglik: -2.2380e+02 - logprior: -1.6468e+00
Fitted a model with MAP estimate = -225.3791
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (44, 10), (51, 1), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 277.7850 - loglik: -2.2426e+02 - logprior: -5.3505e+01
Epoch 2/2
10/10 - 2s - loss: 233.9221 - loglik: -2.1682e+02 - logprior: -1.6975e+01
Fitted a model with MAP estimate = -225.5326
expansions: []
discards: [ 0  1 75]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.8252 - loglik: -2.1683e+02 - logprior: -4.6970e+01
Epoch 2/2
10/10 - 2s - loss: 234.2215 - loglik: -2.1503e+02 - logprior: -1.9054e+01
Fitted a model with MAP estimate = -229.0711
expansions: [(0, 3)]
discards: [0 7]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 257.3615 - loglik: -2.1503e+02 - logprior: -4.2308e+01
Epoch 2/10
10/10 - 2s - loss: 225.0175 - loglik: -2.1316e+02 - logprior: -1.1731e+01
Epoch 3/10
10/10 - 2s - loss: 218.1812 - loglik: -2.1290e+02 - logprior: -5.0194e+00
Epoch 4/10
10/10 - 2s - loss: 215.2585 - loglik: -2.1233e+02 - logprior: -2.5801e+00
Epoch 5/10
10/10 - 2s - loss: 213.9603 - loglik: -2.1224e+02 - logprior: -1.3482e+00
Epoch 6/10
10/10 - 2s - loss: 212.9157 - loglik: -2.1187e+02 - logprior: -6.8324e-01
Epoch 7/10
10/10 - 2s - loss: 212.5006 - loglik: -2.1185e+02 - logprior: -3.0585e-01
Epoch 8/10
10/10 - 2s - loss: 212.1584 - loglik: -2.1177e+02 - logprior: -6.5867e-02
Epoch 9/10
10/10 - 2s - loss: 211.8208 - loglik: -2.1160e+02 - logprior: 0.1039
Epoch 10/10
10/10 - 2s - loss: 211.6975 - loglik: -2.1162e+02 - logprior: 0.2499
Fitted a model with MAP estimate = -211.1829
Time for alignment: 55.1924
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 330.4477 - loglik: -2.8906e+02 - logprior: -4.1375e+01
Epoch 2/10
10/10 - 1s - loss: 277.6989 - loglik: -2.6648e+02 - logprior: -1.1210e+01
Epoch 3/10
10/10 - 1s - loss: 252.2839 - loglik: -2.4693e+02 - logprior: -5.3471e+00
Epoch 4/10
10/10 - 1s - loss: 238.3027 - loglik: -2.3501e+02 - logprior: -3.2541e+00
Epoch 5/10
10/10 - 1s - loss: 233.6331 - loglik: -2.3111e+02 - logprior: -2.3162e+00
Epoch 6/10
10/10 - 1s - loss: 230.3255 - loglik: -2.2809e+02 - logprior: -1.8316e+00
Epoch 7/10
10/10 - 1s - loss: 228.2508 - loglik: -2.2624e+02 - logprior: -1.6064e+00
Epoch 8/10
10/10 - 1s - loss: 227.4043 - loglik: -2.2553e+02 - logprior: -1.4843e+00
Epoch 9/10
10/10 - 1s - loss: 226.9566 - loglik: -2.2520e+02 - logprior: -1.3802e+00
Epoch 10/10
10/10 - 1s - loss: 226.4505 - loglik: -2.2481e+02 - logprior: -1.2986e+00
Fitted a model with MAP estimate = -225.8591
expansions: [(0, 3), (8, 4), (37, 1), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 277.9224 - loglik: -2.2440e+02 - logprior: -5.3496e+01
Epoch 2/2
10/10 - 2s - loss: 234.1775 - loglik: -2.1719e+02 - logprior: -1.6825e+01
Fitted a model with MAP estimate = -225.4089
expansions: []
discards: [ 0  1  2 11 74]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.3035 - loglik: -2.1649e+02 - logprior: -3.8784e+01
Epoch 2/2
10/10 - 2s - loss: 225.6438 - loglik: -2.1504e+02 - logprior: -1.0469e+01
Fitted a model with MAP estimate = -220.4775
expansions: [(0, 3)]
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 266.7856 - loglik: -2.1519e+02 - logprior: -5.1573e+01
Epoch 2/10
10/10 - 2s - loss: 229.7787 - loglik: -2.1417e+02 - logprior: -1.5500e+01
Epoch 3/10
10/10 - 2s - loss: 220.2023 - loglik: -2.1321e+02 - logprior: -6.7591e+00
Epoch 4/10
10/10 - 2s - loss: 216.8758 - loglik: -2.1320e+02 - logprior: -3.3808e+00
Epoch 5/10
10/10 - 2s - loss: 214.7952 - loglik: -2.1269e+02 - logprior: -1.7846e+00
Epoch 6/10
10/10 - 2s - loss: 214.0407 - loglik: -2.1275e+02 - logprior: -9.5982e-01
Epoch 7/10
10/10 - 2s - loss: 213.5247 - loglik: -2.1270e+02 - logprior: -4.8947e-01
Epoch 8/10
10/10 - 2s - loss: 213.2215 - loglik: -2.1268e+02 - logprior: -1.9756e-01
Epoch 9/10
10/10 - 2s - loss: 212.5174 - loglik: -2.1217e+02 - logprior: -6.5345e-04
Epoch 10/10
10/10 - 2s - loss: 212.5291 - loglik: -2.1234e+02 - logprior: 0.1590
Fitted a model with MAP estimate = -212.0247
Time for alignment: 54.8621
Computed alignments with likelihoods: ['-212.3418', '-211.1829', '-212.0247']
Best model has likelihood: -211.1829
time for generating output: 0.2033
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7360022906525757
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6af493ff40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6af4b73880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83fddd60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac8d1d4f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69e844cd90>, <__main__.SimpleDirichletPrior object at 0x7f6aca2c74c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 555.6669 - loglik: -4.7338e+02 - logprior: -8.2282e+01
Epoch 2/10
10/10 - 3s - loss: 423.6917 - loglik: -4.0395e+02 - logprior: -1.9725e+01
Epoch 3/10
10/10 - 3s - loss: 346.4373 - loglik: -3.3651e+02 - logprior: -9.8378e+00
Epoch 4/10
10/10 - 3s - loss: 298.1937 - loglik: -2.9031e+02 - logprior: -7.8342e+00
Epoch 5/10
10/10 - 3s - loss: 280.6269 - loglik: -2.7336e+02 - logprior: -7.2468e+00
Epoch 6/10
10/10 - 3s - loss: 273.2231 - loglik: -2.6713e+02 - logprior: -6.0562e+00
Epoch 7/10
10/10 - 3s - loss: 269.8813 - loglik: -2.6443e+02 - logprior: -5.3397e+00
Epoch 8/10
10/10 - 3s - loss: 268.1339 - loglik: -2.6286e+02 - logprior: -4.9969e+00
Epoch 9/10
10/10 - 3s - loss: 266.6688 - loglik: -2.6163e+02 - logprior: -4.7019e+00
Epoch 10/10
10/10 - 3s - loss: 266.3987 - loglik: -2.6178e+02 - logprior: -4.3288e+00
Fitted a model with MAP estimate = -265.8253
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 3), (41, 2), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 351.8286 - loglik: -2.5771e+02 - logprior: -9.4095e+01
Epoch 2/2
10/10 - 3s - loss: 274.9595 - loglik: -2.3714e+02 - logprior: -3.7727e+01
Fitted a model with MAP estimate = -262.3129
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (89, 1)]
discards: [  0  43  45  46  49  56  96 122]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 305.3884 - loglik: -2.3034e+02 - logprior: -7.5021e+01
Epoch 2/2
10/10 - 3s - loss: 240.6419 - loglik: -2.2310e+02 - logprior: -1.7440e+01
Fitted a model with MAP estimate = -230.4441
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 314.5100 - loglik: -2.2408e+02 - logprior: -9.0410e+01
Epoch 2/10
10/10 - 3s - loss: 253.0736 - loglik: -2.2193e+02 - logprior: -3.1041e+01
Epoch 3/10
10/10 - 3s - loss: 230.8135 - loglik: -2.2072e+02 - logprior: -1.0010e+01
Epoch 4/10
10/10 - 3s - loss: 219.6156 - loglik: -2.1996e+02 - logprior: 0.3781
Epoch 5/10
10/10 - 3s - loss: 216.3659 - loglik: -2.2042e+02 - logprior: 4.0734
Epoch 6/10
10/10 - 3s - loss: 214.2513 - loglik: -2.2022e+02 - logprior: 6.0044
Epoch 7/10
10/10 - 3s - loss: 212.8170 - loglik: -2.1992e+02 - logprior: 7.1742
Epoch 8/10
10/10 - 3s - loss: 211.7117 - loglik: -2.1946e+02 - logprior: 7.9404
Epoch 9/10
10/10 - 3s - loss: 211.8522 - loglik: -2.2020e+02 - logprior: 8.6355
Fitted a model with MAP estimate = -210.4154
Time for alignment: 89.0610
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 555.5013 - loglik: -4.7321e+02 - logprior: -8.2281e+01
Epoch 2/10
10/10 - 3s - loss: 424.3312 - loglik: -4.0459e+02 - logprior: -1.9724e+01
Epoch 3/10
10/10 - 3s - loss: 345.9124 - loglik: -3.3596e+02 - logprior: -9.8604e+00
Epoch 4/10
10/10 - 3s - loss: 297.8631 - loglik: -2.8995e+02 - logprior: -7.8572e+00
Epoch 5/10
10/10 - 3s - loss: 280.4945 - loglik: -2.7329e+02 - logprior: -7.1811e+00
Epoch 6/10
10/10 - 3s - loss: 273.2790 - loglik: -2.6690e+02 - logprior: -6.3526e+00
Epoch 7/10
10/10 - 3s - loss: 269.7236 - loglik: -2.6419e+02 - logprior: -5.4500e+00
Epoch 8/10
10/10 - 3s - loss: 268.5220 - loglik: -2.6316e+02 - logprior: -5.1123e+00
Epoch 9/10
10/10 - 3s - loss: 266.3095 - loglik: -2.6105e+02 - logprior: -4.9004e+00
Epoch 10/10
10/10 - 3s - loss: 266.7063 - loglik: -2.6187e+02 - logprior: -4.5108e+00
Fitted a model with MAP estimate = -265.8431
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 350.7571 - loglik: -2.5664e+02 - logprior: -9.4098e+01
Epoch 2/2
10/10 - 3s - loss: 274.7585 - loglik: -2.3731e+02 - logprior: -3.7383e+01
Fitted a model with MAP estimate = -262.0722
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53  93 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 305.4298 - loglik: -2.3030e+02 - logprior: -7.5105e+01
Epoch 2/2
10/10 - 3s - loss: 240.4207 - loglik: -2.2288e+02 - logprior: -1.7439e+01
Fitted a model with MAP estimate = -230.2911
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 314.3603 - loglik: -2.2395e+02 - logprior: -9.0389e+01
Epoch 2/10
10/10 - 3s - loss: 252.4837 - loglik: -2.2144e+02 - logprior: -3.0942e+01
Epoch 3/10
10/10 - 3s - loss: 230.7840 - loglik: -2.2078e+02 - logprior: -9.9152e+00
Epoch 4/10
10/10 - 3s - loss: 220.8393 - loglik: -2.2120e+02 - logprior: 0.3996
Epoch 5/10
10/10 - 3s - loss: 215.1879 - loglik: -2.1924e+02 - logprior: 4.0758
Epoch 6/10
10/10 - 3s - loss: 215.0528 - loglik: -2.2103e+02 - logprior: 6.0168
Epoch 7/10
10/10 - 3s - loss: 213.3813 - loglik: -2.2049e+02 - logprior: 7.1723
Epoch 8/10
10/10 - 3s - loss: 211.3656 - loglik: -2.1910e+02 - logprior: 7.9034
Epoch 9/10
10/10 - 3s - loss: 211.4336 - loglik: -2.1973e+02 - logprior: 8.5725
Fitted a model with MAP estimate = -210.3916
Time for alignment: 88.8977
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 556.1075 - loglik: -4.7382e+02 - logprior: -8.2282e+01
Epoch 2/10
10/10 - 3s - loss: 422.9725 - loglik: -4.0323e+02 - logprior: -1.9724e+01
Epoch 3/10
10/10 - 3s - loss: 346.0062 - loglik: -3.3606e+02 - logprior: -9.8524e+00
Epoch 4/10
10/10 - 3s - loss: 298.7863 - loglik: -2.9093e+02 - logprior: -7.7993e+00
Epoch 5/10
10/10 - 3s - loss: 281.1385 - loglik: -2.7419e+02 - logprior: -6.9275e+00
Epoch 6/10
10/10 - 3s - loss: 274.0344 - loglik: -2.6806e+02 - logprior: -5.9380e+00
Epoch 7/10
10/10 - 3s - loss: 269.6582 - loglik: -2.6427e+02 - logprior: -5.2837e+00
Epoch 8/10
10/10 - 3s - loss: 267.6474 - loglik: -2.6255e+02 - logprior: -4.8261e+00
Epoch 9/10
10/10 - 3s - loss: 267.0269 - loglik: -2.6205e+02 - logprior: -4.6351e+00
Epoch 10/10
10/10 - 3s - loss: 266.5615 - loglik: -2.6197e+02 - logprior: -4.2808e+00
Fitted a model with MAP estimate = -265.6045
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 352.2329 - loglik: -2.5806e+02 - logprior: -9.4153e+01
Epoch 2/2
10/10 - 3s - loss: 275.5686 - loglik: -2.3776e+02 - logprior: -3.7719e+01
Fitted a model with MAP estimate = -263.4292
expansions: [(0, 3), (15, 4), (87, 1)]
discards: [  0  43  47  54  94 117 121]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 306.2554 - loglik: -2.3129e+02 - logprior: -7.4942e+01
Epoch 2/2
10/10 - 3s - loss: 242.3328 - loglik: -2.2497e+02 - logprior: -1.7266e+01
Fitted a model with MAP estimate = -231.6150
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 315.5679 - loglik: -2.2514e+02 - logprior: -9.0407e+01
Epoch 2/10
10/10 - 3s - loss: 252.1707 - loglik: -2.2080e+02 - logprior: -3.1270e+01
Epoch 3/10
10/10 - 3s - loss: 232.0051 - loglik: -2.2157e+02 - logprior: -1.0346e+01
Epoch 4/10
10/10 - 3s - loss: 220.5616 - loglik: -2.2082e+02 - logprior: 0.2934
Epoch 5/10
10/10 - 3s - loss: 217.1062 - loglik: -2.2111e+02 - logprior: 4.0234
Epoch 6/10
10/10 - 3s - loss: 212.9051 - loglik: -2.1882e+02 - logprior: 5.9548
Epoch 7/10
10/10 - 3s - loss: 214.1260 - loglik: -2.2114e+02 - logprior: 7.0932
Fitted a model with MAP estimate = -212.1994
Time for alignment: 83.0817
Computed alignments with likelihoods: ['-210.4154', '-210.3916', '-212.1994']
Best model has likelihood: -210.3916
time for generating output: 0.2302
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6abee74d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a836e19d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a836e1040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6afd7369a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6afd736eb0>, <__main__.SimpleDirichletPrior object at 0x7f69e9697d90>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 813.6812 - loglik: -8.1087e+02 - logprior: -2.4963e+00
Epoch 2/10
33/33 - 28s - loss: 716.4672 - loglik: -7.1405e+02 - logprior: -1.3756e+00
Epoch 3/10
33/33 - 28s - loss: 706.5596 - loglik: -7.0378e+02 - logprior: -1.3850e+00
Epoch 4/10
33/33 - 28s - loss: 701.6345 - loglik: -6.9890e+02 - logprior: -1.4051e+00
Epoch 5/10
33/33 - 28s - loss: 703.0430 - loglik: -7.0017e+02 - logprior: -1.4897e+00
Fitted a model with MAP estimate = -698.0328
expansions: [(0, 5), (5, 1), (8, 1), (9, 1), (33, 4), (60, 1), (63, 1), (71, 1), (72, 1), (78, 1), (111, 1), (113, 1), (116, 1), (118, 1), (132, 2), (155, 4), (163, 5), (204, 1), (221, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 39s - loss: 714.0040 - loglik: -7.1008e+02 - logprior: -3.7071e+00
Epoch 2/2
33/33 - 34s - loss: 697.4773 - loglik: -6.9515e+02 - logprior: -1.3954e+00
Fitted a model with MAP estimate = -692.3323
expansions: [(79, 1), (258, 1), (267, 4)]
discards: [  1  42 180 181 191 192 193 194 259 260 261 263 264 265 266]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 36s - loss: 708.1623 - loglik: -7.0524e+02 - logprior: -2.7008e+00
Epoch 2/2
33/33 - 33s - loss: 696.2784 - loglik: -6.9449e+02 - logprior: -1.0652e+00
Fitted a model with MAP estimate = -693.0663
expansions: [(258, 4)]
discards: [  4   5   6 254 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 36s - loss: 707.1494 - loglik: -7.0439e+02 - logprior: -2.5324e+00
Epoch 2/10
33/33 - 32s - loss: 696.8801 - loglik: -6.9535e+02 - logprior: -6.1431e-01
Epoch 3/10
33/33 - 33s - loss: 695.6787 - loglik: -6.9369e+02 - logprior: -6.1957e-01
Epoch 4/10
33/33 - 33s - loss: 689.8000 - loglik: -6.8775e+02 - logprior: -5.8167e-01
Epoch 5/10
33/33 - 33s - loss: 689.6288 - loglik: -6.8772e+02 - logprior: -5.0420e-01
Epoch 6/10
33/33 - 33s - loss: 690.4013 - loglik: -6.8864e+02 - logprior: -4.3391e-01
Fitted a model with MAP estimate = -687.0840
Time for alignment: 595.3603
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 813.7325 - loglik: -8.1094e+02 - logprior: -2.4788e+00
Epoch 2/10
33/33 - 28s - loss: 722.2914 - loglik: -7.2014e+02 - logprior: -1.2650e+00
Epoch 3/10
33/33 - 28s - loss: 707.7644 - loglik: -7.0505e+02 - logprior: -1.3351e+00
Epoch 4/10
33/33 - 28s - loss: 704.0778 - loglik: -7.0124e+02 - logprior: -1.4249e+00
Epoch 5/10
33/33 - 28s - loss: 702.7659 - loglik: -6.9990e+02 - logprior: -1.4915e+00
Epoch 6/10
33/33 - 28s - loss: 701.0443 - loglik: -6.9822e+02 - logprior: -1.5207e+00
Epoch 7/10
33/33 - 28s - loss: 699.8374 - loglik: -6.9708e+02 - logprior: -1.5262e+00
Epoch 8/10
33/33 - 28s - loss: 699.1635 - loglik: -6.9644e+02 - logprior: -1.5642e+00
Epoch 9/10
33/33 - 28s - loss: 698.5800 - loglik: -6.9591e+02 - logprior: -1.5754e+00
Epoch 10/10
33/33 - 28s - loss: 696.1131 - loglik: -6.9348e+02 - logprior: -1.5943e+00
Fitted a model with MAP estimate = -696.6726
expansions: [(0, 5), (1, 1), (5, 1), (7, 2), (33, 5), (67, 1), (73, 1), (74, 2), (78, 1), (84, 1), (111, 5), (114, 1), (117, 1), (119, 1), (133, 2), (156, 3), (157, 1), (163, 5), (171, 1), (204, 1), (221, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 37s - loss: 723.1722 - loglik: -7.1914e+02 - logprior: -3.8229e+00
Epoch 2/2
33/33 - 34s - loss: 697.2379 - loglik: -6.9499e+02 - logprior: -1.3920e+00
Fitted a model with MAP estimate = -691.1554
expansions: [(275, 4)]
discards: [  1   5   6   7   8   9  16  44  45 131 132 133 134 135 187 198 199 200
 201 202 267 271 272 273 274]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 35s - loss: 708.9036 - loglik: -7.0596e+02 - logprior: -2.7143e+00
Epoch 2/2
33/33 - 32s - loss: 697.8885 - loglik: -6.9621e+02 - logprior: -8.2267e-01
Fitted a model with MAP estimate = -694.0466
expansions: [(0, 6), (254, 4)]
discards: [  3 250 251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 37s - loss: 707.0391 - loglik: -7.0325e+02 - logprior: -3.5673e+00
Epoch 2/10
33/33 - 33s - loss: 698.1839 - loglik: -6.9651e+02 - logprior: -8.2915e-01
Epoch 3/10
33/33 - 33s - loss: 694.1749 - loglik: -6.9213e+02 - logprior: -7.8496e-01
Epoch 4/10
33/33 - 33s - loss: 692.0029 - loglik: -6.8981e+02 - logprior: -7.3160e-01
Epoch 5/10
33/33 - 33s - loss: 687.9611 - loglik: -6.8589e+02 - logprior: -6.6280e-01
Epoch 6/10
33/33 - 33s - loss: 686.0586 - loglik: -6.8412e+02 - logprior: -5.9738e-01
Epoch 7/10
33/33 - 33s - loss: 689.6298 - loglik: -6.8780e+02 - logprior: -5.1117e-01
Fitted a model with MAP estimate = -685.4811
Time for alignment: 768.6933
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 812.3008 - loglik: -8.0949e+02 - logprior: -2.5034e+00
Epoch 2/10
33/33 - 28s - loss: 721.5477 - loglik: -7.1906e+02 - logprior: -1.2205e+00
Epoch 3/10
33/33 - 28s - loss: 709.8334 - loglik: -7.0704e+02 - logprior: -1.2273e+00
Epoch 4/10
33/33 - 28s - loss: 704.0269 - loglik: -7.0111e+02 - logprior: -1.4305e+00
Epoch 5/10
33/33 - 28s - loss: 702.0909 - loglik: -6.9919e+02 - logprior: -1.4842e+00
Epoch 6/10
33/33 - 28s - loss: 698.3139 - loglik: -6.9548e+02 - logprior: -1.5043e+00
Epoch 7/10
33/33 - 28s - loss: 700.2639 - loglik: -6.9753e+02 - logprior: -1.4964e+00
Fitted a model with MAP estimate = -697.7873
expansions: [(0, 5), (9, 1), (10, 1), (34, 5), (61, 1), (62, 2), (73, 3), (83, 1), (107, 4), (110, 1), (113, 1), (115, 1), (135, 2), (143, 1), (155, 1), (163, 4), (184, 1), (206, 1), (223, 5), (230, 3)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 39s - loss: 716.9403 - loglik: -7.1298e+02 - logprior: -3.7503e+00
Epoch 2/2
33/33 - 35s - loss: 697.7396 - loglik: -6.9561e+02 - logprior: -1.2544e+00
Fitted a model with MAP estimate = -691.9673
expansions: []
discards: [  1   5  43  44  73 128 129 130 194 195 196 265 266 267 268 269 270]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 35s - loss: 706.7401 - loglik: -7.0388e+02 - logprior: -2.6331e+00
Epoch 2/2
33/33 - 32s - loss: 699.3105 - loglik: -6.9760e+02 - logprior: -9.0688e-01
Fitted a model with MAP estimate = -693.8709
expansions: [(0, 4), (250, 1), (254, 4)]
discards: [  4   5 123 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 38s - loss: 706.7448 - loglik: -7.0296e+02 - logprior: -3.5629e+00
Epoch 2/10
33/33 - 33s - loss: 697.2030 - loglik: -6.9555e+02 - logprior: -7.9918e-01
Epoch 3/10
33/33 - 33s - loss: 692.7514 - loglik: -6.9070e+02 - logprior: -7.9972e-01
Epoch 4/10
33/33 - 33s - loss: 695.1648 - loglik: -6.9297e+02 - logprior: -7.6415e-01
Fitted a model with MAP estimate = -688.7985
Time for alignment: 587.8757
Computed alignments with likelihoods: ['-687.0840', '-685.4811', '-688.7985']
Best model has likelihood: -685.4811
time for generating output: 0.3373
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7738325504282951
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ab65e4cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aca27e370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca27e3d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a9c0a19d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6af4b6e7f0>, <__main__.SimpleDirichletPrior object at 0x7f6ac94fdd60>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 412.8106 - loglik: -3.4852e+02 - logprior: -6.4283e+01
Epoch 2/10
10/10 - 2s - loss: 323.7503 - loglik: -3.0709e+02 - logprior: -1.6657e+01
Epoch 3/10
10/10 - 2s - loss: 274.9890 - loglik: -2.6639e+02 - logprior: -8.5387e+00
Epoch 4/10
10/10 - 2s - loss: 250.7410 - loglik: -2.4429e+02 - logprior: -6.2941e+00
Epoch 5/10
10/10 - 2s - loss: 239.6110 - loglik: -2.3380e+02 - logprior: -5.6336e+00
Epoch 6/10
10/10 - 2s - loss: 235.3717 - loglik: -2.3016e+02 - logprior: -4.9079e+00
Epoch 7/10
10/10 - 2s - loss: 233.5799 - loglik: -2.2890e+02 - logprior: -4.3133e+00
Epoch 8/10
10/10 - 2s - loss: 232.3942 - loglik: -2.2809e+02 - logprior: -3.9574e+00
Epoch 9/10
10/10 - 2s - loss: 231.6457 - loglik: -2.2767e+02 - logprior: -3.6802e+00
Epoch 10/10
10/10 - 2s - loss: 231.0052 - loglik: -2.2726e+02 - logprior: -3.4726e+00
Fitted a model with MAP estimate = -230.7166
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.3871 - loglik: -2.3071e+02 - logprior: -5.8650e+01
Epoch 2/2
10/10 - 2s - loss: 225.9262 - loglik: -2.1087e+02 - logprior: -1.4891e+01
Fitted a model with MAP estimate = -214.2338
expansions: []
discards: [  0 109]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 283.4745 - loglik: -2.1211e+02 - logprior: -7.1335e+01
Epoch 2/2
10/10 - 2s - loss: 236.3307 - loglik: -2.0711e+02 - logprior: -2.9071e+01
Fitted a model with MAP estimate = -228.0400
expansions: [(0, 2)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 265.9412 - loglik: -2.0895e+02 - logprior: -5.6965e+01
Epoch 2/10
10/10 - 2s - loss: 219.1291 - loglik: -2.0505e+02 - logprior: -1.3947e+01
Epoch 3/10
10/10 - 2s - loss: 207.8261 - loglik: -2.0225e+02 - logprior: -5.2548e+00
Epoch 4/10
10/10 - 2s - loss: 202.6572 - loglik: -2.0026e+02 - logprior: -1.8653e+00
Epoch 5/10
10/10 - 2s - loss: 198.7370 - loglik: -1.9829e+02 - logprior: 0.1227
Epoch 6/10
10/10 - 2s - loss: 197.6866 - loglik: -1.9860e+02 - logprior: 1.3436
Epoch 7/10
10/10 - 2s - loss: 197.0060 - loglik: -1.9876e+02 - logprior: 2.0929
Epoch 8/10
10/10 - 2s - loss: 196.1923 - loglik: -1.9850e+02 - logprior: 2.6127
Epoch 9/10
10/10 - 2s - loss: 195.8009 - loglik: -1.9854e+02 - logprior: 3.0195
Epoch 10/10
10/10 - 2s - loss: 195.0542 - loglik: -1.9820e+02 - logprior: 3.4019
Fitted a model with MAP estimate = -194.7860
Time for alignment: 64.7945
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 413.0991 - loglik: -3.4881e+02 - logprior: -6.4281e+01
Epoch 2/10
10/10 - 2s - loss: 322.9487 - loglik: -3.0628e+02 - logprior: -1.6662e+01
Epoch 3/10
10/10 - 2s - loss: 275.7590 - loglik: -2.6713e+02 - logprior: -8.5607e+00
Epoch 4/10
10/10 - 2s - loss: 250.2177 - loglik: -2.4376e+02 - logprior: -6.2894e+00
Epoch 5/10
10/10 - 2s - loss: 239.5793 - loglik: -2.3387e+02 - logprior: -5.5149e+00
Epoch 6/10
10/10 - 2s - loss: 235.4796 - loglik: -2.3032e+02 - logprior: -4.8384e+00
Epoch 7/10
10/10 - 2s - loss: 233.7053 - loglik: -2.2925e+02 - logprior: -4.1144e+00
Epoch 8/10
10/10 - 2s - loss: 232.5719 - loglik: -2.2849e+02 - logprior: -3.7823e+00
Epoch 9/10
10/10 - 2s - loss: 231.3034 - loglik: -2.2742e+02 - logprior: -3.5723e+00
Epoch 10/10
10/10 - 2s - loss: 230.7365 - loglik: -2.2708e+02 - logprior: -3.3393e+00
Fitted a model with MAP estimate = -230.5788
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 3), (70, 1), (78, 1), (79, 1), (85, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 307.1656 - loglik: -2.3443e+02 - logprior: -7.2708e+01
Epoch 2/2
10/10 - 2s - loss: 242.9465 - loglik: -2.1351e+02 - logprior: -2.9282e+01
Fitted a model with MAP estimate = -231.3827
expansions: [(0, 3)]
discards: [  0  78 109]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 268.5868 - loglik: -2.1123e+02 - logprior: -5.7330e+01
Epoch 2/2
10/10 - 2s - loss: 219.1479 - loglik: -2.0433e+02 - logprior: -1.4654e+01
Fitted a model with MAP estimate = -210.8395
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 279.0862 - loglik: -2.1068e+02 - logprior: -6.8381e+01
Epoch 2/10
10/10 - 2s - loss: 226.7504 - loglik: -2.0660e+02 - logprior: -2.0020e+01
Epoch 3/10
10/10 - 2s - loss: 210.1133 - loglik: -2.0311e+02 - logprior: -6.6655e+00
Epoch 4/10
10/10 - 2s - loss: 203.8661 - loglik: -2.0109e+02 - logprior: -2.2787e+00
Epoch 5/10
10/10 - 2s - loss: 201.1550 - loglik: -2.0047e+02 - logprior: -1.5991e-01
Epoch 6/10
10/10 - 2s - loss: 198.6205 - loglik: -1.9922e+02 - logprior: 1.0185
Epoch 7/10
10/10 - 2s - loss: 197.5667 - loglik: -1.9897e+02 - logprior: 1.7393
Epoch 8/10
10/10 - 2s - loss: 197.4406 - loglik: -1.9946e+02 - logprior: 2.3200
Epoch 9/10
10/10 - 2s - loss: 197.0459 - loglik: -1.9959e+02 - logprior: 2.8192
Epoch 10/10
10/10 - 2s - loss: 196.4349 - loglik: -1.9946e+02 - logprior: 3.2791
Fitted a model with MAP estimate = -195.8774
Time for alignment: 65.1260
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 413.1383 - loglik: -3.4884e+02 - logprior: -6.4283e+01
Epoch 2/10
10/10 - 2s - loss: 323.2099 - loglik: -3.0654e+02 - logprior: -1.6660e+01
Epoch 3/10
10/10 - 2s - loss: 275.5230 - loglik: -2.6692e+02 - logprior: -8.5394e+00
Epoch 4/10
10/10 - 2s - loss: 251.5817 - loglik: -2.4525e+02 - logprior: -6.1720e+00
Epoch 5/10
10/10 - 2s - loss: 241.6093 - loglik: -2.3633e+02 - logprior: -5.0552e+00
Epoch 6/10
10/10 - 2s - loss: 236.3109 - loglik: -2.3160e+02 - logprior: -4.3043e+00
Epoch 7/10
10/10 - 2s - loss: 234.3784 - loglik: -2.3013e+02 - logprior: -3.8048e+00
Epoch 8/10
10/10 - 2s - loss: 233.2281 - loglik: -2.2937e+02 - logprior: -3.4965e+00
Epoch 9/10
10/10 - 2s - loss: 232.4530 - loglik: -2.2890e+02 - logprior: -3.2516e+00
Epoch 10/10
10/10 - 2s - loss: 231.9267 - loglik: -2.2863e+02 - logprior: -3.0106e+00
Fitted a model with MAP estimate = -231.3909
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 305.6769 - loglik: -2.3305e+02 - logprior: -7.2596e+01
Epoch 2/2
10/10 - 2s - loss: 243.2496 - loglik: -2.1396e+02 - logprior: -2.9125e+01
Fitted a model with MAP estimate = -230.9555
expansions: [(0, 3)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 268.1963 - loglik: -2.1087e+02 - logprior: -5.7297e+01
Epoch 2/2
10/10 - 2s - loss: 219.7165 - loglik: -2.0491e+02 - logprior: -1.4639e+01
Fitted a model with MAP estimate = -210.5840
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.6692 - loglik: -2.1040e+02 - logprior: -6.8250e+01
Epoch 2/10
10/10 - 2s - loss: 226.5268 - loglik: -2.0657e+02 - logprior: -1.9825e+01
Epoch 3/10
10/10 - 2s - loss: 210.2061 - loglik: -2.0330e+02 - logprior: -6.5923e+00
Epoch 4/10
10/10 - 2s - loss: 203.5401 - loglik: -2.0084e+02 - logprior: -2.2546e+00
Epoch 5/10
10/10 - 2s - loss: 200.5524 - loglik: -1.9992e+02 - logprior: -1.4492e-01
Epoch 6/10
10/10 - 2s - loss: 198.6986 - loglik: -1.9928e+02 - logprior: 1.0094
Epoch 7/10
10/10 - 2s - loss: 197.4211 - loglik: -1.9882e+02 - logprior: 1.7391
Epoch 8/10
10/10 - 2s - loss: 197.7624 - loglik: -1.9980e+02 - logprior: 2.3195
Fitted a model with MAP estimate = -196.5441
Time for alignment: 58.8228
Computed alignments with likelihoods: ['-194.7860', '-195.8774', '-196.5441']
Best model has likelihood: -194.7860
time for generating output: 0.1710
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9273021001615509
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b1f196640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e8dae9d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e8daefd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b16df9250>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6aad6a36d0>, <__main__.SimpleDirichletPrior object at 0x7f69a46cf970>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.4880 - loglik: -1.9318e+02 - logprior: -2.2628e+00
Epoch 2/10
22/22 - 2s - loss: 162.4530 - loglik: -1.6072e+02 - logprior: -1.4026e+00
Epoch 3/10
22/22 - 2s - loss: 154.5811 - loglik: -1.5257e+02 - logprior: -1.5535e+00
Epoch 4/10
22/22 - 2s - loss: 153.3012 - loglik: -1.5145e+02 - logprior: -1.4519e+00
Epoch 5/10
22/22 - 2s - loss: 152.5140 - loglik: -1.5069e+02 - logprior: -1.4459e+00
Epoch 6/10
22/22 - 2s - loss: 152.3789 - loglik: -1.5062e+02 - logprior: -1.4158e+00
Epoch 7/10
22/22 - 2s - loss: 152.1352 - loglik: -1.5041e+02 - logprior: -1.4012e+00
Epoch 8/10
22/22 - 2s - loss: 151.8353 - loglik: -1.5011e+02 - logprior: -1.3947e+00
Epoch 9/10
22/22 - 2s - loss: 151.5787 - loglik: -1.4987e+02 - logprior: -1.3888e+00
Epoch 10/10
22/22 - 2s - loss: 151.7290 - loglik: -1.5003e+02 - logprior: -1.3865e+00
Fitted a model with MAP estimate = -153.4868
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (20, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 159.3373 - loglik: -1.5620e+02 - logprior: -3.0343e+00
Epoch 2/2
22/22 - 2s - loss: 147.9767 - loglik: -1.4593e+02 - logprior: -1.6782e+00
Fitted a model with MAP estimate = -145.3992
expansions: [(0, 2)]
discards: [ 0  9 18 25 31 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.5783 - loglik: -1.4529e+02 - logprior: -2.2106e+00
Epoch 2/2
22/22 - 2s - loss: 143.9441 - loglik: -1.4259e+02 - logprior: -1.1240e+00
Fitted a model with MAP estimate = -143.5379
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 7s - loss: 143.5444 - loglik: -1.4233e+02 - logprior: -1.1343e+00
Epoch 2/10
32/32 - 3s - loss: 141.9390 - loglik: -1.4086e+02 - logprior: -8.6709e-01
Epoch 3/10
32/32 - 3s - loss: 141.0135 - loglik: -1.3978e+02 - logprior: -8.4523e-01
Epoch 4/10
32/32 - 3s - loss: 140.2020 - loglik: -1.3887e+02 - logprior: -8.2981e-01
Epoch 5/10
32/32 - 3s - loss: 140.0440 - loglik: -1.3872e+02 - logprior: -8.2499e-01
Epoch 6/10
32/32 - 3s - loss: 139.8628 - loglik: -1.3855e+02 - logprior: -8.1880e-01
Epoch 7/10
32/32 - 3s - loss: 139.1813 - loglik: -1.3782e+02 - logprior: -8.1344e-01
Epoch 8/10
32/32 - 3s - loss: 139.2215 - loglik: -1.3791e+02 - logprior: -8.0851e-01
Fitted a model with MAP estimate = -138.3177
Time for alignment: 89.1443
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.7155 - loglik: -1.9341e+02 - logprior: -2.2629e+00
Epoch 2/10
22/22 - 2s - loss: 163.8768 - loglik: -1.6205e+02 - logprior: -1.3953e+00
Epoch 3/10
22/22 - 2s - loss: 155.3359 - loglik: -1.5328e+02 - logprior: -1.4999e+00
Epoch 4/10
22/22 - 2s - loss: 153.7486 - loglik: -1.5189e+02 - logprior: -1.4100e+00
Epoch 5/10
22/22 - 2s - loss: 152.7463 - loglik: -1.5092e+02 - logprior: -1.4521e+00
Epoch 6/10
22/22 - 2s - loss: 152.3685 - loglik: -1.5059e+02 - logprior: -1.4320e+00
Epoch 7/10
22/22 - 2s - loss: 152.0220 - loglik: -1.5025e+02 - logprior: -1.4134e+00
Epoch 8/10
22/22 - 2s - loss: 151.7077 - loglik: -1.4995e+02 - logprior: -1.4050e+00
Epoch 9/10
22/22 - 2s - loss: 151.4319 - loglik: -1.4967e+02 - logprior: -1.3999e+00
Epoch 10/10
22/22 - 2s - loss: 151.4361 - loglik: -1.4969e+02 - logprior: -1.3972e+00
Fitted a model with MAP estimate = -153.9431
expansions: [(8, 1), (9, 2), (12, 2), (13, 2), (20, 2), (21, 2), (22, 1), (25, 1), (27, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 160.2902 - loglik: -1.5715e+02 - logprior: -3.0417e+00
Epoch 2/2
22/22 - 2s - loss: 148.0110 - loglik: -1.4597e+02 - logprior: -1.6913e+00
Fitted a model with MAP estimate = -145.4439
expansions: [(0, 2)]
discards: [ 0  9 14 18 27 30 69]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.8095 - loglik: -1.4552e+02 - logprior: -2.2139e+00
Epoch 2/2
22/22 - 2s - loss: 144.1297 - loglik: -1.4281e+02 - logprior: -1.1219e+00
Fitted a model with MAP estimate = -143.6151
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 7s - loss: 145.1228 - loglik: -1.4378e+02 - logprior: -1.2542e+00
Epoch 2/10
32/32 - 3s - loss: 142.4305 - loglik: -1.4122e+02 - logprior: -9.6858e-01
Epoch 3/10
32/32 - 3s - loss: 141.1975 - loglik: -1.3987e+02 - logprior: -9.5254e-01
Epoch 4/10
32/32 - 3s - loss: 140.9160 - loglik: -1.3947e+02 - logprior: -9.5267e-01
Epoch 5/10
32/32 - 3s - loss: 140.4620 - loglik: -1.3902e+02 - logprior: -9.4606e-01
Epoch 6/10
32/32 - 3s - loss: 139.9552 - loglik: -1.3853e+02 - logprior: -9.4482e-01
Epoch 7/10
32/32 - 3s - loss: 139.9803 - loglik: -1.3850e+02 - logprior: -9.3758e-01
Fitted a model with MAP estimate = -139.0335
Time for alignment: 85.2422
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.4363 - loglik: -1.9313e+02 - logprior: -2.2646e+00
Epoch 2/10
22/22 - 2s - loss: 162.6655 - loglik: -1.6099e+02 - logprior: -1.4169e+00
Epoch 3/10
22/22 - 2s - loss: 155.0934 - loglik: -1.5308e+02 - logprior: -1.5633e+00
Epoch 4/10
22/22 - 2s - loss: 153.1716 - loglik: -1.5128e+02 - logprior: -1.4752e+00
Epoch 5/10
22/22 - 2s - loss: 153.1444 - loglik: -1.5130e+02 - logprior: -1.4627e+00
Epoch 6/10
22/22 - 2s - loss: 152.4377 - loglik: -1.5065e+02 - logprior: -1.4313e+00
Epoch 7/10
22/22 - 2s - loss: 152.2022 - loglik: -1.5044e+02 - logprior: -1.4202e+00
Epoch 8/10
22/22 - 2s - loss: 152.2119 - loglik: -1.5046e+02 - logprior: -1.4118e+00
Fitted a model with MAP estimate = -152.7914
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 157.8705 - loglik: -1.5476e+02 - logprior: -3.0160e+00
Epoch 2/2
22/22 - 2s - loss: 147.9774 - loglik: -1.4603e+02 - logprior: -1.6153e+00
Fitted a model with MAP estimate = -145.0754
expansions: [(0, 2)]
discards: [ 0  9 18 25 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.3224 - loglik: -1.4504e+02 - logprior: -2.2056e+00
Epoch 2/2
22/22 - 2s - loss: 144.1477 - loglik: -1.4274e+02 - logprior: -1.1298e+00
Fitted a model with MAP estimate = -143.3921
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1742 - loglik: -1.4383e+02 - logprior: -1.2638e+00
Epoch 2/10
32/32 - 3s - loss: 142.3371 - loglik: -1.4117e+02 - logprior: -9.6800e-01
Epoch 3/10
32/32 - 3s - loss: 141.2402 - loglik: -1.3991e+02 - logprior: -9.5732e-01
Epoch 4/10
32/32 - 3s - loss: 140.9921 - loglik: -1.3955e+02 - logprior: -9.5586e-01
Epoch 5/10
32/32 - 3s - loss: 140.3247 - loglik: -1.3889e+02 - logprior: -9.4663e-01
Epoch 6/10
32/32 - 3s - loss: 140.1065 - loglik: -1.3867e+02 - logprior: -9.4804e-01
Epoch 7/10
32/32 - 3s - loss: 139.9103 - loglik: -1.3843e+02 - logprior: -9.4484e-01
Epoch 8/10
32/32 - 3s - loss: 139.5576 - loglik: -1.3813e+02 - logprior: -9.3959e-01
Epoch 9/10
32/32 - 3s - loss: 139.3636 - loglik: -1.3792e+02 - logprior: -9.3070e-01
Epoch 10/10
32/32 - 3s - loss: 139.2977 - loglik: -1.3786e+02 - logprior: -9.2862e-01
Fitted a model with MAP estimate = -138.3583
Time for alignment: 88.0334
Computed alignments with likelihoods: ['-138.3177', '-139.0335', '-138.3583']
Best model has likelihood: -138.3177
time for generating output: 0.1507
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8186532128091935
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f697a5eb4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b2f95b760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f697a657df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f697a657eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f697a857850>, <__main__.SimpleDirichletPrior object at 0x7f6aca16d550>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 462.4924 - loglik: -4.4828e+02 - logprior: -1.4204e+01
Epoch 2/10
17/17 - 6s - loss: 316.5784 - loglik: -3.1346e+02 - logprior: -3.0276e+00
Epoch 3/10
17/17 - 5s - loss: 265.0022 - loglik: -2.6228e+02 - logprior: -2.7024e+00
Epoch 4/10
17/17 - 5s - loss: 250.8246 - loglik: -2.4786e+02 - logprior: -2.8356e+00
Epoch 5/10
17/17 - 6s - loss: 248.4665 - loglik: -2.4519e+02 - logprior: -2.9273e+00
Epoch 6/10
17/17 - 5s - loss: 243.7971 - loglik: -2.4044e+02 - logprior: -2.9382e+00
Epoch 7/10
17/17 - 5s - loss: 244.0626 - loglik: -2.4068e+02 - logprior: -2.9926e+00
Fitted a model with MAP estimate = -243.6017
expansions: [(25, 1), (51, 1), (52, 1), (73, 1), (83, 1), (91, 1), (117, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 270.9841 - loglik: -2.5192e+02 - logprior: -1.9008e+01
Epoch 2/2
17/17 - 6s - loss: 248.2014 - loglik: -2.4054e+02 - logprior: -7.4374e+00
Fitted a model with MAP estimate = -244.7830
expansions: [(0, 18)]
discards: [ 0 48]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 257.1677 - loglik: -2.4216e+02 - logprior: -1.4968e+01
Epoch 2/2
17/17 - 6s - loss: 237.1029 - loglik: -2.3355e+02 - logprior: -3.3547e+00
Fitted a model with MAP estimate = -232.6850
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 260.6817 - loglik: -2.4276e+02 - logprior: -1.7882e+01
Epoch 2/10
17/17 - 6s - loss: 244.5114 - loglik: -2.4031e+02 - logprior: -4.0176e+00
Epoch 3/10
17/17 - 6s - loss: 238.2089 - loglik: -2.3691e+02 - logprior: -1.0142e+00
Epoch 4/10
17/17 - 5s - loss: 236.0242 - loglik: -2.3500e+02 - logprior: -7.0044e-01
Epoch 5/10
17/17 - 6s - loss: 234.1169 - loglik: -2.3315e+02 - logprior: -6.2630e-01
Epoch 6/10
17/17 - 5s - loss: 232.5438 - loglik: -2.3174e+02 - logprior: -4.5779e-01
Epoch 7/10
17/17 - 6s - loss: 233.0733 - loglik: -2.3238e+02 - logprior: -3.4041e-01
Fitted a model with MAP estimate = -231.7557
Time for alignment: 135.5913
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 466.5885 - loglik: -4.5240e+02 - logprior: -1.4184e+01
Epoch 2/10
17/17 - 5s - loss: 322.5294 - loglik: -3.2001e+02 - logprior: -2.4500e+00
Epoch 3/10
17/17 - 5s - loss: 271.0327 - loglik: -2.6908e+02 - logprior: -1.9245e+00
Epoch 4/10
17/17 - 5s - loss: 259.8039 - loglik: -2.5784e+02 - logprior: -1.8445e+00
Epoch 5/10
17/17 - 6s - loss: 255.3727 - loglik: -2.5310e+02 - logprior: -1.9865e+00
Epoch 6/10
17/17 - 5s - loss: 254.7382 - loglik: -2.5237e+02 - logprior: -2.0055e+00
Epoch 7/10
17/17 - 6s - loss: 252.2104 - loglik: -2.4980e+02 - logprior: -2.0354e+00
Epoch 8/10
17/17 - 5s - loss: 251.4350 - loglik: -2.4900e+02 - logprior: -2.0521e+00
Epoch 9/10
17/17 - 6s - loss: 251.8700 - loglik: -2.4945e+02 - logprior: -2.0392e+00
Fitted a model with MAP estimate = -250.9937
expansions: [(0, 31), (8, 2), (25, 2), (26, 1), (27, 1), (50, 1), (59, 1), (80, 1), (91, 1), (108, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 266.9861 - loglik: -2.4842e+02 - logprior: -1.8522e+01
Epoch 2/2
17/17 - 7s - loss: 222.6401 - loglik: -2.1869e+02 - logprior: -3.7418e+00
Fitted a model with MAP estimate = -214.1163
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 256.8741 - loglik: -2.4173e+02 - logprior: -1.5101e+01
Epoch 2/2
17/17 - 6s - loss: 229.6457 - loglik: -2.2739e+02 - logprior: -2.0517e+00
Fitted a model with MAP estimate = -223.3387
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 251.8812 - loglik: -2.3839e+02 - logprior: -1.3450e+01
Epoch 2/10
17/17 - 7s - loss: 224.3761 - loglik: -2.2247e+02 - logprior: -1.7074e+00
Epoch 3/10
17/17 - 6s - loss: 211.2884 - loglik: -2.1033e+02 - logprior: -6.3592e-01
Epoch 4/10
17/17 - 6s - loss: 208.8434 - loglik: -2.0829e+02 - logprior: -1.7987e-01
Epoch 5/10
17/17 - 7s - loss: 208.5290 - loglik: -2.0813e+02 - logprior: 0.0106
Epoch 6/10
17/17 - 7s - loss: 205.3078 - loglik: -2.0505e+02 - logprior: 0.1798
Epoch 7/10
17/17 - 7s - loss: 204.7560 - loglik: -2.0469e+02 - logprior: 0.3595
Epoch 8/10
17/17 - 7s - loss: 204.2708 - loglik: -2.0440e+02 - logprior: 0.5517
Epoch 9/10
17/17 - 7s - loss: 204.5238 - loglik: -2.0488e+02 - logprior: 0.7723
Fitted a model with MAP estimate = -203.3540
Time for alignment: 169.2639
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 464.9031 - loglik: -4.5074e+02 - logprior: -1.4157e+01
Epoch 2/10
17/17 - 6s - loss: 320.1925 - loglik: -3.1775e+02 - logprior: -2.3795e+00
Epoch 3/10
17/17 - 6s - loss: 271.0032 - loglik: -2.6916e+02 - logprior: -1.8149e+00
Epoch 4/10
17/17 - 6s - loss: 258.0636 - loglik: -2.5611e+02 - logprior: -1.7310e+00
Epoch 5/10
17/17 - 5s - loss: 254.8769 - loglik: -2.5261e+02 - logprior: -1.8105e+00
Epoch 6/10
17/17 - 5s - loss: 251.2698 - loglik: -2.4899e+02 - logprior: -1.8063e+00
Epoch 7/10
17/17 - 6s - loss: 250.2914 - loglik: -2.4799e+02 - logprior: -1.8501e+00
Epoch 8/10
17/17 - 5s - loss: 250.4227 - loglik: -2.4815e+02 - logprior: -1.8492e+00
Fitted a model with MAP estimate = -249.9028
expansions: [(0, 33), (24, 2), (25, 1), (27, 1), (49, 1), (80, 1), (87, 1), (96, 1), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 268.0146 - loglik: -2.4950e+02 - logprior: -1.8463e+01
Epoch 2/2
17/17 - 7s - loss: 223.5996 - loglik: -2.1963e+02 - logprior: -3.7592e+00
Fitted a model with MAP estimate = -215.0247
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 255.4377 - loglik: -2.4030e+02 - logprior: -1.5085e+01
Epoch 2/2
17/17 - 6s - loss: 227.9859 - loglik: -2.2564e+02 - logprior: -2.1141e+00
Fitted a model with MAP estimate = -221.5452
expansions: [(0, 25)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 251.0035 - loglik: -2.3744e+02 - logprior: -1.3522e+01
Epoch 2/10
17/17 - 7s - loss: 223.9888 - loglik: -2.2210e+02 - logprior: -1.6871e+00
Epoch 3/10
17/17 - 7s - loss: 214.3839 - loglik: -2.1360e+02 - logprior: -4.3991e-01
Epoch 4/10
17/17 - 7s - loss: 209.8197 - loglik: -2.0946e+02 - logprior: 0.0247
Epoch 5/10
17/17 - 7s - loss: 207.6571 - loglik: -2.0740e+02 - logprior: 0.1474
Epoch 6/10
17/17 - 6s - loss: 209.5077 - loglik: -2.0942e+02 - logprior: 0.3331
Fitted a model with MAP estimate = -206.5489
Time for alignment: 141.3162
Computed alignments with likelihoods: ['-231.7557', '-203.3540', '-206.5489']
Best model has likelihood: -203.3540
time for generating output: 0.3732
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.530077551875917
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac860d1f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a836e78b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a836e77f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e84d6f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ad2ee2ee0>, <__main__.SimpleDirichletPrior object at 0x7f6b16ce0c40>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 246.5368 - loglik: -2.2579e+02 - logprior: -2.0724e+01
Epoch 2/10
10/10 - 1s - loss: 216.3075 - loglik: -2.1025e+02 - logprior: -5.8763e+00
Epoch 3/10
10/10 - 1s - loss: 198.5448 - loglik: -1.9494e+02 - logprior: -3.3211e+00
Epoch 4/10
10/10 - 1s - loss: 190.6107 - loglik: -1.8750e+02 - logprior: -2.7319e+00
Epoch 5/10
10/10 - 1s - loss: 187.0948 - loglik: -1.8414e+02 - logprior: -2.5762e+00
Epoch 6/10
10/10 - 1s - loss: 185.5791 - loglik: -1.8288e+02 - logprior: -2.4201e+00
Epoch 7/10
10/10 - 1s - loss: 184.4930 - loglik: -1.8212e+02 - logprior: -2.1425e+00
Epoch 8/10
10/10 - 1s - loss: 183.9983 - loglik: -1.8174e+02 - logprior: -2.0266e+00
Epoch 9/10
10/10 - 1s - loss: 183.5435 - loglik: -1.8126e+02 - logprior: -2.0523e+00
Epoch 10/10
10/10 - 1s - loss: 183.2344 - loglik: -1.8093e+02 - logprior: -2.0583e+00
Fitted a model with MAP estimate = -182.8395
expansions: [(0, 2), (7, 2), (8, 2), (22, 1), (41, 2), (42, 3), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 211.4981 - loglik: -1.8406e+02 - logprior: -2.7411e+01
Epoch 2/2
10/10 - 1s - loss: 187.9906 - loglik: -1.7899e+02 - logprior: -8.8607e+00
Fitted a model with MAP estimate = -183.5236
expansions: []
discards: [ 0 10 12 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.8901 - loglik: -1.8006e+02 - logprior: -2.3803e+01
Epoch 2/2
10/10 - 1s - loss: 188.4714 - loglik: -1.7866e+02 - logprior: -9.6740e+00
Fitted a model with MAP estimate = -185.4615
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 199.1663 - loglik: -1.7826e+02 - logprior: -2.0876e+01
Epoch 2/10
10/10 - 1s - loss: 182.9095 - loglik: -1.7681e+02 - logprior: -5.9648e+00
Epoch 3/10
10/10 - 1s - loss: 180.0993 - loglik: -1.7692e+02 - logprior: -2.9359e+00
Epoch 4/10
10/10 - 1s - loss: 178.1501 - loglik: -1.7586e+02 - logprior: -1.9722e+00
Epoch 5/10
10/10 - 1s - loss: 177.6662 - loglik: -1.7575e+02 - logprior: -1.6102e+00
Epoch 6/10
10/10 - 1s - loss: 176.9242 - loglik: -1.7518e+02 - logprior: -1.4724e+00
Epoch 7/10
10/10 - 1s - loss: 176.5686 - loglik: -1.7500e+02 - logprior: -1.3291e+00
Epoch 8/10
10/10 - 1s - loss: 176.2837 - loglik: -1.7492e+02 - logprior: -1.1305e+00
Epoch 9/10
10/10 - 1s - loss: 175.9991 - loglik: -1.7477e+02 - logprior: -9.9386e-01
Epoch 10/10
10/10 - 1s - loss: 175.8196 - loglik: -1.7463e+02 - logprior: -9.5310e-01
Fitted a model with MAP estimate = -175.4376
Time for alignment: 48.2235
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.4723 - loglik: -2.2572e+02 - logprior: -2.0724e+01
Epoch 2/10
10/10 - 1s - loss: 216.5687 - loglik: -2.1051e+02 - logprior: -5.8791e+00
Epoch 3/10
10/10 - 1s - loss: 199.7390 - loglik: -1.9611e+02 - logprior: -3.3388e+00
Epoch 4/10
10/10 - 1s - loss: 190.6984 - loglik: -1.8776e+02 - logprior: -2.6976e+00
Epoch 5/10
10/10 - 1s - loss: 187.0003 - loglik: -1.8423e+02 - logprior: -2.5368e+00
Epoch 6/10
10/10 - 1s - loss: 185.2908 - loglik: -1.8256e+02 - logprior: -2.4649e+00
Epoch 7/10
10/10 - 1s - loss: 184.3388 - loglik: -1.8190e+02 - logprior: -2.2133e+00
Epoch 8/10
10/10 - 1s - loss: 183.5872 - loglik: -1.8128e+02 - logprior: -2.1218e+00
Epoch 9/10
10/10 - 1s - loss: 183.4930 - loglik: -1.8118e+02 - logprior: -2.1410e+00
Epoch 10/10
10/10 - 1s - loss: 183.4031 - loglik: -1.8109e+02 - logprior: -2.1238e+00
Fitted a model with MAP estimate = -182.9326
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (23, 1), (41, 2), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 211.6807 - loglik: -1.8427e+02 - logprior: -2.7381e+01
Epoch 2/2
10/10 - 1s - loss: 188.5917 - loglik: -1.7966e+02 - logprior: -8.7872e+00
Fitted a model with MAP estimate = -183.8370
expansions: []
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.6502 - loglik: -1.7984e+02 - logprior: -2.3782e+01
Epoch 2/2
10/10 - 1s - loss: 188.8958 - loglik: -1.7907e+02 - logprior: -9.6928e+00
Fitted a model with MAP estimate = -185.5097
expansions: [(0, 2)]
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 199.2896 - loglik: -1.7845e+02 - logprior: -2.0815e+01
Epoch 2/10
10/10 - 1s - loss: 183.7892 - loglik: -1.7772e+02 - logprior: -5.9402e+00
Epoch 3/10
10/10 - 1s - loss: 179.7036 - loglik: -1.7655e+02 - logprior: -2.9065e+00
Epoch 4/10
10/10 - 1s - loss: 178.1897 - loglik: -1.7595e+02 - logprior: -1.9247e+00
Epoch 5/10
10/10 - 1s - loss: 177.4171 - loglik: -1.7556e+02 - logprior: -1.5473e+00
Epoch 6/10
10/10 - 1s - loss: 176.5446 - loglik: -1.7485e+02 - logprior: -1.4264e+00
Epoch 7/10
10/10 - 1s - loss: 175.9726 - loglik: -1.7446e+02 - logprior: -1.2815e+00
Epoch 8/10
10/10 - 1s - loss: 176.2417 - loglik: -1.7492e+02 - logprior: -1.0988e+00
Fitted a model with MAP estimate = -175.5640
Time for alignment: 45.7366
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.4312 - loglik: -2.2568e+02 - logprior: -2.0725e+01
Epoch 2/10
10/10 - 1s - loss: 216.1596 - loglik: -2.1011e+02 - logprior: -5.8794e+00
Epoch 3/10
10/10 - 1s - loss: 198.3030 - loglik: -1.9476e+02 - logprior: -3.2914e+00
Epoch 4/10
10/10 - 1s - loss: 190.0335 - loglik: -1.8712e+02 - logprior: -2.7220e+00
Epoch 5/10
10/10 - 1s - loss: 187.0341 - loglik: -1.8412e+02 - logprior: -2.6597e+00
Epoch 6/10
10/10 - 1s - loss: 185.0992 - loglik: -1.8229e+02 - logprior: -2.5099e+00
Epoch 7/10
10/10 - 1s - loss: 184.3099 - loglik: -1.8183e+02 - logprior: -2.2236e+00
Epoch 8/10
10/10 - 1s - loss: 183.7984 - loglik: -1.8145e+02 - logprior: -2.1145e+00
Epoch 9/10
10/10 - 1s - loss: 183.6970 - loglik: -1.8133e+02 - logprior: -2.1318e+00
Epoch 10/10
10/10 - 1s - loss: 183.0623 - loglik: -1.8075e+02 - logprior: -2.1008e+00
Fitted a model with MAP estimate = -183.0064
expansions: [(0, 2), (7, 2), (8, 2), (34, 1), (41, 2), (42, 2), (43, 1), (44, 2), (46, 1), (49, 1), (50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 212.4958 - loglik: -1.8508e+02 - logprior: -2.7394e+01
Epoch 2/2
10/10 - 1s - loss: 188.4020 - loglik: -1.7949e+02 - logprior: -8.7742e+00
Fitted a model with MAP estimate = -184.0490
expansions: []
discards: [ 0 10 12 56]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 204.2351 - loglik: -1.8047e+02 - logprior: -2.3744e+01
Epoch 2/2
10/10 - 1s - loss: 188.6673 - loglik: -1.7893e+02 - logprior: -9.5988e+00
Fitted a model with MAP estimate = -185.6354
expansions: [(0, 2)]
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.5144 - loglik: -1.7877e+02 - logprior: -2.0721e+01
Epoch 2/10
10/10 - 1s - loss: 183.2338 - loglik: -1.7728e+02 - logprior: -5.8341e+00
Epoch 3/10
10/10 - 1s - loss: 180.2294 - loglik: -1.7722e+02 - logprior: -2.8019e+00
Epoch 4/10
10/10 - 1s - loss: 178.6118 - loglik: -1.7648e+02 - logprior: -1.8698e+00
Epoch 5/10
10/10 - 1s - loss: 177.2285 - loglik: -1.7540e+02 - logprior: -1.5359e+00
Epoch 6/10
10/10 - 1s - loss: 177.0475 - loglik: -1.7534e+02 - logprior: -1.4204e+00
Epoch 7/10
10/10 - 1s - loss: 176.3989 - loglik: -1.7486e+02 - logprior: -1.2742e+00
Epoch 8/10
10/10 - 1s - loss: 176.6341 - loglik: -1.7529e+02 - logprior: -1.0900e+00
Fitted a model with MAP estimate = -175.8598
Time for alignment: 43.7405
Computed alignments with likelihoods: ['-175.4376', '-175.5640', '-175.8598']
Best model has likelihood: -175.4376
time for generating output: 0.1688
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6587064676616915
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac8a8c580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69a4c0f9a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab6503fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1f4acd90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ae3c2f880>, <__main__.SimpleDirichletPrior object at 0x7f6b1f712430>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 199.0754 - loglik: -1.8733e+02 - logprior: -1.1731e+01
Epoch 2/10
11/11 - 1s - loss: 158.4583 - loglik: -1.5499e+02 - logprior: -3.4554e+00
Epoch 3/10
11/11 - 1s - loss: 125.6868 - loglik: -1.2303e+02 - logprior: -2.6061e+00
Epoch 4/10
11/11 - 1s - loss: 111.5643 - loglik: -1.0906e+02 - logprior: -2.4546e+00
Epoch 5/10
11/11 - 1s - loss: 107.0051 - loglik: -1.0483e+02 - logprior: -2.1631e+00
Epoch 6/10
11/11 - 1s - loss: 104.6834 - loglik: -1.0234e+02 - logprior: -2.1859e+00
Epoch 7/10
11/11 - 1s - loss: 103.3805 - loglik: -1.0083e+02 - logprior: -2.1786e+00
Epoch 8/10
11/11 - 1s - loss: 103.4471 - loglik: -1.0092e+02 - logprior: -2.1274e+00
Fitted a model with MAP estimate = -102.5818
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 1), (35, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.6927 - loglik: -9.8676e+01 - logprior: -1.3990e+01
Epoch 2/2
11/11 - 1s - loss: 95.6432 - loglik: -9.1002e+01 - logprior: -4.5156e+00
Fitted a model with MAP estimate = -92.8023
expansions: []
discards: [ 0 35 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.6905 - loglik: -9.2233e+01 - logprior: -1.3439e+01
Epoch 2/2
11/11 - 1s - loss: 96.7469 - loglik: -9.0927e+01 - logprior: -5.7079e+00
Fitted a model with MAP estimate = -94.2701
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6556 - loglik: -9.0521e+01 - logprior: -1.1114e+01
Epoch 2/10
11/11 - 1s - loss: 93.8089 - loglik: -9.0413e+01 - logprior: -3.2778e+00
Epoch 3/10
11/11 - 1s - loss: 91.8452 - loglik: -8.9468e+01 - logprior: -2.1384e+00
Epoch 4/10
11/11 - 1s - loss: 90.1268 - loglik: -8.7866e+01 - logprior: -1.9653e+00
Epoch 5/10
11/11 - 1s - loss: 89.6669 - loglik: -8.7435e+01 - logprior: -1.9185e+00
Epoch 6/10
11/11 - 1s - loss: 89.6519 - loglik: -8.7559e+01 - logprior: -1.7672e+00
Epoch 7/10
11/11 - 1s - loss: 89.1350 - loglik: -8.7120e+01 - logprior: -1.6622e+00
Epoch 8/10
11/11 - 1s - loss: 89.0769 - loglik: -8.7065e+01 - logprior: -1.6572e+00
Epoch 9/10
11/11 - 1s - loss: 88.5810 - loglik: -8.6566e+01 - logprior: -1.6602e+00
Epoch 10/10
11/11 - 1s - loss: 88.7298 - loglik: -8.6779e+01 - logprior: -1.6336e+00
Fitted a model with MAP estimate = -88.2318
Time for alignment: 42.0496
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 199.4992 - loglik: -1.8775e+02 - logprior: -1.1731e+01
Epoch 2/10
11/11 - 1s - loss: 158.0891 - loglik: -1.5463e+02 - logprior: -3.4456e+00
Epoch 3/10
11/11 - 1s - loss: 124.4593 - loglik: -1.2182e+02 - logprior: -2.5953e+00
Epoch 4/10
11/11 - 1s - loss: 109.7647 - loglik: -1.0722e+02 - logprior: -2.4931e+00
Epoch 5/10
11/11 - 1s - loss: 104.3901 - loglik: -1.0207e+02 - logprior: -2.2771e+00
Epoch 6/10
11/11 - 1s - loss: 101.9585 - loglik: -9.9378e+01 - logprior: -2.3517e+00
Epoch 7/10
11/11 - 1s - loss: 100.8844 - loglik: -9.8123e+01 - logprior: -2.3823e+00
Epoch 8/10
11/11 - 1s - loss: 101.0507 - loglik: -9.8349e+01 - logprior: -2.3418e+00
Fitted a model with MAP estimate = -100.0746
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.5499 - loglik: -9.8530e+01 - logprior: -1.3998e+01
Epoch 2/2
11/11 - 1s - loss: 94.9761 - loglik: -9.0344e+01 - logprior: -4.5190e+00
Fitted a model with MAP estimate = -92.7262
expansions: []
discards: [ 0 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.8014 - loglik: -9.2330e+01 - logprior: -1.3453e+01
Epoch 2/2
11/11 - 1s - loss: 96.6885 - loglik: -9.0846e+01 - logprior: -5.7308e+00
Fitted a model with MAP estimate = -94.2262
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.7916 - loglik: -9.0657e+01 - logprior: -1.1116e+01
Epoch 2/10
11/11 - 1s - loss: 93.6725 - loglik: -9.0271e+01 - logprior: -3.2881e+00
Epoch 3/10
11/11 - 1s - loss: 91.3934 - loglik: -8.9018e+01 - logprior: -2.1510e+00
Epoch 4/10
11/11 - 1s - loss: 90.4483 - loglik: -8.8181e+01 - logprior: -1.9747e+00
Epoch 5/10
11/11 - 1s - loss: 89.6624 - loglik: -8.7432e+01 - logprior: -1.9107e+00
Epoch 6/10
11/11 - 1s - loss: 89.6875 - loglik: -8.7595e+01 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -88.8458
Time for alignment: 37.7087
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.2511 - loglik: -1.8750e+02 - logprior: -1.1731e+01
Epoch 2/10
11/11 - 1s - loss: 157.5886 - loglik: -1.5413e+02 - logprior: -3.4469e+00
Epoch 3/10
11/11 - 1s - loss: 123.2378 - loglik: -1.2058e+02 - logprior: -2.6029e+00
Epoch 4/10
11/11 - 1s - loss: 108.7576 - loglik: -1.0618e+02 - logprior: -2.5176e+00
Epoch 5/10
11/11 - 1s - loss: 103.9097 - loglik: -1.0155e+02 - logprior: -2.2995e+00
Epoch 6/10
11/11 - 1s - loss: 102.1696 - loglik: -9.9615e+01 - logprior: -2.3196e+00
Epoch 7/10
11/11 - 1s - loss: 101.6068 - loglik: -9.8994e+01 - logprior: -2.3050e+00
Epoch 8/10
11/11 - 1s - loss: 100.9068 - loglik: -9.8373e+01 - logprior: -2.2551e+00
Epoch 9/10
11/11 - 1s - loss: 100.7559 - loglik: -9.8202e+01 - logprior: -2.2884e+00
Epoch 10/10
11/11 - 1s - loss: 100.3608 - loglik: -9.7801e+01 - logprior: -2.2846e+00
Fitted a model with MAP estimate = -100.0989
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.8885 - loglik: -9.8744e+01 - logprior: -1.4129e+01
Epoch 2/2
11/11 - 1s - loss: 95.9489 - loglik: -9.1236e+01 - logprior: -4.6320e+00
Fitted a model with MAP estimate = -92.9185
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.8903 - loglik: -9.2413e+01 - logprior: -1.3464e+01
Epoch 2/2
11/11 - 1s - loss: 96.7559 - loglik: -9.0936e+01 - logprior: -5.7439e+00
Fitted a model with MAP estimate = -94.2162
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.5434 - loglik: -9.1297e+01 - logprior: -1.2226e+01
Epoch 2/10
11/11 - 1s - loss: 95.0352 - loglik: -9.1183e+01 - logprior: -3.7630e+00
Epoch 3/10
11/11 - 1s - loss: 92.2263 - loglik: -8.9491e+01 - logprior: -2.5499e+00
Epoch 4/10
11/11 - 1s - loss: 91.3503 - loglik: -8.9110e+01 - logprior: -1.9713e+00
Epoch 5/10
11/11 - 1s - loss: 90.6542 - loglik: -8.8638e+01 - logprior: -1.7158e+00
Epoch 6/10
11/11 - 1s - loss: 89.9665 - loglik: -8.8000e+01 - logprior: -1.6643e+00
Epoch 7/10
11/11 - 1s - loss: 89.5771 - loglik: -8.7672e+01 - logprior: -1.6024e+00
Epoch 8/10
11/11 - 1s - loss: 89.7439 - loglik: -8.7842e+01 - logprior: -1.6000e+00
Fitted a model with MAP estimate = -89.2000
Time for alignment: 40.5925
Computed alignments with likelihoods: ['-88.2318', '-88.8458', '-89.2000']
Best model has likelihood: -88.2318
time for generating output: 0.1325
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a8353c5b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac78c8700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a8212cb80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a82077cd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16a00fd0>, <__main__.SimpleDirichletPrior object at 0x7f6a9c6d3760>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 172.0981 - loglik: -1.7115e+02 - logprior: -7.4279e-01
Epoch 2/10
42/42 - 5s - loss: 81.3251 - loglik: -8.0366e+01 - logprior: -6.6747e-01
Epoch 3/10
42/42 - 5s - loss: 78.8909 - loglik: -7.8016e+01 - logprior: -6.6955e-01
Epoch 4/10
42/42 - 5s - loss: 78.1444 - loglik: -7.7338e+01 - logprior: -6.5941e-01
Epoch 5/10
42/42 - 5s - loss: 78.2071 - loglik: -7.7412e+01 - logprior: -6.4783e-01
Fitted a model with MAP estimate = -77.1161
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (44, 1), (47, 1), (48, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 46.6639 - loglik: -4.5633e+01 - logprior: -8.5380e-01
Epoch 2/2
42/42 - 5s - loss: 33.4250 - loglik: -3.2568e+01 - logprior: -6.7339e-01
Fitted a model with MAP estimate = -32.6372
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 37.5916 - loglik: -3.6525e+01 - logprior: -9.9656e-01
Epoch 2/2
42/42 - 5s - loss: 34.1936 - loglik: -3.3489e+01 - logprior: -5.3317e-01
Fitted a model with MAP estimate = -33.7229
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 33.9214 - loglik: -3.3362e+01 - logprior: -4.7321e-01
Epoch 2/10
59/59 - 7s - loss: 32.3350 - loglik: -3.1573e+01 - logprior: -5.9541e-01
Epoch 3/10
59/59 - 7s - loss: 32.1449 - loglik: -3.1396e+01 - logprior: -5.7143e-01
Epoch 4/10
59/59 - 7s - loss: 31.8310 - loglik: -3.1125e+01 - logprior: -5.6187e-01
Epoch 5/10
59/59 - 7s - loss: 31.8108 - loglik: -3.1129e+01 - logprior: -5.5542e-01
Epoch 6/10
59/59 - 7s - loss: 31.7052 - loglik: -3.1031e+01 - logprior: -5.4862e-01
Epoch 7/10
59/59 - 7s - loss: 31.2081 - loglik: -3.0537e+01 - logprior: -5.4007e-01
Epoch 8/10
59/59 - 7s - loss: 31.1871 - loglik: -3.0515e+01 - logprior: -5.3316e-01
Epoch 9/10
59/59 - 7s - loss: 31.2400 - loglik: -3.0579e+01 - logprior: -5.2477e-01
Fitted a model with MAP estimate = -30.8342
Time for alignment: 222.7541
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.9329 - loglik: -1.7098e+02 - logprior: -7.4320e-01
Epoch 2/10
42/42 - 5s - loss: 81.6115 - loglik: -8.0663e+01 - logprior: -6.6051e-01
Epoch 3/10
42/42 - 5s - loss: 78.3960 - loglik: -7.7446e+01 - logprior: -6.6362e-01
Epoch 4/10
42/42 - 5s - loss: 78.0988 - loglik: -7.7176e+01 - logprior: -6.5299e-01
Epoch 5/10
42/42 - 5s - loss: 77.5027 - loglik: -7.6618e+01 - logprior: -6.4883e-01
Epoch 6/10
42/42 - 5s - loss: 77.4004 - loglik: -7.6555e+01 - logprior: -6.4324e-01
Epoch 7/10
42/42 - 5s - loss: 77.4749 - loglik: -7.6687e+01 - logprior: -6.3649e-01
Fitted a model with MAP estimate = -76.3589
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 46.7309 - loglik: -4.5680e+01 - logprior: -8.6534e-01
Epoch 2/2
42/42 - 5s - loss: 33.7456 - loglik: -3.2868e+01 - logprior: -6.6693e-01
Fitted a model with MAP estimate = -32.6660
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.5492 - loglik: -3.6480e+01 - logprior: -9.9477e-01
Epoch 2/2
42/42 - 5s - loss: 34.5620 - loglik: -3.3852e+01 - logprior: -5.3535e-01
Fitted a model with MAP estimate = -33.7594
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 33.6258 - loglik: -3.2963e+01 - logprior: -5.7398e-01
Epoch 2/10
59/59 - 7s - loss: 32.3149 - loglik: -3.1556e+01 - logprior: -5.9348e-01
Epoch 3/10
59/59 - 7s - loss: 32.1706 - loglik: -3.1416e+01 - logprior: -5.8059e-01
Epoch 4/10
59/59 - 7s - loss: 31.9392 - loglik: -3.1226e+01 - logprior: -5.7088e-01
Epoch 5/10
59/59 - 7s - loss: 31.8676 - loglik: -3.1185e+01 - logprior: -5.6045e-01
Epoch 6/10
59/59 - 7s - loss: 31.6815 - loglik: -3.1006e+01 - logprior: -5.5263e-01
Epoch 7/10
59/59 - 7s - loss: 31.1233 - loglik: -3.0454e+01 - logprior: -5.4001e-01
Epoch 8/10
59/59 - 7s - loss: 31.2636 - loglik: -3.0604e+01 - logprior: -5.2938e-01
Fitted a model with MAP estimate = -30.8708
Time for alignment: 225.1975
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.4313 - loglik: -1.7047e+02 - logprior: -7.5001e-01
Epoch 2/10
42/42 - 5s - loss: 81.1048 - loglik: -8.0170e+01 - logprior: -6.6514e-01
Epoch 3/10
42/42 - 5s - loss: 78.7442 - loglik: -7.7909e+01 - logprior: -6.6912e-01
Epoch 4/10
42/42 - 5s - loss: 78.1309 - loglik: -7.7325e+01 - logprior: -6.5352e-01
Epoch 5/10
42/42 - 5s - loss: 78.0901 - loglik: -7.7289e+01 - logprior: -6.4606e-01
Epoch 6/10
42/42 - 5s - loss: 77.5913 - loglik: -7.6776e+01 - logprior: -6.4146e-01
Epoch 7/10
42/42 - 5s - loss: 77.4277 - loglik: -7.6613e+01 - logprior: -6.4229e-01
Epoch 8/10
42/42 - 5s - loss: 77.1605 - loglik: -7.6347e+01 - logprior: -6.3898e-01
Epoch 9/10
42/42 - 5s - loss: 77.6277 - loglik: -7.6819e+01 - logprior: -6.3291e-01
Fitted a model with MAP estimate = -76.6680
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 47.0479 - loglik: -4.6002e+01 - logprior: -8.7285e-01
Epoch 2/2
42/42 - 5s - loss: 33.1866 - loglik: -3.2340e+01 - logprior: -6.6535e-01
Fitted a model with MAP estimate = -32.6266
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 38.0120 - loglik: -3.6956e+01 - logprior: -9.8668e-01
Epoch 2/2
42/42 - 5s - loss: 33.6835 - loglik: -3.2960e+01 - logprior: -5.5143e-01
Fitted a model with MAP estimate = -33.6780
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.8683 - loglik: -3.3322e+01 - logprior: -4.5913e-01
Epoch 2/10
59/59 - 7s - loss: 32.5503 - loglik: -3.1877e+01 - logprior: -5.0675e-01
Epoch 3/10
59/59 - 7s - loss: 32.2815 - loglik: -3.1536e+01 - logprior: -5.6975e-01
Epoch 4/10
59/59 - 7s - loss: 32.0467 - loglik: -3.1337e+01 - logprior: -5.6239e-01
Epoch 5/10
59/59 - 7s - loss: 31.9937 - loglik: -3.1315e+01 - logprior: -5.5293e-01
Epoch 6/10
59/59 - 7s - loss: 31.3843 - loglik: -3.0710e+01 - logprior: -5.4694e-01
Epoch 7/10
59/59 - 7s - loss: 31.4937 - loglik: -3.0822e+01 - logprior: -5.4047e-01
Fitted a model with MAP estimate = -31.0359
Time for alignment: 225.7998
Computed alignments with likelihoods: ['-30.8342', '-30.8708', '-31.0359']
Best model has likelihood: -30.8342
time for generating output: 0.1699
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.2239512855209743
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a8330ea60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aca4b0c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad6a7b80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac8fac8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb6c4550>, <__main__.SimpleDirichletPrior object at 0x7f6b05b15730>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 475.6230 - loglik: -4.5088e+02 - logprior: -2.4721e+01
Epoch 2/10
14/14 - 4s - loss: 412.3789 - loglik: -4.0745e+02 - logprior: -4.8858e+00
Epoch 3/10
14/14 - 4s - loss: 373.2057 - loglik: -3.6993e+02 - logprior: -3.0854e+00
Epoch 4/10
14/14 - 4s - loss: 359.0348 - loglik: -3.5529e+02 - logprior: -3.1178e+00
Epoch 5/10
14/14 - 4s - loss: 354.9981 - loglik: -3.5130e+02 - logprior: -2.9538e+00
Epoch 6/10
14/14 - 4s - loss: 353.0151 - loglik: -3.4961e+02 - logprior: -2.8340e+00
Epoch 7/10
14/14 - 4s - loss: 354.3013 - loglik: -3.5103e+02 - logprior: -2.7412e+00
Fitted a model with MAP estimate = -351.1101
expansions: [(10, 1), (11, 1), (16, 5), (18, 1), (35, 1), (36, 3), (38, 2), (43, 1), (45, 2), (66, 1), (67, 1), (77, 1), (79, 5), (100, 1), (102, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 383.3910 - loglik: -3.5365e+02 - logprior: -2.9692e+01
Epoch 2/2
14/14 - 5s - loss: 352.5144 - loglik: -3.4083e+02 - logprior: -1.1422e+01
Fitted a model with MAP estimate = -349.4740
expansions: [(0, 21), (59, 1)]
discards: [ 0 12 13 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 368.8689 - loglik: -3.4604e+02 - logprior: -2.2783e+01
Epoch 2/2
14/14 - 5s - loss: 346.3430 - loglik: -3.4142e+02 - logprior: -4.6876e+00
Fitted a model with MAP estimate = -343.0553
expansions: [(32, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  78  79 117]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 371.6664 - loglik: -3.4503e+02 - logprior: -2.6594e+01
Epoch 2/10
14/14 - 5s - loss: 347.7038 - loglik: -3.4264e+02 - logprior: -4.8188e+00
Epoch 3/10
14/14 - 5s - loss: 341.7005 - loglik: -3.4017e+02 - logprior: -1.0373e+00
Epoch 4/10
14/14 - 5s - loss: 341.1048 - loglik: -3.4050e+02 - logprior: -4.0560e-03
Epoch 5/10
14/14 - 5s - loss: 338.0942 - loglik: -3.3778e+02 - logprior: 0.2846
Epoch 6/10
14/14 - 5s - loss: 336.7836 - loglik: -3.3681e+02 - logprior: 0.5765
Epoch 7/10
14/14 - 5s - loss: 338.1219 - loglik: -3.3854e+02 - logprior: 0.9110
Fitted a model with MAP estimate = -335.8919
Time for alignment: 107.9687
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 474.8721 - loglik: -4.5012e+02 - logprior: -2.4727e+01
Epoch 2/10
14/14 - 4s - loss: 412.6211 - loglik: -4.0772e+02 - logprior: -4.8544e+00
Epoch 3/10
14/14 - 4s - loss: 375.2603 - loglik: -3.7220e+02 - logprior: -2.9139e+00
Epoch 4/10
14/14 - 4s - loss: 361.3780 - loglik: -3.5797e+02 - logprior: -2.9135e+00
Epoch 5/10
14/14 - 4s - loss: 353.9854 - loglik: -3.5030e+02 - logprior: -2.9340e+00
Epoch 6/10
14/14 - 4s - loss: 354.8020 - loglik: -3.5125e+02 - logprior: -2.9113e+00
Fitted a model with MAP estimate = -351.8917
expansions: [(12, 1), (17, 5), (35, 1), (36, 3), (42, 1), (43, 1), (44, 1), (45, 1), (66, 1), (67, 1), (74, 1), (76, 1), (78, 2), (79, 4), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 384.2025 - loglik: -3.5430e+02 - logprior: -2.9849e+01
Epoch 2/2
14/14 - 5s - loss: 354.1602 - loglik: -3.4231e+02 - logprior: -1.1592e+01
Fitted a model with MAP estimate = -350.2710
expansions: [(0, 23), (19, 1), (23, 1)]
discards: [ 0 12 13 43 58 59 96]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 372.2701 - loglik: -3.4937e+02 - logprior: -2.2859e+01
Epoch 2/2
14/14 - 5s - loss: 349.8636 - loglik: -3.4484e+02 - logprior: -4.7848e+00
Fitted a model with MAP estimate = -346.1878
expansions: [(34, 2), (77, 1), (79, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 371.3217 - loglik: -3.4455e+02 - logprior: -2.6733e+01
Epoch 2/10
14/14 - 5s - loss: 343.9360 - loglik: -3.3879e+02 - logprior: -4.9108e+00
Epoch 3/10
14/14 - 5s - loss: 339.6535 - loglik: -3.3803e+02 - logprior: -1.1386e+00
Epoch 4/10
14/14 - 5s - loss: 337.0028 - loglik: -3.3624e+02 - logprior: -1.7897e-01
Epoch 5/10
14/14 - 5s - loss: 336.4688 - loglik: -3.3604e+02 - logprior: 0.1533
Epoch 6/10
14/14 - 5s - loss: 334.2640 - loglik: -3.3425e+02 - logprior: 0.5284
Epoch 7/10
14/14 - 5s - loss: 333.1753 - loglik: -3.3350e+02 - logprior: 0.8186
Epoch 8/10
14/14 - 5s - loss: 333.9713 - loglik: -3.3455e+02 - logprior: 1.0526
Fitted a model with MAP estimate = -332.5147
Time for alignment: 108.3492
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 475.9554 - loglik: -4.5120e+02 - logprior: -2.4727e+01
Epoch 2/10
14/14 - 4s - loss: 411.7402 - loglik: -4.0685e+02 - logprior: -4.8447e+00
Epoch 3/10
14/14 - 4s - loss: 375.5760 - loglik: -3.7240e+02 - logprior: -3.0101e+00
Epoch 4/10
14/14 - 4s - loss: 359.6936 - loglik: -3.5625e+02 - logprior: -2.9725e+00
Epoch 5/10
14/14 - 4s - loss: 356.0525 - loglik: -3.5240e+02 - logprior: -2.8945e+00
Epoch 6/10
14/14 - 4s - loss: 353.9180 - loglik: -3.5037e+02 - logprior: -2.8312e+00
Epoch 7/10
14/14 - 4s - loss: 353.6479 - loglik: -3.5026e+02 - logprior: -2.7564e+00
Epoch 8/10
14/14 - 4s - loss: 352.5981 - loglik: -3.4923e+02 - logprior: -2.7725e+00
Epoch 9/10
14/14 - 4s - loss: 351.3120 - loglik: -3.4796e+02 - logprior: -2.7901e+00
Epoch 10/10
14/14 - 4s - loss: 350.8215 - loglik: -3.4746e+02 - logprior: -2.8007e+00
Fitted a model with MAP estimate = -350.1810
expansions: [(10, 1), (11, 2), (15, 1), (16, 5), (17, 1), (34, 1), (36, 2), (38, 2), (41, 1), (44, 1), (45, 1), (66, 3), (79, 2), (80, 4), (81, 1), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 160 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 384.1692 - loglik: -3.5412e+02 - logprior: -2.9991e+01
Epoch 2/2
14/14 - 5s - loss: 354.7022 - loglik: -3.4272e+02 - logprior: -1.1680e+01
Fitted a model with MAP estimate = -349.3973
expansions: [(0, 24)]
discards: [  0  11  12  13  14  47  50  60  61  62  63 100 103]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 374.1821 - loglik: -3.5130e+02 - logprior: -2.2838e+01
Epoch 2/2
14/14 - 5s - loss: 350.5012 - loglik: -3.4552e+02 - logprior: -4.7382e+00
Fitted a model with MAP estimate = -346.9147
expansions: [(34, 2), (76, 3)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 372.5139 - loglik: -3.4574e+02 - logprior: -2.6728e+01
Epoch 2/10
14/14 - 5s - loss: 345.9067 - loglik: -3.4062e+02 - logprior: -5.0535e+00
Epoch 3/10
14/14 - 5s - loss: 341.7804 - loglik: -3.3988e+02 - logprior: -1.4128e+00
Epoch 4/10
14/14 - 5s - loss: 338.2769 - loglik: -3.3718e+02 - logprior: -4.9033e-01
Epoch 5/10
14/14 - 5s - loss: 336.5959 - loglik: -3.3589e+02 - logprior: -1.3230e-01
Epoch 6/10
14/14 - 5s - loss: 335.2268 - loglik: -3.3499e+02 - logprior: 0.2744
Epoch 7/10
14/14 - 5s - loss: 335.4473 - loglik: -3.3553e+02 - logprior: 0.5666
Fitted a model with MAP estimate = -334.0832
Time for alignment: 121.1420
Computed alignments with likelihoods: ['-335.8919', '-332.5147', '-334.0832']
Best model has likelihood: -332.5147
time for generating output: 0.2264
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9815048339638504
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a822b7d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f697a59bb20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f697a59b5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69d7f4e820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac75d7fd0>, <__main__.SimpleDirichletPrior object at 0x7f6ac9f96220>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 857.2454 - loglik: -8.5362e+02 - logprior: -3.5063e+00
Epoch 2/10
29/29 - 24s - loss: 676.4120 - loglik: -6.7311e+02 - logprior: -2.8131e+00
Epoch 3/10
29/29 - 24s - loss: 647.5140 - loglik: -6.4372e+02 - logprior: -3.0601e+00
Epoch 4/10
29/29 - 24s - loss: 641.1981 - loglik: -6.3743e+02 - logprior: -3.0830e+00
Epoch 5/10
29/29 - 24s - loss: 640.7471 - loglik: -6.3704e+02 - logprior: -3.1479e+00
Epoch 6/10
29/29 - 24s - loss: 637.7491 - loglik: -6.3405e+02 - logprior: -3.1780e+00
Epoch 7/10
29/29 - 24s - loss: 639.2355 - loglik: -6.3554e+02 - logprior: -3.2049e+00
Fitted a model with MAP estimate = -636.8932
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 2), (42, 1), (48, 2), (49, 2), (73, 1), (88, 1), (89, 2), (90, 2), (97, 1), (119, 2), (120, 2), (121, 1), (124, 2), (125, 2), (128, 1), (142, 1), (152, 1), (153, 1), (155, 2), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (233, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (270, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 35s - loss: 627.8934 - loglik: -6.2174e+02 - logprior: -5.9906e+00
Epoch 2/2
29/29 - 31s - loss: 595.7720 - loglik: -5.9202e+02 - logprior: -3.2373e+00
Fitted a model with MAP estimate = -591.0226
expansions: [(0, 2), (25, 1), (331, 2)]
discards: [  0  36  58  59 104 107 264]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 35s - loss: 599.2042 - loglik: -5.9497e+02 - logprior: -4.0983e+00
Epoch 2/2
29/29 - 31s - loss: 592.7014 - loglik: -5.9077e+02 - logprior: -1.4583e+00
Fitted a model with MAP estimate = -588.1576
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 36s - loss: 602.8700 - loglik: -5.9718e+02 - logprior: -5.5578e+00
Epoch 2/10
29/29 - 31s - loss: 591.4327 - loglik: -5.8925e+02 - logprior: -1.7113e+00
Epoch 3/10
29/29 - 31s - loss: 588.5901 - loglik: -5.8701e+02 - logprior: -9.6389e-01
Epoch 4/10
29/29 - 31s - loss: 584.8066 - loglik: -5.8356e+02 - logprior: -6.6290e-01
Epoch 5/10
29/29 - 31s - loss: 586.7634 - loglik: -5.8549e+02 - logprior: -7.2367e-01
Fitted a model with MAP estimate = -583.6746
Time for alignment: 576.4075
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 857.5274 - loglik: -8.5392e+02 - logprior: -3.4929e+00
Epoch 2/10
29/29 - 24s - loss: 672.6314 - loglik: -6.6966e+02 - logprior: -2.7549e+00
Epoch 3/10
29/29 - 24s - loss: 642.5967 - loglik: -6.3886e+02 - logprior: -3.1275e+00
Epoch 4/10
29/29 - 24s - loss: 639.6920 - loglik: -6.3602e+02 - logprior: -3.1226e+00
Epoch 5/10
29/29 - 24s - loss: 638.2167 - loglik: -6.3458e+02 - logprior: -3.1551e+00
Epoch 6/10
29/29 - 24s - loss: 636.1933 - loglik: -6.3253e+02 - logprior: -3.1984e+00
Epoch 7/10
29/29 - 24s - loss: 634.1938 - loglik: -6.3048e+02 - logprior: -3.2688e+00
Epoch 8/10
29/29 - 24s - loss: 633.9844 - loglik: -6.3026e+02 - logprior: -3.2746e+00
Epoch 9/10
29/29 - 24s - loss: 633.6406 - loglik: -6.2990e+02 - logprior: -3.2805e+00
Epoch 10/10
29/29 - 24s - loss: 633.0906 - loglik: -6.2934e+02 - logprior: -3.2845e+00
Fitted a model with MAP estimate = -632.1418
expansions: [(15, 1), (16, 1), (17, 1), (23, 1), (27, 1), (28, 1), (29, 1), (36, 1), (38, 1), (47, 1), (48, 1), (50, 2), (76, 1), (87, 1), (88, 2), (89, 2), (120, 2), (121, 2), (124, 2), (125, 1), (126, 1), (142, 1), (148, 1), (152, 1), (155, 2), (156, 1), (167, 2), (174, 1), (185, 1), (186, 1), (187, 1), (192, 1), (193, 1), (205, 1), (206, 1), (218, 2), (219, 2), (220, 1), (233, 1), (249, 1), (250, 1), (252, 1), (253, 3), (261, 2), (262, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 345 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 36s - loss: 624.7500 - loglik: -6.1837e+02 - logprior: -6.2090e+00
Epoch 2/2
29/29 - 32s - loss: 595.7610 - loglik: -5.9165e+02 - logprior: -3.6450e+00
Fitted a model with MAP estimate = -591.8407
expansions: [(0, 2)]
discards: [  0  61 103 141 147 184 200 263 304 305 330]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 600.6675 - loglik: -5.9653e+02 - logprior: -3.9934e+00
Epoch 2/2
29/29 - 31s - loss: 594.8054 - loglik: -5.9287e+02 - logprior: -1.4460e+00
Fitted a model with MAP estimate = -588.5902
expansions: [(105, 1)]
discards: [  0 325]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 35s - loss: 601.1501 - loglik: -5.9563e+02 - logprior: -5.3743e+00
Epoch 2/10
29/29 - 31s - loss: 590.2957 - loglik: -5.8822e+02 - logprior: -1.5872e+00
Epoch 3/10
29/29 - 31s - loss: 585.1413 - loglik: -5.8360e+02 - logprior: -9.2014e-01
Epoch 4/10
29/29 - 31s - loss: 583.2795 - loglik: -5.8213e+02 - logprior: -5.6623e-01
Epoch 5/10
29/29 - 31s - loss: 585.0870 - loglik: -5.8395e+02 - logprior: -5.9221e-01
Fitted a model with MAP estimate = -581.8526
Time for alignment: 645.5192
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 856.5345 - loglik: -8.5292e+02 - logprior: -3.5041e+00
Epoch 2/10
29/29 - 24s - loss: 679.6855 - loglik: -6.7660e+02 - logprior: -2.8603e+00
Epoch 3/10
29/29 - 24s - loss: 644.6143 - loglik: -6.4075e+02 - logprior: -3.2489e+00
Epoch 4/10
29/29 - 24s - loss: 638.6186 - loglik: -6.3481e+02 - logprior: -3.1892e+00
Epoch 5/10
29/29 - 24s - loss: 634.9367 - loglik: -6.3115e+02 - logprior: -3.1980e+00
Epoch 6/10
29/29 - 24s - loss: 633.5760 - loglik: -6.2972e+02 - logprior: -3.3328e+00
Epoch 7/10
29/29 - 24s - loss: 631.2231 - loglik: -6.2733e+02 - logprior: -3.3919e+00
Epoch 8/10
29/29 - 24s - loss: 632.3958 - loglik: -6.2850e+02 - logprior: -3.3902e+00
Fitted a model with MAP estimate = -630.3732
expansions: [(15, 1), (17, 1), (21, 1), (23, 1), (27, 1), (28, 1), (29, 1), (30, 1), (36, 1), (38, 1), (41, 1), (47, 2), (48, 1), (49, 1), (72, 1), (75, 1), (86, 1), (87, 1), (88, 1), (119, 2), (120, 2), (121, 2), (123, 1), (125, 1), (127, 1), (141, 1), (151, 1), (154, 2), (155, 1), (162, 1), (173, 1), (184, 3), (185, 1), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (262, 1), (264, 1), (270, 5)]
discards: [ 0 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 35s - loss: 624.6343 - loglik: -6.1818e+02 - logprior: -6.2856e+00
Epoch 2/2
29/29 - 31s - loss: 594.9761 - loglik: -5.9092e+02 - logprior: -3.5850e+00
Fitted a model with MAP estimate = -590.3541
expansions: [(0, 2), (148, 1), (328, 2)]
discards: [  0  62 136 142 182 260 299]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 597.0332 - loglik: -5.9289e+02 - logprior: -4.0026e+00
Epoch 2/2
29/29 - 31s - loss: 589.7261 - loglik: -5.8789e+02 - logprior: -1.3492e+00
Fitted a model with MAP estimate = -586.5005
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 35s - loss: 600.2234 - loglik: -5.9472e+02 - logprior: -5.3724e+00
Epoch 2/10
29/29 - 31s - loss: 591.2688 - loglik: -5.8919e+02 - logprior: -1.5997e+00
Epoch 3/10
29/29 - 31s - loss: 584.4723 - loglik: -5.8287e+02 - logprior: -9.5586e-01
Epoch 4/10
29/29 - 31s - loss: 585.0583 - loglik: -5.8386e+02 - logprior: -5.9539e-01
Fitted a model with MAP estimate = -583.4570
Time for alignment: 564.0402
Computed alignments with likelihoods: ['-583.6746', '-581.8526', '-583.4570']
Best model has likelihood: -581.8526
time for generating output: 0.4468
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8526772793053545
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6aa48801c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69d7cb4af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5c82460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac79306a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69e861a2e0>, <__main__.SimpleDirichletPrior object at 0x7f697238a1c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.1796 - loglik: -1.2835e+02 - logprior: -4.9481e+02
Epoch 2/10
10/10 - 1s - loss: 247.8754 - loglik: -1.0685e+02 - logprior: -1.4102e+02
Epoch 3/10
10/10 - 1s - loss: 155.0592 - loglik: -8.6722e+01 - logprior: -6.8302e+01
Epoch 4/10
10/10 - 1s - loss: 114.3316 - loglik: -7.3483e+01 - logprior: -4.0777e+01
Epoch 5/10
10/10 - 1s - loss: 95.9323 - loglik: -6.9824e+01 - logprior: -2.6061e+01
Epoch 6/10
10/10 - 1s - loss: 86.0191 - loglik: -6.9612e+01 - logprior: -1.6374e+01
Epoch 7/10
10/10 - 1s - loss: 80.2270 - loglik: -7.0107e+01 - logprior: -1.0095e+01
Epoch 8/10
10/10 - 1s - loss: 76.7008 - loglik: -7.0407e+01 - logprior: -6.2757e+00
Epoch 9/10
10/10 - 1s - loss: 74.3590 - loglik: -7.0593e+01 - logprior: -3.7557e+00
Epoch 10/10
10/10 - 1s - loss: 72.6880 - loglik: -7.0788e+01 - logprior: -1.8944e+00
Fitted a model with MAP estimate = -71.9339
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 721.9938 - loglik: -6.2173e+01 - logprior: -6.5980e+02
Epoch 2/2
10/10 - 1s - loss: 265.8869 - loglik: -5.4720e+01 - logprior: -2.1109e+02
Fitted a model with MAP estimate = -179.3421
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 512.9283 - loglik: -4.9911e+01 - logprior: -4.6299e+02
Epoch 2/2
10/10 - 1s - loss: 178.4819 - loglik: -4.9680e+01 - logprior: -1.2871e+02
Fitted a model with MAP estimate = -128.5311
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 484.2785 - loglik: -4.8924e+01 - logprior: -4.3533e+02
Epoch 2/10
10/10 - 1s - loss: 170.9657 - loglik: -4.9611e+01 - logprior: -1.2127e+02
Epoch 3/10
10/10 - 1s - loss: 105.1122 - loglik: -5.0422e+01 - logprior: -5.4634e+01
Epoch 4/10
10/10 - 1s - loss: 77.0974 - loglik: -5.1029e+01 - logprior: -2.6057e+01
Epoch 5/10
10/10 - 1s - loss: 61.6711 - loglik: -5.1527e+01 - logprior: -1.0142e+01
Epoch 6/10
10/10 - 1s - loss: 52.6544 - loglik: -5.1969e+01 - logprior: -6.8082e-01
Epoch 7/10
10/10 - 1s - loss: 47.0337 - loglik: -5.2304e+01 - logprior: 5.2749
Epoch 8/10
10/10 - 1s - loss: 43.2038 - loglik: -5.2572e+01 - logprior: 9.3717
Epoch 9/10
10/10 - 1s - loss: 40.3472 - loglik: -5.2790e+01 - logprior: 12.4460
Epoch 10/10
10/10 - 1s - loss: 38.0568 - loglik: -5.2969e+01 - logprior: 14.9145
Fitted a model with MAP estimate = -36.9413
Time for alignment: 28.9393
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.1793 - loglik: -1.2835e+02 - logprior: -4.9481e+02
Epoch 2/10
10/10 - 1s - loss: 247.8751 - loglik: -1.0685e+02 - logprior: -1.4102e+02
Epoch 3/10
10/10 - 1s - loss: 155.0591 - loglik: -8.6722e+01 - logprior: -6.8302e+01
Epoch 4/10
10/10 - 1s - loss: 114.3315 - loglik: -7.3483e+01 - logprior: -4.0777e+01
Epoch 5/10
10/10 - 1s - loss: 95.9321 - loglik: -6.9824e+01 - logprior: -2.6061e+01
Epoch 6/10
10/10 - 1s - loss: 86.0191 - loglik: -6.9612e+01 - logprior: -1.6374e+01
Epoch 7/10
10/10 - 1s - loss: 80.2270 - loglik: -7.0107e+01 - logprior: -1.0095e+01
Epoch 8/10
10/10 - 1s - loss: 76.7008 - loglik: -7.0407e+01 - logprior: -6.2757e+00
Epoch 9/10
10/10 - 1s - loss: 74.3590 - loglik: -7.0593e+01 - logprior: -3.7557e+00
Epoch 10/10
10/10 - 1s - loss: 72.6881 - loglik: -7.0788e+01 - logprior: -1.8944e+00
Fitted a model with MAP estimate = -71.9338
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 721.9928 - loglik: -6.2173e+01 - logprior: -6.5980e+02
Epoch 2/2
10/10 - 1s - loss: 265.8865 - loglik: -5.4720e+01 - logprior: -2.1109e+02
Fitted a model with MAP estimate = -179.3419
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 512.9274 - loglik: -4.9911e+01 - logprior: -4.6299e+02
Epoch 2/2
10/10 - 1s - loss: 178.4817 - loglik: -4.9680e+01 - logprior: -1.2871e+02
Fitted a model with MAP estimate = -128.5308
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 484.2775 - loglik: -4.8924e+01 - logprior: -4.3533e+02
Epoch 2/10
10/10 - 1s - loss: 170.9654 - loglik: -4.9612e+01 - logprior: -1.2126e+02
Epoch 3/10
10/10 - 1s - loss: 105.1120 - loglik: -5.0422e+01 - logprior: -5.4633e+01
Epoch 4/10
10/10 - 1s - loss: 77.0972 - loglik: -5.1029e+01 - logprior: -2.6057e+01
Epoch 5/10
10/10 - 1s - loss: 61.6708 - loglik: -5.1527e+01 - logprior: -1.0141e+01
Epoch 6/10
10/10 - 1s - loss: 52.6541 - loglik: -5.1969e+01 - logprior: -6.8049e-01
Epoch 7/10
10/10 - 1s - loss: 47.0333 - loglik: -5.2304e+01 - logprior: 5.2752
Epoch 8/10
10/10 - 1s - loss: 43.2034 - loglik: -5.2572e+01 - logprior: 9.3721
Epoch 9/10
10/10 - 1s - loss: 40.3467 - loglik: -5.2790e+01 - logprior: 12.4465
Epoch 10/10
10/10 - 1s - loss: 38.0563 - loglik: -5.2969e+01 - logprior: 14.9150
Fitted a model with MAP estimate = -36.9407
Time for alignment: 29.7075
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 623.1789 - loglik: -1.2835e+02 - logprior: -4.9481e+02
Epoch 2/10
10/10 - 1s - loss: 247.8746 - loglik: -1.0685e+02 - logprior: -1.4102e+02
Epoch 3/10
10/10 - 1s - loss: 155.0592 - loglik: -8.6722e+01 - logprior: -6.8302e+01
Epoch 4/10
10/10 - 1s - loss: 114.3321 - loglik: -7.3483e+01 - logprior: -4.0778e+01
Epoch 5/10
10/10 - 1s - loss: 95.9323 - loglik: -6.9824e+01 - logprior: -2.6061e+01
Epoch 6/10
10/10 - 1s - loss: 86.0191 - loglik: -6.9612e+01 - logprior: -1.6374e+01
Epoch 7/10
10/10 - 1s - loss: 80.2271 - loglik: -7.0107e+01 - logprior: -1.0095e+01
Epoch 8/10
10/10 - 1s - loss: 76.7008 - loglik: -7.0407e+01 - logprior: -6.2757e+00
Epoch 9/10
10/10 - 1s - loss: 74.3590 - loglik: -7.0593e+01 - logprior: -3.7557e+00
Epoch 10/10
10/10 - 1s - loss: 72.6881 - loglik: -7.0788e+01 - logprior: -1.8944e+00
Fitted a model with MAP estimate = -71.9338
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 721.9942 - loglik: -6.2173e+01 - logprior: -6.5980e+02
Epoch 2/2
10/10 - 1s - loss: 265.8870 - loglik: -5.4720e+01 - logprior: -2.1109e+02
Fitted a model with MAP estimate = -179.3421
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 512.9282 - loglik: -4.9911e+01 - logprior: -4.6299e+02
Epoch 2/2
10/10 - 1s - loss: 178.4819 - loglik: -4.9680e+01 - logprior: -1.2871e+02
Fitted a model with MAP estimate = -128.5310
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 484.2785 - loglik: -4.8924e+01 - logprior: -4.3533e+02
Epoch 2/10
10/10 - 1s - loss: 170.9657 - loglik: -4.9612e+01 - logprior: -1.2127e+02
Epoch 3/10
10/10 - 1s - loss: 105.1122 - loglik: -5.0422e+01 - logprior: -5.4633e+01
Epoch 4/10
10/10 - 1s - loss: 77.0974 - loglik: -5.1029e+01 - logprior: -2.6057e+01
Epoch 5/10
10/10 - 1s - loss: 61.6710 - loglik: -5.1527e+01 - logprior: -1.0141e+01
Epoch 6/10
10/10 - 1s - loss: 52.6543 - loglik: -5.1969e+01 - logprior: -6.8070e-01
Epoch 7/10
10/10 - 1s - loss: 47.0336 - loglik: -5.2304e+01 - logprior: 5.2750
Epoch 8/10
10/10 - 1s - loss: 43.2037 - loglik: -5.2572e+01 - logprior: 9.3719
Epoch 9/10
10/10 - 1s - loss: 40.3471 - loglik: -5.2790e+01 - logprior: 12.4462
Epoch 10/10
10/10 - 1s - loss: 38.0566 - loglik: -5.2969e+01 - logprior: 14.9147
Fitted a model with MAP estimate = -36.9412
Time for alignment: 27.9150
Computed alignments with likelihoods: ['-36.9413', '-36.9407', '-36.9412']
Best model has likelihood: -36.9407
time for generating output: 0.1031
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6adb3be6d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ae3cf8b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b058bfa60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69a44a3910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69d7f77fd0>, <__main__.SimpleDirichletPrior object at 0x7f6ac88405e0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 477.4169 - loglik: -4.7284e+02 - logprior: -4.5002e+00
Epoch 2/10
16/16 - 6s - loss: 439.7365 - loglik: -4.3779e+02 - logprior: -1.3385e+00
Epoch 3/10
16/16 - 6s - loss: 414.9813 - loglik: -4.1206e+02 - logprior: -1.7545e+00
Epoch 4/10
16/16 - 6s - loss: 405.4836 - loglik: -4.0222e+02 - logprior: -1.9484e+00
Epoch 5/10
16/16 - 6s - loss: 401.1844 - loglik: -3.9790e+02 - logprior: -1.9503e+00
Epoch 6/10
16/16 - 6s - loss: 398.4174 - loglik: -3.9530e+02 - logprior: -2.0158e+00
Epoch 7/10
16/16 - 6s - loss: 399.6501 - loglik: -3.9669e+02 - logprior: -2.0190e+00
Fitted a model with MAP estimate = -396.5161
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (49, 1), (53, 1), (56, 1), (57, 2), (69, 1), (71, 1), (72, 2), (74, 2), (82, 1), (94, 4), (95, 1), (99, 1), (114, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (130, 1), (139, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 413.2812 - loglik: -4.0868e+02 - logprior: -4.3421e+00
Epoch 2/2
33/33 - 9s - loss: 393.0278 - loglik: -3.9045e+02 - logprior: -1.6827e+00
Fitted a model with MAP estimate = -386.9843
expansions: [(166, 1), (178, 2)]
discards: [  0  32  33  71  73  90  96 119 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 406.1385 - loglik: -4.0185e+02 - logprior: -4.0826e+00
Epoch 2/2
33/33 - 9s - loss: 395.4129 - loglik: -3.9292e+02 - logprior: -1.4776e+00
Fitted a model with MAP estimate = -389.2457
expansions: [(0, 1), (112, 1), (171, 2)]
discards: [110 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 401.3706 - loglik: -3.9834e+02 - logprior: -2.8247e+00
Epoch 2/10
33/33 - 9s - loss: 391.6903 - loglik: -3.8974e+02 - logprior: -1.1854e+00
Epoch 3/10
33/33 - 9s - loss: 387.6057 - loglik: -3.8526e+02 - logprior: -1.1326e+00
Epoch 4/10
33/33 - 9s - loss: 383.6737 - loglik: -3.8123e+02 - logprior: -1.1190e+00
Epoch 5/10
33/33 - 9s - loss: 383.8745 - loglik: -3.8156e+02 - logprior: -1.0730e+00
Fitted a model with MAP estimate = -381.1306
Time for alignment: 177.2197
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 477.0259 - loglik: -4.7246e+02 - logprior: -4.4856e+00
Epoch 2/10
16/16 - 6s - loss: 439.2884 - loglik: -4.3735e+02 - logprior: -1.3333e+00
Epoch 3/10
16/16 - 6s - loss: 413.4224 - loglik: -4.1050e+02 - logprior: -1.7687e+00
Epoch 4/10
16/16 - 6s - loss: 403.7975 - loglik: -4.0037e+02 - logprior: -1.8950e+00
Epoch 5/10
16/16 - 6s - loss: 401.1307 - loglik: -3.9784e+02 - logprior: -1.8905e+00
Epoch 6/10
16/16 - 6s - loss: 397.5703 - loglik: -3.9448e+02 - logprior: -1.9866e+00
Epoch 7/10
16/16 - 6s - loss: 397.2637 - loglik: -3.9438e+02 - logprior: -2.0049e+00
Epoch 8/10
16/16 - 6s - loss: 396.0336 - loglik: -3.9322e+02 - logprior: -2.0244e+00
Epoch 9/10
16/16 - 6s - loss: 396.3053 - loglik: -3.9355e+02 - logprior: -2.0268e+00
Fitted a model with MAP estimate = -395.0734
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 2), (51, 1), (57, 1), (58, 1), (72, 1), (73, 2), (74, 1), (81, 1), (95, 4), (96, 2), (99, 1), (114, 1), (117, 2), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 417.2378 - loglik: -4.1254e+02 - logprior: -4.4092e+00
Epoch 2/2
33/33 - 9s - loss: 395.5965 - loglik: -3.9276e+02 - logprior: -1.7670e+00
Fitted a model with MAP estimate = -388.7225
expansions: []
discards: [  0  31  32  62  89 146 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 407.5932 - loglik: -4.0330e+02 - logprior: -4.0862e+00
Epoch 2/2
33/33 - 9s - loss: 395.5901 - loglik: -3.9335e+02 - logprior: -1.4655e+00
Fitted a model with MAP estimate = -390.0467
expansions: [(0, 1), (30, 2), (168, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 402.3442 - loglik: -3.9931e+02 - logprior: -2.8253e+00
Epoch 2/10
33/33 - 9s - loss: 391.9546 - loglik: -3.8985e+02 - logprior: -1.0939e+00
Epoch 3/10
33/33 - 9s - loss: 387.8711 - loglik: -3.8538e+02 - logprior: -1.0892e+00
Epoch 4/10
33/33 - 9s - loss: 384.4909 - loglik: -3.8203e+02 - logprior: -1.0659e+00
Epoch 5/10
33/33 - 9s - loss: 383.4667 - loglik: -3.8119e+02 - logprior: -1.0363e+00
Epoch 6/10
33/33 - 9s - loss: 382.7202 - loglik: -3.8062e+02 - logprior: -9.8723e-01
Epoch 7/10
33/33 - 9s - loss: 381.5111 - loglik: -3.7963e+02 - logprior: -9.1452e-01
Epoch 8/10
33/33 - 9s - loss: 380.8228 - loglik: -3.7910e+02 - logprior: -8.5146e-01
Epoch 9/10
33/33 - 9s - loss: 380.6813 - loglik: -3.7911e+02 - logprior: -7.8355e-01
Epoch 10/10
33/33 - 9s - loss: 380.4848 - loglik: -3.7904e+02 - logprior: -7.1869e-01
Fitted a model with MAP estimate = -379.1633
Time for alignment: 231.9916
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 476.8147 - loglik: -4.7225e+02 - logprior: -4.4837e+00
Epoch 2/10
16/16 - 6s - loss: 440.2688 - loglik: -4.3834e+02 - logprior: -1.3226e+00
Epoch 3/10
16/16 - 6s - loss: 415.1431 - loglik: -4.1224e+02 - logprior: -1.6940e+00
Epoch 4/10
16/16 - 6s - loss: 404.9224 - loglik: -4.0150e+02 - logprior: -1.8043e+00
Epoch 5/10
16/16 - 6s - loss: 400.8396 - loglik: -3.9750e+02 - logprior: -1.8930e+00
Epoch 6/10
16/16 - 6s - loss: 398.9512 - loglik: -3.9595e+02 - logprior: -1.9604e+00
Epoch 7/10
16/16 - 6s - loss: 396.4123 - loglik: -3.9359e+02 - logprior: -1.9523e+00
Epoch 8/10
16/16 - 6s - loss: 397.1623 - loglik: -3.9440e+02 - logprior: -1.9824e+00
Fitted a model with MAP estimate = -395.2902
expansions: [(15, 4), (20, 1), (23, 1), (28, 3), (29, 1), (42, 1), (44, 1), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (56, 1), (57, 1), (72, 2), (74, 3), (93, 1), (94, 4), (95, 1), (114, 2), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 417.2875 - loglik: -4.1264e+02 - logprior: -4.4124e+00
Epoch 2/2
33/33 - 9s - loss: 394.1496 - loglik: -3.9142e+02 - logprior: -1.8040e+00
Fitted a model with MAP estimate = -387.4023
expansions: [(180, 2)]
discards: [  0  15  16  34  35  63  66  98 177 178 179]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 405.5526 - loglik: -4.0124e+02 - logprior: -4.1056e+00
Epoch 2/2
33/33 - 9s - loss: 394.6534 - loglik: -3.9236e+02 - logprior: -1.5353e+00
Fitted a model with MAP estimate = -388.6118
expansions: [(0, 1), (31, 1), (33, 1), (171, 2)]
discards: [116 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 401.2699 - loglik: -3.9826e+02 - logprior: -2.7985e+00
Epoch 2/10
33/33 - 9s - loss: 391.2216 - loglik: -3.8914e+02 - logprior: -1.0876e+00
Epoch 3/10
33/33 - 9s - loss: 386.4326 - loglik: -3.8394e+02 - logprior: -1.0715e+00
Epoch 4/10
33/33 - 9s - loss: 383.6953 - loglik: -3.8124e+02 - logprior: -1.0609e+00
Epoch 5/10
33/33 - 9s - loss: 381.3327 - loglik: -3.7904e+02 - logprior: -1.0385e+00
Epoch 6/10
33/33 - 9s - loss: 382.6664 - loglik: -3.8055e+02 - logprior: -9.8435e-01
Fitted a model with MAP estimate = -379.5850
Time for alignment: 191.1143
Computed alignments with likelihoods: ['-381.1306', '-379.1633', '-379.5850']
Best model has likelihood: -379.1633
time for generating output: 0.2435
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7282850779510023
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ae3b9e130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e949af40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c5932e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a824b2520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6a83a31d90>, <__main__.SimpleDirichletPrior object at 0x7f6a79c49f40>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 291.4250 - loglik: -2.7612e+02 - logprior: -1.5303e+01
Epoch 2/10
10/10 - 2s - loss: 253.7679 - loglik: -2.4950e+02 - logprior: -4.2628e+00
Epoch 3/10
10/10 - 2s - loss: 227.0267 - loglik: -2.2441e+02 - logprior: -2.6043e+00
Epoch 4/10
10/10 - 2s - loss: 211.7268 - loglik: -2.0930e+02 - logprior: -2.2657e+00
Epoch 5/10
10/10 - 2s - loss: 205.5572 - loglik: -2.0303e+02 - logprior: -2.2349e+00
Epoch 6/10
10/10 - 2s - loss: 201.9770 - loglik: -1.9925e+02 - logprior: -2.3746e+00
Epoch 7/10
10/10 - 2s - loss: 200.5320 - loglik: -1.9787e+02 - logprior: -2.3488e+00
Epoch 8/10
10/10 - 2s - loss: 198.6280 - loglik: -1.9609e+02 - logprior: -2.3213e+00
Epoch 9/10
10/10 - 2s - loss: 200.2361 - loglik: -1.9773e+02 - logprior: -2.3268e+00
Fitted a model with MAP estimate = -198.4517
expansions: [(10, 2), (34, 1), (37, 1), (38, 1), (39, 1), (47, 3), (48, 1), (54, 1), (56, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 222.9287 - loglik: -2.0513e+02 - logprior: -1.7784e+01
Epoch 2/2
10/10 - 2s - loss: 203.0291 - loglik: -1.9497e+02 - logprior: -7.9450e+00
Fitted a model with MAP estimate = -198.8337
expansions: [(0, 11)]
discards: [ 0  9 41 53 54 66]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 212.0728 - loglik: -1.9725e+02 - logprior: -1.4805e+01
Epoch 2/2
10/10 - 2s - loss: 196.0881 - loglik: -1.9108e+02 - logprior: -4.9165e+00
Fitted a model with MAP estimate = -191.3131
expansions: []
discards: [0 1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 212.1883 - loglik: -1.9553e+02 - logprior: -1.6642e+01
Epoch 2/10
10/10 - 2s - loss: 199.1741 - loglik: -1.9383e+02 - logprior: -5.2652e+00
Epoch 3/10
10/10 - 2s - loss: 194.6853 - loglik: -1.9182e+02 - logprior: -2.6981e+00
Epoch 4/10
10/10 - 2s - loss: 193.8139 - loglik: -1.9164e+02 - logprior: -1.9379e+00
Epoch 5/10
10/10 - 2s - loss: 192.8193 - loglik: -1.9085e+02 - logprior: -1.7087e+00
Epoch 6/10
10/10 - 2s - loss: 190.3504 - loglik: -1.8862e+02 - logprior: -1.5155e+00
Epoch 7/10
10/10 - 2s - loss: 189.9674 - loglik: -1.8842e+02 - logprior: -1.3532e+00
Epoch 8/10
10/10 - 2s - loss: 190.7080 - loglik: -1.8922e+02 - logprior: -1.3029e+00
Fitted a model with MAP estimate = -189.7512
Time for alignment: 56.6910
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 292.2363 - loglik: -2.7693e+02 - logprior: -1.5304e+01
Epoch 2/10
10/10 - 2s - loss: 253.1418 - loglik: -2.4886e+02 - logprior: -4.2697e+00
Epoch 3/10
10/10 - 2s - loss: 228.0988 - loglik: -2.2547e+02 - logprior: -2.6165e+00
Epoch 4/10
10/10 - 2s - loss: 212.2556 - loglik: -2.0981e+02 - logprior: -2.2743e+00
Epoch 5/10
10/10 - 2s - loss: 206.4160 - loglik: -2.0382e+02 - logprior: -2.2890e+00
Epoch 6/10
10/10 - 2s - loss: 202.9333 - loglik: -2.0013e+02 - logprior: -2.4606e+00
Epoch 7/10
10/10 - 2s - loss: 200.5047 - loglik: -1.9786e+02 - logprior: -2.3999e+00
Epoch 8/10
10/10 - 2s - loss: 200.7327 - loglik: -1.9823e+02 - logprior: -2.3127e+00
Fitted a model with MAP estimate = -200.2602
expansions: [(21, 2), (34, 1), (37, 1), (39, 1), (44, 1), (45, 1), (48, 2), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 224.5003 - loglik: -2.0669e+02 - logprior: -1.7790e+01
Epoch 2/2
10/10 - 2s - loss: 205.0690 - loglik: -1.9701e+02 - logprior: -7.9301e+00
Fitted a model with MAP estimate = -202.1711
expansions: []
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 217.1571 - loglik: -1.9977e+02 - logprior: -1.7368e+01
Epoch 2/2
10/10 - 2s - loss: 200.4250 - loglik: -1.9383e+02 - logprior: -6.5075e+00
Fitted a model with MAP estimate = -198.7125
expansions: [(0, 11)]
discards: [20]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 212.9422 - loglik: -1.9813e+02 - logprior: -1.4800e+01
Epoch 2/10
10/10 - 2s - loss: 197.2867 - loglik: -1.9226e+02 - logprior: -4.9674e+00
Epoch 3/10
10/10 - 2s - loss: 191.6706 - loglik: -1.8860e+02 - logprior: -2.9601e+00
Epoch 4/10
10/10 - 2s - loss: 188.7540 - loglik: -1.8627e+02 - logprior: -2.3238e+00
Epoch 5/10
10/10 - 2s - loss: 188.7342 - loglik: -1.8644e+02 - logprior: -2.1002e+00
Epoch 6/10
10/10 - 2s - loss: 186.3855 - loglik: -1.8414e+02 - logprior: -2.0460e+00
Epoch 7/10
10/10 - 2s - loss: 185.6018 - loglik: -1.8343e+02 - logprior: -1.9828e+00
Epoch 8/10
10/10 - 2s - loss: 184.6166 - loglik: -1.8248e+02 - logprior: -1.9393e+00
Epoch 9/10
10/10 - 2s - loss: 186.9822 - loglik: -1.8485e+02 - logprior: -1.9286e+00
Fitted a model with MAP estimate = -185.0748
Time for alignment: 57.8944
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 291.0637 - loglik: -2.7576e+02 - logprior: -1.5304e+01
Epoch 2/10
10/10 - 2s - loss: 254.8478 - loglik: -2.5057e+02 - logprior: -4.2685e+00
Epoch 3/10
10/10 - 2s - loss: 225.6318 - loglik: -2.2302e+02 - logprior: -2.5985e+00
Epoch 4/10
10/10 - 2s - loss: 213.2332 - loglik: -2.1081e+02 - logprior: -2.2668e+00
Epoch 5/10
10/10 - 2s - loss: 207.0908 - loglik: -2.0452e+02 - logprior: -2.2939e+00
Epoch 6/10
10/10 - 2s - loss: 204.9157 - loglik: -2.0218e+02 - logprior: -2.4430e+00
Epoch 7/10
10/10 - 2s - loss: 202.5533 - loglik: -1.9998e+02 - logprior: -2.3426e+00
Epoch 8/10
10/10 - 2s - loss: 202.6225 - loglik: -2.0017e+02 - logprior: -2.2645e+00
Fitted a model with MAP estimate = -202.0838
expansions: [(21, 1), (33, 2), (34, 2), (37, 1), (39, 1), (44, 1), (45, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 225.6305 - loglik: -2.0789e+02 - logprior: -1.7717e+01
Epoch 2/2
10/10 - 2s - loss: 207.6793 - loglik: -1.9965e+02 - logprior: -7.9053e+00
Fitted a model with MAP estimate = -202.0644
expansions: []
discards: [ 0 36 44]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 215.6844 - loglik: -1.9835e+02 - logprior: -1.7314e+01
Epoch 2/2
10/10 - 2s - loss: 202.5710 - loglik: -1.9613e+02 - logprior: -6.3474e+00
Fitted a model with MAP estimate = -198.4496
expansions: [(0, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 209.6410 - loglik: -1.9504e+02 - logprior: -1.4579e+01
Epoch 2/10
10/10 - 2s - loss: 195.0425 - loglik: -1.9030e+02 - logprior: -4.6666e+00
Epoch 3/10
10/10 - 2s - loss: 189.0023 - loglik: -1.8606e+02 - logprior: -2.7899e+00
Epoch 4/10
10/10 - 2s - loss: 187.8416 - loglik: -1.8537e+02 - logprior: -2.2400e+00
Epoch 5/10
10/10 - 2s - loss: 187.7889 - loglik: -1.8543e+02 - logprior: -2.1021e+00
Epoch 6/10
10/10 - 2s - loss: 183.6553 - loglik: -1.8143e+02 - logprior: -1.9969e+00
Epoch 7/10
10/10 - 2s - loss: 185.9524 - loglik: -1.8389e+02 - logprior: -1.8662e+00
Fitted a model with MAP estimate = -184.6938
Time for alignment: 51.7467
Computed alignments with likelihoods: ['-189.7512', '-185.0748', '-184.6938']
Best model has likelihood: -184.6938
time for generating output: 0.1976
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.6677662582469368
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ab6466910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aa49f6ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb7cd730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69a5249580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6adb6bbdc0>, <__main__.SimpleDirichletPrior object at 0x7f6a79e7ae80>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 618.3569 - loglik: -5.6192e+02 - logprior: -5.6422e+01
Epoch 2/10
10/10 - 3s - loss: 521.0654 - loglik: -5.0865e+02 - logprior: -1.2393e+01
Epoch 3/10
10/10 - 3s - loss: 460.2517 - loglik: -4.5477e+02 - logprior: -5.3832e+00
Epoch 4/10
10/10 - 3s - loss: 424.0772 - loglik: -4.2005e+02 - logprior: -3.7853e+00
Epoch 5/10
10/10 - 3s - loss: 406.1571 - loglik: -4.0242e+02 - logprior: -3.4860e+00
Epoch 6/10
10/10 - 3s - loss: 399.7619 - loglik: -3.9614e+02 - logprior: -3.2138e+00
Epoch 7/10
10/10 - 3s - loss: 397.9995 - loglik: -3.9457e+02 - logprior: -2.9737e+00
Epoch 8/10
10/10 - 3s - loss: 395.5125 - loglik: -3.9232e+02 - logprior: -2.7607e+00
Epoch 9/10
10/10 - 3s - loss: 395.1172 - loglik: -3.9210e+02 - logprior: -2.5622e+00
Epoch 10/10
10/10 - 3s - loss: 392.4601 - loglik: -3.8952e+02 - logprior: -2.4781e+00
Fitted a model with MAP estimate = -392.9368
expansions: [(0, 3), (19, 2), (20, 2), (22, 2), (23, 1), (24, 1), (30, 3), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (81, 2), (94, 1), (101, 1), (106, 1), (107, 2), (114, 1), (116, 3), (129, 1), (145, 4), (151, 3), (154, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 488.0928 - loglik: -4.1469e+02 - logprior: -7.3384e+01
Epoch 2/2
10/10 - 4s - loss: 405.1874 - loglik: -3.8451e+02 - logprior: -2.0504e+01
Fitted a model with MAP estimate = -386.9168
expansions: [(96, 1)]
discards: [  0   1   2  23  25  29  70 103 146 189 190]
Re-initialized the encoder parameters.
Fitting a model of length 198 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 440.5509 - loglik: -3.8716e+02 - logprior: -5.3367e+01
Epoch 2/2
10/10 - 3s - loss: 391.7139 - loglik: -3.7952e+02 - logprior: -1.2046e+01
Fitted a model with MAP estimate = -381.2787
expansions: [(0, 9), (174, 1)]
discards: [54]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 451.0270 - loglik: -3.8203e+02 - logprior: -6.8983e+01
Epoch 2/10
10/10 - 4s - loss: 397.5025 - loglik: -3.8005e+02 - logprior: -1.7384e+01
Epoch 3/10
10/10 - 4s - loss: 376.5369 - loglik: -3.7109e+02 - logprior: -5.1729e+00
Epoch 4/10
10/10 - 4s - loss: 369.4990 - loglik: -3.6812e+02 - logprior: -7.7862e-01
Epoch 5/10
10/10 - 3s - loss: 361.0810 - loglik: -3.6173e+02 - logprior: 1.4278
Epoch 6/10
10/10 - 4s - loss: 359.7824 - loglik: -3.6181e+02 - logprior: 2.7137
Epoch 7/10
10/10 - 4s - loss: 357.7454 - loglik: -3.6062e+02 - logprior: 3.4460
Epoch 8/10
10/10 - 4s - loss: 355.1234 - loglik: -3.5860e+02 - logprior: 4.0231
Epoch 9/10
10/10 - 4s - loss: 354.9564 - loglik: -3.5888e+02 - logprior: 4.4659
Epoch 10/10
10/10 - 4s - loss: 353.2301 - loglik: -3.5762e+02 - logprior: 4.9120
Fitted a model with MAP estimate = -352.8240
Time for alignment: 103.3826
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 617.2250 - loglik: -5.6079e+02 - logprior: -5.6423e+01
Epoch 2/10
10/10 - 3s - loss: 519.9489 - loglik: -5.0756e+02 - logprior: -1.2371e+01
Epoch 3/10
10/10 - 3s - loss: 457.3195 - loglik: -4.5220e+02 - logprior: -5.0487e+00
Epoch 4/10
10/10 - 3s - loss: 422.6359 - loglik: -4.1945e+02 - logprior: -2.9323e+00
Epoch 5/10
10/10 - 3s - loss: 411.0675 - loglik: -4.0821e+02 - logprior: -2.3965e+00
Epoch 6/10
10/10 - 3s - loss: 405.2325 - loglik: -4.0260e+02 - logprior: -1.9072e+00
Epoch 7/10
10/10 - 3s - loss: 402.5403 - loglik: -4.0031e+02 - logprior: -1.5432e+00
Epoch 8/10
10/10 - 3s - loss: 400.2118 - loglik: -3.9827e+02 - logprior: -1.4146e+00
Epoch 9/10
10/10 - 3s - loss: 400.6064 - loglik: -3.9877e+02 - logprior: -1.3530e+00
Fitted a model with MAP estimate = -399.0643
expansions: [(0, 3), (23, 3), (24, 1), (25, 2), (30, 3), (44, 3), (46, 1), (52, 1), (76, 3), (81, 1), (103, 1), (104, 1), (107, 2), (114, 1), (117, 3), (119, 1), (150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 198 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 493.1689 - loglik: -4.1958e+02 - logprior: -7.3562e+01
Epoch 2/2
10/10 - 3s - loss: 414.2511 - loglik: -3.9357e+02 - logprior: -2.0511e+01
Fitted a model with MAP estimate = -399.3659
expansions: []
discards: [ 0  1  2 27 32 39 59 94]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 454.0195 - loglik: -4.0038e+02 - logprior: -5.3632e+01
Epoch 2/2
10/10 - 3s - loss: 404.3246 - loglik: -3.9186e+02 - logprior: -1.2381e+01
Fitted a model with MAP estimate = -395.6317
expansions: [(0, 12)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 465.1396 - loglik: -3.9618e+02 - logprior: -6.8949e+01
Epoch 2/10
10/10 - 3s - loss: 409.7805 - loglik: -3.9221e+02 - logprior: -1.7544e+01
Epoch 3/10
10/10 - 3s - loss: 394.5891 - loglik: -3.8930e+02 - logprior: -5.1834e+00
Epoch 4/10
10/10 - 3s - loss: 384.4879 - loglik: -3.8356e+02 - logprior: -5.7933e-01
Epoch 5/10
10/10 - 3s - loss: 379.1920 - loglik: -3.8003e+02 - logprior: 1.4000
Epoch 6/10
10/10 - 3s - loss: 375.2978 - loglik: -3.7724e+02 - logprior: 2.5404
Epoch 7/10
10/10 - 3s - loss: 374.1887 - loglik: -3.7701e+02 - logprior: 3.4123
Epoch 8/10
10/10 - 3s - loss: 372.8720 - loglik: -3.7627e+02 - logprior: 3.9892
Epoch 9/10
10/10 - 3s - loss: 371.9205 - loglik: -3.7574e+02 - logprior: 4.3944
Epoch 10/10
10/10 - 3s - loss: 370.2283 - loglik: -3.7452e+02 - logprior: 4.8493
Fitted a model with MAP estimate = -369.8142
Time for alignment: 96.9789
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 617.2743 - loglik: -5.6084e+02 - logprior: -5.6421e+01
Epoch 2/10
10/10 - 3s - loss: 521.7850 - loglik: -5.0934e+02 - logprior: -1.2421e+01
Epoch 3/10
10/10 - 3s - loss: 461.3118 - loglik: -4.5572e+02 - logprior: -5.4910e+00
Epoch 4/10
10/10 - 3s - loss: 426.7872 - loglik: -4.2272e+02 - logprior: -3.7997e+00
Epoch 5/10
10/10 - 3s - loss: 411.5760 - loglik: -4.0761e+02 - logprior: -3.5799e+00
Epoch 6/10
10/10 - 3s - loss: 404.3511 - loglik: -4.0042e+02 - logprior: -3.2957e+00
Epoch 7/10
10/10 - 3s - loss: 399.4966 - loglik: -3.9577e+02 - logprior: -3.0572e+00
Epoch 8/10
10/10 - 3s - loss: 399.0319 - loglik: -3.9545e+02 - logprior: -3.0173e+00
Epoch 9/10
10/10 - 3s - loss: 396.2277 - loglik: -3.9278e+02 - logprior: -2.9204e+00
Epoch 10/10
10/10 - 3s - loss: 397.1353 - loglik: -3.9387e+02 - logprior: -2.7510e+00
Fitted a model with MAP estimate = -395.4366
expansions: [(9, 1), (15, 1), (19, 2), (20, 1), (22, 2), (23, 1), (31, 2), (32, 1), (43, 1), (44, 1), (45, 2), (46, 1), (52, 2), (76, 3), (81, 2), (94, 1), (101, 1), (106, 1), (107, 2), (114, 1), (117, 4), (119, 1), (145, 4), (153, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 481.0995 - loglik: -4.1556e+02 - logprior: -6.5519e+01
Epoch 2/2
10/10 - 4s - loss: 413.8297 - loglik: -3.8796e+02 - logprior: -2.5706e+01
Fitted a model with MAP estimate = -399.2817
expansions: [(0, 10), (10, 1), (179, 1)]
discards: [  0  21  38  67  94 101 145 146]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 440.8869 - loglik: -3.8967e+02 - logprior: -5.1205e+01
Epoch 2/2
10/10 - 4s - loss: 391.7681 - loglik: -3.7970e+02 - logprior: -1.1964e+01
Fitted a model with MAP estimate = -381.9611
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  65 194]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 433.7569 - loglik: -3.8382e+02 - logprior: -4.9924e+01
Epoch 2/10
10/10 - 3s - loss: 389.5545 - loglik: -3.7871e+02 - logprior: -1.0815e+01
Epoch 3/10
10/10 - 3s - loss: 379.4424 - loglik: -3.7654e+02 - logprior: -2.7837e+00
Epoch 4/10
10/10 - 3s - loss: 370.4073 - loglik: -3.7052e+02 - logprior: 0.4748
Epoch 5/10
10/10 - 3s - loss: 366.0300 - loglik: -3.6773e+02 - logprior: 2.3215
Epoch 6/10
10/10 - 3s - loss: 361.7268 - loglik: -3.6445e+02 - logprior: 3.4360
Epoch 7/10
10/10 - 3s - loss: 361.6707 - loglik: -3.6501e+02 - logprior: 4.0294
Epoch 8/10
10/10 - 3s - loss: 357.6507 - loglik: -3.6147e+02 - logprior: 4.4782
Epoch 9/10
10/10 - 3s - loss: 358.6301 - loglik: -3.6286e+02 - logprior: 4.8843
Fitted a model with MAP estimate = -356.4627
Time for alignment: 99.0387
Computed alignments with likelihoods: ['-352.8240', '-369.8142', '-356.4627']
Best model has likelihood: -352.8240
time for generating output: 0.2801
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7896794370602033
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f698b70f760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ab650d760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6972334e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a9c0096d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16f0ce50>, <__main__.SimpleDirichletPrior object at 0x7f6a83d114f0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.5962 - loglik: -2.3532e+02 - logprior: -3.2235e+00
Epoch 2/10
19/19 - 2s - loss: 206.6930 - loglik: -2.0517e+02 - logprior: -1.3076e+00
Epoch 3/10
19/19 - 2s - loss: 195.5347 - loglik: -1.9365e+02 - logprior: -1.5256e+00
Epoch 4/10
19/19 - 2s - loss: 193.3013 - loglik: -1.9153e+02 - logprior: -1.4456e+00
Epoch 5/10
19/19 - 2s - loss: 192.7104 - loglik: -1.9100e+02 - logprior: -1.4139e+00
Epoch 6/10
19/19 - 2s - loss: 192.4544 - loglik: -1.9080e+02 - logprior: -1.3911e+00
Epoch 7/10
19/19 - 2s - loss: 192.2267 - loglik: -1.9059e+02 - logprior: -1.3805e+00
Epoch 8/10
19/19 - 2s - loss: 191.7719 - loglik: -1.9016e+02 - logprior: -1.3732e+00
Epoch 9/10
19/19 - 2s - loss: 192.0932 - loglik: -1.9049e+02 - logprior: -1.3743e+00
Fitted a model with MAP estimate = -185.5051
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 1), (18, 3), (22, 2), (24, 1), (46, 1), (47, 1), (48, 2), (49, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 195.4823 - loglik: -1.9241e+02 - logprior: -2.9864e+00
Epoch 2/2
19/19 - 2s - loss: 188.0667 - loglik: -1.8639e+02 - logprior: -1.2518e+00
Fitted a model with MAP estimate = -179.4369
expansions: [(22, 4)]
discards: [ 0 31]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 192.6292 - loglik: -1.8865e+02 - logprior: -3.9044e+00
Epoch 2/2
19/19 - 2s - loss: 187.7918 - loglik: -1.8530e+02 - logprior: -2.1601e+00
Fitted a model with MAP estimate = -178.8197
expansions: [(0, 4), (28, 1), (29, 1)]
discards: [ 0 21 22 23 24]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 181.3093 - loglik: -1.7925e+02 - logprior: -1.9723e+00
Epoch 2/10
23/23 - 3s - loss: 178.1092 - loglik: -1.7667e+02 - logprior: -1.0913e+00
Epoch 3/10
23/23 - 3s - loss: 176.7757 - loglik: -1.7516e+02 - logprior: -1.1037e+00
Epoch 4/10
23/23 - 3s - loss: 175.9887 - loglik: -1.7436e+02 - logprior: -1.0711e+00
Epoch 5/10
23/23 - 3s - loss: 175.6546 - loglik: -1.7409e+02 - logprior: -1.0562e+00
Epoch 6/10
23/23 - 3s - loss: 175.2806 - loglik: -1.7376e+02 - logprior: -1.0381e+00
Epoch 7/10
23/23 - 3s - loss: 174.7602 - loglik: -1.7326e+02 - logprior: -1.0373e+00
Epoch 8/10
23/23 - 3s - loss: 174.8382 - loglik: -1.7335e+02 - logprior: -1.0175e+00
Fitted a model with MAP estimate = -174.0895
Time for alignment: 87.2384
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.6037 - loglik: -2.3533e+02 - logprior: -3.2271e+00
Epoch 2/10
19/19 - 2s - loss: 207.5264 - loglik: -2.0595e+02 - logprior: -1.2948e+00
Epoch 3/10
19/19 - 2s - loss: 196.7834 - loglik: -1.9483e+02 - logprior: -1.4789e+00
Epoch 4/10
19/19 - 2s - loss: 193.9132 - loglik: -1.9200e+02 - logprior: -1.4492e+00
Epoch 5/10
19/19 - 2s - loss: 193.0341 - loglik: -1.9127e+02 - logprior: -1.4195e+00
Epoch 6/10
19/19 - 2s - loss: 192.5502 - loglik: -1.9084e+02 - logprior: -1.3951e+00
Epoch 7/10
19/19 - 2s - loss: 192.3496 - loglik: -1.9066e+02 - logprior: -1.3780e+00
Epoch 8/10
19/19 - 2s - loss: 192.0124 - loglik: -1.9033e+02 - logprior: -1.3733e+00
Epoch 9/10
19/19 - 2s - loss: 192.0894 - loglik: -1.9043e+02 - logprior: -1.3703e+00
Fitted a model with MAP estimate = -185.5196
expansions: [(0, 2), (3, 1), (6, 1), (18, 6), (25, 1), (47, 1), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.3422 - loglik: -1.9285e+02 - logprior: -4.4077e+00
Epoch 2/2
19/19 - 2s - loss: 187.7326 - loglik: -1.8577e+02 - logprior: -1.5687e+00
Fitted a model with MAP estimate = -178.6375
expansions: [(7, 1), (20, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 191.1708 - loglik: -1.8708e+02 - logprior: -4.0092e+00
Epoch 2/2
19/19 - 2s - loss: 186.5041 - loglik: -1.8453e+02 - logprior: -1.6800e+00
Fitted a model with MAP estimate = -177.9560
expansions: [(28, 1)]
discards: [0 1 9]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 181.8400 - loglik: -1.7950e+02 - logprior: -2.2479e+00
Epoch 2/10
23/23 - 3s - loss: 178.2671 - loglik: -1.7690e+02 - logprior: -1.0292e+00
Epoch 3/10
23/23 - 3s - loss: 177.2283 - loglik: -1.7567e+02 - logprior: -1.0287e+00
Epoch 4/10
23/23 - 3s - loss: 176.2621 - loglik: -1.7470e+02 - logprior: -1.0174e+00
Epoch 5/10
23/23 - 3s - loss: 175.7701 - loglik: -1.7428e+02 - logprior: -9.9982e-01
Epoch 6/10
23/23 - 3s - loss: 175.6743 - loglik: -1.7423e+02 - logprior: -9.8222e-01
Epoch 7/10
23/23 - 3s - loss: 174.8532 - loglik: -1.7342e+02 - logprior: -9.6980e-01
Epoch 8/10
23/23 - 3s - loss: 175.5839 - loglik: -1.7416e+02 - logprior: -9.5913e-01
Fitted a model with MAP estimate = -174.3204
Time for alignment: 84.0241
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 238.4943 - loglik: -2.3521e+02 - logprior: -3.2253e+00
Epoch 2/10
19/19 - 2s - loss: 208.3474 - loglik: -2.0670e+02 - logprior: -1.3097e+00
Epoch 3/10
19/19 - 2s - loss: 196.5986 - loglik: -1.9451e+02 - logprior: -1.4737e+00
Epoch 4/10
19/19 - 2s - loss: 194.3121 - loglik: -1.9246e+02 - logprior: -1.4337e+00
Epoch 5/10
19/19 - 2s - loss: 193.2883 - loglik: -1.9152e+02 - logprior: -1.4155e+00
Epoch 6/10
19/19 - 2s - loss: 192.6686 - loglik: -1.9097e+02 - logprior: -1.4140e+00
Epoch 7/10
19/19 - 2s - loss: 192.4678 - loglik: -1.9081e+02 - logprior: -1.3786e+00
Epoch 8/10
19/19 - 2s - loss: 192.0163 - loglik: -1.9036e+02 - logprior: -1.3737e+00
Epoch 9/10
19/19 - 2s - loss: 191.9601 - loglik: -1.9032e+02 - logprior: -1.3682e+00
Epoch 10/10
19/19 - 2s - loss: 191.8511 - loglik: -1.9021e+02 - logprior: -1.3616e+00
Fitted a model with MAP estimate = -185.7608
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 4), (18, 2), (23, 1), (36, 1), (46, 1), (47, 1), (48, 2), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.3057 - loglik: -1.9282e+02 - logprior: -4.4080e+00
Epoch 2/2
19/19 - 2s - loss: 187.5997 - loglik: -1.8563e+02 - logprior: -1.5626e+00
Fitted a model with MAP estimate = -178.3433
expansions: [(9, 2)]
discards: [ 0 23 24 25]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 191.7060 - loglik: -1.8763e+02 - logprior: -3.9961e+00
Epoch 2/2
19/19 - 2s - loss: 186.7096 - loglik: -1.8473e+02 - logprior: -1.6752e+00
Fitted a model with MAP estimate = -177.9605
expansions: [(24, 3), (25, 1)]
discards: [ 0  1  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 182.3619 - loglik: -1.7991e+02 - logprior: -2.3503e+00
Epoch 2/10
23/23 - 3s - loss: 178.0628 - loglik: -1.7665e+02 - logprior: -1.0689e+00
Epoch 3/10
23/23 - 3s - loss: 176.6986 - loglik: -1.7509e+02 - logprior: -1.0529e+00
Epoch 4/10
23/23 - 3s - loss: 175.8298 - loglik: -1.7427e+02 - logprior: -1.0278e+00
Epoch 5/10
23/23 - 3s - loss: 175.7477 - loglik: -1.7424e+02 - logprior: -1.0240e+00
Epoch 6/10
23/23 - 3s - loss: 175.0069 - loglik: -1.7353e+02 - logprior: -1.0160e+00
Epoch 7/10
23/23 - 3s - loss: 175.0011 - loglik: -1.7355e+02 - logprior: -1.0021e+00
Epoch 8/10
23/23 - 3s - loss: 174.7607 - loglik: -1.7331e+02 - logprior: -9.8727e-01
Epoch 9/10
23/23 - 3s - loss: 174.6902 - loglik: -1.7325e+02 - logprior: -9.7079e-01
Epoch 10/10
23/23 - 3s - loss: 174.3159 - loglik: -1.7290e+02 - logprior: -9.4714e-01
Fitted a model with MAP estimate = -173.4584
Time for alignment: 93.6952
Computed alignments with likelihoods: ['-174.0895', '-174.3204', '-173.4584']
Best model has likelihood: -173.4584
time for generating output: 0.1679
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.9324116743471582
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b05e82490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac9a7bb50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a825d9490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69be619430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b2fca0b50>, <__main__.SimpleDirichletPrior object at 0x7f6971e27820>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 797.5862 - loglik: -7.9047e+02 - logprior: -6.9892e+00
Epoch 2/10
22/22 - 16s - loss: 685.5379 - loglik: -6.8311e+02 - logprior: -1.9725e+00
Epoch 3/10
22/22 - 16s - loss: 644.3887 - loglik: -6.4010e+02 - logprior: -3.6894e+00
Epoch 4/10
22/22 - 16s - loss: 635.7356 - loglik: -6.3126e+02 - logprior: -3.6571e+00
Epoch 5/10
22/22 - 16s - loss: 635.0222 - loglik: -6.3055e+02 - logprior: -3.6388e+00
Epoch 6/10
22/22 - 16s - loss: 632.4218 - loglik: -6.2790e+02 - logprior: -3.6914e+00
Epoch 7/10
22/22 - 16s - loss: 634.6536 - loglik: -6.3013e+02 - logprior: -3.7370e+00
Fitted a model with MAP estimate = -631.4289
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 1), (49, 1), (50, 1), (51, 1), (66, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (98, 1), (103, 1), (104, 2), (106, 1), (107, 1), (117, 1), (119, 2), (143, 1), (144, 1), (148, 1), (152, 1), (155, 1), (158, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 2), (189, 1), (193, 1), (207, 1), (208, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 649.3193 - loglik: -6.3904e+02 - logprior: -1.0182e+01
Epoch 2/2
22/22 - 21s - loss: 622.3150 - loglik: -6.1763e+02 - logprior: -4.1408e+00
Fitted a model with MAP estimate = -614.6121
expansions: [(0, 3)]
discards: [  0  34 219 228 270]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 627.0505 - loglik: -6.2054e+02 - logprior: -6.3973e+00
Epoch 2/2
22/22 - 21s - loss: 613.3005 - loglik: -6.1217e+02 - logprior: -5.8174e-01
Fitted a model with MAP estimate = -610.2203
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 24s - loss: 630.9837 - loglik: -6.2193e+02 - logprior: -8.9471e+00
Epoch 2/10
22/22 - 20s - loss: 617.1689 - loglik: -6.1346e+02 - logprior: -3.1728e+00
Epoch 3/10
22/22 - 20s - loss: 613.6761 - loglik: -6.1095e+02 - logprior: -1.7165e+00
Epoch 4/10
22/22 - 20s - loss: 609.3162 - loglik: -6.0855e+02 - logprior: 0.4330
Epoch 5/10
22/22 - 20s - loss: 606.4744 - loglik: -6.0591e+02 - logprior: 0.5946
Epoch 6/10
22/22 - 20s - loss: 604.8340 - loglik: -6.0456e+02 - logprior: 0.7756
Epoch 7/10
22/22 - 20s - loss: 605.0406 - loglik: -6.0507e+02 - logprior: 0.9792
Fitted a model with MAP estimate = -602.5366
Time for alignment: 409.5726
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 797.4785 - loglik: -7.9038e+02 - logprior: -6.9698e+00
Epoch 2/10
22/22 - 16s - loss: 689.9437 - loglik: -6.8753e+02 - logprior: -1.9304e+00
Epoch 3/10
22/22 - 16s - loss: 643.2465 - loglik: -6.3904e+02 - logprior: -3.6199e+00
Epoch 4/10
22/22 - 16s - loss: 638.0386 - loglik: -6.3377e+02 - logprior: -3.4421e+00
Epoch 5/10
22/22 - 16s - loss: 634.4233 - loglik: -6.3009e+02 - logprior: -3.4583e+00
Epoch 6/10
22/22 - 16s - loss: 634.4623 - loglik: -6.3010e+02 - logprior: -3.4760e+00
Fitted a model with MAP estimate = -632.2559
expansions: [(12, 1), (13, 1), (32, 1), (33, 3), (34, 3), (36, 1), (45, 1), (46, 1), (48, 1), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (156, 2), (176, 1), (179, 2), (180, 4), (184, 2), (185, 3), (192, 1), (193, 1), (206, 1), (207, 1), (208, 1), (209, 1), (214, 2), (224, 2), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 649.8008 - loglik: -6.3943e+02 - logprior: -1.0270e+01
Epoch 2/2
22/22 - 21s - loss: 622.2998 - loglik: -6.1747e+02 - logprior: -4.2573e+00
Fitted a model with MAP estimate = -614.1680
expansions: [(0, 3), (196, 1)]
discards: [  0  35  40 176 224 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 626.9308 - loglik: -6.2033e+02 - logprior: -6.4957e+00
Epoch 2/2
22/22 - 21s - loss: 615.2366 - loglik: -6.1399e+02 - logprior: -7.0376e-01
Fitted a model with MAP estimate = -610.0373
expansions: []
discards: [  0   1   2 152 282]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 631.9963 - loglik: -6.2285e+02 - logprior: -9.0364e+00
Epoch 2/10
22/22 - 20s - loss: 619.5030 - loglik: -6.1577e+02 - logprior: -3.1838e+00
Epoch 3/10
22/22 - 20s - loss: 614.9470 - loglik: -6.1221e+02 - logprior: -1.7196e+00
Epoch 4/10
22/22 - 20s - loss: 605.9224 - loglik: -6.0522e+02 - logprior: 0.4893
Epoch 5/10
22/22 - 20s - loss: 605.4509 - loglik: -6.0483e+02 - logprior: 0.5428
Epoch 6/10
22/22 - 20s - loss: 607.4398 - loglik: -6.0710e+02 - logprior: 0.7241
Fitted a model with MAP estimate = -603.1437
Time for alignment: 372.7699
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 798.5020 - loglik: -7.9141e+02 - logprior: -6.9631e+00
Epoch 2/10
22/22 - 16s - loss: 686.9558 - loglik: -6.8457e+02 - logprior: -1.9134e+00
Epoch 3/10
22/22 - 16s - loss: 644.0314 - loglik: -6.3968e+02 - logprior: -3.4266e+00
Epoch 4/10
22/22 - 16s - loss: 638.1853 - loglik: -6.3384e+02 - logprior: -3.3035e+00
Epoch 5/10
22/22 - 16s - loss: 634.1099 - loglik: -6.2982e+02 - logprior: -3.3521e+00
Epoch 6/10
22/22 - 16s - loss: 632.8995 - loglik: -6.2848e+02 - logprior: -3.4980e+00
Epoch 7/10
22/22 - 16s - loss: 632.6628 - loglik: -6.2816e+02 - logprior: -3.5719e+00
Epoch 8/10
22/22 - 16s - loss: 631.2125 - loglik: -6.2679e+02 - logprior: -3.5748e+00
Epoch 9/10
22/22 - 16s - loss: 630.9968 - loglik: -6.2662e+02 - logprior: -3.5909e+00
Epoch 10/10
22/22 - 16s - loss: 632.0569 - loglik: -6.2776e+02 - logprior: -3.5482e+00
Fitted a model with MAP estimate = -629.4420
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 2), (48, 2), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 1), (77, 1), (82, 1), (98, 1), (99, 1), (104, 1), (105, 2), (109, 1), (119, 1), (121, 2), (137, 1), (144, 1), (145, 1), (149, 1), (150, 1), (156, 3), (159, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (186, 1), (194, 2), (206, 1), (207, 1), (208, 1), (209, 1), (213, 2), (214, 2), (223, 2), (225, 1), (236, 1), (238, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 647.5312 - loglik: -6.4011e+02 - logprior: -7.3121e+00
Epoch 2/2
22/22 - 21s - loss: 619.4695 - loglik: -6.1768e+02 - logprior: -1.2665e+00
Fitted a model with MAP estimate = -611.7526
expansions: [(245, 1)]
discards: [ 34 218 228 283]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 25s - loss: 627.2702 - loglik: -6.2072e+02 - logprior: -6.4439e+00
Epoch 2/2
22/22 - 21s - loss: 611.6453 - loglik: -6.1055e+02 - logprior: -5.0422e-01
Fitted a model with MAP estimate = -610.1844
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 24s - loss: 626.5014 - loglik: -6.2035e+02 - logprior: -6.0393e+00
Epoch 2/10
22/22 - 21s - loss: 612.5325 - loglik: -6.1184e+02 - logprior: -1.6884e-01
Epoch 3/10
22/22 - 20s - loss: 608.7908 - loglik: -6.0820e+02 - logprior: 0.3097
Epoch 4/10
22/22 - 21s - loss: 605.1287 - loglik: -6.0456e+02 - logprior: 0.5159
Epoch 5/10
22/22 - 21s - loss: 607.1362 - loglik: -6.0671e+02 - logprior: 0.6701
Fitted a model with MAP estimate = -602.4862
Time for alignment: 418.4066
Computed alignments with likelihoods: ['-602.5366', '-603.1437', '-602.4862']
Best model has likelihood: -602.4862
time for generating output: 0.3669
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9146675253582354
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69a4a3e970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e97dae20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2d37250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69721f8280>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac92f0c40>, <__main__.SimpleDirichletPrior object at 0x7f6ac80795b0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.9586 - loglik: -7.9870e+01 - logprior: -9.3067e+01
Epoch 2/10
10/10 - 1s - loss: 95.8987 - loglik: -6.8621e+01 - logprior: -2.7255e+01
Epoch 3/10
10/10 - 1s - loss: 73.8781 - loglik: -5.9895e+01 - logprior: -1.3973e+01
Epoch 4/10
10/10 - 1s - loss: 64.7453 - loglik: -5.5874e+01 - logprior: -8.8486e+00
Epoch 5/10
10/10 - 1s - loss: 60.0173 - loglik: -5.3776e+01 - logprior: -6.2242e+00
Epoch 6/10
10/10 - 1s - loss: 57.9070 - loglik: -5.3113e+01 - logprior: -4.7750e+00
Epoch 7/10
10/10 - 1s - loss: 56.7548 - loglik: -5.2715e+01 - logprior: -3.9651e+00
Epoch 8/10
10/10 - 1s - loss: 56.1002 - loglik: -5.2487e+01 - logprior: -3.4525e+00
Epoch 9/10
10/10 - 1s - loss: 55.7575 - loglik: -5.2465e+01 - logprior: -3.1402e+00
Epoch 10/10
10/10 - 1s - loss: 55.5038 - loglik: -5.2440e+01 - logprior: -2.9265e+00
Fitted a model with MAP estimate = -55.2584
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 173.8644 - loglik: -4.9283e+01 - logprior: -1.2457e+02
Epoch 2/2
10/10 - 1s - loss: 87.1575 - loglik: -4.6130e+01 - logprior: -4.0987e+01
Fitted a model with MAP estimate = -70.1882
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 131.2956 - loglik: -4.3512e+01 - logprior: -8.7766e+01
Epoch 2/10
10/10 - 1s - loss: 69.7750 - loglik: -4.3881e+01 - logprior: -2.5868e+01
Epoch 3/10
10/10 - 1s - loss: 57.4514 - loglik: -4.4308e+01 - logprior: -1.3120e+01
Epoch 4/10
10/10 - 1s - loss: 52.5156 - loglik: -4.4398e+01 - logprior: -8.0321e+00
Epoch 5/10
10/10 - 1s - loss: 49.9202 - loglik: -4.4420e+01 - logprior: -5.3403e+00
Epoch 6/10
10/10 - 1s - loss: 48.4815 - loglik: -4.4477e+01 - logprior: -3.8349e+00
Epoch 7/10
10/10 - 1s - loss: 47.6443 - loglik: -4.4487e+01 - logprior: -2.9922e+00
Epoch 8/10
10/10 - 1s - loss: 46.5949 - loglik: -4.3935e+01 - logprior: -2.4689e+00
Epoch 9/10
10/10 - 1s - loss: 45.9341 - loglik: -4.3622e+01 - logprior: -2.1201e+00
Epoch 10/10
10/10 - 1s - loss: 45.5950 - loglik: -4.3603e+01 - logprior: -1.8015e+00
Fitted a model with MAP estimate = -45.2793
Time for alignment: 28.7274
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.9586 - loglik: -7.9870e+01 - logprior: -9.3067e+01
Epoch 2/10
10/10 - 1s - loss: 95.8987 - loglik: -6.8621e+01 - logprior: -2.7255e+01
Epoch 3/10
10/10 - 1s - loss: 73.8781 - loglik: -5.9895e+01 - logprior: -1.3973e+01
Epoch 4/10
10/10 - 1s - loss: 64.7450 - loglik: -5.5874e+01 - logprior: -8.8486e+00
Epoch 5/10
10/10 - 1s - loss: 59.9855 - loglik: -5.3723e+01 - logprior: -6.2425e+00
Epoch 6/10
10/10 - 1s - loss: 57.7493 - loglik: -5.2824e+01 - logprior: -4.8578e+00
Epoch 7/10
10/10 - 1s - loss: 56.6613 - loglik: -5.2465e+01 - logprior: -4.0329e+00
Epoch 8/10
10/10 - 1s - loss: 56.0209 - loglik: -5.2349e+01 - logprior: -3.5095e+00
Epoch 9/10
10/10 - 1s - loss: 55.6773 - loglik: -5.2365e+01 - logprior: -3.1784e+00
Epoch 10/10
10/10 - 1s - loss: 55.4488 - loglik: -5.2357e+01 - logprior: -2.9494e+00
Fitted a model with MAP estimate = -55.2002
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.9323 - loglik: -4.9330e+01 - logprior: -1.2458e+02
Epoch 2/2
10/10 - 1s - loss: 87.1253 - loglik: -4.6084e+01 - logprior: -4.0994e+01
Fitted a model with MAP estimate = -70.1628
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 131.3076 - loglik: -4.3521e+01 - logprior: -8.7770e+01
Epoch 2/10
10/10 - 1s - loss: 69.7553 - loglik: -4.3869e+01 - logprior: -2.5865e+01
Epoch 3/10
10/10 - 1s - loss: 57.4199 - loglik: -4.4285e+01 - logprior: -1.3117e+01
Epoch 4/10
10/10 - 1s - loss: 52.4809 - loglik: -4.4396e+01 - logprior: -8.0284e+00
Epoch 5/10
10/10 - 1s - loss: 49.8850 - loglik: -4.4380e+01 - logprior: -5.3627e+00
Epoch 6/10
10/10 - 1s - loss: 48.4625 - loglik: -4.4459e+01 - logprior: -3.8414e+00
Epoch 7/10
10/10 - 1s - loss: 47.6143 - loglik: -4.4467e+01 - logprior: -2.9887e+00
Epoch 8/10
10/10 - 1s - loss: 46.5103 - loglik: -4.3856e+01 - logprior: -2.4737e+00
Epoch 9/10
10/10 - 1s - loss: 45.9208 - loglik: -4.3613e+01 - logprior: -2.1146e+00
Epoch 10/10
10/10 - 1s - loss: 45.6093 - loglik: -4.3630e+01 - logprior: -1.7941e+00
Fitted a model with MAP estimate = -45.3034
Time for alignment: 27.8690
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.9586 - loglik: -7.9870e+01 - logprior: -9.3067e+01
Epoch 2/10
10/10 - 1s - loss: 95.8987 - loglik: -6.8621e+01 - logprior: -2.7255e+01
Epoch 3/10
10/10 - 1s - loss: 73.8781 - loglik: -5.9895e+01 - logprior: -1.3973e+01
Epoch 4/10
10/10 - 1s - loss: 64.7452 - loglik: -5.5874e+01 - logprior: -8.8486e+00
Epoch 5/10
10/10 - 1s - loss: 60.0170 - loglik: -5.3775e+01 - logprior: -6.2245e+00
Epoch 6/10
10/10 - 1s - loss: 57.8959 - loglik: -5.3092e+01 - logprior: -4.7815e+00
Epoch 7/10
10/10 - 1s - loss: 56.7521 - loglik: -5.2694e+01 - logprior: -3.9667e+00
Epoch 8/10
10/10 - 1s - loss: 56.1062 - loglik: -5.2488e+01 - logprior: -3.4528e+00
Epoch 9/10
10/10 - 1s - loss: 55.7346 - loglik: -5.2434e+01 - logprior: -3.1505e+00
Epoch 10/10
10/10 - 1s - loss: 55.4824 - loglik: -5.2410e+01 - logprior: -2.9379e+00
Fitted a model with MAP estimate = -55.2525
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.8762 - loglik: -4.9286e+01 - logprior: -1.2457e+02
Epoch 2/2
10/10 - 1s - loss: 87.1553 - loglik: -4.6131e+01 - logprior: -4.0987e+01
Fitted a model with MAP estimate = -70.1911
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 131.2971 - loglik: -4.3513e+01 - logprior: -8.7766e+01
Epoch 2/10
10/10 - 1s - loss: 69.7713 - loglik: -4.3878e+01 - logprior: -2.5869e+01
Epoch 3/10
10/10 - 1s - loss: 57.4441 - loglik: -4.4304e+01 - logprior: -1.3119e+01
Epoch 4/10
10/10 - 1s - loss: 52.5135 - loglik: -4.4410e+01 - logprior: -8.0239e+00
Epoch 5/10
10/10 - 1s - loss: 49.9212 - loglik: -4.4440e+01 - logprior: -5.3327e+00
Epoch 6/10
10/10 - 1s - loss: 48.4658 - loglik: -4.4455e+01 - logprior: -3.8464e+00
Epoch 7/10
10/10 - 1s - loss: 47.6042 - loglik: -4.4448e+01 - logprior: -2.9902e+00
Epoch 8/10
10/10 - 1s - loss: 46.4980 - loglik: -4.3837e+01 - logprior: -2.4751e+00
Epoch 9/10
10/10 - 1s - loss: 45.9121 - loglik: -4.3605e+01 - logprior: -2.1159e+00
Epoch 10/10
10/10 - 1s - loss: 45.5969 - loglik: -4.3613e+01 - logprior: -1.7976e+00
Fitted a model with MAP estimate = -45.2836
Time for alignment: 27.9085
Computed alignments with likelihoods: ['-45.2793', '-45.3034', '-45.2836']
Best model has likelihood: -45.2793
time for generating output: 0.1194
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a8326b4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aa4a0bcd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f697a4cee20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69a4aff9a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f697213bfd0>, <__main__.SimpleDirichletPrior object at 0x7f6a82c73b50>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.6866 - loglik: -3.4992e+02 - logprior: -1.2747e+01
Epoch 2/10
11/11 - 3s - loss: 310.1553 - loglik: -3.0674e+02 - logprior: -3.3849e+00
Epoch 3/10
11/11 - 3s - loss: 270.1451 - loglik: -2.6773e+02 - logprior: -2.3896e+00
Epoch 4/10
11/11 - 3s - loss: 249.7081 - loglik: -2.4698e+02 - logprior: -2.5250e+00
Epoch 5/10
11/11 - 3s - loss: 242.8036 - loglik: -2.3967e+02 - logprior: -2.6905e+00
Epoch 6/10
11/11 - 3s - loss: 239.6420 - loglik: -2.3624e+02 - logprior: -2.7327e+00
Epoch 7/10
11/11 - 3s - loss: 237.7491 - loglik: -2.3443e+02 - logprior: -2.6805e+00
Epoch 8/10
11/11 - 3s - loss: 236.8621 - loglik: -2.3374e+02 - logprior: -2.6267e+00
Epoch 9/10
11/11 - 3s - loss: 237.2236 - loglik: -2.3416e+02 - logprior: -2.6387e+00
Fitted a model with MAP estimate = -235.6483
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 3), (61, 1), (62, 4), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 254.3295 - loglik: -2.3946e+02 - logprior: -1.4835e+01
Epoch 2/2
11/11 - 3s - loss: 229.7666 - loglik: -2.2307e+02 - logprior: -6.5066e+00
Fitted a model with MAP estimate = -224.0676
expansions: [(0, 20)]
discards: [ 0  8 77]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 10s - loss: 226.3505 - loglik: -2.1801e+02 - logprior: -8.2808e+00
Epoch 2/2
22/22 - 5s - loss: 212.7252 - loglik: -2.0971e+02 - logprior: -2.7255e+00
Fitted a model with MAP estimate = -209.5862
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 229.2198 - loglik: -2.1682e+02 - logprior: -1.2377e+01
Epoch 2/10
11/11 - 3s - loss: 218.6212 - loglik: -2.1513e+02 - logprior: -3.3488e+00
Epoch 3/10
11/11 - 3s - loss: 214.8262 - loglik: -2.1272e+02 - logprior: -1.7939e+00
Epoch 4/10
11/11 - 3s - loss: 213.5626 - loglik: -2.1175e+02 - logprior: -1.3779e+00
Epoch 5/10
11/11 - 3s - loss: 212.3059 - loglik: -2.1053e+02 - logprior: -1.2994e+00
Epoch 6/10
11/11 - 3s - loss: 210.0513 - loglik: -2.0838e+02 - logprior: -1.2107e+00
Epoch 7/10
11/11 - 3s - loss: 210.4197 - loglik: -2.0886e+02 - logprior: -1.1307e+00
Fitted a model with MAP estimate = -209.4564
Time for alignment: 89.9400
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 363.3303 - loglik: -3.5056e+02 - logprior: -1.2742e+01
Epoch 2/10
11/11 - 3s - loss: 309.6329 - loglik: -3.0623e+02 - logprior: -3.3753e+00
Epoch 3/10
11/11 - 3s - loss: 268.0681 - loglik: -2.6566e+02 - logprior: -2.3852e+00
Epoch 4/10
11/11 - 3s - loss: 249.0932 - loglik: -2.4634e+02 - logprior: -2.6095e+00
Epoch 5/10
11/11 - 3s - loss: 240.2962 - loglik: -2.3713e+02 - logprior: -2.7952e+00
Epoch 6/10
11/11 - 3s - loss: 239.0748 - loglik: -2.3573e+02 - logprior: -2.7729e+00
Epoch 7/10
11/11 - 3s - loss: 237.3264 - loglik: -2.3404e+02 - logprior: -2.7471e+00
Epoch 8/10
11/11 - 3s - loss: 237.7976 - loglik: -2.3461e+02 - logprior: -2.7352e+00
Fitted a model with MAP estimate = -235.8872
expansions: [(7, 1), (8, 2), (9, 2), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 256.1637 - loglik: -2.4123e+02 - logprior: -1.4905e+01
Epoch 2/2
11/11 - 3s - loss: 229.7413 - loglik: -2.2283e+02 - logprior: -6.7620e+00
Fitted a model with MAP estimate = -225.3738
expansions: [(0, 16)]
discards: [  0   6  73  76  84  87 110]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 236.4034 - loglik: -2.2384e+02 - logprior: -1.2540e+01
Epoch 2/2
11/11 - 4s - loss: 221.5377 - loglik: -2.1708e+02 - logprior: -4.3191e+00
Fitted a model with MAP estimate = -217.8484
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 230.4758 - loglik: -2.1768e+02 - logprior: -1.2778e+01
Epoch 2/10
11/11 - 3s - loss: 219.2342 - loglik: -2.1551e+02 - logprior: -3.5944e+00
Epoch 3/10
11/11 - 3s - loss: 214.9413 - loglik: -2.1261e+02 - logprior: -2.0457e+00
Epoch 4/10
11/11 - 3s - loss: 214.7183 - loglik: -2.1262e+02 - logprior: -1.6584e+00
Epoch 5/10
11/11 - 3s - loss: 212.0166 - loglik: -2.1000e+02 - logprior: -1.5084e+00
Epoch 6/10
11/11 - 3s - loss: 211.6214 - loglik: -2.0973e+02 - logprior: -1.3924e+00
Epoch 7/10
11/11 - 3s - loss: 210.2593 - loglik: -2.0845e+02 - logprior: -1.3549e+00
Epoch 8/10
11/11 - 3s - loss: 209.8276 - loglik: -2.0800e+02 - logprior: -1.4116e+00
Epoch 9/10
11/11 - 3s - loss: 210.3816 - loglik: -2.0858e+02 - logprior: -1.4179e+00
Fitted a model with MAP estimate = -209.2515
Time for alignment: 90.6992
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.8642 - loglik: -3.5009e+02 - logprior: -1.2746e+01
Epoch 2/10
11/11 - 3s - loss: 311.0439 - loglik: -3.0764e+02 - logprior: -3.3803e+00
Epoch 3/10
11/11 - 3s - loss: 267.5886 - loglik: -2.6516e+02 - logprior: -2.4020e+00
Epoch 4/10
11/11 - 3s - loss: 247.9408 - loglik: -2.4510e+02 - logprior: -2.6333e+00
Epoch 5/10
11/11 - 3s - loss: 241.7024 - loglik: -2.3837e+02 - logprior: -2.7870e+00
Epoch 6/10
11/11 - 3s - loss: 238.0609 - loglik: -2.3464e+02 - logprior: -2.7721e+00
Epoch 7/10
11/11 - 3s - loss: 236.8125 - loglik: -2.3355e+02 - logprior: -2.7385e+00
Epoch 8/10
11/11 - 3s - loss: 237.6765 - loglik: -2.3454e+02 - logprior: -2.7158e+00
Fitted a model with MAP estimate = -235.9469
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 255.5856 - loglik: -2.4066e+02 - logprior: -1.4901e+01
Epoch 2/2
11/11 - 3s - loss: 230.8792 - loglik: -2.2393e+02 - logprior: -6.7687e+00
Fitted a model with MAP estimate = -224.8794
expansions: [(0, 16)]
discards: [  0   8  14  77  78  85  88 109]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 235.2307 - loglik: -2.2267e+02 - logprior: -1.2527e+01
Epoch 2/2
11/11 - 3s - loss: 220.8995 - loglik: -2.1640e+02 - logprior: -4.3153e+00
Fitted a model with MAP estimate = -217.3017
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 230.5447 - loglik: -2.1778e+02 - logprior: -1.2739e+01
Epoch 2/10
11/11 - 3s - loss: 217.3595 - loglik: -2.1362e+02 - logprior: -3.5685e+00
Epoch 3/10
11/11 - 3s - loss: 215.8092 - loglik: -2.1342e+02 - logprior: -2.0367e+00
Epoch 4/10
11/11 - 3s - loss: 213.5290 - loglik: -2.1141e+02 - logprior: -1.6558e+00
Epoch 5/10
11/11 - 3s - loss: 211.6545 - loglik: -2.0955e+02 - logprior: -1.5959e+00
Epoch 6/10
11/11 - 3s - loss: 212.1386 - loglik: -2.1004e+02 - logprior: -1.5806e+00
Fitted a model with MAP estimate = -209.7711
Time for alignment: 78.3515
Computed alignments with likelihoods: ['-209.4564', '-209.2515', '-209.7711']
Best model has likelihood: -209.2515
time for generating output: 0.2961
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.6511627906976745
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac8992520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad522190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7a345b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e8a75fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6968fa8ca0>, <__main__.SimpleDirichletPrior object at 0x7f6a79edf130>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 757.8505 - loglik: -7.5535e+02 - logprior: -2.0182e+00
Epoch 2/10
39/39 - 22s - loss: 636.5475 - loglik: -6.3339e+02 - logprior: -2.1023e+00
Epoch 3/10
39/39 - 22s - loss: 623.3542 - loglik: -6.1991e+02 - logprior: -2.2464e+00
Epoch 4/10
39/39 - 22s - loss: 620.7556 - loglik: -6.1732e+02 - logprior: -2.2581e+00
Epoch 5/10
39/39 - 22s - loss: 619.3398 - loglik: -6.1593e+02 - logprior: -2.2849e+00
Epoch 6/10
39/39 - 22s - loss: 618.4271 - loglik: -6.1504e+02 - logprior: -2.3224e+00
Epoch 7/10
39/39 - 22s - loss: 617.3058 - loglik: -6.1394e+02 - logprior: -2.3381e+00
Epoch 8/10
39/39 - 22s - loss: 617.1841 - loglik: -6.1384e+02 - logprior: -2.3571e+00
Epoch 9/10
39/39 - 22s - loss: 616.7417 - loglik: -6.1342e+02 - logprior: -2.3603e+00
Epoch 10/10
39/39 - 22s - loss: 616.3059 - loglik: -6.1297e+02 - logprior: -2.3755e+00
Fitted a model with MAP estimate = -577.8867
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (36, 2), (39, 1), (45, 2), (46, 1), (58, 2), (59, 2), (61, 5), (64, 1), (65, 1), (66, 1), (67, 1), (90, 1), (122, 3), (123, 10), (124, 2), (126, 1), (129, 2), (130, 1), (132, 1), (136, 1), (137, 1), (138, 1), (157, 3), (162, 1), (163, 3), (166, 1), (168, 4), (169, 2), (180, 2), (191, 1), (193, 1), (194, 1), (203, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 624.0148 - loglik: -6.2019e+02 - logprior: -3.4919e+00
Epoch 2/2
39/39 - 31s - loss: 592.6979 - loglik: -5.8957e+02 - logprior: -1.9429e+00
Fitted a model with MAP estimate = -546.2431
expansions: [(0, 2), (44, 1), (227, 1), (230, 1)]
discards: [  0  12  13  57  77  78 156 157 158 204 205 206 207 216 243 282 284]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 603.7665 - loglik: -6.0133e+02 - logprior: -2.1359e+00
Epoch 2/2
39/39 - 29s - loss: 592.2676 - loglik: -5.9005e+02 - logprior: -1.2881e+00
Fitted a model with MAP estimate = -547.5891
expansions: [(199, 4)]
discards: [  0  71 207]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 37s - loss: 555.0521 - loglik: -5.5264e+02 - logprior: -2.0592e+00
Epoch 2/10
45/45 - 32s - loss: 546.3165 - loglik: -5.4402e+02 - logprior: -1.1822e+00
Epoch 3/10
45/45 - 33s - loss: 541.7905 - loglik: -5.3921e+02 - logprior: -1.2199e+00
Epoch 4/10
45/45 - 33s - loss: 540.7109 - loglik: -5.3829e+02 - logprior: -1.0818e+00
Epoch 5/10
45/45 - 32s - loss: 540.9311 - loglik: -5.3855e+02 - logprior: -1.1094e+00
Fitted a model with MAP estimate = -538.0851
Time for alignment: 663.5747
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 757.1677 - loglik: -7.5465e+02 - logprior: -2.0574e+00
Epoch 2/10
39/39 - 22s - loss: 636.4385 - loglik: -6.3302e+02 - logprior: -2.2127e+00
Epoch 3/10
39/39 - 21s - loss: 622.1200 - loglik: -6.1794e+02 - logprior: -2.5440e+00
Epoch 4/10
39/39 - 22s - loss: 618.8514 - loglik: -6.1497e+02 - logprior: -2.5058e+00
Epoch 5/10
39/39 - 22s - loss: 617.6699 - loglik: -6.1387e+02 - logprior: -2.5049e+00
Epoch 6/10
39/39 - 22s - loss: 616.5365 - loglik: -6.1275e+02 - logprior: -2.5295e+00
Epoch 7/10
39/39 - 22s - loss: 615.8008 - loglik: -6.1205e+02 - logprior: -2.5441e+00
Epoch 8/10
39/39 - 22s - loss: 615.1233 - loglik: -6.1141e+02 - logprior: -2.5570e+00
Epoch 9/10
39/39 - 22s - loss: 614.6634 - loglik: -6.1096e+02 - logprior: -2.5796e+00
Epoch 10/10
39/39 - 22s - loss: 613.9755 - loglik: -6.1026e+02 - logprior: -2.5996e+00
Fitted a model with MAP estimate = -576.0566
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 1), (60, 2), (61, 5), (66, 2), (67, 3), (97, 2), (117, 1), (118, 1), (120, 1), (123, 2), (124, 5), (125, 2), (126, 2), (128, 1), (132, 1), (134, 2), (135, 1), (139, 1), (140, 1), (146, 2), (152, 1), (161, 1), (164, 1), (165, 4), (166, 1), (167, 3), (168, 3), (169, 1), (170, 1), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (202, 1), (209, 1), (211, 2), (212, 2), (214, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 626.7567 - loglik: -6.2289e+02 - logprior: -3.5590e+00
Epoch 2/2
39/39 - 31s - loss: 593.1454 - loglik: -5.9019e+02 - logprior: -1.9894e+00
Fitted a model with MAP estimate = -547.1575
expansions: [(0, 2), (231, 1), (232, 1)]
discards: [  0  11  12  58  73  87 123 158 159 160 163 178 195 220 221 227 248 287
 288]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 601.1189 - loglik: -5.9864e+02 - logprior: -2.1795e+00
Epoch 2/2
39/39 - 29s - loss: 590.6534 - loglik: -5.8837e+02 - logprior: -1.2427e+00
Fitted a model with MAP estimate = -545.8827
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 37s - loss: 554.7294 - loglik: -5.5224e+02 - logprior: -2.1490e+00
Epoch 2/10
45/45 - 33s - loss: 545.6276 - loglik: -5.4351e+02 - logprior: -1.1893e+00
Epoch 3/10
45/45 - 33s - loss: 542.3192 - loglik: -5.3993e+02 - logprior: -1.1735e+00
Epoch 4/10
45/45 - 33s - loss: 540.0010 - loglik: -5.3756e+02 - logprior: -1.1725e+00
Epoch 5/10
45/45 - 33s - loss: 539.7761 - loglik: -5.3756e+02 - logprior: -1.0027e+00
Epoch 6/10
45/45 - 33s - loss: 540.5768 - loglik: -5.3850e+02 - logprior: -9.3052e-01
Fitted a model with MAP estimate = -537.5741
Time for alignment: 698.6026
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 755.3594 - loglik: -7.5282e+02 - logprior: -2.0614e+00
Epoch 2/10
39/39 - 22s - loss: 634.2027 - loglik: -6.3073e+02 - logprior: -2.2002e+00
Epoch 3/10
39/39 - 22s - loss: 620.8951 - loglik: -6.1685e+02 - logprior: -2.4520e+00
Epoch 4/10
39/39 - 22s - loss: 617.8765 - loglik: -6.1425e+02 - logprior: -2.4497e+00
Epoch 5/10
39/39 - 22s - loss: 616.5640 - loglik: -6.1303e+02 - logprior: -2.4628e+00
Epoch 6/10
39/39 - 22s - loss: 616.0366 - loglik: -6.1255e+02 - logprior: -2.4847e+00
Epoch 7/10
39/39 - 22s - loss: 615.0045 - loglik: -6.1150e+02 - logprior: -2.5033e+00
Epoch 8/10
39/39 - 22s - loss: 615.0187 - loglik: -6.1152e+02 - logprior: -2.5172e+00
Fitted a model with MAP estimate = -574.6498
expansions: [(12, 4), (13, 1), (14, 1), (15, 1), (19, 1), (35, 1), (36, 1), (45, 3), (46, 1), (58, 2), (59, 2), (61, 5), (66, 2), (67, 2), (68, 2), (69, 1), (117, 1), (118, 1), (120, 1), (122, 3), (123, 5), (124, 2), (125, 2), (127, 2), (133, 1), (135, 1), (139, 1), (140, 1), (141, 1), (145, 2), (153, 1), (159, 2), (164, 3), (165, 1), (166, 1), (167, 2), (168, 4), (169, 2), (180, 2), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (210, 1), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 622.1937 - loglik: -6.1836e+02 - logprior: -3.5146e+00
Epoch 2/2
39/39 - 31s - loss: 592.7474 - loglik: -5.8974e+02 - logprior: -1.9696e+00
Fitted a model with MAP estimate = -546.7856
expansions: [(0, 2), (234, 1), (236, 1)]
discards: [  0  11  12  58  73  78  79  89  94 159 160 164 172 196 209 210 211 222
 249 289]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 603.2222 - loglik: -6.0070e+02 - logprior: -2.2100e+00
Epoch 2/2
39/39 - 29s - loss: 591.7187 - loglik: -5.8933e+02 - logprior: -1.3170e+00
Fitted a model with MAP estimate = -546.7804
expansions: [(71, 1), (199, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 37s - loss: 555.0099 - loglik: -5.5250e+02 - logprior: -2.1648e+00
Epoch 2/10
45/45 - 34s - loss: 545.3757 - loglik: -5.4315e+02 - logprior: -1.2751e+00
Epoch 3/10
45/45 - 32s - loss: 539.9828 - loglik: -5.3751e+02 - logprior: -1.2120e+00
Epoch 4/10
45/45 - 34s - loss: 541.2458 - loglik: -5.3877e+02 - logprior: -1.1655e+00
Fitted a model with MAP estimate = -538.0838
Time for alignment: 589.8885
Computed alignments with likelihoods: ['-538.0851', '-537.5741', '-538.0838']
Best model has likelihood: -537.5741
time for generating output: 0.3375
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8969072164948454
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a83053400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e84ceb20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d8390c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69be7f5190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6a830b2fa0>, <__main__.SimpleDirichletPrior object at 0x7f6ac90b6580>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 446.2679 - loglik: -4.4486e+02 - logprior: -1.1429e+00
Epoch 2/10
30/30 - 9s - loss: 374.0994 - loglik: -3.7166e+02 - logprior: -1.2571e+00
Epoch 3/10
30/30 - 9s - loss: 361.3485 - loglik: -3.5887e+02 - logprior: -1.2646e+00
Epoch 4/10
30/30 - 9s - loss: 359.1049 - loglik: -3.5685e+02 - logprior: -1.3053e+00
Epoch 5/10
30/30 - 9s - loss: 356.4727 - loglik: -3.5448e+02 - logprior: -1.2965e+00
Epoch 6/10
30/30 - 9s - loss: 356.5609 - loglik: -3.5466e+02 - logprior: -1.2730e+00
Fitted a model with MAP estimate = -344.9355
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 352.0355 - loglik: -3.5011e+02 - logprior: -1.3559e+00
Epoch 2/2
61/61 - 15s - loss: 342.9681 - loglik: -3.4052e+02 - logprior: -1.1128e+00
Fitted a model with MAP estimate = -329.2812
expansions: []
discards: [ 25  47  51  92  94 132 146 150 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 347.3967 - loglik: -3.4579e+02 - logprior: -1.2293e+00
Epoch 2/2
61/61 - 15s - loss: 341.7466 - loglik: -3.3966e+02 - logprior: -9.9395e-01
Fitted a model with MAP estimate = -329.5632
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 332.6566 - loglik: -3.3111e+02 - logprior: -8.0058e-01
Epoch 2/10
87/87 - 20s - loss: 328.5601 - loglik: -3.2612e+02 - logprior: -7.3827e-01
Epoch 3/10
87/87 - 20s - loss: 327.4655 - loglik: -3.2540e+02 - logprior: -7.1469e-01
Epoch 4/10
87/87 - 20s - loss: 326.8677 - loglik: -3.2519e+02 - logprior: -6.7313e-01
Epoch 5/10
87/87 - 20s - loss: 327.1471 - loglik: -3.2552e+02 - logprior: -6.5209e-01
Fitted a model with MAP estimate = -325.3390
Time for alignment: 393.7890
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.6270 - loglik: -4.4422e+02 - logprior: -1.1361e+00
Epoch 2/10
30/30 - 9s - loss: 374.3171 - loglik: -3.7162e+02 - logprior: -1.2538e+00
Epoch 3/10
30/30 - 9s - loss: 361.6413 - loglik: -3.5903e+02 - logprior: -1.2945e+00
Epoch 4/10
30/30 - 9s - loss: 358.9892 - loglik: -3.5669e+02 - logprior: -1.2956e+00
Epoch 5/10
30/30 - 9s - loss: 356.7856 - loglik: -3.5472e+02 - logprior: -1.2884e+00
Epoch 6/10
30/30 - 9s - loss: 356.1750 - loglik: -3.5422e+02 - logprior: -1.2660e+00
Epoch 7/10
30/30 - 9s - loss: 356.4258 - loglik: -3.5447e+02 - logprior: -1.2494e+00
Fitted a model with MAP estimate = -345.5513
expansions: [(14, 1), (17, 2), (20, 1), (23, 2), (26, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 1), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 353.3131 - loglik: -3.5147e+02 - logprior: -1.3803e+00
Epoch 2/2
61/61 - 16s - loss: 342.2463 - loglik: -3.4000e+02 - logprior: -1.1348e+00
Fitted a model with MAP estimate = -329.2990
expansions: []
discards: [ 18  26  31  53  94  96 105 135 149 156]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 347.2784 - loglik: -3.4555e+02 - logprior: -1.2358e+00
Epoch 2/2
61/61 - 15s - loss: 342.5464 - loglik: -3.4023e+02 - logprior: -1.0089e+00
Fitted a model with MAP estimate = -329.5940
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 332.2045 - loglik: -3.3085e+02 - logprior: -8.1800e-01
Epoch 2/10
87/87 - 20s - loss: 328.3777 - loglik: -3.2612e+02 - logprior: -7.3981e-01
Epoch 3/10
87/87 - 20s - loss: 328.0229 - loglik: -3.2593e+02 - logprior: -7.1439e-01
Epoch 4/10
87/87 - 20s - loss: 326.9822 - loglik: -3.2527e+02 - logprior: -6.8265e-01
Epoch 5/10
87/87 - 20s - loss: 326.7025 - loglik: -3.2507e+02 - logprior: -6.5455e-01
Epoch 6/10
87/87 - 20s - loss: 326.9739 - loglik: -3.2529e+02 - logprior: -6.3326e-01
Fitted a model with MAP estimate = -324.8636
Time for alignment: 423.9018
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 446.1787 - loglik: -4.4478e+02 - logprior: -1.1327e+00
Epoch 2/10
30/30 - 9s - loss: 372.3418 - loglik: -3.6971e+02 - logprior: -1.2602e+00
Epoch 3/10
30/30 - 9s - loss: 360.4355 - loglik: -3.5800e+02 - logprior: -1.3031e+00
Epoch 4/10
30/30 - 9s - loss: 358.3504 - loglik: -3.5623e+02 - logprior: -1.3232e+00
Epoch 5/10
30/30 - 9s - loss: 357.4476 - loglik: -3.5546e+02 - logprior: -1.2924e+00
Epoch 6/10
30/30 - 9s - loss: 356.8301 - loglik: -3.5490e+02 - logprior: -1.2720e+00
Epoch 7/10
30/30 - 9s - loss: 356.0728 - loglik: -3.5415e+02 - logprior: -1.2626e+00
Epoch 8/10
30/30 - 9s - loss: 356.6251 - loglik: -3.5476e+02 - logprior: -1.2548e+00
Fitted a model with MAP estimate = -345.6340
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 1), (116, 2), (126, 1), (127, 1), (128, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 352.9423 - loglik: -3.5102e+02 - logprior: -1.3616e+00
Epoch 2/2
61/61 - 15s - loss: 342.8590 - loglik: -3.4047e+02 - logprior: -1.1168e+00
Fitted a model with MAP estimate = -329.3119
expansions: []
discards: [ 18  30  48  52  93  95 133 147 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 347.0276 - loglik: -3.4532e+02 - logprior: -1.2338e+00
Epoch 2/2
61/61 - 15s - loss: 342.2186 - loglik: -3.3998e+02 - logprior: -1.0095e+00
Fitted a model with MAP estimate = -329.7162
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 24s - loss: 332.6634 - loglik: -3.3125e+02 - logprior: -8.0790e-01
Epoch 2/10
87/87 - 20s - loss: 328.2831 - loglik: -3.2596e+02 - logprior: -7.3609e-01
Epoch 3/10
87/87 - 20s - loss: 327.5985 - loglik: -3.2554e+02 - logprior: -7.1026e-01
Epoch 4/10
87/87 - 20s - loss: 327.2091 - loglik: -3.2551e+02 - logprior: -6.8160e-01
Epoch 5/10
87/87 - 20s - loss: 327.3867 - loglik: -3.2576e+02 - logprior: -6.5691e-01
Fitted a model with MAP estimate = -325.4814
Time for alignment: 410.5690
Computed alignments with likelihoods: ['-325.3390', '-324.8636', '-325.4814']
Best model has likelihood: -324.8636
time for generating output: 0.2877
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6612021857923497
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac9ba9f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a83edc940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b56ceb20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a82ba4c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16920b80>, <__main__.SimpleDirichletPrior object at 0x7f698b7809d0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 52s - loss: 1239.7382 - loglik: -1.2377e+03 - logprior: -1.4967e+00
Epoch 2/10
40/40 - 48s - loss: 1128.7301 - loglik: -1.1256e+03 - logprior: -6.7585e-01
Epoch 3/10
40/40 - 48s - loss: 1111.6725 - loglik: -1.1065e+03 - logprior: -9.1811e-01
Epoch 4/10
40/40 - 48s - loss: 1100.8615 - loglik: -1.0952e+03 - logprior: -1.1973e+00
Epoch 5/10
40/40 - 48s - loss: 1096.2166 - loglik: -1.0907e+03 - logprior: -1.3810e+00
Epoch 6/10
40/40 - 48s - loss: 1092.9231 - loglik: -1.0880e+03 - logprior: -1.3712e+00
Epoch 7/10
40/40 - 48s - loss: 1090.1146 - loglik: -1.0855e+03 - logprior: -1.4103e+00
Epoch 8/10
40/40 - 48s - loss: 1088.7601 - loglik: -1.0843e+03 - logprior: -1.4778e+00
Epoch 9/10
40/40 - 48s - loss: 1087.4329 - loglik: -1.0829e+03 - logprior: -1.5497e+00
Epoch 10/10
40/40 - 48s - loss: 1085.4856 - loglik: -1.0810e+03 - logprior: -1.5495e+00
Fitted a model with MAP estimate = -831.0530
expansions: [(18, 2), (25, 1), (70, 1), (123, 1), (124, 1), (172, 1), (292, 2), (293, 8), (294, 1), (295, 2), (298, 4), (315, 1), (317, 1), (318, 5), (328, 10), (329, 9), (330, 78)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 87s - loss: 1101.2694 - loglik: -1.0975e+03 - logprior: -2.7446e+00
Epoch 2/2
80/80 - 82s - loss: 1008.7010 - loglik: -1.0036e+03 - logprior: -1.7973e+00
Fitted a model with MAP estimate = -747.9851
expansions: [(233, 1)]
discards: [ 17  18  72 195 299 310 319 345 370 396 453]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 81s - loss: 1070.3993 - loglik: -1.0680e+03 - logprior: -2.0461e+00
Epoch 2/2
40/40 - 76s - loss: 1018.5075 - loglik: -1.0163e+03 - logprior: -6.1323e-01
Fitted a model with MAP estimate = -754.4767
expansions: [(447, 2)]
discards: [223]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 110s - loss: 771.2274 - loglik: -7.6980e+02 - logprior: -9.5816e-01
Epoch 2/10
56/56 - 107s - loss: 748.0391 - loglik: -7.4564e+02 - logprior: -8.5352e-01
Epoch 3/10
56/56 - 106s - loss: 741.0474 - loglik: -7.3781e+02 - logprior: -7.4623e-01
Epoch 4/10
56/56 - 106s - loss: 735.6590 - loglik: -7.3173e+02 - logprior: -8.4505e-01
Epoch 5/10
56/56 - 107s - loss: 735.9765 - loglik: -7.3212e+02 - logprior: -5.2796e-01
Fitted a model with MAP estimate = -728.1682
Time for alignment: 1805.5615
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 54s - loss: 1243.2091 - loglik: -1.2411e+03 - logprior: -1.5514e+00
Epoch 2/10
40/40 - 48s - loss: 1124.0305 - loglik: -1.1211e+03 - logprior: -5.2037e-01
Epoch 3/10
40/40 - 48s - loss: 1106.0369 - loglik: -1.1013e+03 - logprior: -7.5033e-01
Epoch 4/10
40/40 - 48s - loss: 1094.7762 - loglik: -1.0893e+03 - logprior: -1.0207e+00
Epoch 5/10
40/40 - 48s - loss: 1088.8704 - loglik: -1.0837e+03 - logprior: -1.1754e+00
Epoch 6/10
40/40 - 49s - loss: 1086.5369 - loglik: -1.0818e+03 - logprior: -1.2497e+00
Epoch 7/10
40/40 - 49s - loss: 1083.6296 - loglik: -1.0792e+03 - logprior: -1.3038e+00
Epoch 8/10
40/40 - 48s - loss: 1082.4712 - loglik: -1.0783e+03 - logprior: -1.3487e+00
Epoch 9/10
40/40 - 49s - loss: 1080.6490 - loglik: -1.0764e+03 - logprior: -1.3904e+00
Epoch 10/10
40/40 - 48s - loss: 1079.2914 - loglik: -1.0749e+03 - logprior: -1.4227e+00
Fitted a model with MAP estimate = -826.9116
expansions: [(18, 2), (62, 1), (122, 1), (125, 2), (294, 8), (295, 5), (314, 7), (315, 35), (319, 3), (320, 4), (321, 7), (330, 50)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 85s - loss: 1099.4502 - loglik: -1.0959e+03 - logprior: -2.4552e+00
Epoch 2/2
80/80 - 81s - loss: 1008.8604 - loglik: -1.0035e+03 - logprior: -1.8093e+00
Fitted a model with MAP estimate = -747.1015
expansions: [(232, 1), (348, 2), (353, 1), (441, 2), (451, 1)]
discards: [ 17  18 196 308 309 339 381 386 387 388]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 85s - loss: 1049.1145 - loglik: -1.0467e+03 - logprior: -1.4152e+00
Epoch 2/2
80/80 - 80s - loss: 1007.2485 - loglik: -1.0033e+03 - logprior: -7.5585e-01
Fitted a model with MAP estimate = -748.6523
expansions: [(160, 1)]
discards: [343 450]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 115s - loss: 763.8176 - loglik: -7.6161e+02 - logprior: -6.9098e-01
Epoch 2/10
113/113 - 112s - loss: 748.3541 - loglik: -7.4392e+02 - logprior: -5.1556e-01
Epoch 3/10
113/113 - 112s - loss: 739.1127 - loglik: -7.3479e+02 - logprior: -3.0380e-01
Epoch 4/10
113/113 - 112s - loss: 739.7017 - loglik: -7.3553e+02 - logprior: -6.3672e-02
Fitted a model with MAP estimate = -732.7284
Time for alignment: 1746.2926
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 52s - loss: 1228.3811 - loglik: -1.2258e+03 - logprior: -2.0369e+00
Epoch 2/10
40/40 - 49s - loss: 1093.7012 - loglik: -1.0877e+03 - logprior: -3.2520e+00
Epoch 3/10
40/40 - 48s - loss: 1069.1393 - loglik: -1.0614e+03 - logprior: -3.4864e+00
Epoch 4/10
40/40 - 49s - loss: 1058.0916 - loglik: -1.0499e+03 - logprior: -3.6382e+00
Epoch 5/10
40/40 - 48s - loss: 1051.5095 - loglik: -1.0431e+03 - logprior: -3.7603e+00
Epoch 6/10
40/40 - 48s - loss: 1049.0704 - loglik: -1.0409e+03 - logprior: -3.8568e+00
Epoch 7/10
40/40 - 49s - loss: 1045.7999 - loglik: -1.0378e+03 - logprior: -3.9154e+00
Epoch 8/10
40/40 - 48s - loss: 1043.8232 - loglik: -1.0358e+03 - logprior: -3.9738e+00
Epoch 9/10
40/40 - 48s - loss: 1040.0642 - loglik: -1.0318e+03 - logprior: -4.0752e+00
Epoch 10/10
40/40 - 48s - loss: 1037.8844 - loglik: -1.0298e+03 - logprior: -4.1134e+00
Fitted a model with MAP estimate = -808.9691
expansions: [(125, 1), (126, 1), (134, 1), (175, 1), (203, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 159 160 161 162 163 164 165 180 181 182 183 184 185 186 187 188
 189 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250
 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268
 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286
 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 1335.1543 - loglik: -1.3312e+03 - logprior: -3.3647e+00
Epoch 2/2
40/40 - 17s - loss: 1253.4586 - loglik: -1.2485e+03 - logprior: -1.9571e+00
Fitted a model with MAP estimate = -900.1046
expansions: [(0, 317), (18, 7), (52, 1), (53, 1), (55, 5), (56, 14), (58, 1), (59, 1), (61, 1), (101, 2), (111, 1)]
discards: [  0  62  63  64  65  66 112 114]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 86s - loss: 1084.2976 - loglik: -1.0812e+03 - logprior: -2.0337e+00
Epoch 2/2
80/80 - 82s - loss: 1010.6104 - loglik: -1.0059e+03 - logprior: -1.6863e+00
Fitted a model with MAP estimate = -749.3354
expansions: [(264, 1), (282, 1), (289, 1), (297, 1), (301, 1), (307, 1)]
discards: [  0  19  67  68  69 100 136 137 203 384 385]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 118s - loss: 768.5897 - loglik: -7.6573e+02 - logprior: -1.2984e+00
Epoch 2/10
113/113 - 113s - loss: 743.5482 - loglik: -7.3829e+02 - logprior: -1.2073e+00
Epoch 3/10
113/113 - 113s - loss: 740.4695 - loglik: -7.3520e+02 - logprior: -1.0611e+00
Epoch 4/10
113/113 - 113s - loss: 741.1481 - loglik: -7.3588e+02 - logprior: -9.5330e-01
Fitted a model with MAP estimate = -732.7544
Time for alignment: 1504.8755
Computed alignments with likelihoods: ['-728.1682', '-732.7284', '-732.7544']
Best model has likelihood: -728.1682
time for generating output: 0.5347
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7319761464116801
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69be7123a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6960ac47f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6982d6a9d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a8349c640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6960121460>, <__main__.SimpleDirichletPrior object at 0x7f6982c817c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 448.9386 - loglik: -4.4578e+02 - logprior: -3.1409e+00
Epoch 2/10
19/19 - 5s - loss: 280.3980 - loglik: -2.7859e+02 - logprior: -1.5749e+00
Epoch 3/10
19/19 - 5s - loss: 212.7117 - loglik: -2.1069e+02 - logprior: -1.9372e+00
Epoch 4/10
19/19 - 5s - loss: 200.7238 - loglik: -1.9834e+02 - logprior: -2.2924e+00
Epoch 5/10
19/19 - 5s - loss: 195.8948 - loglik: -1.9324e+02 - logprior: -2.4087e+00
Epoch 6/10
19/19 - 5s - loss: 192.8825 - loglik: -1.9011e+02 - logprior: -2.4721e+00
Epoch 7/10
19/19 - 5s - loss: 191.0984 - loglik: -1.8839e+02 - logprior: -2.4027e+00
Epoch 8/10
19/19 - 5s - loss: 190.8566 - loglik: -1.8817e+02 - logprior: -2.3913e+00
Epoch 9/10
19/19 - 5s - loss: 190.3941 - loglik: -1.8771e+02 - logprior: -2.3831e+00
Epoch 10/10
19/19 - 5s - loss: 189.4219 - loglik: -1.8673e+02 - logprior: -2.3827e+00
Fitted a model with MAP estimate = -184.4972
expansions: [(0, 2), (6, 1), (7, 1), (12, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (111, 2), (114, 2), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 183.7016 - loglik: -1.7922e+02 - logprior: -4.4308e+00
Epoch 2/2
19/19 - 7s - loss: 144.0442 - loglik: -1.4235e+02 - logprior: -1.5213e+00
Fitted a model with MAP estimate = -142.0798
expansions: []
discards: [  0  51  71  78 140 145 156]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 149.4305 - loglik: -1.4531e+02 - logprior: -4.0427e+00
Epoch 2/2
19/19 - 7s - loss: 142.6504 - loglik: -1.4096e+02 - logprior: -1.4581e+00
Fitted a model with MAP estimate = -141.5857
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 143.5216 - loglik: -1.4129e+02 - logprior: -2.1307e+00
Epoch 2/10
22/22 - 8s - loss: 141.6182 - loglik: -1.4043e+02 - logprior: -9.4647e-01
Epoch 3/10
22/22 - 8s - loss: 136.5596 - loglik: -1.3540e+02 - logprior: -9.5908e-01
Epoch 4/10
22/22 - 8s - loss: 134.7201 - loglik: -1.3336e+02 - logprior: -1.0687e+00
Epoch 5/10
22/22 - 8s - loss: 130.0186 - loglik: -1.2832e+02 - logprior: -1.3279e+00
Epoch 6/10
22/22 - 8s - loss: 128.8690 - loglik: -1.2716e+02 - logprior: -1.3064e+00
Epoch 7/10
22/22 - 8s - loss: 126.7757 - loglik: -1.2512e+02 - logprior: -1.2675e+00
Epoch 8/10
22/22 - 8s - loss: 127.5719 - loglik: -1.2597e+02 - logprior: -1.2222e+00
Fitted a model with MAP estimate = -126.2259
Time for alignment: 202.9924
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 449.5660 - loglik: -4.4642e+02 - logprior: -3.1236e+00
Epoch 2/10
19/19 - 5s - loss: 283.8692 - loglik: -2.8204e+02 - logprior: -1.5905e+00
Epoch 3/10
19/19 - 5s - loss: 214.3223 - loglik: -2.1226e+02 - logprior: -1.9634e+00
Epoch 4/10
19/19 - 5s - loss: 201.4213 - loglik: -1.9891e+02 - logprior: -2.2839e+00
Epoch 5/10
19/19 - 5s - loss: 197.5175 - loglik: -1.9465e+02 - logprior: -2.4297e+00
Epoch 6/10
19/19 - 5s - loss: 193.5698 - loglik: -1.9041e+02 - logprior: -2.6726e+00
Epoch 7/10
19/19 - 5s - loss: 193.2578 - loglik: -1.9021e+02 - logprior: -2.5730e+00
Epoch 8/10
19/19 - 5s - loss: 192.2058 - loglik: -1.8919e+02 - logprior: -2.5825e+00
Epoch 9/10
19/19 - 5s - loss: 191.4695 - loglik: -1.8850e+02 - logprior: -2.5693e+00
Epoch 10/10
19/19 - 5s - loss: 190.1197 - loglik: -1.8715e+02 - logprior: -2.5781e+00
Fitted a model with MAP estimate = -185.8266
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (101, 1), (104, 1), (114, 1), (120, 2), (121, 2), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 181.4425 - loglik: -1.7826e+02 - logprior: -3.1213e+00
Epoch 2/2
19/19 - 7s - loss: 145.3925 - loglik: -1.4392e+02 - logprior: -1.2774e+00
Fitted a model with MAP estimate = -142.9122
expansions: []
discards: [ 75 148 149 154]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 146.8925 - loglik: -1.4385e+02 - logprior: -2.9707e+00
Epoch 2/2
19/19 - 7s - loss: 140.8908 - loglik: -1.3954e+02 - logprior: -1.1020e+00
Fitted a model with MAP estimate = -141.5983
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 12s - loss: 144.1654 - loglik: -1.4190e+02 - logprior: -2.1667e+00
Epoch 2/10
22/22 - 8s - loss: 141.1157 - loglik: -1.3984e+02 - logprior: -1.0285e+00
Epoch 3/10
22/22 - 8s - loss: 136.8252 - loglik: -1.3547e+02 - logprior: -1.1445e+00
Epoch 4/10
22/22 - 8s - loss: 134.4174 - loglik: -1.3274e+02 - logprior: -1.3753e+00
Epoch 5/10
22/22 - 8s - loss: 130.1398 - loglik: -1.2842e+02 - logprior: -1.3367e+00
Epoch 6/10
22/22 - 8s - loss: 128.0066 - loglik: -1.2632e+02 - logprior: -1.2897e+00
Epoch 7/10
22/22 - 8s - loss: 127.6937 - loglik: -1.2605e+02 - logprior: -1.2626e+00
Epoch 8/10
22/22 - 8s - loss: 126.6908 - loglik: -1.2511e+02 - logprior: -1.2173e+00
Epoch 9/10
22/22 - 8s - loss: 125.9398 - loglik: -1.2439e+02 - logprior: -1.1817e+00
Epoch 10/10
22/22 - 8s - loss: 126.5677 - loglik: -1.2502e+02 - logprior: -1.1603e+00
Fitted a model with MAP estimate = -125.6152
Time for alignment: 219.0522
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.4114 - loglik: -4.4628e+02 - logprior: -3.1147e+00
Epoch 2/10
19/19 - 5s - loss: 281.1523 - loglik: -2.7933e+02 - logprior: -1.5824e+00
Epoch 3/10
19/19 - 5s - loss: 212.6582 - loglik: -2.1062e+02 - logprior: -1.9553e+00
Epoch 4/10
19/19 - 5s - loss: 201.1310 - loglik: -1.9867e+02 - logprior: -2.3012e+00
Epoch 5/10
19/19 - 5s - loss: 195.6513 - loglik: -1.9274e+02 - logprior: -2.4893e+00
Epoch 6/10
19/19 - 5s - loss: 192.7509 - loglik: -1.8962e+02 - logprior: -2.6662e+00
Epoch 7/10
19/19 - 5s - loss: 192.5298 - loglik: -1.8947e+02 - logprior: -2.6084e+00
Epoch 8/10
19/19 - 5s - loss: 190.3179 - loglik: -1.8730e+02 - logprior: -2.6052e+00
Epoch 9/10
19/19 - 5s - loss: 190.8906 - loglik: -1.8791e+02 - logprior: -2.5975e+00
Fitted a model with MAP estimate = -184.9788
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 180.9618 - loglik: -1.7777e+02 - logprior: -3.1277e+00
Epoch 2/2
19/19 - 7s - loss: 144.9550 - loglik: -1.4348e+02 - logprior: -1.2531e+00
Fitted a model with MAP estimate = -142.7274
expansions: []
discards: [ 50  76 152]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 146.1261 - loglik: -1.4304e+02 - logprior: -3.0115e+00
Epoch 2/2
19/19 - 7s - loss: 141.5698 - loglik: -1.4021e+02 - logprior: -1.1247e+00
Fitted a model with MAP estimate = -141.3849
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 143.6436 - loglik: -1.4134e+02 - logprior: -2.2048e+00
Epoch 2/10
22/22 - 8s - loss: 140.0678 - loglik: -1.3877e+02 - logprior: -1.0519e+00
Epoch 3/10
22/22 - 8s - loss: 138.3787 - loglik: -1.3704e+02 - logprior: -1.1250e+00
Epoch 4/10
22/22 - 8s - loss: 133.4613 - loglik: -1.3179e+02 - logprior: -1.3778e+00
Epoch 5/10
22/22 - 8s - loss: 131.0350 - loglik: -1.2932e+02 - logprior: -1.3287e+00
Epoch 6/10
22/22 - 8s - loss: 128.2036 - loglik: -1.2652e+02 - logprior: -1.2968e+00
Epoch 7/10
22/22 - 8s - loss: 127.8494 - loglik: -1.2621e+02 - logprior: -1.2570e+00
Epoch 8/10
22/22 - 8s - loss: 126.9744 - loglik: -1.2537e+02 - logprior: -1.2265e+00
Epoch 9/10
22/22 - 8s - loss: 126.9937 - loglik: -1.2544e+02 - logprior: -1.1812e+00
Fitted a model with MAP estimate = -125.8659
Time for alignment: 202.9575
Computed alignments with likelihoods: ['-126.2259', '-125.6152', '-125.8659']
Best model has likelihood: -125.6152
time for generating output: 0.2204
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6aca376190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6abef75850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad02e340>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69441abb80>, <__main__.SimpleDirichletPrior object at 0x7f6968fd7910>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 610.6799 - loglik: -5.7577e+02 - logprior: -3.4891e+01
Epoch 2/10
12/12 - 4s - loss: 526.3790 - loglik: -5.2019e+02 - logprior: -6.1552e+00
Epoch 3/10
12/12 - 4s - loss: 463.5379 - loglik: -4.5994e+02 - logprior: -3.4723e+00
Epoch 4/10
12/12 - 4s - loss: 435.2444 - loglik: -4.3165e+02 - logprior: -3.1574e+00
Epoch 5/10
12/12 - 4s - loss: 428.3572 - loglik: -4.2479e+02 - logprior: -2.9448e+00
Epoch 6/10
12/12 - 4s - loss: 426.6290 - loglik: -4.2355e+02 - logprior: -2.6015e+00
Epoch 7/10
12/12 - 4s - loss: 424.3556 - loglik: -4.2141e+02 - logprior: -2.5515e+00
Epoch 8/10
12/12 - 4s - loss: 422.7159 - loglik: -4.1983e+02 - logprior: -2.4757e+00
Epoch 9/10
12/12 - 4s - loss: 422.5615 - loglik: -4.1976e+02 - logprior: -2.3890e+00
Epoch 10/10
12/12 - 4s - loss: 421.2221 - loglik: -4.1847e+02 - logprior: -2.3300e+00
Fitted a model with MAP estimate = -420.7712
expansions: [(11, 3), (21, 1), (32, 2), (33, 1), (39, 1), (41, 1), (44, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 1), (86, 1), (87, 1), (90, 1), (92, 1), (100, 1), (101, 1), (120, 2), (123, 1), (124, 5), (129, 2), (130, 2), (137, 1), (147, 3), (149, 2), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 459.7593 - loglik: -4.1882e+02 - logprior: -4.0901e+01
Epoch 2/2
12/12 - 5s - loss: 419.3158 - loglik: -4.0423e+02 - logprior: -1.4879e+01
Fitted a model with MAP estimate = -412.0806
expansions: [(0, 4)]
discards: [  0 126 145 151 152 153 163 189 219]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 436.1794 - loglik: -4.0530e+02 - logprior: -3.0849e+01
Epoch 2/2
12/12 - 5s - loss: 406.0861 - loglik: -4.0108e+02 - logprior: -4.8215e+00
Fitted a model with MAP estimate = -401.0829
expansions: [(183, 1)]
discards: [  1   2   3 184 185]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 433.0472 - loglik: -4.0313e+02 - logprior: -2.9888e+01
Epoch 2/10
12/12 - 5s - loss: 405.3876 - loglik: -4.0104e+02 - logprior: -4.1644e+00
Epoch 3/10
12/12 - 5s - loss: 399.5699 - loglik: -3.9959e+02 - logprior: 0.4216
Epoch 4/10
12/12 - 5s - loss: 396.6566 - loglik: -3.9852e+02 - logprior: 2.3999
Epoch 5/10
12/12 - 5s - loss: 393.9523 - loglik: -3.9679e+02 - logprior: 3.3545
Epoch 6/10
12/12 - 5s - loss: 394.6133 - loglik: -3.9808e+02 - logprior: 3.9417
Fitted a model with MAP estimate = -392.4928
Time for alignment: 115.5873
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 610.0656 - loglik: -5.7516e+02 - logprior: -3.4885e+01
Epoch 2/10
12/12 - 4s - loss: 526.8691 - loglik: -5.2070e+02 - logprior: -6.1508e+00
Epoch 3/10
12/12 - 4s - loss: 463.6476 - loglik: -4.5994e+02 - logprior: -3.5911e+00
Epoch 4/10
12/12 - 4s - loss: 436.3427 - loglik: -4.3271e+02 - logprior: -3.3282e+00
Epoch 5/10
12/12 - 4s - loss: 428.7771 - loglik: -4.2527e+02 - logprior: -2.9062e+00
Epoch 6/10
12/12 - 4s - loss: 426.4182 - loglik: -4.2323e+02 - logprior: -2.5682e+00
Epoch 7/10
12/12 - 4s - loss: 424.4036 - loglik: -4.2137e+02 - logprior: -2.5183e+00
Epoch 8/10
12/12 - 4s - loss: 423.8462 - loglik: -4.2098e+02 - logprior: -2.3946e+00
Epoch 9/10
12/12 - 4s - loss: 423.5855 - loglik: -4.2073e+02 - logprior: -2.3781e+00
Epoch 10/10
12/12 - 4s - loss: 422.9230 - loglik: -4.2012e+02 - logprior: -2.3147e+00
Fitted a model with MAP estimate = -422.0727
expansions: [(11, 3), (19, 1), (30, 1), (31, 3), (32, 1), (41, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 3), (76, 1), (77, 2), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (127, 5), (128, 1), (130, 2), (131, 1), (132, 1), (146, 1), (147, 2), (149, 3), (153, 1), (163, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 461.9822 - loglik: -4.2106e+02 - logprior: -4.0902e+01
Epoch 2/2
12/12 - 5s - loss: 422.5598 - loglik: -4.0755e+02 - logprior: -1.4841e+01
Fitted a model with MAP estimate = -416.0813
expansions: [(0, 4), (106, 1)]
discards: [  0  34  95 154 217]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 438.0471 - loglik: -4.0724e+02 - logprior: -3.0772e+01
Epoch 2/2
12/12 - 5s - loss: 407.8591 - loglik: -4.0301e+02 - logprior: -4.6633e+00
Fitted a model with MAP estimate = -401.3273
expansions: []
discards: [  1   2   3 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 433.6290 - loglik: -4.0387e+02 - logprior: -2.9726e+01
Epoch 2/10
12/12 - 5s - loss: 405.0580 - loglik: -4.0075e+02 - logprior: -4.1268e+00
Epoch 3/10
12/12 - 5s - loss: 398.9139 - loglik: -3.9899e+02 - logprior: 0.4585
Epoch 4/10
12/12 - 5s - loss: 393.5822 - loglik: -3.9547e+02 - logprior: 2.4374
Epoch 5/10
12/12 - 5s - loss: 393.9413 - loglik: -3.9680e+02 - logprior: 3.4324
Fitted a model with MAP estimate = -392.0215
Time for alignment: 111.0528
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 610.4507 - loglik: -5.7556e+02 - logprior: -3.4874e+01
Epoch 2/10
12/12 - 4s - loss: 525.8288 - loglik: -5.1965e+02 - logprior: -6.1545e+00
Epoch 3/10
12/12 - 4s - loss: 467.1025 - loglik: -4.6354e+02 - logprior: -3.4268e+00
Epoch 4/10
12/12 - 4s - loss: 439.8079 - loglik: -4.3636e+02 - logprior: -2.9613e+00
Epoch 5/10
12/12 - 4s - loss: 431.9292 - loglik: -4.2855e+02 - logprior: -2.7101e+00
Epoch 6/10
12/12 - 4s - loss: 427.7750 - loglik: -4.2478e+02 - logprior: -2.5003e+00
Epoch 7/10
12/12 - 4s - loss: 427.3914 - loglik: -4.2446e+02 - logprior: -2.4928e+00
Epoch 8/10
12/12 - 4s - loss: 424.7842 - loglik: -4.2190e+02 - logprior: -2.4399e+00
Epoch 9/10
12/12 - 4s - loss: 424.3354 - loglik: -4.2140e+02 - logprior: -2.4782e+00
Epoch 10/10
12/12 - 4s - loss: 422.0721 - loglik: -4.1911e+02 - logprior: -2.4743e+00
Fitted a model with MAP estimate = -421.9070
expansions: [(11, 3), (19, 1), (24, 1), (29, 1), (30, 3), (31, 1), (40, 1), (47, 1), (59, 1), (60, 1), (61, 1), (62, 1), (74, 1), (75, 2), (76, 2), (77, 2), (85, 1), (90, 1), (92, 1), (101, 1), (102, 1), (120, 2), (126, 5), (127, 1), (130, 1), (131, 1), (132, 1), (146, 1), (147, 2), (149, 2), (152, 2), (153, 2), (172, 1), (173, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 463.3473 - loglik: -4.2224e+02 - logprior: -4.1067e+01
Epoch 2/2
12/12 - 5s - loss: 420.7942 - loglik: -4.0559e+02 - logprior: -1.4979e+01
Fitted a model with MAP estimate = -414.0924
expansions: [(0, 4), (211, 1)]
discards: [  0  36  94 149 155 156 157 195 221]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 438.3481 - loglik: -4.0737e+02 - logprior: -3.0947e+01
Epoch 2/2
12/12 - 5s - loss: 405.8359 - loglik: -4.0072e+02 - logprior: -4.9277e+00
Fitted a model with MAP estimate = -401.3916
expansions: []
discards: [  1   2   3 183 187]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 433.3984 - loglik: -4.0353e+02 - logprior: -2.9829e+01
Epoch 2/10
12/12 - 5s - loss: 405.1396 - loglik: -4.0077e+02 - logprior: -4.1826e+00
Epoch 3/10
12/12 - 5s - loss: 398.6940 - loglik: -3.9876e+02 - logprior: 0.4591
Epoch 4/10
12/12 - 5s - loss: 395.7790 - loglik: -3.9766e+02 - logprior: 2.3890
Epoch 5/10
12/12 - 5s - loss: 394.4626 - loglik: -3.9738e+02 - logprior: 3.4167
Epoch 6/10
12/12 - 5s - loss: 392.6319 - loglik: -3.9616e+02 - logprior: 3.9853
Epoch 7/10
12/12 - 5s - loss: 393.7423 - loglik: -3.9776e+02 - logprior: 4.4373
Fitted a model with MAP estimate = -391.8070
Time for alignment: 120.3508
Computed alignments with likelihoods: ['-392.4928', '-392.0215', '-391.8070']
Best model has likelihood: -391.8070
time for generating output: 0.3052
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9310860405056409
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b16e936d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69426e0b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69426e0af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f698304f790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6968bd0c40>, <__main__.SimpleDirichletPrior object at 0x7f6aa4b7dee0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 605.6591 - loglik: -5.9741e+02 - logprior: -8.2250e+00
Epoch 2/10
21/21 - 9s - loss: 481.8668 - loglik: -4.7888e+02 - logprior: -2.9018e+00
Epoch 3/10
21/21 - 9s - loss: 440.6208 - loglik: -4.3611e+02 - logprior: -3.9480e+00
Epoch 4/10
21/21 - 9s - loss: 434.4574 - loglik: -4.3016e+02 - logprior: -3.7310e+00
Epoch 5/10
21/21 - 9s - loss: 433.2519 - loglik: -4.2910e+02 - logprior: -3.6788e+00
Epoch 6/10
21/21 - 9s - loss: 428.5085 - loglik: -4.2434e+02 - logprior: -3.7280e+00
Epoch 7/10
21/21 - 9s - loss: 431.4219 - loglik: -4.2724e+02 - logprior: -3.7331e+00
Fitted a model with MAP estimate = -429.2124
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (78, 1), (97, 1), (100, 2), (115, 1), (119, 1), (120, 1), (121, 1), (122, 1), (131, 1), (143, 1), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (175, 1), (177, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 433.6790 - loglik: -4.2219e+02 - logprior: -1.1402e+01
Epoch 2/2
21/21 - 12s - loss: 404.7730 - loglik: -3.9945e+02 - logprior: -4.9519e+00
Fitted a model with MAP estimate = -399.7973
expansions: [(0, 5), (99, 1)]
discards: [  0  13  18  28  52  82 133 219]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 18s - loss: 411.5833 - loglik: -4.0352e+02 - logprior: -7.9773e+00
Epoch 2/2
21/21 - 12s - loss: 396.3180 - loglik: -3.9437e+02 - logprior: -1.5673e+00
Fitted a model with MAP estimate = -393.3938
expansions: []
discards: [ 1  2  3  4 51 86]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 406.5624 - loglik: -3.9899e+02 - logprior: -7.4775e+00
Epoch 2/10
21/21 - 12s - loss: 397.3713 - loglik: -3.9604e+02 - logprior: -9.4274e-01
Epoch 3/10
21/21 - 11s - loss: 393.8058 - loglik: -3.9293e+02 - logprior: -3.0567e-01
Epoch 4/10
21/21 - 12s - loss: 392.2645 - loglik: -3.9175e+02 - logprior: 0.0221
Epoch 5/10
21/21 - 12s - loss: 390.4567 - loglik: -3.9009e+02 - logprior: 0.1404
Epoch 6/10
21/21 - 11s - loss: 388.0841 - loglik: -3.8781e+02 - logprior: 0.2305
Epoch 7/10
21/21 - 11s - loss: 390.2884 - loglik: -3.9012e+02 - logprior: 0.3402
Fitted a model with MAP estimate = -387.6324
Time for alignment: 242.8425
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 606.1633 - loglik: -5.9791e+02 - logprior: -8.2265e+00
Epoch 2/10
21/21 - 9s - loss: 476.9058 - loglik: -4.7398e+02 - logprior: -2.8714e+00
Epoch 3/10
21/21 - 9s - loss: 433.2175 - loglik: -4.2893e+02 - logprior: -3.7626e+00
Epoch 4/10
21/21 - 9s - loss: 428.5664 - loglik: -4.2431e+02 - logprior: -3.6672e+00
Epoch 5/10
21/21 - 9s - loss: 422.9975 - loglik: -4.1872e+02 - logprior: -3.7163e+00
Epoch 6/10
21/21 - 9s - loss: 422.6000 - loglik: -4.1831e+02 - logprior: -3.7550e+00
Epoch 7/10
21/21 - 9s - loss: 421.8935 - loglik: -4.1763e+02 - logprior: -3.7484e+00
Epoch 8/10
21/21 - 9s - loss: 420.8389 - loglik: -4.1660e+02 - logprior: -3.7374e+00
Epoch 9/10
21/21 - 9s - loss: 421.8695 - loglik: -4.1761e+02 - logprior: -3.7388e+00
Fitted a model with MAP estimate = -420.1230
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (38, 1), (39, 1), (40, 1), (41, 1), (46, 1), (50, 1), (64, 3), (66, 1), (76, 8), (78, 1), (81, 2), (96, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (148, 1), (150, 1), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 424.5510 - loglik: -4.1322e+02 - logprior: -1.1253e+01
Epoch 2/2
21/21 - 12s - loss: 394.4837 - loglik: -3.8940e+02 - logprior: -4.7263e+00
Fitted a model with MAP estimate = -390.8103
expansions: [(0, 5)]
discards: [  0  18  28 110 152 218]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 17s - loss: 401.4990 - loglik: -3.9358e+02 - logprior: -7.8392e+00
Epoch 2/2
21/21 - 12s - loss: 388.3894 - loglik: -3.8660e+02 - logprior: -1.4346e+00
Fitted a model with MAP estimate = -385.2533
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 16s - loss: 398.4446 - loglik: -3.9107e+02 - logprior: -7.2921e+00
Epoch 2/10
21/21 - 11s - loss: 388.2695 - loglik: -3.8717e+02 - logprior: -7.3942e-01
Epoch 3/10
21/21 - 12s - loss: 383.9124 - loglik: -3.8325e+02 - logprior: -9.8376e-02
Epoch 4/10
21/21 - 12s - loss: 383.7098 - loglik: -3.8338e+02 - logprior: 0.2548
Epoch 5/10
21/21 - 12s - loss: 380.9727 - loglik: -3.8079e+02 - logprior: 0.3742
Epoch 6/10
21/21 - 11s - loss: 380.4012 - loglik: -3.8028e+02 - logprior: 0.4271
Epoch 7/10
21/21 - 12s - loss: 380.1981 - loglik: -3.8011e+02 - logprior: 0.4914
Epoch 8/10
21/21 - 12s - loss: 379.4507 - loglik: -3.7951e+02 - logprior: 0.6571
Epoch 9/10
21/21 - 11s - loss: 378.2928 - loglik: -3.7857e+02 - logprior: 0.8732
Epoch 10/10
21/21 - 12s - loss: 378.5553 - loglik: -3.7902e+02 - logprior: 1.0689
Fitted a model with MAP estimate = -377.1849
Time for alignment: 298.0799
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 605.5281 - loglik: -5.9727e+02 - logprior: -8.2359e+00
Epoch 2/10
21/21 - 10s - loss: 477.2433 - loglik: -4.7444e+02 - logprior: -2.7522e+00
Epoch 3/10
21/21 - 9s - loss: 433.6768 - loglik: -4.2952e+02 - logprior: -3.5663e+00
Epoch 4/10
21/21 - 9s - loss: 427.3823 - loglik: -4.2320e+02 - logprior: -3.4962e+00
Epoch 5/10
21/21 - 9s - loss: 424.0669 - loglik: -4.2001e+02 - logprior: -3.4783e+00
Epoch 6/10
21/21 - 9s - loss: 424.9187 - loglik: -4.2078e+02 - logprior: -3.6163e+00
Fitted a model with MAP estimate = -422.3218
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (60, 1), (63, 2), (66, 1), (76, 9), (78, 1), (95, 2), (96, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (147, 2), (158, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 3), (169, 1), (170, 1), (177, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 244 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 16s - loss: 417.8937 - loglik: -4.0957e+02 - logprior: -8.2399e+00
Epoch 2/2
21/21 - 12s - loss: 392.1418 - loglik: -3.8989e+02 - logprior: -1.8928e+00
Fitted a model with MAP estimate = -386.5623
expansions: []
discards: [ 24  28  85 125 153 219 220]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 397.8573 - loglik: -3.9004e+02 - logprior: -7.7342e+00
Epoch 2/2
21/21 - 12s - loss: 388.9869 - loglik: -3.8741e+02 - logprior: -1.1895e+00
Fitted a model with MAP estimate = -385.0780
expansions: []
discards: [96]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 396.5413 - loglik: -3.8907e+02 - logprior: -7.3883e+00
Epoch 2/10
21/21 - 12s - loss: 387.9872 - loglik: -3.8679e+02 - logprior: -8.4325e-01
Epoch 3/10
21/21 - 11s - loss: 384.4852 - loglik: -3.8375e+02 - logprior: -2.0093e-01
Epoch 4/10
21/21 - 12s - loss: 382.4611 - loglik: -3.8205e+02 - logprior: 0.1128
Epoch 5/10
21/21 - 12s - loss: 381.1170 - loglik: -3.8079e+02 - logprior: 0.1807
Epoch 6/10
21/21 - 11s - loss: 380.7922 - loglik: -3.8055e+02 - logprior: 0.2711
Epoch 7/10
21/21 - 12s - loss: 379.4295 - loglik: -3.7938e+02 - logprior: 0.4595
Epoch 8/10
21/21 - 11s - loss: 379.3665 - loglik: -3.7949e+02 - logprior: 0.6358
Epoch 9/10
21/21 - 12s - loss: 377.1039 - loglik: -3.7738e+02 - logprior: 0.8057
Epoch 10/10
21/21 - 12s - loss: 378.8118 - loglik: -3.7927e+02 - logprior: 1.0000
Fitted a model with MAP estimate = -377.2996
Time for alignment: 267.2441
Computed alignments with likelihoods: ['-387.6324', '-377.1849', '-377.2996']
Best model has likelihood: -377.1849
time for generating output: 0.4628
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.61890416117492
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6993fe1250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b1f3f9250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6993e45f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f696086a070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f696086a3d0>, <__main__.SimpleDirichletPrior object at 0x7f6a9c655580>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 283.3893 - loglik: -2.8014e+02 - logprior: -3.1316e+00
Epoch 2/10
19/19 - 3s - loss: 252.6174 - loglik: -2.5095e+02 - logprior: -1.0003e+00
Epoch 3/10
19/19 - 4s - loss: 242.4093 - loglik: -2.4054e+02 - logprior: -1.1110e+00
Epoch 4/10
19/19 - 3s - loss: 238.6202 - loglik: -2.3669e+02 - logprior: -1.0740e+00
Epoch 5/10
19/19 - 3s - loss: 236.7454 - loglik: -2.3477e+02 - logprior: -1.0677e+00
Epoch 6/10
19/19 - 3s - loss: 235.8406 - loglik: -2.3400e+02 - logprior: -1.0461e+00
Epoch 7/10
19/19 - 4s - loss: 236.2086 - loglik: -2.3445e+02 - logprior: -1.0243e+00
Fitted a model with MAP estimate = -233.3951
expansions: [(0, 12), (16, 3), (19, 2), (20, 1), (29, 2), (57, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 257.3636 - loglik: -2.5316e+02 - logprior: -4.1098e+00
Epoch 2/2
19/19 - 4s - loss: 239.5709 - loglik: -2.3769e+02 - logprior: -1.4169e+00
Fitted a model with MAP estimate = -235.0341
expansions: [(0, 8), (35, 1)]
discards: [12 13 14 15 16 17 18 19 37 48 79 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.3147 - loglik: -2.4024e+02 - logprior: -4.0019e+00
Epoch 2/2
19/19 - 4s - loss: 237.3878 - loglik: -2.3546e+02 - logprior: -1.4679e+00
Fitted a model with MAP estimate = -233.3118
expansions: [(0, 6), (30, 1)]
discards: [ 1  2  3  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 243.2913 - loglik: -2.3913e+02 - logprior: -4.0889e+00
Epoch 2/10
19/19 - 4s - loss: 237.4603 - loglik: -2.3563e+02 - logprior: -1.3897e+00
Epoch 3/10
19/19 - 4s - loss: 234.5434 - loglik: -2.3248e+02 - logprior: -1.0366e+00
Epoch 4/10
19/19 - 4s - loss: 231.4745 - loglik: -2.2926e+02 - logprior: -9.8277e-01
Epoch 5/10
19/19 - 4s - loss: 230.3793 - loglik: -2.2827e+02 - logprior: -1.0052e+00
Epoch 6/10
19/19 - 4s - loss: 229.5590 - loglik: -2.2759e+02 - logprior: -9.8690e-01
Epoch 7/10
19/19 - 4s - loss: 228.0327 - loglik: -2.2617e+02 - logprior: -9.7223e-01
Epoch 8/10
19/19 - 4s - loss: 228.0862 - loglik: -2.2624e+02 - logprior: -9.5725e-01
Fitted a model with MAP estimate = -226.2997
Time for alignment: 105.3465
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.3340 - loglik: -2.8008e+02 - logprior: -3.1345e+00
Epoch 2/10
19/19 - 4s - loss: 252.6002 - loglik: -2.5096e+02 - logprior: -1.0048e+00
Epoch 3/10
19/19 - 3s - loss: 242.3056 - loglik: -2.4029e+02 - logprior: -1.0830e+00
Epoch 4/10
19/19 - 4s - loss: 239.2446 - loglik: -2.3708e+02 - logprior: -1.0446e+00
Epoch 5/10
19/19 - 3s - loss: 237.1934 - loglik: -2.3521e+02 - logprior: -1.0503e+00
Epoch 6/10
19/19 - 3s - loss: 235.3872 - loglik: -2.3352e+02 - logprior: -1.0764e+00
Epoch 7/10
19/19 - 3s - loss: 234.8936 - loglik: -2.3308e+02 - logprior: -1.0564e+00
Epoch 8/10
19/19 - 3s - loss: 234.1048 - loglik: -2.3225e+02 - logprior: -1.0528e+00
Epoch 9/10
19/19 - 3s - loss: 233.7178 - loglik: -2.3181e+02 - logprior: -1.0501e+00
Epoch 10/10
19/19 - 3s - loss: 232.1131 - loglik: -2.3010e+02 - logprior: -1.0690e+00
Fitted a model with MAP estimate = -230.1756
expansions: [(0, 12), (11, 2), (15, 2), (22, 1), (54, 1), (57, 3), (59, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 264.5515 - loglik: -2.6015e+02 - logprior: -4.3076e+00
Epoch 2/2
19/19 - 4s - loss: 242.1241 - loglik: -2.4021e+02 - logprior: -1.4369e+00
Fitted a model with MAP estimate = -236.5372
expansions: [(0, 10)]
discards: [11 12 13 14 15 16 17 18 19 20 21 77 78 82]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.6124 - loglik: -2.4059e+02 - logprior: -3.9455e+00
Epoch 2/2
19/19 - 4s - loss: 238.2159 - loglik: -2.3632e+02 - logprior: -1.4507e+00
Fitted a model with MAP estimate = -234.3679
expansions: [(0, 6), (28, 1), (29, 1)]
discards: [0 1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.5324 - loglik: -2.3800e+02 - logprior: -3.4545e+00
Epoch 2/10
19/19 - 4s - loss: 237.0095 - loglik: -2.3535e+02 - logprior: -1.2113e+00
Epoch 3/10
19/19 - 4s - loss: 233.9370 - loglik: -2.3178e+02 - logprior: -1.1132e+00
Epoch 4/10
19/19 - 4s - loss: 231.9140 - loglik: -2.2963e+02 - logprior: -1.0142e+00
Epoch 5/10
19/19 - 4s - loss: 230.6060 - loglik: -2.2848e+02 - logprior: -1.0162e+00
Epoch 6/10
19/19 - 4s - loss: 229.7786 - loglik: -2.2776e+02 - logprior: -1.0198e+00
Epoch 7/10
19/19 - 4s - loss: 228.6119 - loglik: -2.2667e+02 - logprior: -1.0186e+00
Epoch 8/10
19/19 - 4s - loss: 227.7236 - loglik: -2.2580e+02 - logprior: -1.0074e+00
Epoch 9/10
19/19 - 4s - loss: 226.3100 - loglik: -2.2433e+02 - logprior: -1.0101e+00
Epoch 10/10
19/19 - 4s - loss: 226.1543 - loglik: -2.2417e+02 - logprior: -9.9688e-01
Fitted a model with MAP estimate = -224.6014
Time for alignment: 122.8504
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 284.3976 - loglik: -2.8113e+02 - logprior: -3.1568e+00
Epoch 2/10
19/19 - 3s - loss: 254.5686 - loglik: -2.5286e+02 - logprior: -1.0262e+00
Epoch 3/10
19/19 - 3s - loss: 242.5619 - loglik: -2.4058e+02 - logprior: -1.1605e+00
Epoch 4/10
19/19 - 3s - loss: 237.8700 - loglik: -2.3587e+02 - logprior: -1.1631e+00
Epoch 5/10
19/19 - 3s - loss: 236.8376 - loglik: -2.3477e+02 - logprior: -1.1482e+00
Epoch 6/10
19/19 - 3s - loss: 235.7320 - loglik: -2.3377e+02 - logprior: -1.1341e+00
Epoch 7/10
19/19 - 3s - loss: 234.8035 - loglik: -2.3292e+02 - logprior: -1.1182e+00
Epoch 8/10
19/19 - 3s - loss: 234.6469 - loglik: -2.3280e+02 - logprior: -1.1098e+00
Epoch 9/10
19/19 - 3s - loss: 233.5509 - loglik: -2.3168e+02 - logprior: -1.1156e+00
Epoch 10/10
19/19 - 3s - loss: 233.7350 - loglik: -2.3184e+02 - logprior: -1.1180e+00
Fitted a model with MAP estimate = -231.2333
expansions: [(0, 11), (17, 1), (20, 2), (32, 2), (55, 1), (57, 3), (58, 1), (59, 1), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 265.7649 - loglik: -2.6141e+02 - logprior: -4.2547e+00
Epoch 2/2
19/19 - 4s - loss: 241.7850 - loglik: -2.3983e+02 - logprior: -1.4895e+00
Fitted a model with MAP estimate = -236.3494
expansions: [(0, 12), (35, 2)]
discards: [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 46 76 77 83]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 245.4913 - loglik: -2.4138e+02 - logprior: -4.0374e+00
Epoch 2/2
19/19 - 4s - loss: 238.5127 - loglik: -2.3651e+02 - logprior: -1.5522e+00
Fitted a model with MAP estimate = -234.4146
expansions: [(0, 5), (16, 2), (17, 2), (21, 1), (33, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.8924 - loglik: -2.3800e+02 - logprior: -3.8107e+00
Epoch 2/10
19/19 - 4s - loss: 236.3224 - loglik: -2.3437e+02 - logprior: -1.5452e+00
Epoch 3/10
19/19 - 4s - loss: 233.3688 - loglik: -2.3141e+02 - logprior: -1.3191e+00
Epoch 4/10
19/19 - 4s - loss: 231.0002 - loglik: -2.2891e+02 - logprior: -1.2169e+00
Epoch 5/10
19/19 - 4s - loss: 229.8460 - loglik: -2.2778e+02 - logprior: -1.1095e+00
Epoch 6/10
19/19 - 4s - loss: 228.6247 - loglik: -2.2663e+02 - logprior: -1.0569e+00
Epoch 7/10
19/19 - 4s - loss: 228.8062 - loglik: -2.2689e+02 - logprior: -1.0192e+00
Fitted a model with MAP estimate = -226.8079
Time for alignment: 112.0555
Computed alignments with likelihoods: ['-226.2997', '-224.6014', '-226.8079']
Best model has likelihood: -224.6014
time for generating output: 0.2578
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.464
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b2fb51370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69a4c21370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2edbdf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b16249d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69e91ce6d0>, <__main__.SimpleDirichletPrior object at 0x7f69b5718fa0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 732.6891 - loglik: -7.1915e+02 - logprior: -1.3519e+01
Epoch 2/10
17/17 - 9s - loss: 557.4641 - loglik: -5.5410e+02 - logprior: -3.3404e+00
Epoch 3/10
17/17 - 9s - loss: 460.8032 - loglik: -4.5586e+02 - logprior: -4.9213e+00
Epoch 4/10
17/17 - 9s - loss: 435.2794 - loglik: -4.2928e+02 - logprior: -5.7605e+00
Epoch 5/10
17/17 - 9s - loss: 428.4445 - loglik: -4.2252e+02 - logprior: -5.4096e+00
Epoch 6/10
17/17 - 9s - loss: 426.9529 - loglik: -4.2118e+02 - logprior: -5.1964e+00
Epoch 7/10
17/17 - 9s - loss: 426.3162 - loglik: -4.2060e+02 - logprior: -5.1628e+00
Epoch 8/10
17/17 - 9s - loss: 426.6281 - loglik: -4.2094e+02 - logprior: -5.1592e+00
Fitted a model with MAP estimate = -424.1677
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 3), (42, 1), (45, 1), (57, 1), (58, 2), (60, 1), (61, 1), (63, 1), (70, 6), (86, 1), (94, 2), (96, 1), (99, 1), (113, 1), (114, 1), (115, 1), (122, 1), (128, 2), (130, 1), (141, 1), (155, 1), (160, 1), (163, 1), (173, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 431.6364 - loglik: -4.1344e+02 - logprior: -1.8136e+01
Epoch 2/2
17/17 - 12s - loss: 393.0864 - loglik: -3.8592e+02 - logprior: -6.8605e+00
Fitted a model with MAP estimate = -386.2384
expansions: [(0, 9)]
discards: [  0  39  88  89  90  91 159]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 16s - loss: 401.8181 - loglik: -3.8887e+02 - logprior: -1.2903e+01
Epoch 2/2
17/17 - 12s - loss: 382.8610 - loglik: -3.8083e+02 - logprior: -1.8217e+00
Fitted a model with MAP estimate = -380.2858
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 397.0433 - loglik: -3.8497e+02 - logprior: -1.2001e+01
Epoch 2/10
17/17 - 11s - loss: 381.4738 - loglik: -3.8030e+02 - logprior: -8.7979e-01
Epoch 3/10
17/17 - 12s - loss: 379.1877 - loglik: -3.7933e+02 - logprior: 0.5371
Epoch 4/10
17/17 - 11s - loss: 375.9493 - loglik: -3.7673e+02 - logprior: 1.1989
Epoch 5/10
17/17 - 12s - loss: 374.9388 - loglik: -3.7601e+02 - logprior: 1.5152
Epoch 6/10
17/17 - 11s - loss: 371.9515 - loglik: -3.7318e+02 - logprior: 1.6677
Epoch 7/10
17/17 - 12s - loss: 369.7726 - loglik: -3.7127e+02 - logprior: 1.9626
Epoch 8/10
17/17 - 11s - loss: 372.8988 - loglik: -3.7467e+02 - logprior: 2.2407
Fitted a model with MAP estimate = -369.7849
Time for alignment: 256.4119
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 733.7729 - loglik: -7.2022e+02 - logprior: -1.3541e+01
Epoch 2/10
17/17 - 9s - loss: 548.5567 - loglik: -5.4523e+02 - logprior: -3.2972e+00
Epoch 3/10
17/17 - 9s - loss: 453.1486 - loglik: -4.4822e+02 - logprior: -4.8926e+00
Epoch 4/10
17/17 - 9s - loss: 434.1183 - loglik: -4.2831e+02 - logprior: -5.4500e+00
Epoch 5/10
17/17 - 9s - loss: 428.2756 - loglik: -4.2255e+02 - logprior: -5.1276e+00
Epoch 6/10
17/17 - 9s - loss: 427.2367 - loglik: -4.2177e+02 - logprior: -4.9332e+00
Epoch 7/10
17/17 - 9s - loss: 426.0034 - loglik: -4.2050e+02 - logprior: -5.0242e+00
Epoch 8/10
17/17 - 9s - loss: 425.3227 - loglik: -4.1976e+02 - logprior: -5.0665e+00
Epoch 9/10
17/17 - 9s - loss: 424.9711 - loglik: -4.1938e+02 - logprior: -5.0847e+00
Epoch 10/10
17/17 - 9s - loss: 423.7732 - loglik: -4.1816e+02 - logprior: -5.0875e+00
Fitted a model with MAP estimate = -423.1366
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (22, 1), (26, 1), (32, 2), (45, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (87, 4), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (181, 1), (182, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 431.6750 - loglik: -4.1345e+02 - logprior: -1.8153e+01
Epoch 2/2
17/17 - 11s - loss: 391.2325 - loglik: -3.8407e+02 - logprior: -6.8137e+00
Fitted a model with MAP estimate = -386.9605
expansions: [(0, 10)]
discards: [  0 104 105 137 157]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 17s - loss: 400.1472 - loglik: -3.8695e+02 - logprior: -1.3134e+01
Epoch 2/2
17/17 - 12s - loss: 382.4433 - loglik: -3.8025e+02 - logprior: -1.8917e+00
Fitted a model with MAP estimate = -380.0304
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 395.8334 - loglik: -3.8375e+02 - logprior: -1.2024e+01
Epoch 2/10
17/17 - 12s - loss: 382.3224 - loglik: -3.8109e+02 - logprior: -9.3724e-01
Epoch 3/10
17/17 - 12s - loss: 379.9331 - loglik: -3.8001e+02 - logprior: 0.5301
Epoch 4/10
17/17 - 11s - loss: 374.7708 - loglik: -3.7550e+02 - logprior: 1.2109
Epoch 5/10
17/17 - 11s - loss: 374.0367 - loglik: -3.7512e+02 - logprior: 1.5428
Epoch 6/10
17/17 - 12s - loss: 371.8340 - loglik: -3.7309e+02 - logprior: 1.6968
Epoch 7/10
17/17 - 11s - loss: 372.8322 - loglik: -3.7432e+02 - logprior: 1.9444
Fitted a model with MAP estimate = -370.2912
Time for alignment: 263.5871
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 734.4198 - loglik: -7.2088e+02 - logprior: -1.3523e+01
Epoch 2/10
17/17 - 9s - loss: 552.4946 - loglik: -5.4916e+02 - logprior: -3.3129e+00
Epoch 3/10
17/17 - 9s - loss: 458.2050 - loglik: -4.5335e+02 - logprior: -4.8017e+00
Epoch 4/10
17/17 - 9s - loss: 435.3004 - loglik: -4.2925e+02 - logprior: -5.6395e+00
Epoch 5/10
17/17 - 9s - loss: 429.8829 - loglik: -4.2386e+02 - logprior: -5.4421e+00
Epoch 6/10
17/17 - 9s - loss: 428.1674 - loglik: -4.2237e+02 - logprior: -5.2955e+00
Epoch 7/10
17/17 - 9s - loss: 426.5254 - loglik: -4.2072e+02 - logprior: -5.3387e+00
Epoch 8/10
17/17 - 9s - loss: 426.3837 - loglik: -4.2057e+02 - logprior: -5.3292e+00
Epoch 9/10
17/17 - 9s - loss: 424.9720 - loglik: -4.1915e+02 - logprior: -5.3285e+00
Epoch 10/10
17/17 - 9s - loss: 425.0641 - loglik: -4.1924e+02 - logprior: -5.3248e+00
Fitted a model with MAP estimate = -424.3758
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 2), (46, 1), (55, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 1), (123, 1), (129, 2), (131, 1), (142, 1), (151, 1), (160, 1), (161, 1), (162, 1), (163, 1), (179, 1), (181, 2), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 424.2088 - loglik: -4.1067e+02 - logprior: -1.3468e+01
Epoch 2/2
17/17 - 11s - loss: 384.8286 - loglik: -3.8229e+02 - logprior: -2.2102e+00
Fitted a model with MAP estimate = -380.8012
expansions: []
discards: [  0 155]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 404.2998 - loglik: -3.8692e+02 - logprior: -1.7337e+01
Epoch 2/2
17/17 - 12s - loss: 390.8242 - loglik: -3.8438e+02 - logprior: -6.1939e+00
Fitted a model with MAP estimate = -386.1560
expansions: [(0, 9)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 16s - loss: 398.5831 - loglik: -3.8597e+02 - logprior: -1.2552e+01
Epoch 2/10
17/17 - 12s - loss: 381.8813 - loglik: -3.8031e+02 - logprior: -1.2865e+00
Epoch 3/10
17/17 - 12s - loss: 380.3521 - loglik: -3.8009e+02 - logprior: 0.1378
Epoch 4/10
17/17 - 12s - loss: 375.2829 - loglik: -3.7534e+02 - logprior: 0.4750
Epoch 5/10
17/17 - 12s - loss: 371.9344 - loglik: -3.7257e+02 - logprior: 1.0643
Epoch 6/10
17/17 - 12s - loss: 373.9871 - loglik: -3.7494e+02 - logprior: 1.3965
Fitted a model with MAP estimate = -370.7224
Time for alignment: 253.2754
Computed alignments with likelihoods: ['-369.7849', '-370.2912', '-370.7224']
Best model has likelihood: -369.7849
time for generating output: 0.3318
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.956721562619686
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f67c039a580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ad2fc5e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4e107f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69722b4220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac88441c0>, <__main__.SimpleDirichletPrior object at 0x7f69574c1ee0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.7836 - loglik: -1.5944e+02 - logprior: -2.0321e+01
Epoch 2/10
10/10 - 1s - loss: 148.7138 - loglik: -1.4268e+02 - logprior: -6.0107e+00
Epoch 3/10
10/10 - 1s - loss: 131.0052 - loglik: -1.2744e+02 - logprior: -3.5633e+00
Epoch 4/10
10/10 - 1s - loss: 118.9331 - loglik: -1.1584e+02 - logprior: -3.0807e+00
Epoch 5/10
10/10 - 1s - loss: 113.8019 - loglik: -1.1057e+02 - logprior: -3.0949e+00
Epoch 6/10
10/10 - 1s - loss: 111.3398 - loglik: -1.0788e+02 - logprior: -3.0853e+00
Epoch 7/10
10/10 - 1s - loss: 111.2283 - loglik: -1.0785e+02 - logprior: -2.9586e+00
Epoch 8/10
10/10 - 1s - loss: 110.4034 - loglik: -1.0726e+02 - logprior: -2.8031e+00
Epoch 9/10
10/10 - 1s - loss: 110.2876 - loglik: -1.0726e+02 - logprior: -2.7234e+00
Epoch 10/10
10/10 - 1s - loss: 110.0561 - loglik: -1.0700e+02 - logprior: -2.7169e+00
Fitted a model with MAP estimate = -109.6581
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 136.0140 - loglik: -1.1305e+02 - logprior: -2.2938e+01
Epoch 2/2
10/10 - 1s - loss: 116.5777 - loglik: -1.0633e+02 - logprior: -1.0099e+01
Fitted a model with MAP estimate = -112.7990
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 121.9876 - loglik: -1.0375e+02 - logprior: -1.8212e+01
Epoch 2/2
10/10 - 1s - loss: 108.1943 - loglik: -1.0275e+02 - logprior: -5.3012e+00
Fitted a model with MAP estimate = -106.0132
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 125.2436 - loglik: -1.0457e+02 - logprior: -2.0646e+01
Epoch 2/10
10/10 - 1s - loss: 110.4909 - loglik: -1.0416e+02 - logprior: -6.2023e+00
Epoch 3/10
10/10 - 1s - loss: 106.7037 - loglik: -1.0311e+02 - logprior: -3.3635e+00
Epoch 4/10
10/10 - 1s - loss: 105.5621 - loglik: -1.0270e+02 - logprior: -2.5583e+00
Epoch 5/10
10/10 - 1s - loss: 104.6176 - loglik: -1.0229e+02 - logprior: -1.9900e+00
Epoch 6/10
10/10 - 1s - loss: 104.0570 - loglik: -1.0199e+02 - logprior: -1.7429e+00
Epoch 7/10
10/10 - 1s - loss: 103.9187 - loglik: -1.0200e+02 - logprior: -1.6256e+00
Epoch 8/10
10/10 - 1s - loss: 103.7859 - loglik: -1.0201e+02 - logprior: -1.4838e+00
Epoch 9/10
10/10 - 1s - loss: 103.5248 - loglik: -1.0180e+02 - logprior: -1.4239e+00
Epoch 10/10
10/10 - 1s - loss: 103.2664 - loglik: -1.0157e+02 - logprior: -1.3963e+00
Fitted a model with MAP estimate = -103.0305
Time for alignment: 36.4974
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.6798 - loglik: -1.5934e+02 - logprior: -2.0322e+01
Epoch 2/10
10/10 - 1s - loss: 148.5007 - loglik: -1.4247e+02 - logprior: -6.0093e+00
Epoch 3/10
10/10 - 1s - loss: 131.4142 - loglik: -1.2788e+02 - logprior: -3.5328e+00
Epoch 4/10
10/10 - 1s - loss: 120.1360 - loglik: -1.1716e+02 - logprior: -2.9708e+00
Epoch 5/10
10/10 - 1s - loss: 113.9893 - loglik: -1.1091e+02 - logprior: -2.9666e+00
Epoch 6/10
10/10 - 1s - loss: 111.7397 - loglik: -1.0842e+02 - logprior: -2.9924e+00
Epoch 7/10
10/10 - 1s - loss: 110.7428 - loglik: -1.0747e+02 - logprior: -2.9089e+00
Epoch 8/10
10/10 - 1s - loss: 110.5194 - loglik: -1.0745e+02 - logprior: -2.7893e+00
Epoch 9/10
10/10 - 1s - loss: 110.1676 - loglik: -1.0720e+02 - logprior: -2.7302e+00
Epoch 10/10
10/10 - 1s - loss: 109.6312 - loglik: -1.0663e+02 - logprior: -2.7279e+00
Fitted a model with MAP estimate = -109.4801
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 136.1243 - loglik: -1.1315e+02 - logprior: -2.2953e+01
Epoch 2/2
10/10 - 1s - loss: 116.1328 - loglik: -1.0591e+02 - logprior: -1.0082e+01
Fitted a model with MAP estimate = -112.7501
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.9307 - loglik: -1.0370e+02 - logprior: -1.8202e+01
Epoch 2/2
10/10 - 1s - loss: 108.1405 - loglik: -1.0271e+02 - logprior: -5.2984e+00
Fitted a model with MAP estimate = -106.0233
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 125.2418 - loglik: -1.0457e+02 - logprior: -2.0653e+01
Epoch 2/10
10/10 - 1s - loss: 110.2846 - loglik: -1.0397e+02 - logprior: -6.2055e+00
Epoch 3/10
10/10 - 1s - loss: 106.9239 - loglik: -1.0334e+02 - logprior: -3.3658e+00
Epoch 4/10
10/10 - 1s - loss: 105.1641 - loglik: -1.0228e+02 - logprior: -2.5915e+00
Epoch 5/10
10/10 - 1s - loss: 104.6435 - loglik: -1.0231e+02 - logprior: -2.0216e+00
Epoch 6/10
10/10 - 1s - loss: 103.9033 - loglik: -1.0187e+02 - logprior: -1.7380e+00
Epoch 7/10
10/10 - 1s - loss: 104.0445 - loglik: -1.0213e+02 - logprior: -1.6185e+00
Fitted a model with MAP estimate = -103.4014
Time for alignment: 33.5518
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.7387 - loglik: -1.5940e+02 - logprior: -2.0320e+01
Epoch 2/10
10/10 - 1s - loss: 148.5957 - loglik: -1.4256e+02 - logprior: -6.0108e+00
Epoch 3/10
10/10 - 1s - loss: 131.5511 - loglik: -1.2801e+02 - logprior: -3.5436e+00
Epoch 4/10
10/10 - 1s - loss: 119.9994 - loglik: -1.1696e+02 - logprior: -3.0173e+00
Epoch 5/10
10/10 - 1s - loss: 113.4269 - loglik: -1.1021e+02 - logprior: -3.0300e+00
Epoch 6/10
10/10 - 1s - loss: 111.9954 - loglik: -1.0858e+02 - logprior: -2.9997e+00
Epoch 7/10
10/10 - 1s - loss: 110.6335 - loglik: -1.0732e+02 - logprior: -2.9152e+00
Epoch 8/10
10/10 - 1s - loss: 110.1956 - loglik: -1.0707e+02 - logprior: -2.8102e+00
Epoch 9/10
10/10 - 1s - loss: 110.0385 - loglik: -1.0699e+02 - logprior: -2.7357e+00
Epoch 10/10
10/10 - 1s - loss: 109.7034 - loglik: -1.0668e+02 - logprior: -2.7132e+00
Fitted a model with MAP estimate = -109.3226
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.7009 - loglik: -1.1273e+02 - logprior: -2.2948e+01
Epoch 2/2
10/10 - 1s - loss: 116.2415 - loglik: -1.0602e+02 - logprior: -1.0084e+01
Fitted a model with MAP estimate = -112.7356
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.9019 - loglik: -1.0367e+02 - logprior: -1.8205e+01
Epoch 2/2
10/10 - 1s - loss: 108.2025 - loglik: -1.0278e+02 - logprior: -5.2952e+00
Fitted a model with MAP estimate = -106.0268
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 125.1189 - loglik: -1.0444e+02 - logprior: -2.0656e+01
Epoch 2/10
10/10 - 1s - loss: 110.4885 - loglik: -1.0417e+02 - logprior: -6.2091e+00
Epoch 3/10
10/10 - 1s - loss: 106.8607 - loglik: -1.0328e+02 - logprior: -3.3567e+00
Epoch 4/10
10/10 - 1s - loss: 105.4883 - loglik: -1.0262e+02 - logprior: -2.5808e+00
Epoch 5/10
10/10 - 1s - loss: 104.7203 - loglik: -1.0242e+02 - logprior: -2.0074e+00
Epoch 6/10
10/10 - 1s - loss: 103.9457 - loglik: -1.0194e+02 - logprior: -1.7195e+00
Epoch 7/10
10/10 - 1s - loss: 103.9442 - loglik: -1.0206e+02 - logprior: -1.5958e+00
Epoch 8/10
10/10 - 1s - loss: 103.7433 - loglik: -1.0199e+02 - logprior: -1.4727e+00
Epoch 9/10
10/10 - 1s - loss: 103.4404 - loglik: -1.0174e+02 - logprior: -1.4109e+00
Epoch 10/10
10/10 - 1s - loss: 103.6032 - loglik: -1.0193e+02 - logprior: -1.3799e+00
Fitted a model with MAP estimate = -103.0421
Time for alignment: 35.9552
Computed alignments with likelihoods: ['-103.0305', '-103.4014', '-103.0421']
Best model has likelihood: -103.0305
time for generating output: 0.1137
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9928712871287129
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a821acd00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac930dcd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fcc34f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a83495520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6afd593970>, <__main__.SimpleDirichletPrior object at 0x7f6942939940>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 469.1038 - loglik: -4.6709e+02 - logprior: -1.9931e+00
Epoch 2/10
39/39 - 11s - loss: 384.3702 - loglik: -3.8264e+02 - logprior: -1.4559e+00
Epoch 3/10
39/39 - 11s - loss: 375.6609 - loglik: -3.7354e+02 - logprior: -1.5490e+00
Epoch 4/10
39/39 - 11s - loss: 372.1553 - loglik: -3.6990e+02 - logprior: -1.6033e+00
Epoch 5/10
39/39 - 11s - loss: 371.0465 - loglik: -3.6884e+02 - logprior: -1.6465e+00
Epoch 6/10
39/39 - 11s - loss: 370.2133 - loglik: -3.6805e+02 - logprior: -1.6483e+00
Epoch 7/10
39/39 - 11s - loss: 369.5753 - loglik: -3.6742e+02 - logprior: -1.6497e+00
Epoch 8/10
39/39 - 11s - loss: 369.2669 - loglik: -3.6711e+02 - logprior: -1.6654e+00
Epoch 9/10
39/39 - 11s - loss: 368.8222 - loglik: -3.6666e+02 - logprior: -1.6819e+00
Epoch 10/10
39/39 - 11s - loss: 368.8112 - loglik: -3.6666e+02 - logprior: -1.6777e+00
Fitted a model with MAP estimate = -305.3763
expansions: [(0, 13), (10, 1), (15, 1), (18, 1), (28, 1), (30, 1), (31, 1), (32, 2), (33, 1), (38, 1), (43, 1), (44, 1), (45, 1), (56, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 362.2121 - loglik: -3.5887e+02 - logprior: -3.1601e+00
Epoch 2/2
39/39 - 13s - loss: 347.1428 - loglik: -3.4498e+02 - logprior: -1.8151e+00
Fitted a model with MAP estimate = -287.8896
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11 51]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 352.6276 - loglik: -3.5006e+02 - logprior: -2.4106e+00
Epoch 2/2
39/39 - 13s - loss: 348.3866 - loglik: -3.4678e+02 - logprior: -1.2460e+00
Fitted a model with MAP estimate = -289.9707
expansions: [(0, 15), (35, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 21s - loss: 287.9203 - loglik: -2.8586e+02 - logprior: -1.9011e+00
Epoch 2/10
52/52 - 16s - loss: 283.7967 - loglik: -2.8189e+02 - logprior: -1.6051e+00
Epoch 3/10
52/52 - 17s - loss: 280.6224 - loglik: -2.7869e+02 - logprior: -1.5599e+00
Epoch 4/10
52/52 - 17s - loss: 281.2237 - loglik: -2.7931e+02 - logprior: -1.5031e+00
Fitted a model with MAP estimate = -278.6853
Time for alignment: 336.0514
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 466.3421 - loglik: -4.6433e+02 - logprior: -1.9881e+00
Epoch 2/10
39/39 - 11s - loss: 382.1374 - loglik: -3.8043e+02 - logprior: -1.4834e+00
Epoch 3/10
39/39 - 11s - loss: 373.6431 - loglik: -3.7145e+02 - logprior: -1.6074e+00
Epoch 4/10
39/39 - 11s - loss: 370.8844 - loglik: -3.6866e+02 - logprior: -1.6010e+00
Epoch 5/10
39/39 - 11s - loss: 370.0842 - loglik: -3.6795e+02 - logprior: -1.5775e+00
Epoch 6/10
39/39 - 11s - loss: 369.1891 - loglik: -3.6709e+02 - logprior: -1.5773e+00
Epoch 7/10
39/39 - 12s - loss: 368.6396 - loglik: -3.6657e+02 - logprior: -1.5612e+00
Epoch 8/10
39/39 - 11s - loss: 368.2844 - loglik: -3.6624e+02 - logprior: -1.5583e+00
Epoch 9/10
39/39 - 11s - loss: 368.0162 - loglik: -3.6598e+02 - logprior: -1.5574e+00
Epoch 10/10
39/39 - 11s - loss: 367.6465 - loglik: -3.6559e+02 - logprior: -1.5614e+00
Fitted a model with MAP estimate = -304.4301
expansions: [(0, 17), (10, 1), (15, 1), (18, 1), (28, 1), (29, 2), (34, 3), (37, 1), (42, 1), (43, 1), (44, 1), (71, 1), (88, 3), (89, 1), (90, 1), (102, 1), (104, 1), (107, 1), (126, 10), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 357.3913 - loglik: -3.5401e+02 - logprior: -3.1875e+00
Epoch 2/2
39/39 - 15s - loss: 339.8732 - loglik: -3.3719e+02 - logprior: -2.3794e+00
Fitted a model with MAP estimate = -281.6974
expansions: [(169, 1), (170, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  50  57 180]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 346.0014 - loglik: -3.4321e+02 - logprior: -2.6340e+00
Epoch 2/2
39/39 - 14s - loss: 341.2685 - loglik: -3.3969e+02 - logprior: -1.2501e+00
Fitted a model with MAP estimate = -284.8996
expansions: [(0, 17), (39, 1)]
discards: [154]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 21s - loss: 280.9508 - loglik: -2.7900e+02 - logprior: -1.7951e+00
Epoch 2/10
52/52 - 17s - loss: 276.2257 - loglik: -2.7447e+02 - logprior: -1.4844e+00
Epoch 3/10
52/52 - 18s - loss: 275.8413 - loglik: -2.7405e+02 - logprior: -1.4567e+00
Epoch 4/10
52/52 - 18s - loss: 272.8094 - loglik: -2.7102e+02 - logprior: -1.4119e+00
Epoch 5/10
52/52 - 16s - loss: 271.8152 - loglik: -2.7010e+02 - logprior: -1.3442e+00
Epoch 6/10
52/52 - 18s - loss: 271.4389 - loglik: -2.6981e+02 - logprior: -1.2815e+00
Epoch 7/10
52/52 - 19s - loss: 270.4563 - loglik: -2.6889e+02 - logprior: -1.2205e+00
Epoch 8/10
52/52 - 17s - loss: 269.4683 - loglik: -2.6796e+02 - logprior: -1.1544e+00
Epoch 9/10
52/52 - 17s - loss: 270.6066 - loglik: -2.6916e+02 - logprior: -1.0878e+00
Fitted a model with MAP estimate = -269.2994
Time for alignment: 433.3077
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 468.2177 - loglik: -4.6622e+02 - logprior: -1.9742e+00
Epoch 2/10
39/39 - 11s - loss: 386.8849 - loglik: -3.8536e+02 - logprior: -1.2958e+00
Epoch 3/10
39/39 - 11s - loss: 377.8688 - loglik: -3.7599e+02 - logprior: -1.3685e+00
Epoch 4/10
39/39 - 11s - loss: 375.0114 - loglik: -3.7300e+02 - logprior: -1.4041e+00
Epoch 5/10
39/39 - 11s - loss: 373.8556 - loglik: -3.7191e+02 - logprior: -1.3881e+00
Epoch 6/10
39/39 - 11s - loss: 372.4828 - loglik: -3.7057e+02 - logprior: -1.3759e+00
Epoch 7/10
39/39 - 11s - loss: 372.2970 - loglik: -3.7038e+02 - logprior: -1.3895e+00
Epoch 8/10
39/39 - 11s - loss: 371.6674 - loglik: -3.6978e+02 - logprior: -1.3749e+00
Epoch 9/10
39/39 - 12s - loss: 371.1335 - loglik: -3.6926e+02 - logprior: -1.3555e+00
Epoch 10/10
39/39 - 11s - loss: 371.0396 - loglik: -3.6913e+02 - logprior: -1.3682e+00
Fitted a model with MAP estimate = -308.1276
expansions: [(0, 14), (10, 1), (11, 1), (18, 1), (28, 2), (29, 2), (30, 1), (32, 3), (33, 1), (37, 1), (43, 1), (44, 3), (71, 2), (88, 1), (89, 1), (91, 1), (102, 1), (103, 1), (104, 2), (107, 2), (126, 4), (129, 4), (134, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 362.3673 - loglik: -3.5896e+02 - logprior: -3.2227e+00
Epoch 2/2
39/39 - 14s - loss: 341.6201 - loglik: -3.3939e+02 - logprior: -1.9560e+00
Fitted a model with MAP estimate = -282.2451
expansions: [(156, 1), (170, 1), (177, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  47  48  55  73 102 143
 147 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 351.1987 - loglik: -3.4876e+02 - logprior: -2.2788e+00
Epoch 2/2
39/39 - 14s - loss: 345.6101 - loglik: -3.4420e+02 - logprior: -1.0544e+00
Fitted a model with MAP estimate = -287.5764
expansions: [(0, 15), (156, 4), (168, 7)]
discards: [43]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 22s - loss: 284.1409 - loglik: -2.8200e+02 - logprior: -1.9806e+00
Epoch 2/10
52/52 - 18s - loss: 275.4373 - loglik: -2.7346e+02 - logprior: -1.6981e+00
Epoch 3/10
52/52 - 17s - loss: 275.2238 - loglik: -2.7321e+02 - logprior: -1.6887e+00
Epoch 4/10
52/52 - 19s - loss: 274.0001 - loglik: -2.7198e+02 - logprior: -1.6501e+00
Epoch 5/10
52/52 - 19s - loss: 271.7963 - loglik: -2.6988e+02 - logprior: -1.5925e+00
Epoch 6/10
52/52 - 17s - loss: 271.5297 - loglik: -2.6967e+02 - logprior: -1.5318e+00
Epoch 7/10
52/52 - 19s - loss: 271.4136 - loglik: -2.6961e+02 - logprior: -1.4741e+00
Epoch 8/10
52/52 - 19s - loss: 270.3260 - loglik: -2.6858e+02 - logprior: -1.4089e+00
Epoch 9/10
52/52 - 17s - loss: 270.5679 - loglik: -2.6890e+02 - logprior: -1.3387e+00
Fitted a model with MAP estimate = -269.6738
Time for alignment: 435.8360
Computed alignments with likelihoods: ['-278.6853', '-269.2994', '-269.6738']
Best model has likelihood: -269.2994
time for generating output: 0.7488
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.22630726735879608
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f694396ce80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f694dfb0700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694dfb06a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6942edeca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f67c00bbd90>, <__main__.SimpleDirichletPrior object at 0x7f6ac76dc520>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 814.7070 - loglik: -8.1226e+02 - logprior: -1.9693e+00
Epoch 2/10
37/37 - 27s - loss: 725.3221 - loglik: -7.2268e+02 - logprior: -1.2018e+00
Epoch 3/10
37/37 - 27s - loss: 708.7307 - loglik: -7.0567e+02 - logprior: -1.5742e+00
Epoch 4/10
37/37 - 26s - loss: 704.0607 - loglik: -7.0084e+02 - logprior: -1.5644e+00
Epoch 5/10
37/37 - 26s - loss: 702.5310 - loglik: -6.9924e+02 - logprior: -1.7783e+00
Epoch 6/10
37/37 - 26s - loss: 699.8553 - loglik: -6.9688e+02 - logprior: -1.6287e+00
Epoch 7/10
37/37 - 26s - loss: 698.8299 - loglik: -6.9595e+02 - logprior: -1.6558e+00
Epoch 8/10
37/37 - 27s - loss: 697.3562 - loglik: -6.9448e+02 - logprior: -1.7374e+00
Epoch 9/10
37/37 - 27s - loss: 696.5155 - loglik: -6.9378e+02 - logprior: -1.6434e+00
Epoch 10/10
37/37 - 26s - loss: 698.1853 - loglik: -6.9537e+02 - logprior: -1.7777e+00
Fitted a model with MAP estimate = -695.6884
expansions: [(0, 4), (30, 1), (33, 3), (34, 2), (38, 1), (39, 1), (65, 1), (91, 5), (92, 12), (99, 1), (100, 1), (141, 1), (142, 3), (197, 10), (210, 3), (240, 1), (242, 1)]
discards: [101 104 105]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 38s - loss: 745.0166 - loglik: -7.4112e+02 - logprior: -3.5900e+00
Epoch 2/2
37/37 - 32s - loss: 701.5792 - loglik: -6.9837e+02 - logprior: -2.0712e+00
Fitted a model with MAP estimate = -692.3310
expansions: [(37, 1), (44, 1), (107, 1), (173, 2), (174, 14), (234, 2), (235, 5), (241, 2), (254, 5), (255, 3)]
discards: [  0 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 221 222 248 249 250 251 252 256]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 37s - loss: 718.3396 - loglik: -7.1452e+02 - logprior: -3.5600e+00
Epoch 2/2
37/37 - 33s - loss: 698.2163 - loglik: -6.9564e+02 - logprior: -1.4545e+00
Fitted a model with MAP estimate = -688.5736
expansions: [(0, 2), (175, 1)]
discards: [186 187 192 246 247 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 38s - loss: 711.2697 - loglik: -7.0851e+02 - logprior: -2.5015e+00
Epoch 2/10
37/37 - 33s - loss: 697.7664 - loglik: -6.9538e+02 - logprior: -1.1952e+00
Epoch 3/10
37/37 - 33s - loss: 688.9857 - loglik: -6.8574e+02 - logprior: -1.1876e+00
Epoch 4/10
37/37 - 33s - loss: 683.7686 - loglik: -6.8042e+02 - logprior: -1.1622e+00
Epoch 5/10
37/37 - 32s - loss: 681.2837 - loglik: -6.7819e+02 - logprior: -1.0806e+00
Epoch 6/10
37/37 - 33s - loss: 679.9994 - loglik: -6.7711e+02 - logprior: -1.0447e+00
Epoch 7/10
37/37 - 33s - loss: 678.5399 - loglik: -6.7590e+02 - logprior: -9.6502e-01
Epoch 8/10
37/37 - 33s - loss: 676.3910 - loglik: -6.7397e+02 - logprior: -7.9631e-01
Epoch 9/10
37/37 - 33s - loss: 675.8799 - loglik: -6.7360e+02 - logprior: -7.0524e-01
Epoch 10/10
37/37 - 33s - loss: 677.0330 - loglik: -6.7485e+02 - logprior: -6.4502e-01
Fitted a model with MAP estimate = -672.7619
Time for alignment: 877.8041
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 815.0192 - loglik: -8.1260e+02 - logprior: -1.9557e+00
Epoch 2/10
37/37 - 26s - loss: 729.8417 - loglik: -7.2714e+02 - logprior: -1.1260e+00
Epoch 3/10
37/37 - 27s - loss: 716.5114 - loglik: -7.1301e+02 - logprior: -1.2712e+00
Epoch 4/10
37/37 - 26s - loss: 709.6882 - loglik: -7.0624e+02 - logprior: -1.3496e+00
Epoch 5/10
37/37 - 26s - loss: 706.7893 - loglik: -7.0371e+02 - logprior: -1.4486e+00
Epoch 6/10
37/37 - 26s - loss: 702.6685 - loglik: -6.9986e+02 - logprior: -1.4680e+00
Epoch 7/10
37/37 - 27s - loss: 706.3965 - loglik: -7.0375e+02 - logprior: -1.4839e+00
Fitted a model with MAP estimate = -701.9133
expansions: [(0, 4), (30, 1), (32, 1), (33, 2), (34, 2), (38, 1), (66, 1), (73, 3), (75, 2), (88, 16), (106, 1), (121, 2), (123, 2), (128, 1), (156, 2), (197, 11), (239, 1)]
discards: [100 107]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 37s - loss: 741.8833 - loglik: -7.3818e+02 - logprior: -3.3833e+00
Epoch 2/2
37/37 - 33s - loss: 703.4514 - loglik: -7.0048e+02 - logprior: -1.9137e+00
Fitted a model with MAP estimate = -693.2867
expansions: [(0, 2), (41, 1), (44, 1), (113, 1), (114, 1), (115, 1), (181, 5), (240, 1), (260, 3)]
discards: [  0  87 116 117 118 119 120 192 193 194 228]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 37s - loss: 713.9452 - loglik: -7.1097e+02 - logprior: -2.7144e+00
Epoch 2/2
37/37 - 33s - loss: 696.0501 - loglik: -6.9354e+02 - logprior: -1.3198e+00
Fitted a model with MAP estimate = -688.1044
expansions: [(0, 2), (182, 1), (184, 1), (246, 5)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 39s - loss: 709.7858 - loglik: -7.0661e+02 - logprior: -2.9326e+00
Epoch 2/10
37/37 - 34s - loss: 695.0008 - loglik: -6.9274e+02 - logprior: -1.2383e+00
Epoch 3/10
37/37 - 34s - loss: 685.8158 - loglik: -6.8296e+02 - logprior: -1.2255e+00
Epoch 4/10
37/37 - 34s - loss: 681.5258 - loglik: -6.7851e+02 - logprior: -1.1384e+00
Epoch 5/10
37/37 - 34s - loss: 679.7783 - loglik: -6.7687e+02 - logprior: -1.1014e+00
Epoch 6/10
37/37 - 34s - loss: 677.1030 - loglik: -6.7451e+02 - logprior: -9.7103e-01
Epoch 7/10
37/37 - 34s - loss: 676.2183 - loglik: -6.7378e+02 - logprior: -9.5800e-01
Epoch 8/10
37/37 - 34s - loss: 677.8561 - loglik: -6.7567e+02 - logprior: -8.0468e-01
Fitted a model with MAP estimate = -673.0747
Time for alignment: 744.3393
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 814.8016 - loglik: -8.1236e+02 - logprior: -1.9737e+00
Epoch 2/10
37/37 - 26s - loss: 725.8713 - loglik: -7.2347e+02 - logprior: -1.0017e+00
Epoch 3/10
37/37 - 26s - loss: 710.8801 - loglik: -7.0801e+02 - logprior: -1.2560e+00
Epoch 4/10
37/37 - 26s - loss: 707.6888 - loglik: -7.0461e+02 - logprior: -1.2448e+00
Epoch 5/10
37/37 - 26s - loss: 703.5589 - loglik: -7.0063e+02 - logprior: -1.3425e+00
Epoch 6/10
37/37 - 27s - loss: 701.6761 - loglik: -6.9895e+02 - logprior: -1.3776e+00
Epoch 7/10
37/37 - 26s - loss: 700.6071 - loglik: -6.9802e+02 - logprior: -1.3887e+00
Epoch 8/10
37/37 - 26s - loss: 699.7585 - loglik: -6.9728e+02 - logprior: -1.4064e+00
Epoch 9/10
37/37 - 27s - loss: 699.0769 - loglik: -6.9668e+02 - logprior: -1.3920e+00
Epoch 10/10
37/37 - 26s - loss: 699.9680 - loglik: -6.9766e+02 - logprior: -1.3909e+00
Fitted a model with MAP estimate = -697.7836
expansions: [(0, 4), (30, 1), (34, 5), (38, 1), (66, 2), (75, 2), (91, 5), (92, 10), (93, 4), (128, 1), (197, 11), (210, 3), (239, 1), (240, 1)]
discards: [156]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 36s - loss: 742.4189 - loglik: -7.3890e+02 - logprior: -3.2187e+00
Epoch 2/2
37/37 - 32s - loss: 701.2197 - loglik: -6.9858e+02 - logprior: -1.6151e+00
Fitted a model with MAP estimate = -691.3010
expansions: [(0, 2), (39, 1), (40, 1), (89, 2), (114, 3), (179, 5), (237, 1), (239, 4), (256, 3), (257, 2)]
discards: [  0  78 105 106 223 250 251 252 253 254 258]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 39s - loss: 712.9189 - loglik: -7.1006e+02 - logprior: -2.5961e+00
Epoch 2/2
37/37 - 34s - loss: 695.8002 - loglik: -6.9326e+02 - logprior: -1.3311e+00
Fitted a model with MAP estimate = -687.8863
expansions: [(186, 1), (187, 1), (264, 1), (270, 2)]
discards: [  1  90  91 117 118 255 265]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 37s - loss: 709.0225 - loglik: -7.0649e+02 - logprior: -2.2741e+00
Epoch 2/10
37/37 - 34s - loss: 694.7150 - loglik: -6.9253e+02 - logprior: -1.0317e+00
Epoch 3/10
37/37 - 34s - loss: 685.8835 - loglik: -6.8302e+02 - logprior: -1.0695e+00
Epoch 4/10
37/37 - 34s - loss: 682.2924 - loglik: -6.7925e+02 - logprior: -1.0240e+00
Epoch 5/10
37/37 - 34s - loss: 678.6542 - loglik: -6.7574e+02 - logprior: -1.0157e+00
Epoch 6/10
37/37 - 34s - loss: 676.3458 - loglik: -6.7370e+02 - logprior: -9.1440e-01
Epoch 7/10
37/37 - 34s - loss: 676.7560 - loglik: -6.7438e+02 - logprior: -7.6878e-01
Fitted a model with MAP estimate = -673.1585
Time for alignment: 790.1575
Computed alignments with likelihoods: ['-672.7619', '-673.0747', '-673.1585']
Best model has likelihood: -672.7619
time for generating output: 0.3033
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.8842032011134308
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69720e8700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f698b6dd970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab64e3f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6983025460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f698312dfd0>, <__main__.SimpleDirichletPrior object at 0x7f694e8c9af0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.1058 - loglik: -2.7272e+02 - logprior: -1.1636e+02
Epoch 2/10
10/10 - 1s - loss: 292.2589 - loglik: -2.6162e+02 - logprior: -3.0464e+01
Epoch 3/10
10/10 - 1s - loss: 263.9323 - loglik: -2.5084e+02 - logprior: -1.2928e+01
Epoch 4/10
10/10 - 1s - loss: 248.7728 - loglik: -2.4212e+02 - logprior: -6.5979e+00
Epoch 5/10
10/10 - 1s - loss: 239.9424 - loglik: -2.3621e+02 - logprior: -3.6438e+00
Epoch 6/10
10/10 - 1s - loss: 234.1049 - loglik: -2.3154e+02 - logprior: -2.2211e+00
Epoch 7/10
10/10 - 1s - loss: 230.7319 - loglik: -2.2872e+02 - logprior: -1.4108e+00
Epoch 8/10
10/10 - 1s - loss: 228.7032 - loglik: -2.2720e+02 - logprior: -8.7238e-01
Epoch 9/10
10/10 - 1s - loss: 227.3975 - loglik: -2.2630e+02 - logprior: -5.1583e-01
Epoch 10/10
10/10 - 1s - loss: 226.5610 - loglik: -2.2583e+02 - logprior: -1.8898e-01
Fitted a model with MAP estimate = -225.6848
expansions: [(0, 6), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 389.5589 - loglik: -2.3461e+02 - logprior: -1.5493e+02
Epoch 2/2
10/10 - 1s - loss: 274.5556 - loglik: -2.2746e+02 - logprior: -4.6922e+01
Fitted a model with MAP estimate = -253.0740
expansions: [(0, 4), (43, 4)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 350.0031 - loglik: -2.2737e+02 - logprior: -1.2261e+02
Epoch 2/2
10/10 - 1s - loss: 259.8781 - loglik: -2.2515e+02 - logprior: -3.4586e+01
Fitted a model with MAP estimate = -243.2778
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.2407 - loglik: -2.2552e+02 - logprior: -1.0370e+02
Epoch 2/10
10/10 - 1s - loss: 251.6103 - loglik: -2.2475e+02 - logprior: -2.6724e+01
Epoch 3/10
10/10 - 1s - loss: 235.0819 - loglik: -2.2409e+02 - logprior: -1.0703e+01
Epoch 4/10
10/10 - 1s - loss: 228.0859 - loglik: -2.2369e+02 - logprior: -3.9657e+00
Epoch 5/10
10/10 - 1s - loss: 224.2092 - loglik: -2.2362e+02 - logprior: -1.5284e-01
Epoch 6/10
10/10 - 1s - loss: 221.8560 - loglik: -2.2358e+02 - logprior: 2.1155
Epoch 7/10
10/10 - 1s - loss: 220.2989 - loglik: -2.2338e+02 - logprior: 3.4789
Epoch 8/10
10/10 - 1s - loss: 219.1615 - loglik: -2.2311e+02 - logprior: 4.3807
Epoch 9/10
10/10 - 1s - loss: 218.2806 - loglik: -2.2288e+02 - logprior: 5.0731
Epoch 10/10
10/10 - 1s - loss: 217.5495 - loglik: -2.2275e+02 - logprior: 5.6851
Fitted a model with MAP estimate = -216.6845
Time for alignment: 47.6142
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.1058 - loglik: -2.7272e+02 - logprior: -1.1636e+02
Epoch 2/10
10/10 - 1s - loss: 292.2589 - loglik: -2.6162e+02 - logprior: -3.0464e+01
Epoch 3/10
10/10 - 1s - loss: 263.9324 - loglik: -2.5084e+02 - logprior: -1.2928e+01
Epoch 4/10
10/10 - 1s - loss: 248.7708 - loglik: -2.4212e+02 - logprior: -6.5985e+00
Epoch 5/10
10/10 - 1s - loss: 239.9303 - loglik: -2.3620e+02 - logprior: -3.6432e+00
Epoch 6/10
10/10 - 1s - loss: 234.1034 - loglik: -2.3153e+02 - logprior: -2.2162e+00
Epoch 7/10
10/10 - 1s - loss: 230.7491 - loglik: -2.2875e+02 - logprior: -1.3949e+00
Epoch 8/10
10/10 - 1s - loss: 228.7335 - loglik: -2.2726e+02 - logprior: -8.4559e-01
Epoch 9/10
10/10 - 1s - loss: 227.4232 - loglik: -2.2637e+02 - logprior: -4.9318e-01
Epoch 10/10
10/10 - 1s - loss: 226.5736 - loglik: -2.2587e+02 - logprior: -1.7761e-01
Fitted a model with MAP estimate = -225.7073
expansions: [(0, 6), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 389.5083 - loglik: -2.3458e+02 - logprior: -1.5491e+02
Epoch 2/2
10/10 - 1s - loss: 274.5526 - loglik: -2.2747e+02 - logprior: -4.6911e+01
Fitted a model with MAP estimate = -253.0841
expansions: [(0, 4), (43, 4)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.9957 - loglik: -2.2737e+02 - logprior: -1.2260e+02
Epoch 2/2
10/10 - 1s - loss: 259.8722 - loglik: -2.2515e+02 - logprior: -3.4571e+01
Fitted a model with MAP estimate = -243.2717
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2499 - loglik: -2.2554e+02 - logprior: -1.0369e+02
Epoch 2/10
10/10 - 1s - loss: 251.6294 - loglik: -2.2478e+02 - logprior: -2.6715e+01
Epoch 3/10
10/10 - 1s - loss: 235.0964 - loglik: -2.2410e+02 - logprior: -1.0706e+01
Epoch 4/10
10/10 - 1s - loss: 228.0898 - loglik: -2.2369e+02 - logprior: -3.9668e+00
Epoch 5/10
10/10 - 1s - loss: 224.2109 - loglik: -2.2362e+02 - logprior: -1.5635e-01
Epoch 6/10
10/10 - 1s - loss: 221.8602 - loglik: -2.2358e+02 - logprior: 2.1129
Epoch 7/10
10/10 - 1s - loss: 220.2991 - loglik: -2.2338e+02 - logprior: 3.4742
Epoch 8/10
10/10 - 1s - loss: 219.1534 - loglik: -2.2309e+02 - logprior: 4.3715
Epoch 9/10
10/10 - 1s - loss: 218.2731 - loglik: -2.2285e+02 - logprior: 5.0558
Epoch 10/10
10/10 - 1s - loss: 217.5378 - loglik: -2.2271e+02 - logprior: 5.6685
Fitted a model with MAP estimate = -216.6723
Time for alignment: 45.8913
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.1058 - loglik: -2.7272e+02 - logprior: -1.1636e+02
Epoch 2/10
10/10 - 1s - loss: 292.2591 - loglik: -2.6162e+02 - logprior: -3.0464e+01
Epoch 3/10
10/10 - 1s - loss: 263.9324 - loglik: -2.5084e+02 - logprior: -1.2928e+01
Epoch 4/10
10/10 - 1s - loss: 248.7193 - loglik: -2.4205e+02 - logprior: -6.6160e+00
Epoch 5/10
10/10 - 1s - loss: 239.7155 - loglik: -2.3581e+02 - logprior: -3.7820e+00
Epoch 6/10
10/10 - 1s - loss: 234.1357 - loglik: -2.3147e+02 - logprior: -2.1743e+00
Epoch 7/10
10/10 - 1s - loss: 230.9414 - loglik: -2.2879e+02 - logprior: -1.3363e+00
Epoch 8/10
10/10 - 1s - loss: 229.0474 - loglik: -2.2736e+02 - logprior: -7.8526e-01
Epoch 9/10
10/10 - 1s - loss: 227.8634 - loglik: -2.2666e+02 - logprior: -3.6137e-01
Epoch 10/10
10/10 - 1s - loss: 227.0878 - loglik: -2.2635e+02 - logprior: 0.0505
Fitted a model with MAP estimate = -225.9810
expansions: [(0, 6), (37, 4), (51, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 388.5299 - loglik: -2.3466e+02 - logprior: -1.5385e+02
Epoch 2/2
10/10 - 1s - loss: 274.3216 - loglik: -2.2819e+02 - logprior: -4.6054e+01
Fitted a model with MAP estimate = -252.9191
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 349.2279 - loglik: -2.2703e+02 - logprior: -1.2218e+02
Epoch 2/2
10/10 - 1s - loss: 259.2391 - loglik: -2.2489e+02 - logprior: -3.4208e+01
Fitted a model with MAP estimate = -242.8377
expansions: [(0, 4), (80, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 354.1276 - loglik: -2.2415e+02 - logprior: -1.2995e+02
Epoch 2/10
10/10 - 1s - loss: 262.6203 - loglik: -2.2269e+02 - logprior: -3.9804e+01
Epoch 3/10
10/10 - 1s - loss: 237.3699 - loglik: -2.2195e+02 - logprior: -1.5156e+01
Epoch 4/10
10/10 - 1s - loss: 226.6745 - loglik: -2.2169e+02 - logprior: -4.5904e+00
Epoch 5/10
10/10 - 1s - loss: 221.8560 - loglik: -2.2164e+02 - logprior: 0.1910
Epoch 6/10
10/10 - 1s - loss: 219.2677 - loglik: -2.2161e+02 - logprior: 2.7244
Epoch 7/10
10/10 - 1s - loss: 217.6427 - loglik: -2.2150e+02 - logprior: 4.2339
Epoch 8/10
10/10 - 1s - loss: 216.5027 - loglik: -2.2133e+02 - logprior: 5.2348
Epoch 9/10
10/10 - 1s - loss: 215.6077 - loglik: -2.2116e+02 - logprior: 5.9862
Epoch 10/10
10/10 - 1s - loss: 214.8318 - loglik: -2.2098e+02 - logprior: 6.6061
Fitted a model with MAP estimate = -213.9710
Time for alignment: 46.7757
Computed alignments with likelihoods: ['-216.6845', '-216.6723', '-213.9710']
Best model has likelihood: -213.9710
time for generating output: 0.1513
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7242284963887065
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac9e63a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f696934b550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16e15610>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f67c6037a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac94793d0>, <__main__.SimpleDirichletPrior object at 0x7f69be613d30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 998.1478 - loglik: -9.8853e+02 - logprior: -9.5094e+00
Epoch 2/10
19/19 - 22s - loss: 879.5592 - loglik: -8.7808e+02 - logprior: -8.8870e-01
Epoch 3/10
19/19 - 22s - loss: 808.6489 - loglik: -8.0544e+02 - logprior: -2.3758e+00
Epoch 4/10
19/19 - 22s - loss: 793.9773 - loglik: -7.8946e+02 - logprior: -3.2211e+00
Epoch 5/10
19/19 - 21s - loss: 786.4417 - loglik: -7.8196e+02 - logprior: -3.3564e+00
Epoch 6/10
19/19 - 22s - loss: 781.4366 - loglik: -7.7701e+02 - logprior: -3.5018e+00
Epoch 7/10
19/19 - 22s - loss: 781.7986 - loglik: -7.7741e+02 - logprior: -3.5733e+00
Fitted a model with MAP estimate = -778.3484
expansions: [(14, 1), (32, 1), (72, 1), (101, 2), (108, 1), (114, 1), (115, 1), (119, 1), (120, 1), (121, 1), (124, 8), (126, 2), (144, 1), (145, 1), (159, 1), (165, 2), (167, 1), (168, 1), (169, 2), (170, 1), (173, 1), (176, 2), (177, 1), (178, 1), (190, 4), (198, 1), (204, 1), (218, 1), (220, 2), (222, 3), (223, 3), (236, 1), (237, 1), (238, 1), (239, 1), (259, 1), (262, 1), (263, 2), (265, 3), (272, 4), (273, 2), (294, 1), (301, 5), (303, 1)]
discards: [  0 244]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 33s - loss: 809.1213 - loglik: -7.9534e+02 - logprior: -1.3688e+01
Epoch 2/2
19/19 - 29s - loss: 762.1509 - loglik: -7.5669e+02 - logprior: -5.0584e+00
Fitted a model with MAP estimate = -751.6675
expansions: [(0, 2), (371, 1)]
discards: [  0 104 146 196 208 263 267]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 33s - loss: 768.0519 - loglik: -7.5890e+02 - logprior: -9.0760e+00
Epoch 2/2
19/19 - 28s - loss: 747.5491 - loglik: -7.4672e+02 - logprior: -4.2095e-01
Fitted a model with MAP estimate = -742.1171
expansions: [(318, 1)]
discards: [  0 145]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 32s - loss: 769.8096 - loglik: -7.5741e+02 - logprior: -1.2309e+01
Epoch 2/10
19/19 - 28s - loss: 751.4581 - loglik: -7.4895e+02 - logprior: -2.0419e+00
Epoch 3/10
19/19 - 28s - loss: 739.1145 - loglik: -7.3916e+02 - logprior: 0.9271
Epoch 4/10
19/19 - 28s - loss: 736.1172 - loglik: -7.3662e+02 - logprior: 1.5908
Epoch 5/10
19/19 - 28s - loss: 733.6352 - loglik: -7.3444e+02 - logprior: 1.8126
Epoch 6/10
19/19 - 28s - loss: 733.5903 - loglik: -7.3474e+02 - logprior: 2.0318
Epoch 7/10
19/19 - 28s - loss: 733.8019 - loglik: -7.3520e+02 - logprior: 2.1917
Fitted a model with MAP estimate = -730.3855
Time for alignment: 553.4823
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 998.0326 - loglik: -9.8845e+02 - logprior: -9.4726e+00
Epoch 2/10
19/19 - 22s - loss: 871.2388 - loglik: -8.6959e+02 - logprior: -1.0250e+00
Epoch 3/10
19/19 - 22s - loss: 806.7401 - loglik: -8.0326e+02 - logprior: -2.5465e+00
Epoch 4/10
19/19 - 22s - loss: 788.3218 - loglik: -7.8377e+02 - logprior: -3.1909e+00
Epoch 5/10
19/19 - 22s - loss: 784.4726 - loglik: -7.7992e+02 - logprior: -3.2690e+00
Epoch 6/10
19/19 - 22s - loss: 778.7498 - loglik: -7.7418e+02 - logprior: -3.4638e+00
Epoch 7/10
19/19 - 22s - loss: 777.8060 - loglik: -7.7320e+02 - logprior: -3.6473e+00
Epoch 8/10
19/19 - 22s - loss: 779.1627 - loglik: -7.7442e+02 - logprior: -3.8980e+00
Fitted a model with MAP estimate = -775.3045
expansions: [(14, 1), (50, 1), (67, 1), (68, 1), (95, 1), (96, 1), (97, 2), (105, 1), (111, 1), (112, 1), (115, 1), (117, 1), (120, 8), (121, 2), (143, 1), (155, 1), (156, 1), (166, 3), (169, 1), (176, 1), (177, 3), (178, 2), (190, 2), (195, 1), (210, 1), (218, 1), (220, 2), (221, 1), (222, 2), (224, 1), (237, 1), (240, 1), (242, 1), (260, 1), (265, 6), (290, 1), (292, 2), (293, 6), (301, 1), (302, 1), (303, 1), (311, 3), (312, 1)]
discards: [  0 273]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 814.1089 - loglik: -8.0013e+02 - logprior: -1.3884e+01
Epoch 2/2
19/19 - 28s - loss: 760.0071 - loglik: -7.5434e+02 - logprior: -5.2574e+00
Fitted a model with MAP estimate = -748.7040
expansions: [(0, 2), (371, 1)]
discards: [  0 102 225 349]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 32s - loss: 766.4324 - loglik: -7.5766e+02 - logprior: -8.7021e+00
Epoch 2/2
19/19 - 28s - loss: 742.0481 - loglik: -7.4155e+02 - logprior: -9.8902e-02
Fitted a model with MAP estimate = -738.2064
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 33s - loss: 765.6135 - loglik: -7.5369e+02 - logprior: -1.1837e+01
Epoch 2/10
19/19 - 28s - loss: 746.0983 - loglik: -7.4331e+02 - logprior: -2.3228e+00
Epoch 3/10
19/19 - 28s - loss: 735.3455 - loglik: -7.3532e+02 - logprior: 0.7856
Epoch 4/10
19/19 - 29s - loss: 728.7067 - loglik: -7.2930e+02 - logprior: 1.5616
Epoch 5/10
19/19 - 28s - loss: 730.8694 - loglik: -7.3172e+02 - logprior: 1.8041
Fitted a model with MAP estimate = -727.0031
Time for alignment: 518.6288
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 999.2225 - loglik: -9.8960e+02 - logprior: -9.5091e+00
Epoch 2/10
19/19 - 22s - loss: 872.0002 - loglik: -8.7057e+02 - logprior: -8.5102e-01
Epoch 3/10
19/19 - 22s - loss: 812.0602 - loglik: -8.0879e+02 - logprior: -2.4827e+00
Epoch 4/10
19/19 - 22s - loss: 792.8470 - loglik: -7.8848e+02 - logprior: -3.1343e+00
Epoch 5/10
19/19 - 22s - loss: 789.7700 - loglik: -7.8562e+02 - logprior: -3.0606e+00
Epoch 6/10
19/19 - 22s - loss: 786.0549 - loglik: -7.8217e+02 - logprior: -3.0688e+00
Epoch 7/10
19/19 - 22s - loss: 786.2739 - loglik: -7.8243e+02 - logprior: -3.1672e+00
Fitted a model with MAP estimate = -783.1650
expansions: [(14, 1), (32, 1), (65, 1), (67, 1), (98, 1), (99, 3), (100, 1), (112, 1), (113, 1), (115, 1), (116, 1), (117, 1), (118, 1), (121, 2), (122, 4), (124, 3), (143, 1), (144, 1), (157, 1), (164, 1), (166, 1), (167, 1), (170, 1), (173, 2), (176, 2), (177, 1), (178, 1), (180, 1), (189, 1), (190, 2), (205, 1), (208, 1), (219, 2), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (226, 1), (241, 3), (263, 5), (295, 4), (301, 1), (302, 3), (309, 1), (310, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 33s - loss: 812.6096 - loglik: -7.9912e+02 - logprior: -1.3403e+01
Epoch 2/2
19/19 - 28s - loss: 764.7548 - loglik: -7.5927e+02 - logprior: -5.0958e+00
Fitted a model with MAP estimate = -755.2220
expansions: [(0, 2)]
discards: [  0 103 104 137 146 147 204 208 271 355 356 357]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 773.5687 - loglik: -7.6498e+02 - logprior: -8.5168e+00
Epoch 2/2
19/19 - 27s - loss: 754.6392 - loglik: -7.5370e+02 - logprior: -5.4154e-01
Fitted a model with MAP estimate = -749.7615
expansions: [(143, 3)]
discards: [  0 354]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 31s - loss: 776.4821 - loglik: -7.6469e+02 - logprior: -1.1704e+01
Epoch 2/10
19/19 - 27s - loss: 758.7715 - loglik: -7.5625e+02 - logprior: -2.0550e+00
Epoch 3/10
19/19 - 27s - loss: 748.8677 - loglik: -7.4884e+02 - logprior: 0.8581
Epoch 4/10
19/19 - 27s - loss: 742.3602 - loglik: -7.4284e+02 - logprior: 1.5617
Epoch 5/10
19/19 - 27s - loss: 742.9999 - loglik: -7.4386e+02 - logprior: 1.9077
Fitted a model with MAP estimate = -739.7777
Time for alignment: 483.1321
Computed alignments with likelihoods: ['-730.3855', '-727.0031', '-739.7777']
Best model has likelihood: -727.0031
time for generating output: 0.5232
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7582632211538461
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69b5817850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aa4b37c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aa4b37f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6944e9d2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f694eaaa130>, <__main__.SimpleDirichletPrior object at 0x7f6944302dc0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 370.6346 - loglik: -3.1231e+02 - logprior: -5.8309e+01
Epoch 2/10
10/10 - 1s - loss: 292.2231 - loglik: -2.7681e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 245.2013 - loglik: -2.3718e+02 - logprior: -7.9951e+00
Epoch 4/10
10/10 - 1s - loss: 217.9718 - loglik: -2.1191e+02 - logprior: -6.0046e+00
Epoch 5/10
10/10 - 1s - loss: 205.4535 - loglik: -2.0056e+02 - logprior: -4.8634e+00
Epoch 6/10
10/10 - 1s - loss: 200.6316 - loglik: -1.9631e+02 - logprior: -4.2727e+00
Epoch 7/10
10/10 - 1s - loss: 198.5490 - loglik: -1.9451e+02 - logprior: -3.9020e+00
Epoch 8/10
10/10 - 1s - loss: 197.2418 - loglik: -1.9336e+02 - logprior: -3.6708e+00
Epoch 9/10
10/10 - 1s - loss: 197.0426 - loglik: -1.9334e+02 - logprior: -3.5065e+00
Epoch 10/10
10/10 - 1s - loss: 196.2509 - loglik: -1.9275e+02 - logprior: -3.3335e+00
Fitted a model with MAP estimate = -196.0494
expansions: [(13, 4), (17, 2), (30, 1), (32, 1), (33, 1), (36, 1), (53, 1), (55, 2), (56, 2), (57, 1), (60, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.3432 - loglik: -1.8707e+02 - logprior: -6.6270e+01
Epoch 2/2
10/10 - 2s - loss: 201.7993 - loglik: -1.7437e+02 - logprior: -2.7392e+01
Fitted a model with MAP estimate = -193.0918
expansions: [(0, 2), (14, 1), (68, 1)]
discards: [ 0 20 21 97]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 223.9015 - loglik: -1.7135e+02 - logprior: -5.2530e+01
Epoch 2/2
10/10 - 2s - loss: 181.8938 - loglik: -1.6821e+02 - logprior: -1.3600e+01
Fitted a model with MAP estimate = -175.4840
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 232.9173 - loglik: -1.6990e+02 - logprior: -6.2996e+01
Epoch 2/10
10/10 - 2s - loss: 188.8050 - loglik: -1.6910e+02 - logprior: -1.9618e+01
Epoch 3/10
10/10 - 2s - loss: 175.5171 - loglik: -1.6876e+02 - logprior: -6.6914e+00
Epoch 4/10
10/10 - 2s - loss: 171.0725 - loglik: -1.6881e+02 - logprior: -2.2254e+00
Epoch 5/10
10/10 - 2s - loss: 168.8396 - loglik: -1.6839e+02 - logprior: -3.8075e-01
Epoch 6/10
10/10 - 2s - loss: 167.4039 - loglik: -1.6780e+02 - logprior: 0.5453
Epoch 7/10
10/10 - 2s - loss: 167.0723 - loglik: -1.6805e+02 - logprior: 1.1650
Epoch 8/10
10/10 - 2s - loss: 166.4163 - loglik: -1.6797e+02 - logprior: 1.7123
Epoch 9/10
10/10 - 2s - loss: 166.1974 - loglik: -1.6823e+02 - logprior: 2.1919
Epoch 10/10
10/10 - 2s - loss: 165.6487 - loglik: -1.6805e+02 - logprior: 2.5699
Fitted a model with MAP estimate = -165.3925
Time for alignment: 57.0114
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 370.9262 - loglik: -3.1260e+02 - logprior: -5.8309e+01
Epoch 2/10
10/10 - 1s - loss: 292.5859 - loglik: -2.7716e+02 - logprior: -1.5413e+01
Epoch 3/10
10/10 - 1s - loss: 244.0065 - loglik: -2.3591e+02 - logprior: -8.0728e+00
Epoch 4/10
10/10 - 1s - loss: 219.2306 - loglik: -2.1318e+02 - logprior: -5.9884e+00
Epoch 5/10
10/10 - 1s - loss: 207.4924 - loglik: -2.0265e+02 - logprior: -4.8125e+00
Epoch 6/10
10/10 - 1s - loss: 201.3449 - loglik: -1.9706e+02 - logprior: -4.2506e+00
Epoch 7/10
10/10 - 1s - loss: 198.8016 - loglik: -1.9475e+02 - logprior: -3.9881e+00
Epoch 8/10
10/10 - 1s - loss: 197.1284 - loglik: -1.9308e+02 - logprior: -3.8693e+00
Epoch 9/10
10/10 - 1s - loss: 196.3842 - loglik: -1.9234e+02 - logprior: -3.7591e+00
Epoch 10/10
10/10 - 1s - loss: 195.5327 - loglik: -1.9164e+02 - logprior: -3.6177e+00
Fitted a model with MAP estimate = -195.2740
expansions: [(13, 5), (17, 2), (18, 1), (32, 1), (33, 1), (34, 2), (53, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (76, 1), (77, 3), (78, 1), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 252.6685 - loglik: -1.8653e+02 - logprior: -6.6109e+01
Epoch 2/2
10/10 - 2s - loss: 200.5280 - loglik: -1.7312e+02 - logprior: -2.7272e+01
Fitted a model with MAP estimate = -191.7252
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 44 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.6970 - loglik: -1.7112e+02 - logprior: -5.2560e+01
Epoch 2/2
10/10 - 2s - loss: 181.8551 - loglik: -1.6813e+02 - logprior: -1.3642e+01
Fitted a model with MAP estimate = -175.6689
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 233.5530 - loglik: -1.7028e+02 - logprior: -6.3247e+01
Epoch 2/10
10/10 - 2s - loss: 189.7047 - loglik: -1.6959e+02 - logprior: -2.0028e+01
Epoch 3/10
10/10 - 2s - loss: 176.2878 - loglik: -1.6938e+02 - logprior: -6.8437e+00
Epoch 4/10
10/10 - 2s - loss: 171.6601 - loglik: -1.6938e+02 - logprior: -2.2443e+00
Epoch 5/10
10/10 - 2s - loss: 169.4282 - loglik: -1.6897e+02 - logprior: -3.7331e-01
Epoch 6/10
10/10 - 2s - loss: 167.9695 - loglik: -1.6831e+02 - logprior: 0.5461
Epoch 7/10
10/10 - 2s - loss: 167.3817 - loglik: -1.6831e+02 - logprior: 1.1862
Epoch 8/10
10/10 - 2s - loss: 167.0625 - loglik: -1.6856e+02 - logprior: 1.7148
Epoch 9/10
10/10 - 2s - loss: 166.4761 - loglik: -1.6845e+02 - logprior: 2.1841
Epoch 10/10
10/10 - 2s - loss: 166.3811 - loglik: -1.6872e+02 - logprior: 2.5661
Fitted a model with MAP estimate = -165.8122
Time for alignment: 54.7722
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 370.9150 - loglik: -3.1259e+02 - logprior: -5.8309e+01
Epoch 2/10
10/10 - 1s - loss: 291.7529 - loglik: -2.7633e+02 - logprior: -1.5409e+01
Epoch 3/10
10/10 - 1s - loss: 242.0889 - loglik: -2.3406e+02 - logprior: -7.9997e+00
Epoch 4/10
10/10 - 1s - loss: 215.9729 - loglik: -2.1004e+02 - logprior: -5.8766e+00
Epoch 5/10
10/10 - 1s - loss: 206.1774 - loglik: -2.0140e+02 - logprior: -4.7516e+00
Epoch 6/10
10/10 - 1s - loss: 200.9137 - loglik: -1.9677e+02 - logprior: -4.0999e+00
Epoch 7/10
10/10 - 1s - loss: 197.8729 - loglik: -1.9397e+02 - logprior: -3.7394e+00
Epoch 8/10
10/10 - 1s - loss: 196.8224 - loglik: -1.9305e+02 - logprior: -3.4836e+00
Epoch 9/10
10/10 - 1s - loss: 196.0135 - loglik: -1.9236e+02 - logprior: -3.3511e+00
Epoch 10/10
10/10 - 1s - loss: 195.1979 - loglik: -1.9171e+02 - logprior: -3.2239e+00
Fitted a model with MAP estimate = -194.8737
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.2234 - loglik: -1.8616e+02 - logprior: -6.6044e+01
Epoch 2/2
10/10 - 2s - loss: 200.5180 - loglik: -1.7325e+02 - logprior: -2.7165e+01
Fitted a model with MAP estimate = -191.6511
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.2526 - loglik: -1.7077e+02 - logprior: -5.2462e+01
Epoch 2/2
10/10 - 2s - loss: 181.3889 - loglik: -1.6783e+02 - logprior: -1.3484e+01
Fitted a model with MAP estimate = -175.4574
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 232.9461 - loglik: -1.7005e+02 - logprior: -6.2874e+01
Epoch 2/10
10/10 - 2s - loss: 189.2028 - loglik: -1.6959e+02 - logprior: -1.9528e+01
Epoch 3/10
10/10 - 2s - loss: 175.2853 - loglik: -1.6863e+02 - logprior: -6.5971e+00
Epoch 4/10
10/10 - 2s - loss: 170.8932 - loglik: -1.6868e+02 - logprior: -2.1702e+00
Epoch 5/10
10/10 - 2s - loss: 168.7021 - loglik: -1.6822e+02 - logprior: -3.7849e-01
Epoch 6/10
10/10 - 2s - loss: 167.6836 - loglik: -1.6808e+02 - logprior: 0.5940
Epoch 7/10
10/10 - 2s - loss: 166.8796 - loglik: -1.6789e+02 - logprior: 1.2238
Epoch 8/10
10/10 - 2s - loss: 166.2963 - loglik: -1.6788e+02 - logprior: 1.7751
Epoch 9/10
10/10 - 2s - loss: 165.9763 - loglik: -1.6804e+02 - logprior: 2.2637
Epoch 10/10
10/10 - 2s - loss: 165.7210 - loglik: -1.6815e+02 - logprior: 2.6468
Fitted a model with MAP estimate = -165.2562
Time for alignment: 56.4962
Computed alignments with likelihoods: ['-165.3925', '-165.8122', '-165.2562']
Best model has likelihood: -165.2562
time for generating output: 0.1846
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9300963767231915
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69e96af5b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b166af400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8d4b820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f698b6bd5b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6a83391760>, <__main__.SimpleDirichletPrior object at 0x7f6a82bbb5b0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 397.4984 - loglik: -3.8901e+02 - logprior: -8.4413e+00
Epoch 2/10
13/13 - 3s - loss: 353.7793 - loglik: -3.5125e+02 - logprior: -2.2162e+00
Epoch 3/10
13/13 - 3s - loss: 326.5289 - loglik: -3.2407e+02 - logprior: -1.9558e+00
Epoch 4/10
13/13 - 3s - loss: 312.9026 - loglik: -3.0989e+02 - logprior: -2.1982e+00
Epoch 5/10
13/13 - 3s - loss: 308.3025 - loglik: -3.0541e+02 - logprior: -2.1292e+00
Epoch 6/10
13/13 - 3s - loss: 305.4850 - loglik: -3.0284e+02 - logprior: -2.0750e+00
Epoch 7/10
13/13 - 3s - loss: 303.7509 - loglik: -3.0117e+02 - logprior: -2.1126e+00
Epoch 8/10
13/13 - 3s - loss: 301.7335 - loglik: -2.9916e+02 - logprior: -2.1956e+00
Epoch 9/10
13/13 - 3s - loss: 301.9887 - loglik: -2.9942e+02 - logprior: -2.2331e+00
Fitted a model with MAP estimate = -300.6983
expansions: [(12, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (45, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (80, 1), (92, 2), (99, 1), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 321.4319 - loglik: -3.1138e+02 - logprior: -1.0013e+01
Epoch 2/2
13/13 - 4s - loss: 300.5108 - loglik: -2.9548e+02 - logprior: -4.7848e+00
Fitted a model with MAP estimate = -295.5600
expansions: [(0, 2)]
discards: [  0  38  81 114]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 300.8029 - loglik: -2.9316e+02 - logprior: -7.5961e+00
Epoch 2/2
13/13 - 4s - loss: 291.2324 - loglik: -2.8871e+02 - logprior: -2.2581e+00
Fitted a model with MAP estimate = -288.4584
expansions: []
discards: [  0  23  74 126]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 302.9397 - loglik: -2.9368e+02 - logprior: -9.2161e+00
Epoch 2/10
13/13 - 3s - loss: 293.6118 - loglik: -2.9043e+02 - logprior: -2.9300e+00
Epoch 3/10
13/13 - 3s - loss: 289.3707 - loglik: -2.8728e+02 - logprior: -1.5580e+00
Epoch 4/10
13/13 - 4s - loss: 287.1656 - loglik: -2.8526e+02 - logprior: -1.2441e+00
Epoch 5/10
13/13 - 3s - loss: 287.0301 - loglik: -2.8534e+02 - logprior: -1.0869e+00
Epoch 6/10
13/13 - 3s - loss: 285.8596 - loglik: -2.8436e+02 - logprior: -1.0131e+00
Epoch 7/10
13/13 - 4s - loss: 285.9764 - loglik: -2.8459e+02 - logprior: -9.8663e-01
Fitted a model with MAP estimate = -285.1361
Time for alignment: 93.9956
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 397.1997 - loglik: -3.8871e+02 - logprior: -8.4468e+00
Epoch 2/10
13/13 - 3s - loss: 354.9971 - loglik: -3.5247e+02 - logprior: -2.2116e+00
Epoch 3/10
13/13 - 3s - loss: 326.9074 - loglik: -3.2448e+02 - logprior: -1.9405e+00
Epoch 4/10
13/13 - 3s - loss: 311.9644 - loglik: -3.0895e+02 - logprior: -2.2462e+00
Epoch 5/10
13/13 - 3s - loss: 307.4899 - loglik: -3.0452e+02 - logprior: -2.1794e+00
Epoch 6/10
13/13 - 3s - loss: 304.7036 - loglik: -3.0207e+02 - logprior: -2.0790e+00
Epoch 7/10
13/13 - 3s - loss: 303.5333 - loglik: -3.0091e+02 - logprior: -2.1669e+00
Epoch 8/10
13/13 - 3s - loss: 302.3831 - loglik: -2.9977e+02 - logprior: -2.2314e+00
Epoch 9/10
13/13 - 3s - loss: 301.4979 - loglik: -2.9891e+02 - logprior: -2.2302e+00
Epoch 10/10
13/13 - 3s - loss: 301.1580 - loglik: -2.9857e+02 - logprior: -2.2346e+00
Fitted a model with MAP estimate = -300.9280
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (80, 1), (100, 2), (101, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 321.6898 - loglik: -3.1162e+02 - logprior: -1.0028e+01
Epoch 2/2
13/13 - 4s - loss: 302.0016 - loglik: -2.9702e+02 - logprior: -4.7581e+00
Fitted a model with MAP estimate = -297.4226
expansions: [(0, 2)]
discards: [ 0 38 74 81]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 302.6415 - loglik: -2.9503e+02 - logprior: -7.5679e+00
Epoch 2/2
13/13 - 3s - loss: 293.4324 - loglik: -2.9093e+02 - logprior: -2.2457e+00
Fitted a model with MAP estimate = -290.9839
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 304.9106 - loglik: -2.9566e+02 - logprior: -9.2069e+00
Epoch 2/10
13/13 - 3s - loss: 294.7904 - loglik: -2.9160e+02 - logprior: -2.9283e+00
Epoch 3/10
13/13 - 3s - loss: 291.0844 - loglik: -2.8902e+02 - logprior: -1.5415e+00
Epoch 4/10
13/13 - 3s - loss: 289.4100 - loglik: -2.8749e+02 - logprior: -1.2518e+00
Epoch 5/10
13/13 - 3s - loss: 288.2001 - loglik: -2.8653e+02 - logprior: -1.0535e+00
Epoch 6/10
13/13 - 3s - loss: 287.7738 - loglik: -2.8623e+02 - logprior: -1.0147e+00
Epoch 7/10
13/13 - 3s - loss: 287.5510 - loglik: -2.8614e+02 - logprior: -9.8382e-01
Epoch 8/10
13/13 - 3s - loss: 286.9058 - loglik: -2.8558e+02 - logprior: -9.5080e-01
Epoch 9/10
13/13 - 3s - loss: 287.1628 - loglik: -2.8590e+02 - logprior: -9.1946e-01
Fitted a model with MAP estimate = -286.3355
Time for alignment: 104.4962
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 397.0680 - loglik: -3.8857e+02 - logprior: -8.4498e+00
Epoch 2/10
13/13 - 3s - loss: 354.6513 - loglik: -3.5213e+02 - logprior: -2.2078e+00
Epoch 3/10
13/13 - 3s - loss: 325.8338 - loglik: -3.2334e+02 - logprior: -1.9839e+00
Epoch 4/10
13/13 - 3s - loss: 311.7803 - loglik: -3.0862e+02 - logprior: -2.3833e+00
Epoch 5/10
13/13 - 3s - loss: 308.1046 - loglik: -3.0494e+02 - logprior: -2.3224e+00
Epoch 6/10
13/13 - 3s - loss: 305.1857 - loglik: -3.0233e+02 - logprior: -2.2076e+00
Epoch 7/10
13/13 - 3s - loss: 304.7102 - loglik: -3.0193e+02 - logprior: -2.2432e+00
Epoch 8/10
13/13 - 3s - loss: 303.4329 - loglik: -3.0069e+02 - logprior: -2.2529e+00
Epoch 9/10
13/13 - 3s - loss: 303.2391 - loglik: -3.0053e+02 - logprior: -2.2437e+00
Epoch 10/10
13/13 - 3s - loss: 302.6501 - loglik: -2.9994e+02 - logprior: -2.2636e+00
Fitted a model with MAP estimate = -302.0252
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 2), (80, 2), (81, 2), (82, 1), (93, 1), (100, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 325.0904 - loglik: -3.1498e+02 - logprior: -1.0073e+01
Epoch 2/2
13/13 - 4s - loss: 303.4099 - loglik: -2.9824e+02 - logprior: -4.9004e+00
Fitted a model with MAP estimate = -298.0244
expansions: [(0, 2)]
discards: [  0  28  33  39  82 100 103]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 303.2531 - loglik: -2.9560e+02 - logprior: -7.6114e+00
Epoch 2/2
13/13 - 4s - loss: 293.0618 - loglik: -2.9052e+02 - logprior: -2.2890e+00
Fitted a model with MAP estimate = -290.4676
expansions: [(123, 1)]
discards: [ 0 73]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.5971 - loglik: -2.9516e+02 - logprior: -9.3881e+00
Epoch 2/10
13/13 - 4s - loss: 293.9609 - loglik: -2.9059e+02 - logprior: -3.1057e+00
Epoch 3/10
13/13 - 3s - loss: 289.8603 - loglik: -2.8774e+02 - logprior: -1.5689e+00
Epoch 4/10
13/13 - 3s - loss: 288.1831 - loglik: -2.8624e+02 - logprior: -1.2565e+00
Epoch 5/10
13/13 - 3s - loss: 286.9970 - loglik: -2.8525e+02 - logprior: -1.1062e+00
Epoch 6/10
13/13 - 3s - loss: 286.3600 - loglik: -2.8482e+02 - logprior: -1.0298e+00
Epoch 7/10
13/13 - 3s - loss: 286.0664 - loglik: -2.8466e+02 - logprior: -9.9420e-01
Epoch 8/10
13/13 - 4s - loss: 285.8903 - loglik: -2.8455e+02 - logprior: -9.7454e-01
Epoch 9/10
13/13 - 4s - loss: 285.3272 - loglik: -2.8404e+02 - logprior: -9.4141e-01
Epoch 10/10
13/13 - 3s - loss: 285.4926 - loglik: -2.8426e+02 - logprior: -8.9702e-01
Fitted a model with MAP estimate = -284.8776
Time for alignment: 106.1195
Computed alignments with likelihoods: ['-285.1361', '-286.3355', '-284.8776']
Best model has likelihood: -284.8776
time for generating output: 0.2374
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.842456608811749
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a82a39040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69573bbd60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69426d60a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f694ea842b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69e8d3a4f0>, <__main__.SimpleDirichletPrior object at 0x7f61804399a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 222.9018 - loglik: -2.1407e+02 - logprior: -8.8221e+00
Epoch 2/10
13/13 - 2s - loss: 173.6611 - loglik: -1.7115e+02 - logprior: -2.4471e+00
Epoch 3/10
13/13 - 2s - loss: 146.1895 - loglik: -1.4414e+02 - logprior: -1.9813e+00
Epoch 4/10
13/13 - 2s - loss: 137.3562 - loglik: -1.3553e+02 - logprior: -1.8197e+00
Epoch 5/10
13/13 - 2s - loss: 134.2489 - loglik: -1.3241e+02 - logprior: -1.7366e+00
Epoch 6/10
13/13 - 2s - loss: 133.5839 - loglik: -1.3159e+02 - logprior: -1.7559e+00
Epoch 7/10
13/13 - 2s - loss: 132.9537 - loglik: -1.3103e+02 - logprior: -1.7131e+00
Epoch 8/10
13/13 - 2s - loss: 132.7426 - loglik: -1.3084e+02 - logprior: -1.7161e+00
Epoch 9/10
13/13 - 2s - loss: 132.3904 - loglik: -1.3048e+02 - logprior: -1.7147e+00
Epoch 10/10
13/13 - 2s - loss: 132.5403 - loglik: -1.3067e+02 - logprior: -1.6953e+00
Fitted a model with MAP estimate = -132.1265
expansions: [(0, 4), (13, 1), (36, 4), (37, 1), (42, 1), (43, 2), (44, 2), (45, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.0033 - loglik: -1.2742e+02 - logprior: -1.0558e+01
Epoch 2/2
13/13 - 2s - loss: 121.1834 - loglik: -1.1751e+02 - logprior: -3.5539e+00
Fitted a model with MAP estimate = -117.5536
expansions: [(0, 2), (44, 2)]
discards: [54]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 124.8257 - loglik: -1.1428e+02 - logprior: -1.0518e+01
Epoch 2/2
13/13 - 2s - loss: 114.7932 - loglik: -1.1095e+02 - logprior: -3.7114e+00
Fitted a model with MAP estimate = -112.3583
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 118.9118 - loglik: -1.1045e+02 - logprior: -8.4341e+00
Epoch 2/10
13/13 - 2s - loss: 112.6426 - loglik: -1.0974e+02 - logprior: -2.7543e+00
Epoch 3/10
13/13 - 2s - loss: 111.6027 - loglik: -1.0912e+02 - logprior: -2.2544e+00
Epoch 4/10
13/13 - 2s - loss: 110.4595 - loglik: -1.0832e+02 - logprior: -1.9023e+00
Epoch 5/10
13/13 - 2s - loss: 110.1482 - loglik: -1.0810e+02 - logprior: -1.8300e+00
Epoch 6/10
13/13 - 2s - loss: 110.0619 - loglik: -1.0808e+02 - logprior: -1.7722e+00
Epoch 7/10
13/13 - 2s - loss: 109.7013 - loglik: -1.0775e+02 - logprior: -1.7471e+00
Epoch 8/10
13/13 - 2s - loss: 109.7111 - loglik: -1.0780e+02 - logprior: -1.7066e+00
Fitted a model with MAP estimate = -109.3489
Time for alignment: 71.6015
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.9924 - loglik: -2.1416e+02 - logprior: -8.8295e+00
Epoch 2/10
13/13 - 2s - loss: 172.7964 - loglik: -1.7028e+02 - logprior: -2.4572e+00
Epoch 3/10
13/13 - 2s - loss: 144.1889 - loglik: -1.4208e+02 - logprior: -2.0365e+00
Epoch 4/10
13/13 - 2s - loss: 136.5618 - loglik: -1.3458e+02 - logprior: -1.9612e+00
Epoch 5/10
13/13 - 2s - loss: 133.6452 - loglik: -1.3160e+02 - logprior: -1.8965e+00
Epoch 6/10
13/13 - 2s - loss: 133.0356 - loglik: -1.3093e+02 - logprior: -1.9038e+00
Epoch 7/10
13/13 - 2s - loss: 132.4783 - loglik: -1.3044e+02 - logprior: -1.8727e+00
Epoch 8/10
13/13 - 2s - loss: 132.1089 - loglik: -1.3008e+02 - logprior: -1.8638e+00
Epoch 9/10
13/13 - 2s - loss: 131.9022 - loglik: -1.2988e+02 - logprior: -1.8514e+00
Epoch 10/10
13/13 - 2s - loss: 132.0895 - loglik: -1.3007e+02 - logprior: -1.8380e+00
Fitted a model with MAP estimate = -131.6828
expansions: [(0, 4), (13, 1), (16, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.8538 - loglik: -1.2825e+02 - logprior: -1.0584e+01
Epoch 2/2
13/13 - 2s - loss: 122.5460 - loglik: -1.1902e+02 - logprior: -3.4216e+00
Fitted a model with MAP estimate = -119.6102
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 128.0503 - loglik: -1.1763e+02 - logprior: -1.0401e+01
Epoch 2/2
13/13 - 2s - loss: 118.6126 - loglik: -1.1494e+02 - logprior: -3.5669e+00
Fitted a model with MAP estimate = -116.5413
expansions: [(64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 123.0740 - loglik: -1.1474e+02 - logprior: -8.3067e+00
Epoch 2/10
13/13 - 2s - loss: 114.7953 - loglik: -1.1200e+02 - logprior: -2.6820e+00
Epoch 3/10
13/13 - 2s - loss: 113.0057 - loglik: -1.1060e+02 - logprior: -2.2252e+00
Epoch 4/10
13/13 - 2s - loss: 112.1841 - loglik: -1.1006e+02 - logprior: -1.9068e+00
Epoch 5/10
13/13 - 2s - loss: 111.9850 - loglik: -1.0995e+02 - logprior: -1.8365e+00
Epoch 6/10
13/13 - 2s - loss: 111.2468 - loglik: -1.0927e+02 - logprior: -1.7808e+00
Epoch 7/10
13/13 - 2s - loss: 111.3097 - loglik: -1.0937e+02 - logprior: -1.7414e+00
Fitted a model with MAP estimate = -110.9191
Time for alignment: 65.6225
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 223.1041 - loglik: -2.1427e+02 - logprior: -8.8261e+00
Epoch 2/10
13/13 - 2s - loss: 172.7054 - loglik: -1.7018e+02 - logprior: -2.4603e+00
Epoch 3/10
13/13 - 2s - loss: 144.6019 - loglik: -1.4250e+02 - logprior: -2.0309e+00
Epoch 4/10
13/13 - 2s - loss: 137.5092 - loglik: -1.3555e+02 - logprior: -1.9359e+00
Epoch 5/10
13/13 - 2s - loss: 134.4137 - loglik: -1.3237e+02 - logprior: -1.8665e+00
Epoch 6/10
13/13 - 2s - loss: 132.8391 - loglik: -1.3069e+02 - logprior: -1.9052e+00
Epoch 7/10
13/13 - 2s - loss: 132.3091 - loglik: -1.3022e+02 - logprior: -1.8851e+00
Epoch 8/10
13/13 - 2s - loss: 132.5579 - loglik: -1.3048e+02 - logprior: -1.8809e+00
Fitted a model with MAP estimate = -131.9055
expansions: [(0, 4), (13, 1), (16, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.3966 - loglik: -1.2787e+02 - logprior: -1.0500e+01
Epoch 2/2
13/13 - 2s - loss: 122.9491 - loglik: -1.1946e+02 - logprior: -3.3597e+00
Fitted a model with MAP estimate = -119.6692
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 127.7432 - loglik: -1.1731e+02 - logprior: -1.0406e+01
Epoch 2/2
13/13 - 2s - loss: 118.8441 - loglik: -1.1517e+02 - logprior: -3.5297e+00
Fitted a model with MAP estimate = -116.2495
expansions: [(64, 3)]
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 122.8086 - loglik: -1.1450e+02 - logprior: -8.2750e+00
Epoch 2/10
13/13 - 2s - loss: 114.7460 - loglik: -1.1193e+02 - logprior: -2.6767e+00
Epoch 3/10
13/13 - 2s - loss: 113.2292 - loglik: -1.1083e+02 - logprior: -2.1875e+00
Epoch 4/10
13/13 - 2s - loss: 111.8175 - loglik: -1.0973e+02 - logprior: -1.8646e+00
Epoch 5/10
13/13 - 2s - loss: 111.6255 - loglik: -1.0959e+02 - logprior: -1.8157e+00
Epoch 6/10
13/13 - 2s - loss: 111.1182 - loglik: -1.0915e+02 - logprior: -1.7561e+00
Epoch 7/10
13/13 - 2s - loss: 110.6258 - loglik: -1.0870e+02 - logprior: -1.7260e+00
Epoch 8/10
13/13 - 2s - loss: 111.3169 - loglik: -1.0942e+02 - logprior: -1.6929e+00
Fitted a model with MAP estimate = -110.5730
Time for alignment: 65.5097
Computed alignments with likelihoods: ['-109.3489', '-110.9191', '-110.5730']
Best model has likelihood: -109.3489
time for generating output: 0.2362
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5071509220925856
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6942513be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e9a5fb20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6944eef0d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f67c068e730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f67c0779fa0>, <__main__.SimpleDirichletPrior object at 0x7f6ae3fa85b0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.9372 - loglik: -5.9067e+01 - logprior: -8.5993e-01
Epoch 2/10
41/41 - 2s - loss: 46.4935 - loglik: -4.5493e+01 - logprior: -9.1728e-01
Epoch 3/10
41/41 - 2s - loss: 45.6540 - loglik: -4.4649e+01 - logprior: -8.9358e-01
Epoch 4/10
41/41 - 2s - loss: 45.2980 - loglik: -4.4271e+01 - logprior: -8.9096e-01
Epoch 5/10
41/41 - 2s - loss: 45.0649 - loglik: -4.4015e+01 - logprior: -8.8932e-01
Epoch 6/10
41/41 - 2s - loss: 44.9286 - loglik: -4.3870e+01 - logprior: -8.8315e-01
Epoch 7/10
41/41 - 2s - loss: 44.8274 - loglik: -4.3770e+01 - logprior: -8.8604e-01
Epoch 8/10
41/41 - 2s - loss: 44.8262 - loglik: -4.3767e+01 - logprior: -8.8383e-01
Epoch 9/10
41/41 - 2s - loss: 44.7575 - loglik: -4.3700e+01 - logprior: -8.8079e-01
Epoch 10/10
41/41 - 1s - loss: 44.7293 - loglik: -4.3668e+01 - logprior: -8.8249e-01
Fitted a model with MAP estimate = -44.6983
expansions: [(8, 2), (9, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 46.0843 - loglik: -4.4888e+01 - logprior: -1.1068e+00
Epoch 2/2
41/41 - 2s - loss: 43.4942 - loglik: -4.2458e+01 - logprior: -8.9687e-01
Fitted a model with MAP estimate = -42.6049
expansions: []
discards: [ 8 13 15 17]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.3234 - loglik: -4.3170e+01 - logprior: -1.0676e+00
Epoch 2/2
41/41 - 2s - loss: 43.3908 - loglik: -4.2398e+01 - logprior: -8.5655e-01
Fitted a model with MAP estimate = -42.6097
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 7s - loss: 42.7562 - loglik: -4.1964e+01 - logprior: -6.9014e-01
Epoch 2/10
58/58 - 2s - loss: 42.2124 - loglik: -4.1482e+01 - logprior: -5.9095e-01
Epoch 3/10
58/58 - 2s - loss: 42.1991 - loglik: -4.1470e+01 - logprior: -5.8428e-01
Epoch 4/10
58/58 - 2s - loss: 41.7795 - loglik: -4.1026e+01 - logprior: -5.8152e-01
Epoch 5/10
58/58 - 2s - loss: 41.7517 - loglik: -4.0991e+01 - logprior: -5.7858e-01
Epoch 6/10
58/58 - 2s - loss: 41.7449 - loglik: -4.0983e+01 - logprior: -5.7740e-01
Epoch 7/10
58/58 - 2s - loss: 41.4708 - loglik: -4.0715e+01 - logprior: -5.7630e-01
Epoch 8/10
58/58 - 2s - loss: 41.5178 - loglik: -4.0766e+01 - logprior: -5.7278e-01
Fitted a model with MAP estimate = -41.2423
Time for alignment: 86.7340
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 60.0096 - loglik: -5.9136e+01 - logprior: -8.6331e-01
Epoch 2/10
41/41 - 2s - loss: 46.4790 - loglik: -4.5516e+01 - logprior: -9.1248e-01
Epoch 3/10
41/41 - 2s - loss: 45.5558 - loglik: -4.4559e+01 - logprior: -8.9197e-01
Epoch 4/10
41/41 - 2s - loss: 45.4064 - loglik: -4.4370e+01 - logprior: -8.9167e-01
Epoch 5/10
41/41 - 1s - loss: 44.9047 - loglik: -4.3853e+01 - logprior: -8.9154e-01
Epoch 6/10
41/41 - 1s - loss: 45.0136 - loglik: -4.3951e+01 - logprior: -8.8750e-01
Fitted a model with MAP estimate = -44.5477
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.3408 - loglik: -4.4157e+01 - logprior: -1.0999e+00
Epoch 2/2
41/41 - 2s - loss: 43.6062 - loglik: -4.2601e+01 - logprior: -8.8138e-01
Fitted a model with MAP estimate = -42.5822
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 44.1088 - loglik: -4.2960e+01 - logprior: -1.0655e+00
Epoch 2/2
41/41 - 2s - loss: 43.4286 - loglik: -4.2439e+01 - logprior: -8.5503e-01
Fitted a model with MAP estimate = -42.5860
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.6983 - loglik: -4.1909e+01 - logprior: -6.8894e-01
Epoch 2/10
58/58 - 2s - loss: 42.2790 - loglik: -4.1550e+01 - logprior: -5.9175e-01
Epoch 3/10
58/58 - 2s - loss: 42.1974 - loglik: -4.1471e+01 - logprior: -5.8512e-01
Epoch 4/10
58/58 - 2s - loss: 41.7563 - loglik: -4.1005e+01 - logprior: -5.8244e-01
Epoch 5/10
58/58 - 2s - loss: 41.7821 - loglik: -4.1018e+01 - logprior: -5.8220e-01
Fitted a model with MAP estimate = -41.4225
Time for alignment: 72.8160
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.8051 - loglik: -5.8973e+01 - logprior: -8.2102e-01
Epoch 2/10
41/41 - 1s - loss: 46.3861 - loglik: -4.5537e+01 - logprior: -7.5221e-01
Epoch 3/10
41/41 - 2s - loss: 45.5314 - loglik: -4.4721e+01 - logprior: -7.0557e-01
Epoch 4/10
41/41 - 1s - loss: 45.3512 - loglik: -4.4517e+01 - logprior: -7.0472e-01
Epoch 5/10
41/41 - 2s - loss: 45.1438 - loglik: -4.4292e+01 - logprior: -7.0066e-01
Epoch 6/10
41/41 - 2s - loss: 45.0464 - loglik: -4.4177e+01 - logprior: -6.9896e-01
Epoch 7/10
41/41 - 2s - loss: 44.7598 - loglik: -4.3890e+01 - logprior: -7.0011e-01
Epoch 8/10
41/41 - 2s - loss: 44.8555 - loglik: -4.3982e+01 - logprior: -6.9755e-01
Fitted a model with MAP estimate = -44.5234
expansions: [(4, 1), (8, 2), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 47.5202 - loglik: -4.6301e+01 - logprior: -1.1298e+00
Epoch 2/2
41/41 - 2s - loss: 43.5374 - loglik: -4.2522e+01 - logprior: -8.9332e-01
Fitted a model with MAP estimate = -42.5570
expansions: []
discards: [ 8 12 16]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1477 - loglik: -4.2996e+01 - logprior: -1.0678e+00
Epoch 2/2
41/41 - 2s - loss: 43.4900 - loglik: -4.2504e+01 - logprior: -8.5300e-01
Fitted a model with MAP estimate = -42.5619
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7120 - loglik: -4.1921e+01 - logprior: -6.8952e-01
Epoch 2/10
58/58 - 2s - loss: 42.2228 - loglik: -4.1493e+01 - logprior: -5.9100e-01
Epoch 3/10
58/58 - 2s - loss: 42.2062 - loglik: -4.1481e+01 - logprior: -5.8490e-01
Epoch 4/10
58/58 - 2s - loss: 41.6865 - loglik: -4.0934e+01 - logprior: -5.8096e-01
Epoch 5/10
58/58 - 2s - loss: 41.8097 - loglik: -4.1047e+01 - logprior: -5.7951e-01
Fitted a model with MAP estimate = -41.4155
Time for alignment: 75.7978
Computed alignments with likelihoods: ['-41.2423', '-41.4225', '-41.4155']
Best model has likelihood: -41.2423
time for generating output: 0.1227
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6b05ba2130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6957732d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aec45a700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6af496ae20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69721f8ac0>, <__main__.SimpleDirichletPrior object at 0x7f694de5f850>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 283.0137 - loglik: -1.5687e+02 - logprior: -1.2613e+02
Epoch 2/10
10/10 - 1s - loss: 170.8333 - loglik: -1.3509e+02 - logprior: -3.5734e+01
Epoch 3/10
10/10 - 1s - loss: 135.9379 - loglik: -1.1794e+02 - logprior: -1.7977e+01
Epoch 4/10
10/10 - 1s - loss: 120.1005 - loglik: -1.0853e+02 - logprior: -1.1547e+01
Epoch 5/10
10/10 - 1s - loss: 112.0908 - loglik: -1.0409e+02 - logprior: -8.0010e+00
Epoch 6/10
10/10 - 1s - loss: 108.4044 - loglik: -1.0247e+02 - logprior: -5.9246e+00
Epoch 7/10
10/10 - 1s - loss: 106.6203 - loglik: -1.0198e+02 - logprior: -4.6226e+00
Epoch 8/10
10/10 - 1s - loss: 105.3566 - loglik: -1.0137e+02 - logprior: -3.8663e+00
Epoch 9/10
10/10 - 1s - loss: 104.6917 - loglik: -1.0108e+02 - logprior: -3.3157e+00
Epoch 10/10
10/10 - 1s - loss: 104.2374 - loglik: -1.0106e+02 - logprior: -2.9147e+00
Fitted a model with MAP estimate = -103.8317
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.1297 - loglik: -1.0144e+02 - logprior: -1.4167e+02
Epoch 2/2
10/10 - 1s - loss: 155.4395 - loglik: -9.5400e+01 - logprior: -5.9985e+01
Fitted a model with MAP estimate = -140.5447
expansions: [(0, 2)]
discards: [ 0  8 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.2870 - loglik: -9.1172e+01 - logprior: -1.1410e+02
Epoch 2/2
10/10 - 1s - loss: 122.0093 - loglik: -8.9535e+01 - logprior: -3.2427e+01
Fitted a model with MAP estimate = -109.6322
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.4497 - loglik: -9.2031e+01 - logprior: -1.3140e+02
Epoch 2/10
10/10 - 1s - loss: 130.5553 - loglik: -9.1295e+01 - logprior: -3.9195e+01
Epoch 3/10
10/10 - 1s - loss: 108.0181 - loglik: -9.1242e+01 - logprior: -1.6676e+01
Epoch 4/10
10/10 - 1s - loss: 100.0085 - loglik: -9.1298e+01 - logprior: -8.5277e+00
Epoch 5/10
10/10 - 1s - loss: 96.0375 - loglik: -9.1378e+01 - logprior: -4.4290e+00
Epoch 6/10
10/10 - 1s - loss: 93.8369 - loglik: -9.1452e+01 - logprior: -2.1353e+00
Epoch 7/10
10/10 - 1s - loss: 92.5149 - loglik: -9.1485e+01 - logprior: -7.6804e-01
Epoch 8/10
10/10 - 1s - loss: 91.6475 - loglik: -9.1533e+01 - logprior: 0.1274
Epoch 9/10
10/10 - 1s - loss: 91.0436 - loglik: -9.1594e+01 - logprior: 0.7961
Epoch 10/10
10/10 - 1s - loss: 90.5698 - loglik: -9.1644e+01 - logprior: 1.3360
Fitted a model with MAP estimate = -90.0802
Time for alignment: 33.7895
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 283.0137 - loglik: -1.5687e+02 - logprior: -1.2613e+02
Epoch 2/10
10/10 - 1s - loss: 170.8334 - loglik: -1.3509e+02 - logprior: -3.5734e+01
Epoch 3/10
10/10 - 1s - loss: 135.9380 - loglik: -1.1794e+02 - logprior: -1.7977e+01
Epoch 4/10
10/10 - 1s - loss: 120.1005 - loglik: -1.0853e+02 - logprior: -1.1548e+01
Epoch 5/10
10/10 - 1s - loss: 112.0907 - loglik: -1.0409e+02 - logprior: -8.0011e+00
Epoch 6/10
10/10 - 1s - loss: 108.3864 - loglik: -1.0244e+02 - logprior: -5.9368e+00
Epoch 7/10
10/10 - 1s - loss: 106.4061 - loglik: -1.0162e+02 - logprior: -4.7118e+00
Epoch 8/10
10/10 - 1s - loss: 105.3183 - loglik: -1.0114e+02 - logprior: -3.8891e+00
Epoch 9/10
10/10 - 1s - loss: 104.7383 - loglik: -1.0111e+02 - logprior: -3.3036e+00
Epoch 10/10
10/10 - 1s - loss: 104.2764 - loglik: -1.0110e+02 - logprior: -2.9208e+00
Fitted a model with MAP estimate = -103.8314
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 243.3735 - loglik: -1.0164e+02 - logprior: -1.4171e+02
Epoch 2/2
10/10 - 1s - loss: 155.2100 - loglik: -9.5016e+01 - logprior: -6.0119e+01
Fitted a model with MAP estimate = -140.2379
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.6984 - loglik: -9.1449e+01 - logprior: -1.1423e+02
Epoch 2/2
10/10 - 1s - loss: 122.2668 - loglik: -8.9704e+01 - logprior: -3.2515e+01
Fitted a model with MAP estimate = -109.9024
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.1129 - loglik: -9.2231e+01 - logprior: -1.3186e+02
Epoch 2/10
10/10 - 1s - loss: 131.2464 - loglik: -9.1585e+01 - logprior: -3.9606e+01
Epoch 3/10
10/10 - 1s - loss: 108.4004 - loglik: -9.1559e+01 - logprior: -1.6750e+01
Epoch 4/10
10/10 - 1s - loss: 100.3535 - loglik: -9.1656e+01 - logprior: -8.5321e+00
Epoch 5/10
10/10 - 1s - loss: 96.4015 - loglik: -9.1758e+01 - logprior: -4.4351e+00
Epoch 6/10
10/10 - 1s - loss: 94.1600 - loglik: -9.1778e+01 - logprior: -2.1691e+00
Epoch 7/10
10/10 - 1s - loss: 92.8478 - loglik: -9.1840e+01 - logprior: -7.9158e-01
Epoch 8/10
10/10 - 1s - loss: 91.9960 - loglik: -9.1884e+01 - logprior: 0.1200
Epoch 9/10
10/10 - 1s - loss: 91.3902 - loglik: -9.1914e+01 - logprior: 0.7747
Epoch 10/10
10/10 - 1s - loss: 90.9198 - loglik: -9.1992e+01 - logprior: 1.3286
Fitted a model with MAP estimate = -90.4344
Time for alignment: 33.5544
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 283.0138 - loglik: -1.5687e+02 - logprior: -1.2613e+02
Epoch 2/10
10/10 - 1s - loss: 170.8334 - loglik: -1.3509e+02 - logprior: -3.5734e+01
Epoch 3/10
10/10 - 1s - loss: 135.9379 - loglik: -1.1794e+02 - logprior: -1.7977e+01
Epoch 4/10
10/10 - 1s - loss: 120.1005 - loglik: -1.0853e+02 - logprior: -1.1547e+01
Epoch 5/10
10/10 - 1s - loss: 112.0876 - loglik: -1.0408e+02 - logprior: -8.0034e+00
Epoch 6/10
10/10 - 1s - loss: 108.2349 - loglik: -1.0220e+02 - logprior: -6.0085e+00
Epoch 7/10
10/10 - 1s - loss: 106.2375 - loglik: -1.0125e+02 - logprior: -4.7814e+00
Epoch 8/10
10/10 - 1s - loss: 105.2865 - loglik: -1.0102e+02 - logprior: -3.9365e+00
Epoch 9/10
10/10 - 1s - loss: 104.6834 - loglik: -1.0104e+02 - logprior: -3.3628e+00
Epoch 10/10
10/10 - 1s - loss: 104.2381 - loglik: -1.0104e+02 - logprior: -2.9617e+00
Fitted a model with MAP estimate = -103.8305
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.4374 - loglik: -1.0169e+02 - logprior: -1.4173e+02
Epoch 2/2
10/10 - 1s - loss: 155.2518 - loglik: -9.5076e+01 - logprior: -6.0113e+01
Fitted a model with MAP estimate = -140.3515
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.7480 - loglik: -9.1514e+01 - logprior: -1.1422e+02
Epoch 2/2
10/10 - 1s - loss: 122.3067 - loglik: -8.9779e+01 - logprior: -3.2500e+01
Fitted a model with MAP estimate = -109.9735
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.0916 - loglik: -9.2231e+01 - logprior: -1.3184e+02
Epoch 2/10
10/10 - 1s - loss: 131.2629 - loglik: -9.1658e+01 - logprior: -3.9574e+01
Epoch 3/10
10/10 - 1s - loss: 108.4688 - loglik: -9.1757e+01 - logprior: -1.6683e+01
Epoch 4/10
10/10 - 1s - loss: 100.4145 - loglik: -9.1806e+01 - logprior: -8.4867e+00
Epoch 5/10
10/10 - 1s - loss: 96.4976 - loglik: -9.1899e+01 - logprior: -4.3952e+00
Epoch 6/10
10/10 - 1s - loss: 94.2434 - loglik: -9.1908e+01 - logprior: -2.1198e+00
Epoch 7/10
10/10 - 1s - loss: 92.8847 - loglik: -9.1889e+01 - logprior: -7.7640e-01
Epoch 8/10
10/10 - 1s - loss: 92.0283 - loglik: -9.1950e+01 - logprior: 0.1403
Epoch 9/10
10/10 - 1s - loss: 91.4175 - loglik: -9.1983e+01 - logprior: 0.8006
Epoch 10/10
10/10 - 1s - loss: 90.9375 - loglik: -9.2019e+01 - logprior: 1.3317
Fitted a model with MAP estimate = -90.4549
Time for alignment: 33.6144
Computed alignments with likelihoods: ['-90.0802', '-90.4344', '-90.4549']
Best model has likelihood: -90.0802
time for generating output: 0.1202
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8824228028503563
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69b5776a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f67a01ac640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69430a93d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f694e0a5940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f67c074dee0>, <__main__.SimpleDirichletPrior object at 0x7f69428d4f40>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.3811 - loglik: -1.9216e+02 - logprior: -3.2155e+00
Epoch 2/10
19/19 - 2s - loss: 156.9406 - loglik: -1.5559e+02 - logprior: -1.3209e+00
Epoch 3/10
19/19 - 2s - loss: 141.6687 - loglik: -1.4009e+02 - logprior: -1.4348e+00
Epoch 4/10
19/19 - 2s - loss: 139.4328 - loglik: -1.3789e+02 - logprior: -1.4172e+00
Epoch 5/10
19/19 - 2s - loss: 138.6132 - loglik: -1.3715e+02 - logprior: -1.3640e+00
Epoch 6/10
19/19 - 2s - loss: 138.4325 - loglik: -1.3699e+02 - logprior: -1.3328e+00
Epoch 7/10
19/19 - 2s - loss: 138.1759 - loglik: -1.3675e+02 - logprior: -1.3104e+00
Epoch 8/10
19/19 - 2s - loss: 138.1707 - loglik: -1.3676e+02 - logprior: -1.2933e+00
Epoch 9/10
19/19 - 2s - loss: 137.8986 - loglik: -1.3649e+02 - logprior: -1.2899e+00
Epoch 10/10
19/19 - 2s - loss: 137.9034 - loglik: -1.3650e+02 - logprior: -1.2789e+00
Fitted a model with MAP estimate = -138.5569
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 143.4349 - loglik: -1.3886e+02 - logprior: -4.5198e+00
Epoch 2/2
19/19 - 2s - loss: 132.3376 - loglik: -1.3048e+02 - logprior: -1.7475e+00
Fitted a model with MAP estimate = -131.9619
expansions: [(0, 2)]
discards: [19 26 29 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.3439 - loglik: -1.3020e+02 - logprior: -4.0969e+00
Epoch 2/2
19/19 - 2s - loss: 128.5718 - loglik: -1.2692e+02 - logprior: -1.5409e+00
Fitted a model with MAP estimate = -128.3665
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.8523 - loglik: -1.2923e+02 - logprior: -3.5766e+00
Epoch 2/10
21/21 - 2s - loss: 129.4208 - loglik: -1.2747e+02 - logprior: -1.8323e+00
Epoch 3/10
21/21 - 2s - loss: 128.0936 - loglik: -1.2660e+02 - logprior: -1.3120e+00
Epoch 4/10
21/21 - 2s - loss: 127.5416 - loglik: -1.2607e+02 - logprior: -1.2481e+00
Epoch 5/10
21/21 - 2s - loss: 127.0709 - loglik: -1.2559e+02 - logprior: -1.2460e+00
Epoch 6/10
21/21 - 2s - loss: 126.9940 - loglik: -1.2554e+02 - logprior: -1.2310e+00
Epoch 7/10
21/21 - 2s - loss: 126.6912 - loglik: -1.2526e+02 - logprior: -1.2095e+00
Epoch 8/10
21/21 - 2s - loss: 126.7879 - loglik: -1.2538e+02 - logprior: -1.1921e+00
Fitted a model with MAP estimate = -126.3865
Time for alignment: 68.1531
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.5621 - loglik: -1.9234e+02 - logprior: -3.2169e+00
Epoch 2/10
19/19 - 2s - loss: 155.6337 - loglik: -1.5428e+02 - logprior: -1.3319e+00
Epoch 3/10
19/19 - 2s - loss: 141.0137 - loglik: -1.3939e+02 - logprior: -1.4505e+00
Epoch 4/10
19/19 - 2s - loss: 139.1680 - loglik: -1.3762e+02 - logprior: -1.4130e+00
Epoch 5/10
19/19 - 2s - loss: 138.5564 - loglik: -1.3708e+02 - logprior: -1.3608e+00
Epoch 6/10
19/19 - 2s - loss: 138.1867 - loglik: -1.3675e+02 - logprior: -1.3313e+00
Epoch 7/10
19/19 - 2s - loss: 138.1137 - loglik: -1.3669e+02 - logprior: -1.3082e+00
Epoch 8/10
19/19 - 2s - loss: 138.0499 - loglik: -1.3664e+02 - logprior: -1.2985e+00
Epoch 9/10
19/19 - 2s - loss: 137.8581 - loglik: -1.3646e+02 - logprior: -1.2858e+00
Epoch 10/10
19/19 - 2s - loss: 137.9664 - loglik: -1.3657e+02 - logprior: -1.2835e+00
Fitted a model with MAP estimate = -138.5257
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (24, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 143.3148 - loglik: -1.3876e+02 - logprior: -4.5060e+00
Epoch 2/2
19/19 - 2s - loss: 132.4285 - loglik: -1.3058e+02 - logprior: -1.7388e+00
Fitted a model with MAP estimate = -132.0779
expansions: [(0, 2)]
discards: [19 26 33 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 134.8097 - loglik: -1.3068e+02 - logprior: -4.0859e+00
Epoch 2/2
19/19 - 2s - loss: 129.4193 - loglik: -1.2777e+02 - logprior: -1.5403e+00
Fitted a model with MAP estimate = -129.2181
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.2896 - loglik: -1.2968e+02 - logprior: -3.5542e+00
Epoch 2/10
21/21 - 2s - loss: 129.6186 - loglik: -1.2765e+02 - logprior: -1.8400e+00
Epoch 3/10
21/21 - 2s - loss: 128.0988 - loglik: -1.2661e+02 - logprior: -1.3084e+00
Epoch 4/10
21/21 - 2s - loss: 127.3862 - loglik: -1.2591e+02 - logprior: -1.2482e+00
Epoch 5/10
21/21 - 2s - loss: 127.0325 - loglik: -1.2555e+02 - logprior: -1.2422e+00
Epoch 6/10
21/21 - 2s - loss: 126.7873 - loglik: -1.2533e+02 - logprior: -1.2287e+00
Epoch 7/10
21/21 - 2s - loss: 126.6321 - loglik: -1.2520e+02 - logprior: -1.2116e+00
Epoch 8/10
21/21 - 2s - loss: 126.6263 - loglik: -1.2522e+02 - logprior: -1.1888e+00
Epoch 9/10
21/21 - 2s - loss: 126.6252 - loglik: -1.2524e+02 - logprior: -1.1696e+00
Epoch 10/10
21/21 - 2s - loss: 126.5518 - loglik: -1.2518e+02 - logprior: -1.1554e+00
Fitted a model with MAP estimate = -126.1868
Time for alignment: 68.9341
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.2918 - loglik: -1.9207e+02 - logprior: -3.2141e+00
Epoch 2/10
19/19 - 2s - loss: 154.7226 - loglik: -1.5338e+02 - logprior: -1.3179e+00
Epoch 3/10
19/19 - 2s - loss: 141.6920 - loglik: -1.4015e+02 - logprior: -1.4167e+00
Epoch 4/10
19/19 - 2s - loss: 139.7319 - loglik: -1.3820e+02 - logprior: -1.4015e+00
Epoch 5/10
19/19 - 2s - loss: 139.0788 - loglik: -1.3761e+02 - logprior: -1.3601e+00
Epoch 6/10
19/19 - 2s - loss: 138.7042 - loglik: -1.3726e+02 - logprior: -1.3445e+00
Epoch 7/10
19/19 - 2s - loss: 138.5247 - loglik: -1.3709e+02 - logprior: -1.3212e+00
Epoch 8/10
19/19 - 2s - loss: 138.4523 - loglik: -1.3704e+02 - logprior: -1.3103e+00
Epoch 9/10
19/19 - 2s - loss: 138.3833 - loglik: -1.3697e+02 - logprior: -1.3028e+00
Epoch 10/10
19/19 - 2s - loss: 138.3757 - loglik: -1.3697e+02 - logprior: -1.2990e+00
Fitted a model with MAP estimate = -138.8265
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 142.4842 - loglik: -1.3794e+02 - logprior: -4.4976e+00
Epoch 2/2
19/19 - 2s - loss: 131.9343 - loglik: -1.3015e+02 - logprior: -1.6824e+00
Fitted a model with MAP estimate = -131.6644
expansions: [(0, 2)]
discards: [27 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 134.1396 - loglik: -1.3001e+02 - logprior: -4.0925e+00
Epoch 2/2
19/19 - 2s - loss: 129.0112 - loglik: -1.2743e+02 - logprior: -1.4667e+00
Fitted a model with MAP estimate = -129.3916
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.7023 - loglik: -1.3012e+02 - logprior: -3.5305e+00
Epoch 2/10
21/21 - 2s - loss: 129.9944 - loglik: -1.2813e+02 - logprior: -1.7479e+00
Epoch 3/10
21/21 - 2s - loss: 129.0489 - loglik: -1.2759e+02 - logprior: -1.2837e+00
Epoch 4/10
21/21 - 2s - loss: 128.1349 - loglik: -1.2669e+02 - logprior: -1.2333e+00
Epoch 5/10
21/21 - 2s - loss: 128.1086 - loglik: -1.2666e+02 - logprior: -1.2253e+00
Epoch 6/10
21/21 - 2s - loss: 127.7826 - loglik: -1.2635e+02 - logprior: -1.2124e+00
Epoch 7/10
21/21 - 2s - loss: 127.5558 - loglik: -1.2615e+02 - logprior: -1.1938e+00
Epoch 8/10
21/21 - 2s - loss: 127.4595 - loglik: -1.2608e+02 - logprior: -1.1712e+00
Epoch 9/10
21/21 - 2s - loss: 127.4017 - loglik: -1.2603e+02 - logprior: -1.1540e+00
Epoch 10/10
21/21 - 2s - loss: 127.4114 - loglik: -1.2607e+02 - logprior: -1.1320e+00
Fitted a model with MAP estimate = -127.0758
Time for alignment: 71.3295
Computed alignments with likelihoods: ['-126.3865', '-126.1868', '-127.0758']
Best model has likelihood: -126.1868
time for generating output: 0.1685
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.941983780411728
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac9201a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac8b7e5b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8b7e940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f67c0211be0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f694513c0a0>, <__main__.SimpleDirichletPrior object at 0x7f61701d3bb0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 391.2368 - loglik: -3.8268e+02 - logprior: -8.5181e+00
Epoch 2/10
13/13 - 3s - loss: 339.0854 - loglik: -3.3667e+02 - logprior: -2.2872e+00
Epoch 3/10
13/13 - 3s - loss: 297.3623 - loglik: -2.9472e+02 - logprior: -2.2660e+00
Epoch 4/10
13/13 - 3s - loss: 281.4091 - loglik: -2.7789e+02 - logprior: -2.7315e+00
Epoch 5/10
13/13 - 3s - loss: 277.7166 - loglik: -2.7428e+02 - logprior: -2.7973e+00
Epoch 6/10
13/13 - 3s - loss: 276.2904 - loglik: -2.7315e+02 - logprior: -2.7104e+00
Epoch 7/10
13/13 - 3s - loss: 275.7165 - loglik: -2.7265e+02 - logprior: -2.7040e+00
Epoch 8/10
13/13 - 3s - loss: 274.5005 - loglik: -2.7147e+02 - logprior: -2.7225e+00
Epoch 9/10
13/13 - 3s - loss: 274.1486 - loglik: -2.7115e+02 - logprior: -2.7125e+00
Epoch 10/10
13/13 - 3s - loss: 274.1162 - loglik: -2.7112e+02 - logprior: -2.7147e+00
Fitted a model with MAP estimate = -273.8951
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 3), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 286.1114 - loglik: -2.7592e+02 - logprior: -1.0150e+01
Epoch 2/2
13/13 - 3s - loss: 265.7437 - loglik: -2.6093e+02 - logprior: -4.5501e+00
Fitted a model with MAP estimate = -261.2264
expansions: [(0, 3)]
discards: [  0  68  88  89 128]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 268.2957 - loglik: -2.6036e+02 - logprior: -7.8906e+00
Epoch 2/2
13/13 - 3s - loss: 257.9420 - loglik: -2.5538e+02 - logprior: -2.3182e+00
Fitted a model with MAP estimate = -255.8304
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 270.1149 - loglik: -2.6034e+02 - logprior: -9.7358e+00
Epoch 2/10
13/13 - 3s - loss: 260.9010 - loglik: -2.5736e+02 - logprior: -3.3102e+00
Epoch 3/10
13/13 - 3s - loss: 256.7927 - loglik: -2.5463e+02 - logprior: -1.6553e+00
Epoch 4/10
13/13 - 3s - loss: 253.5346 - loglik: -2.5179e+02 - logprior: -1.1091e+00
Epoch 5/10
13/13 - 3s - loss: 253.8069 - loglik: -2.5222e+02 - logprior: -1.0230e+00
Fitted a model with MAP estimate = -252.5517
Time for alignment: 87.1738
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 390.7901 - loglik: -3.8226e+02 - logprior: -8.4975e+00
Epoch 2/10
13/13 - 3s - loss: 340.1187 - loglik: -3.3772e+02 - logprior: -2.2726e+00
Epoch 3/10
13/13 - 3s - loss: 298.5998 - loglik: -2.9605e+02 - logprior: -2.2625e+00
Epoch 4/10
13/13 - 3s - loss: 284.1999 - loglik: -2.8092e+02 - logprior: -2.7123e+00
Epoch 5/10
13/13 - 3s - loss: 278.7985 - loglik: -2.7538e+02 - logprior: -2.7656e+00
Epoch 6/10
13/13 - 3s - loss: 277.9323 - loglik: -2.7476e+02 - logprior: -2.6443e+00
Epoch 7/10
13/13 - 3s - loss: 276.6500 - loglik: -2.7359e+02 - logprior: -2.6156e+00
Epoch 8/10
13/13 - 3s - loss: 276.5846 - loglik: -2.7361e+02 - logprior: -2.5960e+00
Epoch 9/10
13/13 - 3s - loss: 275.3741 - loglik: -2.7242e+02 - logprior: -2.6000e+00
Epoch 10/10
13/13 - 3s - loss: 276.0677 - loglik: -2.7312e+02 - logprior: -2.6008e+00
Fitted a model with MAP estimate = -275.1329
expansions: [(7, 3), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (35, 1), (38, 1), (39, 1), (49, 1), (52, 1), (53, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 287.8983 - loglik: -2.7770e+02 - logprior: -1.0158e+01
Epoch 2/2
13/13 - 4s - loss: 265.0683 - loglik: -2.6021e+02 - logprior: -4.6004e+00
Fitted a model with MAP estimate = -261.5817
expansions: [(0, 3)]
discards: [  0   8  68  88  89 128]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 268.5960 - loglik: -2.6068e+02 - logprior: -7.8734e+00
Epoch 2/2
13/13 - 3s - loss: 258.2670 - loglik: -2.5573e+02 - logprior: -2.2979e+00
Fitted a model with MAP estimate = -255.9590
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 269.5774 - loglik: -2.5982e+02 - logprior: -9.7179e+00
Epoch 2/10
13/13 - 3s - loss: 260.9767 - loglik: -2.5747e+02 - logprior: -3.2625e+00
Epoch 3/10
13/13 - 3s - loss: 256.2588 - loglik: -2.5410e+02 - logprior: -1.6351e+00
Epoch 4/10
13/13 - 3s - loss: 253.6919 - loglik: -2.5192e+02 - logprior: -1.1451e+00
Epoch 5/10
13/13 - 3s - loss: 253.8213 - loglik: -2.5220e+02 - logprior: -1.0808e+00
Fitted a model with MAP estimate = -252.4244
Time for alignment: 88.4495
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 391.2346 - loglik: -3.8269e+02 - logprior: -8.5060e+00
Epoch 2/10
13/13 - 3s - loss: 338.7628 - loglik: -3.3635e+02 - logprior: -2.2723e+00
Epoch 3/10
13/13 - 3s - loss: 296.0571 - loglik: -2.9355e+02 - logprior: -2.2399e+00
Epoch 4/10
13/13 - 3s - loss: 282.0198 - loglik: -2.7899e+02 - logprior: -2.7086e+00
Epoch 5/10
13/13 - 3s - loss: 278.6727 - loglik: -2.7550e+02 - logprior: -2.7526e+00
Epoch 6/10
13/13 - 3s - loss: 277.5473 - loglik: -2.7448e+02 - logprior: -2.6581e+00
Epoch 7/10
13/13 - 3s - loss: 275.5098 - loglik: -2.7248e+02 - logprior: -2.6470e+00
Epoch 8/10
13/13 - 3s - loss: 276.5603 - loglik: -2.7358e+02 - logprior: -2.6433e+00
Fitted a model with MAP estimate = -275.2745
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 286.6151 - loglik: -2.7642e+02 - logprior: -1.0152e+01
Epoch 2/2
13/13 - 3s - loss: 265.3533 - loglik: -2.6055e+02 - logprior: -4.5425e+00
Fitted a model with MAP estimate = -261.6006
expansions: [(0, 3)]
discards: [  0  65  87  88 127]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 268.3190 - loglik: -2.6041e+02 - logprior: -7.8709e+00
Epoch 2/2
13/13 - 3s - loss: 258.1771 - loglik: -2.5565e+02 - logprior: -2.2972e+00
Fitted a model with MAP estimate = -256.0239
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 269.4367 - loglik: -2.5965e+02 - logprior: -9.7456e+00
Epoch 2/10
13/13 - 3s - loss: 260.8433 - loglik: -2.5727e+02 - logprior: -3.3388e+00
Epoch 3/10
13/13 - 3s - loss: 255.9225 - loglik: -2.5380e+02 - logprior: -1.6804e+00
Epoch 4/10
13/13 - 3s - loss: 254.6565 - loglik: -2.5297e+02 - logprior: -1.1772e+00
Epoch 5/10
13/13 - 3s - loss: 253.3812 - loglik: -2.5184e+02 - logprior: -1.0584e+00
Epoch 6/10
13/13 - 3s - loss: 252.7154 - loglik: -2.5125e+02 - logprior: -1.0376e+00
Epoch 7/10
13/13 - 3s - loss: 252.5108 - loglik: -2.5109e+02 - logprior: -1.0478e+00
Epoch 8/10
13/13 - 3s - loss: 252.7429 - loglik: -2.5135e+02 - logprior: -1.0496e+00
Fitted a model with MAP estimate = -251.7446
Time for alignment: 90.8904
Computed alignments with likelihoods: ['-252.5517', '-252.4244', '-251.7446']
Best model has likelihood: -251.7446
time for generating output: 0.2092
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.940771349862259
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6956dd6d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6968bdbca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d80f1d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6993bb2580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69941caac0>, <__main__.SimpleDirichletPrior object at 0x7f6aad48c160>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 38s - loss: 1084.8804 - loglik: -1.0800e+03 - logprior: -4.8140e+00
Epoch 2/10
25/25 - 34s - loss: 826.9240 - loglik: -8.2444e+02 - logprior: -2.4254e+00
Epoch 3/10
25/25 - 34s - loss: 770.7186 - loglik: -7.6617e+02 - logprior: -4.1042e+00
Epoch 4/10
25/25 - 34s - loss: 758.1334 - loglik: -7.5315e+02 - logprior: -4.3201e+00
Epoch 5/10
25/25 - 34s - loss: 749.7501 - loglik: -7.4453e+02 - logprior: -4.4770e+00
Epoch 6/10
25/25 - 34s - loss: 749.1870 - loglik: -7.4393e+02 - logprior: -4.5207e+00
Epoch 7/10
25/25 - 34s - loss: 747.8079 - loglik: -7.4245e+02 - logprior: -4.6222e+00
Epoch 8/10
25/25 - 34s - loss: 746.6267 - loglik: -7.4124e+02 - logprior: -4.6455e+00
Epoch 9/10
25/25 - 35s - loss: 745.2887 - loglik: -7.3991e+02 - logprior: -4.6564e+00
Epoch 10/10
25/25 - 34s - loss: 744.9470 - loglik: -7.3957e+02 - logprior: -4.6489e+00
Fitted a model with MAP estimate = -744.2274
expansions: [(50, 1), (132, 1), (135, 1), (144, 1), (163, 1), (165, 1), (171, 1), (175, 4), (176, 2), (177, 2), (192, 1), (193, 1), (194, 5), (195, 2), (198, 1), (199, 1), (200, 1), (201, 1), (203, 1), (204, 2), (205, 1), (207, 1), (209, 1), (210, 1), (213, 1), (214, 1), (216, 1), (218, 3), (224, 1), (225, 2), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (234, 1), (237, 1), (238, 1), (239, 1), (250, 1), (251, 1), (255, 2), (256, 3), (258, 1), (259, 1), (267, 1), (281, 1), (283, 1), (284, 1), (300, 1), (301, 1), (302, 2), (304, 1), (305, 1), (314, 1), (317, 1), (318, 1), (321, 2), (325, 2), (327, 2), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 54s - loss: 738.4619 - loglik: -7.3230e+02 - logprior: -6.0113e+00
Epoch 2/2
25/25 - 48s - loss: 701.6619 - loglik: -6.9891e+02 - logprior: -2.1245e+00
Fitted a model with MAP estimate = -694.5241
expansions: []
discards: [184 185 214 396]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 51s - loss: 709.4059 - loglik: -7.0476e+02 - logprior: -4.5111e+00
Epoch 2/2
25/25 - 47s - loss: 697.2625 - loglik: -6.9597e+02 - logprior: -6.9719e-01
Fitted a model with MAP estimate = -692.7152
expansions: []
discards: [186]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 52s - loss: 706.2383 - loglik: -7.0191e+02 - logprior: -4.1969e+00
Epoch 2/10
25/25 - 47s - loss: 700.4423 - loglik: -6.9946e+02 - logprior: -3.5788e-01
Epoch 3/10
25/25 - 47s - loss: 689.0023 - loglik: -6.8802e+02 - logprior: 0.0810
Epoch 4/10
25/25 - 47s - loss: 692.1072 - loglik: -6.9122e+02 - logprior: 0.2343
Fitted a model with MAP estimate = -686.7122
Time for alignment: 894.9939
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 38s - loss: 1081.5289 - loglik: -1.0767e+03 - logprior: -4.7430e+00
Epoch 2/10
25/25 - 34s - loss: 827.0430 - loglik: -8.2459e+02 - logprior: -2.3517e+00
Epoch 3/10
25/25 - 35s - loss: 767.9274 - loglik: -7.6344e+02 - logprior: -3.8485e+00
Epoch 4/10
25/25 - 34s - loss: 754.6559 - loglik: -7.4970e+02 - logprior: -4.1425e+00
Epoch 5/10
25/25 - 34s - loss: 751.0578 - loglik: -7.4596e+02 - logprior: -4.2161e+00
Epoch 6/10
25/25 - 35s - loss: 751.1421 - loglik: -7.4597e+02 - logprior: -4.3264e+00
Fitted a model with MAP estimate = -747.3464
expansions: [(146, 1), (165, 1), (166, 2), (167, 1), (170, 1), (171, 1), (175, 5), (176, 2), (177, 2), (178, 1), (179, 1), (191, 1), (193, 4), (194, 1), (195, 1), (198, 1), (199, 3), (200, 1), (202, 1), (203, 1), (205, 1), (207, 1), (210, 1), (212, 1), (213, 1), (214, 1), (216, 1), (218, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (231, 1), (238, 3), (239, 2), (250, 1), (253, 1), (255, 2), (256, 3), (258, 1), (259, 1), (267, 1), (281, 1), (283, 1), (300, 1), (302, 1), (303, 2), (305, 1), (315, 2), (317, 1), (320, 1), (326, 3), (328, 2), (341, 1), (359, 1), (361, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 456 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 53s - loss: 733.6387 - loglik: -7.2774e+02 - logprior: -5.7596e+00
Epoch 2/2
25/25 - 48s - loss: 702.9009 - loglik: -7.0046e+02 - logprior: -1.8821e+00
Fitted a model with MAP estimate = -695.8026
expansions: [(437, 1)]
discards: [170 184 185 186 214 290]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 711.4514 - loglik: -7.0592e+02 - logprior: -5.3933e+00
Epoch 2/2
25/25 - 47s - loss: 698.2768 - loglik: -6.9642e+02 - logprior: -1.2758e+00
Fitted a model with MAP estimate = -692.8739
expansions: [(183, 3)]
discards: [188 429]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 50s - loss: 709.4733 - loglik: -7.0499e+02 - logprior: -4.3502e+00
Epoch 2/10
25/25 - 47s - loss: 696.7204 - loglik: -6.9553e+02 - logprior: -5.9036e-01
Epoch 3/10
25/25 - 47s - loss: 691.9905 - loglik: -6.9080e+02 - logprior: -2.2372e-01
Epoch 4/10
25/25 - 47s - loss: 690.0564 - loglik: -6.8889e+02 - logprior: -7.9568e-02
Epoch 5/10
25/25 - 47s - loss: 690.2168 - loglik: -6.8933e+02 - logprior: 0.1370
Fitted a model with MAP estimate = -685.7717
Time for alignment: 803.3974
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 40s - loss: 1079.0402 - loglik: -1.0743e+03 - logprior: -4.6396e+00
Epoch 2/10
25/25 - 35s - loss: 819.5214 - loglik: -8.1708e+02 - logprior: -2.3360e+00
Epoch 3/10
25/25 - 34s - loss: 763.8128 - loglik: -7.5887e+02 - logprior: -4.2017e+00
Epoch 4/10
25/25 - 34s - loss: 753.6464 - loglik: -7.4866e+02 - logprior: -4.0489e+00
Epoch 5/10
25/25 - 34s - loss: 748.8555 - loglik: -7.4386e+02 - logprior: -4.0411e+00
Epoch 6/10
25/25 - 35s - loss: 746.3109 - loglik: -7.4127e+02 - logprior: -4.1524e+00
Epoch 7/10
25/25 - 35s - loss: 744.2690 - loglik: -7.3912e+02 - logprior: -4.2869e+00
Epoch 8/10
25/25 - 34s - loss: 745.2667 - loglik: -7.4010e+02 - logprior: -4.3285e+00
Fitted a model with MAP estimate = -742.6412
expansions: [(3, 1), (133, 1), (136, 1), (165, 1), (167, 1), (173, 1), (177, 6), (178, 2), (179, 1), (180, 1), (193, 1), (194, 4), (195, 1), (196, 1), (199, 1), (200, 3), (201, 2), (203, 1), (204, 1), (206, 1), (208, 1), (210, 1), (211, 1), (212, 1), (213, 1), (220, 1), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (232, 1), (233, 3), (240, 2), (241, 1), (252, 1), (254, 1), (255, 2), (257, 3), (258, 3), (260, 5), (268, 1), (282, 1), (283, 2), (284, 1), (300, 1), (301, 4), (303, 1), (304, 1), (316, 1), (317, 1), (318, 1), (326, 1), (328, 2), (351, 1), (359, 1), (361, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 52s - loss: 734.5097 - loglik: -7.2865e+02 - logprior: -5.7151e+00
Epoch 2/2
25/25 - 48s - loss: 699.3014 - loglik: -6.9697e+02 - logprior: -1.7708e+00
Fitted a model with MAP estimate = -694.7200
expansions: [(439, 1)]
discards: [213 228 308 321 322]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 51s - loss: 708.3691 - loglik: -7.0305e+02 - logprior: -5.1811e+00
Epoch 2/2
25/25 - 48s - loss: 698.1537 - loglik: -6.9652e+02 - logprior: -1.0065e+00
Fitted a model with MAP estimate = -692.0238
expansions: []
discards: [187 310 435]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 51s - loss: 708.0266 - loglik: -7.0377e+02 - logprior: -4.1280e+00
Epoch 2/10
25/25 - 47s - loss: 697.5789 - loglik: -6.9675e+02 - logprior: -2.6484e-01
Epoch 3/10
25/25 - 47s - loss: 692.5476 - loglik: -6.9175e+02 - logprior: 0.1372
Epoch 4/10
25/25 - 47s - loss: 687.2711 - loglik: -6.8652e+02 - logprior: 0.3296
Epoch 5/10
25/25 - 47s - loss: 688.0552 - loglik: -6.8755e+02 - logprior: 0.5462
Fitted a model with MAP estimate = -684.6467
Time for alignment: 876.7466
Computed alignments with likelihoods: ['-686.7122', '-685.7717', '-684.6467']
Best model has likelihood: -684.6467
time for generating output: 0.5383
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9149433555115987
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac961f160>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6944e5cbe0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706c1760>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f61706d7310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac867feb0>, <__main__.SimpleDirichletPrior object at 0x7f6aca3e3a00>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 326.1357 - loglik: -3.2141e+02 - logprior: -4.7217e+00
Epoch 2/10
16/16 - 5s - loss: 253.8803 - loglik: -2.5218e+02 - logprior: -1.6901e+00
Epoch 3/10
16/16 - 4s - loss: 220.8015 - loglik: -2.1850e+02 - logprior: -2.1050e+00
Epoch 4/10
16/16 - 5s - loss: 208.1160 - loglik: -2.0542e+02 - logprior: -2.1346e+00
Epoch 5/10
16/16 - 5s - loss: 200.5474 - loglik: -1.9791e+02 - logprior: -2.0550e+00
Epoch 6/10
16/16 - 5s - loss: 199.3192 - loglik: -1.9674e+02 - logprior: -2.0599e+00
Epoch 7/10
16/16 - 5s - loss: 197.5191 - loglik: -1.9495e+02 - logprior: -2.0579e+00
Epoch 8/10
16/16 - 5s - loss: 197.6942 - loglik: -1.9512e+02 - logprior: -2.0637e+00
Fitted a model with MAP estimate = -196.0296
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (69, 1), (94, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 217.6067 - loglik: -2.1155e+02 - logprior: -5.9881e+00
Epoch 2/2
16/16 - 5s - loss: 200.6264 - loglik: -1.9729e+02 - logprior: -3.0493e+00
Fitted a model with MAP estimate = -197.7448
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 198.3104 - loglik: -1.9489e+02 - logprior: -3.3361e+00
Epoch 2/2
33/33 - 7s - loss: 193.9431 - loglik: -1.9194e+02 - logprior: -1.7965e+00
Fitted a model with MAP estimate = -191.6151
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 197.4466 - loglik: -1.9405e+02 - logprior: -3.3154e+00
Epoch 2/10
33/33 - 6s - loss: 193.4188 - loglik: -1.9156e+02 - logprior: -1.6567e+00
Epoch 3/10
33/33 - 7s - loss: 190.8113 - loglik: -1.8892e+02 - logprior: -1.6054e+00
Epoch 4/10
33/33 - 7s - loss: 190.2541 - loglik: -1.8844e+02 - logprior: -1.4901e+00
Epoch 5/10
33/33 - 7s - loss: 188.1983 - loglik: -1.8642e+02 - logprior: -1.4306e+00
Epoch 6/10
33/33 - 7s - loss: 188.2934 - loglik: -1.8657e+02 - logprior: -1.3569e+00
Fitted a model with MAP estimate = -187.3301
Time for alignment: 143.6011
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 326.7853 - loglik: -3.2203e+02 - logprior: -4.7514e+00
Epoch 2/10
16/16 - 5s - loss: 256.3316 - loglik: -2.5460e+02 - logprior: -1.7195e+00
Epoch 3/10
16/16 - 5s - loss: 224.9027 - loglik: -2.2258e+02 - logprior: -2.1464e+00
Epoch 4/10
16/16 - 5s - loss: 208.6780 - loglik: -2.0594e+02 - logprior: -2.2036e+00
Epoch 5/10
16/16 - 5s - loss: 203.5483 - loglik: -2.0081e+02 - logprior: -2.1062e+00
Epoch 6/10
16/16 - 5s - loss: 200.6658 - loglik: -1.9801e+02 - logprior: -2.1219e+00
Epoch 7/10
16/16 - 5s - loss: 198.4967 - loglik: -1.9589e+02 - logprior: -2.1070e+00
Epoch 8/10
16/16 - 5s - loss: 198.2846 - loglik: -1.9569e+02 - logprior: -2.1177e+00
Epoch 9/10
16/16 - 5s - loss: 197.8349 - loglik: -1.9521e+02 - logprior: -2.1231e+00
Epoch 10/10
16/16 - 5s - loss: 198.1120 - loglik: -1.9549e+02 - logprior: -2.1348e+00
Fitted a model with MAP estimate = -196.5339
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (21, 1), (70, 1), (73, 1), (96, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 211.9552 - loglik: -2.0729e+02 - logprior: -4.4656e+00
Epoch 2/2
33/33 - 7s - loss: 196.2015 - loglik: -1.9331e+02 - logprior: -2.5894e+00
Fitted a model with MAP estimate = -193.0620
expansions: [(0, 2), (19, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 199.3401 - loglik: -1.9594e+02 - logprior: -3.3075e+00
Epoch 2/2
33/33 - 6s - loss: 192.5051 - loglik: -1.9052e+02 - logprior: -1.7789e+00
Fitted a model with MAP estimate = -191.6289
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 11s - loss: 196.4973 - loglik: -1.9310e+02 - logprior: -3.3113e+00
Epoch 2/10
33/33 - 7s - loss: 192.6196 - loglik: -1.9076e+02 - logprior: -1.6533e+00
Epoch 3/10
33/33 - 6s - loss: 191.7473 - loglik: -1.8992e+02 - logprior: -1.5551e+00
Epoch 4/10
33/33 - 7s - loss: 189.7772 - loglik: -1.8800e+02 - logprior: -1.4555e+00
Epoch 5/10
33/33 - 7s - loss: 187.7080 - loglik: -1.8598e+02 - logprior: -1.3864e+00
Epoch 6/10
33/33 - 7s - loss: 187.3157 - loglik: -1.8565e+02 - logprior: -1.3016e+00
Epoch 7/10
33/33 - 7s - loss: 187.3410 - loglik: -1.8572e+02 - logprior: -1.2407e+00
Fitted a model with MAP estimate = -186.6118
Time for alignment: 168.1048
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 327.1594 - loglik: -3.2242e+02 - logprior: -4.7338e+00
Epoch 2/10
16/16 - 4s - loss: 255.8709 - loglik: -2.5415e+02 - logprior: -1.7096e+00
Epoch 3/10
16/16 - 5s - loss: 223.5434 - loglik: -2.2129e+02 - logprior: -2.1475e+00
Epoch 4/10
16/16 - 5s - loss: 208.7636 - loglik: -2.0623e+02 - logprior: -2.2097e+00
Epoch 5/10
16/16 - 5s - loss: 203.4981 - loglik: -2.0091e+02 - logprior: -2.1445e+00
Epoch 6/10
16/16 - 5s - loss: 202.3399 - loglik: -1.9978e+02 - logprior: -2.1451e+00
Epoch 7/10
16/16 - 5s - loss: 198.0132 - loglik: -1.9546e+02 - logprior: -2.1441e+00
Epoch 8/10
16/16 - 5s - loss: 200.4726 - loglik: -1.9793e+02 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -198.4780
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (72, 1), (75, 1), (97, 1), (98, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 210.6962 - loglik: -2.0604e+02 - logprior: -4.4853e+00
Epoch 2/2
33/33 - 6s - loss: 196.9664 - loglik: -1.9415e+02 - logprior: -2.5836e+00
Fitted a model with MAP estimate = -193.6243
expansions: [(0, 1), (19, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 197.9783 - loglik: -1.9445e+02 - logprior: -3.4415e+00
Epoch 2/2
33/33 - 7s - loss: 193.4996 - loglik: -1.9150e+02 - logprior: -1.7934e+00
Fitted a model with MAP estimate = -191.5008
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 196.8950 - loglik: -1.9352e+02 - logprior: -3.2999e+00
Epoch 2/10
33/33 - 7s - loss: 193.3596 - loglik: -1.9154e+02 - logprior: -1.6306e+00
Epoch 3/10
33/33 - 7s - loss: 192.0629 - loglik: -1.9024e+02 - logprior: -1.5487e+00
Epoch 4/10
33/33 - 7s - loss: 188.7324 - loglik: -1.8694e+02 - logprior: -1.4518e+00
Epoch 5/10
33/33 - 7s - loss: 187.5905 - loglik: -1.8585e+02 - logprior: -1.3784e+00
Epoch 6/10
33/33 - 7s - loss: 188.5730 - loglik: -1.8690e+02 - logprior: -1.2977e+00
Fitted a model with MAP estimate = -186.9375
Time for alignment: 149.1635
Computed alignments with likelihoods: ['-187.3301', '-186.6118', '-186.9375']
Best model has likelihood: -186.6118
time for generating output: 0.3455
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5299861040005851
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f673ceb6a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ae3f7da90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c1f9bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6983310490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6942af7e50>, <__main__.SimpleDirichletPrior object at 0x7f67c60de9a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.7971 - loglik: -3.0543e+02 - logprior: -3.2613e+00
Epoch 2/10
19/19 - 3s - loss: 276.6405 - loglik: -2.7469e+02 - logprior: -1.2863e+00
Epoch 3/10
19/19 - 3s - loss: 260.8850 - loglik: -2.5848e+02 - logprior: -1.6911e+00
Epoch 4/10
19/19 - 3s - loss: 257.7570 - loglik: -2.5536e+02 - logprior: -1.6747e+00
Epoch 5/10
19/19 - 3s - loss: 255.7789 - loglik: -2.5347e+02 - logprior: -1.6048e+00
Epoch 6/10
19/19 - 3s - loss: 255.2238 - loglik: -2.5300e+02 - logprior: -1.5771e+00
Epoch 7/10
19/19 - 3s - loss: 254.5672 - loglik: -2.5242e+02 - logprior: -1.5510e+00
Epoch 8/10
19/19 - 3s - loss: 254.4099 - loglik: -2.5231e+02 - logprior: -1.5376e+00
Epoch 9/10
19/19 - 3s - loss: 253.4182 - loglik: -2.5136e+02 - logprior: -1.5314e+00
Epoch 10/10
19/19 - 3s - loss: 253.8197 - loglik: -2.5180e+02 - logprior: -1.5234e+00
Fitted a model with MAP estimate = -241.0846
expansions: [(6, 3), (7, 2), (10, 2), (39, 8), (58, 2), (60, 1), (63, 2), (66, 1), (68, 3), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 271.2141 - loglik: -2.6695e+02 - logprior: -4.1657e+00
Epoch 2/2
19/19 - 3s - loss: 254.5549 - loglik: -2.5168e+02 - logprior: -2.3773e+00
Fitted a model with MAP estimate = -235.7384
expansions: [(0, 2)]
discards: [ 0  8 14 50 51 52 73 81 89]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 254.7272 - loglik: -2.5159e+02 - logprior: -3.0589e+00
Epoch 2/2
19/19 - 3s - loss: 249.9523 - loglik: -2.4826e+02 - logprior: -1.2760e+00
Fitted a model with MAP estimate = -233.3148
expansions: [(39, 10)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 238.2755 - loglik: -2.3541e+02 - logprior: -2.7571e+00
Epoch 2/10
23/23 - 4s - loss: 232.4569 - loglik: -2.3068e+02 - logprior: -1.3300e+00
Epoch 3/10
23/23 - 4s - loss: 228.7292 - loglik: -2.2675e+02 - logprior: -1.2805e+00
Epoch 4/10
23/23 - 4s - loss: 227.8618 - loglik: -2.2567e+02 - logprior: -1.2652e+00
Epoch 5/10
23/23 - 4s - loss: 226.7965 - loglik: -2.2462e+02 - logprior: -1.2520e+00
Epoch 6/10
23/23 - 4s - loss: 226.2874 - loglik: -2.2418e+02 - logprior: -1.2514e+00
Epoch 7/10
23/23 - 4s - loss: 225.7595 - loglik: -2.2375e+02 - logprior: -1.2376e+00
Epoch 8/10
23/23 - 4s - loss: 225.1685 - loglik: -2.2321e+02 - logprior: -1.2348e+00
Epoch 9/10
23/23 - 4s - loss: 224.8832 - loglik: -2.2300e+02 - logprior: -1.2238e+00
Epoch 10/10
23/23 - 4s - loss: 225.0093 - loglik: -2.2319e+02 - logprior: -1.2075e+00
Fitted a model with MAP estimate = -223.8218
Time for alignment: 119.8448
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 308.5111 - loglik: -3.0515e+02 - logprior: -3.2590e+00
Epoch 2/10
19/19 - 3s - loss: 276.6080 - loglik: -2.7467e+02 - logprior: -1.2982e+00
Epoch 3/10
19/19 - 3s - loss: 260.7379 - loglik: -2.5824e+02 - logprior: -1.7274e+00
Epoch 4/10
19/19 - 3s - loss: 256.8050 - loglik: -2.5435e+02 - logprior: -1.7110e+00
Epoch 5/10
19/19 - 3s - loss: 255.3579 - loglik: -2.5299e+02 - logprior: -1.6518e+00
Epoch 6/10
19/19 - 3s - loss: 254.4765 - loglik: -2.5214e+02 - logprior: -1.6285e+00
Epoch 7/10
19/19 - 3s - loss: 253.9943 - loglik: -2.5174e+02 - logprior: -1.6036e+00
Epoch 8/10
19/19 - 3s - loss: 253.5996 - loglik: -2.5141e+02 - logprior: -1.5895e+00
Epoch 9/10
19/19 - 3s - loss: 253.2225 - loglik: -2.5106e+02 - logprior: -1.5871e+00
Epoch 10/10
19/19 - 3s - loss: 252.6470 - loglik: -2.5052e+02 - logprior: -1.5942e+00
Fitted a model with MAP estimate = -240.8485
expansions: [(6, 3), (7, 1), (10, 2), (34, 10), (39, 2), (40, 3), (55, 2), (58, 1), (60, 1), (63, 2), (65, 2), (66, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 271.4866 - loglik: -2.6720e+02 - logprior: -4.1937e+00
Epoch 2/2
19/19 - 4s - loss: 252.5109 - loglik: -2.4960e+02 - logprior: -2.4140e+00
Fitted a model with MAP estimate = -234.1466
expansions: [(0, 2)]
discards: [ 0 13 45 46 47 56 76 88 92]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 253.3333 - loglik: -2.5019e+02 - logprior: -3.0671e+00
Epoch 2/2
19/19 - 3s - loss: 248.0207 - loglik: -2.4628e+02 - logprior: -1.3117e+00
Fitted a model with MAP estimate = -232.3781
expansions: [(45, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 236.7896 - loglik: -2.3394e+02 - logprior: -2.7390e+00
Epoch 2/10
23/23 - 4s - loss: 231.1938 - loglik: -2.2945e+02 - logprior: -1.2832e+00
Epoch 3/10
23/23 - 4s - loss: 229.6865 - loglik: -2.2765e+02 - logprior: -1.2724e+00
Epoch 4/10
23/23 - 4s - loss: 227.4165 - loglik: -2.2523e+02 - logprior: -1.2523e+00
Epoch 5/10
23/23 - 4s - loss: 227.2102 - loglik: -2.2504e+02 - logprior: -1.2482e+00
Epoch 6/10
23/23 - 4s - loss: 226.5011 - loglik: -2.2442e+02 - logprior: -1.2369e+00
Epoch 7/10
23/23 - 4s - loss: 225.7916 - loglik: -2.2379e+02 - logprior: -1.2214e+00
Epoch 8/10
23/23 - 4s - loss: 225.4197 - loglik: -2.2348e+02 - logprior: -1.2125e+00
Epoch 9/10
23/23 - 4s - loss: 224.6906 - loglik: -2.2282e+02 - logprior: -1.1953e+00
Epoch 10/10
23/23 - 4s - loss: 224.9281 - loglik: -2.2313e+02 - logprior: -1.1780e+00
Fitted a model with MAP estimate = -223.8642
Time for alignment: 122.6316
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.5376 - loglik: -3.0518e+02 - logprior: -3.2605e+00
Epoch 2/10
19/19 - 3s - loss: 276.6859 - loglik: -2.7472e+02 - logprior: -1.2932e+00
Epoch 3/10
19/19 - 3s - loss: 260.7010 - loglik: -2.5794e+02 - logprior: -1.6842e+00
Epoch 4/10
19/19 - 3s - loss: 256.6930 - loglik: -2.5415e+02 - logprior: -1.6538e+00
Epoch 5/10
19/19 - 3s - loss: 255.3593 - loglik: -2.5302e+02 - logprior: -1.5996e+00
Epoch 6/10
19/19 - 3s - loss: 254.0839 - loglik: -2.5179e+02 - logprior: -1.6198e+00
Epoch 7/10
19/19 - 3s - loss: 253.2518 - loglik: -2.5104e+02 - logprior: -1.6202e+00
Epoch 8/10
19/19 - 3s - loss: 252.9626 - loglik: -2.5079e+02 - logprior: -1.6091e+00
Epoch 9/10
19/19 - 3s - loss: 252.6979 - loglik: -2.5054e+02 - logprior: -1.6051e+00
Epoch 10/10
19/19 - 3s - loss: 252.5059 - loglik: -2.5039e+02 - logprior: -1.6067e+00
Fitted a model with MAP estimate = -240.4799
expansions: [(6, 3), (7, 1), (10, 2), (21, 2), (33, 10), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 269.2934 - loglik: -2.6501e+02 - logprior: -4.1910e+00
Epoch 2/2
19/19 - 4s - loss: 252.2086 - loglik: -2.4931e+02 - logprior: -2.4143e+00
Fitted a model with MAP estimate = -233.7111
expansions: [(0, 2)]
discards: [ 0 13 27 45 46 47 48 56 81 87 89]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 253.6811 - loglik: -2.5053e+02 - logprior: -3.0706e+00
Epoch 2/2
19/19 - 3s - loss: 248.3976 - loglik: -2.4666e+02 - logprior: -1.3101e+00
Fitted a model with MAP estimate = -232.4537
expansions: [(43, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 237.3758 - loglik: -2.3451e+02 - logprior: -2.7493e+00
Epoch 2/10
23/23 - 4s - loss: 231.4257 - loglik: -2.2962e+02 - logprior: -1.2980e+00
Epoch 3/10
23/23 - 4s - loss: 229.2116 - loglik: -2.2712e+02 - logprior: -1.2762e+00
Epoch 4/10
23/23 - 4s - loss: 227.6503 - loglik: -2.2543e+02 - logprior: -1.2614e+00
Epoch 5/10
23/23 - 4s - loss: 227.0988 - loglik: -2.2491e+02 - logprior: -1.2573e+00
Epoch 6/10
23/23 - 4s - loss: 225.9212 - loglik: -2.2384e+02 - logprior: -1.2362e+00
Epoch 7/10
23/23 - 4s - loss: 225.4924 - loglik: -2.2349e+02 - logprior: -1.2239e+00
Epoch 8/10
23/23 - 4s - loss: 225.0826 - loglik: -2.2314e+02 - logprior: -1.2246e+00
Epoch 9/10
23/23 - 4s - loss: 224.6277 - loglik: -2.2277e+02 - logprior: -1.2033e+00
Epoch 10/10
23/23 - 4s - loss: 224.7933 - loglik: -2.2298e+02 - logprior: -1.1866e+00
Fitted a model with MAP estimate = -223.7493
Time for alignment: 119.8604
Computed alignments with likelihoods: ['-223.8218', '-223.8642', '-223.7493']
Best model has likelihood: -223.7493
time for generating output: 0.2204
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.8085683297180043
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69426cde80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6160584ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb2426a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f694e093f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6942a94f40>, <__main__.SimpleDirichletPrior object at 0x7f69721f8040>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 434.0790 - loglik: -3.8674e+02 - logprior: -4.7313e+01
Epoch 2/10
10/10 - 2s - loss: 371.9359 - loglik: -3.5969e+02 - logprior: -1.2115e+01
Epoch 3/10
10/10 - 2s - loss: 335.5126 - loglik: -3.2948e+02 - logprior: -5.9182e+00
Epoch 4/10
10/10 - 2s - loss: 312.9373 - loglik: -3.0883e+02 - logprior: -4.0257e+00
Epoch 5/10
10/10 - 2s - loss: 303.1527 - loglik: -2.9966e+02 - logprior: -3.2972e+00
Epoch 6/10
10/10 - 2s - loss: 301.6606 - loglik: -2.9847e+02 - logprior: -2.8434e+00
Epoch 7/10
10/10 - 2s - loss: 298.7485 - loglik: -2.9592e+02 - logprior: -2.4673e+00
Epoch 8/10
10/10 - 2s - loss: 297.2318 - loglik: -2.9456e+02 - logprior: -2.2951e+00
Epoch 9/10
10/10 - 2s - loss: 296.4467 - loglik: -2.9385e+02 - logprior: -2.1874e+00
Epoch 10/10
10/10 - 2s - loss: 296.3266 - loglik: -2.9386e+02 - logprior: -2.0553e+00
Fitted a model with MAP estimate = -295.6636
expansions: [(10, 4), (17, 1), (18, 1), (19, 1), (26, 1), (42, 1), (58, 2), (59, 2), (61, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 350.8366 - loglik: -2.9759e+02 - logprior: -5.3226e+01
Epoch 2/2
10/10 - 3s - loss: 308.8351 - loglik: -2.8735e+02 - logprior: -2.1348e+01
Fitted a model with MAP estimate = -301.2141
expansions: [(10, 1), (34, 3)]
discards: [ 0 67]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 339.0851 - loglik: -2.8707e+02 - logprior: -5.1990e+01
Epoch 2/2
10/10 - 3s - loss: 302.2214 - loglik: -2.8244e+02 - logprior: -1.9659e+01
Fitted a model with MAP estimate = -295.2109
expansions: [(2, 2), (11, 1)]
discards: [ 0 34]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 335.5795 - loglik: -2.8367e+02 - logprior: -5.1883e+01
Epoch 2/10
10/10 - 3s - loss: 300.3366 - loglik: -2.8004e+02 - logprior: -2.0179e+01
Epoch 3/10
10/10 - 3s - loss: 290.0705 - loglik: -2.7791e+02 - logprior: -1.1978e+01
Epoch 4/10
10/10 - 3s - loss: 281.5052 - loglik: -2.7721e+02 - logprior: -4.0481e+00
Epoch 5/10
10/10 - 3s - loss: 277.1408 - loglik: -2.7688e+02 - logprior: 0.0644
Epoch 6/10
10/10 - 3s - loss: 274.9550 - loglik: -2.7586e+02 - logprior: 1.2274
Epoch 7/10
10/10 - 3s - loss: 275.0506 - loglik: -2.7654e+02 - logprior: 1.7806
Fitted a model with MAP estimate = -273.9907
Time for alignment: 76.8465
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 434.2201 - loglik: -3.8688e+02 - logprior: -4.7313e+01
Epoch 2/10
10/10 - 2s - loss: 372.1964 - loglik: -3.5995e+02 - logprior: -1.2116e+01
Epoch 3/10
10/10 - 2s - loss: 335.3173 - loglik: -3.2926e+02 - logprior: -5.9401e+00
Epoch 4/10
10/10 - 2s - loss: 313.3857 - loglik: -3.0929e+02 - logprior: -3.9788e+00
Epoch 5/10
10/10 - 2s - loss: 302.9430 - loglik: -2.9939e+02 - logprior: -3.2336e+00
Epoch 6/10
10/10 - 2s - loss: 299.7469 - loglik: -2.9646e+02 - logprior: -2.7913e+00
Epoch 7/10
10/10 - 2s - loss: 297.5994 - loglik: -2.9480e+02 - logprior: -2.3530e+00
Epoch 8/10
10/10 - 2s - loss: 297.5949 - loglik: -2.9512e+02 - logprior: -2.0819e+00
Epoch 9/10
10/10 - 2s - loss: 296.5497 - loglik: -2.9419e+02 - logprior: -1.9754e+00
Epoch 10/10
10/10 - 2s - loss: 296.2520 - loglik: -2.9400e+02 - logprior: -1.8741e+00
Fitted a model with MAP estimate = -295.5544
expansions: [(10, 4), (17, 1), (18, 1), (29, 3), (42, 2), (49, 2), (58, 2), (59, 2), (61, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 351.3695 - loglik: -2.9812e+02 - logprior: -5.3225e+01
Epoch 2/2
10/10 - 3s - loss: 308.6237 - loglik: -2.8695e+02 - logprior: -2.1543e+01
Fitted a model with MAP estimate = -300.6864
expansions: [(11, 1)]
discards: [59 71]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 334.5534 - loglik: -2.8382e+02 - logprior: -5.0713e+01
Epoch 2/2
10/10 - 3s - loss: 296.4320 - loglik: -2.8101e+02 - logprior: -1.5296e+01
Fitted a model with MAP estimate = -286.8345
expansions: [(10, 1)]
discards: [36 52]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 323.6308 - loglik: -2.8081e+02 - logprior: -4.2801e+01
Epoch 2/10
10/10 - 3s - loss: 289.1832 - loglik: -2.7901e+02 - logprior: -1.0062e+01
Epoch 3/10
10/10 - 3s - loss: 282.2237 - loglik: -2.7859e+02 - logprior: -3.4561e+00
Epoch 4/10
10/10 - 3s - loss: 278.4769 - loglik: -2.7733e+02 - logprior: -9.0854e-01
Epoch 5/10
10/10 - 3s - loss: 276.4575 - loglik: -2.7658e+02 - logprior: 0.4235
Epoch 6/10
10/10 - 3s - loss: 275.6989 - loglik: -2.7650e+02 - logprior: 1.1123
Epoch 7/10
10/10 - 3s - loss: 274.8129 - loglik: -2.7612e+02 - logprior: 1.5936
Epoch 8/10
10/10 - 3s - loss: 274.8199 - loglik: -2.7659e+02 - logprior: 2.0313
Fitted a model with MAP estimate = -274.0132
Time for alignment: 78.8751
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 434.2188 - loglik: -3.8688e+02 - logprior: -4.7312e+01
Epoch 2/10
10/10 - 2s - loss: 372.4265 - loglik: -3.6019e+02 - logprior: -1.2109e+01
Epoch 3/10
10/10 - 2s - loss: 335.8923 - loglik: -3.2984e+02 - logprior: -5.9286e+00
Epoch 4/10
10/10 - 2s - loss: 314.1664 - loglik: -3.1008e+02 - logprior: -3.9629e+00
Epoch 5/10
10/10 - 2s - loss: 304.1884 - loglik: -3.0070e+02 - logprior: -3.1434e+00
Epoch 6/10
10/10 - 2s - loss: 300.8305 - loglik: -2.9759e+02 - logprior: -2.7934e+00
Epoch 7/10
10/10 - 2s - loss: 298.1262 - loglik: -2.9526e+02 - logprior: -2.5234e+00
Epoch 8/10
10/10 - 2s - loss: 297.1911 - loglik: -2.9466e+02 - logprior: -2.2735e+00
Epoch 9/10
10/10 - 2s - loss: 295.8175 - loglik: -2.9335e+02 - logprior: -2.1880e+00
Epoch 10/10
10/10 - 2s - loss: 296.2224 - loglik: -2.9380e+02 - logprior: -2.1004e+00
Fitted a model with MAP estimate = -295.3766
expansions: [(10, 4), (17, 1), (20, 1), (27, 1), (28, 3), (42, 2), (48, 2), (59, 3), (62, 1), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 351.6562 - loglik: -2.9841e+02 - logprior: -5.3224e+01
Epoch 2/2
10/10 - 3s - loss: 306.7562 - loglik: -2.8523e+02 - logprior: -2.1379e+01
Fitted a model with MAP estimate = -299.3500
expansions: [(10, 2)]
discards: [34 60 73]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 334.0463 - loglik: -2.8344e+02 - logprior: -5.0585e+01
Epoch 2/2
10/10 - 3s - loss: 294.7651 - loglik: -2.7940e+02 - logprior: -1.5246e+01
Fitted a model with MAP estimate = -285.9805
expansions: []
discards: [53]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 321.7943 - loglik: -2.7975e+02 - logprior: -4.2024e+01
Epoch 2/10
10/10 - 3s - loss: 287.7565 - loglik: -2.7777e+02 - logprior: -9.8746e+00
Epoch 3/10
10/10 - 3s - loss: 280.8864 - loglik: -2.7733e+02 - logprior: -3.3784e+00
Epoch 4/10
10/10 - 3s - loss: 277.4681 - loglik: -2.7637e+02 - logprior: -8.4483e-01
Epoch 5/10
10/10 - 3s - loss: 276.0800 - loglik: -2.7623e+02 - logprior: 0.4903
Epoch 6/10
10/10 - 3s - loss: 274.9740 - loglik: -2.7582e+02 - logprior: 1.1938
Epoch 7/10
10/10 - 3s - loss: 274.1939 - loglik: -2.7556e+02 - logprior: 1.6677
Epoch 8/10
10/10 - 3s - loss: 273.7529 - loglik: -2.7557e+02 - logprior: 2.0969
Epoch 9/10
10/10 - 3s - loss: 273.3026 - loglik: -2.7548e+02 - logprior: 2.4464
Epoch 10/10
10/10 - 3s - loss: 273.0376 - loglik: -2.7547e+02 - logprior: 2.7072
Fitted a model with MAP estimate = -272.5400
Time for alignment: 87.4745
Computed alignments with likelihoods: ['-273.9907', '-274.0132', '-272.5400']
Best model has likelihood: -272.5400
time for generating output: 0.2020
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.888631090487239
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69578eab80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ac83425b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f618026ffa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f618026fcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f69b5865550>, <__main__.SimpleDirichletPrior object at 0x7f6ab628df40>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 458.2392 - loglik: -1.8612e+02 - logprior: -2.7211e+02
Epoch 2/10
10/10 - 1s - loss: 235.5634 - loglik: -1.6043e+02 - logprior: -7.5131e+01
Epoch 3/10
10/10 - 1s - loss: 175.8609 - loglik: -1.3991e+02 - logprior: -3.5953e+01
Epoch 4/10
10/10 - 1s - loss: 147.9326 - loglik: -1.2604e+02 - logprior: -2.1895e+01
Epoch 5/10
10/10 - 1s - loss: 134.3443 - loglik: -1.2015e+02 - logprior: -1.4196e+01
Epoch 6/10
10/10 - 1s - loss: 126.7091 - loglik: -1.1756e+02 - logprior: -9.1035e+00
Epoch 7/10
10/10 - 1s - loss: 121.5931 - loglik: -1.1540e+02 - logprior: -5.9640e+00
Epoch 8/10
10/10 - 1s - loss: 118.7735 - loglik: -1.1463e+02 - logprior: -3.8556e+00
Epoch 9/10
10/10 - 1s - loss: 117.1656 - loglik: -1.1463e+02 - logprior: -2.2781e+00
Epoch 10/10
10/10 - 1s - loss: 116.0883 - loglik: -1.1478e+02 - logprior: -1.0880e+00
Fitted a model with MAP estimate = -115.3967
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 471.7923 - loglik: -1.0995e+02 - logprior: -3.6182e+02
Epoch 2/2
10/10 - 1s - loss: 210.8813 - loglik: -9.7366e+01 - logprior: -1.1346e+02
Fitted a model with MAP estimate = -162.3650
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 404.3051 - loglik: -9.7353e+01 - logprior: -3.0693e+02
Epoch 2/2
10/10 - 1s - loss: 216.6187 - loglik: -9.5776e+01 - logprior: -1.2082e+02
Fitted a model with MAP estimate = -186.7585
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.1749 - loglik: -9.4766e+01 - logprior: -2.8139e+02
Epoch 2/10
10/10 - 1s - loss: 173.7921 - loglik: -9.4158e+01 - logprior: -7.9595e+01
Epoch 3/10
10/10 - 1s - loss: 123.0889 - loglik: -9.4408e+01 - logprior: -2.8678e+01
Epoch 4/10
10/10 - 1s - loss: 104.9799 - loglik: -9.4921e+01 - logprior: -1.0058e+01
Epoch 5/10
10/10 - 1s - loss: 95.8576 - loglik: -9.5393e+01 - logprior: -4.5821e-01
Epoch 6/10
10/10 - 1s - loss: 90.5170 - loglik: -9.5565e+01 - logprior: 5.0756
Epoch 7/10
10/10 - 1s - loss: 86.9501 - loglik: -9.5214e+01 - logprior: 8.4352
Epoch 8/10
10/10 - 1s - loss: 84.4752 - loglik: -9.4957e+01 - logprior: 10.8050
Epoch 9/10
10/10 - 1s - loss: 82.5620 - loglik: -9.4986e+01 - logprior: 12.7028
Epoch 10/10
10/10 - 1s - loss: 80.9944 - loglik: -9.5055e+01 - logprior: 14.2858
Fitted a model with MAP estimate = -80.0200
Time for alignment: 35.1758
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 458.2396 - loglik: -1.8612e+02 - logprior: -2.7211e+02
Epoch 2/10
10/10 - 1s - loss: 235.5634 - loglik: -1.6043e+02 - logprior: -7.5131e+01
Epoch 3/10
10/10 - 1s - loss: 175.8606 - loglik: -1.3991e+02 - logprior: -3.5953e+01
Epoch 4/10
10/10 - 1s - loss: 147.9327 - loglik: -1.2604e+02 - logprior: -2.1895e+01
Epoch 5/10
10/10 - 1s - loss: 134.3565 - loglik: -1.2017e+02 - logprior: -1.4187e+01
Epoch 6/10
10/10 - 1s - loss: 126.9348 - loglik: -1.1796e+02 - logprior: -8.9565e+00
Epoch 7/10
10/10 - 1s - loss: 121.6930 - loglik: -1.1565e+02 - logprior: -5.8312e+00
Epoch 8/10
10/10 - 1s - loss: 118.8508 - loglik: -1.1468e+02 - logprior: -3.7757e+00
Epoch 9/10
10/10 - 1s - loss: 117.2373 - loglik: -1.1471e+02 - logprior: -2.2080e+00
Epoch 10/10
10/10 - 1s - loss: 116.1496 - loglik: -1.1485e+02 - logprior: -1.0222e+00
Fitted a model with MAP estimate = -115.3843
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 471.7199 - loglik: -1.0993e+02 - logprior: -3.6177e+02
Epoch 2/2
10/10 - 1s - loss: 210.8720 - loglik: -9.7353e+01 - logprior: -1.1346e+02
Fitted a model with MAP estimate = -162.3421
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.2945 - loglik: -9.7342e+01 - logprior: -3.0693e+02
Epoch 2/2
10/10 - 1s - loss: 216.6095 - loglik: -9.5766e+01 - logprior: -1.2082e+02
Fitted a model with MAP estimate = -186.7549
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.1693 - loglik: -9.4762e+01 - logprior: -2.8139e+02
Epoch 2/10
10/10 - 1s - loss: 173.7875 - loglik: -9.4159e+01 - logprior: -7.9590e+01
Epoch 3/10
10/10 - 1s - loss: 123.0857 - loglik: -9.4409e+01 - logprior: -2.8673e+01
Epoch 4/10
10/10 - 1s - loss: 104.9763 - loglik: -9.4922e+01 - logprior: -1.0054e+01
Epoch 5/10
10/10 - 1s - loss: 95.8560 - loglik: -9.5398e+01 - logprior: -4.5211e-01
Epoch 6/10
10/10 - 1s - loss: 90.5402 - loglik: -9.5637e+01 - logprior: 5.1119
Epoch 7/10
10/10 - 1s - loss: 86.9104 - loglik: -9.5231e+01 - logprior: 8.4389
Epoch 8/10
10/10 - 1s - loss: 84.3921 - loglik: -9.4866e+01 - logprior: 10.7529
Epoch 9/10
10/10 - 1s - loss: 82.5277 - loglik: -9.4949e+01 - logprior: 12.6772
Epoch 10/10
10/10 - 1s - loss: 80.9899 - loglik: -9.5097e+01 - logprior: 14.3077
Fitted a model with MAP estimate = -80.0393
Time for alignment: 35.1676
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 458.2392 - loglik: -1.8612e+02 - logprior: -2.7211e+02
Epoch 2/10
10/10 - 1s - loss: 235.5636 - loglik: -1.6043e+02 - logprior: -7.5132e+01
Epoch 3/10
10/10 - 1s - loss: 175.8611 - loglik: -1.3991e+02 - logprior: -3.5954e+01
Epoch 4/10
10/10 - 1s - loss: 147.9328 - loglik: -1.2604e+02 - logprior: -2.1895e+01
Epoch 5/10
10/10 - 1s - loss: 134.3529 - loglik: -1.2016e+02 - logprior: -1.4190e+01
Epoch 6/10
10/10 - 1s - loss: 126.7868 - loglik: -1.1769e+02 - logprior: -9.0418e+00
Epoch 7/10
10/10 - 1s - loss: 121.6480 - loglik: -1.1546e+02 - logprior: -5.8882e+00
Epoch 8/10
10/10 - 1s - loss: 118.8330 - loglik: -1.1465e+02 - logprior: -3.7963e+00
Epoch 9/10
10/10 - 1s - loss: 117.2286 - loglik: -1.1472e+02 - logprior: -2.2061e+00
Epoch 10/10
10/10 - 1s - loss: 116.1582 - loglik: -1.1487e+02 - logprior: -1.0046e+00
Fitted a model with MAP estimate = -115.3828
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 471.7209 - loglik: -1.0995e+02 - logprior: -3.6175e+02
Epoch 2/2
10/10 - 1s - loss: 210.8927 - loglik: -9.7374e+01 - logprior: -1.1346e+02
Fitted a model with MAP estimate = -162.3600
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.2932 - loglik: -9.7337e+01 - logprior: -3.0694e+02
Epoch 2/2
10/10 - 1s - loss: 216.6095 - loglik: -9.5766e+01 - logprior: -1.2082e+02
Fitted a model with MAP estimate = -186.7547
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 376.1686 - loglik: -9.4760e+01 - logprior: -2.8139e+02
Epoch 2/10
10/10 - 1s - loss: 173.7870 - loglik: -9.4156e+01 - logprior: -7.9592e+01
Epoch 3/10
10/10 - 1s - loss: 123.0847 - loglik: -9.4411e+01 - logprior: -2.8671e+01
Epoch 4/10
10/10 - 1s - loss: 104.9744 - loglik: -9.4921e+01 - logprior: -1.0052e+01
Epoch 5/10
10/10 - 1s - loss: 95.8504 - loglik: -9.5391e+01 - logprior: -4.5253e-01
Epoch 6/10
10/10 - 1s - loss: 90.4802 - loglik: -9.5509e+01 - logprior: 5.0637
Epoch 7/10
10/10 - 1s - loss: 86.8908 - loglik: -9.5097e+01 - logprior: 8.4147
Epoch 8/10
10/10 - 1s - loss: 84.4258 - loglik: -9.4902e+01 - logprior: 10.7963
Epoch 9/10
10/10 - 1s - loss: 82.5220 - loglik: -9.4982e+01 - logprior: 12.7063
Epoch 10/10
10/10 - 1s - loss: 80.9817 - loglik: -9.5067e+01 - logprior: 14.3027
Fitted a model with MAP estimate = -80.0114
Time for alignment: 36.5776
Computed alignments with likelihoods: ['-80.0200', '-80.0393', '-80.0114']
Best model has likelihood: -80.0114
time for generating output: 0.1433
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8474320241691843
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f61802db2b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6adb5155e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac88f6820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69e9533a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f694d779b80>, <__main__.SimpleDirichletPrior object at 0x7f6943963d30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 690.4960 - loglik: -6.8565e+02 - logprior: -4.7390e+00
Epoch 2/10
26/26 - 12s - loss: 583.3021 - loglik: -5.8114e+02 - logprior: -2.0189e+00
Epoch 3/10
26/26 - 12s - loss: 568.2698 - loglik: -5.6552e+02 - logprior: -2.1638e+00
Epoch 4/10
26/26 - 12s - loss: 564.9069 - loglik: -5.6192e+02 - logprior: -2.2745e+00
Epoch 5/10
26/26 - 12s - loss: 561.4451 - loglik: -5.5835e+02 - logprior: -2.3707e+00
Epoch 6/10
26/26 - 12s - loss: 562.5455 - loglik: -5.5940e+02 - logprior: -2.4237e+00
Fitted a model with MAP estimate = -559.7560
expansions: [(0, 3), (11, 1), (107, 5), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 576.2566 - loglik: -5.7043e+02 - logprior: -5.7031e+00
Epoch 2/2
26/26 - 13s - loss: 563.0264 - loglik: -5.6026e+02 - logprior: -2.3661e+00
Fitted a model with MAP estimate = -560.5036
expansions: [(96, 1)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 572.2434 - loglik: -5.6465e+02 - logprior: -7.4846e+00
Epoch 2/2
26/26 - 13s - loss: 565.4365 - loglik: -5.6117e+02 - logprior: -3.8450e+00
Fitted a model with MAP estimate = -562.1356
expansions: [(0, 4), (112, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 18s - loss: 567.9152 - loglik: -5.6269e+02 - logprior: -5.1178e+00
Epoch 2/10
26/26 - 13s - loss: 561.8735 - loglik: -5.5972e+02 - logprior: -1.7626e+00
Epoch 3/10
26/26 - 13s - loss: 556.9896 - loglik: -5.5464e+02 - logprior: -1.7292e+00
Epoch 4/10
26/26 - 13s - loss: 556.2740 - loglik: -5.5384e+02 - logprior: -1.7069e+00
Epoch 5/10
26/26 - 13s - loss: 555.3760 - loglik: -5.5298e+02 - logprior: -1.6523e+00
Epoch 6/10
26/26 - 13s - loss: 551.8672 - loglik: -5.4953e+02 - logprior: -1.6023e+00
Epoch 7/10
26/26 - 13s - loss: 552.8653 - loglik: -5.5061e+02 - logprior: -1.5339e+00
Fitted a model with MAP estimate = -551.3882
Time for alignment: 272.5215
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 16s - loss: 692.4227 - loglik: -6.8759e+02 - logprior: -4.7234e+00
Epoch 2/10
26/26 - 12s - loss: 593.6812 - loglik: -5.9168e+02 - logprior: -1.8953e+00
Epoch 3/10
26/26 - 12s - loss: 575.5201 - loglik: -5.7294e+02 - logprior: -2.0691e+00
Epoch 4/10
26/26 - 12s - loss: 571.8770 - loglik: -5.6915e+02 - logprior: -2.1217e+00
Epoch 5/10
26/26 - 12s - loss: 569.2071 - loglik: -5.6638e+02 - logprior: -2.2140e+00
Epoch 6/10
26/26 - 12s - loss: 568.1532 - loglik: -5.6532e+02 - logprior: -2.2413e+00
Epoch 7/10
26/26 - 12s - loss: 566.9130 - loglik: -5.6408e+02 - logprior: -2.2475e+00
Epoch 8/10
26/26 - 12s - loss: 567.4956 - loglik: -5.6465e+02 - logprior: -2.2542e+00
Fitted a model with MAP estimate = -565.7608
expansions: [(11, 1), (44, 1), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 583.3062 - loglik: -5.7761e+02 - logprior: -5.5365e+00
Epoch 2/2
26/26 - 12s - loss: 572.8902 - loglik: -5.7042e+02 - logprior: -1.9621e+00
Fitted a model with MAP estimate = -569.5005
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 16s - loss: 578.2722 - loglik: -5.7293e+02 - logprior: -5.2218e+00
Epoch 2/10
26/26 - 12s - loss: 571.4517 - loglik: -5.6934e+02 - logprior: -1.7023e+00
Epoch 3/10
26/26 - 12s - loss: 568.0808 - loglik: -5.6591e+02 - logprior: -1.5804e+00
Epoch 4/10
26/26 - 12s - loss: 567.6562 - loglik: -5.6548e+02 - logprior: -1.5505e+00
Epoch 5/10
26/26 - 12s - loss: 565.7489 - loglik: -5.6369e+02 - logprior: -1.4429e+00
Epoch 6/10
26/26 - 12s - loss: 566.3517 - loglik: -5.6441e+02 - logprior: -1.3468e+00
Fitted a model with MAP estimate = -563.8576
Time for alignment: 237.3006
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 689.9703 - loglik: -6.8512e+02 - logprior: -4.7474e+00
Epoch 2/10
26/26 - 12s - loss: 584.9135 - loglik: -5.8231e+02 - logprior: -2.3396e+00
Epoch 3/10
26/26 - 12s - loss: 570.0083 - loglik: -5.6692e+02 - logprior: -2.4632e+00
Epoch 4/10
26/26 - 12s - loss: 564.7748 - loglik: -5.6156e+02 - logprior: -2.5631e+00
Epoch 5/10
26/26 - 12s - loss: 563.7877 - loglik: -5.6050e+02 - logprior: -2.6380e+00
Epoch 6/10
26/26 - 12s - loss: 562.2142 - loglik: -5.5888e+02 - logprior: -2.6799e+00
Epoch 7/10
26/26 - 12s - loss: 560.5948 - loglik: -5.5721e+02 - logprior: -2.7139e+00
Epoch 8/10
26/26 - 12s - loss: 560.4028 - loglik: -5.5699e+02 - logprior: -2.7388e+00
Epoch 9/10
26/26 - 12s - loss: 560.4636 - loglik: -5.5705e+02 - logprior: -2.7289e+00
Fitted a model with MAP estimate = -559.1850
expansions: [(11, 1), (40, 1), (109, 4), (130, 1), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 579.4078 - loglik: -5.7339e+02 - logprior: -5.8599e+00
Epoch 2/2
26/26 - 13s - loss: 564.0510 - loglik: -5.6122e+02 - logprior: -2.3165e+00
Fitted a model with MAP estimate = -561.0521
expansions: [(113, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 569.2416 - loglik: -5.6358e+02 - logprior: -5.5338e+00
Epoch 2/2
26/26 - 13s - loss: 563.7244 - loglik: -5.6125e+02 - logprior: -2.0354e+00
Fitted a model with MAP estimate = -560.1789
expansions: [(0, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 17s - loss: 569.2325 - loglik: -5.6264e+02 - logprior: -6.4852e+00
Epoch 2/10
26/26 - 14s - loss: 562.0338 - loglik: -5.5947e+02 - logprior: -2.1825e+00
Epoch 3/10
26/26 - 14s - loss: 558.2509 - loglik: -5.5545e+02 - logprior: -2.1722e+00
Epoch 4/10
26/26 - 14s - loss: 557.1498 - loglik: -5.5429e+02 - logprior: -2.1098e+00
Epoch 5/10
26/26 - 14s - loss: 554.4669 - loglik: -5.5172e+02 - logprior: -1.9945e+00
Epoch 6/10
26/26 - 14s - loss: 553.7110 - loglik: -5.5106e+02 - logprior: -1.9179e+00
Epoch 7/10
26/26 - 14s - loss: 553.6786 - loglik: -5.5116e+02 - logprior: -1.8173e+00
Epoch 8/10
26/26 - 14s - loss: 552.2509 - loglik: -5.4984e+02 - logprior: -1.7074e+00
Epoch 9/10
26/26 - 13s - loss: 552.4332 - loglik: -5.5016e+02 - logprior: -1.5853e+00
Fitted a model with MAP estimate = -551.0055
Time for alignment: 340.1603
Computed alignments with likelihoods: ['-551.3882', '-563.8576', '-551.0055']
Best model has likelihood: -551.0055
time for generating output: 0.3701
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.6425806451612903
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6ac7b54e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f617063fd60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617063f5b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69941d41c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6b16ce0d30>, <__main__.SimpleDirichletPrior object at 0x7f697a9c9af0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 424.9017 - loglik: -3.4530e+02 - logprior: -7.9572e+01
Epoch 2/10
10/10 - 2s - loss: 339.1088 - loglik: -3.1855e+02 - logprior: -2.0433e+01
Epoch 3/10
10/10 - 2s - loss: 299.7619 - loglik: -2.9041e+02 - logprior: -9.2613e+00
Epoch 4/10
10/10 - 2s - loss: 280.6493 - loglik: -2.7511e+02 - logprior: -5.4457e+00
Epoch 5/10
10/10 - 2s - loss: 271.5758 - loglik: -2.6754e+02 - logprior: -3.8759e+00
Epoch 6/10
10/10 - 2s - loss: 267.1733 - loglik: -2.6373e+02 - logprior: -3.0866e+00
Epoch 7/10
10/10 - 2s - loss: 264.7848 - loglik: -2.6200e+02 - logprior: -2.3684e+00
Epoch 8/10
10/10 - 2s - loss: 263.1266 - loglik: -2.6095e+02 - logprior: -1.7563e+00
Epoch 9/10
10/10 - 2s - loss: 262.5479 - loglik: -2.6071e+02 - logprior: -1.4455e+00
Epoch 10/10
10/10 - 2s - loss: 262.1223 - loglik: -2.6050e+02 - logprior: -1.2444e+00
Fitted a model with MAP estimate = -261.3459
expansions: [(5, 1), (6, 1), (11, 3), (12, 1), (18, 2), (30, 1), (36, 5), (44, 3), (47, 1), (54, 1), (88, 6), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 354.9939 - loglik: -2.6552e+02 - logprior: -8.9451e+01
Epoch 2/2
10/10 - 2s - loss: 287.5930 - loglik: -2.5168e+02 - logprior: -3.5772e+01
Fitted a model with MAP estimate = -275.0048
expansions: [(0, 2), (11, 2), (81, 3)]
discards: [ 0 12 46]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.4825 - loglik: -2.4755e+02 - logprior: -7.0908e+01
Epoch 2/2
10/10 - 2s - loss: 258.7603 - loglik: -2.4101e+02 - logprior: -1.7645e+01
Fitted a model with MAP estimate = -248.4950
expansions: [(111, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 328.8423 - loglik: -2.4253e+02 - logprior: -8.6297e+01
Epoch 2/10
10/10 - 2s - loss: 268.2803 - loglik: -2.3858e+02 - logprior: -2.9595e+01
Epoch 3/10
10/10 - 2s - loss: 247.0874 - loglik: -2.3673e+02 - logprior: -1.0161e+01
Epoch 4/10
10/10 - 2s - loss: 237.5472 - loglik: -2.3505e+02 - logprior: -2.2327e+00
Epoch 5/10
10/10 - 2s - loss: 234.2922 - loglik: -2.3471e+02 - logprior: 0.8123
Epoch 6/10
10/10 - 2s - loss: 232.2653 - loglik: -2.3426e+02 - logprior: 2.4192
Epoch 7/10
10/10 - 2s - loss: 230.6929 - loglik: -2.3367e+02 - logprior: 3.3893
Epoch 8/10
10/10 - 2s - loss: 229.2959 - loglik: -2.3296e+02 - logprior: 4.0564
Epoch 9/10
10/10 - 2s - loss: 229.3691 - loglik: -2.3356e+02 - logprior: 4.5798
Fitted a model with MAP estimate = -228.1451
Time for alignment: 63.4342
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 424.8609 - loglik: -3.4526e+02 - logprior: -7.9571e+01
Epoch 2/10
10/10 - 2s - loss: 339.3458 - loglik: -3.1880e+02 - logprior: -2.0420e+01
Epoch 3/10
10/10 - 2s - loss: 300.4810 - loglik: -2.9111e+02 - logprior: -9.2789e+00
Epoch 4/10
10/10 - 2s - loss: 281.3776 - loglik: -2.7571e+02 - logprior: -5.5436e+00
Epoch 5/10
10/10 - 2s - loss: 272.7662 - loglik: -2.6870e+02 - logprior: -3.7008e+00
Epoch 6/10
10/10 - 2s - loss: 268.2836 - loglik: -2.6509e+02 - logprior: -2.7306e+00
Epoch 7/10
10/10 - 2s - loss: 265.2768 - loglik: -2.6260e+02 - logprior: -2.2833e+00
Epoch 8/10
10/10 - 2s - loss: 263.2650 - loglik: -2.6103e+02 - logprior: -1.8645e+00
Epoch 9/10
10/10 - 2s - loss: 261.6559 - loglik: -2.5969e+02 - logprior: -1.5939e+00
Epoch 10/10
10/10 - 2s - loss: 260.5981 - loglik: -2.5886e+02 - logprior: -1.3695e+00
Fitted a model with MAP estimate = -259.7383
expansions: [(5, 1), (6, 1), (10, 2), (12, 4), (18, 2), (36, 4), (45, 1), (46, 1), (62, 4), (64, 1), (78, 1), (84, 4), (86, 1), (88, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 352.4191 - loglik: -2.6308e+02 - logprior: -8.9312e+01
Epoch 2/2
10/10 - 2s - loss: 283.0483 - loglik: -2.4697e+02 - logprior: -3.5933e+01
Fitted a model with MAP estimate = -270.0971
expansions: [(0, 3)]
discards: [  0  47  81  84 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 317.3081 - loglik: -2.4650e+02 - logprior: -7.0783e+01
Epoch 2/2
10/10 - 2s - loss: 257.8209 - loglik: -2.3991e+02 - logprior: -1.7786e+01
Fitted a model with MAP estimate = -248.0671
expansions: [(113, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 327.5297 - loglik: -2.4109e+02 - logprior: -8.6420e+01
Epoch 2/10
10/10 - 2s - loss: 267.3242 - loglik: -2.3711e+02 - logprior: -3.0106e+01
Epoch 3/10
10/10 - 2s - loss: 245.9994 - loglik: -2.3511e+02 - logprior: -1.0664e+01
Epoch 4/10
10/10 - 2s - loss: 235.6859 - loglik: -2.3304e+02 - logprior: -2.2791e+00
Epoch 5/10
10/10 - 2s - loss: 232.0202 - loglik: -2.3236e+02 - logprior: 0.8213
Epoch 6/10
10/10 - 2s - loss: 229.9758 - loglik: -2.3187e+02 - logprior: 2.3939
Epoch 7/10
10/10 - 2s - loss: 228.4118 - loglik: -2.3132e+02 - logprior: 3.3542
Epoch 8/10
10/10 - 2s - loss: 227.9277 - loglik: -2.3157e+02 - logprior: 4.0350
Epoch 9/10
10/10 - 2s - loss: 227.1131 - loglik: -2.3135e+02 - logprior: 4.6089
Epoch 10/10
10/10 - 2s - loss: 226.5802 - loglik: -2.3133e+02 - logprior: 5.1172
Fitted a model with MAP estimate = -225.9621
Time for alignment: 66.4059
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 424.6546 - loglik: -3.4506e+02 - logprior: -7.9571e+01
Epoch 2/10
10/10 - 2s - loss: 338.7830 - loglik: -3.1823e+02 - logprior: -2.0432e+01
Epoch 3/10
10/10 - 2s - loss: 298.9223 - loglik: -2.8958e+02 - logprior: -9.2547e+00
Epoch 4/10
10/10 - 2s - loss: 279.1323 - loglik: -2.7348e+02 - logprior: -5.4818e+00
Epoch 5/10
10/10 - 2s - loss: 271.3238 - loglik: -2.6713e+02 - logprior: -3.8188e+00
Epoch 6/10
10/10 - 2s - loss: 266.6771 - loglik: -2.6340e+02 - logprior: -2.8898e+00
Epoch 7/10
10/10 - 2s - loss: 263.5133 - loglik: -2.6086e+02 - logprior: -2.3148e+00
Epoch 8/10
10/10 - 2s - loss: 262.2158 - loglik: -2.6012e+02 - logprior: -1.7768e+00
Epoch 9/10
10/10 - 2s - loss: 261.5531 - loglik: -2.5984e+02 - logprior: -1.3828e+00
Epoch 10/10
10/10 - 2s - loss: 260.9013 - loglik: -2.5950e+02 - logprior: -1.0711e+00
Fitted a model with MAP estimate = -260.3980
expansions: [(10, 1), (11, 4), (12, 1), (14, 1), (18, 1), (36, 4), (45, 3), (48, 2), (63, 3), (83, 3), (88, 6), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 354.6819 - loglik: -2.6505e+02 - logprior: -8.9607e+01
Epoch 2/2
10/10 - 2s - loss: 285.8665 - loglik: -2.4945e+02 - logprior: -3.6264e+01
Fitted a model with MAP estimate = -272.6512
expansions: [(0, 2), (11, 1), (108, 1)]
discards: [  0  43 104]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 316.2158 - loglik: -2.4521e+02 - logprior: -7.0988e+01
Epoch 2/2
10/10 - 2s - loss: 256.4157 - loglik: -2.3850e+02 - logprior: -1.7788e+01
Fitted a model with MAP estimate = -246.4507
expansions: [(64, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 327.0740 - loglik: -2.4130e+02 - logprior: -8.5750e+01
Epoch 2/10
10/10 - 2s - loss: 265.3055 - loglik: -2.3805e+02 - logprior: -2.7136e+01
Epoch 3/10
10/10 - 2s - loss: 245.1552 - loglik: -2.3607e+02 - logprior: -8.7938e+00
Epoch 4/10
10/10 - 2s - loss: 237.1740 - loglik: -2.3451e+02 - logprior: -2.2141e+00
Epoch 5/10
10/10 - 2s - loss: 234.1057 - loglik: -2.3443e+02 - logprior: 0.7179
Epoch 6/10
10/10 - 2s - loss: 231.9753 - loglik: -2.3396e+02 - logprior: 2.2963
Epoch 7/10
10/10 - 2s - loss: 230.8394 - loglik: -2.3377e+02 - logprior: 3.2472
Epoch 8/10
10/10 - 2s - loss: 229.8725 - loglik: -2.3342e+02 - logprior: 3.8906
Epoch 9/10
10/10 - 2s - loss: 229.1463 - loglik: -2.3323e+02 - logprior: 4.4483
Epoch 10/10
10/10 - 2s - loss: 228.5701 - loglik: -2.3316e+02 - logprior: 4.9504
Fitted a model with MAP estimate = -227.9464
Time for alignment: 63.9709
Computed alignments with likelihoods: ['-228.1451', '-225.9621', '-227.9464']
Best model has likelihood: -225.9621
time for generating output: 0.2039
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.8136690647482014
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f6a83f03f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f61703bd2b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6aad944730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f694288f3d0>, <__main__.SimpleDirichletPrior object at 0x7f6168854d30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1526 - loglik: -9.2661e+01 - logprior: -4.4709e+00
Epoch 2/10
17/17 - 1s - loss: 74.9077 - loglik: -7.3313e+01 - logprior: -1.5826e+00
Epoch 3/10
17/17 - 1s - loss: 65.6475 - loglik: -6.3920e+01 - logprior: -1.7225e+00
Epoch 4/10
17/17 - 1s - loss: 63.5271 - loglik: -6.1761e+01 - logprior: -1.6956e+00
Epoch 5/10
17/17 - 1s - loss: 63.0629 - loglik: -6.1299e+01 - logprior: -1.6310e+00
Epoch 6/10
17/17 - 1s - loss: 62.9611 - loglik: -6.1190e+01 - logprior: -1.6414e+00
Epoch 7/10
17/17 - 1s - loss: 62.9395 - loglik: -6.1165e+01 - logprior: -1.6231e+00
Epoch 8/10
17/17 - 1s - loss: 62.6888 - loglik: -6.0917e+01 - logprior: -1.6084e+00
Epoch 9/10
17/17 - 1s - loss: 62.6432 - loglik: -6.0861e+01 - logprior: -1.6032e+00
Epoch 10/10
17/17 - 1s - loss: 62.7133 - loglik: -6.0933e+01 - logprior: -1.5978e+00
Fitted a model with MAP estimate = -62.3783
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 70.3135 - loglik: -6.4685e+01 - logprior: -5.5973e+00
Epoch 2/2
17/17 - 1s - loss: 62.0367 - loglik: -5.9228e+01 - logprior: -2.6808e+00
Fitted a model with MAP estimate = -59.4582
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.4746 - loglik: -5.7991e+01 - logprior: -4.4546e+00
Epoch 2/2
17/17 - 1s - loss: 58.6487 - loglik: -5.6803e+01 - logprior: -1.7438e+00
Fitted a model with MAP estimate = -58.0797
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6349 - loglik: -5.7296e+01 - logprior: -4.3119e+00
Epoch 2/10
17/17 - 1s - loss: 58.5504 - loglik: -5.6710e+01 - logprior: -1.7401e+00
Epoch 3/10
17/17 - 1s - loss: 58.0843 - loglik: -5.6441e+01 - logprior: -1.5105e+00
Epoch 4/10
17/17 - 1s - loss: 57.7181 - loglik: -5.6111e+01 - logprior: -1.4533e+00
Epoch 5/10
17/17 - 1s - loss: 57.6781 - loglik: -5.6077e+01 - logprior: -1.4241e+00
Epoch 6/10
17/17 - 1s - loss: 57.5131 - loglik: -5.5931e+01 - logprior: -1.3991e+00
Epoch 7/10
17/17 - 1s - loss: 57.5241 - loglik: -5.5952e+01 - logprior: -1.3800e+00
Fitted a model with MAP estimate = -57.2130
Time for alignment: 39.9282
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 97.2708 - loglik: -9.2777e+01 - logprior: -4.4728e+00
Epoch 2/10
17/17 - 1s - loss: 75.2100 - loglik: -7.3613e+01 - logprior: -1.5860e+00
Epoch 3/10
17/17 - 1s - loss: 65.8164 - loglik: -6.4076e+01 - logprior: -1.7244e+00
Epoch 4/10
17/17 - 1s - loss: 63.4774 - loglik: -6.1648e+01 - logprior: -1.7091e+00
Epoch 5/10
17/17 - 1s - loss: 63.2715 - loglik: -6.1492e+01 - logprior: -1.6367e+00
Epoch 6/10
17/17 - 1s - loss: 62.8545 - loglik: -6.1070e+01 - logprior: -1.6484e+00
Epoch 7/10
17/17 - 1s - loss: 62.6440 - loglik: -6.0869e+01 - logprior: -1.6297e+00
Epoch 8/10
17/17 - 1s - loss: 62.6543 - loglik: -6.0878e+01 - logprior: -1.6167e+00
Fitted a model with MAP estimate = -62.4380
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2620 - loglik: -6.4630e+01 - logprior: -5.6008e+00
Epoch 2/2
17/17 - 1s - loss: 61.6734 - loglik: -5.8911e+01 - logprior: -2.6398e+00
Fitted a model with MAP estimate = -59.2762
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.2856 - loglik: -5.7829e+01 - logprior: -4.4286e+00
Epoch 2/2
17/17 - 1s - loss: 58.6992 - loglik: -5.6845e+01 - logprior: -1.7523e+00
Fitted a model with MAP estimate = -58.0979
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.6136 - loglik: -5.7269e+01 - logprior: -4.3176e+00
Epoch 2/10
17/17 - 1s - loss: 58.4228 - loglik: -5.6586e+01 - logprior: -1.7386e+00
Epoch 3/10
17/17 - 1s - loss: 58.1694 - loglik: -5.6536e+01 - logprior: -1.5045e+00
Epoch 4/10
17/17 - 1s - loss: 57.7092 - loglik: -5.6101e+01 - logprior: -1.4560e+00
Epoch 5/10
17/17 - 1s - loss: 57.7496 - loglik: -5.6140e+01 - logprior: -1.4307e+00
Fitted a model with MAP estimate = -57.3730
Time for alignment: 37.3001
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1773 - loglik: -9.2683e+01 - logprior: -4.4742e+00
Epoch 2/10
17/17 - 1s - loss: 74.8262 - loglik: -7.3213e+01 - logprior: -1.5995e+00
Epoch 3/10
17/17 - 1s - loss: 65.2693 - loglik: -6.3519e+01 - logprior: -1.7232e+00
Epoch 4/10
17/17 - 1s - loss: 63.1875 - loglik: -6.1368e+01 - logprior: -1.6907e+00
Epoch 5/10
17/17 - 1s - loss: 62.8917 - loglik: -6.1179e+01 - logprior: -1.6067e+00
Epoch 6/10
17/17 - 1s - loss: 62.4801 - loglik: -6.0720e+01 - logprior: -1.6301e+00
Epoch 7/10
17/17 - 1s - loss: 62.4764 - loglik: -6.0727e+01 - logprior: -1.6113e+00
Epoch 8/10
17/17 - 1s - loss: 62.5333 - loglik: -6.0792e+01 - logprior: -1.5958e+00
Fitted a model with MAP estimate = -62.2305
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.8198 - loglik: -6.4146e+01 - logprior: -5.6336e+00
Epoch 2/2
17/17 - 1s - loss: 61.4147 - loglik: -5.8449e+01 - logprior: -2.8633e+00
Fitted a model with MAP estimate = -59.4703
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 64.1302 - loglik: -5.9101e+01 - logprior: -5.0024e+00
Epoch 2/2
17/17 - 1s - loss: 58.7240 - loglik: -5.6825e+01 - logprior: -1.8009e+00
Fitted a model with MAP estimate = -58.1376
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.6367 - loglik: -5.7291e+01 - logprior: -4.3188e+00
Epoch 2/10
17/17 - 1s - loss: 58.5330 - loglik: -5.6694e+01 - logprior: -1.7357e+00
Epoch 3/10
17/17 - 1s - loss: 58.1307 - loglik: -5.6496e+01 - logprior: -1.5047e+00
Epoch 4/10
17/17 - 1s - loss: 57.6448 - loglik: -5.6027e+01 - logprior: -1.4631e+00
Epoch 5/10
17/17 - 1s - loss: 57.7368 - loglik: -5.6140e+01 - logprior: -1.4189e+00
Fitted a model with MAP estimate = -57.3778
Time for alignment: 35.5881
Computed alignments with likelihoods: ['-57.2130', '-57.3730', '-57.3778']
Best model has likelihood: -57.2130
time for generating output: 0.1347
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7823234272481667
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f699407b9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f699407b850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea19e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6945060a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6942a66e50>, <__main__.SimpleDirichletPrior object at 0x7f6a9c1fd7c0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7821 - loglik: -1.8550e+02 - logprior: -8.2784e+00
Epoch 2/10
13/13 - 1s - loss: 164.0508 - loglik: -1.6165e+02 - logprior: -2.3990e+00
Epoch 3/10
13/13 - 1s - loss: 145.6411 - loglik: -1.4345e+02 - logprior: -2.1145e+00
Epoch 4/10
13/13 - 1s - loss: 138.8471 - loglik: -1.3627e+02 - logprior: -2.2618e+00
Epoch 5/10
13/13 - 1s - loss: 136.1440 - loglik: -1.3368e+02 - logprior: -2.1914e+00
Epoch 6/10
13/13 - 1s - loss: 134.6186 - loglik: -1.3230e+02 - logprior: -2.1050e+00
Epoch 7/10
13/13 - 1s - loss: 134.4388 - loglik: -1.3215e+02 - logprior: -2.1213e+00
Epoch 8/10
13/13 - 1s - loss: 133.9501 - loglik: -1.3169e+02 - logprior: -2.0996e+00
Epoch 9/10
13/13 - 1s - loss: 133.8639 - loglik: -1.3162e+02 - logprior: -2.0857e+00
Epoch 10/10
13/13 - 1s - loss: 133.5042 - loglik: -1.3127e+02 - logprior: -2.0801e+00
Fitted a model with MAP estimate = -133.4519
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.9237 - loglik: -1.3504e+02 - logprior: -9.8430e+00
Epoch 2/2
13/13 - 1s - loss: 129.6189 - loglik: -1.2463e+02 - logprior: -4.7734e+00
Fitted a model with MAP estimate = -127.2167
expansions: [(0, 2)]
discards: [ 0 24 56]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 130.9025 - loglik: -1.2328e+02 - logprior: -7.5811e+00
Epoch 2/2
13/13 - 1s - loss: 124.6517 - loglik: -1.2205e+02 - logprior: -2.4473e+00
Fitted a model with MAP estimate = -122.7595
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.1803 - loglik: -1.2463e+02 - logprior: -9.5191e+00
Epoch 2/10
13/13 - 1s - loss: 125.7891 - loglik: -1.2186e+02 - logprior: -3.7695e+00
Epoch 3/10
13/13 - 1s - loss: 121.8860 - loglik: -1.1959e+02 - logprior: -2.0113e+00
Epoch 4/10
13/13 - 1s - loss: 120.4096 - loglik: -1.1844e+02 - logprior: -1.6796e+00
Epoch 5/10
13/13 - 1s - loss: 120.2341 - loglik: -1.1844e+02 - logprior: -1.5572e+00
Epoch 6/10
13/13 - 1s - loss: 119.3885 - loglik: -1.1760e+02 - logprior: -1.5704e+00
Epoch 7/10
13/13 - 1s - loss: 119.2039 - loglik: -1.1742e+02 - logprior: -1.5850e+00
Epoch 8/10
13/13 - 1s - loss: 118.9328 - loglik: -1.1720e+02 - logprior: -1.5366e+00
Epoch 9/10
13/13 - 1s - loss: 118.9825 - loglik: -1.1728e+02 - logprior: -1.5176e+00
Fitted a model with MAP estimate = -118.5527
Time for alignment: 47.3172
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.9055 - loglik: -1.8562e+02 - logprior: -8.2799e+00
Epoch 2/10
13/13 - 1s - loss: 163.8778 - loglik: -1.6147e+02 - logprior: -2.4029e+00
Epoch 3/10
13/13 - 1s - loss: 146.2066 - loglik: -1.4401e+02 - logprior: -2.1216e+00
Epoch 4/10
13/13 - 1s - loss: 139.1217 - loglik: -1.3660e+02 - logprior: -2.2338e+00
Epoch 5/10
13/13 - 1s - loss: 136.9564 - loglik: -1.3456e+02 - logprior: -2.1303e+00
Epoch 6/10
13/13 - 1s - loss: 134.9752 - loglik: -1.3267e+02 - logprior: -2.0589e+00
Epoch 7/10
13/13 - 1s - loss: 133.9571 - loglik: -1.3165e+02 - logprior: -2.0899e+00
Epoch 8/10
13/13 - 1s - loss: 133.7804 - loglik: -1.3148e+02 - logprior: -2.1084e+00
Epoch 9/10
13/13 - 1s - loss: 133.4131 - loglik: -1.3115e+02 - logprior: -2.0886e+00
Epoch 10/10
13/13 - 1s - loss: 133.0102 - loglik: -1.3076e+02 - logprior: -2.0873e+00
Fitted a model with MAP estimate = -132.8857
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (32, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 144.8695 - loglik: -1.3497e+02 - logprior: -9.8608e+00
Epoch 2/2
13/13 - 1s - loss: 129.9798 - loglik: -1.2493e+02 - logprior: -4.8143e+00
Fitted a model with MAP estimate = -127.2619
expansions: [(0, 2)]
discards: [ 0 25 56]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.3508 - loglik: -1.2372e+02 - logprior: -7.5907e+00
Epoch 2/2
13/13 - 1s - loss: 123.9009 - loglik: -1.2126e+02 - logprior: -2.4654e+00
Fitted a model with MAP estimate = -122.4317
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.0043 - loglik: -1.2444e+02 - logprior: -9.5289e+00
Epoch 2/10
13/13 - 1s - loss: 125.4694 - loglik: -1.2150e+02 - logprior: -3.7856e+00
Epoch 3/10
13/13 - 1s - loss: 121.5282 - loglik: -1.1919e+02 - logprior: -2.0205e+00
Epoch 4/10
13/13 - 1s - loss: 120.4145 - loglik: -1.1843e+02 - logprior: -1.6677e+00
Epoch 5/10
13/13 - 1s - loss: 119.6390 - loglik: -1.1786e+02 - logprior: -1.5464e+00
Epoch 6/10
13/13 - 1s - loss: 119.2435 - loglik: -1.1748e+02 - logprior: -1.5623e+00
Epoch 7/10
13/13 - 1s - loss: 118.9962 - loglik: -1.1723e+02 - logprior: -1.5776e+00
Epoch 8/10
13/13 - 1s - loss: 118.5316 - loglik: -1.1682e+02 - logprior: -1.5323e+00
Epoch 9/10
13/13 - 1s - loss: 118.3583 - loglik: -1.1666e+02 - logprior: -1.5202e+00
Epoch 10/10
13/13 - 1s - loss: 118.6479 - loglik: -1.1701e+02 - logprior: -1.4755e+00
Fitted a model with MAP estimate = -118.2308
Time for alignment: 47.2415
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7680 - loglik: -1.8548e+02 - logprior: -8.2792e+00
Epoch 2/10
13/13 - 1s - loss: 164.3402 - loglik: -1.6194e+02 - logprior: -2.3960e+00
Epoch 3/10
13/13 - 1s - loss: 147.4096 - loglik: -1.4524e+02 - logprior: -2.0808e+00
Epoch 4/10
13/13 - 1s - loss: 139.1157 - loglik: -1.3654e+02 - logprior: -2.2442e+00
Epoch 5/10
13/13 - 1s - loss: 136.2753 - loglik: -1.3371e+02 - logprior: -2.1924e+00
Epoch 6/10
13/13 - 1s - loss: 134.7013 - loglik: -1.3233e+02 - logprior: -2.0890e+00
Epoch 7/10
13/13 - 1s - loss: 133.5293 - loglik: -1.3120e+02 - logprior: -2.1123e+00
Epoch 8/10
13/13 - 1s - loss: 133.4718 - loglik: -1.3118e+02 - logprior: -2.1046e+00
Epoch 9/10
13/13 - 1s - loss: 133.1941 - loglik: -1.3093e+02 - logprior: -2.1004e+00
Epoch 10/10
13/13 - 1s - loss: 133.4773 - loglik: -1.3123e+02 - logprior: -2.0921e+00
Fitted a model with MAP estimate = -132.9167
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (32, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.8678 - loglik: -1.3496e+02 - logprior: -9.8609e+00
Epoch 2/2
13/13 - 1s - loss: 129.6554 - loglik: -1.2462e+02 - logprior: -4.8098e+00
Fitted a model with MAP estimate = -127.2760
expansions: [(0, 2)]
discards: [ 0 25 56]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.1276 - loglik: -1.2350e+02 - logprior: -7.5844e+00
Epoch 2/2
13/13 - 1s - loss: 124.2584 - loglik: -1.2165e+02 - logprior: -2.4553e+00
Fitted a model with MAP estimate = -122.7260
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.8678 - loglik: -1.2429e+02 - logprior: -9.5404e+00
Epoch 2/10
13/13 - 1s - loss: 126.1339 - loglik: -1.2215e+02 - logprior: -3.8070e+00
Epoch 3/10
13/13 - 1s - loss: 122.2271 - loglik: -1.1990e+02 - logprior: -2.0181e+00
Epoch 4/10
13/13 - 1s - loss: 120.6697 - loglik: -1.1869e+02 - logprior: -1.6778e+00
Epoch 5/10
13/13 - 1s - loss: 119.7560 - loglik: -1.1797e+02 - logprior: -1.5451e+00
Epoch 6/10
13/13 - 1s - loss: 119.3240 - loglik: -1.1754e+02 - logprior: -1.5715e+00
Epoch 7/10
13/13 - 1s - loss: 119.0207 - loglik: -1.1725e+02 - logprior: -1.5728e+00
Epoch 8/10
13/13 - 1s - loss: 118.3569 - loglik: -1.1664e+02 - logprior: -1.5266e+00
Epoch 9/10
13/13 - 1s - loss: 118.8203 - loglik: -1.1712e+02 - logprior: -1.5141e+00
Fitted a model with MAP estimate = -118.3575
Time for alignment: 45.8333
Computed alignments with likelihoods: ['-118.5527', '-118.2308', '-118.3575']
Best model has likelihood: -118.2308
time for generating output: 0.1426
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69699fb850>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6a82425550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6971f3c370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6ab60f7fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6a83dbad60>, <__main__.SimpleDirichletPrior object at 0x7f616949c9a0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 241.1357 - loglik: -2.2827e+02 - logprior: -1.2866e+01
Epoch 2/10
11/11 - 1s - loss: 208.9613 - loglik: -2.0538e+02 - logprior: -3.5760e+00
Epoch 3/10
11/11 - 1s - loss: 182.6052 - loglik: -1.8007e+02 - logprior: -2.4001e+00
Epoch 4/10
11/11 - 1s - loss: 166.0917 - loglik: -1.6354e+02 - logprior: -2.1925e+00
Epoch 5/10
11/11 - 1s - loss: 160.9988 - loglik: -1.5861e+02 - logprior: -1.9939e+00
Epoch 6/10
11/11 - 1s - loss: 158.6605 - loglik: -1.5635e+02 - logprior: -1.9046e+00
Epoch 7/10
11/11 - 1s - loss: 157.2301 - loglik: -1.5525e+02 - logprior: -1.6787e+00
Epoch 8/10
11/11 - 1s - loss: 156.5998 - loglik: -1.5487e+02 - logprior: -1.5219e+00
Epoch 9/10
11/11 - 1s - loss: 156.7872 - loglik: -1.5517e+02 - logprior: -1.4200e+00
Fitted a model with MAP estimate = -156.0044
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 172.8728 - loglik: -1.5677e+02 - logprior: -1.6077e+01
Epoch 2/2
11/11 - 1s - loss: 150.8344 - loglik: -1.4584e+02 - logprior: -4.8326e+00
Fitted a model with MAP estimate = -147.3201
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.6824 - loglik: -1.4813e+02 - logprior: -1.4520e+01
Epoch 2/2
11/11 - 1s - loss: 152.0576 - loglik: -1.4602e+02 - logprior: -5.8763e+00
Fitted a model with MAP estimate = -148.9515
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 158.9051 - loglik: -1.4619e+02 - logprior: -1.2684e+01
Epoch 2/10
11/11 - 1s - loss: 148.0365 - loglik: -1.4439e+02 - logprior: -3.5130e+00
Epoch 3/10
11/11 - 1s - loss: 145.7246 - loglik: -1.4323e+02 - logprior: -2.2473e+00
Epoch 4/10
11/11 - 1s - loss: 144.4343 - loglik: -1.4250e+02 - logprior: -1.6454e+00
Epoch 5/10
11/11 - 1s - loss: 143.3755 - loglik: -1.4175e+02 - logprior: -1.3557e+00
Epoch 6/10
11/11 - 1s - loss: 143.2513 - loglik: -1.4172e+02 - logprior: -1.2961e+00
Epoch 7/10
11/11 - 1s - loss: 142.6688 - loglik: -1.4129e+02 - logprior: -1.1599e+00
Epoch 8/10
11/11 - 1s - loss: 142.7018 - loglik: -1.4135e+02 - logprior: -1.1328e+00
Fitted a model with MAP estimate = -142.1662
Time for alignment: 45.5188
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.2590 - loglik: -2.2839e+02 - logprior: -1.2868e+01
Epoch 2/10
11/11 - 1s - loss: 208.2630 - loglik: -2.0469e+02 - logprior: -3.5666e+00
Epoch 3/10
11/11 - 1s - loss: 181.8620 - loglik: -1.7937e+02 - logprior: -2.3630e+00
Epoch 4/10
11/11 - 1s - loss: 165.5967 - loglik: -1.6311e+02 - logprior: -2.1213e+00
Epoch 5/10
11/11 - 1s - loss: 160.6895 - loglik: -1.5834e+02 - logprior: -1.9181e+00
Epoch 6/10
11/11 - 1s - loss: 158.4742 - loglik: -1.5630e+02 - logprior: -1.7979e+00
Epoch 7/10
11/11 - 1s - loss: 157.6068 - loglik: -1.5574e+02 - logprior: -1.5848e+00
Epoch 8/10
11/11 - 1s - loss: 156.9225 - loglik: -1.5521e+02 - logprior: -1.4748e+00
Epoch 9/10
11/11 - 1s - loss: 156.4372 - loglik: -1.5483e+02 - logprior: -1.4039e+00
Epoch 10/10
11/11 - 1s - loss: 156.1766 - loglik: -1.5461e+02 - logprior: -1.3647e+00
Fitted a model with MAP estimate = -155.8950
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.2121 - loglik: -1.5700e+02 - logprior: -1.6183e+01
Epoch 2/2
11/11 - 1s - loss: 150.9809 - loglik: -1.4589e+02 - logprior: -4.9242e+00
Fitted a model with MAP estimate = -147.3594
expansions: []
discards: [ 0 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 162.6203 - loglik: -1.4806e+02 - logprior: -1.4531e+01
Epoch 2/2
11/11 - 1s - loss: 151.6114 - loglik: -1.4556e+02 - logprior: -5.8948e+00
Fitted a model with MAP estimate = -148.7692
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.2075 - loglik: -1.4547e+02 - logprior: -1.2708e+01
Epoch 2/10
11/11 - 1s - loss: 148.6027 - loglik: -1.4495e+02 - logprior: -3.5263e+00
Epoch 3/10
11/11 - 1s - loss: 145.5064 - loglik: -1.4302e+02 - logprior: -2.2479e+00
Epoch 4/10
11/11 - 1s - loss: 144.2659 - loglik: -1.4234e+02 - logprior: -1.6486e+00
Epoch 5/10
11/11 - 1s - loss: 143.6055 - loglik: -1.4197e+02 - logprior: -1.3730e+00
Epoch 6/10
11/11 - 1s - loss: 143.1033 - loglik: -1.4158e+02 - logprior: -1.2931e+00
Epoch 7/10
11/11 - 1s - loss: 142.7513 - loglik: -1.4136e+02 - logprior: -1.1638e+00
Epoch 8/10
11/11 - 1s - loss: 142.3849 - loglik: -1.4103e+02 - logprior: -1.1366e+00
Epoch 9/10
11/11 - 1s - loss: 142.5613 - loglik: -1.4125e+02 - logprior: -1.0958e+00
Fitted a model with MAP estimate = -141.9944
Time for alignment: 45.5542
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.1781 - loglik: -2.2831e+02 - logprior: -1.2865e+01
Epoch 2/10
11/11 - 1s - loss: 208.0436 - loglik: -2.0446e+02 - logprior: -3.5740e+00
Epoch 3/10
11/11 - 1s - loss: 181.1214 - loglik: -1.7861e+02 - logprior: -2.3866e+00
Epoch 4/10
11/11 - 1s - loss: 165.2903 - loglik: -1.6275e+02 - logprior: -2.2053e+00
Epoch 5/10
11/11 - 1s - loss: 160.2920 - loglik: -1.5784e+02 - logprior: -1.9915e+00
Epoch 6/10
11/11 - 1s - loss: 158.7153 - loglik: -1.5645e+02 - logprior: -1.8499e+00
Epoch 7/10
11/11 - 1s - loss: 157.1420 - loglik: -1.5522e+02 - logprior: -1.6129e+00
Epoch 8/10
11/11 - 1s - loss: 157.1514 - loglik: -1.5541e+02 - logprior: -1.4898e+00
Fitted a model with MAP estimate = -156.3559
expansions: [(0, 6), (21, 1), (23, 2), (26, 1), (28, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 174.2200 - loglik: -1.5813e+02 - logprior: -1.6068e+01
Epoch 2/2
11/11 - 1s - loss: 151.7778 - loglik: -1.4676e+02 - logprior: -4.9299e+00
Fitted a model with MAP estimate = -147.8054
expansions: []
discards: [ 0 30 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.1824 - loglik: -1.4861e+02 - logprior: -1.4547e+01
Epoch 2/2
11/11 - 1s - loss: 151.5608 - loglik: -1.4554e+02 - logprior: -5.9030e+00
Fitted a model with MAP estimate = -148.8496
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.5816 - loglik: -1.4584e+02 - logprior: -1.2718e+01
Epoch 2/10
11/11 - 1s - loss: 147.9716 - loglik: -1.4432e+02 - logprior: -3.5247e+00
Epoch 3/10
11/11 - 1s - loss: 145.5730 - loglik: -1.4308e+02 - logprior: -2.2643e+00
Epoch 4/10
11/11 - 1s - loss: 144.2867 - loglik: -1.4231e+02 - logprior: -1.6846e+00
Epoch 5/10
11/11 - 1s - loss: 143.5005 - loglik: -1.4185e+02 - logprior: -1.3708e+00
Epoch 6/10
11/11 - 1s - loss: 143.1560 - loglik: -1.4161e+02 - logprior: -1.3060e+00
Epoch 7/10
11/11 - 1s - loss: 142.9032 - loglik: -1.4151e+02 - logprior: -1.1694e+00
Epoch 8/10
11/11 - 1s - loss: 142.3758 - loglik: -1.4100e+02 - logprior: -1.1492e+00
Epoch 9/10
11/11 - 1s - loss: 142.6888 - loglik: -1.4137e+02 - logprior: -1.1035e+00
Fitted a model with MAP estimate = -141.9829
Time for alignment: 43.1891
Computed alignments with likelihoods: ['-142.1662', '-141.9944', '-141.9829']
Best model has likelihood: -141.9829
time for generating output: 0.1412
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f616838ff40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6afd679280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617008f670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f69720d0b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706abd30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f677c5e8d30>, <__main__.SimpleDirichletPrior object at 0x7f6160313d30>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 14s - loss: 526.5355 - loglik: -5.2041e+02 - logprior: -6.1109e+00
Epoch 2/10
24/24 - 8s - loss: 378.6223 - loglik: -3.7549e+02 - logprior: -3.0906e+00
Epoch 3/10
24/24 - 8s - loss: 348.3031 - loglik: -3.4414e+02 - logprior: -3.7713e+00
Epoch 4/10
24/24 - 8s - loss: 343.2520 - loglik: -3.3915e+02 - logprior: -3.6246e+00
Epoch 5/10
24/24 - 8s - loss: 342.1059 - loglik: -3.3808e+02 - logprior: -3.6219e+00
Epoch 6/10
24/24 - 8s - loss: 339.9191 - loglik: -3.3591e+02 - logprior: -3.6439e+00
Epoch 7/10
24/24 - 8s - loss: 342.1823 - loglik: -3.3817e+02 - logprior: -3.6635e+00
Fitted a model with MAP estimate = -339.8874
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (85, 1), (88, 1), (90, 1), (103, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (171, 1), (172, 1), (173, 1), (174, 1), (186, 1), (187, 1), (189, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 331.4456 - loglik: -3.2522e+02 - logprior: -6.1303e+00
Epoch 2/2
24/24 - 10s - loss: 312.0165 - loglik: -3.0989e+02 - logprior: -1.7210e+00
Fitted a model with MAP estimate = -307.5958
expansions: [(192, 1), (194, 1)]
discards: [13]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 316.6902 - loglik: -3.1095e+02 - logprior: -5.6363e+00
Epoch 2/2
24/24 - 10s - loss: 307.9485 - loglik: -3.0622e+02 - logprior: -1.3075e+00
Fitted a model with MAP estimate = -305.1433
expansions: [(151, 1)]
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 15s - loss: 316.4331 - loglik: -3.1102e+02 - logprior: -5.2961e+00
Epoch 2/10
24/24 - 10s - loss: 307.2133 - loglik: -3.0582e+02 - logprior: -9.2960e-01
Epoch 3/10
24/24 - 10s - loss: 303.6044 - loglik: -3.0242e+02 - logprior: -5.6564e-01
Epoch 4/10
24/24 - 10s - loss: 302.9572 - loglik: -3.0199e+02 - logprior: -3.9122e-01
Epoch 5/10
24/24 - 10s - loss: 303.3648 - loglik: -3.0261e+02 - logprior: -2.3443e-01
Fitted a model with MAP estimate = -301.2121
Time for alignment: 202.6578
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 527.2209 - loglik: -5.2108e+02 - logprior: -6.1268e+00
Epoch 2/10
24/24 - 8s - loss: 378.2089 - loglik: -3.7512e+02 - logprior: -3.0535e+00
Epoch 3/10
24/24 - 8s - loss: 347.2464 - loglik: -3.4343e+02 - logprior: -3.6144e+00
Epoch 4/10
24/24 - 8s - loss: 344.3066 - loglik: -3.4048e+02 - logprior: -3.4153e+00
Epoch 5/10
24/24 - 8s - loss: 342.9652 - loglik: -3.3917e+02 - logprior: -3.4153e+00
Epoch 6/10
24/24 - 8s - loss: 342.6453 - loglik: -3.3881e+02 - logprior: -3.4719e+00
Epoch 7/10
24/24 - 8s - loss: 341.7148 - loglik: -3.3785e+02 - logprior: -3.5018e+00
Epoch 8/10
24/24 - 8s - loss: 342.0251 - loglik: -3.3816e+02 - logprior: -3.5072e+00
Fitted a model with MAP estimate = -340.9817
expansions: [(11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (34, 2), (35, 1), (36, 1), (37, 2), (39, 2), (46, 1), (47, 1), (48, 1), (63, 1), (65, 1), (75, 1), (82, 1), (87, 2), (90, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 337.4713 - loglik: -3.2887e+02 - logprior: -8.5164e+00
Epoch 2/2
24/24 - 10s - loss: 314.9787 - loglik: -3.1070e+02 - logprior: -3.9056e+00
Fitted a model with MAP estimate = -311.4541
expansions: [(0, 2), (192, 1), (194, 1)]
discards: [ 0 24 49 53]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 317.5331 - loglik: -3.1177e+02 - logprior: -5.6414e+00
Epoch 2/2
24/24 - 10s - loss: 307.6320 - loglik: -3.0586e+02 - logprior: -1.2829e+00
Fitted a model with MAP estimate = -305.4172
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 14s - loss: 315.3844 - loglik: -3.0996e+02 - logprior: -5.3003e+00
Epoch 2/10
24/24 - 10s - loss: 308.0714 - loglik: -3.0663e+02 - logprior: -9.0407e-01
Epoch 3/10
24/24 - 10s - loss: 304.1844 - loglik: -3.0301e+02 - logprior: -5.1446e-01
Epoch 4/10
24/24 - 10s - loss: 303.5343 - loglik: -3.0263e+02 - logprior: -3.5176e-01
Epoch 5/10
24/24 - 10s - loss: 302.2144 - loglik: -3.0153e+02 - logprior: -1.8772e-01
Epoch 6/10
24/24 - 10s - loss: 300.5668 - loglik: -3.0008e+02 - logprior: -1.0384e-02
Epoch 7/10
24/24 - 10s - loss: 302.5597 - loglik: -3.0228e+02 - logprior: 0.1899
Fitted a model with MAP estimate = -300.4672
Time for alignment: 227.5937
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 528.7287 - loglik: -5.2262e+02 - logprior: -6.0994e+00
Epoch 2/10
24/24 - 8s - loss: 379.1641 - loglik: -3.7608e+02 - logprior: -3.0388e+00
Epoch 3/10
24/24 - 8s - loss: 350.0568 - loglik: -3.4603e+02 - logprior: -3.6060e+00
Epoch 4/10
24/24 - 8s - loss: 343.6984 - loglik: -3.3972e+02 - logprior: -3.4792e+00
Epoch 5/10
24/24 - 8s - loss: 344.2971 - loglik: -3.4038e+02 - logprior: -3.5303e+00
Fitted a model with MAP estimate = -342.3726
expansions: [(12, 3), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (64, 1), (66, 1), (76, 1), (83, 1), (87, 1), (88, 1), (91, 1), (93, 1), (110, 1), (113, 1), (115, 1), (119, 1), (120, 1), (121, 1), (122, 2), (123, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 337.4210 - loglik: -3.2883e+02 - logprior: -8.4913e+00
Epoch 2/2
24/24 - 10s - loss: 316.0888 - loglik: -3.1178e+02 - logprior: -3.9115e+00
Fitted a model with MAP estimate = -311.9500
expansions: [(0, 2), (192, 1), (194, 1)]
discards: [  0  12  25 153]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 318.2143 - loglik: -3.1241e+02 - logprior: -5.7239e+00
Epoch 2/2
24/24 - 10s - loss: 310.2357 - loglik: -3.0842e+02 - logprior: -1.4280e+00
Fitted a model with MAP estimate = -306.8750
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 317.8662 - loglik: -3.1230e+02 - logprior: -5.4895e+00
Epoch 2/10
24/24 - 10s - loss: 307.9316 - loglik: -3.0643e+02 - logprior: -1.1129e+00
Epoch 3/10
24/24 - 10s - loss: 306.0202 - loglik: -3.0474e+02 - logprior: -6.7065e-01
Epoch 4/10
24/24 - 10s - loss: 303.7373 - loglik: -3.0267e+02 - logprior: -4.8874e-01
Epoch 5/10
24/24 - 10s - loss: 303.7201 - loglik: -3.0289e+02 - logprior: -3.2096e-01
Epoch 6/10
24/24 - 10s - loss: 303.3550 - loglik: -3.0274e+02 - logprior: -1.4354e-01
Epoch 7/10
24/24 - 10s - loss: 301.7907 - loglik: -3.0137e+02 - logprior: 0.0487
Epoch 8/10
24/24 - 10s - loss: 302.1681 - loglik: -3.0194e+02 - logprior: 0.2283
Fitted a model with MAP estimate = -301.1954
Time for alignment: 212.5195
Computed alignments with likelihoods: ['-301.2121', '-300.4672', '-301.1954']
Best model has likelihood: -300.4672
time for generating output: 0.3070
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9675230685724625
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f61801dad30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6993dc0070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd5180a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f694d78d4f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706abd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942faf8b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6944b8d580>, <__main__.SimpleDirichletPrior object at 0x7f695730b730>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 304.5112 - loglik: -2.7658e+02 - logprior: -2.7910e+01
Epoch 2/10
10/10 - 1s - loss: 248.0257 - loglik: -2.4029e+02 - logprior: -7.7153e+00
Epoch 3/10
10/10 - 1s - loss: 209.7072 - loglik: -2.0529e+02 - logprior: -4.4085e+00
Epoch 4/10
10/10 - 1s - loss: 189.9514 - loglik: -1.8653e+02 - logprior: -3.4109e+00
Epoch 5/10
10/10 - 1s - loss: 180.9466 - loglik: -1.7766e+02 - logprior: -3.1748e+00
Epoch 6/10
10/10 - 1s - loss: 177.9509 - loglik: -1.7457e+02 - logprior: -3.0993e+00
Epoch 7/10
10/10 - 1s - loss: 176.4644 - loglik: -1.7329e+02 - logprior: -2.8419e+00
Epoch 8/10
10/10 - 1s - loss: 175.8244 - loglik: -1.7286e+02 - logprior: -2.6751e+00
Epoch 9/10
10/10 - 1s - loss: 175.9860 - loglik: -1.7309e+02 - logprior: -2.6312e+00
Fitted a model with MAP estimate = -175.1850
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (35, 4), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 208.1528 - loglik: -1.7290e+02 - logprior: -3.5225e+01
Epoch 2/2
10/10 - 1s - loss: 174.5617 - loglik: -1.6324e+02 - logprior: -1.1165e+01
Fitted a model with MAP estimate = -167.3612
expansions: []
discards: [ 0 85]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.9447 - loglik: -1.6286e+02 - logprior: -3.2059e+01
Epoch 2/2
10/10 - 1s - loss: 174.0525 - loglik: -1.6057e+02 - logprior: -1.3365e+01
Fitted a model with MAP estimate = -169.1545
expansions: [(3, 1), (41, 1), (59, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 189.6143 - loglik: -1.5942e+02 - logprior: -3.0175e+01
Epoch 2/10
10/10 - 1s - loss: 165.2317 - loglik: -1.5573e+02 - logprior: -9.4145e+00
Epoch 3/10
10/10 - 1s - loss: 158.9875 - loglik: -1.5454e+02 - logprior: -4.1979e+00
Epoch 4/10
10/10 - 1s - loss: 155.3333 - loglik: -1.5240e+02 - logprior: -2.5551e+00
Epoch 5/10
10/10 - 1s - loss: 154.2950 - loglik: -1.5207e+02 - logprior: -1.8504e+00
Epoch 6/10
10/10 - 1s - loss: 153.0396 - loglik: -1.5115e+02 - logprior: -1.5320e+00
Epoch 7/10
10/10 - 1s - loss: 152.2281 - loglik: -1.5049e+02 - logprior: -1.3862e+00
Epoch 8/10
10/10 - 1s - loss: 152.5290 - loglik: -1.5097e+02 - logprior: -1.2197e+00
Fitted a model with MAP estimate = -151.5288
Time for alignment: 47.4198
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.7002 - loglik: -2.7677e+02 - logprior: -2.7910e+01
Epoch 2/10
10/10 - 1s - loss: 248.1634 - loglik: -2.4043e+02 - logprior: -7.7153e+00
Epoch 3/10
10/10 - 1s - loss: 210.9453 - loglik: -2.0651e+02 - logprior: -4.4280e+00
Epoch 4/10
10/10 - 1s - loss: 191.7496 - loglik: -1.8831e+02 - logprior: -3.4288e+00
Epoch 5/10
10/10 - 1s - loss: 182.6161 - loglik: -1.7940e+02 - logprior: -3.1566e+00
Epoch 6/10
10/10 - 1s - loss: 177.2397 - loglik: -1.7380e+02 - logprior: -3.2030e+00
Epoch 7/10
10/10 - 1s - loss: 175.2540 - loglik: -1.7181e+02 - logprior: -3.0186e+00
Epoch 8/10
10/10 - 1s - loss: 174.6461 - loglik: -1.7139e+02 - logprior: -2.8485e+00
Epoch 9/10
10/10 - 1s - loss: 174.1652 - loglik: -1.7105e+02 - logprior: -2.7762e+00
Epoch 10/10
10/10 - 1s - loss: 173.7078 - loglik: -1.7064e+02 - logprior: -2.7570e+00
Fitted a model with MAP estimate = -173.2531
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 3), (36, 2), (37, 1), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 108 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.2053 - loglik: -1.6993e+02 - logprior: -3.5250e+01
Epoch 2/2
10/10 - 1s - loss: 169.9409 - loglik: -1.5859e+02 - logprior: -1.1174e+01
Fitted a model with MAP estimate = -162.9658
expansions: []
discards: [ 0 45 93]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 190.6321 - loglik: -1.5878e+02 - logprior: -3.1826e+01
Epoch 2/2
10/10 - 1s - loss: 168.8831 - loglik: -1.5556e+02 - logprior: -1.3180e+01
Fitted a model with MAP estimate = -164.4219
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.5027 - loglik: -1.5549e+02 - logprior: -2.9988e+01
Epoch 2/10
10/10 - 1s - loss: 163.0047 - loglik: -1.5371e+02 - logprior: -9.1654e+00
Epoch 3/10
10/10 - 1s - loss: 156.4165 - loglik: -1.5210e+02 - logprior: -4.0207e+00
Epoch 4/10
10/10 - 1s - loss: 154.2268 - loglik: -1.5145e+02 - logprior: -2.3643e+00
Epoch 5/10
10/10 - 1s - loss: 152.4695 - loglik: -1.5042e+02 - logprior: -1.6472e+00
Epoch 6/10
10/10 - 1s - loss: 151.6646 - loglik: -1.4992e+02 - logprior: -1.3582e+00
Epoch 7/10
10/10 - 1s - loss: 151.2994 - loglik: -1.4973e+02 - logprior: -1.1830e+00
Epoch 8/10
10/10 - 1s - loss: 150.6233 - loglik: -1.4921e+02 - logprior: -1.0237e+00
Epoch 9/10
10/10 - 1s - loss: 150.8247 - loglik: -1.4953e+02 - logprior: -8.9255e-01
Fitted a model with MAP estimate = -150.0195
Time for alignment: 47.9561
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.6226 - loglik: -2.7669e+02 - logprior: -2.7911e+01
Epoch 2/10
10/10 - 1s - loss: 248.2908 - loglik: -2.4056e+02 - logprior: -7.7165e+00
Epoch 3/10
10/10 - 1s - loss: 209.2251 - loglik: -2.0481e+02 - logprior: -4.4128e+00
Epoch 4/10
10/10 - 1s - loss: 189.4113 - loglik: -1.8598e+02 - logprior: -3.4260e+00
Epoch 5/10
10/10 - 1s - loss: 179.8499 - loglik: -1.7650e+02 - logprior: -3.2468e+00
Epoch 6/10
10/10 - 1s - loss: 176.6926 - loglik: -1.7317e+02 - logprior: -3.1725e+00
Epoch 7/10
10/10 - 1s - loss: 175.0146 - loglik: -1.7162e+02 - logprior: -2.9264e+00
Epoch 8/10
10/10 - 1s - loss: 174.7313 - loglik: -1.7155e+02 - logprior: -2.7526e+00
Epoch 9/10
10/10 - 1s - loss: 174.2290 - loglik: -1.7112e+02 - logprior: -2.7308e+00
Epoch 10/10
10/10 - 1s - loss: 173.5617 - loglik: -1.7048e+02 - logprior: -2.7242e+00
Fitted a model with MAP estimate = -173.2021
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 205.6127 - loglik: -1.7026e+02 - logprior: -3.5332e+01
Epoch 2/2
10/10 - 1s - loss: 170.2840 - loglik: -1.5895e+02 - logprior: -1.1192e+01
Fitted a model with MAP estimate = -162.8851
expansions: []
discards: [ 0 92]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 190.3785 - loglik: -1.5852e+02 - logprior: -3.1833e+01
Epoch 2/2
10/10 - 1s - loss: 169.1153 - loglik: -1.5579e+02 - logprior: -1.3215e+01
Fitted a model with MAP estimate = -164.9217
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.4106 - loglik: -1.5638e+02 - logprior: -3.0020e+01
Epoch 2/10
10/10 - 1s - loss: 163.1116 - loglik: -1.5384e+02 - logprior: -9.1867e+00
Epoch 3/10
10/10 - 1s - loss: 156.1607 - loglik: -1.5186e+02 - logprior: -4.0519e+00
Epoch 4/10
10/10 - 1s - loss: 153.9353 - loglik: -1.5113e+02 - logprior: -2.4190e+00
Epoch 5/10
10/10 - 1s - loss: 152.2017 - loglik: -1.5005e+02 - logprior: -1.7417e+00
Epoch 6/10
10/10 - 1s - loss: 151.4032 - loglik: -1.4960e+02 - logprior: -1.4317e+00
Epoch 7/10
10/10 - 1s - loss: 150.9138 - loglik: -1.4932e+02 - logprior: -1.2572e+00
Epoch 8/10
10/10 - 1s - loss: 150.3568 - loglik: -1.4892e+02 - logprior: -1.0968e+00
Epoch 9/10
10/10 - 1s - loss: 150.2340 - loglik: -1.4894e+02 - logprior: -9.4775e-01
Epoch 10/10
10/10 - 1s - loss: 149.9906 - loglik: -1.4881e+02 - logprior: -8.2456e-01
Fitted a model with MAP estimate = -149.4999
Time for alignment: 48.6479
Computed alignments with likelihoods: ['-151.5288', '-150.0195', '-149.4999']
Best model has likelihood: -149.4999
time for generating output: 0.1737
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7827004219409283
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f67c025b9d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f698ba159d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca0999d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6993fe2eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706abd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942faf8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e0a5af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6ac736d9d0>, <__main__.SimpleDirichletPrior object at 0x7f694e134280>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.1707 - loglik: -1.4598e+02 - logprior: -5.1921e+00
Epoch 2/10
16/16 - 2s - loss: 128.0835 - loglik: -1.2635e+02 - logprior: -1.6738e+00
Epoch 3/10
16/16 - 2s - loss: 116.1302 - loglik: -1.1405e+02 - logprior: -1.8139e+00
Epoch 4/10
16/16 - 2s - loss: 111.0313 - loglik: -1.0885e+02 - logprior: -1.8665e+00
Epoch 5/10
16/16 - 2s - loss: 109.4074 - loglik: -1.0730e+02 - logprior: -1.8269e+00
Epoch 6/10
16/16 - 2s - loss: 108.7706 - loglik: -1.0672e+02 - logprior: -1.8000e+00
Epoch 7/10
16/16 - 2s - loss: 108.3208 - loglik: -1.0631e+02 - logprior: -1.7754e+00
Epoch 8/10
16/16 - 2s - loss: 108.0735 - loglik: -1.0608e+02 - logprior: -1.7557e+00
Epoch 9/10
16/16 - 2s - loss: 107.9337 - loglik: -1.0595e+02 - logprior: -1.7404e+00
Epoch 10/10
16/16 - 2s - loss: 107.7352 - loglik: -1.0576e+02 - logprior: -1.7340e+00
Fitted a model with MAP estimate = -107.5513
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (24, 6), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 114.9808 - loglik: -1.0848e+02 - logprior: -6.4317e+00
Epoch 2/2
16/16 - 2s - loss: 105.9697 - loglik: -1.0250e+02 - logprior: -3.2262e+00
Fitted a model with MAP estimate = -103.8656
expansions: [(0, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.7242 - loglik: -1.0192e+02 - logprior: -4.7503e+00
Epoch 2/2
16/16 - 2s - loss: 102.8117 - loglik: -1.0086e+02 - logprior: -1.7728e+00
Fitted a model with MAP estimate = -101.7891
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.0475 - loglik: -1.0268e+02 - logprior: -6.3238e+00
Epoch 2/10
16/16 - 2s - loss: 104.5548 - loglik: -1.0151e+02 - logprior: -2.9042e+00
Epoch 3/10
16/16 - 2s - loss: 102.1666 - loglik: -1.0020e+02 - logprior: -1.7223e+00
Epoch 4/10
16/16 - 2s - loss: 101.4881 - loglik: -9.9654e+01 - logprior: -1.4997e+00
Epoch 5/10
16/16 - 2s - loss: 100.2197 - loglik: -9.8376e+01 - logprior: -1.4934e+00
Epoch 6/10
16/16 - 2s - loss: 100.0290 - loglik: -9.8217e+01 - logprior: -1.4834e+00
Epoch 7/10
16/16 - 2s - loss: 99.7648 - loglik: -9.7997e+01 - logprior: -1.4516e+00
Epoch 8/10
16/16 - 2s - loss: 99.1008 - loglik: -9.7350e+01 - logprior: -1.4441e+00
Epoch 9/10
16/16 - 2s - loss: 99.6142 - loglik: -9.7889e+01 - logprior: -1.4245e+00
Fitted a model with MAP estimate = -98.8516
Time for alignment: 65.5927
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 151.1437 - loglik: -1.4595e+02 - logprior: -5.1951e+00
Epoch 2/10
16/16 - 2s - loss: 127.2068 - loglik: -1.2549e+02 - logprior: -1.6618e+00
Epoch 3/10
16/16 - 2s - loss: 116.6318 - loglik: -1.1452e+02 - logprior: -1.7960e+00
Epoch 4/10
16/16 - 2s - loss: 111.7140 - loglik: -1.0956e+02 - logprior: -1.8449e+00
Epoch 5/10
16/16 - 2s - loss: 109.4220 - loglik: -1.0734e+02 - logprior: -1.8130e+00
Epoch 6/10
16/16 - 2s - loss: 108.8921 - loglik: -1.0685e+02 - logprior: -1.7812e+00
Epoch 7/10
16/16 - 2s - loss: 108.5185 - loglik: -1.0652e+02 - logprior: -1.7475e+00
Epoch 8/10
16/16 - 2s - loss: 108.0649 - loglik: -1.0609e+02 - logprior: -1.7488e+00
Epoch 9/10
16/16 - 2s - loss: 108.0720 - loglik: -1.0609e+02 - logprior: -1.7364e+00
Fitted a model with MAP estimate = -107.6675
expansions: [(3, 1), (6, 1), (12, 1), (15, 2), (17, 1), (23, 2), (24, 6), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 115.0890 - loglik: -1.0862e+02 - logprior: -6.4048e+00
Epoch 2/2
16/16 - 2s - loss: 106.0725 - loglik: -1.0253e+02 - logprior: -3.3257e+00
Fitted a model with MAP estimate = -104.1097
expansions: [(0, 1)]
discards: [ 0 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 107.0718 - loglik: -1.0227e+02 - logprior: -4.7530e+00
Epoch 2/2
16/16 - 2s - loss: 102.8492 - loglik: -1.0089e+02 - logprior: -1.7742e+00
Fitted a model with MAP estimate = -101.7732
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 106.0659 - loglik: -1.0129e+02 - logprior: -4.7391e+00
Epoch 2/10
16/16 - 2s - loss: 102.3834 - loglik: -1.0049e+02 - logprior: -1.7617e+00
Epoch 3/10
16/16 - 2s - loss: 100.9086 - loglik: -9.9184e+01 - logprior: -1.5015e+00
Epoch 4/10
16/16 - 2s - loss: 100.0415 - loglik: -9.8250e+01 - logprior: -1.4624e+00
Epoch 5/10
16/16 - 2s - loss: 99.8461 - loglik: -9.8057e+01 - logprior: -1.4326e+00
Epoch 6/10
16/16 - 2s - loss: 99.2216 - loglik: -9.7467e+01 - logprior: -1.4129e+00
Epoch 7/10
16/16 - 2s - loss: 98.9695 - loglik: -9.7249e+01 - logprior: -1.3950e+00
Epoch 8/10
16/16 - 2s - loss: 99.0110 - loglik: -9.7307e+01 - logprior: -1.3765e+00
Fitted a model with MAP estimate = -98.2921
Time for alignment: 63.5314
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.1277 - loglik: -1.4593e+02 - logprior: -5.1941e+00
Epoch 2/10
16/16 - 2s - loss: 125.1303 - loglik: -1.2344e+02 - logprior: -1.6440e+00
Epoch 3/10
16/16 - 2s - loss: 115.5722 - loglik: -1.1359e+02 - logprior: -1.7039e+00
Epoch 4/10
16/16 - 2s - loss: 111.6398 - loglik: -1.0968e+02 - logprior: -1.7044e+00
Epoch 5/10
16/16 - 2s - loss: 109.8921 - loglik: -1.0797e+02 - logprior: -1.6719e+00
Epoch 6/10
16/16 - 2s - loss: 109.1950 - loglik: -1.0732e+02 - logprior: -1.6675e+00
Epoch 7/10
16/16 - 2s - loss: 109.2423 - loglik: -1.0738e+02 - logprior: -1.6465e+00
Fitted a model with MAP estimate = -108.6745
expansions: [(3, 1), (6, 1), (16, 3), (17, 2), (23, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.8368 - loglik: -1.0834e+02 - logprior: -6.4401e+00
Epoch 2/2
16/16 - 2s - loss: 105.5411 - loglik: -1.0209e+02 - logprior: -3.2613e+00
Fitted a model with MAP estimate = -103.8909
expansions: [(0, 1)]
discards: [ 0 18 32 33]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 106.7063 - loglik: -1.0189e+02 - logprior: -4.7633e+00
Epoch 2/2
16/16 - 2s - loss: 103.1270 - loglik: -1.0115e+02 - logprior: -1.7936e+00
Fitted a model with MAP estimate = -101.8276
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.2563 - loglik: -1.0287e+02 - logprior: -6.3438e+00
Epoch 2/10
16/16 - 2s - loss: 104.4290 - loglik: -1.0133e+02 - logprior: -2.9369e+00
Epoch 3/10
16/16 - 2s - loss: 102.3598 - loglik: -1.0037e+02 - logprior: -1.7490e+00
Epoch 4/10
16/16 - 2s - loss: 101.1987 - loglik: -9.9333e+01 - logprior: -1.5352e+00
Epoch 5/10
16/16 - 2s - loss: 100.4082 - loglik: -9.8528e+01 - logprior: -1.5288e+00
Epoch 6/10
16/16 - 2s - loss: 99.8986 - loglik: -9.8051e+01 - logprior: -1.5177e+00
Epoch 7/10
16/16 - 2s - loss: 99.4112 - loglik: -9.7582e+01 - logprior: -1.4983e+00
Epoch 8/10
16/16 - 2s - loss: 99.3347 - loglik: -9.7544e+01 - logprior: -1.4755e+00
Epoch 9/10
16/16 - 2s - loss: 99.2848 - loglik: -9.7508e+01 - logprior: -1.4628e+00
Epoch 10/10
16/16 - 2s - loss: 99.2861 - loglik: -9.7523e+01 - logprior: -1.4509e+00
Fitted a model with MAP estimate = -98.7039
Time for alignment: 60.9851
Computed alignments with likelihoods: ['-98.8516', '-98.2921', '-98.7039']
Best model has likelihood: -98.2921
time for generating output: 0.1376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.8998384491114702
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f616806f670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f615972edf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aec16f730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f6b16ab6550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706abd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942faf8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e0a5af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6169992f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6943eeb0d0>, <__main__.SimpleDirichletPrior object at 0x7f6ab652acd0>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 588.6393 - loglik: -5.8622e+02 - logprior: -1.9132e+00
Epoch 2/10
39/39 - 13s - loss: 534.8361 - loglik: -5.3154e+02 - logprior: -1.3672e+00
Epoch 3/10
39/39 - 13s - loss: 524.0905 - loglik: -5.1991e+02 - logprior: -1.4452e+00
Epoch 4/10
39/39 - 13s - loss: 519.0494 - loglik: -5.1496e+02 - logprior: -1.5287e+00
Epoch 5/10
39/39 - 13s - loss: 515.7474 - loglik: -5.1191e+02 - logprior: -1.6111e+00
Epoch 6/10
39/39 - 13s - loss: 513.9705 - loglik: -5.1044e+02 - logprior: -1.6458e+00
Epoch 7/10
39/39 - 13s - loss: 512.7868 - loglik: -5.0950e+02 - logprior: -1.6622e+00
Epoch 8/10
39/39 - 13s - loss: 511.9466 - loglik: -5.0886e+02 - logprior: -1.6661e+00
Epoch 9/10
39/39 - 13s - loss: 511.4206 - loglik: -5.0846e+02 - logprior: -1.6685e+00
Epoch 10/10
39/39 - 13s - loss: 510.6962 - loglik: -5.0780e+02 - logprior: -1.6625e+00
Fitted a model with MAP estimate = -492.3100
expansions: [(4, 1), (6, 1), (10, 1), (13, 1), (20, 3), (23, 3), (24, 1), (54, 1), (55, 2), (69, 1), (76, 7), (91, 1), (92, 1), (119, 1), (120, 3), (122, 3), (123, 2), (125, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 563.4744 - loglik: -5.6035e+02 - logprior: -2.7800e+00
Epoch 2/2
39/39 - 16s - loss: 524.1126 - loglik: -5.2079e+02 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -472.0541
expansions: [(32, 1), (94, 1), (113, 1), (114, 2), (115, 1), (146, 1), (147, 2), (149, 2), (150, 1), (151, 2)]
discards: [  2  24  25  67  96  97  98  99 117 118 159 160 161 162 163 164 165 166
 167 168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 535.1320 - loglik: -5.3262e+02 - logprior: -2.2098e+00
Epoch 2/2
39/39 - 15s - loss: 520.7352 - loglik: -5.1858e+02 - logprior: -1.1529e+00
Fitted a model with MAP estimate = -472.2334
expansions: [(5, 1), (27, 1), (94, 1), (142, 1)]
discards: [110 114 148]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 23s - loss: 477.3588 - loglik: -4.7553e+02 - logprior: -1.3042e+00
Epoch 2/10
51/51 - 19s - loss: 465.5786 - loglik: -4.6285e+02 - logprior: -9.5720e-01
Epoch 3/10
51/51 - 19s - loss: 460.5298 - loglik: -4.5693e+02 - logprior: -9.3854e-01
Epoch 4/10
51/51 - 19s - loss: 456.8260 - loglik: -4.5319e+02 - logprior: -9.0517e-01
Epoch 5/10
51/51 - 19s - loss: 454.3768 - loglik: -4.5103e+02 - logprior: -8.6704e-01
Epoch 6/10
51/51 - 19s - loss: 452.8445 - loglik: -4.4972e+02 - logprior: -8.3479e-01
Epoch 7/10
51/51 - 19s - loss: 450.3070 - loglik: -4.4751e+02 - logprior: -8.0238e-01
Epoch 8/10
51/51 - 19s - loss: 450.2931 - loglik: -4.4778e+02 - logprior: -7.6894e-01
Epoch 9/10
51/51 - 19s - loss: 449.4905 - loglik: -4.4718e+02 - logprior: -7.2571e-01
Epoch 10/10
51/51 - 19s - loss: 447.1082 - loglik: -4.4484e+02 - logprior: -6.9027e-01
Fitted a model with MAP estimate = -446.0135
Time for alignment: 490.5278
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 587.1402 - loglik: -5.8471e+02 - logprior: -1.9291e+00
Epoch 2/10
39/39 - 13s - loss: 532.9462 - loglik: -5.3009e+02 - logprior: -1.4455e+00
Epoch 3/10
39/39 - 13s - loss: 521.7499 - loglik: -5.1809e+02 - logprior: -1.5727e+00
Epoch 4/10
39/39 - 13s - loss: 517.8244 - loglik: -5.1397e+02 - logprior: -1.5559e+00
Epoch 5/10
39/39 - 13s - loss: 515.5808 - loglik: -5.1191e+02 - logprior: -1.5803e+00
Epoch 6/10
39/39 - 13s - loss: 514.0416 - loglik: -5.1065e+02 - logprior: -1.5980e+00
Epoch 7/10
39/39 - 13s - loss: 512.9847 - loglik: -5.0978e+02 - logprior: -1.6175e+00
Epoch 8/10
39/39 - 13s - loss: 512.0338 - loglik: -5.0897e+02 - logprior: -1.6293e+00
Epoch 9/10
39/39 - 13s - loss: 511.3719 - loglik: -5.0839e+02 - logprior: -1.6425e+00
Epoch 10/10
39/39 - 13s - loss: 510.4511 - loglik: -5.0750e+02 - logprior: -1.6537e+00
Fitted a model with MAP estimate = -491.7629
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (25, 1), (28, 1), (31, 1), (56, 1), (57, 2), (71, 2), (74, 1), (77, 3), (91, 1), (114, 2), (121, 1), (123, 6), (124, 1), (126, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 564.4379 - loglik: -5.6138e+02 - logprior: -2.6939e+00
Epoch 2/2
39/39 - 15s - loss: 522.9151 - loglik: -5.1950e+02 - logprior: -1.7361e+00
Fitted a model with MAP estimate = -471.3326
expansions: [(2, 1), (15, 1), (25, 2), (87, 1), (92, 2), (148, 1), (149, 2), (153, 3)]
discards: [  0  16  66  88 134 156 157 158 159 160 161 162 163 164 165 166 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 536.7740 - loglik: -5.3335e+02 - logprior: -3.1025e+00
Epoch 2/2
39/39 - 15s - loss: 522.8887 - loglik: -5.2025e+02 - logprior: -1.4188e+00
Fitted a model with MAP estimate = -473.3810
expansions: [(0, 1), (145, 5), (157, 1), (165, 8)]
discards: [  0 153]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 25s - loss: 476.5290 - loglik: -4.7473e+02 - logprior: -1.3163e+00
Epoch 2/10
51/51 - 21s - loss: 464.5658 - loglik: -4.6194e+02 - logprior: -1.1493e+00
Epoch 3/10
51/51 - 21s - loss: 457.6555 - loglik: -4.5418e+02 - logprior: -1.1332e+00
Epoch 4/10
51/51 - 20s - loss: 455.5930 - loglik: -4.5192e+02 - logprior: -1.0988e+00
Epoch 5/10
51/51 - 21s - loss: 452.9680 - loglik: -4.4947e+02 - logprior: -1.0777e+00
Epoch 6/10
51/51 - 21s - loss: 449.9832 - loglik: -4.4666e+02 - logprior: -1.0527e+00
Epoch 7/10
51/51 - 20s - loss: 450.1920 - loglik: -4.4725e+02 - logprior: -1.0060e+00
Fitted a model with MAP estimate = -446.9766
Time for alignment: 441.2403
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 587.4664 - loglik: -5.8504e+02 - logprior: -1.9267e+00
Epoch 2/10
39/39 - 13s - loss: 539.8300 - loglik: -5.3680e+02 - logprior: -1.2418e+00
Epoch 3/10
39/39 - 13s - loss: 527.9720 - loglik: -5.2405e+02 - logprior: -1.3434e+00
Epoch 4/10
39/39 - 13s - loss: 522.4564 - loglik: -5.1865e+02 - logprior: -1.4564e+00
Epoch 5/10
39/39 - 13s - loss: 519.0903 - loglik: -5.1555e+02 - logprior: -1.5266e+00
Epoch 6/10
39/39 - 13s - loss: 517.3219 - loglik: -5.1391e+02 - logprior: -1.5668e+00
Epoch 7/10
39/39 - 13s - loss: 515.5958 - loglik: -5.1221e+02 - logprior: -1.5741e+00
Epoch 8/10
39/39 - 13s - loss: 514.4219 - loglik: -5.1113e+02 - logprior: -1.5981e+00
Epoch 9/10
39/39 - 13s - loss: 513.4219 - loglik: -5.1027e+02 - logprior: -1.6154e+00
Epoch 10/10
39/39 - 13s - loss: 512.9410 - loglik: -5.0990e+02 - logprior: -1.6153e+00
Fitted a model with MAP estimate = -494.8230
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 3), (29, 1), (31, 2), (55, 1), (56, 1), (57, 2), (70, 2), (77, 12), (78, 1), (82, 1), (88, 2), (93, 1), (123, 8), (126, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 563.7599 - loglik: -5.6074e+02 - logprior: -2.6840e+00
Epoch 2/2
39/39 - 17s - loss: 524.9466 - loglik: -5.2164e+02 - logprior: -1.8594e+00
Fitted a model with MAP estimate = -473.1516
expansions: [(2, 1), (15, 1), (25, 6), (100, 1), (105, 1)]
discards: [  0  16  26  27  28  29  67 106 107 108 109 110 111 112 113 114 117 162
 163 167 168 169 170 171 172 173 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 538.5814 - loglik: -5.3519e+02 - logprior: -3.0674e+00
Epoch 2/2
39/39 - 15s - loss: 523.8853 - loglik: -5.2138e+02 - logprior: -1.3511e+00
Fitted a model with MAP estimate = -473.6156
expansions: [(0, 1), (150, 2), (152, 2), (153, 6), (154, 3), (155, 2), (157, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 23s - loss: 478.3989 - loglik: -4.7660e+02 - logprior: -1.2760e+00
Epoch 2/10
51/51 - 20s - loss: 464.0996 - loglik: -4.6123e+02 - logprior: -1.0277e+00
Epoch 3/10
51/51 - 20s - loss: 460.1623 - loglik: -4.5645e+02 - logprior: -1.0337e+00
Epoch 4/10
51/51 - 20s - loss: 456.5081 - loglik: -4.5269e+02 - logprior: -1.0160e+00
Epoch 5/10
51/51 - 20s - loss: 453.7822 - loglik: -4.5024e+02 - logprior: -9.8631e-01
Epoch 6/10
51/51 - 20s - loss: 451.3885 - loglik: -4.4807e+02 - logprior: -9.7009e-01
Epoch 7/10
51/51 - 20s - loss: 449.8599 - loglik: -4.4685e+02 - logprior: -9.3433e-01
Epoch 8/10
51/51 - 20s - loss: 449.6494 - loglik: -4.4691e+02 - logprior: -9.0554e-01
Epoch 9/10
51/51 - 20s - loss: 447.7962 - loglik: -4.4528e+02 - logprior: -8.6036e-01
Epoch 10/10
51/51 - 20s - loss: 446.5109 - loglik: -4.4401e+02 - logprior: -8.2387e-01
Fitted a model with MAP estimate = -444.4518
Time for alignment: 499.0061
Computed alignments with likelihoods: ['-446.0135', '-446.9766', '-444.4518']
Best model has likelihood: -444.4518
time for generating output: 0.2859
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6414602346805737
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f68d810aeb0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f68d810aa30>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d810af10>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070a60>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80702e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f68d8070be0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070af0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070130>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070040>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80706a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070670>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80705e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070f40>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070700>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070100>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d80709d0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f68d8070b20> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f69e8979970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f61594a5100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad6c51c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f61606c50d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f68d80afee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d8070df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b346f2880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d90d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af4f33490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb17bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b166bb160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f52b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9a5d610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac950ef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ab5ff82e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ad2b532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb23adf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd284400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a82260ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9d9c220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1686c610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aca2f08e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6adb3942b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ae39c9880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2fca5670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f68d814fe20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05d81c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be3d0fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd736df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac9898910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b16df99a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e9b2e040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69e84d6190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f4ac6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a9c15e130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac8fac070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f4e7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4a55160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69d7f77c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1f7c0490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a5249d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5b43f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69be619250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a83f24250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b1671bb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b2f82c460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6af496df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69691bb7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696924bf70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad02e550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a4b86940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f696086a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a7989c730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69722b4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac87f6af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942ededc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b165b86a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6a832850a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6b05bad700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c548100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694ea841f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f677c2aa400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69a470df40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f617047f4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f67c0211af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6afd755340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6956efed60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6ac92eb430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e093dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69b5865c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f699429a610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f69941d4d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6aad9447f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6158798d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61597dd040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f61706abd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6942faf8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694e0a5af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f6169992f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f694dc20730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f6b4cd263a0> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f69ecceb430>, <function make_default_emission_matrix at 0x7f69ecceb430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f6957064190>, <__main__.SimpleDirichletPrior object at 0x7f6af4f31070>]
 , kernel_dim : [25, 5] , num_rate_matrices : 5 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.4034 - loglik: -4.8500e+02 - logprior: -6.3473e+00
Epoch 2/10
14/14 - 5s - loss: 434.4806 - loglik: -4.3239e+02 - logprior: -1.6458e+00
Epoch 3/10
14/14 - 5s - loss: 401.0229 - loglik: -3.9842e+02 - logprior: -1.7619e+00
Epoch 4/10
14/14 - 5s - loss: 387.6860 - loglik: -3.8499e+02 - logprior: -1.9347e+00
Epoch 5/10
14/14 - 5s - loss: 382.7166 - loglik: -3.8001e+02 - logprior: -2.0097e+00
Epoch 6/10
14/14 - 5s - loss: 380.8970 - loglik: -3.7827e+02 - logprior: -2.0076e+00
Epoch 7/10
14/14 - 5s - loss: 380.9431 - loglik: -3.7842e+02 - logprior: -1.9558e+00
Fitted a model with MAP estimate = -378.9108
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (61, 1), (69, 1), (71, 1), (74, 1), (82, 1), (102, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 386.6563 - loglik: -3.8161e+02 - logprior: -4.8496e+00
Epoch 2/2
29/29 - 8s - loss: 366.9823 - loglik: -3.6480e+02 - logprior: -1.5681e+00
Fitted a model with MAP estimate = -362.8546
expansions: [(125, 1), (129, 1)]
discards: [ 41 147 166]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 14s - loss: 372.8729 - loglik: -3.6908e+02 - logprior: -3.6576e+00
Epoch 2/2
29/29 - 8s - loss: 365.0545 - loglik: -3.6334e+02 - logprior: -1.2195e+00
Fitted a model with MAP estimate = -362.0166
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 371.2475 - loglik: -3.6759e+02 - logprior: -3.5176e+00
Epoch 2/10
29/29 - 8s - loss: 365.0247 - loglik: -3.6352e+02 - logprior: -1.0538e+00
Epoch 3/10
29/29 - 8s - loss: 361.2751 - loglik: -3.5958e+02 - logprior: -9.8272e-01
Epoch 4/10
29/29 - 8s - loss: 359.6326 - loglik: -3.5797e+02 - logprior: -9.0886e-01
Epoch 5/10
29/29 - 8s - loss: 358.7502 - loglik: -3.5726e+02 - logprior: -8.0918e-01
Epoch 6/10
29/29 - 8s - loss: 357.8230 - loglik: -3.5650e+02 - logprior: -7.1107e-01
Epoch 7/10
29/29 - 8s - loss: 357.9368 - loglik: -3.5677e+02 - logprior: -6.1171e-01
Fitted a model with MAP estimate = -356.6939
Time for alignment: 165.3956
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 492.1122 - loglik: -4.8570e+02 - logprior: -6.3501e+00
Epoch 2/10
14/14 - 5s - loss: 435.6103 - loglik: -4.3353e+02 - logprior: -1.6436e+00
Epoch 3/10
14/14 - 5s - loss: 402.1570 - loglik: -3.9945e+02 - logprior: -1.7723e+00
Epoch 4/10
14/14 - 5s - loss: 389.0455 - loglik: -3.8621e+02 - logprior: -1.8570e+00
Epoch 5/10
14/14 - 5s - loss: 384.2791 - loglik: -3.8133e+02 - logprior: -1.9401e+00
Epoch 6/10
14/14 - 5s - loss: 381.3414 - loglik: -3.7857e+02 - logprior: -1.9333e+00
Epoch 7/10
14/14 - 5s - loss: 380.1974 - loglik: -3.7764e+02 - logprior: -1.8791e+00
Epoch 8/10
14/14 - 5s - loss: 379.8931 - loglik: -3.7739e+02 - logprior: -1.8810e+00
Epoch 9/10
14/14 - 5s - loss: 379.0403 - loglik: -3.7651e+02 - logprior: -1.9193e+00
Epoch 10/10
14/14 - 5s - loss: 377.9940 - loglik: -3.7543e+02 - logprior: -1.9770e+00
Fitted a model with MAP estimate = -377.0615
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (30, 1), (33, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 1), (72, 1), (85, 2), (89, 1), (102, 5), (117, 1), (120, 2), (128, 1), (129, 1), (130, 2), (131, 2), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 386.7253 - loglik: -3.8159e+02 - logprior: -4.9457e+00
Epoch 2/2
29/29 - 8s - loss: 363.6674 - loglik: -3.6142e+02 - logprior: -1.6034e+00
Fitted a model with MAP estimate = -359.7471
expansions: [(126, 1), (130, 1)]
discards: [104 148 167 169]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 370.6797 - loglik: -3.6686e+02 - logprior: -3.6827e+00
Epoch 2/2
29/29 - 8s - loss: 361.8148 - loglik: -3.6000e+02 - logprior: -1.2638e+00
Fitted a model with MAP estimate = -358.8974
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 368.4528 - loglik: -3.6476e+02 - logprior: -3.5475e+00
Epoch 2/10
29/29 - 8s - loss: 362.2074 - loglik: -3.6061e+02 - logprior: -1.0796e+00
Epoch 3/10
29/29 - 8s - loss: 358.6777 - loglik: -3.5684e+02 - logprior: -1.0251e+00
Epoch 4/10
29/29 - 8s - loss: 356.4034 - loglik: -3.5461e+02 - logprior: -9.5255e-01
Epoch 5/10
29/29 - 8s - loss: 354.6953 - loglik: -3.5309e+02 - logprior: -8.5426e-01
Epoch 6/10
29/29 - 8s - loss: 355.9119 - loglik: -3.5448e+02 - logprior: -7.6845e-01
Fitted a model with MAP estimate = -353.8309
Time for alignment: 173.0171
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 491.8034 - loglik: -4.8540e+02 - logprior: -6.3502e+00
Epoch 2/10
14/14 - 5s - loss: 437.0354 - loglik: -4.3494e+02 - logprior: -1.6462e+00
Epoch 3/10
14/14 - 5s - loss: 400.4514 - loglik: -3.9776e+02 - logprior: -1.7952e+00
Epoch 4/10
14/14 - 5s - loss: 389.3208 - loglik: -3.8622e+02 - logprior: -1.9098e+00
Epoch 5/10
14/14 - 5s - loss: 384.2489 - loglik: -3.8126e+02 - logprior: -1.9177e+00
Epoch 6/10
14/14 - 5s - loss: 382.6197 - loglik: -3.7989e+02 - logprior: -1.9538e+00
Epoch 7/10
14/14 - 5s - loss: 380.4190 - loglik: -3.7788e+02 - logprior: -1.9718e+00
Epoch 8/10
14/14 - 5s - loss: 379.4405 - loglik: -3.7694e+02 - logprior: -1.9750e+00
Epoch 9/10
14/14 - 5s - loss: 379.1842 - loglik: -3.7672e+02 - logprior: -1.9708e+00
Epoch 10/10
14/14 - 5s - loss: 378.5220 - loglik: -3.7606e+02 - logprior: -1.9982e+00
Fitted a model with MAP estimate = -377.8745
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (71, 3), (72, 2), (102, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 390.4847 - loglik: -3.8536e+02 - logprior: -4.9414e+00
Epoch 2/2
29/29 - 8s - loss: 365.7127 - loglik: -3.6338e+02 - logprior: -1.6825e+00
Fitted a model with MAP estimate = -362.0268
expansions: [(104, 1), (126, 1), (130, 1), (166, 1)]
discards: [ 41  92  93 148]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 372.6377 - loglik: -3.6883e+02 - logprior: -3.6639e+00
Epoch 2/2
29/29 - 8s - loss: 363.7035 - loglik: -3.6182e+02 - logprior: -1.2572e+00
Fitted a model with MAP estimate = -360.7012
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 13s - loss: 370.6523 - loglik: -3.6697e+02 - logprior: -3.5298e+00
Epoch 2/10
29/29 - 8s - loss: 363.2157 - loglik: -3.6165e+02 - logprior: -1.0892e+00
Epoch 3/10
29/29 - 8s - loss: 360.0827 - loglik: -3.5833e+02 - logprior: -1.0354e+00
Epoch 4/10
29/29 - 8s - loss: 358.6601 - loglik: -3.5693e+02 - logprior: -9.6483e-01
Epoch 5/10
29/29 - 8s - loss: 358.2081 - loglik: -3.5662e+02 - logprior: -8.7777e-01
Epoch 6/10
29/29 - 8s - loss: 356.7466 - loglik: -3.5533e+02 - logprior: -7.8402e-01
Epoch 7/10
29/29 - 8s - loss: 356.0766 - loglik: -3.5478e+02 - logprior: -6.9793e-01
Epoch 8/10
29/29 - 8s - loss: 355.7236 - loglik: -3.5455e+02 - logprior: -6.1181e-01
Epoch 9/10
29/29 - 8s - loss: 356.5126 - loglik: -3.5545e+02 - logprior: -5.2213e-01
Fitted a model with MAP estimate = -354.7437
Time for alignment: 197.4767
Computed alignments with likelihoods: ['-356.6939', '-353.8309', '-354.7437']
Best model has likelihood: -353.8309
time for generating output: 0.2948
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.7953289899798072
