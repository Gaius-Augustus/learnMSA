Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac76dfbd00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76dfb850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac76dfb8b0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76dfbf40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76dfbee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac76dfbfa0>, <__main__.SimpleDirichletPrior object at 0x7faa28544490>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.5251 - loglik: -5.3341e+02 - logprior: -4.9093e+00
Epoch 2/10
25/25 - 8s - loss: 427.7094 - loglik: -4.2533e+02 - logprior: -1.8563e+00
Epoch 3/10
25/25 - 8s - loss: 405.0505 - loglik: -4.0214e+02 - logprior: -2.0562e+00
Epoch 4/10
25/25 - 8s - loss: 402.9415 - loglik: -4.0023e+02 - logprior: -1.9449e+00
Epoch 5/10
25/25 - 8s - loss: 401.2211 - loglik: -3.9853e+02 - logprior: -1.9540e+00
Epoch 6/10
25/25 - 8s - loss: 401.2553 - loglik: -3.9858e+02 - logprior: -1.9429e+00
Fitted a model with MAP estimate = -399.4292
expansions: [(9, 3), (10, 1), (11, 2), (12, 1), (31, 1), (32, 1), (33, 3), (34, 2), (47, 1), (48, 1), (58, 1), (60, 1), (62, 2), (66, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (99, 1), (102, 1), (113, 2), (115, 1), (124, 2), (127, 2), (138, 1), (139, 2), (140, 1), (150, 1), (160, 1), (162, 2), (167, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 404.4831 - loglik: -3.9745e+02 - logprior: -6.8758e+00
Epoch 2/2
25/25 - 9s - loss: 386.9713 - loglik: -3.8389e+02 - logprior: -2.6251e+00
Fitted a model with MAP estimate = -381.5387
expansions: [(0, 3)]
discards: [  0   9  42  46  81 106 143 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 391.6977 - loglik: -3.8712e+02 - logprior: -4.4354e+00
Epoch 2/2
25/25 - 9s - loss: 384.0946 - loglik: -3.8308e+02 - logprior: -5.2398e-01
Fitted a model with MAP estimate = -379.5670
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 395.2407 - loglik: -3.8880e+02 - logprior: -6.3525e+00
Epoch 2/10
25/25 - 9s - loss: 384.7728 - loglik: -3.8287e+02 - logprior: -1.4138e+00
Epoch 3/10
25/25 - 9s - loss: 379.6551 - loglik: -3.7893e+02 - logprior: 0.1997
Epoch 4/10
25/25 - 9s - loss: 378.4160 - loglik: -3.7769e+02 - logprior: 0.2763
Epoch 5/10
25/25 - 9s - loss: 377.3403 - loglik: -3.7684e+02 - logprior: 0.3850
Epoch 6/10
25/25 - 9s - loss: 376.3148 - loglik: -3.7599e+02 - logprior: 0.4976
Epoch 7/10
25/25 - 9s - loss: 374.7516 - loglik: -3.7459e+02 - logprior: 0.6178
Epoch 8/10
25/25 - 9s - loss: 376.0128 - loglik: -3.7602e+02 - logprior: 0.7498
Fitted a model with MAP estimate = -374.1237
Time for alignment: 200.6300
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 535.7109 - loglik: -5.3076e+02 - logprior: -4.8680e+00
Epoch 2/10
25/25 - 8s - loss: 430.2051 - loglik: -4.2817e+02 - logprior: -1.5978e+00
Epoch 3/10
25/25 - 8s - loss: 407.7482 - loglik: -4.0510e+02 - logprior: -1.7262e+00
Epoch 4/10
25/25 - 8s - loss: 403.2860 - loglik: -4.0065e+02 - logprior: -1.6389e+00
Epoch 5/10
25/25 - 8s - loss: 403.1297 - loglik: -4.0063e+02 - logprior: -1.5879e+00
Epoch 6/10
25/25 - 8s - loss: 400.8684 - loglik: -3.9843e+02 - logprior: -1.5892e+00
Epoch 7/10
25/25 - 8s - loss: 401.7040 - loglik: -3.9932e+02 - logprior: -1.5848e+00
Fitted a model with MAP estimate = -399.8088
expansions: [(9, 5), (10, 2), (31, 3), (32, 2), (34, 2), (47, 3), (57, 2), (59, 1), (62, 1), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (96, 1), (102, 1), (112, 3), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 403.1213 - loglik: -3.9580e+02 - logprior: -6.8979e+00
Epoch 2/2
25/25 - 9s - loss: 385.3135 - loglik: -3.8189e+02 - logprior: -2.6782e+00
Fitted a model with MAP estimate = -380.2517
expansions: [(0, 3)]
discards: [  0  11  42  45  62  82 108 144 178 207]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 392.0720 - loglik: -3.8739e+02 - logprior: -4.3941e+00
Epoch 2/2
25/25 - 9s - loss: 381.3907 - loglik: -3.8035e+02 - logprior: -4.9089e-01
Fitted a model with MAP estimate = -379.4101
expansions: []
discards: [  0   2 154]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 393.2336 - loglik: -3.8664e+02 - logprior: -6.3186e+00
Epoch 2/10
25/25 - 9s - loss: 384.7193 - loglik: -3.8274e+02 - logprior: -1.3577e+00
Epoch 3/10
25/25 - 9s - loss: 380.8467 - loglik: -3.8023e+02 - logprior: 0.2609
Epoch 4/10
25/25 - 9s - loss: 377.1223 - loglik: -3.7649e+02 - logprior: 0.3295
Epoch 5/10
25/25 - 9s - loss: 376.6837 - loglik: -3.7620e+02 - logprior: 0.4326
Epoch 6/10
25/25 - 9s - loss: 376.8130 - loglik: -3.7653e+02 - logprior: 0.5686
Fitted a model with MAP estimate = -374.9835
Time for alignment: 189.2867
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.3772 - loglik: -5.3346e+02 - logprior: -4.8613e+00
Epoch 2/10
25/25 - 8s - loss: 429.0069 - loglik: -4.2689e+02 - logprior: -1.7541e+00
Epoch 3/10
25/25 - 8s - loss: 407.6827 - loglik: -4.0483e+02 - logprior: -2.0614e+00
Epoch 4/10
25/25 - 8s - loss: 404.5887 - loglik: -4.0186e+02 - logprior: -1.9380e+00
Epoch 5/10
25/25 - 8s - loss: 402.4442 - loglik: -3.9977e+02 - logprior: -1.9038e+00
Epoch 6/10
25/25 - 8s - loss: 403.0584 - loglik: -4.0040e+02 - logprior: -1.8975e+00
Fitted a model with MAP estimate = -401.1994
expansions: [(9, 3), (10, 1), (11, 2), (28, 1), (31, 3), (32, 2), (34, 2), (35, 1), (46, 2), (56, 1), (57, 1), (59, 1), (62, 1), (75, 1), (80, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (102, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (140, 1), (150, 1), (159, 1), (168, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 408.2440 - loglik: -4.0132e+02 - logprior: -6.8879e+00
Epoch 2/2
25/25 - 9s - loss: 389.5848 - loglik: -3.8684e+02 - logprior: -2.6650e+00
Fitted a model with MAP estimate = -383.4871
expansions: [(0, 3), (86, 1)]
discards: [  0   9  42  46  62 107 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 391.1270 - loglik: -3.8644e+02 - logprior: -4.3974e+00
Epoch 2/2
25/25 - 10s - loss: 381.4045 - loglik: -3.8009e+02 - logprior: -5.5187e-01
Fitted a model with MAP estimate = -378.4189
expansions: []
discards: [ 0  2 81]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 395.5558 - loglik: -3.8905e+02 - logprior: -6.3810e+00
Epoch 2/10
25/25 - 9s - loss: 384.1321 - loglik: -3.8251e+02 - logprior: -1.3242e+00
Epoch 3/10
25/25 - 9s - loss: 381.5400 - loglik: -3.8118e+02 - logprior: 0.1724
Epoch 4/10
25/25 - 9s - loss: 375.9191 - loglik: -3.7544e+02 - logprior: 0.2415
Epoch 5/10
25/25 - 9s - loss: 378.1245 - loglik: -3.7766e+02 - logprior: 0.3396
Fitted a model with MAP estimate = -375.5780
Time for alignment: 174.8387
Computed alignments with likelihoods: ['-374.1237', '-374.9835', '-375.5780']
Best model has likelihood: -374.1237
time for generating output: 0.2677
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9194832231202548
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac76dfb8b0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76dfbf40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac76dfbd00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76dfb850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac33063040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac19fb3100>, <__main__.SimpleDirichletPrior object at 0x7fac337a23d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.4767 - loglik: -3.8629e+02 - logprior: -2.0139e+01
Epoch 2/10
10/10 - 2s - loss: 341.1546 - loglik: -3.3650e+02 - logprior: -4.6442e+00
Epoch 3/10
10/10 - 2s - loss: 287.5217 - loglik: -2.8497e+02 - logprior: -2.5093e+00
Epoch 4/10
10/10 - 2s - loss: 255.5610 - loglik: -2.5301e+02 - logprior: -2.3291e+00
Epoch 5/10
10/10 - 2s - loss: 244.3059 - loglik: -2.4173e+02 - logprior: -2.1689e+00
Epoch 6/10
10/10 - 2s - loss: 240.3931 - loglik: -2.3820e+02 - logprior: -1.8211e+00
Epoch 7/10
10/10 - 2s - loss: 237.5377 - loglik: -2.3554e+02 - logprior: -1.6479e+00
Epoch 8/10
10/10 - 2s - loss: 236.2823 - loglik: -2.3433e+02 - logprior: -1.6158e+00
Epoch 9/10
10/10 - 2s - loss: 235.6615 - loglik: -2.3368e+02 - logprior: -1.6327e+00
Epoch 10/10
10/10 - 2s - loss: 234.6320 - loglik: -2.3264e+02 - logprior: -1.6279e+00
Fitted a model with MAP estimate = -234.2307
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.5261 - loglik: -2.3195e+02 - logprior: -2.6458e+01
Epoch 2/2
10/10 - 2s - loss: 219.9697 - loglik: -2.1238e+02 - logprior: -7.4586e+00
Fitted a model with MAP estimate = -211.8263
expansions: []
discards: [ 59  62 136]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 226.4964 - loglik: -2.0786e+02 - logprior: -1.8555e+01
Epoch 2/2
10/10 - 2s - loss: 209.3162 - loglik: -2.0507e+02 - logprior: -4.0104e+00
Fitted a model with MAP estimate = -205.9281
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 224.1020 - loglik: -2.0653e+02 - logprior: -1.7479e+01
Epoch 2/10
10/10 - 2s - loss: 208.6028 - loglik: -2.0474e+02 - logprior: -3.6727e+00
Epoch 3/10
10/10 - 2s - loss: 203.8095 - loglik: -2.0240e+02 - logprior: -1.0591e+00
Epoch 4/10
10/10 - 2s - loss: 203.2449 - loglik: -2.0262e+02 - logprior: -1.8022e-01
Epoch 5/10
10/10 - 2s - loss: 201.0186 - loglik: -2.0080e+02 - logprior: 0.2500
Epoch 6/10
10/10 - 2s - loss: 200.2203 - loglik: -2.0033e+02 - logprior: 0.5755
Epoch 7/10
10/10 - 2s - loss: 200.4045 - loglik: -2.0078e+02 - logprior: 0.8327
Fitted a model with MAP estimate = -199.2970
Time for alignment: 63.8971
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.1922 - loglik: -3.8599e+02 - logprior: -2.0139e+01
Epoch 2/10
10/10 - 2s - loss: 342.3262 - loglik: -3.3766e+02 - logprior: -4.6473e+00
Epoch 3/10
10/10 - 2s - loss: 289.9183 - loglik: -2.8737e+02 - logprior: -2.5094e+00
Epoch 4/10
10/10 - 2s - loss: 255.1398 - loglik: -2.5260e+02 - logprior: -2.3442e+00
Epoch 5/10
10/10 - 2s - loss: 242.3537 - loglik: -2.3965e+02 - logprior: -2.3058e+00
Epoch 6/10
10/10 - 2s - loss: 238.6811 - loglik: -2.3620e+02 - logprior: -2.1058e+00
Epoch 7/10
10/10 - 2s - loss: 236.1879 - loglik: -2.3392e+02 - logprior: -1.9257e+00
Epoch 8/10
10/10 - 2s - loss: 235.4847 - loglik: -2.3334e+02 - logprior: -1.8163e+00
Epoch 9/10
10/10 - 2s - loss: 234.3699 - loglik: -2.3227e+02 - logprior: -1.7592e+00
Epoch 10/10
10/10 - 2s - loss: 233.9630 - loglik: -2.3185e+02 - logprior: -1.7515e+00
Fitted a model with MAP estimate = -233.5060
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (22, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (62, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 257.0739 - loglik: -2.3028e+02 - logprior: -2.6499e+01
Epoch 2/2
10/10 - 2s - loss: 218.8840 - loglik: -2.1105e+02 - logprior: -7.4779e+00
Fitted a model with MAP estimate = -211.2122
expansions: []
discards: [ 59  62 136]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 226.2957 - loglik: -2.0763e+02 - logprior: -1.8564e+01
Epoch 2/2
10/10 - 2s - loss: 209.7314 - loglik: -2.0548e+02 - logprior: -4.0091e+00
Fitted a model with MAP estimate = -205.9890
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.5295 - loglik: -2.0705e+02 - logprior: -1.7452e+01
Epoch 2/10
10/10 - 2s - loss: 208.0206 - loglik: -2.0428e+02 - logprior: -3.6298e+00
Epoch 3/10
10/10 - 2s - loss: 205.2021 - loglik: -2.0393e+02 - logprior: -1.0206e+00
Epoch 4/10
10/10 - 2s - loss: 201.7251 - loglik: -2.0120e+02 - logprior: -1.4745e-01
Epoch 5/10
10/10 - 2s - loss: 201.5934 - loglik: -2.0142e+02 - logprior: 0.2752
Epoch 6/10
10/10 - 2s - loss: 200.3051 - loglik: -2.0043e+02 - logprior: 0.5944
Epoch 7/10
10/10 - 2s - loss: 200.4183 - loglik: -2.0081e+02 - logprior: 0.8616
Fitted a model with MAP estimate = -199.2606
Time for alignment: 63.2356
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.4469 - loglik: -3.8627e+02 - logprior: -2.0137e+01
Epoch 2/10
10/10 - 2s - loss: 342.1972 - loglik: -3.3754e+02 - logprior: -4.6420e+00
Epoch 3/10
10/10 - 2s - loss: 290.1848 - loglik: -2.8761e+02 - logprior: -2.5297e+00
Epoch 4/10
10/10 - 2s - loss: 255.8868 - loglik: -2.5331e+02 - logprior: -2.4140e+00
Epoch 5/10
10/10 - 2s - loss: 244.0085 - loglik: -2.4127e+02 - logprior: -2.4054e+00
Epoch 6/10
10/10 - 2s - loss: 239.8741 - loglik: -2.3728e+02 - logprior: -2.2166e+00
Epoch 7/10
10/10 - 2s - loss: 237.1746 - loglik: -2.3478e+02 - logprior: -2.0412e+00
Epoch 8/10
10/10 - 2s - loss: 236.0777 - loglik: -2.3374e+02 - logprior: -1.9709e+00
Epoch 9/10
10/10 - 2s - loss: 234.7056 - loglik: -2.3239e+02 - logprior: -1.9383e+00
Epoch 10/10
10/10 - 2s - loss: 234.9261 - loglik: -2.3262e+02 - logprior: -1.9075e+00
Fitted a model with MAP estimate = -234.0981
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (71, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.4907 - loglik: -2.3189e+02 - logprior: -2.6511e+01
Epoch 2/2
10/10 - 2s - loss: 219.2159 - loglik: -2.1157e+02 - logprior: -7.4385e+00
Fitted a model with MAP estimate = -211.3921
expansions: []
discards: [ 62 135]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.5246 - loglik: -2.0891e+02 - logprior: -1.8552e+01
Epoch 2/2
10/10 - 2s - loss: 209.4306 - loglik: -2.0535e+02 - logprior: -3.9866e+00
Fitted a model with MAP estimate = -206.4487
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 224.4353 - loglik: -2.0697e+02 - logprior: -1.7442e+01
Epoch 2/10
10/10 - 2s - loss: 208.5350 - loglik: -2.0484e+02 - logprior: -3.6349e+00
Epoch 3/10
10/10 - 2s - loss: 204.7275 - loglik: -2.0353e+02 - logprior: -1.0060e+00
Epoch 4/10
10/10 - 2s - loss: 203.3771 - loglik: -2.0292e+02 - logprior: -1.4453e-01
Epoch 5/10
10/10 - 2s - loss: 201.0117 - loglik: -2.0091e+02 - logprior: 0.2746
Epoch 6/10
10/10 - 2s - loss: 200.5623 - loglik: -2.0073e+02 - logprior: 0.5753
Epoch 7/10
10/10 - 2s - loss: 200.0833 - loglik: -2.0048e+02 - logprior: 0.8410
Epoch 8/10
10/10 - 2s - loss: 199.7877 - loglik: -2.0034e+02 - logprior: 1.0110
Epoch 9/10
10/10 - 2s - loss: 199.3937 - loglik: -2.0004e+02 - logprior: 1.1101
Epoch 10/10
10/10 - 2s - loss: 198.6973 - loglik: -1.9943e+02 - logprior: 1.2008
Fitted a model with MAP estimate = -198.4363
Time for alignment: 70.6307
Computed alignments with likelihoods: ['-199.2970', '-199.2606', '-198.4363']
Best model has likelihood: -198.4363
time for generating output: 0.1863
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9174061433447099
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac4cb5baf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac55515eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac4420be50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac66283040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4444c5b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac76dfbdf0>, <__main__.SimpleDirichletPrior object at 0x7fabcd79f9d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.0180 - loglik: -2.4844e+02 - logprior: -6.8548e+01
Epoch 2/10
10/10 - 2s - loss: 236.4528 - loglik: -2.1913e+02 - logprior: -1.7288e+01
Epoch 3/10
10/10 - 2s - loss: 204.1656 - loglik: -1.9666e+02 - logprior: -7.4254e+00
Epoch 4/10
10/10 - 2s - loss: 189.9625 - loglik: -1.8608e+02 - logprior: -3.8275e+00
Epoch 5/10
10/10 - 2s - loss: 183.4580 - loglik: -1.8153e+02 - logprior: -1.8064e+00
Epoch 6/10
10/10 - 2s - loss: 179.9107 - loglik: -1.7896e+02 - logprior: -7.3799e-01
Epoch 7/10
10/10 - 2s - loss: 177.7450 - loglik: -1.7740e+02 - logprior: -1.4932e-01
Epoch 8/10
10/10 - 2s - loss: 176.1508 - loglik: -1.7616e+02 - logprior: 0.2042
Epoch 9/10
10/10 - 2s - loss: 175.1605 - loglik: -1.7536e+02 - logprior: 0.4344
Epoch 10/10
10/10 - 2s - loss: 174.5897 - loglik: -1.7487e+02 - logprior: 0.5596
Fitted a model with MAP estimate = -174.0131
expansions: [(9, 3), (10, 1), (25, 2), (26, 1), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 252.8652 - loglik: -1.7568e+02 - logprior: -7.7052e+01
Epoch 2/2
10/10 - 2s - loss: 199.9674 - loglik: -1.6873e+02 - logprior: -3.1056e+01
Fitted a model with MAP estimate = -190.8551
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 240.3317 - loglik: -1.6720e+02 - logprior: -7.3079e+01
Epoch 2/10
10/10 - 2s - loss: 185.4379 - loglik: -1.6464e+02 - logprior: -2.0747e+01
Epoch 3/10
10/10 - 2s - loss: 170.9051 - loglik: -1.6382e+02 - logprior: -6.9804e+00
Epoch 4/10
10/10 - 2s - loss: 165.9515 - loglik: -1.6358e+02 - logprior: -2.2095e+00
Epoch 5/10
10/10 - 2s - loss: 163.6200 - loglik: -1.6356e+02 - logprior: 0.1223
Epoch 6/10
10/10 - 2s - loss: 162.2253 - loglik: -1.6344e+02 - logprior: 1.4015
Epoch 7/10
10/10 - 2s - loss: 161.1938 - loglik: -1.6313e+02 - logprior: 2.1618
Epoch 8/10
10/10 - 2s - loss: 160.4998 - loglik: -1.6293e+02 - logprior: 2.6865
Epoch 9/10
10/10 - 2s - loss: 160.0070 - loglik: -1.6282e+02 - logprior: 3.1020
Epoch 10/10
10/10 - 2s - loss: 159.6719 - loglik: -1.6285e+02 - logprior: 3.4770
Fitted a model with MAP estimate = -159.1730
Time for alignment: 58.6261
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.0905 - loglik: -2.4844e+02 - logprior: -6.8548e+01
Epoch 2/10
10/10 - 2s - loss: 236.2092 - loglik: -2.1885e+02 - logprior: -1.7312e+01
Epoch 3/10
10/10 - 2s - loss: 203.0529 - loglik: -1.9550e+02 - logprior: -7.4677e+00
Epoch 4/10
10/10 - 2s - loss: 189.1783 - loglik: -1.8528e+02 - logprior: -3.8554e+00
Epoch 5/10
10/10 - 2s - loss: 182.2031 - loglik: -1.8022e+02 - logprior: -1.9275e+00
Epoch 6/10
10/10 - 2s - loss: 178.2863 - loglik: -1.7711e+02 - logprior: -9.9263e-01
Epoch 7/10
10/10 - 2s - loss: 176.1476 - loglik: -1.7545e+02 - logprior: -4.6517e-01
Epoch 8/10
10/10 - 2s - loss: 174.7028 - loglik: -1.7444e+02 - logprior: -6.9807e-02
Epoch 9/10
10/10 - 2s - loss: 174.0307 - loglik: -1.7399e+02 - logprior: 0.1621
Epoch 10/10
10/10 - 2s - loss: 173.6651 - loglik: -1.7375e+02 - logprior: 0.3142
Fitted a model with MAP estimate = -173.2385
expansions: [(14, 2), (15, 1), (24, 1), (26, 2), (27, 3), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 251.8266 - loglik: -1.7493e+02 - logprior: -7.6871e+01
Epoch 2/2
10/10 - 2s - loss: 197.4798 - loglik: -1.6646e+02 - logprior: -3.0989e+01
Fitted a model with MAP estimate = -188.3736
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.2200 - loglik: -1.6563e+02 - logprior: -7.5467e+01
Epoch 2/2
10/10 - 2s - loss: 190.2234 - loglik: -1.6396e+02 - logprior: -2.6120e+01
Fitted a model with MAP estimate = -178.7940
expansions: [(0, 5), (5, 2), (7, 2)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 226.0177 - loglik: -1.6497e+02 - logprior: -6.1007e+01
Epoch 2/10
10/10 - 2s - loss: 175.8974 - loglik: -1.6088e+02 - logprior: -1.4931e+01
Epoch 3/10
10/10 - 2s - loss: 165.1379 - loglik: -1.5961e+02 - logprior: -5.3903e+00
Epoch 4/10
10/10 - 2s - loss: 161.0148 - loglik: -1.5949e+02 - logprior: -1.3700e+00
Epoch 5/10
10/10 - 2s - loss: 158.9001 - loglik: -1.5952e+02 - logprior: 0.7843
Epoch 6/10
10/10 - 2s - loss: 157.7263 - loglik: -1.5960e+02 - logprior: 2.0679
Epoch 7/10
10/10 - 2s - loss: 156.8153 - loglik: -1.5954e+02 - logprior: 2.9278
Epoch 8/10
10/10 - 2s - loss: 156.1912 - loglik: -1.5947e+02 - logprior: 3.4840
Epoch 9/10
10/10 - 2s - loss: 155.7683 - loglik: -1.5937e+02 - logprior: 3.8874
Epoch 10/10
10/10 - 2s - loss: 155.4443 - loglik: -1.5935e+02 - logprior: 4.2268
Fitted a model with MAP estimate = -154.9228
Time for alignment: 66.9657
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 317.0438 - loglik: -2.4845e+02 - logprior: -6.8549e+01
Epoch 2/10
10/10 - 2s - loss: 236.1835 - loglik: -2.1885e+02 - logprior: -1.7300e+01
Epoch 3/10
10/10 - 2s - loss: 203.8567 - loglik: -1.9631e+02 - logprior: -7.4571e+00
Epoch 4/10
10/10 - 2s - loss: 189.8368 - loglik: -1.8583e+02 - logprior: -3.9738e+00
Epoch 5/10
10/10 - 2s - loss: 183.0517 - loglik: -1.8102e+02 - logprior: -1.9872e+00
Epoch 6/10
10/10 - 2s - loss: 178.3100 - loglik: -1.7713e+02 - logprior: -1.0052e+00
Epoch 7/10
10/10 - 2s - loss: 175.7321 - loglik: -1.7499e+02 - logprior: -4.8898e-01
Epoch 8/10
10/10 - 2s - loss: 174.2523 - loglik: -1.7386e+02 - logprior: -1.4371e-01
Epoch 9/10
10/10 - 2s - loss: 173.5985 - loglik: -1.7343e+02 - logprior: 0.0892
Epoch 10/10
10/10 - 2s - loss: 173.1981 - loglik: -1.7320e+02 - logprior: 0.2770
Fitted a model with MAP estimate = -172.7398
expansions: [(9, 3), (14, 1), (15, 1), (37, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.3062 - loglik: -1.7411e+02 - logprior: -7.7030e+01
Epoch 2/2
10/10 - 2s - loss: 198.9387 - loglik: -1.6789e+02 - logprior: -3.0892e+01
Fitted a model with MAP estimate = -189.9291
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 239.3874 - loglik: -1.6662e+02 - logprior: -7.2751e+01
Epoch 2/10
10/10 - 2s - loss: 185.0775 - loglik: -1.6467e+02 - logprior: -2.0383e+01
Epoch 3/10
10/10 - 2s - loss: 170.9566 - loglik: -1.6398e+02 - logprior: -6.8964e+00
Epoch 4/10
10/10 - 2s - loss: 166.1734 - loglik: -1.6385e+02 - logprior: -2.1906e+00
Epoch 5/10
10/10 - 2s - loss: 163.8720 - loglik: -1.6383e+02 - logprior: 0.1118
Epoch 6/10
10/10 - 2s - loss: 162.5578 - loglik: -1.6378e+02 - logprior: 1.4010
Epoch 7/10
10/10 - 2s - loss: 161.8011 - loglik: -1.6376e+02 - logprior: 2.1557
Epoch 8/10
10/10 - 2s - loss: 161.2429 - loglik: -1.6371e+02 - logprior: 2.6794
Epoch 9/10
10/10 - 2s - loss: 160.8658 - loglik: -1.6374e+02 - logprior: 3.0931
Epoch 10/10
10/10 - 2s - loss: 160.5490 - loglik: -1.6382e+02 - logprior: 3.4932
Fitted a model with MAP estimate = -160.1547
Time for alignment: 58.4843
Computed alignments with likelihoods: ['-159.1730', '-154.9228', '-160.1547']
Best model has likelihood: -154.9228
time for generating output: 0.2292
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3718160767449553
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabcd284fa0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd745850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac33549f70>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac33549ee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac19b6a9d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabde00f130>, <__main__.SimpleDirichletPrior object at 0x7fac4cfe2dc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 756.1248 - loglik: -7.4784e+02 - logprior: -7.9467e+00
Epoch 2/10
20/20 - 11s - loss: 677.4106 - loglik: -6.7665e+02 - logprior: 0.1469
Epoch 3/10
20/20 - 11s - loss: 649.2626 - loglik: -6.4776e+02 - logprior: -5.5299e-02
Epoch 4/10
20/20 - 11s - loss: 639.4794 - loglik: -6.3797e+02 - logprior: -2.2611e-02
Epoch 5/10
20/20 - 11s - loss: 634.7160 - loglik: -6.3348e+02 - logprior: 0.0609
Epoch 6/10
20/20 - 11s - loss: 632.2809 - loglik: -6.3116e+02 - logprior: 0.0559
Epoch 7/10
20/20 - 11s - loss: 633.6510 - loglik: -6.3263e+02 - logprior: 0.0457
Fitted a model with MAP estimate = -630.6733
expansions: [(0, 4), (49, 1), (52, 1), (54, 2), (82, 7), (100, 1), (111, 5), (112, 2), (118, 1), (121, 1), (154, 2), (169, 1), (171, 11), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 697.7661 - loglik: -6.8531e+02 - logprior: -1.2386e+01
Epoch 2/2
20/20 - 13s - loss: 646.8896 - loglik: -6.4561e+02 - logprior: -9.7319e-01
Fitted a model with MAP estimate = -636.4873
expansions: [(0, 4), (95, 8), (240, 1)]
discards: [  1   2   3   4   5   6 128 129 130 131 132 133 199 200 201 202 203 209]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 660.2557 - loglik: -6.4822e+02 - logprior: -1.1982e+01
Epoch 2/2
20/20 - 13s - loss: 636.2952 - loglik: -6.3553e+02 - logprior: -4.9386e-01
Fitted a model with MAP estimate = -631.6362
expansions: [(0, 4)]
discards: [  0 180]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 649.9540 - loglik: -6.4145e+02 - logprior: -8.3907e+00
Epoch 2/10
20/20 - 13s - loss: 633.4843 - loglik: -6.3399e+02 - logprior: 0.8427
Epoch 3/10
20/20 - 13s - loss: 628.6764 - loglik: -6.2963e+02 - logprior: 1.5990
Epoch 4/10
20/20 - 13s - loss: 625.4851 - loglik: -6.2662e+02 - logprior: 2.0049
Epoch 5/10
20/20 - 13s - loss: 620.6879 - loglik: -6.2182e+02 - logprior: 2.1679
Epoch 6/10
20/20 - 13s - loss: 620.6537 - loglik: -6.2186e+02 - logprior: 2.3432
Epoch 7/10
20/20 - 13s - loss: 617.9869 - loglik: -6.1930e+02 - logprior: 2.4965
Epoch 8/10
20/20 - 13s - loss: 618.1479 - loglik: -6.1963e+02 - logprior: 2.6848
Fitted a model with MAP estimate = -614.8142
Time for alignment: 280.8540
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 756.9462 - loglik: -7.4892e+02 - logprior: -7.9618e+00
Epoch 2/10
20/20 - 11s - loss: 684.9841 - loglik: -6.8479e+02 - logprior: 0.1796
Epoch 3/10
20/20 - 11s - loss: 650.6490 - loglik: -6.4955e+02 - logprior: -2.3200e-01
Epoch 4/10
20/20 - 11s - loss: 639.8041 - loglik: -6.3836e+02 - logprior: -2.9925e-01
Epoch 5/10
20/20 - 10s - loss: 637.0099 - loglik: -6.3551e+02 - logprior: -2.3560e-01
Epoch 6/10
20/20 - 11s - loss: 632.3911 - loglik: -6.3096e+02 - logprior: -1.9922e-01
Epoch 7/10
20/20 - 11s - loss: 634.2375 - loglik: -6.3282e+02 - logprior: -2.3785e-01
Fitted a model with MAP estimate = -630.3760
expansions: [(0, 3), (19, 5), (25, 1), (45, 1), (52, 1), (54, 2), (82, 1), (86, 13), (88, 1), (93, 1), (94, 1), (103, 1), (111, 5), (116, 2), (118, 1), (151, 1), (154, 1), (172, 11), (175, 1), (176, 1), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 693.7665 - loglik: -6.8115e+02 - logprior: -1.2536e+01
Epoch 2/2
20/20 - 14s - loss: 641.1570 - loglik: -6.3972e+02 - logprior: -1.1025e+00
Fitted a model with MAP estimate = -631.3156
expansions: []
discards: [  0   2  24  63  92  93 144 145 146 152 213 214 215 216 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 655.2021 - loglik: -6.4391e+02 - logprior: -1.1066e+01
Epoch 2/2
20/20 - 13s - loss: 634.6981 - loglik: -6.3194e+02 - logprior: -2.2465e+00
Fitted a model with MAP estimate = -629.0398
expansions: [(0, 3), (59, 1), (88, 1), (89, 1)]
discards: [ 0 97]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 643.6525 - loglik: -6.3638e+02 - logprior: -7.0752e+00
Epoch 2/10
20/20 - 14s - loss: 632.4025 - loglik: -6.3268e+02 - logprior: 0.8152
Epoch 3/10
20/20 - 13s - loss: 622.8554 - loglik: -6.2350e+02 - logprior: 1.5002
Epoch 4/10
20/20 - 14s - loss: 621.9905 - loglik: -6.2280e+02 - logprior: 1.9305
Epoch 5/10
20/20 - 14s - loss: 618.1263 - loglik: -6.1897e+02 - logprior: 2.1046
Epoch 6/10
20/20 - 13s - loss: 614.1368 - loglik: -6.1514e+02 - logprior: 2.2879
Epoch 7/10
20/20 - 13s - loss: 615.3202 - loglik: -6.1647e+02 - logprior: 2.4104
Fitted a model with MAP estimate = -612.7756
Time for alignment: 274.1699
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 755.7967 - loglik: -7.4763e+02 - logprior: -7.9451e+00
Epoch 2/10
20/20 - 11s - loss: 679.6855 - loglik: -6.7914e+02 - logprior: 0.1462
Epoch 3/10
20/20 - 11s - loss: 649.7910 - loglik: -6.4829e+02 - logprior: -2.5128e-01
Epoch 4/10
20/20 - 11s - loss: 639.6841 - loglik: -6.3807e+02 - logprior: -2.2358e-01
Epoch 5/10
20/20 - 11s - loss: 633.1473 - loglik: -6.3170e+02 - logprior: -1.1471e-01
Epoch 6/10
20/20 - 11s - loss: 633.3226 - loglik: -6.3207e+02 - logprior: -7.9065e-02
Fitted a model with MAP estimate = -630.9694
expansions: [(0, 3), (21, 5), (43, 2), (49, 2), (50, 1), (51, 1), (82, 1), (83, 1), (85, 7), (92, 1), (94, 1), (100, 1), (111, 5), (112, 2), (115, 2), (117, 1), (146, 3), (152, 1), (154, 2), (170, 11), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 18s - loss: 701.8334 - loglik: -6.8914e+02 - logprior: -1.2608e+01
Epoch 2/2
20/20 - 14s - loss: 646.4778 - loglik: -6.4480e+02 - logprior: -1.4198e+00
Fitted a model with MAP estimate = -636.2935
expansions: [(106, 3), (254, 1), (276, 1)]
discards: [  3  49  59 138 139 140 141 142 143 148 184 185 194 195 222]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 651.5223 - loglik: -6.4380e+02 - logprior: -7.4350e+00
Epoch 2/2
20/20 - 14s - loss: 634.3151 - loglik: -6.3376e+02 - logprior: 0.3089
Fitted a model with MAP estimate = -627.2702
expansions: [(0, 3), (26, 3), (48, 1), (102, 1), (211, 1), (234, 1)]
discards: [  1   2 201 202 203 239]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 18s - loss: 654.9288 - loglik: -6.4118e+02 - logprior: -1.3549e+01
Epoch 2/10
20/20 - 14s - loss: 631.8923 - loglik: -6.3030e+02 - logprior: -9.8193e-01
Epoch 3/10
20/20 - 14s - loss: 625.2880 - loglik: -6.2532e+02 - logprior: 1.1797
Epoch 4/10
20/20 - 14s - loss: 621.3317 - loglik: -6.2163e+02 - logprior: 1.7814
Epoch 5/10
20/20 - 14s - loss: 617.6676 - loglik: -6.1813e+02 - logprior: 1.9959
Epoch 6/10
20/20 - 14s - loss: 617.5980 - loglik: -6.1829e+02 - logprior: 2.1339
Epoch 7/10
20/20 - 14s - loss: 613.5492 - loglik: -6.1451e+02 - logprior: 2.2980
Epoch 8/10
20/20 - 14s - loss: 613.6280 - loglik: -6.1475e+02 - logprior: 2.3678
Fitted a model with MAP estimate = -610.7462
Time for alignment: 280.1323
Computed alignments with likelihoods: ['-614.8142', '-612.7756', '-610.7462']
Best model has likelihood: -610.7462
time for generating output: 0.3963
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6843267108167771
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac55082430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac0043b2e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac2a87f6d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac55102910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4cfbc040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac552ce130>, <__main__.SimpleDirichletPrior object at 0x7fabc4e96190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7283 - loglik: -1.3147e+02 - logprior: -3.2413e+00
Epoch 2/10
19/19 - 1s - loss: 113.3725 - loglik: -1.1194e+02 - logprior: -1.3652e+00
Epoch 3/10
19/19 - 1s - loss: 106.1329 - loglik: -1.0433e+02 - logprior: -1.5009e+00
Epoch 4/10
19/19 - 1s - loss: 104.5577 - loglik: -1.0287e+02 - logprior: -1.4289e+00
Epoch 5/10
19/19 - 1s - loss: 103.7979 - loglik: -1.0216e+02 - logprior: -1.4042e+00
Epoch 6/10
19/19 - 1s - loss: 103.7524 - loglik: -1.0216e+02 - logprior: -1.3987e+00
Epoch 7/10
19/19 - 1s - loss: 103.2805 - loglik: -1.0171e+02 - logprior: -1.3892e+00
Epoch 8/10
19/19 - 2s - loss: 103.2763 - loglik: -1.0171e+02 - logprior: -1.3870e+00
Epoch 9/10
19/19 - 1s - loss: 103.1512 - loglik: -1.0160e+02 - logprior: -1.3811e+00
Epoch 10/10
19/19 - 2s - loss: 103.3252 - loglik: -1.0177e+02 - logprior: -1.3796e+00
Fitted a model with MAP estimate = -101.4908
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 3), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.1017 - loglik: -1.0376e+02 - logprior: -4.2423e+00
Epoch 2/2
19/19 - 2s - loss: 98.1883 - loglik: -9.5928e+01 - logprior: -2.2082e+00
Fitted a model with MAP estimate = -94.8939
expansions: [(0, 1), (38, 1)]
discards: [ 0 10 30]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 97.9841 - loglik: -9.4785e+01 - logprior: -3.0438e+00
Epoch 2/2
19/19 - 2s - loss: 93.8624 - loglik: -9.2224e+01 - logprior: -1.3928e+00
Fitted a model with MAP estimate = -91.6111
expansions: []
discards: [38 39]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.0817 - loglik: -9.2041e+01 - logprior: -3.0004e+00
Epoch 2/10
19/19 - 2s - loss: 91.9731 - loglik: -9.0539e+01 - logprior: -1.2880e+00
Epoch 3/10
19/19 - 2s - loss: 91.3575 - loglik: -8.9867e+01 - logprior: -1.2461e+00
Epoch 4/10
19/19 - 2s - loss: 90.7799 - loglik: -8.9288e+01 - logprior: -1.2019e+00
Epoch 5/10
19/19 - 1s - loss: 90.4635 - loglik: -8.9016e+01 - logprior: -1.1719e+00
Epoch 6/10
19/19 - 2s - loss: 90.2800 - loglik: -8.8875e+01 - logprior: -1.1626e+00
Epoch 7/10
19/19 - 2s - loss: 90.3707 - loglik: -8.9008e+01 - logprior: -1.1453e+00
Fitted a model with MAP estimate = -89.9085
Time for alignment: 55.3229
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7944 - loglik: -1.3153e+02 - logprior: -3.2408e+00
Epoch 2/10
19/19 - 2s - loss: 114.2426 - loglik: -1.1284e+02 - logprior: -1.3571e+00
Epoch 3/10
19/19 - 1s - loss: 107.5230 - loglik: -1.0582e+02 - logprior: -1.4428e+00
Epoch 4/10
19/19 - 1s - loss: 105.0706 - loglik: -1.0346e+02 - logprior: -1.3700e+00
Epoch 5/10
19/19 - 2s - loss: 102.9424 - loglik: -1.0133e+02 - logprior: -1.3733e+00
Epoch 6/10
19/19 - 2s - loss: 102.0653 - loglik: -1.0053e+02 - logprior: -1.3480e+00
Epoch 7/10
19/19 - 2s - loss: 101.7020 - loglik: -1.0025e+02 - logprior: -1.2859e+00
Epoch 8/10
19/19 - 2s - loss: 101.3659 - loglik: -9.9922e+01 - logprior: -1.2903e+00
Epoch 9/10
19/19 - 2s - loss: 101.1983 - loglik: -9.9755e+01 - logprior: -1.2880e+00
Epoch 10/10
19/19 - 2s - loss: 100.8282 - loglik: -9.9375e+01 - logprior: -1.2931e+00
Fitted a model with MAP estimate = -99.4253
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (35, 1), (36, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.6232 - loglik: -1.0137e+02 - logprior: -4.2303e+00
Epoch 2/2
19/19 - 1s - loss: 95.9270 - loglik: -9.3600e+01 - logprior: -2.1878e+00
Fitted a model with MAP estimate = -93.0422
expansions: [(0, 1)]
discards: [ 0  9 30 46]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.2345 - loglik: -9.4042e+01 - logprior: -3.1146e+00
Epoch 2/2
19/19 - 2s - loss: 93.9601 - loglik: -9.2427e+01 - logprior: -1.4571e+00
Fitted a model with MAP estimate = -91.8201
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.7187 - loglik: -9.1518e+01 - logprior: -3.1585e+00
Epoch 2/10
19/19 - 1s - loss: 92.0580 - loglik: -9.0559e+01 - logprior: -1.4269e+00
Epoch 3/10
19/19 - 2s - loss: 91.0151 - loglik: -8.9444e+01 - logprior: -1.3261e+00
Epoch 4/10
19/19 - 1s - loss: 90.4187 - loglik: -8.8839e+01 - logprior: -1.3112e+00
Epoch 5/10
19/19 - 2s - loss: 89.9633 - loglik: -8.8422e+01 - logprior: -1.2954e+00
Epoch 6/10
19/19 - 2s - loss: 89.6154 - loglik: -8.8110e+01 - logprior: -1.2766e+00
Epoch 7/10
19/19 - 1s - loss: 89.8363 - loglik: -8.8357e+01 - logprior: -1.2682e+00
Fitted a model with MAP estimate = -89.4642
Time for alignment: 55.5518
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7379 - loglik: -1.3146e+02 - logprior: -3.2389e+00
Epoch 2/10
19/19 - 2s - loss: 113.8403 - loglik: -1.1244e+02 - logprior: -1.3522e+00
Epoch 3/10
19/19 - 2s - loss: 106.7418 - loglik: -1.0495e+02 - logprior: -1.5076e+00
Epoch 4/10
19/19 - 1s - loss: 104.2812 - loglik: -1.0253e+02 - logprior: -1.4594e+00
Epoch 5/10
19/19 - 1s - loss: 103.2971 - loglik: -1.0161e+02 - logprior: -1.4316e+00
Epoch 6/10
19/19 - 1s - loss: 102.7332 - loglik: -1.0110e+02 - logprior: -1.4219e+00
Epoch 7/10
19/19 - 1s - loss: 102.7547 - loglik: -1.0115e+02 - logprior: -1.4140e+00
Fitted a model with MAP estimate = -101.0156
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (25, 1), (28, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.3327 - loglik: -1.0084e+02 - logprior: -4.2308e+00
Epoch 2/2
19/19 - 2s - loss: 95.5715 - loglik: -9.3162e+01 - logprior: -2.1011e+00
Fitted a model with MAP estimate = -92.3612
expansions: [(0, 1)]
discards: [ 0 10 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.8270 - loglik: -9.3579e+01 - logprior: -3.0685e+00
Epoch 2/2
19/19 - 1s - loss: 93.3782 - loglik: -9.1665e+01 - logprior: -1.4445e+00
Fitted a model with MAP estimate = -91.2373
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.4042 - loglik: -9.1203e+01 - logprior: -3.1249e+00
Epoch 2/10
19/19 - 2s - loss: 91.7726 - loglik: -9.0134e+01 - logprior: -1.4400e+00
Epoch 3/10
19/19 - 2s - loss: 90.9695 - loglik: -8.9356e+01 - logprior: -1.3399e+00
Epoch 4/10
19/19 - 2s - loss: 90.3908 - loglik: -8.8770e+01 - logprior: -1.3136e+00
Epoch 5/10
19/19 - 2s - loss: 90.4506 - loglik: -8.8880e+01 - logprior: -1.2829e+00
Fitted a model with MAP estimate = -89.7655
Time for alignment: 47.4245
Computed alignments with likelihoods: ['-89.9085', '-89.4642', '-89.7655']
Best model has likelihood: -89.4642
time for generating output: 0.1122
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9438877755511023
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac33760490>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac19f1d760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac089de1f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd109bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4e96e80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac55215f10>, <__main__.SimpleDirichletPrior object at 0x7fac114a1a30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 650.7361 - loglik: -6.3190e+02 - logprior: -1.8624e+01
Epoch 2/10
15/15 - 8s - loss: 571.3621 - loglik: -5.7031e+02 - logprior: -6.2460e-01
Epoch 3/10
15/15 - 8s - loss: 506.1217 - loglik: -5.0553e+02 - logprior: -9.5067e-02
Epoch 4/10
15/15 - 8s - loss: 477.8370 - loglik: -4.7643e+02 - logprior: -8.4576e-01
Epoch 5/10
15/15 - 8s - loss: 466.4452 - loglik: -4.6469e+02 - logprior: -1.2251e+00
Epoch 6/10
15/15 - 8s - loss: 464.2774 - loglik: -4.6271e+02 - logprior: -1.0866e+00
Epoch 7/10
15/15 - 8s - loss: 464.6245 - loglik: -4.6325e+02 - logprior: -8.8722e-01
Fitted a model with MAP estimate = -461.6770
expansions: [(23, 2), (24, 2), (25, 1), (37, 3), (39, 1), (41, 1), (48, 1), (50, 1), (52, 2), (53, 2), (66, 1), (67, 5), (68, 2), (69, 1), (70, 1), (71, 1), (73, 1), (75, 2), (100, 1), (101, 1), (102, 3), (104, 2), (131, 2), (132, 2), (133, 2), (134, 2), (135, 2), (138, 1), (140, 1), (157, 1), (158, 4), (159, 3), (161, 1), (173, 11), (197, 1)]
discards: [  1   2   6 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 471.6103 - loglik: -4.5470e+02 - logprior: -1.6725e+01
Epoch 2/2
15/15 - 10s - loss: 437.0493 - loglik: -4.3665e+02 - logprior: 0.0460
Fitted a model with MAP estimate = -428.8012
expansions: [(0, 3), (61, 1), (121, 1), (206, 1)]
discards: [ 87 136 168 174 177 234]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 457.4397 - loglik: -4.3257e+02 - logprior: -2.4708e+01
Epoch 2/2
15/15 - 10s - loss: 428.7888 - loglik: -4.2622e+02 - logprior: -2.2273e+00
Fitted a model with MAP estimate = -423.6263
expansions: [(28, 1)]
discards: [  0   1   2  90 168 234]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 446.6209 - loglik: -4.2916e+02 - logprior: -1.7240e+01
Epoch 2/10
15/15 - 10s - loss: 427.2836 - loglik: -4.2874e+02 - logprior: 1.8986
Epoch 3/10
15/15 - 10s - loss: 421.3618 - loglik: -4.2561e+02 - logprior: 4.7771
Epoch 4/10
15/15 - 10s - loss: 420.5344 - loglik: -4.2561e+02 - logprior: 5.6736
Epoch 5/10
15/15 - 10s - loss: 416.2884 - loglik: -4.2190e+02 - logprior: 6.2055
Epoch 6/10
15/15 - 10s - loss: 419.0529 - loglik: -4.2522e+02 - logprior: 6.7431
Fitted a model with MAP estimate = -416.8798
Time for alignment: 189.1602
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 652.2748 - loglik: -6.3319e+02 - logprior: -1.8611e+01
Epoch 2/10
15/15 - 8s - loss: 573.5681 - loglik: -5.7233e+02 - logprior: -5.6136e-01
Epoch 3/10
15/15 - 8s - loss: 510.9433 - loglik: -5.1010e+02 - logprior: -1.0768e-01
Epoch 4/10
15/15 - 8s - loss: 479.0713 - loglik: -4.7765e+02 - logprior: -5.5087e-01
Epoch 5/10
15/15 - 8s - loss: 466.7862 - loglik: -4.6506e+02 - logprior: -9.7905e-01
Epoch 6/10
15/15 - 8s - loss: 465.0630 - loglik: -4.6349e+02 - logprior: -9.4229e-01
Epoch 7/10
15/15 - 8s - loss: 464.7674 - loglik: -4.6333e+02 - logprior: -8.3192e-01
Epoch 8/10
15/15 - 8s - loss: 462.5529 - loglik: -4.6129e+02 - logprior: -6.6544e-01
Epoch 9/10
15/15 - 8s - loss: 461.9932 - loglik: -4.6080e+02 - logprior: -6.1041e-01
Epoch 10/10
15/15 - 8s - loss: 464.0245 - loglik: -4.6287e+02 - logprior: -5.4920e-01
Fitted a model with MAP estimate = -460.1406
expansions: [(23, 1), (24, 2), (25, 1), (26, 1), (28, 1), (37, 2), (38, 1), (39, 1), (55, 2), (57, 1), (58, 1), (68, 1), (69, 1), (70, 5), (71, 2), (72, 1), (73, 1), (74, 1), (75, 2), (77, 2), (78, 1), (98, 1), (100, 1), (101, 1), (102, 3), (104, 2), (131, 2), (132, 2), (133, 4), (134, 2), (137, 1), (154, 3), (155, 5), (174, 9), (176, 4), (198, 1)]
discards: [  1   2 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 475.9673 - loglik: -4.5920e+02 - logprior: -1.6557e+01
Epoch 2/2
15/15 - 11s - loss: 433.0840 - loglik: -4.3299e+02 - logprior: 0.3359
Fitted a model with MAP estimate = -427.3679
expansions: [(0, 3), (106, 1), (203, 1)]
discards: [ 90  91 139 171 179 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 460.7196 - loglik: -4.3609e+02 - logprior: -2.4550e+01
Epoch 2/2
15/15 - 10s - loss: 430.6017 - loglik: -4.2865e+02 - logprior: -1.7212e+00
Fitted a model with MAP estimate = -423.4672
expansions: [(239, 1)]
discards: [  0   1   2 169]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 447.1415 - loglik: -4.3020e+02 - logprior: -1.6854e+01
Epoch 2/10
15/15 - 10s - loss: 424.7689 - loglik: -4.2679e+02 - logprior: 2.2198
Epoch 3/10
15/15 - 10s - loss: 418.9175 - loglik: -4.2366e+02 - logprior: 5.0409
Epoch 4/10
15/15 - 10s - loss: 421.2269 - loglik: -4.2677e+02 - logprior: 5.9377
Fitted a model with MAP estimate = -417.0877
Time for alignment: 193.1926
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 653.1342 - loglik: -6.3447e+02 - logprior: -1.8640e+01
Epoch 2/10
15/15 - 8s - loss: 568.4803 - loglik: -5.6779e+02 - logprior: -5.8618e-01
Epoch 3/10
15/15 - 8s - loss: 508.0841 - loglik: -5.0763e+02 - logprior: -1.0113e-01
Epoch 4/10
15/15 - 8s - loss: 480.1795 - loglik: -4.7895e+02 - logprior: -7.0562e-01
Epoch 5/10
15/15 - 8s - loss: 469.1565 - loglik: -4.6749e+02 - logprior: -1.1195e+00
Epoch 6/10
15/15 - 8s - loss: 468.5092 - loglik: -4.6688e+02 - logprior: -1.0813e+00
Epoch 7/10
15/15 - 8s - loss: 461.8902 - loglik: -4.6047e+02 - logprior: -8.7267e-01
Epoch 8/10
15/15 - 8s - loss: 463.3072 - loglik: -4.6203e+02 - logprior: -7.3811e-01
Fitted a model with MAP estimate = -462.9965
expansions: [(23, 1), (24, 2), (25, 2), (27, 1), (36, 1), (37, 1), (39, 1), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 2), (65, 1), (66, 5), (67, 2), (70, 1), (71, 1), (72, 1), (74, 2), (83, 1), (99, 3), (100, 2), (101, 2), (108, 1), (130, 2), (131, 2), (132, 2), (133, 2), (134, 3), (136, 1), (156, 1), (157, 5), (158, 2), (173, 10)]
discards: [  1   2 182 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 479.6352 - loglik: -4.6287e+02 - logprior: -1.6733e+01
Epoch 2/2
15/15 - 10s - loss: 436.2018 - loglik: -4.3603e+02 - logprior: 0.0471
Fitted a model with MAP estimate = -429.7854
expansions: [(26, 1), (62, 1)]
discards: [ 23  86 132 133 170 176 179]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 450.6487 - loglik: -4.3518e+02 - logprior: -1.5375e+01
Epoch 2/2
15/15 - 10s - loss: 429.2259 - loglik: -4.3015e+02 - logprior: 1.2667
Fitted a model with MAP estimate = -424.3178
expansions: [(203, 1)]
discards: [165]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 445.8062 - loglik: -4.3092e+02 - logprior: -1.4797e+01
Epoch 2/10
15/15 - 10s - loss: 427.7465 - loglik: -4.2949e+02 - logprior: 1.9943
Epoch 3/10
15/15 - 10s - loss: 423.0997 - loglik: -4.2734e+02 - logprior: 4.6202
Epoch 4/10
15/15 - 10s - loss: 417.0928 - loglik: -4.2220e+02 - logprior: 5.5730
Epoch 5/10
15/15 - 10s - loss: 421.1617 - loglik: -4.2677e+02 - logprior: 6.1156
Fitted a model with MAP estimate = -418.0561
Time for alignment: 186.2645
Computed alignments with likelihoods: ['-416.8798', '-417.0877', '-418.0561']
Best model has likelihood: -416.8798
time for generating output: 0.3502
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9076508257499157
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac768d2790>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac11059f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac5de97580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6bf3df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd109bb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac19f1d760>, <__main__.SimpleDirichletPrior object at 0x7fabef6ec5b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.8285 - loglik: -1.8502e+02 - logprior: -4.1695e+01
Epoch 2/10
10/10 - 1s - loss: 180.3698 - loglik: -1.6910e+02 - logprior: -1.1221e+01
Epoch 3/10
10/10 - 1s - loss: 161.4432 - loglik: -1.5586e+02 - logprior: -5.5410e+00
Epoch 4/10
10/10 - 1s - loss: 151.5591 - loglik: -1.4786e+02 - logprior: -3.5533e+00
Epoch 5/10
10/10 - 1s - loss: 147.6492 - loglik: -1.4488e+02 - logprior: -2.5501e+00
Epoch 6/10
10/10 - 1s - loss: 145.9011 - loglik: -1.4363e+02 - logprior: -2.0754e+00
Epoch 7/10
10/10 - 1s - loss: 144.7647 - loglik: -1.4272e+02 - logprior: -1.8371e+00
Epoch 8/10
10/10 - 1s - loss: 144.2260 - loglik: -1.4237e+02 - logprior: -1.6237e+00
Epoch 9/10
10/10 - 1s - loss: 143.5074 - loglik: -1.4186e+02 - logprior: -1.3936e+00
Epoch 10/10
10/10 - 1s - loss: 143.1065 - loglik: -1.4157e+02 - logprior: -1.2685e+00
Fitted a model with MAP estimate = -142.5950
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.6147 - loglik: -1.4047e+02 - logprior: -5.4921e+01
Epoch 2/2
10/10 - 1s - loss: 151.7915 - loglik: -1.3484e+02 - logprior: -1.6697e+01
Fitted a model with MAP estimate = -143.4853
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.7086 - loglik: -1.3532e+02 - logprior: -4.7348e+01
Epoch 2/2
10/10 - 1s - loss: 152.8922 - loglik: -1.3436e+02 - logprior: -1.8421e+01
Fitted a model with MAP estimate = -147.4054
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.7392 - loglik: -1.3391e+02 - logprior: -4.2637e+01
Epoch 2/10
10/10 - 1s - loss: 144.5074 - loglik: -1.3264e+02 - logprior: -1.1633e+01
Epoch 3/10
10/10 - 1s - loss: 137.1046 - loglik: -1.3223e+02 - logprior: -4.5636e+00
Epoch 4/10
10/10 - 1s - loss: 134.2293 - loglik: -1.3189e+02 - logprior: -2.0559e+00
Epoch 5/10
10/10 - 1s - loss: 132.7889 - loglik: -1.3168e+02 - logprior: -8.3805e-01
Epoch 6/10
10/10 - 1s - loss: 131.6961 - loglik: -1.3114e+02 - logprior: -2.7246e-01
Epoch 7/10
10/10 - 1s - loss: 131.4462 - loglik: -1.3116e+02 - logprior: -3.2425e-03
Epoch 8/10
10/10 - 1s - loss: 130.8642 - loglik: -1.3086e+02 - logprior: 0.2869
Epoch 9/10
10/10 - 1s - loss: 130.7145 - loglik: -1.3101e+02 - logprior: 0.5918
Epoch 10/10
10/10 - 1s - loss: 130.5567 - loglik: -1.3106e+02 - logprior: 0.7962
Fitted a model with MAP estimate = -130.1560
Time for alignment: 35.0782
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 226.9063 - loglik: -1.8510e+02 - logprior: -4.1700e+01
Epoch 2/10
10/10 - 1s - loss: 179.9084 - loglik: -1.6864e+02 - logprior: -1.1233e+01
Epoch 3/10
10/10 - 1s - loss: 160.9926 - loglik: -1.5543e+02 - logprior: -5.5292e+00
Epoch 4/10
10/10 - 1s - loss: 151.0478 - loglik: -1.4740e+02 - logprior: -3.5252e+00
Epoch 5/10
10/10 - 1s - loss: 147.0512 - loglik: -1.4427e+02 - logprior: -2.5660e+00
Epoch 6/10
10/10 - 1s - loss: 144.9369 - loglik: -1.4264e+02 - logprior: -2.0768e+00
Epoch 7/10
10/10 - 1s - loss: 143.9095 - loglik: -1.4184e+02 - logprior: -1.8375e+00
Epoch 8/10
10/10 - 1s - loss: 142.9818 - loglik: -1.4112e+02 - logprior: -1.6058e+00
Epoch 9/10
10/10 - 1s - loss: 142.9246 - loglik: -1.4131e+02 - logprior: -1.3774e+00
Epoch 10/10
10/10 - 1s - loss: 142.7028 - loglik: -1.4121e+02 - logprior: -1.2371e+00
Fitted a model with MAP estimate = -142.3084
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.5222 - loglik: -1.4057e+02 - logprior: -5.4929e+01
Epoch 2/2
10/10 - 1s - loss: 150.9184 - loglik: -1.3412e+02 - logprior: -1.6723e+01
Fitted a model with MAP estimate = -142.7394
expansions: [(19, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.5749 - loglik: -1.3428e+02 - logprior: -4.7265e+01
Epoch 2/2
10/10 - 1s - loss: 151.9358 - loglik: -1.3347e+02 - logprior: -1.8357e+01
Fitted a model with MAP estimate = -146.4856
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.2011 - loglik: -1.3356e+02 - logprior: -4.2618e+01
Epoch 2/10
10/10 - 1s - loss: 144.0241 - loglik: -1.3234e+02 - logprior: -1.1567e+01
Epoch 3/10
10/10 - 1s - loss: 136.6303 - loglik: -1.3191e+02 - logprior: -4.5048e+00
Epoch 4/10
10/10 - 1s - loss: 133.5846 - loglik: -1.3130e+02 - logprior: -2.0010e+00
Epoch 5/10
10/10 - 1s - loss: 132.0335 - loglik: -1.3093e+02 - logprior: -8.0122e-01
Epoch 6/10
10/10 - 1s - loss: 131.3196 - loglik: -1.3078e+02 - logprior: -2.4720e-01
Epoch 7/10
10/10 - 1s - loss: 130.8069 - loglik: -1.3053e+02 - logprior: 0.0160
Epoch 8/10
10/10 - 1s - loss: 130.4969 - loglik: -1.3049e+02 - logprior: 0.2838
Epoch 9/10
10/10 - 1s - loss: 130.3421 - loglik: -1.3063e+02 - logprior: 0.5819
Epoch 10/10
10/10 - 1s - loss: 130.2409 - loglik: -1.3073e+02 - logprior: 0.7906
Fitted a model with MAP estimate = -129.7578
Time for alignment: 36.1663
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.9688 - loglik: -1.8521e+02 - logprior: -4.1696e+01
Epoch 2/10
10/10 - 1s - loss: 180.3923 - loglik: -1.6913e+02 - logprior: -1.1226e+01
Epoch 3/10
10/10 - 1s - loss: 161.4057 - loglik: -1.5582e+02 - logprior: -5.5572e+00
Epoch 4/10
10/10 - 1s - loss: 151.3100 - loglik: -1.4764e+02 - logprior: -3.5608e+00
Epoch 5/10
10/10 - 1s - loss: 147.1918 - loglik: -1.4440e+02 - logprior: -2.5774e+00
Epoch 6/10
10/10 - 1s - loss: 145.0572 - loglik: -1.4276e+02 - logprior: -2.0730e+00
Epoch 7/10
10/10 - 1s - loss: 144.1647 - loglik: -1.4206e+02 - logprior: -1.8616e+00
Epoch 8/10
10/10 - 1s - loss: 143.3978 - loglik: -1.4146e+02 - logprior: -1.6890e+00
Epoch 9/10
10/10 - 1s - loss: 143.1610 - loglik: -1.4144e+02 - logprior: -1.4602e+00
Epoch 10/10
10/10 - 1s - loss: 142.8910 - loglik: -1.4132e+02 - logprior: -1.2943e+00
Fitted a model with MAP estimate = -142.4875
expansions: [(0, 2), (17, 2), (18, 2), (22, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.9479 - loglik: -1.4089e+02 - logprior: -5.4950e+01
Epoch 2/2
10/10 - 1s - loss: 151.5854 - loglik: -1.3467e+02 - logprior: -1.6684e+01
Fitted a model with MAP estimate = -143.3962
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.7192 - loglik: -1.3535e+02 - logprior: -4.7342e+01
Epoch 2/2
10/10 - 1s - loss: 152.6767 - loglik: -1.3422e+02 - logprior: -1.8354e+01
Fitted a model with MAP estimate = -147.2062
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.7273 - loglik: -1.3417e+02 - logprior: -4.2506e+01
Epoch 2/10
10/10 - 1s - loss: 144.4922 - loglik: -1.3293e+02 - logprior: -1.1502e+01
Epoch 3/10
10/10 - 1s - loss: 136.9933 - loglik: -1.3234e+02 - logprior: -4.4822e+00
Epoch 4/10
10/10 - 1s - loss: 134.0723 - loglik: -1.3181e+02 - logprior: -2.0099e+00
Epoch 5/10
10/10 - 1s - loss: 132.6816 - loglik: -1.3159e+02 - logprior: -8.2318e-01
Epoch 6/10
10/10 - 1s - loss: 131.8994 - loglik: -1.3135e+02 - logprior: -2.7072e-01
Epoch 7/10
10/10 - 1s - loss: 131.3106 - loglik: -1.3103e+02 - logprior: 0.0025
Epoch 8/10
10/10 - 1s - loss: 131.1556 - loglik: -1.3115e+02 - logprior: 0.2760
Epoch 9/10
10/10 - 1s - loss: 130.9243 - loglik: -1.3119e+02 - logprior: 0.5567
Epoch 10/10
10/10 - 1s - loss: 130.7097 - loglik: -1.3116e+02 - logprior: 0.7537
Fitted a model with MAP estimate = -130.3063
Time for alignment: 35.6903
Computed alignments with likelihoods: ['-130.1560', '-129.7578', '-130.3063']
Best model has likelihood: -129.7578
time for generating output: 0.1451
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.907318451539983
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac663d3310>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3347a5b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac332dfb50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4f4a700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac6655cfd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac665af2e0>, <__main__.SimpleDirichletPrior object at 0x7fabde691460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.0322 - loglik: -2.3774e+02 - logprior: -1.3209e+01
Epoch 2/10
11/11 - 2s - loss: 220.9838 - loglik: -2.1745e+02 - logprior: -3.4445e+00
Epoch 3/10
11/11 - 2s - loss: 199.9376 - loglik: -1.9746e+02 - logprior: -2.1839e+00
Epoch 4/10
11/11 - 2s - loss: 188.1389 - loglik: -1.8551e+02 - logprior: -2.1406e+00
Epoch 5/10
11/11 - 2s - loss: 184.9993 - loglik: -1.8232e+02 - logprior: -2.1318e+00
Epoch 6/10
11/11 - 2s - loss: 184.6762 - loglik: -1.8222e+02 - logprior: -1.9764e+00
Epoch 7/10
11/11 - 2s - loss: 183.7319 - loglik: -1.8151e+02 - logprior: -1.8220e+00
Epoch 8/10
11/11 - 2s - loss: 183.6304 - loglik: -1.8147e+02 - logprior: -1.7773e+00
Epoch 9/10
11/11 - 2s - loss: 183.2578 - loglik: -1.8109e+02 - logprior: -1.7888e+00
Epoch 10/10
11/11 - 2s - loss: 182.8076 - loglik: -1.8063e+02 - logprior: -1.7928e+00
Fitted a model with MAP estimate = -182.4268
expansions: [(8, 2), (9, 1), (10, 2), (17, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 201.9066 - loglik: -1.8675e+02 - logprior: -1.5050e+01
Epoch 2/2
11/11 - 2s - loss: 184.4358 - loglik: -1.7779e+02 - logprior: -6.3481e+00
Fitted a model with MAP estimate = -181.3448
expansions: [(0, 2)]
discards: [ 0 10]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 189.7723 - loglik: -1.7784e+02 - logprior: -1.1860e+01
Epoch 2/2
11/11 - 2s - loss: 177.9875 - loglik: -1.7475e+02 - logprior: -3.1297e+00
Fitted a model with MAP estimate = -175.9215
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.2018 - loglik: -1.7682e+02 - logprior: -1.4339e+01
Epoch 2/10
11/11 - 2s - loss: 180.0635 - loglik: -1.7565e+02 - logprior: -4.3349e+00
Epoch 3/10
11/11 - 2s - loss: 176.3327 - loglik: -1.7391e+02 - logprior: -2.2172e+00
Epoch 4/10
11/11 - 2s - loss: 174.9400 - loglik: -1.7308e+02 - logprior: -1.4962e+00
Epoch 5/10
11/11 - 2s - loss: 174.2625 - loglik: -1.7270e+02 - logprior: -1.1047e+00
Epoch 6/10
11/11 - 2s - loss: 173.3421 - loglik: -1.7187e+02 - logprior: -1.0096e+00
Epoch 7/10
11/11 - 2s - loss: 172.3869 - loglik: -1.7103e+02 - logprior: -9.0687e-01
Epoch 8/10
11/11 - 2s - loss: 173.4010 - loglik: -1.7209e+02 - logprior: -8.6737e-01
Fitted a model with MAP estimate = -172.0308
Time for alignment: 59.1089
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.2265 - loglik: -2.3696e+02 - logprior: -1.3206e+01
Epoch 2/10
11/11 - 2s - loss: 220.7073 - loglik: -2.1720e+02 - logprior: -3.4409e+00
Epoch 3/10
11/11 - 2s - loss: 198.8664 - loglik: -1.9648e+02 - logprior: -2.1532e+00
Epoch 4/10
11/11 - 2s - loss: 188.1029 - loglik: -1.8556e+02 - logprior: -2.0958e+00
Epoch 5/10
11/11 - 2s - loss: 184.7655 - loglik: -1.8213e+02 - logprior: -2.1073e+00
Epoch 6/10
11/11 - 2s - loss: 184.2527 - loglik: -1.8182e+02 - logprior: -1.9515e+00
Epoch 7/10
11/11 - 2s - loss: 184.2604 - loglik: -1.8204e+02 - logprior: -1.7956e+00
Fitted a model with MAP estimate = -182.7579
expansions: [(8, 2), (9, 1), (10, 3), (12, 1), (24, 1), (26, 2), (32, 1), (35, 2), (45, 1), (48, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 200.3353 - loglik: -1.8518e+02 - logprior: -1.5063e+01
Epoch 2/2
11/11 - 2s - loss: 184.4404 - loglik: -1.7780e+02 - logprior: -6.4004e+00
Fitted a model with MAP estimate = -180.5674
expansions: [(0, 2)]
discards: [ 0  9 13 34]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.1912 - loglik: -1.7629e+02 - logprior: -1.1849e+01
Epoch 2/2
11/11 - 2s - loss: 177.6319 - loglik: -1.7438e+02 - logprior: -3.1275e+00
Fitted a model with MAP estimate = -175.7209
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.1413 - loglik: -1.7582e+02 - logprior: -1.4229e+01
Epoch 2/10
11/11 - 2s - loss: 180.3836 - loglik: -1.7596e+02 - logprior: -4.2220e+00
Epoch 3/10
11/11 - 2s - loss: 175.6907 - loglik: -1.7313e+02 - logprior: -2.1940e+00
Epoch 4/10
11/11 - 2s - loss: 174.2021 - loglik: -1.7220e+02 - logprior: -1.4922e+00
Epoch 5/10
11/11 - 2s - loss: 173.4227 - loglik: -1.7175e+02 - logprior: -1.1077e+00
Epoch 6/10
11/11 - 2s - loss: 172.6948 - loglik: -1.7113e+02 - logprior: -1.0169e+00
Epoch 7/10
11/11 - 2s - loss: 172.8464 - loglik: -1.7143e+02 - logprior: -9.0798e-01
Fitted a model with MAP estimate = -171.8435
Time for alignment: 52.1862
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.8262 - loglik: -2.3750e+02 - logprior: -1.3206e+01
Epoch 2/10
11/11 - 2s - loss: 221.5958 - loglik: -2.1807e+02 - logprior: -3.4370e+00
Epoch 3/10
11/11 - 2s - loss: 198.5256 - loglik: -1.9607e+02 - logprior: -2.1533e+00
Epoch 4/10
11/11 - 2s - loss: 188.2417 - loglik: -1.8558e+02 - logprior: -2.1140e+00
Epoch 5/10
11/11 - 2s - loss: 185.6653 - loglik: -1.8300e+02 - logprior: -2.1276e+00
Epoch 6/10
11/11 - 2s - loss: 183.6468 - loglik: -1.8123e+02 - logprior: -1.9607e+00
Epoch 7/10
11/11 - 2s - loss: 184.9842 - loglik: -1.8279e+02 - logprior: -1.7978e+00
Fitted a model with MAP estimate = -183.1554
expansions: [(8, 2), (9, 1), (10, 3), (12, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 202.6418 - loglik: -1.8752e+02 - logprior: -1.5076e+01
Epoch 2/2
11/11 - 2s - loss: 184.6524 - loglik: -1.7825e+02 - logprior: -6.3400e+00
Fitted a model with MAP estimate = -181.6113
expansions: [(0, 2)]
discards: [ 0  9 13]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 187.7667 - loglik: -1.7570e+02 - logprior: -1.1836e+01
Epoch 2/2
11/11 - 2s - loss: 176.4888 - loglik: -1.7294e+02 - logprior: -3.1390e+00
Fitted a model with MAP estimate = -175.1184
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 190.5744 - loglik: -1.7639e+02 - logprior: -1.4151e+01
Epoch 2/10
11/11 - 2s - loss: 180.1865 - loglik: -1.7596e+02 - logprior: -4.1061e+00
Epoch 3/10
11/11 - 2s - loss: 176.0151 - loglik: -1.7360e+02 - logprior: -2.1824e+00
Epoch 4/10
11/11 - 2s - loss: 174.9464 - loglik: -1.7309e+02 - logprior: -1.5001e+00
Epoch 5/10
11/11 - 2s - loss: 173.1854 - loglik: -1.7163e+02 - logprior: -1.1012e+00
Epoch 6/10
11/11 - 2s - loss: 173.9137 - loglik: -1.7241e+02 - logprior: -1.0165e+00
Fitted a model with MAP estimate = -172.6266
Time for alignment: 51.2494
Computed alignments with likelihoods: ['-172.0308', '-171.8435', '-172.6266']
Best model has likelihood: -171.8435
time for generating output: 0.1454
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8193172356369692
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac332dfb50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4f4a700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac5dc15a90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac19a235e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba0c89af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabef11bf10>, <__main__.SimpleDirichletPrior object at 0x7fab2efcebe0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.4335 - loglik: -2.2372e+02 - logprior: -4.0683e+01
Epoch 2/10
10/10 - 1s - loss: 208.7126 - loglik: -1.9789e+02 - logprior: -1.0797e+01
Epoch 3/10
10/10 - 1s - loss: 179.0485 - loglik: -1.7356e+02 - logprior: -5.4664e+00
Epoch 4/10
10/10 - 1s - loss: 158.5650 - loglik: -1.5480e+02 - logprior: -3.6979e+00
Epoch 5/10
10/10 - 1s - loss: 148.9675 - loglik: -1.4583e+02 - logprior: -2.9139e+00
Epoch 6/10
10/10 - 1s - loss: 145.2202 - loglik: -1.4228e+02 - logprior: -2.5925e+00
Epoch 7/10
10/10 - 1s - loss: 144.0147 - loglik: -1.4130e+02 - logprior: -2.3659e+00
Epoch 8/10
10/10 - 1s - loss: 142.9264 - loglik: -1.4037e+02 - logprior: -2.1742e+00
Epoch 9/10
10/10 - 1s - loss: 142.6300 - loglik: -1.4015e+02 - logprior: -2.0554e+00
Epoch 10/10
10/10 - 1s - loss: 142.2979 - loglik: -1.3991e+02 - logprior: -1.9829e+00
Fitted a model with MAP estimate = -141.7616
expansions: [(2, 1), (5, 2), (13, 1), (16, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (51, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 173.7069 - loglik: -1.3661e+02 - logprior: -3.7051e+01
Epoch 2/2
10/10 - 1s - loss: 138.4252 - loglik: -1.2836e+02 - logprior: -9.9533e+00
Fitted a model with MAP estimate = -132.9833
expansions: []
discards: [45 58 66 69]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.9532 - loglik: -1.2786e+02 - logprior: -3.5987e+01
Epoch 2/2
10/10 - 1s - loss: 135.6511 - loglik: -1.2629e+02 - logprior: -9.3498e+00
Fitted a model with MAP estimate = -131.9674
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 175.8022 - loglik: -1.3066e+02 - logprior: -4.5025e+01
Epoch 2/10
10/10 - 1s - loss: 148.2957 - loglik: -1.2974e+02 - logprior: -1.8394e+01
Epoch 3/10
10/10 - 1s - loss: 141.6100 - loglik: -1.2844e+02 - logprior: -1.2921e+01
Epoch 4/10
10/10 - 1s - loss: 138.6256 - loglik: -1.2778e+02 - logprior: -1.0567e+01
Epoch 5/10
10/10 - 1s - loss: 135.8317 - loglik: -1.2785e+02 - logprior: -7.6894e+00
Epoch 6/10
10/10 - 1s - loss: 129.9751 - loglik: -1.2790e+02 - logprior: -1.7644e+00
Epoch 7/10
10/10 - 1s - loss: 127.5293 - loglik: -1.2763e+02 - logprior: 0.4396
Epoch 8/10
10/10 - 1s - loss: 127.6909 - loglik: -1.2814e+02 - logprior: 0.8027
Fitted a model with MAP estimate = -126.9298
Time for alignment: 37.7299
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.5975 - loglik: -2.2388e+02 - logprior: -4.0686e+01
Epoch 2/10
10/10 - 1s - loss: 208.4871 - loglik: -1.9767e+02 - logprior: -1.0805e+01
Epoch 3/10
10/10 - 1s - loss: 177.4719 - loglik: -1.7198e+02 - logprior: -5.4638e+00
Epoch 4/10
10/10 - 1s - loss: 155.6402 - loglik: -1.5184e+02 - logprior: -3.7310e+00
Epoch 5/10
10/10 - 1s - loss: 147.4460 - loglik: -1.4423e+02 - logprior: -3.0078e+00
Epoch 6/10
10/10 - 1s - loss: 144.3709 - loglik: -1.4138e+02 - logprior: -2.6438e+00
Epoch 7/10
10/10 - 1s - loss: 143.6057 - loglik: -1.4094e+02 - logprior: -2.3243e+00
Epoch 8/10
10/10 - 1s - loss: 142.8038 - loglik: -1.4034e+02 - logprior: -2.1315e+00
Epoch 9/10
10/10 - 1s - loss: 142.1734 - loglik: -1.3981e+02 - logprior: -2.0236e+00
Epoch 10/10
10/10 - 1s - loss: 141.8971 - loglik: -1.3959e+02 - logprior: -1.9577e+00
Fitted a model with MAP estimate = -141.4417
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.2046 - loglik: -1.3422e+02 - logprior: -3.6922e+01
Epoch 2/2
10/10 - 1s - loss: 137.0334 - loglik: -1.2725e+02 - logprior: -9.6194e+00
Fitted a model with MAP estimate = -131.6693
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.7419 - loglik: -1.3141e+02 - logprior: -4.5240e+01
Epoch 2/2
10/10 - 1s - loss: 148.7649 - loglik: -1.3006e+02 - logprior: -1.8523e+01
Fitted a model with MAP estimate = -144.3632
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 164.7498 - loglik: -1.2837e+02 - logprior: -3.6351e+01
Epoch 2/10
10/10 - 1s - loss: 135.7223 - loglik: -1.2653e+02 - logprior: -9.0809e+00
Epoch 3/10
10/10 - 1s - loss: 129.1503 - loglik: -1.2535e+02 - logprior: -3.5806e+00
Epoch 4/10
10/10 - 1s - loss: 127.2853 - loglik: -1.2565e+02 - logprior: -1.3818e+00
Epoch 5/10
10/10 - 1s - loss: 125.8532 - loglik: -1.2535e+02 - logprior: -2.3332e-01
Epoch 6/10
10/10 - 1s - loss: 125.5449 - loglik: -1.2565e+02 - logprior: 0.3889
Epoch 7/10
10/10 - 1s - loss: 125.0416 - loglik: -1.2547e+02 - logprior: 0.7419
Epoch 8/10
10/10 - 1s - loss: 124.7787 - loglik: -1.2544e+02 - logprior: 0.9862
Epoch 9/10
10/10 - 1s - loss: 124.5259 - loglik: -1.2537e+02 - logprior: 1.1932
Epoch 10/10
10/10 - 1s - loss: 124.2770 - loglik: -1.2529e+02 - logprior: 1.3662
Fitted a model with MAP estimate = -123.8707
Time for alignment: 40.1502
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.3658 - loglik: -2.2354e+02 - logprior: -4.0682e+01
Epoch 2/10
10/10 - 1s - loss: 209.0352 - loglik: -1.9817e+02 - logprior: -1.0798e+01
Epoch 3/10
10/10 - 1s - loss: 178.3947 - loglik: -1.7289e+02 - logprior: -5.4745e+00
Epoch 4/10
10/10 - 1s - loss: 155.9876 - loglik: -1.5212e+02 - logprior: -3.7960e+00
Epoch 5/10
10/10 - 1s - loss: 147.3283 - loglik: -1.4405e+02 - logprior: -3.0785e+00
Epoch 6/10
10/10 - 1s - loss: 144.6723 - loglik: -1.4165e+02 - logprior: -2.6744e+00
Epoch 7/10
10/10 - 1s - loss: 143.1852 - loglik: -1.4040e+02 - logprior: -2.4129e+00
Epoch 8/10
10/10 - 1s - loss: 142.8016 - loglik: -1.4024e+02 - logprior: -2.2038e+00
Epoch 9/10
10/10 - 1s - loss: 141.8632 - loglik: -1.3942e+02 - logprior: -2.0753e+00
Epoch 10/10
10/10 - 1s - loss: 141.4470 - loglik: -1.3909e+02 - logprior: -2.0122e+00
Fitted a model with MAP estimate = -141.0365
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 170.4972 - loglik: -1.3338e+02 - logprior: -3.6899e+01
Epoch 2/2
10/10 - 1s - loss: 136.3741 - loglik: -1.2652e+02 - logprior: -9.6537e+00
Fitted a model with MAP estimate = -131.5541
expansions: []
discards: [ 0 46 59 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 177.3638 - loglik: -1.3199e+02 - logprior: -4.5288e+01
Epoch 2/2
10/10 - 1s - loss: 149.1301 - loglik: -1.3054e+02 - logprior: -1.8511e+01
Fitted a model with MAP estimate = -144.7864
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 165.0848 - loglik: -1.2852e+02 - logprior: -3.6399e+01
Epoch 2/10
10/10 - 1s - loss: 136.0973 - loglik: -1.2700e+02 - logprior: -9.0495e+00
Epoch 3/10
10/10 - 1s - loss: 129.7563 - loglik: -1.2619e+02 - logprior: -3.5235e+00
Epoch 4/10
10/10 - 1s - loss: 127.6343 - loglik: -1.2615e+02 - logprior: -1.3291e+00
Epoch 5/10
10/10 - 1s - loss: 126.2265 - loglik: -1.2580e+02 - logprior: -1.9238e-01
Epoch 6/10
10/10 - 1s - loss: 125.4775 - loglik: -1.2565e+02 - logprior: 0.4203
Epoch 7/10
10/10 - 1s - loss: 125.3940 - loglik: -1.2589e+02 - logprior: 0.7664
Epoch 8/10
10/10 - 1s - loss: 124.7932 - loglik: -1.2551e+02 - logprior: 1.0006
Epoch 9/10
10/10 - 1s - loss: 124.7897 - loglik: -1.2567e+02 - logprior: 1.1956
Epoch 10/10
10/10 - 1s - loss: 124.3604 - loglik: -1.2539e+02 - logprior: 1.3627
Fitted a model with MAP estimate = -124.0224
Time for alignment: 39.5056
Computed alignments with likelihoods: ['-126.9298', '-123.8707', '-124.0224']
Best model has likelihood: -123.8707
time for generating output: 0.1423
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.9243339253996448
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac3368fd00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabb3652040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabcd584b20>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac447224c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4cab8f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac5dc15a90>, <__main__.SimpleDirichletPrior object at 0x7fac76d74820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4826 - loglik: -1.6665e+02 - logprior: -5.6215e+00
Epoch 2/10
15/15 - 1s - loss: 146.3179 - loglik: -1.4448e+02 - logprior: -1.7652e+00
Epoch 3/10
15/15 - 1s - loss: 130.2506 - loglik: -1.2815e+02 - logprior: -1.8372e+00
Epoch 4/10
15/15 - 1s - loss: 125.7538 - loglik: -1.2341e+02 - logprior: -1.8084e+00
Epoch 5/10
15/15 - 1s - loss: 124.8807 - loglik: -1.2269e+02 - logprior: -1.7090e+00
Epoch 6/10
15/15 - 1s - loss: 124.3954 - loglik: -1.2225e+02 - logprior: -1.7137e+00
Epoch 7/10
15/15 - 1s - loss: 124.1268 - loglik: -1.2201e+02 - logprior: -1.6920e+00
Epoch 8/10
15/15 - 1s - loss: 124.0014 - loglik: -1.2192e+02 - logprior: -1.6703e+00
Epoch 9/10
15/15 - 1s - loss: 123.8471 - loglik: -1.2175e+02 - logprior: -1.6657e+00
Epoch 10/10
15/15 - 1s - loss: 123.7075 - loglik: -1.2161e+02 - logprior: -1.6578e+00
Fitted a model with MAP estimate = -123.1353
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 137.0946 - loglik: -1.2996e+02 - logprior: -6.9214e+00
Epoch 2/2
15/15 - 1s - loss: 126.7881 - loglik: -1.2281e+02 - logprior: -3.4949e+00
Fitted a model with MAP estimate = -123.9702
expansions: [(0, 2)]
discards: [ 0 15 31 46]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0411 - loglik: -1.2285e+02 - logprior: -5.1479e+00
Epoch 2/2
15/15 - 1s - loss: 120.9553 - loglik: -1.1912e+02 - logprior: -1.6937e+00
Fitted a model with MAP estimate = -119.6164
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 129.8428 - loglik: -1.2326e+02 - logprior: -6.5422e+00
Epoch 2/10
15/15 - 1s - loss: 122.0925 - loglik: -1.1984e+02 - logprior: -2.2076e+00
Epoch 3/10
15/15 - 1s - loss: 120.5019 - loglik: -1.1882e+02 - logprior: -1.5069e+00
Epoch 4/10
15/15 - 1s - loss: 119.8601 - loglik: -1.1822e+02 - logprior: -1.3473e+00
Epoch 5/10
15/15 - 1s - loss: 119.4243 - loglik: -1.1776e+02 - logprior: -1.2941e+00
Epoch 6/10
15/15 - 1s - loss: 119.0323 - loglik: -1.1734e+02 - logprior: -1.2791e+00
Epoch 7/10
15/15 - 1s - loss: 118.7505 - loglik: -1.1705e+02 - logprior: -1.2739e+00
Epoch 8/10
15/15 - 1s - loss: 118.1675 - loglik: -1.1648e+02 - logprior: -1.2515e+00
Epoch 9/10
15/15 - 1s - loss: 118.3568 - loglik: -1.1668e+02 - logprior: -1.2345e+00
Fitted a model with MAP estimate = -117.6176
Time for alignment: 45.6539
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4490 - loglik: -1.6674e+02 - logprior: -5.6200e+00
Epoch 2/10
15/15 - 1s - loss: 145.3400 - loglik: -1.4351e+02 - logprior: -1.7659e+00
Epoch 3/10
15/15 - 1s - loss: 129.8212 - loglik: -1.2770e+02 - logprior: -1.8393e+00
Epoch 4/10
15/15 - 1s - loss: 125.7756 - loglik: -1.2340e+02 - logprior: -1.8113e+00
Epoch 5/10
15/15 - 1s - loss: 124.6657 - loglik: -1.2244e+02 - logprior: -1.7147e+00
Epoch 6/10
15/15 - 1s - loss: 124.1978 - loglik: -1.2204e+02 - logprior: -1.7178e+00
Epoch 7/10
15/15 - 1s - loss: 124.0498 - loglik: -1.2194e+02 - logprior: -1.6914e+00
Epoch 8/10
15/15 - 1s - loss: 123.8579 - loglik: -1.2177e+02 - logprior: -1.6717e+00
Epoch 9/10
15/15 - 1s - loss: 123.4941 - loglik: -1.2140e+02 - logprior: -1.6644e+00
Epoch 10/10
15/15 - 1s - loss: 123.5520 - loglik: -1.2144e+02 - logprior: -1.6590e+00
Fitted a model with MAP estimate = -122.9087
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 137.4242 - loglik: -1.3030e+02 - logprior: -6.8905e+00
Epoch 2/2
15/15 - 1s - loss: 126.7223 - loglik: -1.2294e+02 - logprior: -3.4761e+00
Fitted a model with MAP estimate = -124.1620
expansions: [(0, 2)]
discards: [ 0 15 45]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.2208 - loglik: -1.2196e+02 - logprior: -5.1485e+00
Epoch 2/2
15/15 - 1s - loss: 120.6706 - loglik: -1.1870e+02 - logprior: -1.6977e+00
Fitted a model with MAP estimate = -119.3505
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.0357 - loglik: -1.2243e+02 - logprior: -6.5284e+00
Epoch 2/10
15/15 - 1s - loss: 121.8738 - loglik: -1.1949e+02 - logprior: -2.1969e+00
Epoch 3/10
15/15 - 1s - loss: 119.8746 - loglik: -1.1804e+02 - logprior: -1.5129e+00
Epoch 4/10
15/15 - 1s - loss: 119.4644 - loglik: -1.1767e+02 - logprior: -1.3510e+00
Epoch 5/10
15/15 - 1s - loss: 118.9431 - loglik: -1.1714e+02 - logprior: -1.3047e+00
Epoch 6/10
15/15 - 1s - loss: 118.6206 - loglik: -1.1684e+02 - logprior: -1.2872e+00
Epoch 7/10
15/15 - 1s - loss: 118.3052 - loglik: -1.1654e+02 - logprior: -1.2800e+00
Epoch 8/10
15/15 - 1s - loss: 118.1889 - loglik: -1.1644e+02 - logprior: -1.2613e+00
Epoch 9/10
15/15 - 1s - loss: 117.9309 - loglik: -1.1620e+02 - logprior: -1.2454e+00
Epoch 10/10
15/15 - 1s - loss: 117.7796 - loglik: -1.1606e+02 - logprior: -1.2287e+00
Fitted a model with MAP estimate = -117.1577
Time for alignment: 45.3302
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 172.3727 - loglik: -1.6671e+02 - logprior: -5.6240e+00
Epoch 2/10
15/15 - 1s - loss: 145.0887 - loglik: -1.4325e+02 - logprior: -1.7729e+00
Epoch 3/10
15/15 - 1s - loss: 130.1120 - loglik: -1.2798e+02 - logprior: -1.8260e+00
Epoch 4/10
15/15 - 1s - loss: 126.1228 - loglik: -1.2380e+02 - logprior: -1.7880e+00
Epoch 5/10
15/15 - 1s - loss: 124.9872 - loglik: -1.2281e+02 - logprior: -1.7071e+00
Epoch 6/10
15/15 - 1s - loss: 124.4809 - loglik: -1.2233e+02 - logprior: -1.7228e+00
Epoch 7/10
15/15 - 1s - loss: 124.2011 - loglik: -1.2209e+02 - logprior: -1.6928e+00
Epoch 8/10
15/15 - 1s - loss: 123.8204 - loglik: -1.2173e+02 - logprior: -1.6757e+00
Epoch 9/10
15/15 - 1s - loss: 123.7463 - loglik: -1.2165e+02 - logprior: -1.6673e+00
Epoch 10/10
15/15 - 1s - loss: 123.7424 - loglik: -1.2164e+02 - logprior: -1.6580e+00
Fitted a model with MAP estimate = -123.1342
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 2), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 137.4708 - loglik: -1.3028e+02 - logprior: -6.9114e+00
Epoch 2/2
15/15 - 1s - loss: 126.9237 - loglik: -1.2299e+02 - logprior: -3.4938e+00
Fitted a model with MAP estimate = -124.1252
expansions: [(0, 2)]
discards: [ 0 13 15 31 39 46]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.1498 - loglik: -1.2295e+02 - logprior: -5.1255e+00
Epoch 2/2
15/15 - 1s - loss: 120.7501 - loglik: -1.1883e+02 - logprior: -1.6832e+00
Fitted a model with MAP estimate = -119.6070
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.2537 - loglik: -1.2265e+02 - logprior: -6.5271e+00
Epoch 2/10
15/15 - 1s - loss: 122.0387 - loglik: -1.1958e+02 - logprior: -2.2043e+00
Epoch 3/10
15/15 - 1s - loss: 120.1517 - loglik: -1.1822e+02 - logprior: -1.5012e+00
Epoch 4/10
15/15 - 1s - loss: 119.6868 - loglik: -1.1787e+02 - logprior: -1.3404e+00
Epoch 5/10
15/15 - 1s - loss: 119.2450 - loglik: -1.1747e+02 - logprior: -1.2883e+00
Epoch 6/10
15/15 - 1s - loss: 119.0545 - loglik: -1.1730e+02 - logprior: -1.2749e+00
Epoch 7/10
15/15 - 1s - loss: 118.5686 - loglik: -1.1683e+02 - logprior: -1.2625e+00
Epoch 8/10
15/15 - 1s - loss: 118.5777 - loglik: -1.1687e+02 - logprior: -1.2429e+00
Fitted a model with MAP estimate = -117.9202
Time for alignment: 43.7168
Computed alignments with likelihoods: ['-117.6176', '-117.1577', '-117.9202']
Best model has likelihood: -117.1577
time for generating output: 0.1327
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabef0556a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1f37400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac447c7f40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac447f2c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe684bdf0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabe69a4f10>, <__main__.SimpleDirichletPrior object at 0x7fac3365af10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.6722 - loglik: -1.8329e+02 - logprior: -1.4340e+01
Epoch 2/10
10/10 - 2s - loss: 170.5519 - loglik: -1.6645e+02 - logprior: -4.0851e+00
Epoch 3/10
10/10 - 2s - loss: 152.6493 - loglik: -1.5012e+02 - logprior: -2.4419e+00
Epoch 4/10
10/10 - 2s - loss: 142.0396 - loglik: -1.3965e+02 - logprior: -2.1735e+00
Epoch 5/10
10/10 - 2s - loss: 136.6582 - loglik: -1.3416e+02 - logprior: -2.2012e+00
Epoch 6/10
10/10 - 2s - loss: 135.1425 - loglik: -1.3267e+02 - logprior: -2.2359e+00
Epoch 7/10
10/10 - 2s - loss: 134.3165 - loglik: -1.3197e+02 - logprior: -2.1442e+00
Epoch 8/10
10/10 - 2s - loss: 133.2761 - loglik: -1.3108e+02 - logprior: -2.0316e+00
Epoch 9/10
10/10 - 2s - loss: 133.3165 - loglik: -1.3117e+02 - logprior: -1.9933e+00
Fitted a model with MAP estimate = -132.8100
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (33, 3), (34, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 148.2167 - loglik: -1.3209e+02 - logprior: -1.6051e+01
Epoch 2/2
10/10 - 3s - loss: 129.2656 - loglik: -1.2236e+02 - logprior: -6.8329e+00
Fitted a model with MAP estimate = -125.5394
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 132.0329 - loglik: -1.1933e+02 - logprior: -1.2679e+01
Epoch 2/2
10/10 - 2s - loss: 121.6467 - loglik: -1.1810e+02 - logprior: -3.4891e+00
Fitted a model with MAP estimate = -119.8489
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.2508 - loglik: -1.2012e+02 - logprior: -1.5056e+01
Epoch 2/10
10/10 - 2s - loss: 123.5517 - loglik: -1.1864e+02 - logprior: -4.7273e+00
Epoch 3/10
10/10 - 2s - loss: 120.2618 - loglik: -1.1745e+02 - logprior: -2.5388e+00
Epoch 4/10
10/10 - 2s - loss: 119.2689 - loglik: -1.1717e+02 - logprior: -1.8010e+00
Epoch 5/10
10/10 - 2s - loss: 118.3278 - loglik: -1.1676e+02 - logprior: -1.2995e+00
Epoch 6/10
10/10 - 2s - loss: 117.9032 - loglik: -1.1650e+02 - logprior: -1.1749e+00
Epoch 7/10
10/10 - 2s - loss: 118.3285 - loglik: -1.1707e+02 - logprior: -1.0568e+00
Fitted a model with MAP estimate = -117.6294
Time for alignment: 65.0688
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.4879 - loglik: -1.8291e+02 - logprior: -1.4339e+01
Epoch 2/10
10/10 - 2s - loss: 170.8808 - loglik: -1.6671e+02 - logprior: -4.0837e+00
Epoch 3/10
10/10 - 2s - loss: 152.2498 - loglik: -1.4968e+02 - logprior: -2.4534e+00
Epoch 4/10
10/10 - 2s - loss: 140.7052 - loglik: -1.3826e+02 - logprior: -2.2130e+00
Epoch 5/10
10/10 - 2s - loss: 136.2486 - loglik: -1.3370e+02 - logprior: -2.3012e+00
Epoch 6/10
10/10 - 2s - loss: 134.1301 - loglik: -1.3154e+02 - logprior: -2.3735e+00
Epoch 7/10
10/10 - 2s - loss: 133.0442 - loglik: -1.3055e+02 - logprior: -2.3080e+00
Epoch 8/10
10/10 - 2s - loss: 132.0880 - loglik: -1.2973e+02 - logprior: -2.2065e+00
Epoch 9/10
10/10 - 2s - loss: 131.8539 - loglik: -1.2956e+02 - logprior: -2.1687e+00
Epoch 10/10
10/10 - 2s - loss: 131.2715 - loglik: -1.2897e+02 - logprior: -2.1751e+00
Fitted a model with MAP estimate = -131.1192
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (34, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 149.7406 - loglik: -1.3359e+02 - logprior: -1.6070e+01
Epoch 2/2
10/10 - 2s - loss: 129.5988 - loglik: -1.2267e+02 - logprior: -6.8514e+00
Fitted a model with MAP estimate = -125.9608
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 132.5154 - loglik: -1.1980e+02 - logprior: -1.2679e+01
Epoch 2/2
10/10 - 2s - loss: 121.4985 - loglik: -1.1794e+02 - logprior: -3.4857e+00
Fitted a model with MAP estimate = -119.8734
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.4568 - loglik: -1.2041e+02 - logprior: -1.5030e+01
Epoch 2/10
10/10 - 2s - loss: 123.7843 - loglik: -1.1904e+02 - logprior: -4.6885e+00
Epoch 3/10
10/10 - 2s - loss: 120.6929 - loglik: -1.1799e+02 - logprior: -2.5119e+00
Epoch 4/10
10/10 - 2s - loss: 119.4115 - loglik: -1.1734e+02 - logprior: -1.7911e+00
Epoch 5/10
10/10 - 2s - loss: 118.3315 - loglik: -1.1678e+02 - logprior: -1.2872e+00
Epoch 6/10
10/10 - 2s - loss: 118.6094 - loglik: -1.1722e+02 - logprior: -1.1712e+00
Fitted a model with MAP estimate = -117.8232
Time for alignment: 63.4075
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.8005 - loglik: -1.8342e+02 - logprior: -1.4343e+01
Epoch 2/10
10/10 - 2s - loss: 170.6476 - loglik: -1.6651e+02 - logprior: -4.0992e+00
Epoch 3/10
10/10 - 2s - loss: 153.6171 - loglik: -1.5094e+02 - logprior: -2.5118e+00
Epoch 4/10
10/10 - 2s - loss: 140.9781 - loglik: -1.3830e+02 - logprior: -2.3628e+00
Epoch 5/10
10/10 - 2s - loss: 136.6029 - loglik: -1.3376e+02 - logprior: -2.5027e+00
Epoch 6/10
10/10 - 2s - loss: 134.5302 - loglik: -1.3171e+02 - logprior: -2.5821e+00
Epoch 7/10
10/10 - 2s - loss: 133.4009 - loglik: -1.3074e+02 - logprior: -2.4825e+00
Epoch 8/10
10/10 - 2s - loss: 133.0705 - loglik: -1.3060e+02 - logprior: -2.3355e+00
Epoch 9/10
10/10 - 2s - loss: 132.7800 - loglik: -1.3040e+02 - logprior: -2.2572e+00
Epoch 10/10
10/10 - 2s - loss: 132.4941 - loglik: -1.3013e+02 - logprior: -2.2403e+00
Fitted a model with MAP estimate = -132.3515
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 150.0854 - loglik: -1.3399e+02 - logprior: -1.6052e+01
Epoch 2/2
10/10 - 2s - loss: 129.5943 - loglik: -1.2275e+02 - logprior: -6.8320e+00
Fitted a model with MAP estimate = -125.9890
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 131.6913 - loglik: -1.1884e+02 - logprior: -1.2680e+01
Epoch 2/2
10/10 - 2s - loss: 121.2355 - loglik: -1.1744e+02 - logprior: -3.4990e+00
Fitted a model with MAP estimate = -119.2361
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 136.1419 - loglik: -1.2089e+02 - logprior: -1.5073e+01
Epoch 2/10
10/10 - 2s - loss: 124.2638 - loglik: -1.1948e+02 - logprior: -4.7053e+00
Epoch 3/10
10/10 - 2s - loss: 120.4613 - loglik: -1.1790e+02 - logprior: -2.4871e+00
Epoch 4/10
10/10 - 2s - loss: 119.1688 - loglik: -1.1725e+02 - logprior: -1.8007e+00
Epoch 5/10
10/10 - 2s - loss: 118.7752 - loglik: -1.1732e+02 - logprior: -1.2866e+00
Epoch 6/10
10/10 - 2s - loss: 118.1247 - loglik: -1.1677e+02 - logprior: -1.1583e+00
Epoch 7/10
10/10 - 2s - loss: 118.2940 - loglik: -1.1703e+02 - logprior: -1.0629e+00
Fitted a model with MAP estimate = -117.6756
Time for alignment: 64.5370
Computed alignments with likelihoods: ['-117.6294', '-117.8232', '-117.6756']
Best model has likelihood: -117.6294
time for generating output: 0.2736
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabc4d80430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac19db8370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac330b5fd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac33361580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaa852370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabe6aece20>, <__main__.SimpleDirichletPrior object at 0x7fac33640130>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 526.8347 - loglik: -5.2028e+02 - logprior: -6.2139e+00
Epoch 2/10
23/23 - 6s - loss: 452.0259 - loglik: -4.4990e+02 - logprior: -9.5699e-01
Epoch 3/10
23/23 - 6s - loss: 428.9076 - loglik: -4.2623e+02 - logprior: -9.6163e-01
Epoch 4/10
23/23 - 6s - loss: 426.4621 - loglik: -4.2404e+02 - logprior: -7.7468e-01
Epoch 5/10
23/23 - 6s - loss: 422.0065 - loglik: -4.2004e+02 - logprior: -6.8475e-01
Epoch 6/10
23/23 - 6s - loss: 421.9847 - loglik: -4.2029e+02 - logprior: -6.7750e-01
Epoch 7/10
23/23 - 6s - loss: 421.9313 - loglik: -4.2041e+02 - logprior: -6.5555e-01
Epoch 8/10
23/23 - 6s - loss: 420.8699 - loglik: -4.1947e+02 - logprior: -6.4942e-01
Epoch 9/10
23/23 - 6s - loss: 420.1501 - loglik: -4.1883e+02 - logprior: -6.3627e-01
Epoch 10/10
23/23 - 6s - loss: 420.1391 - loglik: -4.1889e+02 - logprior: -6.2067e-01
Fitted a model with MAP estimate = -419.2639
expansions: [(0, 8), (8, 5), (18, 1), (51, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 1), (80, 3), (83, 2), (86, 1), (106, 1), (113, 1), (119, 1), (121, 1), (124, 1), (126, 1), (127, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 449.8843 - loglik: -4.4105e+02 - logprior: -8.6411e+00
Epoch 2/2
23/23 - 7s - loss: 418.2530 - loglik: -4.1716e+02 - logprior: -5.9687e-01
Fitted a model with MAP estimate = -411.8091
expansions: [(0, 8)]
discards: [  1   2   3   4   5   6   7   8 108 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 432.5281 - loglik: -4.2343e+02 - logprior: -8.8219e+00
Epoch 2/2
23/23 - 7s - loss: 415.6951 - loglik: -4.1471e+02 - logprior: -3.8536e-01
Fitted a model with MAP estimate = -410.7339
expansions: [(0, 7), (196, 5)]
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 433.5203 - loglik: -4.2485e+02 - logprior: -8.5609e+00
Epoch 2/10
23/23 - 7s - loss: 417.4500 - loglik: -4.1665e+02 - logprior: -4.7310e-01
Epoch 3/10
23/23 - 7s - loss: 408.5056 - loglik: -4.0876e+02 - logprior: 0.8323
Epoch 4/10
23/23 - 7s - loss: 404.5111 - loglik: -4.0491e+02 - logprior: 1.2097
Epoch 5/10
23/23 - 7s - loss: 403.9475 - loglik: -4.0439e+02 - logprior: 1.3735
Epoch 6/10
23/23 - 7s - loss: 402.6459 - loglik: -4.0321e+02 - logprior: 1.5152
Epoch 7/10
23/23 - 7s - loss: 402.5100 - loglik: -4.0323e+02 - logprior: 1.6462
Epoch 8/10
23/23 - 7s - loss: 400.8472 - loglik: -4.0174e+02 - logprior: 1.7722
Epoch 9/10
23/23 - 7s - loss: 401.4200 - loglik: -4.0250e+02 - logprior: 1.9112
Fitted a model with MAP estimate = -399.6770
Time for alignment: 197.3810
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 527.7238 - loglik: -5.2124e+02 - logprior: -6.2493e+00
Epoch 2/10
23/23 - 6s - loss: 453.5973 - loglik: -4.5183e+02 - logprior: -9.4388e-01
Epoch 3/10
23/23 - 6s - loss: 431.4140 - loglik: -4.2898e+02 - logprior: -1.0299e+00
Epoch 4/10
23/23 - 6s - loss: 425.6115 - loglik: -4.2339e+02 - logprior: -8.4046e-01
Epoch 5/10
23/23 - 6s - loss: 422.9255 - loglik: -4.2100e+02 - logprior: -7.9446e-01
Epoch 6/10
23/23 - 6s - loss: 422.3271 - loglik: -4.2056e+02 - logprior: -8.1019e-01
Epoch 7/10
23/23 - 6s - loss: 419.4838 - loglik: -4.1780e+02 - logprior: -8.2094e-01
Epoch 8/10
23/23 - 6s - loss: 421.7666 - loglik: -4.2019e+02 - logprior: -8.0595e-01
Fitted a model with MAP estimate = -419.1443
expansions: [(0, 8), (8, 5), (30, 1), (41, 1), (50, 1), (55, 2), (56, 2), (58, 1), (65, 1), (69, 1), (70, 2), (71, 1), (76, 1), (77, 1), (86, 1), (107, 1), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 447.9207 - loglik: -4.3919e+02 - logprior: -8.5315e+00
Epoch 2/2
23/23 - 7s - loss: 417.4493 - loglik: -4.1645e+02 - logprior: -5.6733e-01
Fitted a model with MAP estimate = -411.3206
expansions: [(0, 9)]
discards: [  1   2   3   4   5   6   7   8  72  74  93 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 431.0899 - loglik: -4.2195e+02 - logprior: -8.7924e+00
Epoch 2/2
23/23 - 7s - loss: 415.8745 - loglik: -4.1471e+02 - logprior: -4.4021e-01
Fitted a model with MAP estimate = -409.8110
expansions: [(0, 7), (197, 5)]
discards: [  1   2   3   4   5   6   7   8   9 140]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 435.4214 - loglik: -4.2645e+02 - logprior: -8.9150e+00
Epoch 2/10
23/23 - 7s - loss: 418.5298 - loglik: -4.1765e+02 - logprior: -5.5261e-01
Epoch 3/10
23/23 - 7s - loss: 409.0793 - loglik: -4.0885e+02 - logprior: 0.7876
Epoch 4/10
23/23 - 7s - loss: 404.9629 - loglik: -4.0479e+02 - logprior: 1.1542
Epoch 5/10
23/23 - 7s - loss: 404.2846 - loglik: -4.0440e+02 - logprior: 1.3135
Epoch 6/10
23/23 - 7s - loss: 402.6151 - loglik: -4.0302e+02 - logprior: 1.4527
Epoch 7/10
23/23 - 8s - loss: 401.8486 - loglik: -4.0248e+02 - logprior: 1.5688
Epoch 8/10
23/23 - 7s - loss: 401.8446 - loglik: -4.0272e+02 - logprior: 1.7063
Epoch 9/10
23/23 - 7s - loss: 400.4086 - loglik: -4.0150e+02 - logprior: 1.8399
Epoch 10/10
23/23 - 7s - loss: 401.1549 - loglik: -4.0241e+02 - logprior: 1.9664
Fitted a model with MAP estimate = -399.3368
Time for alignment: 192.3161
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 527.0495 - loglik: -5.2058e+02 - logprior: -6.2486e+00
Epoch 2/10
23/23 - 6s - loss: 451.1596 - loglik: -4.4940e+02 - logprior: -1.0483e+00
Epoch 3/10
23/23 - 6s - loss: 428.3344 - loglik: -4.2611e+02 - logprior: -9.7778e-01
Epoch 4/10
23/23 - 6s - loss: 424.0365 - loglik: -4.2194e+02 - logprior: -7.7720e-01
Epoch 5/10
23/23 - 6s - loss: 421.5815 - loglik: -4.1967e+02 - logprior: -7.1489e-01
Epoch 6/10
23/23 - 6s - loss: 421.0217 - loglik: -4.1927e+02 - logprior: -7.1269e-01
Epoch 7/10
23/23 - 6s - loss: 420.4886 - loglik: -4.1885e+02 - logprior: -7.0814e-01
Epoch 8/10
23/23 - 6s - loss: 419.2237 - loglik: -4.1769e+02 - logprior: -7.0224e-01
Epoch 9/10
23/23 - 6s - loss: 419.2742 - loglik: -4.1784e+02 - logprior: -6.9235e-01
Fitted a model with MAP estimate = -418.1487
expansions: [(0, 8), (8, 5), (30, 1), (54, 1), (56, 2), (57, 2), (59, 1), (66, 1), (71, 1), (72, 1), (78, 4), (87, 1), (108, 1), (112, 2), (113, 1), (119, 1), (121, 1), (124, 1), (127, 1), (133, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 12s - loss: 446.4977 - loglik: -4.3778e+02 - logprior: -8.5909e+00
Epoch 2/2
23/23 - 8s - loss: 421.4890 - loglik: -4.2047e+02 - logprior: -5.8042e-01
Fitted a model with MAP estimate = -411.6305
expansions: [(0, 10)]
discards: [  1   2   3   4   5   6   7   8  72  74 103 142 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 434.3207 - loglik: -4.2550e+02 - logprior: -8.7294e+00
Epoch 2/2
23/23 - 7s - loss: 417.1826 - loglik: -4.1654e+02 - logprior: -3.1309e-01
Fitted a model with MAP estimate = -412.1983
expansions: [(0, 10), (197, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 433.2782 - loglik: -4.2450e+02 - logprior: -8.6821e+00
Epoch 2/10
23/23 - 7s - loss: 417.2932 - loglik: -4.1640e+02 - logprior: -6.0538e-01
Epoch 3/10
23/23 - 7s - loss: 408.6516 - loglik: -4.0889e+02 - logprior: 0.8181
Epoch 4/10
23/23 - 7s - loss: 404.2509 - loglik: -4.0458e+02 - logprior: 1.1628
Epoch 5/10
23/23 - 8s - loss: 404.4841 - loglik: -4.0485e+02 - logprior: 1.3063
Fitted a model with MAP estimate = -401.7214
Time for alignment: 161.5445
Computed alignments with likelihoods: ['-399.6770', '-399.3368', '-401.7214']
Best model has likelihood: -399.3368
time for generating output: 0.3131
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.6975788031064413
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac55741730>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac445bcf40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac1121c1f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabde629f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabde629a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabaadafb50>, <__main__.SimpleDirichletPrior object at 0x7fac5de0e2b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.6221 - loglik: -2.4824e+02 - logprior: -1.1634e+00
Epoch 2/10
29/29 - 4s - loss: 223.0421 - loglik: -2.2134e+02 - logprior: -8.4458e-01
Epoch 3/10
29/29 - 4s - loss: 218.6575 - loglik: -2.1687e+02 - logprior: -8.2145e-01
Epoch 4/10
29/29 - 4s - loss: 217.3731 - loglik: -2.1571e+02 - logprior: -8.2611e-01
Epoch 5/10
29/29 - 4s - loss: 216.7503 - loglik: -2.1516e+02 - logprior: -8.1108e-01
Epoch 6/10
29/29 - 4s - loss: 216.2752 - loglik: -2.1473e+02 - logprior: -8.0633e-01
Epoch 7/10
29/29 - 4s - loss: 215.7356 - loglik: -2.1417e+02 - logprior: -8.0219e-01
Epoch 8/10
29/29 - 4s - loss: 215.2505 - loglik: -2.1361e+02 - logprior: -8.0291e-01
Epoch 9/10
29/29 - 4s - loss: 214.5450 - loglik: -2.1297e+02 - logprior: -8.0747e-01
Epoch 10/10
29/29 - 4s - loss: 213.6263 - loglik: -2.1197e+02 - logprior: -8.2292e-01
Fitted a model with MAP estimate = -204.9395
expansions: [(2, 1), (3, 1), (13, 3), (14, 1), (15, 2), (22, 1), (27, 2), (38, 2), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 234.6077 - loglik: -2.3326e+02 - logprior: -1.2266e+00
Epoch 2/2
29/29 - 4s - loss: 216.2189 - loglik: -2.1484e+02 - logprior: -8.8885e-01
Fitted a model with MAP estimate = -195.6533
expansions: [(2, 1), (16, 1), (20, 1), (63, 1)]
discards: [ 0  3 18 22 23 24 36 50 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 219.5071 - loglik: -2.1797e+02 - logprior: -1.4658e+00
Epoch 2/2
29/29 - 4s - loss: 214.8703 - loglik: -2.1362e+02 - logprior: -9.6513e-01
Fitted a model with MAP estimate = -195.8516
expansions: [(2, 1), (17, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 196.7944 - loglik: -1.9568e+02 - logprior: -7.2165e-01
Epoch 2/10
42/42 - 5s - loss: 194.6631 - loglik: -1.9321e+02 - logprior: -5.3500e-01
Epoch 3/10
42/42 - 6s - loss: 193.6029 - loglik: -1.9208e+02 - logprior: -5.1397e-01
Epoch 4/10
42/42 - 5s - loss: 193.3967 - loglik: -1.9179e+02 - logprior: -5.0402e-01
Epoch 5/10
42/42 - 6s - loss: 192.4271 - loglik: -1.9088e+02 - logprior: -4.9898e-01
Epoch 6/10
42/42 - 6s - loss: 191.7595 - loglik: -1.9016e+02 - logprior: -4.9368e-01
Epoch 7/10
42/42 - 6s - loss: 191.0858 - loglik: -1.8949e+02 - logprior: -4.8927e-01
Epoch 8/10
42/42 - 5s - loss: 190.2126 - loglik: -1.8840e+02 - logprior: -4.8714e-01
Epoch 9/10
42/42 - 6s - loss: 189.1191 - loglik: -1.8722e+02 - logprior: -4.8856e-01
Epoch 10/10
42/42 - 6s - loss: 188.7890 - loglik: -1.8714e+02 - logprior: -4.8228e-01
Fitted a model with MAP estimate = -186.9639
Time for alignment: 175.5895
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.1950 - loglik: -2.4785e+02 - logprior: -1.1607e+00
Epoch 2/10
29/29 - 4s - loss: 223.0575 - loglik: -2.2146e+02 - logprior: -8.2940e-01
Epoch 3/10
29/29 - 4s - loss: 217.6840 - loglik: -2.1594e+02 - logprior: -8.0373e-01
Epoch 4/10
29/29 - 4s - loss: 216.8548 - loglik: -2.1521e+02 - logprior: -8.1012e-01
Epoch 5/10
29/29 - 4s - loss: 216.5513 - loglik: -2.1499e+02 - logprior: -7.9505e-01
Epoch 6/10
29/29 - 4s - loss: 216.1660 - loglik: -2.1465e+02 - logprior: -7.8814e-01
Epoch 7/10
29/29 - 4s - loss: 215.1702 - loglik: -2.1363e+02 - logprior: -7.8895e-01
Epoch 8/10
29/29 - 4s - loss: 214.9122 - loglik: -2.1331e+02 - logprior: -7.8845e-01
Epoch 9/10
29/29 - 4s - loss: 213.7201 - loglik: -2.1217e+02 - logprior: -7.9432e-01
Epoch 10/10
29/29 - 4s - loss: 214.3716 - loglik: -2.1276e+02 - logprior: -8.0082e-01
Fitted a model with MAP estimate = -204.5323
expansions: [(2, 1), (12, 2), (13, 2), (14, 2), (16, 1), (21, 1), (38, 2), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 226.9111 - loglik: -2.2538e+02 - logprior: -1.1861e+00
Epoch 2/2
29/29 - 4s - loss: 214.6835 - loglik: -2.1319e+02 - logprior: -7.8584e-01
Fitted a model with MAP estimate = -195.1395
expansions: [(4, 1), (18, 1)]
discards: [15 21 48 58]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 217.7761 - loglik: -2.1661e+02 - logprior: -1.0938e+00
Epoch 2/2
29/29 - 4s - loss: 214.4686 - loglik: -2.1347e+02 - logprior: -7.0763e-01
Fitted a model with MAP estimate = -194.9882
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 197.0998 - loglik: -1.9637e+02 - logprior: -6.7478e-01
Epoch 2/10
42/42 - 5s - loss: 194.7599 - loglik: -1.9390e+02 - logprior: -4.9197e-01
Epoch 3/10
42/42 - 6s - loss: 193.1154 - loglik: -1.9195e+02 - logprior: -4.8556e-01
Epoch 4/10
42/42 - 5s - loss: 192.4328 - loglik: -1.9106e+02 - logprior: -4.7625e-01
Epoch 5/10
42/42 - 5s - loss: 191.8826 - loglik: -1.9042e+02 - logprior: -4.7234e-01
Epoch 6/10
42/42 - 5s - loss: 190.9405 - loglik: -1.8948e+02 - logprior: -4.6823e-01
Epoch 7/10
42/42 - 5s - loss: 190.4452 - loglik: -1.8893e+02 - logprior: -4.6527e-01
Epoch 8/10
42/42 - 6s - loss: 189.5585 - loglik: -1.8786e+02 - logprior: -4.6268e-01
Epoch 9/10
42/42 - 5s - loss: 188.9526 - loglik: -1.8708e+02 - logprior: -4.5788e-01
Epoch 10/10
42/42 - 6s - loss: 188.0504 - loglik: -1.8639e+02 - logprior: -4.5737e-01
Fitted a model with MAP estimate = -186.1628
Time for alignment: 174.7630
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 249.6844 - loglik: -2.4836e+02 - logprior: -1.1617e+00
Epoch 2/10
29/29 - 4s - loss: 222.7682 - loglik: -2.2123e+02 - logprior: -8.3288e-01
Epoch 3/10
29/29 - 4s - loss: 218.7005 - loglik: -2.1697e+02 - logprior: -8.2040e-01
Epoch 4/10
29/29 - 4s - loss: 217.3440 - loglik: -2.1568e+02 - logprior: -8.1841e-01
Epoch 5/10
29/29 - 4s - loss: 216.7840 - loglik: -2.1520e+02 - logprior: -8.0066e-01
Epoch 6/10
29/29 - 4s - loss: 215.9978 - loglik: -2.1443e+02 - logprior: -8.1000e-01
Epoch 7/10
29/29 - 4s - loss: 215.1119 - loglik: -2.1347e+02 - logprior: -8.1850e-01
Epoch 8/10
29/29 - 4s - loss: 214.1897 - loglik: -2.1250e+02 - logprior: -8.1555e-01
Epoch 9/10
29/29 - 4s - loss: 214.0097 - loglik: -2.1241e+02 - logprior: -8.1543e-01
Epoch 10/10
29/29 - 4s - loss: 213.7025 - loglik: -2.1209e+02 - logprior: -8.1681e-01
Fitted a model with MAP estimate = -204.3230
expansions: [(1, 1), (2, 1), (13, 1), (14, 5), (27, 2), (38, 1), (41, 2), (43, 2), (44, 2), (46, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 233.0146 - loglik: -2.3162e+02 - logprior: -1.1893e+00
Epoch 2/2
29/29 - 4s - loss: 216.5140 - loglik: -2.1492e+02 - logprior: -8.4164e-01
Fitted a model with MAP estimate = -195.3118
expansions: [(18, 2)]
discards: [15 16 22 23 24 36 53 56 59]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 216.6804 - loglik: -2.1528e+02 - logprior: -1.0964e+00
Epoch 2/2
29/29 - 4s - loss: 213.6377 - loglik: -2.1227e+02 - logprior: -7.1141e-01
Fitted a model with MAP estimate = -194.4243
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 196.6614 - loglik: -1.9584e+02 - logprior: -6.7471e-01
Epoch 2/10
42/42 - 5s - loss: 194.6514 - loglik: -1.9364e+02 - logprior: -4.9291e-01
Epoch 3/10
42/42 - 5s - loss: 193.0600 - loglik: -1.9181e+02 - logprior: -4.8459e-01
Epoch 4/10
42/42 - 5s - loss: 192.5051 - loglik: -1.9108e+02 - logprior: -4.7481e-01
Epoch 5/10
42/42 - 5s - loss: 191.9003 - loglik: -1.9040e+02 - logprior: -4.7114e-01
Epoch 6/10
42/42 - 5s - loss: 191.1514 - loglik: -1.8966e+02 - logprior: -4.6426e-01
Epoch 7/10
42/42 - 5s - loss: 190.7670 - loglik: -1.8925e+02 - logprior: -4.6215e-01
Epoch 8/10
42/42 - 5s - loss: 189.7343 - loglik: -1.8806e+02 - logprior: -4.5901e-01
Epoch 9/10
42/42 - 6s - loss: 189.1865 - loglik: -1.8734e+02 - logprior: -4.5637e-01
Epoch 10/10
42/42 - 6s - loss: 188.5836 - loglik: -1.8700e+02 - logprior: -4.5313e-01
Fitted a model with MAP estimate = -186.6938
Time for alignment: 174.6597
Computed alignments with likelihoods: ['-186.9639', '-186.1628', '-186.6938']
Best model has likelihood: -186.1628
time for generating output: 0.1912
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6169055857637172
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac006569a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1f31220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac11656640>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe698fa30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaaa4cfd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabde2e96a0>, <__main__.SimpleDirichletPrior object at 0x7fabb3b37d90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 375.7528 - loglik: -3.3040e+02 - logprior: -4.5327e+01
Epoch 2/10
10/10 - 1s - loss: 298.4385 - loglik: -2.8753e+02 - logprior: -1.0890e+01
Epoch 3/10
10/10 - 1s - loss: 249.5069 - loglik: -2.4459e+02 - logprior: -4.8820e+00
Epoch 4/10
10/10 - 1s - loss: 221.2386 - loglik: -2.1815e+02 - logprior: -2.9881e+00
Epoch 5/10
10/10 - 1s - loss: 208.2345 - loglik: -2.0578e+02 - logprior: -2.1699e+00
Epoch 6/10
10/10 - 1s - loss: 203.8612 - loglik: -2.0181e+02 - logprior: -1.6276e+00
Epoch 7/10
10/10 - 1s - loss: 201.7081 - loglik: -1.9996e+02 - logprior: -1.3154e+00
Epoch 8/10
10/10 - 1s - loss: 200.9781 - loglik: -1.9942e+02 - logprior: -1.1509e+00
Epoch 9/10
10/10 - 1s - loss: 199.9759 - loglik: -1.9854e+02 - logprior: -1.0416e+00
Epoch 10/10
10/10 - 1s - loss: 200.0909 - loglik: -1.9872e+02 - logprior: -9.5762e-01
Fitted a model with MAP estimate = -199.2745
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.6429 - loglik: -1.9917e+02 - logprior: -5.1370e+01
Epoch 2/2
10/10 - 2s - loss: 207.7094 - loglik: -1.8714e+02 - logprior: -2.0286e+01
Fitted a model with MAP estimate = -199.6871
expansions: [(0, 3)]
discards: [  0   9  13 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.6712 - loglik: -1.8793e+02 - logprior: -4.0550e+01
Epoch 2/2
10/10 - 2s - loss: 192.2995 - loglik: -1.8258e+02 - logprior: -9.5234e+00
Fitted a model with MAP estimate = -186.2446
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 234.3293 - loglik: -1.8485e+02 - logprior: -4.9438e+01
Epoch 2/10
10/10 - 2s - loss: 198.5667 - loglik: -1.8342e+02 - logprior: -1.5012e+01
Epoch 3/10
10/10 - 2s - loss: 185.5269 - loglik: -1.8082e+02 - logprior: -4.3981e+00
Epoch 4/10
10/10 - 2s - loss: 180.9389 - loglik: -1.7976e+02 - logprior: -7.4103e-01
Epoch 5/10
10/10 - 2s - loss: 178.4480 - loglik: -1.7865e+02 - logprior: 0.6701
Epoch 6/10
10/10 - 2s - loss: 177.3716 - loglik: -1.7838e+02 - logprior: 1.4547
Epoch 7/10
10/10 - 2s - loss: 176.8835 - loglik: -1.7856e+02 - logprior: 2.1102
Epoch 8/10
10/10 - 2s - loss: 176.3912 - loglik: -1.7858e+02 - logprior: 2.6238
Epoch 9/10
10/10 - 2s - loss: 175.4302 - loglik: -1.7795e+02 - logprior: 2.9692
Epoch 10/10
10/10 - 2s - loss: 175.7421 - loglik: -1.7852e+02 - logprior: 3.2348
Fitted a model with MAP estimate = -174.7194
Time for alignment: 54.2683
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.5331 - loglik: -3.3005e+02 - logprior: -4.5326e+01
Epoch 2/10
10/10 - 1s - loss: 298.7780 - loglik: -2.8780e+02 - logprior: -1.0885e+01
Epoch 3/10
10/10 - 1s - loss: 251.5625 - loglik: -2.4648e+02 - logprior: -5.0370e+00
Epoch 4/10
10/10 - 1s - loss: 222.7809 - loglik: -2.1946e+02 - logprior: -3.2563e+00
Epoch 5/10
10/10 - 1s - loss: 209.0806 - loglik: -2.0651e+02 - logprior: -2.3535e+00
Epoch 6/10
10/10 - 1s - loss: 205.0548 - loglik: -2.0284e+02 - logprior: -1.8002e+00
Epoch 7/10
10/10 - 1s - loss: 203.0241 - loglik: -2.0100e+02 - logprior: -1.5758e+00
Epoch 8/10
10/10 - 1s - loss: 201.8766 - loglik: -2.0001e+02 - logprior: -1.4666e+00
Epoch 9/10
10/10 - 1s - loss: 201.0604 - loglik: -1.9933e+02 - logprior: -1.3396e+00
Epoch 10/10
10/10 - 1s - loss: 200.7559 - loglik: -1.9913e+02 - logprior: -1.2159e+00
Fitted a model with MAP estimate = -200.1947
expansions: [(10, 4), (12, 1), (13, 2), (14, 1), (15, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.6386 - loglik: -1.9912e+02 - logprior: -5.1393e+01
Epoch 2/2
10/10 - 2s - loss: 207.4917 - loglik: -1.8705e+02 - logprior: -2.0276e+01
Fitted a model with MAP estimate = -199.0599
expansions: [(0, 3)]
discards: [  0  17 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.5380 - loglik: -1.8601e+02 - logprior: -4.0516e+01
Epoch 2/2
10/10 - 2s - loss: 192.0307 - loglik: -1.8243e+02 - logprior: -9.4928e+00
Fitted a model with MAP estimate = -185.9137
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 235.3687 - loglik: -1.8574e+02 - logprior: -4.9527e+01
Epoch 2/10
10/10 - 2s - loss: 199.0179 - loglik: -1.8370e+02 - logprior: -1.5218e+01
Epoch 3/10
10/10 - 2s - loss: 186.2243 - loglik: -1.8153e+02 - logprior: -4.4672e+00
Epoch 4/10
10/10 - 2s - loss: 181.0939 - loglik: -1.8002e+02 - logprior: -7.2479e-01
Epoch 5/10
10/10 - 2s - loss: 178.3752 - loglik: -1.7873e+02 - logprior: 0.6915
Epoch 6/10
10/10 - 2s - loss: 177.4600 - loglik: -1.7859e+02 - logprior: 1.4659
Epoch 7/10
10/10 - 2s - loss: 176.3476 - loglik: -1.7810e+02 - logprior: 2.1245
Epoch 8/10
10/10 - 2s - loss: 176.0224 - loglik: -1.7828e+02 - logprior: 2.6463
Epoch 9/10
10/10 - 2s - loss: 175.5335 - loglik: -1.7813e+02 - logprior: 2.9989
Epoch 10/10
10/10 - 2s - loss: 175.3364 - loglik: -1.7817e+02 - logprior: 3.2490
Fitted a model with MAP estimate = -174.4968
Time for alignment: 52.4483
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.5540 - loglik: -3.3016e+02 - logprior: -4.5327e+01
Epoch 2/10
10/10 - 1s - loss: 298.4325 - loglik: -2.8750e+02 - logprior: -1.0888e+01
Epoch 3/10
10/10 - 1s - loss: 250.5380 - loglik: -2.4549e+02 - logprior: -5.0240e+00
Epoch 4/10
10/10 - 1s - loss: 223.2955 - loglik: -2.2004e+02 - logprior: -3.1923e+00
Epoch 5/10
10/10 - 1s - loss: 210.5143 - loglik: -2.0798e+02 - logprior: -2.3225e+00
Epoch 6/10
10/10 - 1s - loss: 205.6437 - loglik: -2.0340e+02 - logprior: -1.8529e+00
Epoch 7/10
10/10 - 1s - loss: 204.0883 - loglik: -2.0207e+02 - logprior: -1.6058e+00
Epoch 8/10
10/10 - 1s - loss: 203.0728 - loglik: -2.0127e+02 - logprior: -1.4086e+00
Epoch 9/10
10/10 - 1s - loss: 202.1985 - loglik: -2.0047e+02 - logprior: -1.3172e+00
Epoch 10/10
10/10 - 1s - loss: 201.4286 - loglik: -1.9975e+02 - logprior: -1.2432e+00
Fitted a model with MAP estimate = -200.8351
expansions: [(10, 4), (12, 1), (13, 2), (14, 1), (16, 1), (29, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 1), (79, 1), (81, 2), (82, 2), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.7752 - loglik: -2.0027e+02 - logprior: -5.1452e+01
Epoch 2/2
10/10 - 2s - loss: 207.4955 - loglik: -1.8701e+02 - logprior: -2.0347e+01
Fitted a model with MAP estimate = -199.4760
expansions: [(0, 3)]
discards: [  0  17 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.8095 - loglik: -1.8609e+02 - logprior: -4.0600e+01
Epoch 2/2
10/10 - 2s - loss: 191.0123 - loglik: -1.8121e+02 - logprior: -9.5138e+00
Fitted a model with MAP estimate = -184.7970
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 233.9436 - loglik: -1.8449e+02 - logprior: -4.9311e+01
Epoch 2/10
10/10 - 2s - loss: 196.4398 - loglik: -1.8164e+02 - logprior: -1.4645e+01
Epoch 3/10
10/10 - 2s - loss: 184.7788 - loglik: -1.8031e+02 - logprior: -4.1999e+00
Epoch 4/10
10/10 - 2s - loss: 179.6626 - loglik: -1.7865e+02 - logprior: -6.3181e-01
Epoch 5/10
10/10 - 2s - loss: 177.3491 - loglik: -1.7768e+02 - logprior: 0.7743
Epoch 6/10
10/10 - 2s - loss: 176.3381 - loglik: -1.7744e+02 - logprior: 1.5569
Epoch 7/10
10/10 - 2s - loss: 176.0429 - loglik: -1.7777e+02 - logprior: 2.1874
Epoch 8/10
10/10 - 2s - loss: 175.7190 - loglik: -1.7796e+02 - logprior: 2.7075
Epoch 9/10
10/10 - 2s - loss: 175.1905 - loglik: -1.7780e+02 - logprior: 3.0733
Epoch 10/10
10/10 - 2s - loss: 174.7002 - loglik: -1.7757e+02 - logprior: 3.3353
Fitted a model with MAP estimate = -174.2773
Time for alignment: 52.4755
Computed alignments with likelihoods: ['-174.7194', '-174.4968', '-174.2773']
Best model has likelihood: -174.2773
time for generating output: 0.1678
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9315663455648978
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac44025310>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac444066a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac331914c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaafd8f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac22403340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabaade3550>, <__main__.SimpleDirichletPrior object at 0x7faba0de5a60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 630.8898 - loglik: -6.0816e+02 - logprior: -2.2546e+01
Epoch 2/10
14/14 - 4s - loss: 542.2145 - loglik: -5.3978e+02 - logprior: -2.0940e+00
Epoch 3/10
14/14 - 4s - loss: 485.6137 - loglik: -4.8393e+02 - logprior: -9.3705e-01
Epoch 4/10
14/14 - 4s - loss: 465.5311 - loglik: -4.6399e+02 - logprior: -6.8122e-01
Epoch 5/10
14/14 - 4s - loss: 457.4839 - loglik: -4.5639e+02 - logprior: -4.1943e-01
Epoch 6/10
14/14 - 4s - loss: 454.6897 - loglik: -4.5374e+02 - logprior: -2.6383e-01
Epoch 7/10
14/14 - 4s - loss: 454.6113 - loglik: -4.5370e+02 - logprior: -2.5633e-01
Epoch 8/10
14/14 - 4s - loss: 454.8098 - loglik: -4.5394e+02 - logprior: -2.0763e-01
Fitted a model with MAP estimate = -452.2613
expansions: [(14, 1), (16, 1), (29, 1), (30, 1), (31, 2), (33, 1), (41, 2), (42, 3), (43, 2), (51, 1), (52, 1), (53, 1), (61, 2), (80, 1), (90, 1), (113, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (136, 1), (164, 1), (166, 2), (167, 3), (168, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 487.6963 - loglik: -4.6066e+02 - logprior: -2.6949e+01
Epoch 2/2
14/14 - 5s - loss: 448.2364 - loglik: -4.4012e+02 - logprior: -7.8616e+00
Fitted a model with MAP estimate = -441.9794
expansions: [(71, 4), (142, 1)]
discards: [  0  54  89 149 151]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 467.6311 - loglik: -4.4174e+02 - logprior: -2.5731e+01
Epoch 2/2
14/14 - 5s - loss: 443.3167 - loglik: -4.3671e+02 - logprior: -6.2085e+00
Fitted a model with MAP estimate = -435.6622
expansions: [(0, 4), (69, 2)]
discards: [ 0 50 73 74 75 79]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 462.0063 - loglik: -4.4343e+02 - logprior: -1.8364e+01
Epoch 2/10
14/14 - 5s - loss: 436.2755 - loglik: -4.3641e+02 - logprior: 0.4163
Epoch 3/10
14/14 - 5s - loss: 431.2115 - loglik: -4.3414e+02 - logprior: 3.4389
Epoch 4/10
14/14 - 5s - loss: 428.5750 - loglik: -4.3248e+02 - logprior: 4.5587
Epoch 5/10
14/14 - 5s - loss: 426.6004 - loglik: -4.3096e+02 - logprior: 5.0568
Epoch 6/10
14/14 - 5s - loss: 426.1167 - loglik: -4.3083e+02 - logprior: 5.4013
Epoch 7/10
14/14 - 5s - loss: 425.1090 - loglik: -4.3013e+02 - logprior: 5.7079
Epoch 8/10
14/14 - 5s - loss: 425.0319 - loglik: -4.3039e+02 - logprior: 6.0274
Epoch 9/10
14/14 - 5s - loss: 423.9430 - loglik: -4.2960e+02 - logprior: 6.3070
Epoch 10/10
14/14 - 5s - loss: 422.5695 - loglik: -4.2850e+02 - logprior: 6.5877
Fitted a model with MAP estimate = -422.3439
Time for alignment: 136.3665
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 631.6852 - loglik: -6.0903e+02 - logprior: -2.2550e+01
Epoch 2/10
14/14 - 4s - loss: 538.3786 - loglik: -5.3603e+02 - logprior: -2.0741e+00
Epoch 3/10
14/14 - 4s - loss: 480.5807 - loglik: -4.7901e+02 - logprior: -8.7322e-01
Epoch 4/10
14/14 - 4s - loss: 462.5956 - loglik: -4.6101e+02 - logprior: -7.1722e-01
Epoch 5/10
14/14 - 4s - loss: 457.5215 - loglik: -4.5626e+02 - logprior: -5.0478e-01
Epoch 6/10
14/14 - 4s - loss: 454.7529 - loglik: -4.5376e+02 - logprior: -2.7249e-01
Epoch 7/10
14/14 - 4s - loss: 454.9955 - loglik: -4.5417e+02 - logprior: -1.4924e-01
Fitted a model with MAP estimate = -453.0704
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (29, 1), (30, 2), (31, 1), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 3), (61, 2), (81, 1), (90, 1), (113, 1), (114, 2), (115, 4), (118, 1), (119, 1), (120, 3), (154, 1), (164, 2), (166, 2), (167, 3), (168, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 487.2225 - loglik: -4.6014e+02 - logprior: -2.6990e+01
Epoch 2/2
14/14 - 5s - loss: 448.8750 - loglik: -4.4043e+02 - logprior: -8.0524e+00
Fitted a model with MAP estimate = -441.0752
expansions: [(36, 1), (69, 2), (138, 1), (142, 1)]
discards: [  0  17  50  90 150 203]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 466.8675 - loglik: -4.4082e+02 - logprior: -2.5771e+01
Epoch 2/2
14/14 - 5s - loss: 441.5395 - loglik: -4.3471e+02 - logprior: -6.1718e+00
Fitted a model with MAP estimate = -434.7099
expansions: [(0, 3)]
discards: [ 0 71 72 73 79]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 459.8157 - loglik: -4.4131e+02 - logprior: -1.8403e+01
Epoch 2/10
14/14 - 5s - loss: 436.3711 - loglik: -4.3653e+02 - logprior: 0.4049
Epoch 3/10
14/14 - 5s - loss: 430.8167 - loglik: -4.3394e+02 - logprior: 3.4781
Epoch 4/10
14/14 - 5s - loss: 427.3994 - loglik: -4.3154e+02 - logprior: 4.6346
Epoch 5/10
14/14 - 5s - loss: 426.6905 - loglik: -4.3126e+02 - logprior: 5.1613
Epoch 6/10
14/14 - 5s - loss: 424.4972 - loglik: -4.2934e+02 - logprior: 5.4969
Epoch 7/10
14/14 - 5s - loss: 424.0903 - loglik: -4.2920e+02 - logprior: 5.7991
Epoch 8/10
14/14 - 5s - loss: 423.7034 - loglik: -4.2912e+02 - logprior: 6.1055
Epoch 9/10
14/14 - 5s - loss: 423.6214 - loglik: -4.2931e+02 - logprior: 6.3878
Epoch 10/10
14/14 - 5s - loss: 422.3199 - loglik: -4.2827e+02 - logprior: 6.6590
Fitted a model with MAP estimate = -421.4202
Time for alignment: 129.3164
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 631.2435 - loglik: -6.0855e+02 - logprior: -2.2539e+01
Epoch 2/10
14/14 - 4s - loss: 538.0914 - loglik: -5.3569e+02 - logprior: -2.0576e+00
Epoch 3/10
14/14 - 4s - loss: 478.5017 - loglik: -4.7686e+02 - logprior: -8.7834e-01
Epoch 4/10
14/14 - 4s - loss: 461.5321 - loglik: -4.5993e+02 - logprior: -7.1552e-01
Epoch 5/10
14/14 - 4s - loss: 453.9908 - loglik: -4.5270e+02 - logprior: -5.5866e-01
Epoch 6/10
14/14 - 4s - loss: 452.1762 - loglik: -4.5112e+02 - logprior: -3.3920e-01
Epoch 7/10
14/14 - 4s - loss: 451.4831 - loglik: -4.5046e+02 - logprior: -3.3422e-01
Epoch 8/10
14/14 - 4s - loss: 449.4085 - loglik: -4.4840e+02 - logprior: -3.4420e-01
Epoch 9/10
14/14 - 4s - loss: 449.2246 - loglik: -4.4833e+02 - logprior: -2.4403e-01
Epoch 10/10
14/14 - 4s - loss: 449.7096 - loglik: -4.4890e+02 - logprior: -1.6344e-01
Fitted a model with MAP estimate = -448.2935
expansions: [(14, 4), (28, 1), (29, 1), (30, 2), (31, 1), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 3), (61, 2), (80, 1), (90, 1), (113, 1), (114, 1), (115, 1), (116, 4), (118, 1), (120, 1), (121, 3), (125, 1), (154, 1), (164, 1), (166, 2), (167, 3), (168, 5)]
discards: [ 0 72]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 486.9534 - loglik: -4.5997e+02 - logprior: -2.6815e+01
Epoch 2/2
14/14 - 5s - loss: 448.7468 - loglik: -4.4061e+02 - logprior: -7.6558e+00
Fitted a model with MAP estimate = -441.8973
expansions: [(0, 3), (36, 1), (69, 2), (142, 1)]
discards: [  0  15  50 150]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 458.4727 - loglik: -4.3955e+02 - logprior: -1.8842e+01
Epoch 2/2
14/14 - 5s - loss: 435.3289 - loglik: -4.3514e+02 - logprior: 0.1718
Fitted a model with MAP estimate = -429.8013
expansions: []
discards: [ 0  1 74 75 76 81]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 465.5705 - loglik: -4.4023e+02 - logprior: -2.5223e+01
Epoch 2/10
14/14 - 5s - loss: 442.4507 - loglik: -4.3662e+02 - logprior: -5.4919e+00
Epoch 3/10
14/14 - 5s - loss: 432.9643 - loglik: -4.3424e+02 - logprior: 1.8707
Epoch 4/10
14/14 - 5s - loss: 426.6406 - loglik: -4.3043e+02 - logprior: 4.5047
Epoch 5/10
14/14 - 5s - loss: 426.1366 - loglik: -4.3065e+02 - logprior: 5.2600
Epoch 6/10
14/14 - 5s - loss: 426.0428 - loglik: -4.3099e+02 - logprior: 5.6918
Epoch 7/10
14/14 - 5s - loss: 424.1345 - loglik: -4.2935e+02 - logprior: 5.9350
Epoch 8/10
14/14 - 5s - loss: 423.7176 - loglik: -4.2927e+02 - logprior: 6.2517
Epoch 9/10
14/14 - 5s - loss: 423.5514 - loglik: -4.2944e+02 - logprior: 6.5731
Epoch 10/10
14/14 - 5s - loss: 422.8448 - loglik: -4.2906e+02 - logprior: 6.8854
Fitted a model with MAP estimate = -421.6809
Time for alignment: 142.5768
Computed alignments with likelihoods: ['-422.3439', '-421.4202', '-421.6809']
Best model has likelihood: -421.4202
time for generating output: 0.2675
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8609671848013817
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac553dd490>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd08d460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabcd08d1c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaafd8f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac44561f70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac08f1f6a0>, <__main__.SimpleDirichletPrior object at 0x7fabaaecc820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 736.1621 - loglik: -6.9521e+02 - logprior: -4.0779e+01
Epoch 2/10
11/11 - 9s - loss: 632.5631 - loglik: -6.2921e+02 - logprior: -3.1404e+00
Epoch 3/10
11/11 - 10s - loss: 540.4951 - loglik: -5.4111e+02 - logprior: 0.8211
Epoch 4/10
11/11 - 9s - loss: 490.1572 - loglik: -4.9073e+02 - logprior: 0.9054
Epoch 5/10
11/11 - 9s - loss: 472.8093 - loglik: -4.7314e+02 - logprior: 0.7684
Epoch 6/10
11/11 - 10s - loss: 468.5436 - loglik: -4.6911e+02 - logprior: 1.0164
Epoch 7/10
11/11 - 8s - loss: 464.4301 - loglik: -4.6535e+02 - logprior: 1.3461
Epoch 8/10
11/11 - 10s - loss: 464.2813 - loglik: -4.6564e+02 - logprior: 1.7830
Epoch 9/10
11/11 - 9s - loss: 459.6977 - loglik: -4.6139e+02 - logprior: 2.1182
Epoch 10/10
11/11 - 9s - loss: 464.4175 - loglik: -4.6636e+02 - logprior: 2.3719
Fitted a model with MAP estimate = -460.7856
expansions: [(20, 1), (21, 2), (22, 2), (25, 1), (26, 1), (43, 1), (47, 1), (49, 1), (61, 1), (65, 1), (78, 2), (79, 1), (80, 1), (91, 1), (93, 1), (103, 5), (104, 1), (105, 1), (106, 1), (129, 1), (134, 1), (136, 1), (160, 2), (162, 5), (181, 1), (182, 1), (183, 1), (196, 1), (197, 1), (198, 4), (201, 2), (202, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 518.8216 - loglik: -4.7142e+02 - logprior: -4.7342e+01
Epoch 2/2
11/11 - 12s - loss: 458.7491 - loglik: -4.4599e+02 - logprior: -1.2706e+01
Fitted a model with MAP estimate = -447.8713
expansions: [(0, 3), (72, 1), (242, 3)]
discards: [  0  20  88 122 123 124 296]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 478.8638 - loglik: -4.4366e+02 - logprior: -3.5143e+01
Epoch 2/2
11/11 - 14s - loss: 435.7661 - loglik: -4.3573e+02 - logprior: 0.0280
Fitted a model with MAP estimate = -428.5128
expansions: [(157, 1), (253, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 482.7992 - loglik: -4.3805e+02 - logprior: -4.4592e+01
Epoch 2/10
11/11 - 12s - loss: 441.2665 - loglik: -4.3161e+02 - logprior: -9.3494e+00
Epoch 3/10
11/11 - 14s - loss: 426.1549 - loglik: -4.2851e+02 - logprior: 2.7669
Epoch 4/10
11/11 - 12s - loss: 417.1796 - loglik: -4.2671e+02 - logprior: 10.0580
Epoch 5/10
11/11 - 14s - loss: 413.2719 - loglik: -4.2498e+02 - logprior: 12.2553
Epoch 6/10
11/11 - 12s - loss: 412.0381 - loglik: -4.2490e+02 - logprior: 13.3439
Epoch 7/10
11/11 - 12s - loss: 409.6827 - loglik: -4.2345e+02 - logprior: 14.2313
Epoch 8/10
11/11 - 14s - loss: 410.4698 - loglik: -4.2505e+02 - logprior: 15.0343
Fitted a model with MAP estimate = -408.3810
Time for alignment: 283.5501
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 739.5179 - loglik: -6.9871e+02 - logprior: -4.0781e+01
Epoch 2/10
11/11 - 9s - loss: 628.3356 - loglik: -6.2510e+02 - logprior: -3.1254e+00
Epoch 3/10
11/11 - 9s - loss: 539.0167 - loglik: -5.3974e+02 - logprior: 0.9138
Epoch 4/10
11/11 - 9s - loss: 486.0642 - loglik: -4.8686e+02 - logprior: 1.1923
Epoch 5/10
11/11 - 9s - loss: 472.9632 - loglik: -4.7385e+02 - logprior: 1.3428
Epoch 6/10
11/11 - 10s - loss: 465.1221 - loglik: -4.6631e+02 - logprior: 1.6165
Epoch 7/10
11/11 - 10s - loss: 462.3770 - loglik: -4.6396e+02 - logprior: 1.9925
Epoch 8/10
11/11 - 10s - loss: 460.0388 - loglik: -4.6197e+02 - logprior: 2.3192
Epoch 9/10
11/11 - 10s - loss: 461.5606 - loglik: -4.6377e+02 - logprior: 2.5910
Fitted a model with MAP estimate = -458.9097
expansions: [(19, 2), (20, 3), (21, 2), (24, 1), (25, 1), (47, 1), (49, 1), (51, 2), (60, 1), (62, 1), (64, 1), (77, 1), (78, 1), (79, 1), (90, 1), (91, 1), (102, 5), (103, 1), (105, 1), (128, 1), (160, 2), (162, 5), (163, 1), (180, 1), (181, 1), (182, 2), (198, 4), (199, 2), (201, 1), (203, 6), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 514.1663 - loglik: -4.6657e+02 - logprior: -4.7549e+01
Epoch 2/2
11/11 - 14s - loss: 451.4226 - loglik: -4.3819e+02 - logprior: -1.3093e+01
Fitted a model with MAP estimate = -443.2831
expansions: [(0, 4), (246, 1)]
discards: [  0  20  21  61 123 124 262]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 474.0459 - loglik: -4.3891e+02 - logprior: -3.4964e+01
Epoch 2/2
11/11 - 13s - loss: 435.9644 - loglik: -4.3550e+02 - logprior: -2.8401e-01
Fitted a model with MAP estimate = -426.0929
expansions: [(252, 1), (253, 2)]
discards: [  1   2 295 296]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 17s - loss: 472.8490 - loglik: -4.3954e+02 - logprior: -3.3256e+01
Epoch 2/10
11/11 - 12s - loss: 430.1266 - loglik: -4.3085e+02 - logprior: 0.8286
Epoch 3/10
11/11 - 12s - loss: 423.3951 - loglik: -4.3069e+02 - logprior: 7.5955
Epoch 4/10
11/11 - 13s - loss: 416.7433 - loglik: -4.2676e+02 - logprior: 10.5061
Epoch 5/10
11/11 - 12s - loss: 416.2178 - loglik: -4.2772e+02 - logprior: 12.0144
Epoch 6/10
11/11 - 12s - loss: 410.9221 - loglik: -4.2341e+02 - logprior: 12.9628
Epoch 7/10
11/11 - 13s - loss: 412.2722 - loglik: -4.2558e+02 - logprior: 13.7616
Fitted a model with MAP estimate = -409.6440
Time for alignment: 260.6364
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 738.5786 - loglik: -6.9764e+02 - logprior: -4.0771e+01
Epoch 2/10
11/11 - 10s - loss: 629.6972 - loglik: -6.2634e+02 - logprior: -3.1157e+00
Epoch 3/10
11/11 - 9s - loss: 542.3092 - loglik: -5.4306e+02 - logprior: 0.9729
Epoch 4/10
11/11 - 10s - loss: 494.4697 - loglik: -4.9530e+02 - logprior: 1.1651
Epoch 5/10
11/11 - 10s - loss: 474.2120 - loglik: -4.7500e+02 - logprior: 1.2040
Epoch 6/10
11/11 - 9s - loss: 469.0429 - loglik: -4.7015e+02 - logprior: 1.5582
Epoch 7/10
11/11 - 10s - loss: 467.1172 - loglik: -4.6867e+02 - logprior: 1.9962
Epoch 8/10
11/11 - 10s - loss: 465.3781 - loglik: -4.6731e+02 - logprior: 2.3615
Epoch 9/10
11/11 - 9s - loss: 463.7578 - loglik: -4.6600e+02 - logprior: 2.6822
Epoch 10/10
11/11 - 9s - loss: 463.4559 - loglik: -4.6596e+02 - logprior: 2.9578
Fitted a model with MAP estimate = -462.6638
expansions: [(22, 4), (29, 1), (35, 1), (47, 1), (49, 1), (61, 1), (65, 1), (78, 2), (79, 1), (81, 1), (91, 1), (93, 1), (103, 3), (104, 1), (105, 2), (106, 1), (108, 2), (161, 2), (163, 5), (182, 1), (183, 2), (184, 1), (188, 1), (197, 1), (198, 4), (201, 2), (202, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 519.7430 - loglik: -4.7218e+02 - logprior: -4.7450e+01
Epoch 2/2
11/11 - 13s - loss: 463.5507 - loglik: -4.5045e+02 - logprior: -1.2896e+01
Fitted a model with MAP estimate = -451.1192
expansions: [(0, 3), (70, 1), (198, 2), (239, 3)]
discards: [  0  86 118 119 129 192]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 17s - loss: 481.8900 - loglik: -4.4709e+02 - logprior: -3.4740e+01
Epoch 2/2
11/11 - 12s - loss: 435.8733 - loglik: -4.3581e+02 - logprior: 0.1508
Fitted a model with MAP estimate = -430.2631
expansions: [(22, 3), (252, 2)]
discards: [  0 196]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 18s - loss: 487.3284 - loglik: -4.4298e+02 - logprior: -4.4337e+01
Epoch 2/10
11/11 - 12s - loss: 444.9345 - loglik: -4.3539e+02 - logprior: -9.4378e+00
Epoch 3/10
11/11 - 13s - loss: 429.6975 - loglik: -4.3181e+02 - logprior: 2.3850
Epoch 4/10
11/11 - 12s - loss: 418.7157 - loglik: -4.2828e+02 - logprior: 10.0079
Epoch 5/10
11/11 - 12s - loss: 416.8745 - loglik: -4.2876e+02 - logprior: 12.3863
Epoch 6/10
11/11 - 12s - loss: 415.8388 - loglik: -4.2881e+02 - logprior: 13.4575
Epoch 7/10
11/11 - 13s - loss: 414.2171 - loglik: -4.2810e+02 - logprior: 14.3464
Epoch 8/10
11/11 - 12s - loss: 411.5554 - loglik: -4.2622e+02 - logprior: 15.1300
Epoch 9/10
11/11 - 14s - loss: 410.9897 - loglik: -4.2634e+02 - logprior: 15.8340
Epoch 10/10
11/11 - 13s - loss: 412.6674 - loglik: -4.2860e+02 - logprior: 16.4321
Fitted a model with MAP estimate = -409.6674
Time for alignment: 310.2682
Computed alignments with likelihoods: ['-408.3810', '-409.6440', '-409.6674']
Best model has likelihood: -408.3810
time for generating output: 0.6135
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9290016214964095
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac5dec6850>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaafd8f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabcd08d460>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd08d5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2f0a8ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac446acf40>, <__main__.SimpleDirichletPrior object at 0x7faba1eb2d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 20s - loss: 817.4280 - loglik: -8.1128e+02 - logprior: -6.0194e+00
Epoch 2/10
21/21 - 17s - loss: 689.9132 - loglik: -6.8870e+02 - logprior: -8.1974e-01
Epoch 3/10
21/21 - 17s - loss: 641.0752 - loglik: -6.3729e+02 - logprior: -2.9509e+00
Epoch 4/10
21/21 - 17s - loss: 631.1807 - loglik: -6.2726e+02 - logprior: -2.9973e+00
Epoch 5/10
21/21 - 17s - loss: 628.3145 - loglik: -6.2469e+02 - logprior: -2.7583e+00
Epoch 6/10
21/21 - 17s - loss: 627.3231 - loglik: -6.2374e+02 - logprior: -2.7717e+00
Epoch 7/10
21/21 - 17s - loss: 627.0359 - loglik: -6.2358e+02 - logprior: -2.6682e+00
Epoch 8/10
21/21 - 17s - loss: 624.6442 - loglik: -6.2117e+02 - logprior: -2.7220e+00
Epoch 9/10
21/21 - 17s - loss: 626.0801 - loglik: -6.2265e+02 - logprior: -2.6861e+00
Fitted a model with MAP estimate = -624.0972
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 3), (54, 1), (62, 1), (63, 1), (64, 1), (65, 4), (76, 1), (77, 1), (80, 1), (82, 1), (83, 1), (84, 1), (86, 1), (87, 1), (89, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (103, 1), (104, 1), (111, 1), (113, 1), (115, 1), (132, 1), (136, 1), (138, 1), (139, 1), (142, 1), (145, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 3), (169, 1), (179, 1), (182, 1), (189, 1), (190, 1), (192, 1), (193, 1), (194, 2), (195, 1), (196, 1), (209, 1), (212, 1), (213, 1), (215, 1), (224, 1), (228, 1), (229, 1), (230, 2), (232, 1), (257, 1), (258, 3), (259, 1), (261, 2), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 30s - loss: 642.6503 - loglik: -6.3667e+02 - logprior: -5.9244e+00
Epoch 2/2
21/21 - 25s - loss: 601.6718 - loglik: -6.0230e+02 - logprior: 0.9634
Fitted a model with MAP estimate = -597.4210
expansions: []
discards: [251]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 611.1545 - loglik: -6.0643e+02 - logprior: -4.5627e+00
Epoch 2/2
21/21 - 24s - loss: 596.5046 - loglik: -5.9779e+02 - logprior: 1.7045
Fitted a model with MAP estimate = -594.6893
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 28s - loss: 606.7573 - loglik: -6.0219e+02 - logprior: -4.0652e+00
Epoch 2/10
21/21 - 25s - loss: 597.3973 - loglik: -5.9874e+02 - logprior: 2.2365
Epoch 3/10
21/21 - 25s - loss: 591.8724 - loglik: -5.9358e+02 - logprior: 2.8569
Epoch 4/10
21/21 - 25s - loss: 591.2031 - loglik: -5.9315e+02 - logprior: 3.1535
Epoch 5/10
21/21 - 25s - loss: 588.1429 - loglik: -5.9066e+02 - logprior: 3.6504
Epoch 6/10
21/21 - 25s - loss: 591.0705 - loglik: -5.9398e+02 - logprior: 3.9466
Fitted a model with MAP estimate = -587.2560
Time for alignment: 485.1127
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 22s - loss: 813.9451 - loglik: -8.0782e+02 - logprior: -6.0223e+00
Epoch 2/10
21/21 - 17s - loss: 692.4811 - loglik: -6.9125e+02 - logprior: -8.4592e-01
Epoch 3/10
21/21 - 17s - loss: 637.6981 - loglik: -6.3395e+02 - logprior: -2.9150e+00
Epoch 4/10
21/21 - 17s - loss: 631.4964 - loglik: -6.2785e+02 - logprior: -2.7637e+00
Epoch 5/10
21/21 - 17s - loss: 629.0436 - loglik: -6.2556e+02 - logprior: -2.6218e+00
Epoch 6/10
21/21 - 17s - loss: 628.8361 - loglik: -6.2538e+02 - logprior: -2.6534e+00
Epoch 7/10
21/21 - 17s - loss: 626.5127 - loglik: -6.2311e+02 - logprior: -2.6329e+00
Epoch 8/10
21/21 - 17s - loss: 625.3947 - loglik: -6.2207e+02 - logprior: -2.5971e+00
Epoch 9/10
21/21 - 17s - loss: 627.0247 - loglik: -6.2369e+02 - logprior: -2.6183e+00
Fitted a model with MAP estimate = -624.8102
expansions: [(13, 1), (14, 2), (15, 1), (53, 4), (55, 1), (56, 1), (61, 2), (62, 2), (63, 1), (65, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 1), (84, 1), (85, 1), (87, 1), (90, 1), (92, 1), (94, 1), (95, 1), (97, 1), (101, 1), (110, 1), (112, 1), (114, 1), (116, 1), (134, 1), (135, 1), (138, 1), (141, 1), (144, 1), (153, 1), (154, 1), (155, 1), (157, 3), (159, 1), (160, 1), (169, 1), (179, 1), (182, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (208, 1), (209, 1), (211, 1), (212, 1), (218, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (233, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 1), (261, 1), (272, 2), (273, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 637.4784 - loglik: -6.2881e+02 - logprior: -8.4436e+00
Epoch 2/2
21/21 - 25s - loss: 604.1693 - loglik: -6.0198e+02 - logprior: -1.5654e+00
Fitted a model with MAP estimate = -599.0407
expansions: [(0, 3), (75, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 615.3277 - loglik: -6.1064e+02 - logprior: -4.6540e+00
Epoch 2/2
21/21 - 25s - loss: 602.0391 - loglik: -6.0362e+02 - logprior: 1.7819
Fitted a model with MAP estimate = -596.0647
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 30s - loss: 610.8411 - loglik: -6.0655e+02 - logprior: -4.1859e+00
Epoch 2/10
21/21 - 25s - loss: 599.5787 - loglik: -6.0193e+02 - logprior: 2.5637
Epoch 3/10
21/21 - 24s - loss: 594.8029 - loglik: -5.9744e+02 - logprior: 3.1047
Epoch 4/10
21/21 - 25s - loss: 590.2964 - loglik: -5.9315e+02 - logprior: 3.5002
Epoch 5/10
21/21 - 25s - loss: 590.0445 - loglik: -5.9307e+02 - logprior: 3.7372
Epoch 6/10
21/21 - 25s - loss: 591.8633 - loglik: -5.9507e+02 - logprior: 3.9449
Fitted a model with MAP estimate = -588.0088
Time for alignment: 484.4612
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 813.6993 - loglik: -8.0753e+02 - logprior: -6.0058e+00
Epoch 2/10
21/21 - 17s - loss: 691.8919 - loglik: -6.9048e+02 - logprior: -8.7362e-01
Epoch 3/10
21/21 - 17s - loss: 642.0279 - loglik: -6.3820e+02 - logprior: -2.8703e+00
Epoch 4/10
21/21 - 17s - loss: 633.6921 - loglik: -6.2995e+02 - logprior: -2.7755e+00
Epoch 5/10
21/21 - 17s - loss: 627.6718 - loglik: -6.2424e+02 - logprior: -2.5591e+00
Epoch 6/10
21/21 - 17s - loss: 629.0752 - loglik: -6.2576e+02 - logprior: -2.5213e+00
Fitted a model with MAP estimate = -627.8056
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 2), (53, 2), (54, 5), (56, 1), (58, 2), (59, 1), (60, 1), (62, 2), (71, 1), (73, 1), (74, 1), (75, 1), (78, 3), (79, 1), (81, 1), (83, 1), (84, 1), (87, 1), (90, 1), (91, 1), (92, 1), (99, 1), (108, 1), (113, 1), (115, 1), (134, 1), (136, 1), (137, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 3), (159, 1), (160, 1), (179, 1), (180, 1), (183, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (195, 1), (207, 1), (208, 1), (212, 1), (218, 1), (223, 1), (228, 1), (230, 2), (232, 1), (239, 1), (256, 1), (257, 1), (258, 1), (259, 1), (261, 2), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 647.3216 - loglik: -6.3842e+02 - logprior: -8.6631e+00
Epoch 2/2
21/21 - 25s - loss: 603.8428 - loglik: -6.0158e+02 - logprior: -1.8195e+00
Fitted a model with MAP estimate = -599.9641
expansions: [(0, 3), (78, 1), (103, 1)]
discards: [ 0 58 60 74]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 614.4906 - loglik: -6.0971e+02 - logprior: -4.7513e+00
Epoch 2/2
21/21 - 24s - loss: 603.5675 - loglik: -6.0519e+02 - logprior: 1.7913
Fitted a model with MAP estimate = -595.8765
expansions: []
discards: [  1 337]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 607.3276 - loglik: -6.0296e+02 - logprior: -4.1157e+00
Epoch 2/10
21/21 - 25s - loss: 596.7500 - loglik: -5.9837e+02 - logprior: 2.2717
Epoch 3/10
21/21 - 25s - loss: 594.4822 - loglik: -5.9645e+02 - logprior: 2.9060
Epoch 4/10
21/21 - 25s - loss: 590.7174 - loglik: -5.9297e+02 - logprior: 3.2688
Epoch 5/10
21/21 - 25s - loss: 592.1400 - loglik: -5.9469e+02 - logprior: 3.5295
Fitted a model with MAP estimate = -588.7364
Time for alignment: 406.7507
Computed alignments with likelihoods: ['-587.2560', '-588.0088', '-588.7364']
Best model has likelihood: -587.2560
time for generating output: 0.3870
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9568231332937215
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac55018580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2ab087c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabaaa69ac0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6c5a4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5dcbaaf0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac4cfd7fd0>, <__main__.SimpleDirichletPrior object at 0x7fac6602a3a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 839.9418 - loglik: -8.3768e+02 - logprior: -1.7110e+00
Epoch 2/10
39/39 - 23s - loss: 734.9721 - loglik: -7.3189e+02 - logprior: -1.5386e+00
Epoch 3/10
39/39 - 23s - loss: 721.1324 - loglik: -7.1749e+02 - logprior: -1.6524e+00
Epoch 4/10
39/39 - 23s - loss: 716.6045 - loglik: -7.1288e+02 - logprior: -1.6855e+00
Epoch 5/10
39/39 - 23s - loss: 713.6312 - loglik: -7.1000e+02 - logprior: -1.6799e+00
Epoch 6/10
39/39 - 24s - loss: 711.6411 - loglik: -7.0811e+02 - logprior: -1.6892e+00
Epoch 7/10
39/39 - 23s - loss: 710.8367 - loglik: -7.0749e+02 - logprior: -1.6911e+00
Epoch 8/10
39/39 - 24s - loss: 710.2921 - loglik: -7.0708e+02 - logprior: -1.6879e+00
Epoch 9/10
39/39 - 24s - loss: 709.0804 - loglik: -7.0595e+02 - logprior: -1.7058e+00
Epoch 10/10
39/39 - 23s - loss: 708.4171 - loglik: -7.0541e+02 - logprior: -1.7064e+00
Fitted a model with MAP estimate = -612.3012
expansions: [(14, 1), (30, 1), (40, 1), (44, 1), (55, 1), (75, 1), (79, 2), (80, 3), (81, 1), (83, 1), (88, 1), (91, 2), (93, 2), (101, 1), (109, 3), (116, 1), (118, 1), (120, 3), (121, 1), (160, 6), (168, 6), (169, 2), (172, 3), (174, 1), (175, 1), (176, 1), (180, 8), (181, 1), (187, 1), (188, 1), (189, 1), (191, 3), (208, 2), (209, 1), (213, 4), (215, 1), (239, 1), (244, 3)]
discards: [  0 102 103 104 105 146]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 736.8299 - loglik: -7.3349e+02 - logprior: -3.0545e+00
Epoch 2/2
39/39 - 33s - loss: 699.1812 - loglik: -6.9607e+02 - logprior: -2.1422e+00
Fitted a model with MAP estimate = -596.9487
expansions: [(0, 2), (120, 7), (124, 1), (163, 1), (186, 2), (226, 3), (247, 1), (313, 2)]
discards: [  0  86 121 122 143 169 170 171 211 227 232 273 274 275 310 311 312]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 704.7899 - loglik: -7.0252e+02 - logprior: -2.0073e+00
Epoch 2/2
39/39 - 33s - loss: 690.6541 - loglik: -6.8915e+02 - logprior: -8.7986e-01
Fitted a model with MAP estimate = -592.1405
expansions: [(251, 1)]
discards: [  0 120 121 122 123 124 125 126 169 234 235 252 313 314]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 38s - loss: 605.9351 - loglik: -6.0346e+02 - logprior: -2.1183e+00
Epoch 2/10
43/43 - 34s - loss: 598.7806 - loglik: -5.9736e+02 - logprior: -6.3740e-01
Epoch 3/10
43/43 - 34s - loss: 589.1755 - loglik: -5.8741e+02 - logprior: -4.8148e-01
Epoch 4/10
43/43 - 34s - loss: 591.2119 - loglik: -5.8927e+02 - logprior: -3.9022e-01
Fitted a model with MAP estimate = -586.8778
Time for alignment: 677.7475
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 838.5768 - loglik: -8.3665e+02 - logprior: -1.7273e+00
Epoch 2/10
39/39 - 23s - loss: 736.6239 - loglik: -7.3399e+02 - logprior: -1.4855e+00
Epoch 3/10
39/39 - 23s - loss: 720.8351 - loglik: -7.1745e+02 - logprior: -1.7099e+00
Epoch 4/10
39/39 - 24s - loss: 717.0529 - loglik: -7.1348e+02 - logprior: -1.7520e+00
Epoch 5/10
39/39 - 23s - loss: 714.4364 - loglik: -7.1085e+02 - logprior: -1.7677e+00
Epoch 6/10
39/39 - 23s - loss: 712.1569 - loglik: -7.0862e+02 - logprior: -1.7960e+00
Epoch 7/10
39/39 - 23s - loss: 711.2985 - loglik: -7.0787e+02 - logprior: -1.8125e+00
Epoch 8/10
39/39 - 23s - loss: 709.6387 - loglik: -7.0626e+02 - logprior: -1.8176e+00
Epoch 9/10
39/39 - 23s - loss: 708.5827 - loglik: -7.0527e+02 - logprior: -1.8149e+00
Epoch 10/10
39/39 - 24s - loss: 707.6891 - loglik: -7.0447e+02 - logprior: -1.8048e+00
Fitted a model with MAP estimate = -612.4896
expansions: [(14, 1), (27, 1), (40, 1), (44, 1), (76, 1), (80, 3), (81, 2), (82, 1), (93, 2), (94, 4), (105, 2), (106, 1), (116, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (141, 1), (154, 1), (160, 4), (169, 6), (170, 3), (171, 1), (172, 3), (174, 1), (175, 1), (176, 1), (180, 1), (181, 4), (186, 1), (188, 1), (189, 1), (190, 4), (191, 1), (192, 1), (195, 1), (206, 1), (210, 1), (225, 1), (244, 3)]
discards: [  0 103 148 149 150 151]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 737.8465 - loglik: -7.3442e+02 - logprior: -3.1030e+00
Epoch 2/2
39/39 - 32s - loss: 701.1630 - loglik: -6.9820e+02 - logprior: -1.9850e+00
Fitted a model with MAP estimate = -599.5904
expansions: [(0, 2), (89, 1), (115, 3), (119, 1), (120, 1), (121, 1), (186, 3), (222, 3), (242, 1), (248, 1), (308, 3)]
discards: [  0  86 108 110 111 112 122 123 124 125 126 127 167 249 250 305 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 709.8773 - loglik: -7.0777e+02 - logprior: -1.9616e+00
Epoch 2/2
39/39 - 32s - loss: 692.5143 - loglik: -6.9078e+02 - logprior: -8.4527e-01
Fitted a model with MAP estimate = -592.7138
expansions: [(122, 1), (123, 1), (126, 3), (199, 1), (226, 1), (247, 1), (250, 1), (273, 3)]
discards: [  0 120 201 232 233 252 307 308 309]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 39s - loss: 606.4799 - loglik: -6.0411e+02 - logprior: -2.0829e+00
Epoch 2/10
43/43 - 35s - loss: 589.8607 - loglik: -5.8787e+02 - logprior: -6.4160e-01
Epoch 3/10
43/43 - 36s - loss: 587.2330 - loglik: -5.8504e+02 - logprior: -4.5679e-01
Epoch 4/10
43/43 - 35s - loss: 588.3076 - loglik: -5.8635e+02 - logprior: -3.5687e-01
Fitted a model with MAP estimate = -582.9979
Time for alignment: 680.2418
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 835.0233 - loglik: -8.3296e+02 - logprior: -1.6964e+00
Epoch 2/10
39/39 - 23s - loss: 734.6631 - loglik: -7.3196e+02 - logprior: -1.3565e+00
Epoch 3/10
39/39 - 23s - loss: 722.3528 - loglik: -7.1904e+02 - logprior: -1.4696e+00
Epoch 4/10
39/39 - 24s - loss: 717.7137 - loglik: -7.1426e+02 - logprior: -1.5028e+00
Epoch 5/10
39/39 - 23s - loss: 715.3736 - loglik: -7.1198e+02 - logprior: -1.5048e+00
Epoch 6/10
39/39 - 24s - loss: 713.2715 - loglik: -7.1001e+02 - logprior: -1.5154e+00
Epoch 7/10
39/39 - 23s - loss: 712.1302 - loglik: -7.0899e+02 - logprior: -1.5287e+00
Epoch 8/10
39/39 - 23s - loss: 710.4863 - loglik: -7.0743e+02 - logprior: -1.5474e+00
Epoch 9/10
39/39 - 23s - loss: 709.6627 - loglik: -7.0666e+02 - logprior: -1.5588e+00
Epoch 10/10
39/39 - 23s - loss: 707.9361 - loglik: -7.0490e+02 - logprior: -1.5646e+00
Fitted a model with MAP estimate = -610.6898
expansions: [(14, 1), (78, 1), (79, 1), (81, 3), (82, 1), (84, 1), (86, 1), (96, 3), (111, 1), (117, 1), (119, 1), (120, 1), (121, 1), (122, 4), (123, 1), (144, 17), (145, 5), (158, 1), (173, 1), (174, 1), (175, 1), (179, 1), (181, 2), (182, 2), (184, 1), (186, 1), (187, 1), (188, 1), (190, 3), (191, 2), (195, 1), (206, 1), (207, 1), (211, 4), (244, 3)]
discards: [  0 105 106 107 108 167]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 737.9197 - loglik: -7.3446e+02 - logprior: -3.0254e+00
Epoch 2/2
39/39 - 32s - loss: 702.3740 - loglik: -6.9934e+02 - logprior: -1.8371e+00
Fitted a model with MAP estimate = -599.2047
expansions: [(0, 2), (116, 7), (183, 4), (248, 1), (310, 3)]
discards: [  0 137 167 168 169 170 171 172 173 174 175 206 207 239 240 307 308 309]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 706.8592 - loglik: -7.0442e+02 - logprior: -1.9907e+00
Epoch 2/2
39/39 - 32s - loss: 692.4405 - loglik: -6.9008e+02 - logprior: -8.3649e-01
Fitted a model with MAP estimate = -591.9318
expansions: [(105, 1), (225, 1), (241, 1)]
discards: [  0 117 118 119 120 121 122 123 124 125 183 243 306 307 308]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 37s - loss: 614.3887 - loglik: -6.1222e+02 - logprior: -2.0680e+00
Epoch 2/10
43/43 - 33s - loss: 600.8585 - loglik: -5.9959e+02 - logprior: -6.0501e-01
Epoch 3/10
43/43 - 33s - loss: 593.1060 - loglik: -5.9132e+02 - logprior: -5.2418e-01
Epoch 4/10
43/43 - 34s - loss: 591.3588 - loglik: -5.8941e+02 - logprior: -4.2370e-01
Epoch 5/10
43/43 - 33s - loss: 592.4774 - loglik: -5.9071e+02 - logprior: -2.0468e-01
Fitted a model with MAP estimate = -587.7555
Time for alignment: 702.6083
Computed alignments with likelihoods: ['-586.8778', '-582.9979', '-587.7555']
Best model has likelihood: -582.9979
time for generating output: 0.5216
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5399749305139244
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac2ae6f760>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd0ca730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac4ca14ac0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2230cbe0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba0c5fee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac55018580>, <__main__.SimpleDirichletPrior object at 0x7fac5d9e4460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.4736 - loglik: -2.3622e+02 - logprior: -3.8177e+01
Epoch 2/10
10/10 - 2s - loss: 223.7449 - loglik: -2.1360e+02 - logprior: -1.0094e+01
Epoch 3/10
10/10 - 2s - loss: 196.8032 - loglik: -1.9163e+02 - logprior: -5.1150e+00
Epoch 4/10
10/10 - 2s - loss: 181.0794 - loglik: -1.7732e+02 - logprior: -3.5520e+00
Epoch 5/10
10/10 - 2s - loss: 173.9633 - loglik: -1.7087e+02 - logprior: -2.7295e+00
Epoch 6/10
10/10 - 2s - loss: 171.3759 - loglik: -1.6874e+02 - logprior: -2.2746e+00
Epoch 7/10
10/10 - 2s - loss: 169.8655 - loglik: -1.6748e+02 - logprior: -2.0832e+00
Epoch 8/10
10/10 - 2s - loss: 167.9802 - loglik: -1.6564e+02 - logprior: -2.0566e+00
Epoch 9/10
10/10 - 2s - loss: 167.2258 - loglik: -1.6489e+02 - logprior: -2.0346e+00
Epoch 10/10
10/10 - 2s - loss: 166.7938 - loglik: -1.6451e+02 - logprior: -1.9701e+00
Fitted a model with MAP estimate = -166.1118
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.4958 - loglik: -1.6647e+02 - logprior: -4.3017e+01
Epoch 2/2
10/10 - 2s - loss: 174.3479 - loglik: -1.5669e+02 - logprior: -1.7594e+01
Fitted a model with MAP estimate = -167.8559
expansions: [(0, 2)]
discards: [ 0 18 67]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 186.8457 - loglik: -1.5275e+02 - logprior: -3.3943e+01
Epoch 2/2
10/10 - 2s - loss: 159.5751 - loglik: -1.5077e+02 - logprior: -8.6699e+00
Fitted a model with MAP estimate = -155.3547
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.9137 - loglik: -1.5334e+02 - logprior: -3.9455e+01
Epoch 2/10
10/10 - 2s - loss: 163.3272 - loglik: -1.5239e+02 - logprior: -1.0741e+01
Epoch 3/10
10/10 - 2s - loss: 155.6768 - loglik: -1.5131e+02 - logprior: -4.1063e+00
Epoch 4/10
10/10 - 2s - loss: 152.2431 - loglik: -1.5004e+02 - logprior: -1.9416e+00
Epoch 5/10
10/10 - 2s - loss: 151.5052 - loglik: -1.5025e+02 - logprior: -1.0049e+00
Epoch 6/10
10/10 - 2s - loss: 150.5890 - loglik: -1.4995e+02 - logprior: -3.9819e-01
Epoch 7/10
10/10 - 2s - loss: 150.1894 - loglik: -1.5008e+02 - logprior: 0.1280
Epoch 8/10
10/10 - 2s - loss: 149.7133 - loglik: -1.4996e+02 - logprior: 0.4870
Epoch 9/10
10/10 - 2s - loss: 149.7340 - loglik: -1.5016e+02 - logprior: 0.6737
Fitted a model with MAP estimate = -149.2752
Time for alignment: 54.8565
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 274.3735 - loglik: -2.3607e+02 - logprior: -3.8173e+01
Epoch 2/10
10/10 - 2s - loss: 223.4608 - loglik: -2.1328e+02 - logprior: -1.0098e+01
Epoch 3/10
10/10 - 2s - loss: 196.1336 - loglik: -1.9092e+02 - logprior: -5.0957e+00
Epoch 4/10
10/10 - 2s - loss: 178.5822 - loglik: -1.7477e+02 - logprior: -3.5210e+00
Epoch 5/10
10/10 - 2s - loss: 172.0475 - loglik: -1.6894e+02 - logprior: -2.7509e+00
Epoch 6/10
10/10 - 2s - loss: 169.5717 - loglik: -1.6687e+02 - logprior: -2.3552e+00
Epoch 7/10
10/10 - 2s - loss: 168.1842 - loglik: -1.6571e+02 - logprior: -2.1415e+00
Epoch 8/10
10/10 - 2s - loss: 167.5310 - loglik: -1.6523e+02 - logprior: -2.0003e+00
Epoch 9/10
10/10 - 2s - loss: 167.1582 - loglik: -1.6495e+02 - logprior: -1.9095e+00
Epoch 10/10
10/10 - 2s - loss: 166.3449 - loglik: -1.6419e+02 - logprior: -1.8532e+00
Fitted a model with MAP estimate = -166.0462
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 2), (23, 3), (30, 1), (32, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.3742 - loglik: -1.6635e+02 - logprior: -4.2969e+01
Epoch 2/2
10/10 - 2s - loss: 173.9999 - loglik: -1.5627e+02 - logprior: -1.7575e+01
Fitted a model with MAP estimate = -167.6387
expansions: [(0, 2)]
discards: [ 0 18 26 69]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.3939 - loglik: -1.5348e+02 - logprior: -3.3889e+01
Epoch 2/2
10/10 - 2s - loss: 158.9530 - loglik: -1.5031e+02 - logprior: -8.5755e+00
Fitted a model with MAP estimate = -154.8434
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.7876 - loglik: -1.5328e+02 - logprior: -3.9403e+01
Epoch 2/10
10/10 - 2s - loss: 162.6149 - loglik: -1.5183e+02 - logprior: -1.0658e+01
Epoch 3/10
10/10 - 2s - loss: 155.0028 - loglik: -1.5079e+02 - logprior: -3.9890e+00
Epoch 4/10
10/10 - 2s - loss: 152.0661 - loglik: -1.4997e+02 - logprior: -1.8255e+00
Epoch 5/10
10/10 - 2s - loss: 150.5400 - loglik: -1.4941e+02 - logprior: -8.8672e-01
Epoch 6/10
10/10 - 2s - loss: 149.7709 - loglik: -1.4926e+02 - logprior: -2.8099e-01
Epoch 7/10
10/10 - 2s - loss: 149.3994 - loglik: -1.4943e+02 - logprior: 0.2572
Epoch 8/10
10/10 - 2s - loss: 149.0446 - loglik: -1.4943e+02 - logprior: 0.6133
Epoch 9/10
10/10 - 2s - loss: 148.9787 - loglik: -1.4955e+02 - logprior: 0.8047
Epoch 10/10
10/10 - 2s - loss: 148.7200 - loglik: -1.4942e+02 - logprior: 0.9370
Fitted a model with MAP estimate = -148.3786
Time for alignment: 56.2531
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.4438 - loglik: -2.3612e+02 - logprior: -3.8173e+01
Epoch 2/10
10/10 - 2s - loss: 223.7366 - loglik: -2.1357e+02 - logprior: -1.0092e+01
Epoch 3/10
10/10 - 2s - loss: 196.4454 - loglik: -1.9129e+02 - logprior: -5.0932e+00
Epoch 4/10
10/10 - 2s - loss: 179.6885 - loglik: -1.7595e+02 - logprior: -3.5051e+00
Epoch 5/10
10/10 - 1s - loss: 172.6090 - loglik: -1.6950e+02 - logprior: -2.7363e+00
Epoch 6/10
10/10 - 2s - loss: 169.5500 - loglik: -1.6680e+02 - logprior: -2.3937e+00
Epoch 7/10
10/10 - 1s - loss: 167.9398 - loglik: -1.6538e+02 - logprior: -2.2303e+00
Epoch 8/10
10/10 - 2s - loss: 167.4753 - loglik: -1.6502e+02 - logprior: -2.1119e+00
Epoch 9/10
10/10 - 2s - loss: 166.4344 - loglik: -1.6411e+02 - logprior: -1.9898e+00
Epoch 10/10
10/10 - 2s - loss: 166.1400 - loglik: -1.6387e+02 - logprior: -1.9260e+00
Fitted a model with MAP estimate = -165.6879
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (32, 1), (46, 1), (55, 3), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 208.6143 - loglik: -1.6547e+02 - logprior: -4.3015e+01
Epoch 2/2
10/10 - 2s - loss: 174.0014 - loglik: -1.5616e+02 - logprior: -1.7574e+01
Fitted a model with MAP estimate = -167.2986
expansions: [(0, 2)]
discards: [ 0 18 67]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.0942 - loglik: -1.5310e+02 - logprior: -3.3947e+01
Epoch 2/2
10/10 - 2s - loss: 159.7651 - loglik: -1.5103e+02 - logprior: -8.6573e+00
Fitted a model with MAP estimate = -155.5129
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 193.4851 - loglik: -1.5404e+02 - logprior: -3.9387e+01
Epoch 2/10
10/10 - 2s - loss: 163.5778 - loglik: -1.5292e+02 - logprior: -1.0641e+01
Epoch 3/10
10/10 - 2s - loss: 156.3145 - loglik: -1.5230e+02 - logprior: -3.9965e+00
Epoch 4/10
10/10 - 2s - loss: 153.2703 - loglik: -1.5129e+02 - logprior: -1.8501e+00
Epoch 5/10
10/10 - 2s - loss: 151.4790 - loglik: -1.5028e+02 - logprior: -9.4779e-01
Epoch 6/10
10/10 - 2s - loss: 150.7094 - loglik: -1.5009e+02 - logprior: -3.6610e-01
Epoch 7/10
10/10 - 2s - loss: 150.4339 - loglik: -1.5036e+02 - logprior: 0.1524
Epoch 8/10
10/10 - 2s - loss: 149.8487 - loglik: -1.5012e+02 - logprior: 0.5093
Epoch 9/10
10/10 - 2s - loss: 149.8280 - loglik: -1.5029e+02 - logprior: 0.7030
Epoch 10/10
10/10 - 2s - loss: 149.5432 - loglik: -1.5013e+02 - logprior: 0.8267
Fitted a model with MAP estimate = -149.1954
Time for alignment: 56.3347
Computed alignments with likelihoods: ['-149.2752', '-148.3786', '-149.1954']
Best model has likelihood: -148.3786
time for generating output: 0.1828
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9242723933314496
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac447de3a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac19d9e4c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac22229d60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac220ee430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabef046040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabf7830fd0>, <__main__.SimpleDirichletPrior object at 0x7faba0864f40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 302.8401 - loglik: -2.9183e+02 - logprior: -1.0996e+01
Epoch 2/10
19/19 - 7s - loss: 240.2781 - loglik: -2.3789e+02 - logprior: -2.2449e+00
Epoch 3/10
19/19 - 6s - loss: 219.3205 - loglik: -2.1693e+02 - logprior: -2.0256e+00
Epoch 4/10
19/19 - 7s - loss: 217.4326 - loglik: -2.1542e+02 - logprior: -1.7280e+00
Epoch 5/10
19/19 - 6s - loss: 215.7774 - loglik: -2.1393e+02 - logprior: -1.5801e+00
Epoch 6/10
19/19 - 6s - loss: 213.1140 - loglik: -2.1126e+02 - logprior: -1.5947e+00
Epoch 7/10
19/19 - 8s - loss: 215.3184 - loglik: -2.1346e+02 - logprior: -1.5803e+00
Fitted a model with MAP estimate = -213.9963
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (27, 1), (29, 2), (43, 1), (48, 6), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 224.1629 - loglik: -2.1033e+02 - logprior: -1.3667e+01
Epoch 2/2
19/19 - 7s - loss: 206.4840 - loglik: -2.0186e+02 - logprior: -4.4615e+00
Fitted a model with MAP estimate = -201.0559
expansions: [(0, 2)]
discards: [ 0 12 14 16 37]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 209.4678 - loglik: -1.9992e+02 - logprior: -9.4322e+00
Epoch 2/2
19/19 - 7s - loss: 199.7789 - loglik: -1.9830e+02 - logprior: -1.2553e+00
Fitted a model with MAP estimate = -197.2790
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 212.6072 - loglik: -2.0115e+02 - logprior: -1.1382e+01
Epoch 2/10
19/19 - 7s - loss: 201.1756 - loglik: -1.9964e+02 - logprior: -1.4422e+00
Epoch 3/10
19/19 - 6s - loss: 196.7433 - loglik: -1.9625e+02 - logprior: -2.7908e-01
Epoch 4/10
19/19 - 7s - loss: 197.9741 - loglik: -1.9785e+02 - logprior: 0.1288
Fitted a model with MAP estimate = -196.8250
Time for alignment: 137.6953
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 303.8887 - loglik: -2.9288e+02 - logprior: -1.0995e+01
Epoch 2/10
19/19 - 6s - loss: 239.5459 - loglik: -2.3718e+02 - logprior: -2.2189e+00
Epoch 3/10
19/19 - 6s - loss: 221.1767 - loglik: -2.1876e+02 - logprior: -2.0532e+00
Epoch 4/10
19/19 - 7s - loss: 216.7390 - loglik: -2.1459e+02 - logprior: -1.8614e+00
Epoch 5/10
19/19 - 6s - loss: 214.9485 - loglik: -2.1303e+02 - logprior: -1.6679e+00
Epoch 6/10
19/19 - 8s - loss: 217.1237 - loglik: -2.1521e+02 - logprior: -1.6508e+00
Fitted a model with MAP estimate = -214.7372
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (27, 1), (29, 1), (43, 3), (47, 1), (48, 3), (50, 3), (57, 1), (58, 1), (71, 1), (72, 3), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 224.2860 - loglik: -2.1040e+02 - logprior: -1.3709e+01
Epoch 2/2
19/19 - 7s - loss: 206.3036 - loglik: -2.0165e+02 - logprior: -4.4359e+00
Fitted a model with MAP estimate = -200.9964
expansions: [(0, 2)]
discards: [ 0 12 14 16 53 62]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 209.4832 - loglik: -1.9999e+02 - logprior: -9.4335e+00
Epoch 2/2
19/19 - 6s - loss: 199.3364 - loglik: -1.9789e+02 - logprior: -1.2700e+00
Fitted a model with MAP estimate = -197.5196
expansions: []
discards: [ 0 63]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 212.7979 - loglik: -2.0150e+02 - logprior: -1.1234e+01
Epoch 2/10
19/19 - 5s - loss: 199.9424 - loglik: -1.9832e+02 - logprior: -1.4662e+00
Epoch 3/10
19/19 - 7s - loss: 198.5565 - loglik: -1.9805e+02 - logprior: -2.8450e-01
Epoch 4/10
19/19 - 8s - loss: 197.7741 - loglik: -1.9757e+02 - logprior: 0.0700
Epoch 5/10
19/19 - 8s - loss: 197.7632 - loglik: -1.9775e+02 - logprior: 0.2956
Epoch 6/10
19/19 - 7s - loss: 196.5499 - loglik: -1.9656e+02 - logprior: 0.3491
Epoch 7/10
19/19 - 6s - loss: 196.2894 - loglik: -1.9639e+02 - logprior: 0.4728
Epoch 8/10
19/19 - 7s - loss: 196.4630 - loglik: -1.9664e+02 - logprior: 0.5747
Fitted a model with MAP estimate = -195.5416
Time for alignment: 155.0287
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 303.7153 - loglik: -2.9266e+02 - logprior: -1.0986e+01
Epoch 2/10
19/19 - 7s - loss: 241.3814 - loglik: -2.3902e+02 - logprior: -2.2284e+00
Epoch 3/10
19/19 - 6s - loss: 219.7901 - loglik: -2.1753e+02 - logprior: -1.9425e+00
Epoch 4/10
19/19 - 7s - loss: 217.2687 - loglik: -2.1531e+02 - logprior: -1.6682e+00
Epoch 5/10
19/19 - 8s - loss: 217.5433 - loglik: -2.1577e+02 - logprior: -1.4790e+00
Fitted a model with MAP estimate = -215.3921
expansions: [(10, 4), (11, 2), (26, 1), (29, 1), (43, 1), (48, 5), (49, 1), (50, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 222.5007 - loglik: -2.0880e+02 - logprior: -1.3566e+01
Epoch 2/2
19/19 - 7s - loss: 205.5619 - loglik: -2.0126e+02 - logprior: -4.0629e+00
Fitted a model with MAP estimate = -200.6785
expansions: [(0, 2)]
discards: [ 0 14 59]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 209.4182 - loglik: -1.9973e+02 - logprior: -9.4905e+00
Epoch 2/2
19/19 - 7s - loss: 199.2674 - loglik: -1.9768e+02 - logprior: -1.3226e+00
Fitted a model with MAP estimate = -197.4246
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 212.9810 - loglik: -2.0175e+02 - logprior: -1.1179e+01
Epoch 2/10
19/19 - 7s - loss: 200.8072 - loglik: -1.9926e+02 - logprior: -1.4142e+00
Epoch 3/10
19/19 - 6s - loss: 197.3554 - loglik: -1.9686e+02 - logprior: -2.9157e-01
Epoch 4/10
19/19 - 7s - loss: 198.8666 - loglik: -1.9873e+02 - logprior: 0.1210
Fitted a model with MAP estimate = -197.0827
Time for alignment: 123.6280
Computed alignments with likelihoods: ['-196.8250', '-195.5416', '-197.0827']
Best model has likelihood: -195.5416
time for generating output: 0.6267
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21423206909500564
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac00115700>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac552ced00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabcd273760>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac22091bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabb30b68b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac335a4520>, <__main__.SimpleDirichletPrior object at 0x7fac2a99ec70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 54s - loss: 1096.6207 - loglik: -1.0947e+03 - logprior: -1.3364e+00
Epoch 2/10
49/49 - 48s - loss: 947.4012 - loglik: -9.4532e+02 - logprior: -2.7158e-01
Epoch 3/10
49/49 - 48s - loss: 938.2804 - loglik: -9.3549e+02 - logprior: -3.1288e-01
Epoch 4/10
49/49 - 48s - loss: 931.3722 - loglik: -9.2833e+02 - logprior: -4.9973e-01
Epoch 5/10
49/49 - 48s - loss: 929.6141 - loglik: -9.2691e+02 - logprior: -4.5062e-01
Epoch 6/10
49/49 - 48s - loss: 920.8369 - loglik: -9.1828e+02 - logprior: -5.6217e-01
Epoch 7/10
49/49 - 48s - loss: 927.3046 - loglik: -9.2493e+02 - logprior: -5.7597e-01
Fitted a model with MAP estimate = -921.8093
expansions: [(0, 3), (82, 2), (139, 1), (181, 1), (212, 1), (213, 1), (231, 4), (232, 3), (233, 1), (236, 1), (241, 1), (243, 1), (252, 1), (254, 1), (257, 1), (258, 3), (259, 5), (261, 1), (277, 1), (278, 1), (279, 2), (280, 1), (281, 2), (298, 1), (300, 2), (301, 5), (303, 3), (304, 9), (314, 2), (315, 4), (317, 1), (324, 1), (326, 2), (327, 1), (328, 2), (329, 1), (330, 1), (331, 2), (332, 1), (343, 3), (345, 1), (367, 3), (369, 1), (370, 1), (391, 1), (392, 3), (397, 1), (398, 3), (399, 8)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 962.4839 - loglik: -9.5904e+02 - logprior: -3.1862e+00
Epoch 2/2
49/49 - 68s - loss: 916.3648 - loglik: -9.1735e+02 - logprior: 1.7621
Fitted a model with MAP estimate = -903.9088
expansions: [(0, 3), (39, 3), (375, 1), (376, 1), (424, 2), (491, 1), (496, 1), (497, 2), (498, 1)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  86 379 380 422 448
 501 502 503 504 505]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 69s - loss: 936.3113 - loglik: -9.3446e+02 - logprior: -1.4893e+00
Epoch 2/2
49/49 - 65s - loss: 905.4554 - loglik: -9.0726e+02 - logprior: 3.6034
Fitted a model with MAP estimate = -898.3533
expansions: [(0, 5), (496, 1), (498, 6)]
discards: [  1   2   3   4   5   6   7   8  29  30 340 415]
Re-initialized the encoder parameters.
Fitting a model of length 498 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 69s - loss: 923.6027 - loglik: -9.2197e+02 - logprior: -1.0740e+00
Epoch 2/10
49/49 - 65s - loss: 908.3265 - loglik: -9.1142e+02 - logprior: 4.2897
Epoch 3/10
49/49 - 66s - loss: 893.6562 - loglik: -8.9662e+02 - logprior: 4.7492
Epoch 4/10
49/49 - 65s - loss: 892.6585 - loglik: -8.9563e+02 - logprior: 5.1492
Epoch 5/10
49/49 - 66s - loss: 889.8701 - loglik: -8.9319e+02 - logprior: 5.7009
Epoch 6/10
49/49 - 66s - loss: 887.1180 - loglik: -8.9088e+02 - logprior: 6.2054
Epoch 7/10
49/49 - 65s - loss: 878.6669 - loglik: -8.8309e+02 - logprior: 6.8463
Epoch 8/10
49/49 - 66s - loss: 879.7484 - loglik: -8.8480e+02 - logprior: 7.4316
Fitted a model with MAP estimate = -875.5300
Time for alignment: 1324.3926
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1099.3199 - loglik: -1.0972e+03 - logprior: -1.3889e+00
Epoch 2/10
49/49 - 48s - loss: 942.7598 - loglik: -9.4046e+02 - logprior: -1.9977e-01
Epoch 3/10
49/49 - 48s - loss: 940.8646 - loglik: -9.3800e+02 - logprior: -1.9887e-01
Epoch 4/10
49/49 - 48s - loss: 936.6912 - loglik: -9.3378e+02 - logprior: -3.6848e-01
Epoch 5/10
49/49 - 48s - loss: 925.3157 - loglik: -9.2271e+02 - logprior: -3.9248e-01
Epoch 6/10
49/49 - 48s - loss: 929.5508 - loglik: -9.2719e+02 - logprior: -3.8872e-01
Fitted a model with MAP estimate = -924.9245
expansions: [(0, 3), (82, 2), (135, 1), (161, 2), (180, 1), (211, 1), (212, 1), (225, 1), (226, 1), (228, 3), (229, 2), (230, 4), (232, 2), (235, 1), (236, 2), (237, 2), (238, 1), (239, 1), (246, 1), (249, 1), (250, 2), (251, 5), (253, 1), (269, 2), (270, 2), (271, 4), (288, 1), (291, 5), (292, 1), (294, 2), (296, 8), (297, 1), (309, 6), (321, 1), (323, 1), (324, 1), (325, 1), (330, 1), (344, 3), (346, 1), (394, 1), (395, 4), (396, 1), (400, 13), (404, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 509 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 960.6409 - loglik: -9.5702e+02 - logprior: -3.0394e+00
Epoch 2/2
49/49 - 67s - loss: 912.3860 - loglik: -9.1240e+02 - logprior: 1.5493
Fitted a model with MAP estimate = -901.5408
expansions: [(0, 3), (39, 3), (85, 1), (86, 3), (248, 1), (284, 1), (312, 1), (360, 1), (499, 1), (500, 3)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  87  88  89  90 169 244
 260 476 477 505 506 507 508]
Re-initialized the encoder parameters.
Fitting a model of length 502 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 69s - loss: 936.9319 - loglik: -9.3471e+02 - logprior: -1.7654e+00
Epoch 2/2
49/49 - 66s - loss: 911.8515 - loglik: -9.1328e+02 - logprior: 2.9341
Fitted a model with MAP estimate = -899.6571
expansions: [(0, 5), (81, 2), (338, 1), (497, 1)]
discards: [  2   3   4   5   6   7   8   9  30  31 498 499 500 501]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 69s - loss: 936.9813 - loglik: -9.3601e+02 - logprior: -7.2164e-01
Epoch 2/10
49/49 - 65s - loss: 904.8425 - loglik: -9.0818e+02 - logprior: 4.7823
Epoch 3/10
49/49 - 65s - loss: 902.8644 - loglik: -9.0570e+02 - logprior: 5.0839
Epoch 4/10
49/49 - 65s - loss: 892.2218 - loglik: -8.9542e+02 - logprior: 5.7168
Epoch 5/10
49/49 - 66s - loss: 889.7802 - loglik: -8.9311e+02 - logprior: 5.9198
Epoch 6/10
49/49 - 65s - loss: 889.6630 - loglik: -8.9359e+02 - logprior: 6.5469
Epoch 7/10
49/49 - 65s - loss: 882.4105 - loglik: -8.8721e+02 - logprior: 7.3565
Epoch 8/10
49/49 - 66s - loss: 879.3605 - loglik: -8.8471e+02 - logprior: 7.9153
Epoch 9/10
49/49 - 65s - loss: 879.8149 - loglik: -8.8554e+02 - logprior: 8.3312
Fitted a model with MAP estimate = -873.3314
Time for alignment: 1339.9336
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1101.4641 - loglik: -1.0992e+03 - logprior: -1.6100e+00
Epoch 2/10
49/49 - 48s - loss: 951.2512 - loglik: -9.4814e+02 - logprior: -1.2939e+00
Epoch 3/10
49/49 - 48s - loss: 938.5484 - loglik: -9.3476e+02 - logprior: -1.3553e+00
Epoch 4/10
49/49 - 48s - loss: 934.5930 - loglik: -9.3060e+02 - logprior: -1.5446e+00
Epoch 5/10
49/49 - 49s - loss: 930.8970 - loglik: -9.2723e+02 - logprior: -1.4793e+00
Epoch 6/10
49/49 - 49s - loss: 929.1503 - loglik: -9.2571e+02 - logprior: -1.4649e+00
Epoch 7/10
49/49 - 48s - loss: 927.8544 - loglik: -9.2451e+02 - logprior: -1.5767e+00
Epoch 8/10
49/49 - 48s - loss: 926.7922 - loglik: -9.2374e+02 - logprior: -1.4038e+00
Epoch 9/10
49/49 - 49s - loss: 924.1359 - loglik: -9.2102e+02 - logprior: -1.5436e+00
Epoch 10/10
49/49 - 48s - loss: 926.0044 - loglik: -9.2313e+02 - logprior: -1.2916e+00
Fitted a model with MAP estimate = -922.4506
expansions: [(0, 3), (140, 1), (181, 1), (184, 1), (212, 1), (213, 1), (231, 3), (232, 1), (233, 1), (234, 1), (235, 1), (239, 1), (243, 2), (245, 1), (253, 1), (255, 1), (258, 1), (259, 2), (261, 5), (279, 1), (280, 1), (281, 2), (282, 5), (298, 1), (299, 7), (300, 3), (302, 2), (303, 8), (304, 1), (305, 1), (306, 1), (307, 1), (310, 7), (311, 3), (313, 1), (314, 1), (317, 1), (318, 2), (319, 2), (320, 2), (321, 1), (322, 1), (324, 1), (325, 1), (334, 2), (335, 5), (336, 3), (356, 3), (357, 1), (359, 1), (360, 1), (361, 1), (386, 1), (387, 1), (388, 1), (393, 3), (394, 1), (395, 1)]
discards: [ 18  19 402 403]
Re-initialized the encoder parameters.
Fitting a model of length 511 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 970.6255 - loglik: -9.6737e+02 - logprior: -2.8648e+00
Epoch 2/2
49/49 - 68s - loss: 914.2713 - loglik: -9.1536e+02 - logprior: 2.2342
Fitted a model with MAP estimate = -902.2324
expansions: [(0, 3), (280, 1), (285, 1), (340, 1), (341, 1), (381, 1), (499, 1), (511, 3)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13 366 379 397 420]
Re-initialized the encoder parameters.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 928.1368 - loglik: -9.2585e+02 - logprior: -1.8659e+00
Epoch 2/2
49/49 - 67s - loss: 911.6367 - loglik: -9.1337e+02 - logprior: 3.2531
Fitted a model with MAP estimate = -896.9786
expansions: [(0, 5), (469, 1)]
discards: [  2   3   4   5   6   7   8 280 281 500 501 502 503 504 505]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 68s - loss: 928.9318 - loglik: -9.2814e+02 - logprior: -3.7268e-01
Epoch 2/10
49/49 - 65s - loss: 904.9988 - loglik: -9.0902e+02 - logprior: 4.9991
Epoch 3/10
49/49 - 65s - loss: 901.6199 - loglik: -9.0575e+02 - logprior: 5.6401
Epoch 4/10
49/49 - 65s - loss: 891.6279 - loglik: -8.9590e+02 - logprior: 6.1915
Epoch 5/10
49/49 - 65s - loss: 888.0601 - loglik: -8.9275e+02 - logprior: 6.8627
Epoch 6/10
49/49 - 66s - loss: 891.3877 - loglik: -8.9635e+02 - logprior: 7.2700
Fitted a model with MAP estimate = -880.5306
Time for alignment: 1342.5607
Computed alignments with likelihoods: ['-875.5300', '-873.3314', '-880.5306']
Best model has likelihood: -873.3314
time for generating output: 0.5207
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8059482528674313
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac5dca0940>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe69cdee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac08b8ed00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabbc1fd400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6f2dd00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac115e6880>, <__main__.SimpleDirichletPrior object at 0x7fac554fa610>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 479.0619 - loglik: -4.7674e+02 - logprior: -1.9614e+00
Epoch 2/10
39/39 - 12s - loss: 409.0796 - loglik: -4.0642e+02 - logprior: -1.6695e+00
Epoch 3/10
39/39 - 11s - loss: 400.6149 - loglik: -3.9796e+02 - logprior: -1.6808e+00
Epoch 4/10
39/39 - 11s - loss: 398.2114 - loglik: -3.9565e+02 - logprior: -1.6740e+00
Epoch 5/10
39/39 - 12s - loss: 397.0150 - loglik: -3.9458e+02 - logprior: -1.6597e+00
Epoch 6/10
39/39 - 12s - loss: 396.9349 - loglik: -3.9458e+02 - logprior: -1.6520e+00
Epoch 7/10
39/39 - 11s - loss: 396.1456 - loglik: -3.9384e+02 - logprior: -1.6558e+00
Epoch 8/10
39/39 - 11s - loss: 396.1758 - loglik: -3.9390e+02 - logprior: -1.6539e+00
Fitted a model with MAP estimate = -395.9694
expansions: [(7, 1), (10, 2), (11, 2), (12, 2), (22, 1), (23, 1), (26, 2), (32, 1), (39, 3), (41, 1), (45, 1), (54, 2), (55, 1), (57, 2), (58, 2), (59, 1), (71, 2), (89, 2), (92, 1), (93, 2), (95, 1), (102, 1), (103, 2), (104, 2), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 398.6948 - loglik: -3.9552e+02 - logprior: -2.9734e+00
Epoch 2/2
39/39 - 14s - loss: 381.3154 - loglik: -3.7958e+02 - logprior: -1.2371e+00
Fitted a model with MAP estimate = -378.2144
expansions: []
discards: [ 11  16  35  51  71  77  79  95 115 122 137]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 384.2814 - loglik: -3.8199e+02 - logprior: -2.0154e+00
Epoch 2/2
39/39 - 13s - loss: 379.6039 - loglik: -3.7828e+02 - logprior: -7.7933e-01
Fitted a model with MAP estimate = -377.5794
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 381.5663 - loglik: -3.7967e+02 - logprior: -1.6487e+00
Epoch 2/10
41/41 - 14s - loss: 376.9139 - loglik: -3.7572e+02 - logprior: -6.1792e-01
Epoch 3/10
41/41 - 14s - loss: 375.4362 - loglik: -3.7413e+02 - logprior: -5.2611e-01
Epoch 4/10
41/41 - 14s - loss: 374.2872 - loglik: -3.7292e+02 - logprior: -4.8214e-01
Epoch 5/10
41/41 - 14s - loss: 373.6269 - loglik: -3.7236e+02 - logprior: -4.1158e-01
Epoch 6/10
41/41 - 14s - loss: 373.4044 - loglik: -3.7224e+02 - logprior: -3.3918e-01
Epoch 7/10
41/41 - 14s - loss: 371.5797 - loglik: -3.7058e+02 - logprior: -2.5247e-01
Epoch 8/10
41/41 - 14s - loss: 371.5082 - loglik: -3.7064e+02 - logprior: -1.7301e-01
Epoch 9/10
41/41 - 14s - loss: 371.1052 - loglik: -3.7035e+02 - logprior: -9.4840e-02
Epoch 10/10
41/41 - 14s - loss: 372.0449 - loglik: -3.7138e+02 - logprior: -2.2756e-02
Fitted a model with MAP estimate = -370.2782
Time for alignment: 372.7666
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 478.0449 - loglik: -4.7559e+02 - logprior: -1.9647e+00
Epoch 2/10
39/39 - 11s - loss: 409.5847 - loglik: -4.0688e+02 - logprior: -1.6921e+00
Epoch 3/10
39/39 - 11s - loss: 401.6533 - loglik: -3.9911e+02 - logprior: -1.6646e+00
Epoch 4/10
39/39 - 11s - loss: 399.0300 - loglik: -3.9651e+02 - logprior: -1.7006e+00
Epoch 5/10
39/39 - 12s - loss: 397.3049 - loglik: -3.9489e+02 - logprior: -1.7104e+00
Epoch 6/10
39/39 - 12s - loss: 396.8111 - loglik: -3.9444e+02 - logprior: -1.7132e+00
Epoch 7/10
39/39 - 11s - loss: 396.3315 - loglik: -3.9402e+02 - logprior: -1.7167e+00
Epoch 8/10
39/39 - 11s - loss: 395.8634 - loglik: -3.9358e+02 - logprior: -1.7132e+00
Epoch 9/10
39/39 - 12s - loss: 395.2022 - loglik: -3.9296e+02 - logprior: -1.7056e+00
Epoch 10/10
39/39 - 12s - loss: 395.5386 - loglik: -3.9332e+02 - logprior: -1.7022e+00
Fitted a model with MAP estimate = -395.7423
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 2), (32, 1), (40, 1), (41, 1), (44, 1), (45, 1), (55, 2), (57, 1), (58, 1), (59, 2), (69, 2), (71, 2), (76, 1), (79, 3), (93, 1), (95, 1), (98, 2), (103, 1), (104, 2), (106, 1), (109, 1), (118, 1), (122, 1), (125, 1), (126, 1), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 395.1237 - loglik: -3.9160e+02 - logprior: -3.0153e+00
Epoch 2/2
39/39 - 14s - loss: 379.4639 - loglik: -3.7747e+02 - logprior: -1.1479e+00
Fitted a model with MAP estimate = -375.7791
expansions: []
discards: [ 13  34  70  77  89  93 129]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 384.1431 - loglik: -3.8193e+02 - logprior: -2.0302e+00
Epoch 2/2
39/39 - 13s - loss: 377.5992 - loglik: -3.7619e+02 - logprior: -8.0864e-01
Fitted a model with MAP estimate = -375.5920
expansions: [(156, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 172 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 19s - loss: 381.2404 - loglik: -3.7927e+02 - logprior: -1.6748e+00
Epoch 2/10
41/41 - 14s - loss: 375.5052 - loglik: -3.7411e+02 - logprior: -6.3876e-01
Epoch 3/10
41/41 - 14s - loss: 374.0937 - loglik: -3.7262e+02 - logprior: -5.9132e-01
Epoch 4/10
41/41 - 14s - loss: 370.7899 - loglik: -3.6939e+02 - logprior: -5.2663e-01
Epoch 5/10
41/41 - 14s - loss: 371.4659 - loglik: -3.7016e+02 - logprior: -4.6424e-01
Fitted a model with MAP estimate = -369.7724
Time for alignment: 325.8588
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 477.1484 - loglik: -4.7485e+02 - logprior: -1.9705e+00
Epoch 2/10
39/39 - 12s - loss: 407.6451 - loglik: -4.0489e+02 - logprior: -1.7459e+00
Epoch 3/10
39/39 - 12s - loss: 399.7917 - loglik: -3.9707e+02 - logprior: -1.7877e+00
Epoch 4/10
39/39 - 11s - loss: 397.6130 - loglik: -3.9497e+02 - logprior: -1.7673e+00
Epoch 5/10
39/39 - 12s - loss: 396.8759 - loglik: -3.9436e+02 - logprior: -1.7493e+00
Epoch 6/10
39/39 - 12s - loss: 396.3160 - loglik: -3.9386e+02 - logprior: -1.7506e+00
Epoch 7/10
39/39 - 12s - loss: 396.1524 - loglik: -3.9374e+02 - logprior: -1.7521e+00
Epoch 8/10
39/39 - 12s - loss: 395.6161 - loglik: -3.9324e+02 - logprior: -1.7498e+00
Epoch 9/10
39/39 - 12s - loss: 395.2177 - loglik: -3.9287e+02 - logprior: -1.7420e+00
Epoch 10/10
39/39 - 12s - loss: 395.1107 - loglik: -3.9278e+02 - logprior: -1.7457e+00
Fitted a model with MAP estimate = -395.6057
expansions: [(7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (22, 1), (23, 1), (26, 2), (32, 1), (39, 2), (40, 2), (43, 1), (55, 1), (56, 1), (57, 1), (58, 2), (59, 1), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 397.1423 - loglik: -3.9392e+02 - logprior: -2.9639e+00
Epoch 2/2
39/39 - 14s - loss: 379.9285 - loglik: -3.7820e+02 - logprior: -1.0939e+00
Fitted a model with MAP estimate = -376.7360
expansions: []
discards: [ 15  34  53  76  92 112]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 383.9577 - loglik: -3.8174e+02 - logprior: -1.9986e+00
Epoch 2/2
39/39 - 14s - loss: 378.5290 - loglik: -3.7728e+02 - logprior: -7.6335e-01
Fitted a model with MAP estimate = -377.2315
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 18s - loss: 382.1799 - loglik: -3.8032e+02 - logprior: -1.6460e+00
Epoch 2/10
41/41 - 14s - loss: 377.9977 - loglik: -3.7671e+02 - logprior: -5.9053e-01
Epoch 3/10
41/41 - 14s - loss: 374.5316 - loglik: -3.7308e+02 - logprior: -5.5624e-01
Epoch 4/10
41/41 - 14s - loss: 373.6714 - loglik: -3.7234e+02 - logprior: -4.8301e-01
Epoch 5/10
41/41 - 14s - loss: 373.3265 - loglik: -3.7209e+02 - logprior: -4.3622e-01
Epoch 6/10
41/41 - 14s - loss: 373.0313 - loglik: -3.7189e+02 - logprior: -3.5649e-01
Epoch 7/10
41/41 - 14s - loss: 372.1495 - loglik: -3.7115e+02 - logprior: -2.6801e-01
Epoch 8/10
41/41 - 14s - loss: 370.8282 - loglik: -3.6998e+02 - logprior: -1.8926e-01
Epoch 9/10
41/41 - 15s - loss: 372.2101 - loglik: -3.7146e+02 - logprior: -1.1398e-01
Fitted a model with MAP estimate = -370.4910
Time for alignment: 382.2170
Computed alignments with likelihoods: ['-370.2782', '-369.7724', '-370.4910']
Best model has likelihood: -369.7724
time for generating output: 0.3238
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.4696969696969697
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabf7b567c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac76b5d850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabbc1fd400>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac00324c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe69cdee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac005533d0>, <__main__.SimpleDirichletPrior object at 0x7fab301c3490>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.7099 - loglik: -1.1171e+02 - logprior: -5.8930e+01
Epoch 2/10
10/10 - 1s - loss: 105.6714 - loglik: -8.9046e+01 - logprior: -1.6604e+01
Epoch 3/10
10/10 - 1s - loss: 81.1022 - loglik: -7.2763e+01 - logprior: -8.2988e+00
Epoch 4/10
10/10 - 1s - loss: 70.5186 - loglik: -6.5201e+01 - logprior: -5.2690e+00
Epoch 5/10
10/10 - 1s - loss: 65.1583 - loglik: -6.1348e+01 - logprior: -3.6843e+00
Epoch 6/10
10/10 - 1s - loss: 63.5722 - loglik: -6.0416e+01 - logprior: -2.9009e+00
Epoch 7/10
10/10 - 1s - loss: 62.7771 - loglik: -6.0074e+01 - logprior: -2.4582e+00
Epoch 8/10
10/10 - 1s - loss: 62.2409 - loglik: -5.9852e+01 - logprior: -2.1618e+00
Epoch 9/10
10/10 - 1s - loss: 62.0661 - loglik: -5.9842e+01 - logprior: -1.9685e+00
Epoch 10/10
10/10 - 1s - loss: 61.8772 - loglik: -5.9736e+01 - logprior: -1.8663e+00
Fitted a model with MAP estimate = -61.4074
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.6612 - loglik: -5.9374e+01 - logprior: -7.9182e+01
Epoch 2/2
10/10 - 1s - loss: 79.8067 - loglik: -5.4087e+01 - logprior: -2.5680e+01
Fitted a model with MAP estimate = -68.5902
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 117.5222 - loglik: -5.1769e+01 - logprior: -6.5741e+01
Epoch 2/2
10/10 - 1s - loss: 73.9890 - loglik: -5.0875e+01 - logprior: -2.3083e+01
Fitted a model with MAP estimate = -64.7876
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 105.2142 - loglik: -5.0348e+01 - logprior: -5.4817e+01
Epoch 2/10
10/10 - 1s - loss: 66.3503 - loglik: -5.0942e+01 - logprior: -1.5365e+01
Epoch 3/10
10/10 - 1s - loss: 58.7756 - loglik: -5.1333e+01 - logprior: -7.3421e+00
Epoch 4/10
10/10 - 1s - loss: 55.7575 - loglik: -5.1355e+01 - logprior: -4.2098e+00
Epoch 5/10
10/10 - 1s - loss: 54.0134 - loglik: -5.1111e+01 - logprior: -2.6796e+00
Epoch 6/10
10/10 - 1s - loss: 53.0257 - loglik: -5.1038e+01 - logprior: -1.7289e+00
Epoch 7/10
10/10 - 1s - loss: 52.3564 - loglik: -5.0992e+01 - logprior: -1.0816e+00
Epoch 8/10
10/10 - 1s - loss: 52.2055 - loglik: -5.1184e+01 - logprior: -7.1790e-01
Epoch 9/10
10/10 - 1s - loss: 51.8344 - loglik: -5.1035e+01 - logprior: -4.8092e-01
Epoch 10/10
10/10 - 1s - loss: 51.7050 - loglik: -5.1077e+01 - logprior: -3.0195e-01
Fitted a model with MAP estimate = -51.2851
Time for alignment: 28.9496
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.6944 - loglik: -1.1161e+02 - logprior: -5.8930e+01
Epoch 2/10
10/10 - 1s - loss: 105.9785 - loglik: -8.9311e+01 - logprior: -1.6603e+01
Epoch 3/10
10/10 - 1s - loss: 81.2023 - loglik: -7.2873e+01 - logprior: -8.2876e+00
Epoch 4/10
10/10 - 1s - loss: 70.5940 - loglik: -6.5355e+01 - logprior: -5.2062e+00
Epoch 5/10
10/10 - 1s - loss: 64.8570 - loglik: -6.1205e+01 - logprior: -3.5980e+00
Epoch 6/10
10/10 - 1s - loss: 62.0562 - loglik: -5.9040e+01 - logprior: -2.8174e+00
Epoch 7/10
10/10 - 1s - loss: 61.4255 - loglik: -5.8759e+01 - logprior: -2.3941e+00
Epoch 8/10
10/10 - 1s - loss: 60.7583 - loglik: -5.8392e+01 - logprior: -2.1141e+00
Epoch 9/10
10/10 - 1s - loss: 60.6446 - loglik: -5.8486e+01 - logprior: -1.8935e+00
Epoch 10/10
10/10 - 1s - loss: 60.2899 - loglik: -5.8280e+01 - logprior: -1.7190e+00
Fitted a model with MAP estimate = -60.0516
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.3425 - loglik: -5.7963e+01 - logprior: -7.9345e+01
Epoch 2/2
10/10 - 1s - loss: 79.2081 - loglik: -5.3427e+01 - logprior: -2.5719e+01
Fitted a model with MAP estimate = -68.0425
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.4570 - loglik: -5.1440e+01 - logprior: -6.5894e+01
Epoch 2/2
10/10 - 1s - loss: 73.5647 - loglik: -5.0361e+01 - logprior: -2.3133e+01
Fitted a model with MAP estimate = -64.4156
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 105.3911 - loglik: -5.0455e+01 - logprior: -5.4874e+01
Epoch 2/10
10/10 - 1s - loss: 66.1659 - loglik: -5.0671e+01 - logprior: -1.5456e+01
Epoch 3/10
10/10 - 1s - loss: 58.6577 - loglik: -5.1119e+01 - logprior: -7.4354e+00
Epoch 4/10
10/10 - 1s - loss: 55.6566 - loglik: -5.1167e+01 - logprior: -4.2798e+00
Epoch 5/10
10/10 - 1s - loss: 54.0359 - loglik: -5.1052e+01 - logprior: -2.7382e+00
Epoch 6/10
10/10 - 1s - loss: 53.1294 - loglik: -5.1070e+01 - logprior: -1.7881e+00
Epoch 7/10
10/10 - 1s - loss: 52.3249 - loglik: -5.0883e+01 - logprior: -1.1533e+00
Epoch 8/10
10/10 - 1s - loss: 52.1529 - loglik: -5.1060e+01 - logprior: -7.8762e-01
Epoch 9/10
10/10 - 1s - loss: 51.9733 - loglik: -5.1115e+01 - logprior: -5.4649e-01
Epoch 10/10
10/10 - 1s - loss: 51.4946 - loglik: -5.0797e+01 - logprior: -3.7701e-01
Fitted a model with MAP estimate = -51.2631
Time for alignment: 28.9984
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.6858 - loglik: -1.1171e+02 - logprior: -5.8930e+01
Epoch 2/10
10/10 - 1s - loss: 105.7709 - loglik: -8.9140e+01 - logprior: -1.6607e+01
Epoch 3/10
10/10 - 1s - loss: 80.6441 - loglik: -7.2322e+01 - logprior: -8.2840e+00
Epoch 4/10
10/10 - 1s - loss: 70.0918 - loglik: -6.4860e+01 - logprior: -5.1926e+00
Epoch 5/10
10/10 - 1s - loss: 64.3976 - loglik: -6.0763e+01 - logprior: -3.5984e+00
Epoch 6/10
10/10 - 1s - loss: 62.0943 - loglik: -5.9117e+01 - logprior: -2.8096e+00
Epoch 7/10
10/10 - 1s - loss: 61.1574 - loglik: -5.8500e+01 - logprior: -2.3730e+00
Epoch 8/10
10/10 - 1s - loss: 60.9219 - loglik: -5.8547e+01 - logprior: -2.1025e+00
Epoch 9/10
10/10 - 1s - loss: 60.3285 - loglik: -5.8171e+01 - logprior: -1.8873e+00
Epoch 10/10
10/10 - 1s - loss: 60.5416 - loglik: -5.8523e+01 - logprior: -1.7211e+00
Fitted a model with MAP estimate = -60.0190
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 137.2522 - loglik: -5.7808e+01 - logprior: -7.9328e+01
Epoch 2/2
10/10 - 1s - loss: 78.9401 - loglik: -5.3113e+01 - logprior: -2.5714e+01
Fitted a model with MAP estimate = -68.0368
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.3434 - loglik: -5.1440e+01 - logprior: -6.5891e+01
Epoch 2/2
10/10 - 1s - loss: 73.3801 - loglik: -5.0171e+01 - logprior: -2.3133e+01
Fitted a model with MAP estimate = -64.3426
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 105.1003 - loglik: -5.0203e+01 - logprior: -5.4864e+01
Epoch 2/10
10/10 - 1s - loss: 66.1969 - loglik: -5.0734e+01 - logprior: -1.5445e+01
Epoch 3/10
10/10 - 1s - loss: 58.6433 - loglik: -5.1129e+01 - logprior: -7.4334e+00
Epoch 4/10
10/10 - 1s - loss: 55.7400 - loglik: -5.1263e+01 - logprior: -4.3073e+00
Epoch 5/10
10/10 - 1s - loss: 53.8352 - loglik: -5.0821e+01 - logprior: -2.7977e+00
Epoch 6/10
10/10 - 1s - loss: 52.9418 - loglik: -5.0920e+01 - logprior: -1.7816e+00
Epoch 7/10
10/10 - 1s - loss: 52.5235 - loglik: -5.1092e+01 - logprior: -1.1629e+00
Epoch 8/10
10/10 - 1s - loss: 51.9444 - loglik: -5.0841e+01 - logprior: -8.0661e-01
Epoch 9/10
10/10 - 1s - loss: 51.7522 - loglik: -5.0870e+01 - logprior: -5.6656e-01
Epoch 10/10
10/10 - 1s - loss: 51.7371 - loglik: -5.1026e+01 - logprior: -3.8564e-01
Fitted a model with MAP estimate = -51.2431
Time for alignment: 29.8158
Computed alignments with likelihoods: ['-51.2851', '-51.2631', '-51.2431']
Best model has likelihood: -51.2431
time for generating output: 0.1052
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9734299516908212
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab2f7c2ac0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaaaa5790>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabef476c70>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac66166610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4c89ee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaeb8938b0>, <__main__.SimpleDirichletPrior object at 0x7fab2efb0a00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.6743 - loglik: -3.6975e+02 - logprior: -2.8172e+00
Epoch 2/10
20/20 - 4s - loss: 326.2807 - loglik: -3.2448e+02 - logprior: -1.2810e+00
Epoch 3/10
20/20 - 4s - loss: 305.5814 - loglik: -3.0307e+02 - logprior: -1.5350e+00
Epoch 4/10
20/20 - 4s - loss: 301.2791 - loglik: -2.9877e+02 - logprior: -1.4741e+00
Epoch 5/10
20/20 - 4s - loss: 300.1486 - loglik: -2.9770e+02 - logprior: -1.4723e+00
Epoch 6/10
20/20 - 5s - loss: 299.1194 - loglik: -2.9682e+02 - logprior: -1.4345e+00
Epoch 7/10
20/20 - 4s - loss: 298.4288 - loglik: -2.9621e+02 - logprior: -1.4244e+00
Epoch 8/10
20/20 - 4s - loss: 298.0945 - loglik: -2.9592e+02 - logprior: -1.4145e+00
Epoch 9/10
20/20 - 4s - loss: 297.6604 - loglik: -2.9554e+02 - logprior: -1.4098e+00
Epoch 10/10
20/20 - 4s - loss: 297.1099 - loglik: -2.9502e+02 - logprior: -1.4095e+00
Fitted a model with MAP estimate = -289.9152
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 2), (60, 1), (61, 1), (64, 1), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 312.9838 - loglik: -3.0995e+02 - logprior: -2.7496e+00
Epoch 2/2
40/40 - 7s - loss: 290.5868 - loglik: -2.8885e+02 - logprior: -1.0255e+00
Fitted a model with MAP estimate = -273.7204
expansions: [(98, 1), (108, 1), (137, 2)]
discards: [  8  13  46  75  80 102 104 105 106 125]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 293.8266 - loglik: -2.9153e+02 - logprior: -1.9644e+00
Epoch 2/2
40/40 - 7s - loss: 288.2695 - loglik: -2.8683e+02 - logprior: -8.0900e-01
Fitted a model with MAP estimate = -274.3976
expansions: [(95, 1)]
discards: [130]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 274.1678 - loglik: -2.7272e+02 - logprior: -1.1148e+00
Epoch 2/10
57/57 - 9s - loss: 269.3274 - loglik: -2.6775e+02 - logprior: -7.6903e-01
Epoch 3/10
57/57 - 9s - loss: 267.9496 - loglik: -2.6603e+02 - logprior: -7.5568e-01
Epoch 4/10
57/57 - 9s - loss: 266.5035 - loglik: -2.6453e+02 - logprior: -7.3025e-01
Epoch 5/10
57/57 - 9s - loss: 265.3661 - loglik: -2.6344e+02 - logprior: -7.0296e-01
Epoch 6/10
57/57 - 9s - loss: 264.3807 - loglik: -2.6250e+02 - logprior: -6.6529e-01
Epoch 7/10
57/57 - 9s - loss: 263.9992 - loglik: -2.6228e+02 - logprior: -6.4072e-01
Epoch 8/10
57/57 - 9s - loss: 264.0460 - loglik: -2.6241e+02 - logprior: -6.1602e-01
Fitted a model with MAP estimate = -262.0484
Time for alignment: 214.1391
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.6900 - loglik: -3.6971e+02 - logprior: -2.8156e+00
Epoch 2/10
20/20 - 4s - loss: 327.0651 - loglik: -3.2528e+02 - logprior: -1.2749e+00
Epoch 3/10
20/20 - 4s - loss: 307.0684 - loglik: -3.0456e+02 - logprior: -1.5240e+00
Epoch 4/10
20/20 - 4s - loss: 301.8845 - loglik: -2.9935e+02 - logprior: -1.4927e+00
Epoch 5/10
20/20 - 4s - loss: 300.4933 - loglik: -2.9797e+02 - logprior: -1.5246e+00
Epoch 6/10
20/20 - 4s - loss: 298.8835 - loglik: -2.9646e+02 - logprior: -1.4997e+00
Epoch 7/10
20/20 - 5s - loss: 299.0192 - loglik: -2.9672e+02 - logprior: -1.4841e+00
Fitted a model with MAP estimate = -288.0199
expansions: [(5, 1), (8, 1), (10, 2), (13, 2), (18, 1), (19, 1), (31, 1), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 304.8161 - loglik: -3.0183e+02 - logprior: -2.7242e+00
Epoch 2/2
40/40 - 7s - loss: 288.3362 - loglik: -2.8679e+02 - logprior: -9.7337e-01
Fitted a model with MAP estimate = -273.6868
expansions: []
discards: [ 12  45  81  83 101 104 108]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 294.7013 - loglik: -2.9264e+02 - logprior: -1.8790e+00
Epoch 2/2
40/40 - 7s - loss: 288.0966 - loglik: -2.8700e+02 - logprior: -7.6941e-01
Fitted a model with MAP estimate = -274.8032
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 273.5500 - loglik: -2.7204e+02 - logprior: -1.1066e+00
Epoch 2/10
57/57 - 9s - loss: 268.6589 - loglik: -2.6689e+02 - logprior: -8.0871e-01
Epoch 3/10
57/57 - 9s - loss: 267.7900 - loglik: -2.6569e+02 - logprior: -8.0378e-01
Epoch 4/10
57/57 - 9s - loss: 266.5658 - loglik: -2.6450e+02 - logprior: -7.8718e-01
Epoch 5/10
57/57 - 9s - loss: 264.5416 - loglik: -2.6257e+02 - logprior: -7.6027e-01
Epoch 6/10
57/57 - 9s - loss: 265.1493 - loglik: -2.6318e+02 - logprior: -7.3481e-01
Fitted a model with MAP estimate = -262.7841
Time for alignment: 181.4224
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.4584 - loglik: -3.6950e+02 - logprior: -2.8219e+00
Epoch 2/10
20/20 - 4s - loss: 325.9849 - loglik: -3.2420e+02 - logprior: -1.2913e+00
Epoch 3/10
20/20 - 4s - loss: 306.3596 - loglik: -3.0384e+02 - logprior: -1.5498e+00
Epoch 4/10
20/20 - 4s - loss: 301.7320 - loglik: -2.9925e+02 - logprior: -1.4908e+00
Epoch 5/10
20/20 - 4s - loss: 300.5893 - loglik: -2.9816e+02 - logprior: -1.5010e+00
Epoch 6/10
20/20 - 4s - loss: 299.6700 - loglik: -2.9737e+02 - logprior: -1.4568e+00
Epoch 7/10
20/20 - 4s - loss: 298.8095 - loglik: -2.9660e+02 - logprior: -1.4579e+00
Epoch 8/10
20/20 - 4s - loss: 298.6855 - loglik: -2.9652e+02 - logprior: -1.4432e+00
Epoch 9/10
20/20 - 4s - loss: 297.9632 - loglik: -2.9581e+02 - logprior: -1.4424e+00
Epoch 10/10
20/20 - 4s - loss: 297.4133 - loglik: -2.9531e+02 - logprior: -1.4413e+00
Fitted a model with MAP estimate = -290.3428
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 2), (61, 1), (63, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 308.5079 - loglik: -3.0540e+02 - logprior: -2.7559e+00
Epoch 2/2
40/40 - 7s - loss: 289.7716 - loglik: -2.8788e+02 - logprior: -9.9011e-01
Fitted a model with MAP estimate = -273.5181
expansions: []
discards: [  8  13  46  75 101 104 108]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 293.9940 - loglik: -2.9194e+02 - logprior: -1.8967e+00
Epoch 2/2
40/40 - 7s - loss: 287.7941 - loglik: -2.8658e+02 - logprior: -7.6852e-01
Fitted a model with MAP estimate = -274.7069
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 276.2987 - loglik: -2.7502e+02 - logprior: -1.0777e+00
Epoch 2/10
57/57 - 9s - loss: 269.5045 - loglik: -2.6761e+02 - logprior: -7.9264e-01
Epoch 3/10
57/57 - 9s - loss: 267.6939 - loglik: -2.6550e+02 - logprior: -7.8266e-01
Epoch 4/10
57/57 - 9s - loss: 266.4821 - loglik: -2.6456e+02 - logprior: -7.6006e-01
Epoch 5/10
57/57 - 9s - loss: 265.0580 - loglik: -2.6319e+02 - logprior: -7.3337e-01
Epoch 6/10
57/57 - 9s - loss: 265.0456 - loglik: -2.6309e+02 - logprior: -7.1933e-01
Epoch 7/10
57/57 - 9s - loss: 263.9182 - loglik: -2.6220e+02 - logprior: -6.9113e-01
Epoch 8/10
57/57 - 9s - loss: 263.7827 - loglik: -2.6214e+02 - logprior: -6.6320e-01
Epoch 9/10
57/57 - 9s - loss: 263.0373 - loglik: -2.6144e+02 - logprior: -6.3771e-01
Epoch 10/10
57/57 - 9s - loss: 262.7151 - loglik: -2.6130e+02 - logprior: -6.0346e-01
Fitted a model with MAP estimate = -261.3210
Time for alignment: 229.4969
Computed alignments with likelihoods: ['-262.0484', '-262.7841', '-261.3210']
Best model has likelihood: -261.3210
time for generating output: 0.3575
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6169127516778523
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac110650a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac554f9fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac331d9c10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabef476c70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaaf28c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac003f79a0>, <__main__.SimpleDirichletPrior object at 0x7fac08ace400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 685.9888 - loglik: -6.7626e+02 - logprior: -9.5940e+00
Epoch 2/10
19/19 - 8s - loss: 617.1035 - loglik: -6.1597e+02 - logprior: -6.1295e-01
Epoch 3/10
19/19 - 8s - loss: 583.6175 - loglik: -5.8180e+02 - logprior: -7.8821e-01
Epoch 4/10
19/19 - 8s - loss: 575.0376 - loglik: -5.7297e+02 - logprior: -7.1901e-01
Epoch 5/10
19/19 - 8s - loss: 570.9993 - loglik: -5.6901e+02 - logprior: -5.1875e-01
Epoch 6/10
19/19 - 8s - loss: 568.6129 - loglik: -5.6652e+02 - logprior: -5.6982e-01
Epoch 7/10
19/19 - 8s - loss: 565.0993 - loglik: -5.6301e+02 - logprior: -5.8406e-01
Epoch 8/10
19/19 - 8s - loss: 564.2993 - loglik: -5.6225e+02 - logprior: -6.0596e-01
Epoch 9/10
19/19 - 8s - loss: 562.8532 - loglik: -5.6087e+02 - logprior: -6.0935e-01
Epoch 10/10
19/19 - 8s - loss: 564.4880 - loglik: -5.6263e+02 - logprior: -5.6877e-01
Fitted a model with MAP estimate = -560.9093
expansions: [(25, 1), (27, 1), (28, 2), (29, 3), (43, 6), (65, 3), (91, 1), (93, 2), (102, 1), (103, 2), (104, 1), (105, 1), (118, 1), (119, 2), (137, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [154 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 647.3463 - loglik: -6.3727e+02 - logprior: -9.8613e+00
Epoch 2/2
19/19 - 9s - loss: 588.4299 - loglik: -5.8648e+02 - logprior: -1.4894e+00
Fitted a model with MAP estimate = -577.6378
expansions: [(182, 5), (192, 2), (193, 4), (208, 1)]
discards: [ 29  36  37  38  56  57  58  59 108 142 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 600.9409 - loglik: -5.9185e+02 - logprior: -8.9995e+00
Epoch 2/2
19/19 - 9s - loss: 577.0273 - loglik: -5.7646e+02 - logprior: -8.3119e-02
Fitted a model with MAP estimate = -569.5837
expansions: [(31, 2), (42, 3), (43, 1), (52, 1), (100, 1), (173, 1), (175, 1), (189, 5), (190, 2)]
discards: [ 34  47  48  49 177 178 179 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 589.4056 - loglik: -5.8081e+02 - logprior: -8.2625e+00
Epoch 2/10
19/19 - 9s - loss: 572.7076 - loglik: -5.7242e+02 - logprior: 0.4391
Epoch 3/10
19/19 - 9s - loss: 562.8743 - loglik: -5.6296e+02 - logprior: 1.2863
Epoch 4/10
19/19 - 9s - loss: 560.5934 - loglik: -5.6078e+02 - logprior: 1.6880
Epoch 5/10
19/19 - 9s - loss: 555.2361 - loglik: -5.5549e+02 - logprior: 1.9072
Epoch 6/10
19/19 - 9s - loss: 552.7034 - loglik: -5.5298e+02 - logprior: 2.0201
Epoch 7/10
19/19 - 9s - loss: 549.5039 - loglik: -5.4987e+02 - logprior: 2.1307
Epoch 8/10
19/19 - 9s - loss: 547.6442 - loglik: -5.4819e+02 - logprior: 2.2595
Epoch 9/10
19/19 - 9s - loss: 548.1785 - loglik: -5.4904e+02 - logprior: 2.4384
Fitted a model with MAP estimate = -544.9823
Time for alignment: 236.2166
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 686.6504 - loglik: -6.7690e+02 - logprior: -9.6229e+00
Epoch 2/10
19/19 - 8s - loss: 618.2568 - loglik: -6.1729e+02 - logprior: -4.9594e-01
Epoch 3/10
19/19 - 8s - loss: 582.9216 - loglik: -5.8142e+02 - logprior: -5.1581e-01
Epoch 4/10
19/19 - 8s - loss: 575.4800 - loglik: -5.7364e+02 - logprior: -5.4081e-01
Epoch 5/10
19/19 - 8s - loss: 570.7073 - loglik: -5.6890e+02 - logprior: -3.8833e-01
Epoch 6/10
19/19 - 8s - loss: 567.8203 - loglik: -5.6593e+02 - logprior: -4.3068e-01
Epoch 7/10
19/19 - 8s - loss: 566.5250 - loglik: -5.6462e+02 - logprior: -4.5355e-01
Epoch 8/10
19/19 - 8s - loss: 565.5383 - loglik: -5.6365e+02 - logprior: -4.6942e-01
Epoch 9/10
19/19 - 8s - loss: 562.7242 - loglik: -5.6093e+02 - logprior: -4.2054e-01
Epoch 10/10
19/19 - 8s - loss: 562.2509 - loglik: -5.6051e+02 - logprior: -4.1682e-01
Fitted a model with MAP estimate = -560.8291
expansions: [(0, 2), (25, 1), (27, 1), (28, 2), (33, 17), (43, 5), (64, 2), (89, 1), (91, 1), (93, 2), (102, 7), (103, 1), (104, 1), (119, 1), (135, 1), (154, 5), (183, 1), (184, 2), (185, 1)]
discards: [  1 169 170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 652.0125 - loglik: -6.3764e+02 - logprior: -1.4113e+01
Epoch 2/2
19/19 - 10s - loss: 587.8948 - loglik: -5.8532e+02 - logprior: -2.0985e+00
Fitted a model with MAP estimate = -574.8129
expansions: [(117, 1), (199, 1), (201, 1), (213, 3), (214, 7), (215, 2), (224, 1)]
discards: [  0  30  41  42  43  44  45  46  55  56  57  58  59  60  70  92 119 122
 138 139 140 203 204]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 607.9392 - loglik: -5.9497e+02 - logprior: -1.2924e+01
Epoch 2/2
19/19 - 9s - loss: 581.4711 - loglik: -5.7870e+02 - logprior: -2.6532e+00
Fitted a model with MAP estimate = -572.6804
expansions: [(0, 2), (38, 3), (54, 1), (184, 1), (197, 2)]
discards: [  0 185 190 191 207]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 586.8330 - loglik: -5.7834e+02 - logprior: -8.2915e+00
Epoch 2/10
19/19 - 9s - loss: 570.6287 - loglik: -5.7060e+02 - logprior: 0.5073
Epoch 3/10
19/19 - 9s - loss: 563.2099 - loglik: -5.6354e+02 - logprior: 1.2782
Epoch 4/10
19/19 - 9s - loss: 558.8415 - loglik: -5.5918e+02 - logprior: 1.6174
Epoch 5/10
19/19 - 9s - loss: 555.2738 - loglik: -5.5553e+02 - logprior: 1.7599
Epoch 6/10
19/19 - 9s - loss: 549.3667 - loglik: -5.4954e+02 - logprior: 1.8200
Epoch 7/10
19/19 - 9s - loss: 549.6472 - loglik: -5.4982e+02 - logprior: 1.9051
Fitted a model with MAP estimate = -545.9364
Time for alignment: 222.9345
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 685.5975 - loglik: -6.7565e+02 - logprior: -9.6056e+00
Epoch 2/10
19/19 - 8s - loss: 617.4625 - loglik: -6.1606e+02 - logprior: -5.4274e-01
Epoch 3/10
19/19 - 8s - loss: 585.0051 - loglik: -5.8281e+02 - logprior: -8.4089e-01
Epoch 4/10
19/19 - 8s - loss: 576.7155 - loglik: -5.7423e+02 - logprior: -8.5751e-01
Epoch 5/10
19/19 - 8s - loss: 572.4677 - loglik: -5.7008e+02 - logprior: -7.0115e-01
Epoch 6/10
19/19 - 8s - loss: 569.7731 - loglik: -5.6735e+02 - logprior: -7.5735e-01
Epoch 7/10
19/19 - 8s - loss: 569.3576 - loglik: -5.6701e+02 - logprior: -7.7813e-01
Epoch 8/10
19/19 - 8s - loss: 566.3587 - loglik: -5.6407e+02 - logprior: -8.1745e-01
Epoch 9/10
19/19 - 8s - loss: 565.6974 - loglik: -5.6349e+02 - logprior: -7.9140e-01
Epoch 10/10
19/19 - 8s - loss: 563.1568 - loglik: -5.6105e+02 - logprior: -7.6783e-01
Fitted a model with MAP estimate = -562.6351
expansions: [(25, 1), (27, 1), (28, 2), (33, 1), (43, 5), (59, 1), (63, 2), (91, 1), (93, 3), (101, 1), (102, 3), (103, 1), (104, 1), (118, 2), (120, 1), (135, 1), (162, 1), (163, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  1 170 176]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 655.2340 - loglik: -6.4499e+02 - logprior: -1.0159e+01
Epoch 2/2
19/19 - 9s - loss: 594.7930 - loglik: -5.9265e+02 - logprior: -1.8788e+00
Fitted a model with MAP estimate = -581.3435
expansions: [(123, 1), (177, 1), (178, 5)]
discards: [ 31  32  33  52 104 105 118 126 144 179 180 181 188 189 190 191 192 193
 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 616.0959 - loglik: -6.0661e+02 - logprior: -9.3592e+00
Epoch 2/2
19/19 - 7s - loss: 595.4421 - loglik: -5.9494e+02 - logprior: 0.1140
Fitted a model with MAP estimate = -587.7535
expansions: [(40, 2), (41, 1), (174, 4), (183, 29)]
discards: [70]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 607.2674 - loglik: -5.9866e+02 - logprior: -8.5530e+00
Epoch 2/10
19/19 - 9s - loss: 584.6584 - loglik: -5.8449e+02 - logprior: 0.1079
Epoch 3/10
19/19 - 9s - loss: 573.4972 - loglik: -5.7364e+02 - logprior: 0.9100
Epoch 4/10
19/19 - 9s - loss: 567.1935 - loglik: -5.6734e+02 - logprior: 1.2921
Epoch 5/10
19/19 - 9s - loss: 565.2253 - loglik: -5.6530e+02 - logprior: 1.4463
Epoch 6/10
19/19 - 9s - loss: 560.7122 - loglik: -5.6074e+02 - logprior: 1.5349
Epoch 7/10
19/19 - 9s - loss: 556.4503 - loglik: -5.5650e+02 - logprior: 1.6132
Epoch 8/10
19/19 - 9s - loss: 556.4783 - loglik: -5.5663e+02 - logprior: 1.7092
Fitted a model with MAP estimate = -553.3339
Time for alignment: 221.6960
Computed alignments with likelihoods: ['-544.9823', '-545.9364', '-553.3339']
Best model has likelihood: -544.9823
time for generating output: 0.3136
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7490605427974948
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac33007430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6bffe50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac225d9d90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2f73ce50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2f00bee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabb350d5e0>, <__main__.SimpleDirichletPrior object at 0x7faafc690be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3249 - loglik: -1.5106e+02 - logprior: -3.2323e+00
Epoch 2/10
19/19 - 1s - loss: 120.5984 - loglik: -1.1906e+02 - logprior: -1.5052e+00
Epoch 3/10
19/19 - 1s - loss: 106.2900 - loglik: -1.0444e+02 - logprior: -1.5761e+00
Epoch 4/10
19/19 - 1s - loss: 103.1656 - loglik: -1.0121e+02 - logprior: -1.6759e+00
Epoch 5/10
19/19 - 1s - loss: 102.3271 - loglik: -1.0057e+02 - logprior: -1.5402e+00
Epoch 6/10
19/19 - 1s - loss: 101.8758 - loglik: -1.0014e+02 - logprior: -1.5453e+00
Epoch 7/10
19/19 - 1s - loss: 101.3709 - loglik: -9.9670e+01 - logprior: -1.5318e+00
Epoch 8/10
19/19 - 1s - loss: 101.5651 - loglik: -9.9855e+01 - logprior: -1.5226e+00
Fitted a model with MAP estimate = -97.0685
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 106.7286 - loglik: -1.0249e+02 - logprior: -4.1576e+00
Epoch 2/2
19/19 - 1s - loss: 96.5854 - loglik: -9.4184e+01 - logprior: -2.2699e+00
Fitted a model with MAP estimate = -90.7218
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.8876 - loglik: -9.5840e+01 - logprior: -3.9382e+00
Epoch 2/2
19/19 - 1s - loss: 95.2664 - loglik: -9.3683e+01 - logprior: -1.4006e+00
Fitted a model with MAP estimate = -90.3218
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 94.7234 - loglik: -9.1012e+01 - logprior: -3.6469e+00
Epoch 2/10
21/21 - 2s - loss: 90.3467 - loglik: -8.8500e+01 - logprior: -1.7882e+00
Epoch 3/10
21/21 - 2s - loss: 89.2410 - loglik: -8.7731e+01 - logprior: -1.3610e+00
Epoch 4/10
21/21 - 2s - loss: 88.2900 - loglik: -8.6788e+01 - logprior: -1.2683e+00
Epoch 5/10
21/21 - 2s - loss: 88.0295 - loglik: -8.6524e+01 - logprior: -1.2541e+00
Epoch 6/10
21/21 - 2s - loss: 87.6542 - loglik: -8.6169e+01 - logprior: -1.2344e+00
Epoch 7/10
21/21 - 2s - loss: 87.5728 - loglik: -8.6096e+01 - logprior: -1.2277e+00
Epoch 8/10
21/21 - 2s - loss: 86.8419 - loglik: -8.5372e+01 - logprior: -1.2176e+00
Epoch 9/10
21/21 - 2s - loss: 87.4202 - loglik: -8.5968e+01 - logprior: -1.2016e+00
Fitted a model with MAP estimate = -86.7886
Time for alignment: 57.5962
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2844 - loglik: -1.5103e+02 - logprior: -3.2362e+00
Epoch 2/10
19/19 - 1s - loss: 121.8340 - loglik: -1.2030e+02 - logprior: -1.5012e+00
Epoch 3/10
19/19 - 1s - loss: 107.4289 - loglik: -1.0558e+02 - logprior: -1.5753e+00
Epoch 4/10
19/19 - 1s - loss: 103.8158 - loglik: -1.0188e+02 - logprior: -1.6796e+00
Epoch 5/10
19/19 - 1s - loss: 102.8086 - loglik: -1.0103e+02 - logprior: -1.5578e+00
Epoch 6/10
19/19 - 1s - loss: 102.5179 - loglik: -1.0078e+02 - logprior: -1.5658e+00
Epoch 7/10
19/19 - 1s - loss: 102.1367 - loglik: -1.0042e+02 - logprior: -1.5391e+00
Epoch 8/10
19/19 - 1s - loss: 102.3222 - loglik: -1.0063e+02 - logprior: -1.5277e+00
Fitted a model with MAP estimate = -97.7564
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (13, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.7213 - loglik: -1.0255e+02 - logprior: -4.1313e+00
Epoch 2/2
19/19 - 1s - loss: 96.6791 - loglik: -9.4310e+01 - logprior: -2.2682e+00
Fitted a model with MAP estimate = -90.8825
expansions: [(3, 1)]
discards: [ 0 37 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8894 - loglik: -9.5925e+01 - logprior: -3.9162e+00
Epoch 2/2
19/19 - 1s - loss: 95.4054 - loglik: -9.3825e+01 - logprior: -1.4633e+00
Fitted a model with MAP estimate = -90.4641
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.5976 - loglik: -9.0915e+01 - logprior: -3.6434e+00
Epoch 2/10
21/21 - 2s - loss: 90.3506 - loglik: -8.8419e+01 - logprior: -1.8536e+00
Epoch 3/10
21/21 - 2s - loss: 89.5711 - loglik: -8.8021e+01 - logprior: -1.3815e+00
Epoch 4/10
21/21 - 2s - loss: 88.2509 - loglik: -8.6735e+01 - logprior: -1.2723e+00
Epoch 5/10
21/21 - 2s - loss: 87.9170 - loglik: -8.6403e+01 - logprior: -1.2587e+00
Epoch 6/10
21/21 - 2s - loss: 87.7434 - loglik: -8.6245e+01 - logprior: -1.2478e+00
Epoch 7/10
21/21 - 2s - loss: 87.1632 - loglik: -8.5678e+01 - logprior: -1.2333e+00
Epoch 8/10
21/21 - 2s - loss: 87.3572 - loglik: -8.5882e+01 - logprior: -1.2289e+00
Fitted a model with MAP estimate = -86.9197
Time for alignment: 54.9775
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2980 - loglik: -1.5105e+02 - logprior: -3.2368e+00
Epoch 2/10
19/19 - 1s - loss: 122.4835 - loglik: -1.2094e+02 - logprior: -1.5009e+00
Epoch 3/10
19/19 - 1s - loss: 107.6711 - loglik: -1.0579e+02 - logprior: -1.5345e+00
Epoch 4/10
19/19 - 1s - loss: 104.0944 - loglik: -1.0206e+02 - logprior: -1.7076e+00
Epoch 5/10
19/19 - 1s - loss: 102.9910 - loglik: -1.0117e+02 - logprior: -1.5813e+00
Epoch 6/10
19/19 - 1s - loss: 102.6548 - loglik: -1.0087e+02 - logprior: -1.5819e+00
Epoch 7/10
19/19 - 1s - loss: 102.1768 - loglik: -1.0043e+02 - logprior: -1.5658e+00
Epoch 8/10
19/19 - 1s - loss: 102.4353 - loglik: -1.0070e+02 - logprior: -1.5611e+00
Fitted a model with MAP estimate = -97.7377
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 2), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.3675 - loglik: -1.0314e+02 - logprior: -4.1539e+00
Epoch 2/2
19/19 - 1s - loss: 96.7789 - loglik: -9.4341e+01 - logprior: -2.3026e+00
Fitted a model with MAP estimate = -90.9045
expansions: [(3, 1)]
discards: [ 0 11 12 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 101.8045 - loglik: -9.7838e+01 - logprior: -3.8910e+00
Epoch 2/2
19/19 - 1s - loss: 96.4048 - loglik: -9.4875e+01 - logprior: -1.4082e+00
Fitted a model with MAP estimate = -91.3679
expansions: [(3, 1), (11, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.8027 - loglik: -9.1267e+01 - logprior: -3.4572e+00
Epoch 2/10
21/21 - 2s - loss: 90.1698 - loglik: -8.8287e+01 - logprior: -1.7409e+00
Epoch 3/10
21/21 - 2s - loss: 89.2871 - loglik: -8.7764e+01 - logprior: -1.3050e+00
Epoch 4/10
21/21 - 2s - loss: 88.1427 - loglik: -8.6631e+01 - logprior: -1.2559e+00
Epoch 5/10
21/21 - 2s - loss: 88.1682 - loglik: -8.6650e+01 - logprior: -1.2495e+00
Fitted a model with MAP estimate = -87.4622
Time for alignment: 49.4535
Computed alignments with likelihoods: ['-86.7886', '-86.9197', '-87.4622']
Best model has likelihood: -86.7886
time for generating output: 0.1323
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9478428847392144
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac1131b0a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabde22e820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabaaad50d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2260b850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac556d3d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabc4c78df0>, <__main__.SimpleDirichletPrior object at 0x7fac000df700>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 417.6660 - loglik: -4.1271e+02 - logprior: -4.6879e+00
Epoch 2/10
27/27 - 5s - loss: 331.1582 - loglik: -3.2844e+02 - logprior: -2.0480e+00
Epoch 3/10
27/27 - 5s - loss: 315.8073 - loglik: -3.1322e+02 - logprior: -2.0425e+00
Epoch 4/10
27/27 - 5s - loss: 312.3600 - loglik: -3.0989e+02 - logprior: -2.0017e+00
Epoch 5/10
27/27 - 5s - loss: 310.7701 - loglik: -3.0833e+02 - logprior: -1.9855e+00
Epoch 6/10
27/27 - 5s - loss: 310.3633 - loglik: -3.0788e+02 - logprior: -2.0043e+00
Epoch 7/10
27/27 - 6s - loss: 309.1494 - loglik: -3.0669e+02 - logprior: -1.9935e+00
Epoch 8/10
27/27 - 5s - loss: 309.1398 - loglik: -3.0668e+02 - logprior: -1.9853e+00
Epoch 9/10
27/27 - 5s - loss: 308.9207 - loglik: -3.0644e+02 - logprior: -1.9877e+00
Epoch 10/10
27/27 - 5s - loss: 308.7050 - loglik: -3.0623e+02 - logprior: -1.9764e+00
Fitted a model with MAP estimate = -307.9771
expansions: [(0, 2), (10, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (69, 2), (71, 2), (72, 1), (73, 1), (78, 1), (81, 1), (85, 1), (93, 2), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 307.1205 - loglik: -3.0037e+02 - logprior: -6.5264e+00
Epoch 2/2
27/27 - 6s - loss: 286.6021 - loglik: -2.8469e+02 - logprior: -1.3533e+00
Fitted a model with MAP estimate = -283.8457
expansions: []
discards: [  0  13  30  46  47  48  67  95 138]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 297.1339 - loglik: -2.9052e+02 - logprior: -6.2528e+00
Epoch 2/2
27/27 - 6s - loss: 289.3739 - loglik: -2.8764e+02 - logprior: -1.2035e+00
Fitted a model with MAP estimate = -286.7821
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 10s - loss: 293.8124 - loglik: -2.8955e+02 - logprior: -4.0728e+00
Epoch 2/10
27/27 - 5s - loss: 287.0824 - loglik: -2.8590e+02 - logprior: -7.3679e-01
Epoch 3/10
27/27 - 6s - loss: 286.3518 - loglik: -2.8541e+02 - logprior: -4.2919e-01
Epoch 4/10
27/27 - 6s - loss: 285.6346 - loglik: -2.8479e+02 - logprior: -3.1801e-01
Epoch 5/10
27/27 - 6s - loss: 285.1358 - loglik: -2.8439e+02 - logprior: -2.2037e-01
Epoch 6/10
27/27 - 6s - loss: 284.6777 - loglik: -2.8407e+02 - logprior: -8.7508e-02
Epoch 7/10
27/27 - 6s - loss: 283.8781 - loglik: -2.8338e+02 - logprior: 0.0050
Epoch 8/10
27/27 - 6s - loss: 284.4111 - loglik: -2.8401e+02 - logprior: 0.1133
Fitted a model with MAP estimate = -283.3011
Time for alignment: 165.6797
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 418.0056 - loglik: -4.1303e+02 - logprior: -4.7124e+00
Epoch 2/10
27/27 - 5s - loss: 330.7321 - loglik: -3.2800e+02 - logprior: -2.0345e+00
Epoch 3/10
27/27 - 5s - loss: 315.0934 - loglik: -3.1243e+02 - logprior: -2.0615e+00
Epoch 4/10
27/27 - 5s - loss: 311.7576 - loglik: -3.0928e+02 - logprior: -2.0054e+00
Epoch 5/10
27/27 - 5s - loss: 310.3589 - loglik: -3.0789e+02 - logprior: -2.0142e+00
Epoch 6/10
27/27 - 5s - loss: 310.5321 - loglik: -3.0812e+02 - logprior: -1.9845e+00
Fitted a model with MAP estimate = -309.2921
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (33, 1), (36, 1), (37, 4), (39, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (85, 1), (93, 2), (102, 1), (103, 2), (104, 2), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 308.8912 - loglik: -3.0240e+02 - logprior: -6.3981e+00
Epoch 2/2
27/27 - 6s - loss: 286.4763 - loglik: -2.8467e+02 - logprior: -1.5497e+00
Fitted a model with MAP estimate = -284.4874
expansions: [(43, 1)]
discards: [  0  12  30  48  66  88 110 137 139]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 297.9928 - loglik: -2.9156e+02 - logprior: -6.3071e+00
Epoch 2/2
27/27 - 6s - loss: 288.6544 - loglik: -2.8703e+02 - logprior: -1.2689e+00
Fitted a model with MAP estimate = -286.0402
expansions: [(87, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 293.0280 - loglik: -2.8875e+02 - logprior: -4.1391e+00
Epoch 2/10
27/27 - 6s - loss: 286.2505 - loglik: -2.8509e+02 - logprior: -8.2253e-01
Epoch 3/10
27/27 - 6s - loss: 285.5676 - loglik: -2.8452e+02 - logprior: -5.7040e-01
Epoch 4/10
27/27 - 6s - loss: 284.0768 - loglik: -2.8314e+02 - logprior: -4.5949e-01
Epoch 5/10
27/27 - 6s - loss: 283.4539 - loglik: -2.8264e+02 - logprior: -3.4213e-01
Epoch 6/10
27/27 - 6s - loss: 282.7344 - loglik: -2.8204e+02 - logprior: -2.2815e-01
Epoch 7/10
27/27 - 6s - loss: 283.7150 - loglik: -2.8312e+02 - logprior: -1.4473e-01
Fitted a model with MAP estimate = -282.3403
Time for alignment: 136.7281
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.6198 - loglik: -4.1268e+02 - logprior: -4.7031e+00
Epoch 2/10
27/27 - 6s - loss: 330.7801 - loglik: -3.2810e+02 - logprior: -2.0521e+00
Epoch 3/10
27/27 - 5s - loss: 316.1640 - loglik: -3.1354e+02 - logprior: -2.1165e+00
Epoch 4/10
27/27 - 5s - loss: 313.2006 - loglik: -3.1068e+02 - logprior: -2.0873e+00
Epoch 5/10
27/27 - 5s - loss: 312.3051 - loglik: -3.0985e+02 - logprior: -2.0407e+00
Epoch 6/10
27/27 - 5s - loss: 311.2967 - loglik: -3.0886e+02 - logprior: -2.0201e+00
Epoch 7/10
27/27 - 6s - loss: 311.4526 - loglik: -3.0903e+02 - logprior: -1.9978e+00
Fitted a model with MAP estimate = -310.2105
expansions: [(0, 2), (9, 1), (18, 2), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (69, 2), (71, 2), (72, 1), (73, 1), (78, 1), (82, 2), (94, 2), (99, 1), (102, 1), (103, 2), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 163 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 308.4561 - loglik: -3.0181e+02 - logprior: -6.4350e+00
Epoch 2/2
27/27 - 6s - loss: 289.1120 - loglik: -2.8716e+02 - logprior: -1.4843e+00
Fitted a model with MAP estimate = -286.5447
expansions: []
discards: [  0  21  30  48  49  50  67  90  95 111 138 140]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 300.3792 - loglik: -2.9394e+02 - logprior: -6.2997e+00
Epoch 2/2
27/27 - 6s - loss: 291.3379 - loglik: -2.8978e+02 - logprior: -1.2439e+00
Fitted a model with MAP estimate = -289.0230
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 294.7869 - loglik: -2.9045e+02 - logprior: -4.0914e+00
Epoch 2/10
27/27 - 6s - loss: 290.3970 - loglik: -2.8923e+02 - logprior: -7.4532e-01
Epoch 3/10
27/27 - 6s - loss: 288.1518 - loglik: -2.8716e+02 - logprior: -4.7811e-01
Epoch 4/10
27/27 - 6s - loss: 288.6891 - loglik: -2.8781e+02 - logprior: -3.6246e-01
Fitted a model with MAP estimate = -286.7960
Time for alignment: 124.3822
Computed alignments with likelihoods: ['-283.3011', '-282.3403', '-286.5447']
Best model has likelihood: -282.3403
time for generating output: 0.3325
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5664451827242525
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac11065be0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf7b3fa00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fab1ea2faf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab1ea2f850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4899700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabef450310>, <__main__.SimpleDirichletPrior object at 0x7fac446ad0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 21s - loss: 766.2729 - loglik: -7.6374e+02 - logprior: -2.1599e+00
Epoch 2/10
34/34 - 17s - loss: 634.6304 - loglik: -6.3167e+02 - logprior: -1.8913e+00
Epoch 3/10
34/34 - 16s - loss: 617.9799 - loglik: -6.1489e+02 - logprior: -2.0276e+00
Epoch 4/10
34/34 - 17s - loss: 613.8850 - loglik: -6.1100e+02 - logprior: -1.9290e+00
Epoch 5/10
34/34 - 17s - loss: 612.3930 - loglik: -6.0955e+02 - logprior: -1.9670e+00
Epoch 6/10
34/34 - 16s - loss: 611.6267 - loglik: -6.0884e+02 - logprior: -1.9879e+00
Epoch 7/10
34/34 - 16s - loss: 609.4351 - loglik: -6.0666e+02 - logprior: -2.0293e+00
Epoch 8/10
34/34 - 16s - loss: 608.5553 - loglik: -6.0579e+02 - logprior: -2.0458e+00
Epoch 9/10
34/34 - 16s - loss: 609.1181 - loglik: -6.0635e+02 - logprior: -2.0410e+00
Fitted a model with MAP estimate = -607.5399
expansions: [(13, 1), (14, 2), (15, 3), (16, 5), (17, 2), (29, 3), (51, 3), (52, 1), (57, 1), (60, 1), (64, 1), (66, 1), (71, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (165, 1), (168, 1), (172, 2), (173, 2), (175, 1), (178, 1), (184, 1), (185, 1), (186, 1), (187, 2), (190, 1), (191, 1), (200, 1), (202, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 594.3931 - loglik: -5.9201e+02 - logprior: -2.1939e+00
Epoch 2/2
34/34 - 23s - loss: 569.7706 - loglik: -5.6859e+02 - logprior: -6.1270e-01
Fitted a model with MAP estimate = -566.4320
expansions: [(73, 4), (184, 1)]
discards: [18 27 43]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 578.5162 - loglik: -5.7639e+02 - logprior: -1.9301e+00
Epoch 2/2
34/34 - 23s - loss: 565.9868 - loglik: -5.6488e+02 - logprior: -4.0195e-01
Fitted a model with MAP estimate = -563.2753
expansions: []
discards: [ 71  72  73 188]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 27s - loss: 576.6426 - loglik: -5.7461e+02 - logprior: -1.7294e+00
Epoch 2/10
34/34 - 22s - loss: 568.6857 - loglik: -5.6773e+02 - logprior: -6.0532e-02
Epoch 3/10
34/34 - 23s - loss: 564.2806 - loglik: -5.6329e+02 - logprior: 0.0702
Epoch 4/10
34/34 - 22s - loss: 564.4718 - loglik: -5.6368e+02 - logprior: 0.1674
Fitted a model with MAP estimate = -562.7482
Time for alignment: 452.0221
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 765.6165 - loglik: -7.6295e+02 - logprior: -2.1418e+00
Epoch 2/10
34/34 - 16s - loss: 636.4081 - loglik: -6.3326e+02 - logprior: -1.8743e+00
Epoch 3/10
34/34 - 16s - loss: 616.1990 - loglik: -6.1290e+02 - logprior: -2.0524e+00
Epoch 4/10
34/34 - 17s - loss: 616.2736 - loglik: -6.1330e+02 - logprior: -1.9610e+00
Fitted a model with MAP estimate = -612.2994
expansions: [(12, 1), (13, 1), (14, 1), (15, 3), (16, 4), (17, 2), (29, 3), (41, 1), (51, 2), (55, 4), (57, 1), (61, 1), (64, 2), (66, 1), (92, 1), (94, 1), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (163, 1), (164, 1), (167, 1), (171, 1), (172, 1), (173, 1), (175, 1), (178, 1), (186, 3), (189, 1), (193, 1), (199, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 602.2144 - loglik: -5.9854e+02 - logprior: -3.4710e+00
Epoch 2/2
34/34 - 23s - loss: 574.5344 - loglik: -5.7246e+02 - logprior: -1.5371e+00
Fitted a model with MAP estimate = -568.5760
expansions: [(27, 1)]
discards: [ 18  23  42  72  73  74  75  88 124 126 224]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 588.1917 - loglik: -5.8609e+02 - logprior: -2.0318e+00
Epoch 2/2
34/34 - 22s - loss: 576.9250 - loglik: -5.7619e+02 - logprior: -4.1142e-01
Fitted a model with MAP estimate = -574.4175
expansions: [(70, 4), (179, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 26s - loss: 581.8008 - loglik: -5.7983e+02 - logprior: -1.8328e+00
Epoch 2/10
34/34 - 22s - loss: 572.9468 - loglik: -5.7232e+02 - logprior: -1.5088e-01
Epoch 3/10
34/34 - 22s - loss: 569.4695 - loglik: -5.6880e+02 - logprior: -6.4622e-03
Epoch 4/10
34/34 - 22s - loss: 568.3910 - loglik: -5.6774e+02 - logprior: 0.0990
Epoch 5/10
34/34 - 23s - loss: 568.8907 - loglik: -5.6839e+02 - logprior: 0.2548
Fitted a model with MAP estimate = -566.4914
Time for alignment: 385.7506
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 767.6675 - loglik: -7.6519e+02 - logprior: -2.1482e+00
Epoch 2/10
34/34 - 16s - loss: 637.9233 - loglik: -6.3506e+02 - logprior: -1.8006e+00
Epoch 3/10
34/34 - 16s - loss: 620.3500 - loglik: -6.1730e+02 - logprior: -1.9274e+00
Epoch 4/10
34/34 - 17s - loss: 616.4890 - loglik: -6.1369e+02 - logprior: -1.8620e+00
Epoch 5/10
34/34 - 17s - loss: 614.7017 - loglik: -6.1204e+02 - logprior: -1.8642e+00
Epoch 6/10
34/34 - 16s - loss: 614.1519 - loglik: -6.1154e+02 - logprior: -1.8735e+00
Epoch 7/10
34/34 - 16s - loss: 613.8408 - loglik: -6.1126e+02 - logprior: -1.8719e+00
Epoch 8/10
34/34 - 17s - loss: 612.3862 - loglik: -6.0978e+02 - logprior: -1.9060e+00
Epoch 9/10
34/34 - 16s - loss: 611.6646 - loglik: -6.0906e+02 - logprior: -1.9085e+00
Epoch 10/10
34/34 - 17s - loss: 610.1296 - loglik: -6.0754e+02 - logprior: -1.9083e+00
Fitted a model with MAP estimate = -610.0578
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 3), (49, 2), (51, 1), (52, 2), (55, 3), (57, 1), (65, 1), (66, 2), (90, 1), (94, 1), (95, 2), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (110, 1), (129, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (172, 3), (173, 2), (175, 1), (178, 1), (185, 3), (186, 2), (189, 1), (190, 1), (202, 1), (203, 1), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 27s - loss: 600.1700 - loglik: -5.9618e+02 - logprior: -3.4293e+00
Epoch 2/2
34/34 - 23s - loss: 573.4224 - loglik: -5.7056e+02 - logprior: -1.6890e+00
Fitted a model with MAP estimate = -564.8381
expansions: [(75, 1), (76, 1), (189, 1), (241, 1)]
discards: [ 19  42  63  69  79 124 126 262 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 578.7817 - loglik: -5.7640e+02 - logprior: -2.0024e+00
Epoch 2/2
34/34 - 23s - loss: 568.8797 - loglik: -5.6775e+02 - logprior: -3.4619e-01
Fitted a model with MAP estimate = -566.0780
expansions: []
discards: [87]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 26s - loss: 575.6786 - loglik: -5.7360e+02 - logprior: -1.8215e+00
Epoch 2/10
34/34 - 22s - loss: 570.3719 - loglik: -5.6978e+02 - logprior: 0.0154
Epoch 3/10
34/34 - 22s - loss: 566.7543 - loglik: -5.6616e+02 - logprior: 0.1758
Epoch 4/10
34/34 - 23s - loss: 566.5097 - loglik: -5.6598e+02 - logprior: 0.3122
Epoch 5/10
34/34 - 22s - loss: 565.0630 - loglik: -5.6466e+02 - logprior: 0.4342
Epoch 6/10
34/34 - 22s - loss: 564.0151 - loglik: -5.6387e+02 - logprior: 0.6445
Epoch 7/10
34/34 - 23s - loss: 564.3452 - loglik: -5.6425e+02 - logprior: 0.6726
Fitted a model with MAP estimate = -562.9385
Time for alignment: 530.7953
Computed alignments with likelihoods: ['-562.7482', '-566.4914', '-562.9385']
Best model has likelihood: -562.7482
time for generating output: 0.3980
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.688641975308642
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab2f2218b0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2f53ae20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac2ac2cee0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafcbd7eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac33474dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabb3cab370>, <__main__.SimpleDirichletPrior object at 0x7fab1eaca340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.3768 - loglik: -1.8301e+02 - logprior: -8.8301e+01
Epoch 2/10
10/10 - 1s - loss: 184.1557 - loglik: -1.6031e+02 - logprior: -2.3796e+01
Epoch 3/10
10/10 - 1s - loss: 153.1915 - loglik: -1.4188e+02 - logprior: -1.1289e+01
Epoch 4/10
10/10 - 1s - loss: 138.5841 - loglik: -1.3178e+02 - logprior: -6.7312e+00
Epoch 5/10
10/10 - 1s - loss: 130.7121 - loglik: -1.2606e+02 - logprior: -4.4453e+00
Epoch 6/10
10/10 - 1s - loss: 127.0537 - loglik: -1.2358e+02 - logprior: -3.1471e+00
Epoch 7/10
10/10 - 1s - loss: 125.5009 - loglik: -1.2293e+02 - logprior: -2.2323e+00
Epoch 8/10
10/10 - 1s - loss: 124.6477 - loglik: -1.2268e+02 - logprior: -1.6566e+00
Epoch 9/10
10/10 - 1s - loss: 124.1161 - loglik: -1.2250e+02 - logprior: -1.2963e+00
Epoch 10/10
10/10 - 1s - loss: 123.7119 - loglik: -1.2228e+02 - logprior: -1.0570e+00
Fitted a model with MAP estimate = -123.1407
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.9467 - loglik: -1.2309e+02 - logprior: -9.8813e+01
Epoch 2/2
10/10 - 1s - loss: 156.5057 - loglik: -1.1596e+02 - logprior: -4.0376e+01
Fitted a model with MAP estimate = -144.4585
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 191.2163 - loglik: -1.1222e+02 - logprior: -7.8842e+01
Epoch 2/2
10/10 - 1s - loss: 130.4064 - loglik: -1.0935e+02 - logprior: -2.0841e+01
Fitted a model with MAP estimate = -120.8698
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.7672 - loglik: -1.0935e+02 - logprior: -7.7332e+01
Epoch 2/10
10/10 - 1s - loss: 128.2473 - loglik: -1.0796e+02 - logprior: -2.0094e+01
Epoch 3/10
10/10 - 1s - loss: 115.6964 - loglik: -1.0713e+02 - logprior: -8.2764e+00
Epoch 4/10
10/10 - 1s - loss: 110.8103 - loglik: -1.0718e+02 - logprior: -3.2475e+00
Epoch 5/10
10/10 - 1s - loss: 108.3316 - loglik: -1.0748e+02 - logprior: -4.6468e-01
Epoch 6/10
10/10 - 1s - loss: 106.9670 - loglik: -1.0773e+02 - logprior: 1.1488
Epoch 7/10
10/10 - 1s - loss: 106.1614 - loglik: -1.0786e+02 - logprior: 2.1229
Epoch 8/10
10/10 - 1s - loss: 105.6219 - loglik: -1.0795e+02 - logprior: 2.7714
Epoch 9/10
10/10 - 1s - loss: 105.2087 - loglik: -1.0802e+02 - logprior: 3.2576
Epoch 10/10
10/10 - 1s - loss: 104.8732 - loglik: -1.0804e+02 - logprior: 3.6286
Fitted a model with MAP estimate = -104.2345
Time for alignment: 34.4122
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.6231 - loglik: -1.8300e+02 - logprior: -8.8302e+01
Epoch 2/10
10/10 - 1s - loss: 184.0822 - loglik: -1.6012e+02 - logprior: -2.3798e+01
Epoch 3/10
10/10 - 1s - loss: 152.9151 - loglik: -1.4156e+02 - logprior: -1.1286e+01
Epoch 4/10
10/10 - 1s - loss: 137.7207 - loglik: -1.3088e+02 - logprior: -6.7574e+00
Epoch 5/10
10/10 - 1s - loss: 130.0297 - loglik: -1.2533e+02 - logprior: -4.4765e+00
Epoch 6/10
10/10 - 1s - loss: 126.5037 - loglik: -1.2300e+02 - logprior: -3.1351e+00
Epoch 7/10
10/10 - 1s - loss: 124.9655 - loglik: -1.2234e+02 - logprior: -2.2548e+00
Epoch 8/10
10/10 - 1s - loss: 124.0935 - loglik: -1.2206e+02 - logprior: -1.6992e+00
Epoch 9/10
10/10 - 1s - loss: 123.5155 - loglik: -1.2180e+02 - logprior: -1.3555e+00
Epoch 10/10
10/10 - 1s - loss: 123.1586 - loglik: -1.2165e+02 - logprior: -1.1036e+00
Fitted a model with MAP estimate = -122.5894
expansions: [(7, 2), (8, 4), (12, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.3507 - loglik: -1.2350e+02 - logprior: -9.8813e+01
Epoch 2/2
10/10 - 1s - loss: 156.8806 - loglik: -1.1650e+02 - logprior: -4.0341e+01
Fitted a model with MAP estimate = -144.9129
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.1271 - loglik: -1.1314e+02 - logprior: -7.8832e+01
Epoch 2/2
10/10 - 1s - loss: 131.0530 - loglik: -1.1019e+02 - logprior: -2.0805e+01
Fitted a model with MAP estimate = -121.9393
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.9452 - loglik: -1.0961e+02 - logprior: -7.7303e+01
Epoch 2/10
10/10 - 1s - loss: 128.6177 - loglik: -1.0855e+02 - logprior: -2.0050e+01
Epoch 3/10
10/10 - 1s - loss: 116.0737 - loglik: -1.0775e+02 - logprior: -8.1975e+00
Epoch 4/10
10/10 - 1s - loss: 111.0217 - loglik: -1.0752e+02 - logprior: -3.2008e+00
Epoch 5/10
10/10 - 1s - loss: 108.4322 - loglik: -1.0763e+02 - logprior: -4.4288e-01
Epoch 6/10
10/10 - 1s - loss: 107.0322 - loglik: -1.0784e+02 - logprior: 1.1784
Epoch 7/10
10/10 - 1s - loss: 106.2102 - loglik: -1.0797e+02 - logprior: 2.1582
Epoch 8/10
10/10 - 1s - loss: 105.6685 - loglik: -1.0807e+02 - logprior: 2.8064
Epoch 9/10
10/10 - 1s - loss: 105.2671 - loglik: -1.0815e+02 - logprior: 3.2916
Epoch 10/10
10/10 - 1s - loss: 104.9375 - loglik: -1.0818e+02 - logprior: 3.6720
Fitted a model with MAP estimate = -104.3427
Time for alignment: 34.3759
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.4069 - loglik: -1.8303e+02 - logprior: -8.8301e+01
Epoch 2/10
10/10 - 1s - loss: 183.9006 - loglik: -1.6007e+02 - logprior: -2.3791e+01
Epoch 3/10
10/10 - 1s - loss: 153.0079 - loglik: -1.4171e+02 - logprior: -1.1275e+01
Epoch 4/10
10/10 - 1s - loss: 138.6010 - loglik: -1.3186e+02 - logprior: -6.6726e+00
Epoch 5/10
10/10 - 1s - loss: 130.4264 - loglik: -1.2584e+02 - logprior: -4.3390e+00
Epoch 6/10
10/10 - 1s - loss: 126.6634 - loglik: -1.2321e+02 - logprior: -3.0906e+00
Epoch 7/10
10/10 - 1s - loss: 125.0512 - loglik: -1.2255e+02 - logprior: -2.1969e+00
Epoch 8/10
10/10 - 1s - loss: 124.1467 - loglik: -1.2215e+02 - logprior: -1.6692e+00
Epoch 9/10
10/10 - 1s - loss: 123.5555 - loglik: -1.2182e+02 - logprior: -1.3330e+00
Epoch 10/10
10/10 - 1s - loss: 123.1691 - loglik: -1.2166e+02 - logprior: -1.0827e+00
Fitted a model with MAP estimate = -122.5799
expansions: [(7, 2), (8, 4), (12, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.7708 - loglik: -1.2289e+02 - logprior: -9.8787e+01
Epoch 2/2
10/10 - 1s - loss: 156.3795 - loglik: -1.1587e+02 - logprior: -4.0350e+01
Fitted a model with MAP estimate = -144.1377
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.2916 - loglik: -1.1244e+02 - logprior: -7.8820e+01
Epoch 2/2
10/10 - 1s - loss: 130.3627 - loglik: -1.0937e+02 - logprior: -2.0840e+01
Fitted a model with MAP estimate = -120.8362
expansions: [(11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 186.6765 - loglik: -1.0923e+02 - logprior: -7.7344e+01
Epoch 2/10
10/10 - 1s - loss: 128.1531 - loglik: -1.0783e+02 - logprior: -2.0139e+01
Epoch 3/10
10/10 - 1s - loss: 115.6189 - loglik: -1.0699e+02 - logprior: -8.3063e+00
Epoch 4/10
10/10 - 1s - loss: 110.7611 - loglik: -1.0710e+02 - logprior: -3.2765e+00
Epoch 5/10
10/10 - 1s - loss: 108.2957 - loglik: -1.0743e+02 - logprior: -4.8985e-01
Epoch 6/10
10/10 - 1s - loss: 106.9327 - loglik: -1.0768e+02 - logprior: 1.1303
Epoch 7/10
10/10 - 1s - loss: 106.1214 - loglik: -1.0780e+02 - logprior: 2.0976
Epoch 8/10
10/10 - 1s - loss: 105.5935 - loglik: -1.0790e+02 - logprior: 2.7446
Epoch 9/10
10/10 - 1s - loss: 105.2008 - loglik: -1.0799e+02 - logprior: 3.2354
Epoch 10/10
10/10 - 1s - loss: 104.8676 - loglik: -1.0803e+02 - logprior: 3.6083
Fitted a model with MAP estimate = -104.2347
Time for alignment: 35.3289
Computed alignments with likelihoods: ['-104.2345', '-104.3427', '-104.2347']
Best model has likelihood: -104.2345
time for generating output: 0.1297
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.9205930507549572
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaeb9a6f40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5db64cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabf7bfbf10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabb326f2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafc528610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faafcbd7eb0>, <__main__.SimpleDirichletPrior object at 0x7fac44291790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.9878 - loglik: -3.0923e+02 - logprior: -7.7119e+00
Epoch 2/10
13/13 - 2s - loss: 284.5370 - loglik: -2.8255e+02 - logprior: -1.8948e+00
Epoch 3/10
13/13 - 2s - loss: 261.3088 - loglik: -2.5934e+02 - logprior: -1.7001e+00
Epoch 4/10
13/13 - 2s - loss: 250.6040 - loglik: -2.4810e+02 - logprior: -1.9593e+00
Epoch 5/10
13/13 - 2s - loss: 247.8800 - loglik: -2.4535e+02 - logprior: -1.8972e+00
Epoch 6/10
13/13 - 2s - loss: 246.5875 - loglik: -2.4424e+02 - logprior: -1.7925e+00
Epoch 7/10
13/13 - 2s - loss: 246.3142 - loglik: -2.4394e+02 - logprior: -1.8015e+00
Epoch 8/10
13/13 - 2s - loss: 245.1203 - loglik: -2.4276e+02 - logprior: -1.7909e+00
Epoch 9/10
13/13 - 2s - loss: 244.9350 - loglik: -2.4260e+02 - logprior: -1.7666e+00
Epoch 10/10
13/13 - 2s - loss: 244.9105 - loglik: -2.4259e+02 - logprior: -1.7669e+00
Fitted a model with MAP estimate = -244.0972
expansions: [(9, 1), (10, 1), (13, 1), (20, 1), (21, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 266.1725 - loglik: -2.5704e+02 - logprior: -9.0684e+00
Epoch 2/2
13/13 - 2s - loss: 249.3958 - loglik: -2.4514e+02 - logprior: -4.0594e+00
Fitted a model with MAP estimate = -246.2030
expansions: [(0, 2)]
discards: [ 0 65 87 94]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 250.2108 - loglik: -2.4328e+02 - logprior: -6.8655e+00
Epoch 2/2
13/13 - 2s - loss: 243.1816 - loglik: -2.4108e+02 - logprior: -1.8564e+00
Fitted a model with MAP estimate = -241.6123
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 253.1497 - loglik: -2.4453e+02 - logprior: -8.5860e+00
Epoch 2/10
13/13 - 2s - loss: 245.3076 - loglik: -2.4258e+02 - logprior: -2.6437e+00
Epoch 3/10
13/13 - 2s - loss: 242.8750 - loglik: -2.4128e+02 - logprior: -1.2946e+00
Epoch 4/10
13/13 - 2s - loss: 241.5333 - loglik: -2.3996e+02 - logprior: -1.0278e+00
Epoch 5/10
13/13 - 2s - loss: 240.3256 - loglik: -2.3882e+02 - logprior: -9.0124e-01
Epoch 6/10
13/13 - 2s - loss: 239.6937 - loglik: -2.3821e+02 - logprior: -8.7030e-01
Epoch 7/10
13/13 - 2s - loss: 239.3618 - loglik: -2.3789e+02 - logprior: -8.5643e-01
Epoch 8/10
13/13 - 2s - loss: 238.6027 - loglik: -2.3712e+02 - logprior: -8.5042e-01
Epoch 9/10
13/13 - 2s - loss: 237.6274 - loglik: -2.3615e+02 - logprior: -8.2918e-01
Epoch 10/10
13/13 - 2s - loss: 237.5967 - loglik: -2.3609e+02 - logprior: -8.2412e-01
Fitted a model with MAP estimate = -236.1596
Time for alignment: 70.0246
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.4504 - loglik: -3.0858e+02 - logprior: -7.7040e+00
Epoch 2/10
13/13 - 2s - loss: 285.3260 - loglik: -2.8320e+02 - logprior: -1.8790e+00
Epoch 3/10
13/13 - 2s - loss: 261.6752 - loglik: -2.5956e+02 - logprior: -1.6849e+00
Epoch 4/10
13/13 - 2s - loss: 250.8836 - loglik: -2.4826e+02 - logprior: -1.9794e+00
Epoch 5/10
13/13 - 2s - loss: 247.8640 - loglik: -2.4540e+02 - logprior: -1.8850e+00
Epoch 6/10
13/13 - 2s - loss: 247.1684 - loglik: -2.4484e+02 - logprior: -1.7908e+00
Epoch 7/10
13/13 - 2s - loss: 245.8562 - loglik: -2.4354e+02 - logprior: -1.7805e+00
Epoch 8/10
13/13 - 2s - loss: 245.5135 - loglik: -2.4319e+02 - logprior: -1.7825e+00
Epoch 9/10
13/13 - 2s - loss: 245.1693 - loglik: -2.4289e+02 - logprior: -1.7622e+00
Epoch 10/10
13/13 - 2s - loss: 244.4544 - loglik: -2.4221e+02 - logprior: -1.7521e+00
Fitted a model with MAP estimate = -244.1392
expansions: [(9, 1), (10, 1), (13, 1), (20, 1), (28, 1), (29, 1), (30, 2), (31, 3), (32, 2), (40, 1), (41, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 264.5475 - loglik: -2.5543e+02 - logprior: -9.0693e+00
Epoch 2/2
13/13 - 2s - loss: 248.6239 - loglik: -2.4436e+02 - logprior: -4.0240e+00
Fitted a model with MAP estimate = -245.6152
expansions: [(0, 2)]
discards: [ 0 67 95]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 249.2090 - loglik: -2.4197e+02 - logprior: -6.8575e+00
Epoch 2/2
13/13 - 2s - loss: 242.4210 - loglik: -2.3999e+02 - logprior: -1.8678e+00
Fitted a model with MAP estimate = -240.4221
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.6158 - loglik: -2.4389e+02 - logprior: -8.5817e+00
Epoch 2/10
13/13 - 2s - loss: 244.8688 - loglik: -2.4205e+02 - logprior: -2.6644e+00
Epoch 3/10
13/13 - 2s - loss: 242.0121 - loglik: -2.4044e+02 - logprior: -1.2944e+00
Epoch 4/10
13/13 - 2s - loss: 241.1222 - loglik: -2.3966e+02 - logprior: -1.0301e+00
Epoch 5/10
13/13 - 2s - loss: 240.6938 - loglik: -2.3927e+02 - logprior: -8.9279e-01
Epoch 6/10
13/13 - 2s - loss: 239.3513 - loglik: -2.3791e+02 - logprior: -8.5932e-01
Epoch 7/10
13/13 - 2s - loss: 239.0763 - loglik: -2.3764e+02 - logprior: -8.3075e-01
Epoch 8/10
13/13 - 2s - loss: 238.4665 - loglik: -2.3701e+02 - logprior: -8.2843e-01
Epoch 9/10
13/13 - 2s - loss: 237.8161 - loglik: -2.3638e+02 - logprior: -7.9205e-01
Epoch 10/10
13/13 - 2s - loss: 237.7322 - loglik: -2.3628e+02 - logprior: -7.7937e-01
Fitted a model with MAP estimate = -236.2293
Time for alignment: 69.9723
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 316.4917 - loglik: -3.0858e+02 - logprior: -7.7041e+00
Epoch 2/10
13/13 - 2s - loss: 285.2955 - loglik: -2.8318e+02 - logprior: -1.8806e+00
Epoch 3/10
13/13 - 2s - loss: 261.0851 - loglik: -2.5904e+02 - logprior: -1.6830e+00
Epoch 4/10
13/13 - 2s - loss: 249.9715 - loglik: -2.4734e+02 - logprior: -1.9534e+00
Epoch 5/10
13/13 - 2s - loss: 247.9035 - loglik: -2.4534e+02 - logprior: -1.8974e+00
Epoch 6/10
13/13 - 2s - loss: 246.7516 - loglik: -2.4436e+02 - logprior: -1.8004e+00
Epoch 7/10
13/13 - 2s - loss: 245.6588 - loglik: -2.4329e+02 - logprior: -1.7962e+00
Epoch 8/10
13/13 - 2s - loss: 245.8209 - loglik: -2.4345e+02 - logprior: -1.7915e+00
Fitted a model with MAP estimate = -244.6409
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 2), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 265.2788 - loglik: -2.5605e+02 - logprior: -9.0682e+00
Epoch 2/2
13/13 - 2s - loss: 249.2040 - loglik: -2.4500e+02 - logprior: -4.0981e+00
Fitted a model with MAP estimate = -246.7319
expansions: [(0, 2)]
discards: [ 0 14 34 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.4068 - loglik: -2.4351e+02 - logprior: -6.8756e+00
Epoch 2/2
13/13 - 2s - loss: 243.6477 - loglik: -2.4168e+02 - logprior: -1.8599e+00
Fitted a model with MAP estimate = -241.9813
expansions: []
discards: [ 0 37 88]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.5529 - loglik: -2.4390e+02 - logprior: -8.6025e+00
Epoch 2/10
13/13 - 2s - loss: 244.7621 - loglik: -2.4193e+02 - logprior: -2.6835e+00
Epoch 3/10
13/13 - 2s - loss: 242.3824 - loglik: -2.4078e+02 - logprior: -1.3063e+00
Epoch 4/10
13/13 - 2s - loss: 241.5066 - loglik: -2.3999e+02 - logprior: -1.0365e+00
Epoch 5/10
13/13 - 2s - loss: 239.9741 - loglik: -2.3848e+02 - logprior: -9.1226e-01
Epoch 6/10
13/13 - 2s - loss: 239.6444 - loglik: -2.3814e+02 - logprior: -8.9351e-01
Epoch 7/10
13/13 - 2s - loss: 238.9836 - loglik: -2.3748e+02 - logprior: -8.6478e-01
Epoch 8/10
13/13 - 2s - loss: 238.6805 - loglik: -2.3717e+02 - logprior: -8.4911e-01
Epoch 9/10
13/13 - 2s - loss: 237.8522 - loglik: -2.3634e+02 - logprior: -8.3444e-01
Epoch 10/10
13/13 - 2s - loss: 237.3772 - loglik: -2.3586e+02 - logprior: -8.1564e-01
Fitted a model with MAP estimate = -236.1548
Time for alignment: 66.6322
Computed alignments with likelihoods: ['-236.1596', '-236.2293', '-236.1548']
Best model has likelihood: -236.1548
time for generating output: 0.1863
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9201653944020356
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaf3ea6eb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaf3d704f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac5dce5eb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaebc3f670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaebc3f730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaebc3f310>, <__main__.SimpleDirichletPrior object at 0x7fabb3011220>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.8599 - loglik: -3.4685e+02 - logprior: -8.5692e+01
Epoch 2/10
10/10 - 1s - loss: 328.6665 - loglik: -3.0851e+02 - logprior: -1.9974e+01
Epoch 3/10
10/10 - 1s - loss: 276.0015 - loglik: -2.6758e+02 - logprior: -8.3129e+00
Epoch 4/10
10/10 - 1s - loss: 246.2565 - loglik: -2.4141e+02 - logprior: -4.7268e+00
Epoch 5/10
10/10 - 1s - loss: 233.3375 - loglik: -2.3047e+02 - logprior: -2.6126e+00
Epoch 6/10
10/10 - 1s - loss: 227.8414 - loglik: -2.2593e+02 - logprior: -1.5033e+00
Epoch 7/10
10/10 - 1s - loss: 225.1538 - loglik: -2.2397e+02 - logprior: -7.1752e-01
Epoch 8/10
10/10 - 1s - loss: 223.3937 - loglik: -2.2283e+02 - logprior: -1.1643e-01
Epoch 9/10
10/10 - 1s - loss: 222.3931 - loglik: -2.2234e+02 - logprior: 0.3751
Epoch 10/10
10/10 - 1s - loss: 221.6661 - loglik: -2.2190e+02 - logprior: 0.6711
Fitted a model with MAP estimate = -220.7576
expansions: [(0, 3), (14, 1), (27, 1), (29, 1), (30, 1), (41, 3), (49, 1), (60, 2), (61, 1), (62, 1), (63, 1), (65, 1), (69, 1), (78, 2), (79, 2), (80, 2), (89, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 127 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 334.7147 - loglik: -2.1955e+02 - logprior: -1.1502e+02
Epoch 2/2
10/10 - 2s - loss: 234.5975 - loglik: -2.0220e+02 - logprior: -3.2048e+01
Fitted a model with MAP estimate = -214.4287
expansions: [(16, 1)]
discards: [ 49  97  99 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 279.3370 - loglik: -2.0050e+02 - logprior: -7.8635e+01
Epoch 2/2
10/10 - 2s - loss: 212.9931 - loglik: -1.9608e+02 - logprior: -1.6626e+01
Fitted a model with MAP estimate = -202.2962
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 271.4501 - loglik: -1.9769e+02 - logprior: -7.3631e+01
Epoch 2/10
10/10 - 2s - loss: 211.0085 - loglik: -1.9566e+02 - logprior: -1.5120e+01
Epoch 3/10
10/10 - 2s - loss: 197.3615 - loglik: -1.9411e+02 - logprior: -2.8909e+00
Epoch 4/10
10/10 - 2s - loss: 191.4641 - loglik: -1.9342e+02 - logprior: 2.3576
Epoch 5/10
10/10 - 2s - loss: 187.9855 - loglik: -1.9306e+02 - logprior: 5.4468
Epoch 6/10
10/10 - 2s - loss: 185.9758 - loglik: -1.9302e+02 - logprior: 7.4188
Epoch 7/10
10/10 - 2s - loss: 184.8001 - loglik: -1.9311e+02 - logprior: 8.7054
Epoch 8/10
10/10 - 2s - loss: 183.9733 - loglik: -1.9314e+02 - logprior: 9.5726
Epoch 9/10
10/10 - 2s - loss: 183.3110 - loglik: -1.9315e+02 - logprior: 10.2485
Epoch 10/10
10/10 - 2s - loss: 182.7462 - loglik: -1.9316e+02 - logprior: 10.8267
Fitted a model with MAP estimate = -182.0400
Time for alignment: 56.0105
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.7768 - loglik: -3.4681e+02 - logprior: -8.5693e+01
Epoch 2/10
10/10 - 1s - loss: 328.4060 - loglik: -3.0829e+02 - logprior: -1.9975e+01
Epoch 3/10
10/10 - 1s - loss: 275.2257 - loglik: -2.6691e+02 - logprior: -8.2767e+00
Epoch 4/10
10/10 - 1s - loss: 245.8384 - loglik: -2.4114e+02 - logprior: -4.6136e+00
Epoch 5/10
10/10 - 1s - loss: 232.6335 - loglik: -2.2987e+02 - logprior: -2.5284e+00
Epoch 6/10
10/10 - 1s - loss: 227.1175 - loglik: -2.2522e+02 - logprior: -1.5186e+00
Epoch 7/10
10/10 - 1s - loss: 224.3139 - loglik: -2.2315e+02 - logprior: -7.7231e-01
Epoch 8/10
10/10 - 1s - loss: 222.8815 - loglik: -2.2231e+02 - logprior: -1.9281e-01
Epoch 9/10
10/10 - 1s - loss: 221.8692 - loglik: -2.2174e+02 - logprior: 0.2614
Epoch 10/10
10/10 - 1s - loss: 221.2326 - loglik: -2.2145e+02 - logprior: 0.6304
Fitted a model with MAP estimate = -220.5586
expansions: [(0, 3), (14, 1), (24, 2), (29, 1), (30, 1), (41, 3), (49, 1), (60, 2), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (78, 2), (79, 2), (80, 2), (89, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 128 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 335.7220 - loglik: -2.2083e+02 - logprior: -1.1477e+02
Epoch 2/2
10/10 - 2s - loss: 235.3301 - loglik: -2.0303e+02 - logprior: -3.2109e+01
Fitted a model with MAP estimate = -215.3295
expansions: []
discards: [ 28  50  98 100 115 116]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 279.7551 - loglik: -2.0086e+02 - logprior: -7.8826e+01
Epoch 2/2
10/10 - 2s - loss: 214.0007 - loglik: -1.9703e+02 - logprior: -1.6828e+01
Fitted a model with MAP estimate = -203.4971
expansions: [(16, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 272.1633 - loglik: -1.9848e+02 - logprior: -7.3667e+01
Epoch 2/10
10/10 - 2s - loss: 211.1478 - loglik: -1.9587e+02 - logprior: -1.5172e+01
Epoch 3/10
10/10 - 2s - loss: 197.2904 - loglik: -1.9405e+02 - logprior: -2.9395e+00
Epoch 4/10
10/10 - 2s - loss: 191.2385 - loglik: -1.9318e+02 - logprior: 2.3242
Epoch 5/10
10/10 - 2s - loss: 187.7809 - loglik: -1.9281e+02 - logprior: 5.3864
Epoch 6/10
10/10 - 2s - loss: 185.7815 - loglik: -1.9278e+02 - logprior: 7.3589
Epoch 7/10
10/10 - 2s - loss: 184.6175 - loglik: -1.9288e+02 - logprior: 8.6387
Epoch 8/10
10/10 - 2s - loss: 183.7935 - loglik: -1.9292e+02 - logprior: 9.5047
Epoch 9/10
10/10 - 2s - loss: 183.1430 - loglik: -1.9293e+02 - logprior: 10.1723
Epoch 10/10
10/10 - 2s - loss: 182.5901 - loglik: -1.9294e+02 - logprior: 10.7427
Fitted a model with MAP estimate = -181.9096
Time for alignment: 55.0728
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.0479 - loglik: -3.4679e+02 - logprior: -8.5682e+01
Epoch 2/10
10/10 - 1s - loss: 328.7736 - loglik: -3.0849e+02 - logprior: -1.9972e+01
Epoch 3/10
10/10 - 1s - loss: 275.3525 - loglik: -2.6682e+02 - logprior: -8.4125e+00
Epoch 4/10
10/10 - 1s - loss: 245.4246 - loglik: -2.4035e+02 - logprior: -4.9655e+00
Epoch 5/10
10/10 - 1s - loss: 231.7355 - loglik: -2.2877e+02 - logprior: -2.7416e+00
Epoch 6/10
10/10 - 1s - loss: 226.0249 - loglik: -2.2425e+02 - logprior: -1.4122e+00
Epoch 7/10
10/10 - 1s - loss: 223.4480 - loglik: -2.2242e+02 - logprior: -6.2387e-01
Epoch 8/10
10/10 - 1s - loss: 222.1751 - loglik: -2.2169e+02 - logprior: -8.2807e-02
Epoch 9/10
10/10 - 1s - loss: 221.3512 - loglik: -2.2129e+02 - logprior: 0.3550
Epoch 10/10
10/10 - 1s - loss: 220.8167 - loglik: -2.2108e+02 - logprior: 0.6920
Fitted a model with MAP estimate = -220.1551
expansions: [(0, 3), (13, 1), (14, 1), (21, 2), (29, 2), (30, 1), (41, 2), (49, 1), (57, 1), (60, 1), (61, 1), (62, 3), (67, 1), (76, 1), (79, 2), (80, 2), (89, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 128 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.1264 - loglik: -2.1805e+02 - logprior: -1.1489e+02
Epoch 2/2
10/10 - 2s - loss: 233.1811 - loglik: -2.0080e+02 - logprior: -3.1975e+01
Fitted a model with MAP estimate = -213.4973
expansions: []
discards: [  0   1   2  26  37 100 115 116]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 280.1538 - loglik: -2.0217e+02 - logprior: -7.7833e+01
Epoch 2/2
10/10 - 2s - loss: 214.1978 - loglik: -1.9744e+02 - logprior: -1.6605e+01
Fitted a model with MAP estimate = -203.5920
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 307.4239 - loglik: -1.9850e+02 - logprior: -1.0884e+02
Epoch 2/10
10/10 - 2s - loss: 223.9179 - loglik: -1.9554e+02 - logprior: -2.8194e+01
Epoch 3/10
10/10 - 2s - loss: 201.4988 - loglik: -1.9347e+02 - logprior: -7.7726e+00
Epoch 4/10
10/10 - 2s - loss: 192.7849 - loglik: -1.9285e+02 - logprior: 0.4308
Epoch 5/10
10/10 - 2s - loss: 188.9408 - loglik: -1.9303e+02 - logprior: 4.4868
Epoch 6/10
10/10 - 2s - loss: 186.9777 - loglik: -1.9334e+02 - logprior: 6.7337
Epoch 7/10
10/10 - 2s - loss: 185.7867 - loglik: -1.9352e+02 - logprior: 8.1118
Epoch 8/10
10/10 - 2s - loss: 184.3761 - loglik: -1.9305e+02 - logprior: 9.0535
Epoch 9/10
10/10 - 2s - loss: 183.0825 - loglik: -1.9240e+02 - logprior: 9.7056
Epoch 10/10
10/10 - 2s - loss: 182.1709 - loglik: -1.9202e+02 - logprior: 10.2509
Fitted a model with MAP estimate = -181.3871
Time for alignment: 54.0507
Computed alignments with likelihoods: ['-182.0400', '-181.9096', '-181.3871']
Best model has likelihood: -181.3871
time for generating output: 0.1529
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.9152238805970149
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac3bd7efd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabef1e3d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabcd22ef70>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3baf06a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba0f0cb20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac5dce5eb0>, <__main__.SimpleDirichletPrior object at 0x7faba25744c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.3442 - loglik: -1.3559e+02 - logprior: -3.1691e+01
Epoch 2/10
10/10 - 1s - loss: 123.9641 - loglik: -1.1497e+02 - logprior: -8.9719e+00
Epoch 3/10
10/10 - 1s - loss: 98.2630 - loglik: -9.3322e+01 - logprior: -4.9016e+00
Epoch 4/10
10/10 - 1s - loss: 83.2766 - loglik: -7.9425e+01 - logprior: -3.8137e+00
Epoch 5/10
10/10 - 1s - loss: 78.1304 - loglik: -7.4588e+01 - logprior: -3.4798e+00
Epoch 6/10
10/10 - 1s - loss: 76.3945 - loglik: -7.3026e+01 - logprior: -3.1874e+00
Epoch 7/10
10/10 - 1s - loss: 75.4901 - loglik: -7.2454e+01 - logprior: -2.7845e+00
Epoch 8/10
10/10 - 1s - loss: 75.2313 - loglik: -7.2479e+01 - logprior: -2.5165e+00
Epoch 9/10
10/10 - 1s - loss: 74.8048 - loglik: -7.2177e+01 - logprior: -2.4113e+00
Epoch 10/10
10/10 - 1s - loss: 74.8268 - loglik: -7.2245e+01 - logprior: -2.3622e+00
Fitted a model with MAP estimate = -74.4559
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.5764 - loglik: -7.2120e+01 - logprior: -4.2340e+01
Epoch 2/2
10/10 - 1s - loss: 77.9816 - loglik: -6.4018e+01 - logprior: -1.3818e+01
Fitted a model with MAP estimate = -70.6819
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.0793 - loglik: -6.1083e+01 - logprior: -2.9877e+01
Epoch 2/2
10/10 - 1s - loss: 69.1784 - loglik: -6.0641e+01 - logprior: -8.4621e+00
Fitted a model with MAP estimate = -65.9875
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.7145 - loglik: -6.0454e+01 - logprior: -2.8237e+01
Epoch 2/10
10/10 - 1s - loss: 68.5942 - loglik: -6.0537e+01 - logprior: -8.0106e+00
Epoch 3/10
10/10 - 1s - loss: 64.8849 - loglik: -6.0756e+01 - logprior: -4.0034e+00
Epoch 4/10
10/10 - 1s - loss: 63.3930 - loglik: -6.0706e+01 - logprior: -2.5102e+00
Epoch 5/10
10/10 - 1s - loss: 62.5116 - loglik: -6.0531e+01 - logprior: -1.7952e+00
Epoch 6/10
10/10 - 1s - loss: 62.2173 - loglik: -6.0587e+01 - logprior: -1.4444e+00
Epoch 7/10
10/10 - 1s - loss: 61.9576 - loglik: -6.0544e+01 - logprior: -1.2113e+00
Epoch 8/10
10/10 - 1s - loss: 61.7914 - loglik: -6.0561e+01 - logprior: -1.0114e+00
Epoch 9/10
10/10 - 1s - loss: 61.6733 - loglik: -6.0567e+01 - logprior: -8.6938e-01
Epoch 10/10
10/10 - 1s - loss: 61.6047 - loglik: -6.0588e+01 - logprior: -7.6849e-01
Fitted a model with MAP estimate = -61.2557
Time for alignment: 32.0537
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.3867 - loglik: -1.3566e+02 - logprior: -3.1693e+01
Epoch 2/10
10/10 - 1s - loss: 123.9719 - loglik: -1.1499e+02 - logprior: -8.9751e+00
Epoch 3/10
10/10 - 1s - loss: 98.4466 - loglik: -9.3521e+01 - logprior: -4.8913e+00
Epoch 4/10
10/10 - 1s - loss: 83.5648 - loglik: -7.9732e+01 - logprior: -3.7983e+00
Epoch 5/10
10/10 - 1s - loss: 78.2635 - loglik: -7.4675e+01 - logprior: -3.4749e+00
Epoch 6/10
10/10 - 1s - loss: 76.5287 - loglik: -7.3083e+01 - logprior: -3.2026e+00
Epoch 7/10
10/10 - 1s - loss: 75.5251 - loglik: -7.2504e+01 - logprior: -2.7955e+00
Epoch 8/10
10/10 - 1s - loss: 75.1700 - loglik: -7.2450e+01 - logprior: -2.5185e+00
Epoch 9/10
10/10 - 1s - loss: 74.9481 - loglik: -7.2308e+01 - logprior: -2.4134e+00
Epoch 10/10
10/10 - 1s - loss: 74.7525 - loglik: -7.2157e+01 - logprior: -2.3606e+00
Fitted a model with MAP estimate = -74.4464
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.5563 - loglik: -7.2005e+01 - logprior: -4.2339e+01
Epoch 2/2
10/10 - 1s - loss: 78.0628 - loglik: -6.4107e+01 - logprior: -1.3821e+01
Fitted a model with MAP estimate = -70.7188
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.3871 - loglik: -6.1460e+01 - logprior: -2.9884e+01
Epoch 2/2
10/10 - 1s - loss: 69.2626 - loglik: -6.0786e+01 - logprior: -8.4623e+00
Fitted a model with MAP estimate = -66.1621
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 89.0425 - loglik: -6.0681e+01 - logprior: -2.8248e+01
Epoch 2/10
10/10 - 1s - loss: 68.6698 - loglik: -6.0613e+01 - logprior: -8.0174e+00
Epoch 3/10
10/10 - 1s - loss: 64.8700 - loglik: -6.0791e+01 - logprior: -4.0030e+00
Epoch 4/10
10/10 - 1s - loss: 63.4059 - loglik: -6.0769e+01 - logprior: -2.5032e+00
Epoch 5/10
10/10 - 1s - loss: 62.6314 - loglik: -6.0683e+01 - logprior: -1.7882e+00
Epoch 6/10
10/10 - 1s - loss: 62.1008 - loglik: -6.0489e+01 - logprior: -1.4346e+00
Epoch 7/10
10/10 - 1s - loss: 62.1449 - loglik: -6.0742e+01 - logprior: -1.2115e+00
Fitted a model with MAP estimate = -61.6996
Time for alignment: 30.0974
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.2868 - loglik: -1.3553e+02 - logprior: -3.1691e+01
Epoch 2/10
10/10 - 1s - loss: 124.1142 - loglik: -1.1512e+02 - logprior: -8.9751e+00
Epoch 3/10
10/10 - 1s - loss: 98.3120 - loglik: -9.3383e+01 - logprior: -4.8998e+00
Epoch 4/10
10/10 - 1s - loss: 83.3323 - loglik: -7.9491e+01 - logprior: -3.7983e+00
Epoch 5/10
10/10 - 1s - loss: 78.1698 - loglik: -7.4591e+01 - logprior: -3.4753e+00
Epoch 6/10
10/10 - 1s - loss: 76.4421 - loglik: -7.3037e+01 - logprior: -3.1905e+00
Epoch 7/10
10/10 - 1s - loss: 75.5185 - loglik: -7.2484e+01 - logprior: -2.7916e+00
Epoch 8/10
10/10 - 1s - loss: 75.1556 - loglik: -7.2416e+01 - logprior: -2.5213e+00
Epoch 9/10
10/10 - 1s - loss: 74.9426 - loglik: -7.2299e+01 - logprior: -2.4161e+00
Epoch 10/10
10/10 - 1s - loss: 74.7394 - loglik: -7.2144e+01 - logprior: -2.3611e+00
Fitted a model with MAP estimate = -74.4490
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.6238 - loglik: -7.2168e+01 - logprior: -4.2343e+01
Epoch 2/2
10/10 - 1s - loss: 78.0238 - loglik: -6.4069e+01 - logprior: -1.3812e+01
Fitted a model with MAP estimate = -70.6778
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 90.9140 - loglik: -6.0959e+01 - logprior: -2.9869e+01
Epoch 2/2
10/10 - 1s - loss: 69.2066 - loglik: -6.0634e+01 - logprior: -8.4611e+00
Fitted a model with MAP estimate = -65.9210
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 88.5490 - loglik: -6.0205e+01 - logprior: -2.8250e+01
Epoch 2/10
10/10 - 1s - loss: 68.6823 - loglik: -6.0565e+01 - logprior: -8.0113e+00
Epoch 3/10
10/10 - 1s - loss: 64.7107 - loglik: -6.0532e+01 - logprior: -4.0233e+00
Epoch 4/10
10/10 - 1s - loss: 63.1683 - loglik: -6.0479e+01 - logprior: -2.5273e+00
Epoch 5/10
10/10 - 1s - loss: 62.6366 - loglik: -6.0659e+01 - logprior: -1.8099e+00
Epoch 6/10
10/10 - 1s - loss: 61.9955 - loglik: -6.0346e+01 - logprior: -1.4554e+00
Epoch 7/10
10/10 - 1s - loss: 61.9827 - loglik: -6.0544e+01 - logprior: -1.2243e+00
Epoch 8/10
10/10 - 1s - loss: 61.7484 - loglik: -6.0494e+01 - logprior: -1.0274e+00
Epoch 9/10
10/10 - 1s - loss: 61.7350 - loglik: -6.0626e+01 - logprior: -8.7080e-01
Epoch 10/10
10/10 - 1s - loss: 61.5926 - loglik: -6.0565e+01 - logprior: -7.7951e-01
Fitted a model with MAP estimate = -61.2667
Time for alignment: 32.1463
Computed alignments with likelihoods: ['-61.2557', '-61.6996', '-61.2667']
Best model has likelihood: -61.2557
time for generating output: 0.1171
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaf3fec850>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac665cc9a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faba2574fd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac44178ee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac441783d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaf42e15e0>, <__main__.SimpleDirichletPrior object at 0x7faba0cae880>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.4214 - loglik: -4.4001e+02 - logprior: -5.3336e+00
Epoch 2/10
15/15 - 4s - loss: 365.8615 - loglik: -3.6453e+02 - logprior: -1.2958e+00
Epoch 3/10
15/15 - 4s - loss: 319.3978 - loglik: -3.1779e+02 - logprior: -1.5078e+00
Epoch 4/10
15/15 - 4s - loss: 305.4347 - loglik: -3.0350e+02 - logprior: -1.6656e+00
Epoch 5/10
15/15 - 4s - loss: 301.5055 - loglik: -2.9966e+02 - logprior: -1.5571e+00
Epoch 6/10
15/15 - 4s - loss: 300.0411 - loglik: -2.9825e+02 - logprior: -1.5445e+00
Epoch 7/10
15/15 - 4s - loss: 299.5292 - loglik: -2.9780e+02 - logprior: -1.5188e+00
Epoch 8/10
15/15 - 4s - loss: 298.5229 - loglik: -2.9684e+02 - logprior: -1.4990e+00
Epoch 9/10
15/15 - 4s - loss: 298.0976 - loglik: -2.9639e+02 - logprior: -1.5109e+00
Epoch 10/10
15/15 - 4s - loss: 298.1574 - loglik: -2.9647e+02 - logprior: -1.5003e+00
Fitted a model with MAP estimate = -297.6181
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 2), (66, 3), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 2), (115, 2), (116, 3), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 303.0477 - loglik: -2.9644e+02 - logprior: -6.5456e+00
Epoch 2/2
15/15 - 5s - loss: 287.7427 - loglik: -2.8435e+02 - logprior: -3.2397e+00
Fitted a model with MAP estimate = -284.3426
expansions: [(0, 2)]
discards: [  0   7  77 108 135]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 289.2121 - loglik: -2.8421e+02 - logprior: -4.9734e+00
Epoch 2/2
15/15 - 5s - loss: 281.8969 - loglik: -2.8038e+02 - logprior: -1.4502e+00
Fitted a model with MAP estimate = -279.9667
expansions: []
discards: [ 0 57 66 67 82]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 293.6949 - loglik: -2.8729e+02 - logprior: -6.3086e+00
Epoch 2/10
15/15 - 5s - loss: 286.0511 - loglik: -2.8383e+02 - logprior: -2.1173e+00
Epoch 3/10
15/15 - 5s - loss: 284.0632 - loglik: -2.8276e+02 - logprior: -1.0376e+00
Epoch 4/10
15/15 - 5s - loss: 282.1276 - loglik: -2.8086e+02 - logprior: -8.9518e-01
Epoch 5/10
15/15 - 5s - loss: 281.1929 - loglik: -2.7998e+02 - logprior: -8.4504e-01
Epoch 6/10
15/15 - 5s - loss: 280.5994 - loglik: -2.7946e+02 - logprior: -8.1164e-01
Epoch 7/10
15/15 - 5s - loss: 280.2797 - loglik: -2.7921e+02 - logprior: -7.7957e-01
Epoch 8/10
15/15 - 5s - loss: 280.2103 - loglik: -2.7920e+02 - logprior: -7.3393e-01
Epoch 9/10
15/15 - 5s - loss: 279.8496 - loglik: -2.7890e+02 - logprior: -6.7775e-01
Epoch 10/10
15/15 - 5s - loss: 279.9595 - loglik: -2.7905e+02 - logprior: -6.4513e-01
Fitted a model with MAP estimate = -279.3672
Time for alignment: 155.2051
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.5345 - loglik: -4.4009e+02 - logprior: -5.3476e+00
Epoch 2/10
15/15 - 4s - loss: 367.6944 - loglik: -3.6632e+02 - logprior: -1.3162e+00
Epoch 3/10
15/15 - 4s - loss: 317.9020 - loglik: -3.1618e+02 - logprior: -1.5755e+00
Epoch 4/10
15/15 - 4s - loss: 303.1652 - loglik: -3.0105e+02 - logprior: -1.7799e+00
Epoch 5/10
15/15 - 4s - loss: 299.5713 - loglik: -2.9754e+02 - logprior: -1.6600e+00
Epoch 6/10
15/15 - 4s - loss: 298.4503 - loglik: -2.9649e+02 - logprior: -1.6563e+00
Epoch 7/10
15/15 - 4s - loss: 296.5676 - loglik: -2.9468e+02 - logprior: -1.6405e+00
Epoch 8/10
15/15 - 4s - loss: 296.9030 - loglik: -2.9505e+02 - logprior: -1.6183e+00
Fitted a model with MAP estimate = -296.3352
expansions: [(7, 3), (10, 1), (16, 1), (25, 1), (26, 2), (27, 1), (49, 2), (60, 2), (66, 2), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 1), (115, 1), (116, 4), (119, 1), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 303.5150 - loglik: -2.9694e+02 - logprior: -6.5397e+00
Epoch 2/2
15/15 - 5s - loss: 287.4513 - loglik: -2.8407e+02 - logprior: -3.2314e+00
Fitted a model with MAP estimate = -284.3565
expansions: [(0, 2)]
discards: [  0   7  32  58  79 108 140]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 289.5413 - loglik: -2.8451e+02 - logprior: -4.9397e+00
Epoch 2/2
15/15 - 5s - loss: 282.7932 - loglik: -2.8120e+02 - logprior: -1.4393e+00
Fitted a model with MAP estimate = -281.6486
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 291.9431 - loglik: -2.8555e+02 - logprior: -6.3749e+00
Epoch 2/10
15/15 - 5s - loss: 284.8031 - loglik: -2.8254e+02 - logprior: -2.1293e+00
Epoch 3/10
15/15 - 5s - loss: 281.7303 - loglik: -2.8039e+02 - logprior: -1.0601e+00
Epoch 4/10
15/15 - 5s - loss: 280.8345 - loglik: -2.7958e+02 - logprior: -8.9426e-01
Epoch 5/10
15/15 - 5s - loss: 280.8434 - loglik: -2.7963e+02 - logprior: -8.4368e-01
Fitted a model with MAP estimate = -279.1462
Time for alignment: 118.8895
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 444.8872 - loglik: -4.3948e+02 - logprior: -5.3399e+00
Epoch 2/10
15/15 - 4s - loss: 368.2308 - loglik: -3.6690e+02 - logprior: -1.2958e+00
Epoch 3/10
15/15 - 4s - loss: 322.3017 - loglik: -3.2070e+02 - logprior: -1.5238e+00
Epoch 4/10
15/15 - 4s - loss: 306.4865 - loglik: -3.0449e+02 - logprior: -1.7032e+00
Epoch 5/10
15/15 - 4s - loss: 299.6782 - loglik: -2.9768e+02 - logprior: -1.6854e+00
Epoch 6/10
15/15 - 4s - loss: 298.1352 - loglik: -2.9618e+02 - logprior: -1.6877e+00
Epoch 7/10
15/15 - 4s - loss: 296.0668 - loglik: -2.9420e+02 - logprior: -1.6397e+00
Epoch 8/10
15/15 - 4s - loss: 296.2435 - loglik: -2.9444e+02 - logprior: -1.5985e+00
Fitted a model with MAP estimate = -295.8786
expansions: [(8, 2), (11, 1), (15, 1), (16, 1), (24, 2), (27, 1), (53, 1), (55, 1), (59, 1), (66, 2), (67, 1), (69, 1), (91, 1), (106, 2), (112, 1), (114, 4), (116, 3), (119, 2), (120, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 301.2434 - loglik: -2.9459e+02 - logprior: -6.5661e+00
Epoch 2/2
15/15 - 5s - loss: 285.7061 - loglik: -2.8235e+02 - logprior: -3.2014e+00
Fitted a model with MAP estimate = -283.4974
expansions: [(0, 2)]
discards: [  0   7  77  82 122 147]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 289.1760 - loglik: -2.8415e+02 - logprior: -4.9359e+00
Epoch 2/2
15/15 - 5s - loss: 282.1497 - loglik: -2.8062e+02 - logprior: -1.4313e+00
Fitted a model with MAP estimate = -281.1091
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 290.1839 - loglik: -2.8362e+02 - logprior: -6.3867e+00
Epoch 2/10
15/15 - 5s - loss: 283.0510 - loglik: -2.8058e+02 - logprior: -2.1611e+00
Epoch 3/10
15/15 - 5s - loss: 281.2202 - loglik: -2.7973e+02 - logprior: -1.0899e+00
Epoch 4/10
15/15 - 5s - loss: 281.0255 - loglik: -2.7969e+02 - logprior: -9.2640e-01
Epoch 5/10
15/15 - 5s - loss: 278.2770 - loglik: -2.7703e+02 - logprior: -8.7933e-01
Epoch 6/10
15/15 - 5s - loss: 277.7783 - loglik: -2.7662e+02 - logprior: -8.4262e-01
Epoch 7/10
15/15 - 5s - loss: 279.0247 - loglik: -2.7794e+02 - logprior: -7.9235e-01
Fitted a model with MAP estimate = -277.5665
Time for alignment: 130.8381
Computed alignments with likelihoods: ['-279.3672', '-279.1462', '-277.5665']
Best model has likelihood: -277.5665
time for generating output: 0.2242
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.8925585284280937
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabde1c9550>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6fd74f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabe6847a60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe68471f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe68475b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabef15ad30>, <__main__.SimpleDirichletPrior object at 0x7fabef35a7f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 299.6794 - loglik: -2.9672e+02 - logprior: -2.8606e+00
Epoch 2/10
34/34 - 6s - loss: 206.4827 - loglik: -2.0449e+02 - logprior: -1.8124e+00
Epoch 3/10
34/34 - 6s - loss: 199.5150 - loglik: -1.9746e+02 - logprior: -1.7749e+00
Epoch 4/10
34/34 - 7s - loss: 200.3463 - loglik: -1.9832e+02 - logprior: -1.7474e+00
Fitted a model with MAP estimate = -198.4026
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 2), (56, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 187.8786 - loglik: -1.8469e+02 - logprior: -3.0516e+00
Epoch 2/2
34/34 - 7s - loss: 177.1697 - loglik: -1.7559e+02 - logprior: -1.3178e+00
Fitted a model with MAP estimate = -175.7790
expansions: []
discards: [138]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 180.4565 - loglik: -1.7763e+02 - logprior: -2.6707e+00
Epoch 2/2
34/34 - 7s - loss: 176.6252 - loglik: -1.7531e+02 - logprior: -1.1212e+00
Fitted a model with MAP estimate = -175.4741
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 180.2565 - loglik: -1.7757e+02 - logprior: -2.5408e+00
Epoch 2/10
34/34 - 7s - loss: 175.6824 - loglik: -1.7448e+02 - logprior: -9.8460e-01
Epoch 3/10
34/34 - 7s - loss: 175.3920 - loglik: -1.7420e+02 - logprior: -8.9157e-01
Epoch 4/10
34/34 - 7s - loss: 175.7889 - loglik: -1.7467e+02 - logprior: -8.0199e-01
Fitted a model with MAP estimate = -173.8706
Time for alignment: 133.5132
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 301.1653 - loglik: -2.9823e+02 - logprior: -2.8417e+00
Epoch 2/10
34/34 - 6s - loss: 207.9900 - loglik: -2.0600e+02 - logprior: -1.8021e+00
Epoch 3/10
34/34 - 7s - loss: 200.0641 - loglik: -1.9800e+02 - logprior: -1.7933e+00
Epoch 4/10
34/34 - 5s - loss: 198.5242 - loglik: -1.9649e+02 - logprior: -1.7589e+00
Epoch 5/10
34/34 - 6s - loss: 198.3690 - loglik: -1.9635e+02 - logprior: -1.7414e+00
Epoch 6/10
34/34 - 6s - loss: 197.9245 - loglik: -1.9589e+02 - logprior: -1.7585e+00
Epoch 7/10
34/34 - 6s - loss: 197.1957 - loglik: -1.9515e+02 - logprior: -1.7536e+00
Epoch 8/10
34/34 - 7s - loss: 197.7750 - loglik: -1.9572e+02 - logprior: -1.7659e+00
Fitted a model with MAP estimate = -197.2533
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 187.7963 - loglik: -1.8455e+02 - logprior: -3.1036e+00
Epoch 2/2
34/34 - 7s - loss: 176.5939 - loglik: -1.7508e+02 - logprior: -1.3111e+00
Fitted a model with MAP estimate = -175.3073
expansions: []
discards: [ 20  35 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 12s - loss: 180.9853 - loglik: -1.7829e+02 - logprior: -2.6500e+00
Epoch 2/2
34/34 - 6s - loss: 177.0026 - loglik: -1.7574e+02 - logprior: -1.1000e+00
Fitted a model with MAP estimate = -175.5978
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.7995 - loglik: -1.7714e+02 - logprior: -2.5291e+00
Epoch 2/10
34/34 - 7s - loss: 176.1199 - loglik: -1.7490e+02 - logprior: -9.5655e-01
Epoch 3/10
34/34 - 7s - loss: 175.6561 - loglik: -1.7450e+02 - logprior: -8.5274e-01
Epoch 4/10
34/34 - 7s - loss: 174.6720 - loglik: -1.7360e+02 - logprior: -7.6739e-01
Epoch 5/10
34/34 - 7s - loss: 174.3897 - loglik: -1.7340e+02 - logprior: -6.8592e-01
Epoch 6/10
34/34 - 7s - loss: 173.9231 - loglik: -1.7301e+02 - logprior: -6.0266e-01
Epoch 7/10
34/34 - 7s - loss: 173.5050 - loglik: -1.7268e+02 - logprior: -5.1683e-01
Epoch 8/10
34/34 - 7s - loss: 173.8161 - loglik: -1.7305e+02 - logprior: -4.4170e-01
Fitted a model with MAP estimate = -172.9892
Time for alignment: 186.8363
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 300.0446 - loglik: -2.9709e+02 - logprior: -2.8478e+00
Epoch 2/10
34/34 - 6s - loss: 209.1890 - loglik: -2.0724e+02 - logprior: -1.7729e+00
Epoch 3/10
34/34 - 6s - loss: 199.7139 - loglik: -1.9770e+02 - logprior: -1.7591e+00
Epoch 4/10
34/34 - 7s - loss: 198.9950 - loglik: -1.9700e+02 - logprior: -1.7330e+00
Epoch 5/10
34/34 - 6s - loss: 197.6040 - loglik: -1.9561e+02 - logprior: -1.7211e+00
Epoch 6/10
34/34 - 6s - loss: 198.4030 - loglik: -1.9642e+02 - logprior: -1.7015e+00
Fitted a model with MAP estimate = -197.3375
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 187.1370 - loglik: -1.8385e+02 - logprior: -3.0793e+00
Epoch 2/2
34/34 - 7s - loss: 176.3308 - loglik: -1.7478e+02 - logprior: -1.3017e+00
Fitted a model with MAP estimate = -175.2679
expansions: []
discards: [ 34 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 180.1961 - loglik: -1.7739e+02 - logprior: -2.6637e+00
Epoch 2/2
34/34 - 7s - loss: 176.5861 - loglik: -1.7519e+02 - logprior: -1.1170e+00
Fitted a model with MAP estimate = -175.5472
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.8524 - loglik: -1.7718e+02 - logprior: -2.5362e+00
Epoch 2/10
34/34 - 7s - loss: 176.3515 - loglik: -1.7513e+02 - logprior: -9.6779e-01
Epoch 3/10
34/34 - 7s - loss: 175.8168 - loglik: -1.7465e+02 - logprior: -8.6832e-01
Epoch 4/10
34/34 - 7s - loss: 174.1488 - loglik: -1.7305e+02 - logprior: -7.8547e-01
Epoch 5/10
34/34 - 8s - loss: 174.3931 - loglik: -1.7338e+02 - logprior: -7.0532e-01
Fitted a model with MAP estimate = -173.7561
Time for alignment: 152.7900
Computed alignments with likelihoods: ['-173.8706', '-172.9892', '-173.7561']
Best model has likelihood: -172.9892
time for generating output: 0.3717
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6310333657272432
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabc49ed9d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5de5b0d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaebc09a60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafcc28dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaebc3f730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab2ef5dbe0>, <__main__.SimpleDirichletPrior object at 0x7faba1438f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.5533 - loglik: -3.9879e+02 - logprior: -2.0745e+01
Epoch 2/10
10/10 - 2s - loss: 365.0229 - loglik: -3.6022e+02 - logprior: -4.7848e+00
Epoch 3/10
10/10 - 2s - loss: 321.0363 - loglik: -3.1839e+02 - logprior: -2.5760e+00
Epoch 4/10
10/10 - 2s - loss: 291.6129 - loglik: -2.8920e+02 - logprior: -2.1993e+00
Epoch 5/10
10/10 - 2s - loss: 279.9436 - loglik: -2.7744e+02 - logprior: -2.1360e+00
Epoch 6/10
10/10 - 2s - loss: 275.3470 - loglik: -2.7295e+02 - logprior: -2.0561e+00
Epoch 7/10
10/10 - 2s - loss: 273.7815 - loglik: -2.7149e+02 - logprior: -1.9939e+00
Epoch 8/10
10/10 - 2s - loss: 271.8439 - loglik: -2.6959e+02 - logprior: -1.9488e+00
Epoch 9/10
10/10 - 2s - loss: 272.2092 - loglik: -2.7005e+02 - logprior: -1.8608e+00
Fitted a model with MAP estimate = -271.1991
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (87, 1), (97, 1), (98, 1), (101, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 290.5995 - loglik: -2.7150e+02 - logprior: -1.9031e+01
Epoch 2/2
10/10 - 2s - loss: 264.6332 - loglik: -2.5993e+02 - logprior: -4.6892e+00
Fitted a model with MAP estimate = -259.4208
expansions: []
discards: [  0  31 104]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 283.2825 - loglik: -2.5977e+02 - logprior: -2.3433e+01
Epoch 2/2
10/10 - 2s - loss: 265.8538 - loglik: -2.5661e+02 - logprior: -9.1707e+00
Fitted a model with MAP estimate = -262.2637
expansions: [(0, 5)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.2243 - loglik: -2.5672e+02 - logprior: -1.8468e+01
Epoch 2/10
10/10 - 2s - loss: 259.3744 - loglik: -2.5508e+02 - logprior: -4.1447e+00
Epoch 3/10
10/10 - 2s - loss: 254.1671 - loglik: -2.5253e+02 - logprior: -1.3331e+00
Epoch 4/10
10/10 - 2s - loss: 252.1894 - loglik: -2.5145e+02 - logprior: -3.2314e-01
Epoch 5/10
10/10 - 2s - loss: 250.7593 - loglik: -2.5046e+02 - logprior: 0.1380
Epoch 6/10
10/10 - 2s - loss: 250.1758 - loglik: -2.5014e+02 - logprior: 0.3718
Epoch 7/10
10/10 - 2s - loss: 249.5296 - loglik: -2.4966e+02 - logprior: 0.5331
Epoch 8/10
10/10 - 2s - loss: 248.6750 - loglik: -2.4899e+02 - logprior: 0.7062
Epoch 9/10
10/10 - 2s - loss: 248.8266 - loglik: -2.4928e+02 - logprior: 0.8436
Fitted a model with MAP estimate = -248.3634
Time for alignment: 73.6831
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.0005 - loglik: -3.9923e+02 - logprior: -2.0744e+01
Epoch 2/10
10/10 - 2s - loss: 365.1994 - loglik: -3.6040e+02 - logprior: -4.7896e+00
Epoch 3/10
10/10 - 2s - loss: 322.7888 - loglik: -3.2017e+02 - logprior: -2.5593e+00
Epoch 4/10
10/10 - 2s - loss: 293.2202 - loglik: -2.9073e+02 - logprior: -2.3033e+00
Epoch 5/10
10/10 - 2s - loss: 280.5895 - loglik: -2.7784e+02 - logprior: -2.3814e+00
Epoch 6/10
10/10 - 2s - loss: 275.6824 - loglik: -2.7292e+02 - logprior: -2.3236e+00
Epoch 7/10
10/10 - 2s - loss: 273.6667 - loglik: -2.7106e+02 - logprior: -2.2420e+00
Epoch 8/10
10/10 - 2s - loss: 271.7890 - loglik: -2.6932e+02 - logprior: -2.1596e+00
Epoch 9/10
10/10 - 2s - loss: 271.0541 - loglik: -2.6867e+02 - logprior: -2.0703e+00
Epoch 10/10
10/10 - 2s - loss: 271.0425 - loglik: -2.6869e+02 - logprior: -2.0322e+00
Fitted a model with MAP estimate = -270.3736
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (86, 2), (87, 1), (88, 1), (101, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 289.8409 - loglik: -2.7076e+02 - logprior: -1.9005e+01
Epoch 2/2
10/10 - 3s - loss: 263.6441 - loglik: -2.5876e+02 - logprior: -4.7190e+00
Fitted a model with MAP estimate = -258.4834
expansions: []
discards: [  0   7   8 105 111]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 284.5217 - loglik: -2.6083e+02 - logprior: -2.3510e+01
Epoch 2/2
10/10 - 2s - loss: 266.1260 - loglik: -2.5653e+02 - logprior: -9.2966e+00
Fitted a model with MAP estimate = -263.0384
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.6381 - loglik: -2.5797e+02 - logprior: -1.8597e+01
Epoch 2/10
10/10 - 2s - loss: 259.6706 - loglik: -2.5526e+02 - logprior: -4.1932e+00
Epoch 3/10
10/10 - 2s - loss: 255.4189 - loglik: -2.5370e+02 - logprior: -1.3732e+00
Epoch 4/10
10/10 - 2s - loss: 252.9689 - loglik: -2.5215e+02 - logprior: -3.8152e-01
Epoch 5/10
10/10 - 2s - loss: 251.6828 - loglik: -2.5128e+02 - logprior: 0.0709
Epoch 6/10
10/10 - 3s - loss: 251.0557 - loglik: -2.5088e+02 - logprior: 0.2931
Epoch 7/10
10/10 - 2s - loss: 249.6915 - loglik: -2.4971e+02 - logprior: 0.4676
Epoch 8/10
10/10 - 3s - loss: 250.1625 - loglik: -2.5036e+02 - logprior: 0.6379
Fitted a model with MAP estimate = -249.2942
Time for alignment: 72.4641
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 420.0982 - loglik: -3.9927e+02 - logprior: -2.0748e+01
Epoch 2/10
10/10 - 2s - loss: 363.2310 - loglik: -3.5842e+02 - logprior: -4.7905e+00
Epoch 3/10
10/10 - 2s - loss: 318.7417 - loglik: -3.1615e+02 - logprior: -2.5302e+00
Epoch 4/10
10/10 - 2s - loss: 289.1116 - loglik: -2.8688e+02 - logprior: -2.0602e+00
Epoch 5/10
10/10 - 2s - loss: 280.9240 - loglik: -2.7864e+02 - logprior: -1.9584e+00
Epoch 6/10
10/10 - 2s - loss: 277.0945 - loglik: -2.7489e+02 - logprior: -1.8208e+00
Epoch 7/10
10/10 - 2s - loss: 275.7526 - loglik: -2.7367e+02 - logprior: -1.7357e+00
Epoch 8/10
10/10 - 2s - loss: 274.7746 - loglik: -2.7277e+02 - logprior: -1.6819e+00
Epoch 9/10
10/10 - 2s - loss: 274.3415 - loglik: -2.7236e+02 - logprior: -1.6485e+00
Epoch 10/10
10/10 - 2s - loss: 274.1001 - loglik: -2.7214e+02 - logprior: -1.6337e+00
Fitted a model with MAP estimate = -273.3906
expansions: [(8, 1), (9, 1), (10, 1), (19, 2), (23, 1), (24, 3), (26, 1), (53, 1), (55, 2), (62, 1), (79, 3), (80, 2), (81, 2), (87, 1), (97, 3), (99, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 298.4997 - loglik: -2.7463e+02 - logprior: -2.3708e+01
Epoch 2/2
10/10 - 2s - loss: 270.6101 - loglik: -2.6094e+02 - logprior: -9.5682e+00
Fitted a model with MAP estimate = -264.1915
expansions: [(0, 5)]
discards: [  0  65  96 127]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 277.4242 - loglik: -2.5851e+02 - logprior: -1.8815e+01
Epoch 2/2
10/10 - 3s - loss: 258.1279 - loglik: -2.5337e+02 - logprior: -4.5200e+00
Fitted a model with MAP estimate = -254.6129
expansions: [(122, 1)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.3115 - loglik: -2.5678e+02 - logprior: -1.8446e+01
Epoch 2/10
10/10 - 2s - loss: 257.0670 - loglik: -2.5266e+02 - logprior: -4.3045e+00
Epoch 3/10
10/10 - 2s - loss: 252.9823 - loglik: -2.5118e+02 - logprior: -1.5785e+00
Epoch 4/10
10/10 - 2s - loss: 250.3927 - loglik: -2.4931e+02 - logprior: -6.8895e-01
Epoch 5/10
10/10 - 2s - loss: 248.7677 - loglik: -2.4799e+02 - logprior: -3.1115e-01
Epoch 6/10
10/10 - 3s - loss: 248.7544 - loglik: -2.4846e+02 - logprior: 0.1441
Epoch 7/10
10/10 - 2s - loss: 247.5624 - loglik: -2.4767e+02 - logprior: 0.5060
Epoch 8/10
10/10 - 2s - loss: 246.9800 - loglik: -2.4725e+02 - logprior: 0.6538
Epoch 9/10
10/10 - 2s - loss: 246.6167 - loglik: -2.4699e+02 - logprior: 0.7580
Epoch 10/10
10/10 - 2s - loss: 247.6019 - loglik: -2.4808e+02 - logprior: 0.8749
Fitted a model with MAP estimate = -246.2101
Time for alignment: 78.3016
Computed alignments with likelihoods: ['-248.3634', '-249.2942', '-246.2101']
Best model has likelihood: -246.2101
time for generating output: 0.1909
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8484848484848485
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac768c41f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5dd1afd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaeb50ddf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafc5286d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1418490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac76a61ac0>, <__main__.SimpleDirichletPrior object at 0x7fac4cf41b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 269.9636 - loglik: -2.3246e+02 - logprior: -3.7338e+01
Epoch 2/10
10/10 - 1s - loss: 230.3679 - loglik: -2.2048e+02 - logprior: -9.5662e+00
Epoch 3/10
10/10 - 1s - loss: 213.2662 - loglik: -2.0840e+02 - logprior: -4.3470e+00
Epoch 4/10
10/10 - 1s - loss: 202.4516 - loglik: -1.9945e+02 - logprior: -2.5681e+00
Epoch 5/10
10/10 - 1s - loss: 196.5653 - loglik: -1.9412e+02 - logprior: -1.9279e+00
Epoch 6/10
10/10 - 1s - loss: 192.6554 - loglik: -1.9031e+02 - logprior: -1.8590e+00
Epoch 7/10
10/10 - 1s - loss: 190.9028 - loglik: -1.8895e+02 - logprior: -1.5609e+00
Epoch 8/10
10/10 - 1s - loss: 189.6975 - loglik: -1.8828e+02 - logprior: -1.0611e+00
Epoch 9/10
10/10 - 1s - loss: 189.4193 - loglik: -1.8821e+02 - logprior: -8.7996e-01
Epoch 10/10
10/10 - 1s - loss: 188.8791 - loglik: -1.8774e+02 - logprior: -8.1300e-01
Fitted a model with MAP estimate = -188.4004
expansions: [(0, 4), (6, 1), (20, 1), (21, 1), (27, 1), (30, 2), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 261.1274 - loglik: -2.1188e+02 - logprior: -4.9208e+01
Epoch 2/2
10/10 - 1s - loss: 207.1246 - loglik: -1.9238e+02 - logprior: -1.4617e+01
Fitted a model with MAP estimate = -196.0426
expansions: [(41, 1)]
discards: [ 0  1  3 38]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.4348 - loglik: -1.9232e+02 - logprior: -4.2061e+01
Epoch 2/2
10/10 - 1s - loss: 203.3162 - loglik: -1.8726e+02 - logprior: -1.5979e+01
Fitted a model with MAP estimate = -198.0393
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 222.9120 - loglik: -1.8524e+02 - logprior: -3.7427e+01
Epoch 2/10
10/10 - 1s - loss: 193.1219 - loglik: -1.8344e+02 - logprior: -9.3643e+00
Epoch 3/10
10/10 - 1s - loss: 186.1349 - loglik: -1.8261e+02 - logprior: -3.1455e+00
Epoch 4/10
10/10 - 1s - loss: 183.4799 - loglik: -1.8218e+02 - logprior: -9.1494e-01
Epoch 5/10
10/10 - 1s - loss: 182.1131 - loglik: -1.8194e+02 - logprior: 0.1918
Epoch 6/10
10/10 - 1s - loss: 181.3530 - loglik: -1.8178e+02 - logprior: 0.7670
Epoch 7/10
10/10 - 1s - loss: 180.7844 - loglik: -1.8155e+02 - logprior: 1.0807
Epoch 8/10
10/10 - 1s - loss: 180.1595 - loglik: -1.8111e+02 - logprior: 1.2580
Epoch 9/10
10/10 - 1s - loss: 179.8435 - loglik: -1.8094e+02 - logprior: 1.3830
Epoch 10/10
10/10 - 1s - loss: 179.6230 - loglik: -1.8086e+02 - logprior: 1.5051
Fitted a model with MAP estimate = -179.1312
Time for alignment: 48.0572
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 269.9907 - loglik: -2.3256e+02 - logprior: -3.7340e+01
Epoch 2/10
10/10 - 1s - loss: 230.1879 - loglik: -2.2030e+02 - logprior: -9.5720e+00
Epoch 3/10
10/10 - 1s - loss: 212.4037 - loglik: -2.0759e+02 - logprior: -4.3380e+00
Epoch 4/10
10/10 - 1s - loss: 200.7878 - loglik: -1.9769e+02 - logprior: -2.6008e+00
Epoch 5/10
10/10 - 1s - loss: 195.4900 - loglik: -1.9287e+02 - logprior: -2.0573e+00
Epoch 6/10
10/10 - 1s - loss: 192.4481 - loglik: -1.9006e+02 - logprior: -1.9428e+00
Epoch 7/10
10/10 - 1s - loss: 190.9116 - loglik: -1.8902e+02 - logprior: -1.5068e+00
Epoch 8/10
10/10 - 1s - loss: 189.9755 - loglik: -1.8860e+02 - logprior: -1.0372e+00
Epoch 9/10
10/10 - 1s - loss: 189.3329 - loglik: -1.8813e+02 - logprior: -8.7950e-01
Epoch 10/10
10/10 - 1s - loss: 189.0143 - loglik: -1.8790e+02 - logprior: -8.0572e-01
Fitted a model with MAP estimate = -188.5951
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.1663 - loglik: -2.0388e+02 - logprior: -4.9183e+01
Epoch 2/2
10/10 - 1s - loss: 202.8621 - loglik: -1.8824e+02 - logprior: -1.4361e+01
Fitted a model with MAP estimate = -193.1565
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 230.2835 - loglik: -1.8801e+02 - logprior: -4.1992e+01
Epoch 2/2
10/10 - 1s - loss: 201.5346 - loglik: -1.8502e+02 - logprior: -1.6091e+01
Fitted a model with MAP estimate = -196.4458
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 225.1700 - loglik: -1.8773e+02 - logprior: -3.7421e+01
Epoch 2/10
10/10 - 1s - loss: 194.4624 - loglik: -1.8506e+02 - logprior: -9.3443e+00
Epoch 3/10
10/10 - 1s - loss: 187.3530 - loglik: -1.8407e+02 - logprior: -3.1790e+00
Epoch 4/10
10/10 - 1s - loss: 184.2480 - loglik: -1.8311e+02 - logprior: -9.6229e-01
Epoch 5/10
10/10 - 1s - loss: 182.7332 - loglik: -1.8263e+02 - logprior: 0.1316
Epoch 6/10
10/10 - 1s - loss: 181.7635 - loglik: -1.8218e+02 - logprior: 0.6964
Epoch 7/10
10/10 - 1s - loss: 181.0352 - loglik: -1.8173e+02 - logprior: 0.9958
Epoch 8/10
10/10 - 1s - loss: 180.5485 - loglik: -1.8139e+02 - logprior: 1.1542
Epoch 9/10
10/10 - 1s - loss: 180.3523 - loglik: -1.8130e+02 - logprior: 1.2669
Epoch 10/10
10/10 - 1s - loss: 179.8615 - loglik: -1.8093e+02 - logprior: 1.3881
Fitted a model with MAP estimate = -179.4214
Time for alignment: 46.0271
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.0049 - loglik: -2.3253e+02 - logprior: -3.7343e+01
Epoch 2/10
10/10 - 1s - loss: 230.0808 - loglik: -2.2024e+02 - logprior: -9.5830e+00
Epoch 3/10
10/10 - 1s - loss: 212.6825 - loglik: -2.0795e+02 - logprior: -4.3723e+00
Epoch 4/10
10/10 - 1s - loss: 201.3908 - loglik: -1.9840e+02 - logprior: -2.5773e+00
Epoch 5/10
10/10 - 1s - loss: 195.5089 - loglik: -1.9310e+02 - logprior: -1.9535e+00
Epoch 6/10
10/10 - 1s - loss: 192.1096 - loglik: -1.8988e+02 - logprior: -1.8343e+00
Epoch 7/10
10/10 - 1s - loss: 190.7020 - loglik: -1.8890e+02 - logprior: -1.4204e+00
Epoch 8/10
10/10 - 1s - loss: 189.8518 - loglik: -1.8858e+02 - logprior: -9.2735e-01
Epoch 9/10
10/10 - 1s - loss: 189.0791 - loglik: -1.8798e+02 - logprior: -7.5750e-01
Epoch 10/10
10/10 - 1s - loss: 188.6169 - loglik: -1.8760e+02 - logprior: -7.0096e-01
Fitted a model with MAP estimate = -188.1554
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (29, 1), (30, 3), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 260.4048 - loglik: -2.1115e+02 - logprior: -4.9204e+01
Epoch 2/2
10/10 - 1s - loss: 207.2332 - loglik: -1.9258e+02 - logprior: -1.4472e+01
Fitted a model with MAP estimate = -195.9198
expansions: []
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 234.5516 - loglik: -1.9252e+02 - logprior: -4.2003e+01
Epoch 2/2
10/10 - 1s - loss: 203.9777 - loglik: -1.8796e+02 - logprior: -1.5986e+01
Fitted a model with MAP estimate = -198.7623
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.3072 - loglik: -1.8674e+02 - logprior: -3.7512e+01
Epoch 2/10
10/10 - 1s - loss: 194.3496 - loglik: -1.8479e+02 - logprior: -9.4074e+00
Epoch 3/10
10/10 - 1s - loss: 186.8601 - loglik: -1.8336e+02 - logprior: -3.1725e+00
Epoch 4/10
10/10 - 1s - loss: 183.7790 - loglik: -1.8243e+02 - logprior: -9.3795e-01
Epoch 5/10
10/10 - 1s - loss: 182.3642 - loglik: -1.8215e+02 - logprior: 0.1580
Epoch 6/10
10/10 - 1s - loss: 181.4822 - loglik: -1.8188e+02 - logprior: 0.7272
Epoch 7/10
10/10 - 1s - loss: 180.7719 - loglik: -1.8149e+02 - logprior: 1.0293
Epoch 8/10
10/10 - 1s - loss: 180.2005 - loglik: -1.8111e+02 - logprior: 1.2066
Epoch 9/10
10/10 - 1s - loss: 179.9081 - loglik: -1.8095e+02 - logprior: 1.3262
Epoch 10/10
10/10 - 1s - loss: 179.5274 - loglik: -1.8069e+02 - logprior: 1.4434
Fitted a model with MAP estimate = -179.1752
Time for alignment: 47.5667
Computed alignments with likelihoods: ['-179.1312', '-179.4214', '-179.1752']
Best model has likelihood: -179.1312
time for generating output: 0.2044
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7947780678851175
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabde5fbee0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf78de5e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac5dbd29d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5dbd8970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab0db13c10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa95f2b340>, <__main__.SimpleDirichletPrior object at 0x7fab0d6ffa90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 948.8922 - loglik: -9.4711e+02 - logprior: -1.3292e+00
Epoch 2/10
43/43 - 31s - loss: 830.4583 - loglik: -8.2712e+02 - logprior: -1.9333e+00
Epoch 3/10
43/43 - 31s - loss: 817.8987 - loglik: -8.1423e+02 - logprior: -1.9489e+00
Epoch 4/10
43/43 - 31s - loss: 813.9421 - loglik: -8.1035e+02 - logprior: -1.8885e+00
Epoch 5/10
43/43 - 31s - loss: 812.2720 - loglik: -8.0872e+02 - logprior: -1.8931e+00
Epoch 6/10
43/43 - 31s - loss: 811.6215 - loglik: -8.0817e+02 - logprior: -1.8866e+00
Epoch 7/10
43/43 - 32s - loss: 811.4537 - loglik: -8.0803e+02 - logprior: -1.9150e+00
Epoch 8/10
43/43 - 31s - loss: 809.9512 - loglik: -8.0659e+02 - logprior: -1.9383e+00
Epoch 9/10
43/43 - 31s - loss: 810.3826 - loglik: -8.0707e+02 - logprior: -1.9244e+00
Fitted a model with MAP estimate = -817.4620
expansions: [(8, 1), (13, 1), (16, 1), (21, 3), (22, 2), (23, 1), (24, 1), (30, 1), (40, 1), (41, 1), (42, 1), (43, 1), (45, 2), (46, 1), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 2), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (121, 1), (123, 1), (125, 1), (130, 2), (132, 2), (148, 2), (153, 1), (154, 1), (155, 3), (157, 2), (181, 2), (183, 3), (184, 1), (186, 1), (187, 1), (188, 2), (203, 1), (205, 4), (206, 1), (207, 1), (208, 1), (209, 1), (219, 1), (221, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 2), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 50s - loss: 834.1926 - loglik: -8.3175e+02 - logprior: -2.2682e+00
Epoch 2/2
43/43 - 47s - loss: 788.7025 - loglik: -7.8672e+02 - logprior: -1.2210e+00
Fitted a model with MAP estimate = -782.8669
expansions: [(0, 2)]
discards: [ 26  27  59 109 118 136 169 172 206 233 238 247 268 269 300 315 325 358
 360]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 799.1124 - loglik: -7.9753e+02 - logprior: -1.1791e+00
Epoch 2/2
43/43 - 44s - loss: 783.8856 - loglik: -7.8185e+02 - logprior: -4.2417e-01
Fitted a model with MAP estimate = -778.2941
expansions: [(258, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 779.0636 - loglik: -7.7785e+02 - logprior: -9.2757e-01
Epoch 2/10
61/61 - 60s - loss: 762.3070 - loglik: -7.6087e+02 - logprior: -4.8274e-01
Epoch 3/10
61/61 - 61s - loss: 755.2429 - loglik: -7.5330e+02 - logprior: -4.6048e-01
Epoch 4/10
61/61 - 61s - loss: 754.6846 - loglik: -7.5239e+02 - logprior: -3.9762e-01
Epoch 5/10
61/61 - 61s - loss: 752.5432 - loglik: -7.5018e+02 - logprior: -2.9879e-01
Epoch 6/10
61/61 - 61s - loss: 751.5644 - loglik: -7.4911e+02 - logprior: -2.5758e-01
Epoch 7/10
61/61 - 61s - loss: 750.9825 - loglik: -7.4840e+02 - logprior: -1.8536e-01
Epoch 8/10
61/61 - 61s - loss: 750.8491 - loglik: -7.4850e+02 - logprior: -9.7264e-02
Epoch 9/10
61/61 - 61s - loss: 748.8931 - loglik: -7.4668e+02 - logprior: -5.8355e-02
Epoch 10/10
61/61 - 61s - loss: 747.7086 - loglik: -7.4559e+02 - logprior: 0.0566
Fitted a model with MAP estimate = -744.2668
Time for alignment: 1395.9736
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 947.3621 - loglik: -9.4569e+02 - logprior: -1.3125e+00
Epoch 2/10
43/43 - 31s - loss: 829.1873 - loglik: -8.2585e+02 - logprior: -1.9626e+00
Epoch 3/10
43/43 - 31s - loss: 814.9114 - loglik: -8.1135e+02 - logprior: -1.9754e+00
Epoch 4/10
43/43 - 31s - loss: 813.5761 - loglik: -8.1012e+02 - logprior: -1.9233e+00
Epoch 5/10
43/43 - 31s - loss: 811.7852 - loglik: -8.0840e+02 - logprior: -1.9024e+00
Epoch 6/10
43/43 - 31s - loss: 810.7731 - loglik: -8.0745e+02 - logprior: -1.9288e+00
Epoch 7/10
43/43 - 31s - loss: 809.9496 - loglik: -8.0667e+02 - logprior: -1.9477e+00
Epoch 8/10
43/43 - 31s - loss: 809.8483 - loglik: -8.0663e+02 - logprior: -1.9579e+00
Epoch 9/10
43/43 - 31s - loss: 809.9982 - loglik: -8.0680e+02 - logprior: -1.9718e+00
Fitted a model with MAP estimate = -815.7867
expansions: [(8, 1), (13, 1), (16, 1), (21, 3), (22, 1), (23, 1), (24, 2), (29, 1), (36, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 2), (98, 1), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 1), (150, 1), (153, 2), (156, 3), (157, 2), (159, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 2), (188, 2), (201, 1), (203, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 3), (242, 1), (245, 1), (250, 1), (257, 1), (260, 2), (270, 3), (271, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 826.1293 - loglik: -8.2352e+02 - logprior: -2.2380e+00
Epoch 2/2
43/43 - 47s - loss: 786.7067 - loglik: -7.8407e+02 - logprior: -1.0712e+00
Fitted a model with MAP estimate = -778.9627
expansions: [(0, 2)]
discards: [ 24  59 119 127 169 196 201 204 239 240 241 266 267 268 299 357]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 801.0159 - loglik: -7.9968e+02 - logprior: -1.1719e+00
Epoch 2/2
43/43 - 44s - loss: 785.7617 - loglik: -7.8477e+02 - logprior: -4.1590e-01
Fitted a model with MAP estimate = -783.6925
expansions: [(257, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 777.1567 - loglik: -7.7598e+02 - logprior: -9.3090e-01
Epoch 2/10
61/61 - 60s - loss: 762.8772 - loglik: -7.6146e+02 - logprior: -4.9545e-01
Epoch 3/10
61/61 - 60s - loss: 756.4458 - loglik: -7.5449e+02 - logprior: -4.5209e-01
Epoch 4/10
61/61 - 61s - loss: 753.5409 - loglik: -7.5122e+02 - logprior: -3.8169e-01
Epoch 5/10
61/61 - 62s - loss: 752.3217 - loglik: -7.4987e+02 - logprior: -3.4622e-01
Epoch 6/10
61/61 - 61s - loss: 751.8820 - loglik: -7.4937e+02 - logprior: -2.6427e-01
Epoch 7/10
61/61 - 62s - loss: 751.2764 - loglik: -7.4860e+02 - logprior: -1.7734e-01
Epoch 8/10
61/61 - 60s - loss: 750.2725 - loglik: -7.4780e+02 - logprior: -1.3066e-01
Epoch 9/10
61/61 - 61s - loss: 748.4905 - loglik: -7.4622e+02 - logprior: -3.4049e-02
Epoch 10/10
61/61 - 61s - loss: 748.7575 - loglik: -7.4656e+02 - logprior: 0.0556
Fitted a model with MAP estimate = -743.8938
Time for alignment: 1394.1958
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 947.2701 - loglik: -9.4538e+02 - logprior: -1.3240e+00
Epoch 2/10
43/43 - 31s - loss: 828.2753 - loglik: -8.2481e+02 - logprior: -1.9202e+00
Epoch 3/10
43/43 - 31s - loss: 816.0306 - loglik: -8.1231e+02 - logprior: -1.9348e+00
Epoch 4/10
43/43 - 31s - loss: 813.0164 - loglik: -8.0940e+02 - logprior: -1.9065e+00
Epoch 5/10
43/43 - 31s - loss: 811.9871 - loglik: -8.0845e+02 - logprior: -1.8932e+00
Epoch 6/10
43/43 - 31s - loss: 811.0045 - loglik: -8.0758e+02 - logprior: -1.8925e+00
Epoch 7/10
43/43 - 31s - loss: 810.5107 - loglik: -8.0709e+02 - logprior: -1.9562e+00
Epoch 8/10
43/43 - 31s - loss: 809.3098 - loglik: -8.0598e+02 - logprior: -1.9612e+00
Epoch 9/10
43/43 - 31s - loss: 809.0820 - loglik: -8.0587e+02 - logprior: -1.9120e+00
Epoch 10/10
43/43 - 31s - loss: 808.7934 - loglik: -8.0560e+02 - logprior: -1.9123e+00
Fitted a model with MAP estimate = -818.5922
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 2), (47, 1), (57, 1), (60, 1), (61, 1), (62, 2), (79, 1), (80, 1), (81, 1), (82, 1), (91, 1), (92, 2), (94, 1), (96, 1), (98, 1), (99, 1), (104, 2), (120, 1), (123, 1), (124, 1), (129, 2), (131, 1), (133, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 3), (157, 2), (168, 1), (181, 1), (183, 2), (185, 2), (186, 2), (187, 2), (188, 1), (203, 1), (205, 4), (206, 1), (207, 1), (208, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 2), (242, 1), (245, 2), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 823.4555 - loglik: -8.2078e+02 - logprior: -2.2152e+00
Epoch 2/2
43/43 - 48s - loss: 786.7557 - loglik: -7.8453e+02 - logprior: -1.1638e+00
Fitted a model with MAP estimate = -782.3980
expansions: [(0, 2), (295, 1)]
discards: [ 30  59  78 136 166 172 198 206 237 247 270 300 325 358 360]
Re-initialized the encoder parameters.
Fitting a model of length 358 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 49s - loss: 802.1336 - loglik: -8.0074e+02 - logprior: -1.2094e+00
Epoch 2/2
43/43 - 45s - loss: 785.3112 - loglik: -7.8432e+02 - logprior: -4.5443e-01
Fitted a model with MAP estimate = -782.3894
expansions: []
discards: [  0 116 237 287]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 778.8414 - loglik: -7.7751e+02 - logprior: -9.0586e-01
Epoch 2/10
61/61 - 62s - loss: 761.3301 - loglik: -7.5838e+02 - logprior: -4.7529e-01
Epoch 3/10
61/61 - 61s - loss: 755.7648 - loglik: -7.5252e+02 - logprior: -4.3134e-01
Epoch 4/10
61/61 - 61s - loss: 755.0029 - loglik: -7.5164e+02 - logprior: -3.8244e-01
Epoch 5/10
61/61 - 61s - loss: 753.3023 - loglik: -7.5004e+02 - logprior: -2.8183e-01
Epoch 6/10
61/61 - 61s - loss: 751.5089 - loglik: -7.4846e+02 - logprior: -2.1470e-01
Epoch 7/10
61/61 - 62s - loss: 750.5458 - loglik: -7.4758e+02 - logprior: -1.4972e-01
Epoch 8/10
61/61 - 60s - loss: 749.9917 - loglik: -7.4726e+02 - logprior: -9.6693e-02
Epoch 9/10
61/61 - 61s - loss: 749.1559 - loglik: -7.4685e+02 - logprior: -1.0182e-02
Epoch 10/10
61/61 - 61s - loss: 747.4099 - loglik: -7.4518e+02 - logprior: 0.0292
Fitted a model with MAP estimate = -744.4531
Time for alignment: 1437.0425
Computed alignments with likelihoods: ['-744.2668', '-743.8938', '-744.4531']
Best model has likelihood: -743.8938
time for generating output: 0.4786
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.7765244977088473
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaebbfb250>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaaff431f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faba153a6d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2fa53e50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac08c49eb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faba1f4c760>, <__main__.SimpleDirichletPrior object at 0x7fab300c1ee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 249.5519 - loglik: -1.9281e+02 - logprior: -5.6705e+01
Epoch 2/10
10/10 - 2s - loss: 188.7956 - loglik: -1.7314e+02 - logprior: -1.5614e+01
Epoch 3/10
10/10 - 2s - loss: 161.4996 - loglik: -1.5382e+02 - logprior: -7.6413e+00
Epoch 4/10
10/10 - 2s - loss: 152.1525 - loglik: -1.4733e+02 - logprior: -4.6523e+00
Epoch 5/10
10/10 - 2s - loss: 149.0825 - loglik: -1.4544e+02 - logprior: -3.3418e+00
Epoch 6/10
10/10 - 1s - loss: 146.7293 - loglik: -1.4393e+02 - logprior: -2.4816e+00
Epoch 7/10
10/10 - 1s - loss: 147.1120 - loglik: -1.4483e+02 - logprior: -1.9562e+00
Fitted a model with MAP estimate = -145.6724
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.8227 - loglik: -1.4458e+02 - logprior: -6.4150e+01
Epoch 2/2
10/10 - 2s - loss: 169.4627 - loglik: -1.4221e+02 - logprior: -2.7000e+01
Fitted a model with MAP estimate = -162.6033
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 193.7585 - loglik: -1.4161e+02 - logprior: -5.2075e+01
Epoch 2/2
10/10 - 1s - loss: 154.7856 - loglik: -1.4014e+02 - logprior: -1.4486e+01
Fitted a model with MAP estimate = -149.2615
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.2383 - loglik: -1.4163e+02 - logprior: -6.2490e+01
Epoch 2/10
10/10 - 2s - loss: 162.5149 - loglik: -1.4064e+02 - logprior: -2.1739e+01
Epoch 3/10
10/10 - 2s - loss: 149.8790 - loglik: -1.4089e+02 - logprior: -8.7103e+00
Epoch 4/10
10/10 - 2s - loss: 144.9588 - loglik: -1.4062e+02 - logprior: -4.0266e+00
Epoch 5/10
10/10 - 2s - loss: 143.5523 - loglik: -1.4105e+02 - logprior: -2.1879e+00
Epoch 6/10
10/10 - 2s - loss: 141.7583 - loglik: -1.4010e+02 - logprior: -1.3171e+00
Epoch 7/10
10/10 - 2s - loss: 141.9293 - loglik: -1.4071e+02 - logprior: -8.5293e-01
Fitted a model with MAP estimate = -141.0561
Time for alignment: 44.6685
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 250.4607 - loglik: -1.9371e+02 - logprior: -5.6706e+01
Epoch 2/10
10/10 - 1s - loss: 187.4985 - loglik: -1.7185e+02 - logprior: -1.5613e+01
Epoch 3/10
10/10 - 1s - loss: 162.2162 - loglik: -1.5454e+02 - logprior: -7.6247e+00
Epoch 4/10
10/10 - 2s - loss: 152.4102 - loglik: -1.4758e+02 - logprior: -4.6270e+00
Epoch 5/10
10/10 - 2s - loss: 148.9995 - loglik: -1.4540e+02 - logprior: -3.2676e+00
Epoch 6/10
10/10 - 2s - loss: 147.1931 - loglik: -1.4434e+02 - logprior: -2.5199e+00
Epoch 7/10
10/10 - 2s - loss: 145.7652 - loglik: -1.4337e+02 - logprior: -2.0783e+00
Epoch 8/10
10/10 - 1s - loss: 146.4583 - loglik: -1.4433e+02 - logprior: -1.7897e+00
Fitted a model with MAP estimate = -145.1149
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.1918 - loglik: -1.4490e+02 - logprior: -6.4191e+01
Epoch 2/2
10/10 - 2s - loss: 169.2184 - loglik: -1.4201e+02 - logprior: -2.6948e+01
Fitted a model with MAP estimate = -162.7466
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.2616 - loglik: -1.4202e+02 - logprior: -5.2145e+01
Epoch 2/2
10/10 - 2s - loss: 155.1439 - loglik: -1.4065e+02 - logprior: -1.4454e+01
Fitted a model with MAP estimate = -149.7494
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 204.1474 - loglik: -1.4179e+02 - logprior: -6.2346e+01
Epoch 2/10
10/10 - 2s - loss: 162.9494 - loglik: -1.4138e+02 - logprior: -2.1527e+01
Epoch 3/10
10/10 - 2s - loss: 149.5732 - loglik: -1.4087e+02 - logprior: -8.5303e+00
Epoch 4/10
10/10 - 1s - loss: 145.4443 - loglik: -1.4120e+02 - logprior: -3.9404e+00
Epoch 5/10
10/10 - 1s - loss: 143.3809 - loglik: -1.4092e+02 - logprior: -2.1331e+00
Epoch 6/10
10/10 - 1s - loss: 142.3234 - loglik: -1.4070e+02 - logprior: -1.2737e+00
Epoch 7/10
10/10 - 2s - loss: 141.3172 - loglik: -1.4013e+02 - logprior: -8.0759e-01
Epoch 8/10
10/10 - 1s - loss: 140.8542 - loglik: -1.3996e+02 - logprior: -4.8391e-01
Epoch 9/10
10/10 - 1s - loss: 141.2597 - loglik: -1.4061e+02 - logprior: -2.1171e-01
Fitted a model with MAP estimate = -140.2274
Time for alignment: 49.9850
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 249.9903 - loglik: -1.9318e+02 - logprior: -5.6702e+01
Epoch 2/10
10/10 - 2s - loss: 188.2732 - loglik: -1.7261e+02 - logprior: -1.5604e+01
Epoch 3/10
10/10 - 1s - loss: 161.9083 - loglik: -1.5426e+02 - logprior: -7.6241e+00
Epoch 4/10
10/10 - 2s - loss: 152.6790 - loglik: -1.4798e+02 - logprior: -4.5544e+00
Epoch 5/10
10/10 - 2s - loss: 149.1383 - loglik: -1.4572e+02 - logprior: -3.1012e+00
Epoch 6/10
10/10 - 2s - loss: 146.9940 - loglik: -1.4441e+02 - logprior: -2.2301e+00
Epoch 7/10
10/10 - 2s - loss: 146.3971 - loglik: -1.4437e+02 - logprior: -1.6914e+00
Epoch 8/10
10/10 - 1s - loss: 145.0659 - loglik: -1.4328e+02 - logprior: -1.4477e+00
Epoch 9/10
10/10 - 2s - loss: 145.3523 - loglik: -1.4370e+02 - logprior: -1.2870e+00
Fitted a model with MAP estimate = -144.7064
expansions: [(13, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 209.9472 - loglik: -1.4567e+02 - logprior: -6.4159e+01
Epoch 2/2
10/10 - 2s - loss: 169.3307 - loglik: -1.4210e+02 - logprior: -2.6973e+01
Fitted a model with MAP estimate = -163.4693
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.0031 - loglik: -1.4268e+02 - logprior: -5.2260e+01
Epoch 2/2
10/10 - 2s - loss: 156.0268 - loglik: -1.4151e+02 - logprior: -1.4449e+01
Fitted a model with MAP estimate = -150.3560
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.4762 - loglik: -1.4151e+02 - logprior: -6.1899e+01
Epoch 2/10
10/10 - 2s - loss: 162.8538 - loglik: -1.4225e+02 - logprior: -2.0446e+01
Epoch 3/10
10/10 - 1s - loss: 149.3188 - loglik: -1.4091e+02 - logprior: -8.1084e+00
Epoch 4/10
10/10 - 1s - loss: 145.6471 - loglik: -1.4138e+02 - logprior: -3.9046e+00
Epoch 5/10
10/10 - 2s - loss: 144.3407 - loglik: -1.4186e+02 - logprior: -2.1369e+00
Epoch 6/10
10/10 - 2s - loss: 141.7980 - loglik: -1.4017e+02 - logprior: -1.2703e+00
Epoch 7/10
10/10 - 1s - loss: 142.8836 - loglik: -1.4169e+02 - logprior: -8.1304e-01
Fitted a model with MAP estimate = -141.5378
Time for alignment: 47.0609
Computed alignments with likelihoods: ['-141.0561', '-140.2274', '-141.5378']
Best model has likelihood: -140.2274
time for generating output: 0.1183
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9472954230235784
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac3be8d790>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab16001ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac2adb8d90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaf4302e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab302295b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabcd63e8e0>, <__main__.SimpleDirichletPrior object at 0x7fac0046ec70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 538.6624 - loglik: -4.7174e+02 - logprior: -6.6900e+01
Epoch 2/10
10/10 - 2s - loss: 438.7152 - loglik: -4.2525e+02 - logprior: -1.3451e+01
Epoch 3/10
10/10 - 2s - loss: 384.6492 - loglik: -3.8015e+02 - logprior: -4.4373e+00
Epoch 4/10
10/10 - 2s - loss: 353.3656 - loglik: -3.5162e+02 - logprior: -1.5624e+00
Epoch 5/10
10/10 - 2s - loss: 339.4831 - loglik: -3.3880e+02 - logprior: -3.0623e-01
Epoch 6/10
10/10 - 2s - loss: 331.5107 - loglik: -3.3161e+02 - logprior: 0.5296
Epoch 7/10
10/10 - 2s - loss: 327.9664 - loglik: -3.2866e+02 - logprior: 1.1255
Epoch 8/10
10/10 - 2s - loss: 326.1201 - loglik: -3.2717e+02 - logprior: 1.5086
Epoch 9/10
10/10 - 2s - loss: 325.3144 - loglik: -3.2657e+02 - logprior: 1.7645
Epoch 10/10
10/10 - 2s - loss: 324.7140 - loglik: -3.2620e+02 - logprior: 2.0151
Fitted a model with MAP estimate = -323.8065
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (28, 1), (37, 1), (38, 2), (39, 2), (55, 1), (65, 2), (78, 3), (79, 2), (80, 2), (89, 1), (90, 2), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 407.7700 - loglik: -3.3216e+02 - logprior: -7.5578e+01
Epoch 2/2
10/10 - 3s - loss: 339.4013 - loglik: -3.1175e+02 - logprior: -2.7622e+01
Fitted a model with MAP estimate = -327.1980
expansions: [(0, 1), (33, 1)]
discards: [  0  11  23  49 114]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 367.4106 - loglik: -3.0872e+02 - logprior: -5.8524e+01
Epoch 2/2
10/10 - 3s - loss: 314.1131 - loglik: -3.0349e+02 - logprior: -1.0572e+01
Fitted a model with MAP estimate = -305.1676
expansions: [(124, 1)]
discards: [159]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 361.4769 - loglik: -3.0423e+02 - logprior: -5.7047e+01
Epoch 2/10
10/10 - 3s - loss: 310.7924 - loglik: -3.0134e+02 - logprior: -9.3808e+00
Epoch 3/10
10/10 - 3s - loss: 298.5704 - loglik: -2.9879e+02 - logprior: 0.3407
Epoch 4/10
10/10 - 3s - loss: 292.9492 - loglik: -2.9709e+02 - logprior: 4.4159
Epoch 5/10
10/10 - 3s - loss: 289.6230 - loglik: -2.9604e+02 - logprior: 6.8268
Epoch 6/10
10/10 - 3s - loss: 288.4148 - loglik: -2.9632e+02 - logprior: 8.3466
Epoch 7/10
10/10 - 3s - loss: 286.7424 - loglik: -2.9566e+02 - logprior: 9.3804
Epoch 8/10
10/10 - 3s - loss: 286.6778 - loglik: -2.9636e+02 - logprior: 10.1528
Epoch 9/10
10/10 - 3s - loss: 285.4602 - loglik: -2.9572e+02 - logprior: 10.7393
Epoch 10/10
10/10 - 3s - loss: 285.5549 - loglik: -2.9628e+02 - logprior: 11.2115
Fitted a model with MAP estimate = -284.1210
Time for alignment: 88.8100
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.6490 - loglik: -4.7171e+02 - logprior: -6.6901e+01
Epoch 2/10
10/10 - 2s - loss: 439.4695 - loglik: -4.2598e+02 - logprior: -1.3461e+01
Epoch 3/10
10/10 - 2s - loss: 385.2939 - loglik: -3.8071e+02 - logprior: -4.5269e+00
Epoch 4/10
10/10 - 2s - loss: 354.0925 - loglik: -3.5220e+02 - logprior: -1.7393e+00
Epoch 5/10
10/10 - 2s - loss: 339.6811 - loglik: -3.3889e+02 - logprior: -4.7628e-01
Epoch 6/10
10/10 - 2s - loss: 331.9670 - loglik: -3.3179e+02 - logprior: 0.2155
Epoch 7/10
10/10 - 2s - loss: 327.9557 - loglik: -3.2835e+02 - logprior: 0.8139
Epoch 8/10
10/10 - 2s - loss: 325.2147 - loglik: -3.2595e+02 - logprior: 1.1632
Epoch 9/10
10/10 - 2s - loss: 324.1038 - loglik: -3.2514e+02 - logprior: 1.4929
Epoch 10/10
10/10 - 2s - loss: 323.0562 - loglik: -3.2430e+02 - logprior: 1.7160
Fitted a model with MAP estimate = -322.2443
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (55, 1), (66, 1), (71, 2), (77, 2), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 405.7907 - loglik: -3.2992e+02 - logprior: -7.5837e+01
Epoch 2/2
10/10 - 3s - loss: 338.1238 - loglik: -3.1028e+02 - logprior: -2.7819e+01
Fitted a model with MAP estimate = -325.8079
expansions: [(0, 2), (48, 1), (59, 1), (127, 1)]
discards: [  0  22  45  84 113 160]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 364.6638 - loglik: -3.0602e+02 - logprior: -5.8545e+01
Epoch 2/2
10/10 - 3s - loss: 310.3827 - loglik: -2.9957e+02 - logprior: -1.0621e+01
Fitted a model with MAP estimate = -300.7007
expansions: [(81, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 377.4183 - loglik: -3.0511e+02 - logprior: -7.2197e+01
Epoch 2/10
10/10 - 3s - loss: 319.3151 - loglik: -3.0017e+02 - logprior: -1.9110e+01
Epoch 3/10
10/10 - 3s - loss: 300.1104 - loglik: -2.9776e+02 - logprior: -2.2966e+00
Epoch 4/10
10/10 - 3s - loss: 292.8250 - loglik: -2.9654e+02 - logprior: 3.8082
Epoch 5/10
10/10 - 3s - loss: 288.7826 - loglik: -2.9505e+02 - logprior: 6.4280
Epoch 6/10
10/10 - 3s - loss: 286.2392 - loglik: -2.9386e+02 - logprior: 7.8993
Epoch 7/10
10/10 - 3s - loss: 284.7443 - loglik: -2.9322e+02 - logprior: 8.8493
Epoch 8/10
10/10 - 3s - loss: 284.1889 - loglik: -2.9341e+02 - logprior: 9.6176
Epoch 9/10
10/10 - 3s - loss: 283.4624 - loglik: -2.9339e+02 - logprior: 10.3228
Epoch 10/10
10/10 - 3s - loss: 282.6850 - loglik: -2.9323e+02 - logprior: 10.9474
Fitted a model with MAP estimate = -282.0597
Time for alignment: 89.2623
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.7383 - loglik: -4.7178e+02 - logprior: -6.6890e+01
Epoch 2/10
10/10 - 2s - loss: 440.4115 - loglik: -4.2695e+02 - logprior: -1.3426e+01
Epoch 3/10
10/10 - 2s - loss: 386.0324 - loglik: -3.8158e+02 - logprior: -4.4128e+00
Epoch 4/10
10/10 - 2s - loss: 355.0386 - loglik: -3.5320e+02 - logprior: -1.7112e+00
Epoch 5/10
10/10 - 2s - loss: 340.9930 - loglik: -3.4026e+02 - logprior: -4.1402e-01
Epoch 6/10
10/10 - 2s - loss: 334.8658 - loglik: -3.3503e+02 - logprior: 0.5676
Epoch 7/10
10/10 - 2s - loss: 330.7719 - loglik: -3.3153e+02 - logprior: 1.1692
Epoch 8/10
10/10 - 2s - loss: 327.8589 - loglik: -3.2899e+02 - logprior: 1.5546
Epoch 9/10
10/10 - 2s - loss: 326.6307 - loglik: -3.2795e+02 - logprior: 1.7800
Epoch 10/10
10/10 - 2s - loss: 325.7962 - loglik: -3.2729e+02 - logprior: 1.9925
Fitted a model with MAP estimate = -324.5058
expansions: [(11, 2), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 1), (39, 2), (55, 1), (68, 2), (77, 3), (78, 2), (79, 2), (103, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 406.6185 - loglik: -3.3105e+02 - logprior: -7.5533e+01
Epoch 2/2
10/10 - 3s - loss: 338.0968 - loglik: -3.1073e+02 - logprior: -2.7345e+01
Fitted a model with MAP estimate = -326.1360
expansions: [(0, 2), (112, 3)]
discards: [  0  22  49 159]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 365.9449 - loglik: -3.0735e+02 - logprior: -5.8444e+01
Epoch 2/2
10/10 - 3s - loss: 311.0645 - loglik: -3.0033e+02 - logprior: -1.0540e+01
Fitted a model with MAP estimate = -301.9947
expansions: [(126, 1)]
discards: [  0  93 111 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 378.6586 - loglik: -3.0652e+02 - logprior: -7.2039e+01
Epoch 2/10
10/10 - 3s - loss: 321.9564 - loglik: -3.0286e+02 - logprior: -1.8937e+01
Epoch 3/10
10/10 - 3s - loss: 302.5669 - loglik: -2.9996e+02 - logprior: -2.2937e+00
Epoch 4/10
10/10 - 3s - loss: 295.1996 - loglik: -2.9862e+02 - logprior: 3.7854
Epoch 5/10
10/10 - 3s - loss: 291.5313 - loglik: -2.9759e+02 - logprior: 6.4422
Epoch 6/10
10/10 - 3s - loss: 289.5895 - loglik: -2.9710e+02 - logprior: 7.9279
Epoch 7/10
10/10 - 3s - loss: 288.0558 - loglik: -2.9652e+02 - logprior: 8.9026
Epoch 8/10
10/10 - 3s - loss: 288.1863 - loglik: -2.9743e+02 - logprior: 9.6979
Fitted a model with MAP estimate = -286.8128
Time for alignment: 80.1782
Computed alignments with likelihoods: ['-284.1210', '-282.0597', '-286.8128']
Best model has likelihood: -282.0597
time for generating output: 0.2057
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.851828890266584
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab04e54880>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba16679d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaafaa16d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1211970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba0a37fd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabde5fb0a0>, <__main__.SimpleDirichletPrior object at 0x7fabe6f23bb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 292.6583 - loglik: -2.8270e+02 - logprior: -9.9300e+00
Epoch 2/10
12/12 - 2s - loss: 254.4670 - loglik: -2.5201e+02 - logprior: -2.4145e+00
Epoch 3/10
12/12 - 2s - loss: 225.2063 - loglik: -2.2325e+02 - logprior: -1.7339e+00
Epoch 4/10
12/12 - 2s - loss: 212.7999 - loglik: -2.1049e+02 - logprior: -1.8416e+00
Epoch 5/10
12/12 - 2s - loss: 208.3237 - loglik: -2.0574e+02 - logprior: -1.9257e+00
Epoch 6/10
12/12 - 2s - loss: 206.6209 - loglik: -2.0415e+02 - logprior: -1.8453e+00
Epoch 7/10
12/12 - 2s - loss: 205.1528 - loglik: -2.0283e+02 - logprior: -1.7888e+00
Epoch 8/10
12/12 - 2s - loss: 205.5544 - loglik: -2.0329e+02 - logprior: -1.7974e+00
Fitted a model with MAP estimate = -204.3172
expansions: [(8, 1), (9, 2), (10, 5), (11, 2), (21, 1), (29, 1), (36, 3), (49, 2), (50, 3), (51, 1), (57, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 225.0854 - loglik: -2.1352e+02 - logprior: -1.1496e+01
Epoch 2/2
12/12 - 2s - loss: 202.3515 - loglik: -1.9749e+02 - logprior: -4.8187e+00
Fitted a model with MAP estimate = -196.4089
expansions: [(0, 5)]
discards: [ 0  9 13 14 49 64 79 82 83]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 205.7028 - loglik: -1.9657e+02 - logprior: -9.0947e+00
Epoch 2/2
12/12 - 2s - loss: 193.5435 - loglik: -1.9112e+02 - logprior: -2.3072e+00
Fitted a model with MAP estimate = -190.9436
expansions: [(16, 2)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 201.7322 - loglik: -1.9278e+02 - logprior: -8.8087e+00
Epoch 2/10
12/12 - 2s - loss: 190.0853 - loglik: -1.8788e+02 - logprior: -2.1849e+00
Epoch 3/10
12/12 - 2s - loss: 186.3891 - loglik: -1.8490e+02 - logprior: -1.4199e+00
Epoch 4/10
12/12 - 2s - loss: 183.2657 - loglik: -1.8171e+02 - logprior: -1.3114e+00
Epoch 5/10
12/12 - 2s - loss: 181.1226 - loglik: -1.7967e+02 - logprior: -1.0322e+00
Epoch 6/10
12/12 - 2s - loss: 180.2805 - loglik: -1.7878e+02 - logprior: -1.0238e+00
Epoch 7/10
12/12 - 2s - loss: 178.7416 - loglik: -1.7732e+02 - logprior: -9.5257e-01
Epoch 8/10
12/12 - 2s - loss: 179.6239 - loglik: -1.7824e+02 - logprior: -9.4076e-01
Fitted a model with MAP estimate = -178.4147
Time for alignment: 61.2202
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.6447 - loglik: -2.8234e+02 - logprior: -9.9299e+00
Epoch 2/10
12/12 - 2s - loss: 254.3209 - loglik: -2.5173e+02 - logprior: -2.4109e+00
Epoch 3/10
12/12 - 2s - loss: 226.4370 - loglik: -2.2448e+02 - logprior: -1.6924e+00
Epoch 4/10
12/12 - 2s - loss: 213.4635 - loglik: -2.1117e+02 - logprior: -1.7917e+00
Epoch 5/10
12/12 - 2s - loss: 208.6893 - loglik: -2.0618e+02 - logprior: -1.8472e+00
Epoch 6/10
12/12 - 2s - loss: 207.0434 - loglik: -2.0472e+02 - logprior: -1.7589e+00
Epoch 7/10
12/12 - 2s - loss: 204.9799 - loglik: -2.0277e+02 - logprior: -1.7209e+00
Epoch 8/10
12/12 - 2s - loss: 204.7201 - loglik: -2.0253e+02 - logprior: -1.7343e+00
Epoch 9/10
12/12 - 2s - loss: 204.0455 - loglik: -2.0189e+02 - logprior: -1.7395e+00
Epoch 10/10
12/12 - 2s - loss: 203.9888 - loglik: -2.0186e+02 - logprior: -1.7217e+00
Fitted a model with MAP estimate = -203.4639
expansions: [(6, 3), (10, 4), (11, 2), (21, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 222.3617 - loglik: -2.1081e+02 - logprior: -1.1529e+01
Epoch 2/2
12/12 - 2s - loss: 199.8519 - loglik: -1.9501e+02 - logprior: -4.7657e+00
Fitted a model with MAP estimate = -194.5407
expansions: [(0, 4)]
discards: [ 0 48 80]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 201.8569 - loglik: -1.9251e+02 - logprior: -9.1678e+00
Epoch 2/2
12/12 - 2s - loss: 189.5361 - loglik: -1.8700e+02 - logprior: -2.4056e+00
Fitted a model with MAP estimate = -186.8708
expansions: []
discards: [ 0  1  2 44]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 202.6510 - loglik: -1.9131e+02 - logprior: -1.1284e+01
Epoch 2/10
12/12 - 2s - loss: 192.4752 - loglik: -1.8855e+02 - logprior: -3.7696e+00
Epoch 3/10
12/12 - 2s - loss: 186.9074 - loglik: -1.8464e+02 - logprior: -1.9605e+00
Epoch 4/10
12/12 - 2s - loss: 184.5307 - loglik: -1.8261e+02 - logprior: -1.3924e+00
Epoch 5/10
12/12 - 2s - loss: 181.7305 - loglik: -1.7990e+02 - logprior: -1.1738e+00
Epoch 6/10
12/12 - 2s - loss: 182.1456 - loglik: -1.8035e+02 - logprior: -1.1477e+00
Fitted a model with MAP estimate = -180.3400
Time for alignment: 59.9256
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.7112 - loglik: -2.8269e+02 - logprior: -9.9316e+00
Epoch 2/10
12/12 - 2s - loss: 254.1099 - loglik: -2.5165e+02 - logprior: -2.4199e+00
Epoch 3/10
12/12 - 2s - loss: 225.6104 - loglik: -2.2369e+02 - logprior: -1.7137e+00
Epoch 4/10
12/12 - 2s - loss: 214.5964 - loglik: -2.1232e+02 - logprior: -1.7735e+00
Epoch 5/10
12/12 - 2s - loss: 208.4777 - loglik: -2.0600e+02 - logprior: -1.8137e+00
Epoch 6/10
12/12 - 2s - loss: 206.4424 - loglik: -2.0410e+02 - logprior: -1.7298e+00
Epoch 7/10
12/12 - 2s - loss: 205.1465 - loglik: -2.0294e+02 - logprior: -1.6766e+00
Epoch 8/10
12/12 - 2s - loss: 204.5315 - loglik: -2.0239e+02 - logprior: -1.6738e+00
Epoch 9/10
12/12 - 2s - loss: 203.6431 - loglik: -2.0154e+02 - logprior: -1.6767e+00
Epoch 10/10
12/12 - 2s - loss: 203.4852 - loglik: -2.0139e+02 - logprior: -1.6633e+00
Fitted a model with MAP estimate = -202.9092
expansions: [(6, 3), (10, 4), (11, 2), (21, 1), (36, 3), (49, 1), (50, 3), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 220.4907 - loglik: -2.0883e+02 - logprior: -1.1510e+01
Epoch 2/2
12/12 - 2s - loss: 198.3074 - loglik: -1.9318e+02 - logprior: -4.7853e+00
Fitted a model with MAP estimate = -193.8808
expansions: [(0, 3)]
discards: [ 0 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 199.9523 - loglik: -1.9066e+02 - logprior: -9.0638e+00
Epoch 2/2
12/12 - 2s - loss: 189.5456 - loglik: -1.8679e+02 - logprior: -2.4227e+00
Fitted a model with MAP estimate = -186.1443
expansions: []
discards: [ 0  2 49]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 202.9814 - loglik: -1.9171e+02 - logprior: -1.1196e+01
Epoch 2/10
12/12 - 2s - loss: 192.9237 - loglik: -1.8911e+02 - logprior: -3.6770e+00
Epoch 3/10
12/12 - 2s - loss: 187.6900 - loglik: -1.8559e+02 - logprior: -1.8336e+00
Epoch 4/10
12/12 - 2s - loss: 184.6466 - loglik: -1.8292e+02 - logprior: -1.2837e+00
Epoch 5/10
12/12 - 2s - loss: 183.1609 - loglik: -1.8153e+02 - logprior: -1.0528e+00
Epoch 6/10
12/12 - 2s - loss: 182.5108 - loglik: -1.8086e+02 - logprior: -1.0535e+00
Epoch 7/10
12/12 - 2s - loss: 181.0425 - loglik: -1.7948e+02 - logprior: -9.9062e-01
Epoch 8/10
12/12 - 2s - loss: 181.2203 - loglik: -1.7971e+02 - logprior: -9.9122e-01
Fitted a model with MAP estimate = -180.3390
Time for alignment: 61.4081
Computed alignments with likelihoods: ['-178.4147', '-180.3400', '-180.3390']
Best model has likelihood: -178.4147
time for generating output: 0.2033
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.900714478884052
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac768d50a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafc59a250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabb373b1c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4cfd4520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4cfd4340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faafc85d580>, <__main__.SimpleDirichletPrior object at 0x7fab053d6e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 329.1940 - loglik: -2.8888e+02 - logprior: -4.0283e+01
Epoch 2/10
10/10 - 1s - loss: 276.4656 - loglik: -2.6631e+02 - logprior: -1.0130e+01
Epoch 3/10
10/10 - 1s - loss: 250.2288 - loglik: -2.4587e+02 - logprior: -4.2529e+00
Epoch 4/10
10/10 - 1s - loss: 236.8026 - loglik: -2.3438e+02 - logprior: -2.1279e+00
Epoch 5/10
10/10 - 1s - loss: 230.9678 - loglik: -2.2948e+02 - logprior: -1.1522e+00
Epoch 6/10
10/10 - 1s - loss: 227.9703 - loglik: -2.2693e+02 - logprior: -7.2253e-01
Epoch 7/10
10/10 - 1s - loss: 226.2447 - loglik: -2.2540e+02 - logprior: -5.3280e-01
Epoch 8/10
10/10 - 1s - loss: 225.1720 - loglik: -2.2446e+02 - logprior: -3.6790e-01
Epoch 9/10
10/10 - 1s - loss: 224.7867 - loglik: -2.2415e+02 - logprior: -2.4807e-01
Epoch 10/10
10/10 - 1s - loss: 223.9926 - loglik: -2.2343e+02 - logprior: -1.7183e-01
Fitted a model with MAP estimate = -223.4301
expansions: [(0, 3), (6, 1), (7, 2), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.3574 - loglik: -2.2453e+02 - logprior: -5.1796e+01
Epoch 2/2
10/10 - 1s - loss: 231.4778 - loglik: -2.1596e+02 - logprior: -1.5396e+01
Fitted a model with MAP estimate = -223.0023
expansions: []
discards: [ 0  1  2 11 58 59 60 76]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.3027 - loglik: -2.1782e+02 - logprior: -3.7413e+01
Epoch 2/2
10/10 - 1s - loss: 224.3620 - loglik: -2.1493e+02 - logprior: -9.2372e+00
Fitted a model with MAP estimate = -219.7993
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 266.2724 - loglik: -2.1608e+02 - logprior: -5.0182e+01
Epoch 2/10
10/10 - 1s - loss: 228.7449 - loglik: -2.1460e+02 - logprior: -1.4084e+01
Epoch 3/10
10/10 - 1s - loss: 218.9717 - loglik: -2.1357e+02 - logprior: -5.2125e+00
Epoch 4/10
10/10 - 1s - loss: 215.3895 - loglik: -2.1338e+02 - logprior: -1.7288e+00
Epoch 5/10
10/10 - 1s - loss: 213.2442 - loglik: -2.1280e+02 - logprior: -1.2580e-01
Epoch 6/10
10/10 - 2s - loss: 212.3636 - loglik: -2.1273e+02 - logprior: 0.7194
Epoch 7/10
10/10 - 1s - loss: 212.1934 - loglik: -2.1298e+02 - logprior: 1.1653
Epoch 8/10
10/10 - 1s - loss: 211.3943 - loglik: -2.1245e+02 - logprior: 1.4484
Epoch 9/10
10/10 - 1s - loss: 210.9505 - loglik: -2.1219e+02 - logprior: 1.6435
Epoch 10/10
10/10 - 1s - loss: 211.3540 - loglik: -2.1275e+02 - logprior: 1.8140
Fitted a model with MAP estimate = -210.3600
Time for alignment: 55.2492
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.2706 - loglik: -2.8895e+02 - logprior: -4.0282e+01
Epoch 2/10
10/10 - 1s - loss: 276.7165 - loglik: -2.6656e+02 - logprior: -1.0127e+01
Epoch 3/10
10/10 - 1s - loss: 251.3341 - loglik: -2.4697e+02 - logprior: -4.2477e+00
Epoch 4/10
10/10 - 1s - loss: 237.0298 - loglik: -2.3456e+02 - logprior: -2.1471e+00
Epoch 5/10
10/10 - 1s - loss: 231.2171 - loglik: -2.2964e+02 - logprior: -1.2078e+00
Epoch 6/10
10/10 - 1s - loss: 227.3869 - loglik: -2.2631e+02 - logprior: -7.8205e-01
Epoch 7/10
10/10 - 1s - loss: 225.8870 - loglik: -2.2504e+02 - logprior: -5.6075e-01
Epoch 8/10
10/10 - 1s - loss: 224.8303 - loglik: -2.2412e+02 - logprior: -3.9307e-01
Epoch 9/10
10/10 - 1s - loss: 224.2672 - loglik: -2.2366e+02 - logprior: -2.5817e-01
Epoch 10/10
10/10 - 1s - loss: 223.8072 - loglik: -2.2321e+02 - logprior: -2.1183e-01
Fitted a model with MAP estimate = -223.0384
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 275.8954 - loglik: -2.2404e+02 - logprior: -5.1817e+01
Epoch 2/2
10/10 - 2s - loss: 231.4516 - loglik: -2.1591e+02 - logprior: -1.5435e+01
Fitted a model with MAP estimate = -223.2134
expansions: []
discards: [ 0  1  2  9 58 59 60 76]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.0211 - loglik: -2.1763e+02 - logprior: -3.7370e+01
Epoch 2/2
10/10 - 1s - loss: 225.2916 - loglik: -2.1594e+02 - logprior: -9.1924e+00
Fitted a model with MAP estimate = -220.1452
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 266.4175 - loglik: -2.1615e+02 - logprior: -5.0141e+01
Epoch 2/10
10/10 - 1s - loss: 228.7887 - loglik: -2.1453e+02 - logprior: -1.4068e+01
Epoch 3/10
10/10 - 1s - loss: 218.5992 - loglik: -2.1310e+02 - logprior: -5.1795e+00
Epoch 4/10
10/10 - 1s - loss: 215.4646 - loglik: -2.1341e+02 - logprior: -1.7192e+00
Epoch 5/10
10/10 - 1s - loss: 213.1902 - loglik: -2.1277e+02 - logprior: -8.2180e-02
Epoch 6/10
10/10 - 1s - loss: 212.4820 - loglik: -2.1289e+02 - logprior: 0.7667
Epoch 7/10
10/10 - 1s - loss: 212.3355 - loglik: -2.1317e+02 - logprior: 1.2176
Epoch 8/10
10/10 - 1s - loss: 211.8865 - loglik: -2.1298e+02 - logprior: 1.4908
Epoch 9/10
10/10 - 1s - loss: 211.2202 - loglik: -2.1250e+02 - logprior: 1.6860
Epoch 10/10
10/10 - 1s - loss: 210.6592 - loglik: -2.1211e+02 - logprior: 1.8666
Fitted a model with MAP estimate = -210.5147
Time for alignment: 54.3619
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.5881 - loglik: -2.8925e+02 - logprior: -4.0286e+01
Epoch 2/10
10/10 - 1s - loss: 276.5811 - loglik: -2.6642e+02 - logprior: -1.0138e+01
Epoch 3/10
10/10 - 1s - loss: 253.0019 - loglik: -2.4862e+02 - logprior: -4.2804e+00
Epoch 4/10
10/10 - 1s - loss: 239.6041 - loglik: -2.3711e+02 - logprior: -2.1825e+00
Epoch 5/10
10/10 - 1s - loss: 233.1567 - loglik: -2.3156e+02 - logprior: -1.2202e+00
Epoch 6/10
10/10 - 1s - loss: 229.6206 - loglik: -2.2853e+02 - logprior: -7.9591e-01
Epoch 7/10
10/10 - 1s - loss: 227.8969 - loglik: -2.2701e+02 - logprior: -5.9286e-01
Epoch 8/10
10/10 - 1s - loss: 226.5387 - loglik: -2.2574e+02 - logprior: -4.5835e-01
Epoch 9/10
10/10 - 1s - loss: 225.3000 - loglik: -2.2458e+02 - logprior: -3.5919e-01
Epoch 10/10
10/10 - 1s - loss: 224.4865 - loglik: -2.2374e+02 - logprior: -3.6844e-01
Fitted a model with MAP estimate = -223.7288
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 2), (44, 10), (51, 2), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 277.4675 - loglik: -2.2548e+02 - logprior: -5.1952e+01
Epoch 2/2
10/10 - 1s - loss: 231.9662 - loglik: -2.1619e+02 - logprior: -1.5607e+01
Fitted a model with MAP estimate = -223.4754
expansions: []
discards: [ 0  1  2  9 56 57 71 76]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 254.5309 - loglik: -2.1712e+02 - logprior: -3.7381e+01
Epoch 2/2
10/10 - 1s - loss: 224.7223 - loglik: -2.1537e+02 - logprior: -9.1793e+00
Fitted a model with MAP estimate = -219.6804
expansions: [(0, 3)]
discards: [52 53]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 267.2371 - loglik: -2.1689e+02 - logprior: -5.0224e+01
Epoch 2/10
10/10 - 1s - loss: 229.3416 - loglik: -2.1511e+02 - logprior: -1.4092e+01
Epoch 3/10
10/10 - 1s - loss: 219.5103 - loglik: -2.1402e+02 - logprior: -5.2381e+00
Epoch 4/10
10/10 - 1s - loss: 215.8739 - loglik: -2.1372e+02 - logprior: -1.7818e+00
Epoch 5/10
10/10 - 1s - loss: 213.9562 - loglik: -2.1341e+02 - logprior: -1.2913e-01
Epoch 6/10
10/10 - 1s - loss: 212.7008 - loglik: -2.1302e+02 - logprior: 0.7211
Epoch 7/10
10/10 - 1s - loss: 212.1669 - loglik: -2.1294e+02 - logprior: 1.1762
Epoch 8/10
10/10 - 1s - loss: 211.9853 - loglik: -2.1303e+02 - logprior: 1.4436
Epoch 9/10
10/10 - 1s - loss: 211.6072 - loglik: -2.1285e+02 - logprior: 1.6417
Epoch 10/10
10/10 - 1s - loss: 211.4681 - loglik: -2.1288e+02 - logprior: 1.8227
Fitted a model with MAP estimate = -210.8402
Time for alignment: 51.1009
Computed alignments with likelihoods: ['-210.3600', '-210.5147', '-210.8402']
Best model has likelihood: -210.3600
time for generating output: 0.1938
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7255121430616654
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac5dae8ee0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac226e13a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabc4ad38b0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab0540e5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab0d8a76a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab15da6ca0>, <__main__.SimpleDirichletPrior object at 0x7faba1db6ee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 551.6860 - loglik: -4.7363e+02 - logprior: -7.7963e+01
Epoch 2/10
10/10 - 3s - loss: 419.6487 - loglik: -4.0415e+02 - logprior: -1.5433e+01
Epoch 3/10
10/10 - 3s - loss: 340.7770 - loglik: -3.3509e+02 - logprior: -5.5833e+00
Epoch 4/10
10/10 - 3s - loss: 294.7834 - loglik: -2.9124e+02 - logprior: -3.4766e+00
Epoch 5/10
10/10 - 3s - loss: 276.0547 - loglik: -2.7290e+02 - logprior: -3.0237e+00
Epoch 6/10
10/10 - 3s - loss: 268.6591 - loglik: -2.6626e+02 - logprior: -2.1853e+00
Epoch 7/10
10/10 - 3s - loss: 265.4818 - loglik: -2.6413e+02 - logprior: -1.1317e+00
Epoch 8/10
10/10 - 3s - loss: 263.8118 - loglik: -2.6291e+02 - logprior: -6.6939e-01
Epoch 9/10
10/10 - 3s - loss: 262.3390 - loglik: -2.6165e+02 - logprior: -4.2579e-01
Epoch 10/10
10/10 - 3s - loss: 262.0864 - loglik: -2.6177e+02 - logprior: -3.0852e-02
Fitted a model with MAP estimate = -261.4498
expansions: [(21, 1), (22, 1), (23, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.2508 - loglik: -2.5743e+02 - logprior: -8.8759e+01
Epoch 2/2
10/10 - 2s - loss: 269.2859 - loglik: -2.3701e+02 - logprior: -3.2137e+01
Fitted a model with MAP estimate = -256.9394
expansions: [(0, 3), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53  93 119]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 300.5701 - loglik: -2.3072e+02 - logprior: -6.9602e+01
Epoch 2/2
10/10 - 3s - loss: 235.4699 - loglik: -2.2332e+02 - logprior: -1.1918e+01
Fitted a model with MAP estimate = -224.7462
expansions: []
discards: [ 0  1 21]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.5234 - loglik: -2.2411e+02 - logprior: -8.5202e+01
Epoch 2/10
10/10 - 2s - loss: 248.1954 - loglik: -2.2244e+02 - logprior: -2.5574e+01
Epoch 3/10
10/10 - 2s - loss: 225.7690 - loglik: -2.2107e+02 - logprior: -4.5415e+00
Epoch 4/10
10/10 - 2s - loss: 215.7615 - loglik: -2.2092e+02 - logprior: 5.3532
Epoch 5/10
10/10 - 2s - loss: 210.2033 - loglik: -2.1900e+02 - logprior: 9.0148
Epoch 6/10
10/10 - 2s - loss: 210.0472 - loglik: -2.2075e+02 - logprior: 10.9364
Epoch 7/10
10/10 - 2s - loss: 207.8911 - loglik: -2.1981e+02 - logprior: 12.1753
Epoch 8/10
10/10 - 2s - loss: 207.5352 - loglik: -2.2035e+02 - logprior: 13.0823
Epoch 9/10
10/10 - 2s - loss: 206.5909 - loglik: -2.2014e+02 - logprior: 13.8380
Epoch 10/10
10/10 - 3s - loss: 206.3616 - loglik: -2.2060e+02 - logprior: 14.5295
Fitted a model with MAP estimate = -205.2181
Time for alignment: 87.3980
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 551.6464 - loglik: -4.7360e+02 - logprior: -7.7966e+01
Epoch 2/10
10/10 - 3s - loss: 419.8091 - loglik: -4.0428e+02 - logprior: -1.5456e+01
Epoch 3/10
10/10 - 3s - loss: 342.0972 - loglik: -3.3636e+02 - logprior: -5.6128e+00
Epoch 4/10
10/10 - 3s - loss: 293.9382 - loglik: -2.9014e+02 - logprior: -3.7138e+00
Epoch 5/10
10/10 - 3s - loss: 275.2904 - loglik: -2.7174e+02 - logprior: -3.4267e+00
Epoch 6/10
10/10 - 3s - loss: 267.8607 - loglik: -2.6519e+02 - logprior: -2.4378e+00
Epoch 7/10
10/10 - 3s - loss: 264.8708 - loglik: -2.6312e+02 - logprior: -1.4587e+00
Epoch 8/10
10/10 - 3s - loss: 263.1605 - loglik: -2.6181e+02 - logprior: -1.0693e+00
Epoch 9/10
10/10 - 3s - loss: 261.5501 - loglik: -2.6047e+02 - logprior: -8.1651e-01
Epoch 10/10
10/10 - 3s - loss: 261.6085 - loglik: -2.6094e+02 - logprior: -3.8409e-01
Fitted a model with MAP estimate = -260.6674
expansions: [(17, 4), (19, 1), (21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (128, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 342.5266 - loglik: -2.5391e+02 - logprior: -8.8470e+01
Epoch 2/2
10/10 - 3s - loss: 262.4456 - loglik: -2.3040e+02 - logprior: -3.1860e+01
Fitted a model with MAP estimate = -249.3435
expansions: [(0, 3), (91, 1)]
discards: [  0  48  58  95  99 125]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 294.7333 - loglik: -2.2541e+02 - logprior: -6.9262e+01
Epoch 2/2
10/10 - 2s - loss: 232.9269 - loglik: -2.2097e+02 - logprior: -1.1801e+01
Fitted a model with MAP estimate = -222.8022
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.0894 - loglik: -2.2385e+02 - logprior: -8.5199e+01
Epoch 2/10
10/10 - 2s - loss: 247.8258 - loglik: -2.2235e+02 - logprior: -2.5405e+01
Epoch 3/10
10/10 - 2s - loss: 224.9635 - loglik: -2.2050e+02 - logprior: -4.3078e+00
Epoch 4/10
10/10 - 2s - loss: 214.4075 - loglik: -2.1974e+02 - logprior: 5.5462
Epoch 5/10
10/10 - 2s - loss: 210.7518 - loglik: -2.1968e+02 - logprior: 9.1300
Epoch 6/10
10/10 - 2s - loss: 208.6591 - loglik: -2.1954e+02 - logprior: 11.0939
Epoch 7/10
10/10 - 2s - loss: 207.2054 - loglik: -2.1930e+02 - logprior: 12.3360
Epoch 8/10
10/10 - 2s - loss: 207.5913 - loglik: -2.2059e+02 - logprior: 13.2653
Fitted a model with MAP estimate = -205.9149
Time for alignment: 81.8277
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.3997 - loglik: -4.7338e+02 - logprior: -7.7967e+01
Epoch 2/10
10/10 - 3s - loss: 419.0366 - loglik: -4.0351e+02 - logprior: -1.5431e+01
Epoch 3/10
10/10 - 3s - loss: 341.4937 - loglik: -3.3577e+02 - logprior: -5.5878e+00
Epoch 4/10
10/10 - 3s - loss: 292.9571 - loglik: -2.8908e+02 - logprior: -3.7952e+00
Epoch 5/10
10/10 - 3s - loss: 275.2884 - loglik: -2.7209e+02 - logprior: -3.0028e+00
Epoch 6/10
10/10 - 3s - loss: 269.2077 - loglik: -2.6744e+02 - logprior: -1.4863e+00
Epoch 7/10
10/10 - 3s - loss: 266.8480 - loglik: -2.6613e+02 - logprior: -4.4176e-01
Epoch 8/10
10/10 - 3s - loss: 264.8007 - loglik: -2.6442e+02 - logprior: -1.2293e-01
Epoch 9/10
10/10 - 3s - loss: 263.9701 - loglik: -2.6387e+02 - logprior: 0.1592
Epoch 10/10
10/10 - 3s - loss: 263.1298 - loglik: -2.6339e+02 - logprior: 0.5281
Fitted a model with MAP estimate = -262.2596
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 4), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 2), (124, 1), (125, 1), (128, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 349.2580 - loglik: -2.6026e+02 - logprior: -8.8861e+01
Epoch 2/2
10/10 - 2s - loss: 270.4128 - loglik: -2.3830e+02 - logprior: -3.2084e+01
Fitted a model with MAP estimate = -258.0453
expansions: [(0, 3), (15, 3), (16, 1), (86, 1)]
discards: [  0  43  53  93 119]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 300.9448 - loglik: -2.3148e+02 - logprior: -6.9391e+01
Epoch 2/2
10/10 - 2s - loss: 236.9164 - loglik: -2.2507e+02 - logprior: -1.1763e+01
Fitted a model with MAP estimate = -226.0016
expansions: [(16, 1), (17, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 310.1792 - loglik: -2.2494e+02 - logprior: -8.5045e+01
Epoch 2/10
10/10 - 2s - loss: 247.7787 - loglik: -2.2169e+02 - logprior: -2.5867e+01
Epoch 3/10
10/10 - 2s - loss: 224.8470 - loglik: -2.1971e+02 - logprior: -4.9424e+00
Epoch 4/10
10/10 - 2s - loss: 213.9361 - loglik: -2.1940e+02 - logprior: 5.6306
Epoch 5/10
10/10 - 2s - loss: 210.3270 - loglik: -2.1949e+02 - logprior: 9.3472
Epoch 6/10
10/10 - 2s - loss: 208.4563 - loglik: -2.1948e+02 - logprior: 11.2491
Epoch 7/10
10/10 - 2s - loss: 206.5800 - loglik: -2.1882e+02 - logprior: 12.4967
Epoch 8/10
10/10 - 2s - loss: 205.6639 - loglik: -2.1882e+02 - logprior: 13.4073
Epoch 9/10
10/10 - 2s - loss: 204.9616 - loglik: -2.1886e+02 - logprior: 14.1539
Epoch 10/10
10/10 - 2s - loss: 204.8941 - loglik: -2.1947e+02 - logprior: 14.8480
Fitted a model with MAP estimate = -203.7095
Time for alignment: 86.2861
Computed alignments with likelihoods: ['-205.2181', '-205.9149', '-203.7095']
Best model has likelihood: -203.7095
time for generating output: 0.2259
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac2275dbb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6a02c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faafc528850>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaf43161f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4ad38b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac5dae8ee0>, <__main__.SimpleDirichletPrior object at 0x7fabc4ae9610>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 811.1866 - loglik: -8.0877e+02 - logprior: -2.0380e+00
Epoch 2/10
33/33 - 24s - loss: 719.1063 - loglik: -7.1743e+02 - logprior: -6.5879e-01
Epoch 3/10
33/33 - 25s - loss: 703.7921 - loglik: -7.0192e+02 - logprior: -5.7386e-01
Epoch 4/10
33/33 - 24s - loss: 704.2036 - loglik: -7.0230e+02 - logprior: -5.6639e-01
Fitted a model with MAP estimate = -700.7134
expansions: [(0, 5), (35, 4), (62, 1), (63, 2), (72, 1), (73, 2), (78, 1), (111, 1), (113, 1), (116, 1), (119, 2), (155, 4), (164, 4), (177, 1), (204, 1), (210, 1), (223, 3), (224, 2), (225, 1)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 713.8593 - loglik: -7.1078e+02 - logprior: -2.9392e+00
Epoch 2/2
33/33 - 29s - loss: 697.1425 - loglik: -6.9583e+02 - logprior: -6.5494e-01
Fitted a model with MAP estimate = -692.2973
expansions: [(155, 2)]
discards: [  1  41  71 139 177 179 189 190 191 192 262 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 706.5550 - loglik: -7.0430e+02 - logprior: -2.0949e+00
Epoch 2/2
33/33 - 28s - loss: 695.8363 - loglik: -6.9505e+02 - logprior: -2.8578e-01
Fitted a model with MAP estimate = -694.2057
expansions: [(248, 2), (254, 4)]
discards: [  4   5   6 249 250 251 253]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 704.3589 - loglik: -7.0171e+02 - logprior: -1.9532e+00
Epoch 2/10
33/33 - 28s - loss: 693.2936 - loglik: -6.9207e+02 - logprior: -3.0053e-02
Epoch 3/10
33/33 - 28s - loss: 694.9725 - loglik: -6.9367e+02 - logprior: 0.1716
Fitted a model with MAP estimate = -690.9992
Time for alignment: 416.9773
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 813.5844 - loglik: -8.1114e+02 - logprior: -2.0436e+00
Epoch 2/10
33/33 - 24s - loss: 717.8197 - loglik: -7.1600e+02 - logprior: -7.5439e-01
Epoch 3/10
33/33 - 25s - loss: 703.3434 - loglik: -7.0140e+02 - logprior: -6.3936e-01
Epoch 4/10
33/33 - 24s - loss: 704.9378 - loglik: -7.0298e+02 - logprior: -6.1052e-01
Fitted a model with MAP estimate = -701.0404
expansions: [(0, 5), (34, 7), (72, 1), (73, 1), (74, 2), (79, 2), (91, 1), (110, 1), (113, 1), (116, 1), (118, 2), (134, 1), (155, 2), (163, 4), (177, 1), (204, 1), (221, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 715.7821 - loglik: -7.1265e+02 - logprior: -2.9758e+00
Epoch 2/2
33/33 - 29s - loss: 697.0389 - loglik: -6.9575e+02 - logprior: -6.6662e-01
Fitted a model with MAP estimate = -692.0660
expansions: [(258, 2), (267, 4)]
discards: [  1  41  42  43 141 181 191 192 193 194 259 260 261 263 264 265 266]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 32s - loss: 704.6903 - loglik: -7.0207e+02 - logprior: -2.1235e+00
Epoch 2/2
33/33 - 28s - loss: 696.1976 - loglik: -6.9484e+02 - logprior: -3.8416e-01
Fitted a model with MAP estimate = -693.2174
expansions: [(250, 1), (256, 4)]
discards: [  4   5   6 251 252 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 703.7325 - loglik: -7.0147e+02 - logprior: -1.9676e+00
Epoch 2/10
33/33 - 28s - loss: 697.3088 - loglik: -6.9661e+02 - logprior: -4.4698e-02
Epoch 3/10
33/33 - 27s - loss: 692.6591 - loglik: -6.9191e+02 - logprior: 0.1654
Epoch 4/10
33/33 - 28s - loss: 693.8351 - loglik: -6.9297e+02 - logprior: 0.2481
Fitted a model with MAP estimate = -689.8806
Time for alignment: 446.1693
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 810.6866 - loglik: -8.0852e+02 - logprior: -2.0525e+00
Epoch 2/10
33/33 - 24s - loss: 718.2176 - loglik: -7.1683e+02 - logprior: -6.9080e-01
Epoch 3/10
33/33 - 24s - loss: 705.5049 - loglik: -7.0390e+02 - logprior: -6.0269e-01
Epoch 4/10
33/33 - 25s - loss: 702.0242 - loglik: -7.0028e+02 - logprior: -5.8186e-01
Epoch 5/10
33/33 - 24s - loss: 702.7311 - loglik: -7.0098e+02 - logprior: -5.6797e-01
Fitted a model with MAP estimate = -699.7894
expansions: [(0, 5), (5, 1), (8, 1), (9, 1), (33, 4), (62, 1), (63, 2), (72, 1), (73, 2), (78, 2), (110, 1), (113, 1), (118, 1), (135, 1), (155, 4), (163, 4), (184, 1), (210, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 713.3231 - loglik: -7.1029e+02 - logprior: -2.9419e+00
Epoch 2/2
33/33 - 29s - loss: 696.2364 - loglik: -6.9524e+02 - logprior: -5.8062e-01
Fitted a model with MAP estimate = -692.5425
expansions: [(258, 1), (267, 4)]
discards: [  1  42  74 181 182 192 193 194 259 260 261 263 264 265 266]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 702.7269 - loglik: -7.0033e+02 - logprior: -2.0450e+00
Epoch 2/2
33/33 - 28s - loss: 697.3721 - loglik: -6.9635e+02 - logprior: -2.4198e-01
Fitted a model with MAP estimate = -692.7350
expansions: [(257, 4)]
discards: [  4   5   6 253 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 708.4084 - loglik: -7.0630e+02 - logprior: -1.8750e+00
Epoch 2/10
33/33 - 28s - loss: 695.8415 - loglik: -6.9506e+02 - logprior: 0.0932
Epoch 3/10
33/33 - 28s - loss: 693.3242 - loglik: -6.9223e+02 - logprior: 0.3041
Epoch 4/10
33/33 - 28s - loss: 695.4058 - loglik: -6.9433e+02 - logprior: 0.3923
Fitted a model with MAP estimate = -689.2830
Time for alignment: 473.9383
Computed alignments with likelihoods: ['-690.9992', '-689.8806', '-689.2830']
Best model has likelihood: -689.2830
time for generating output: 0.3395
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7649903288201161
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabe6aca7c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaeb62e100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaeb62e3d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba25018b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba219b130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaf43161f0>, <__main__.SimpleDirichletPrior object at 0x7fabf7d01550>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.8560 - loglik: -3.4875e+02 - logprior: -6.2020e+01
Epoch 2/10
10/10 - 2s - loss: 321.0295 - loglik: -3.0657e+02 - logprior: -1.4424e+01
Epoch 3/10
10/10 - 2s - loss: 273.0236 - loglik: -2.6658e+02 - logprior: -6.3459e+00
Epoch 4/10
10/10 - 2s - loss: 247.4501 - loglik: -2.4322e+02 - logprior: -3.9895e+00
Epoch 5/10
10/10 - 2s - loss: 237.6884 - loglik: -2.3448e+02 - logprior: -2.8606e+00
Epoch 6/10
10/10 - 2s - loss: 232.9733 - loglik: -2.3065e+02 - logprior: -1.9911e+00
Epoch 7/10
10/10 - 2s - loss: 231.5735 - loglik: -2.2994e+02 - logprior: -1.3446e+00
Epoch 8/10
10/10 - 2s - loss: 230.6353 - loglik: -2.2936e+02 - logprior: -9.9946e-01
Epoch 9/10
10/10 - 2s - loss: 229.1740 - loglik: -2.2814e+02 - logprior: -7.4694e-01
Epoch 10/10
10/10 - 2s - loss: 229.0095 - loglik: -2.2820e+02 - logprior: -5.2808e-01
Fitted a model with MAP estimate = -228.4720
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 2), (64, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 304.2454 - loglik: -2.3455e+02 - logprior: -6.9628e+01
Epoch 2/2
10/10 - 2s - loss: 240.0426 - loglik: -2.1354e+02 - logprior: -2.6325e+01
Fitted a model with MAP estimate = -227.8636
expansions: [(0, 3)]
discards: [  0 107]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 265.3857 - loglik: -2.1078e+02 - logprior: -5.4522e+01
Epoch 2/2
10/10 - 2s - loss: 217.0955 - loglik: -2.0515e+02 - logprior: -1.1802e+01
Fitted a model with MAP estimate = -207.5248
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 276.7876 - loglik: -2.1118e+02 - logprior: -6.5584e+01
Epoch 2/10
10/10 - 2s - loss: 223.5305 - loglik: -2.0629e+02 - logprior: -1.7154e+01
Epoch 3/10
10/10 - 2s - loss: 207.3496 - loglik: -2.0336e+02 - logprior: -3.7991e+00
Epoch 4/10
10/10 - 2s - loss: 199.9588 - loglik: -2.0042e+02 - logprior: 0.7577
Epoch 5/10
10/10 - 2s - loss: 197.1143 - loglik: -1.9968e+02 - logprior: 2.9422
Epoch 6/10
10/10 - 2s - loss: 195.7349 - loglik: -1.9948e+02 - logprior: 4.1235
Epoch 7/10
10/10 - 2s - loss: 193.9058 - loglik: -1.9842e+02 - logprior: 4.8770
Epoch 8/10
10/10 - 2s - loss: 194.1630 - loglik: -1.9930e+02 - logprior: 5.4831
Fitted a model with MAP estimate = -193.2381
Time for alignment: 59.0406
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 410.9107 - loglik: -3.4885e+02 - logprior: -6.2013e+01
Epoch 2/10
10/10 - 2s - loss: 321.2414 - loglik: -3.0683e+02 - logprior: -1.4391e+01
Epoch 3/10
10/10 - 2s - loss: 272.7939 - loglik: -2.6637e+02 - logprior: -6.3310e+00
Epoch 4/10
10/10 - 2s - loss: 246.8206 - loglik: -2.4225e+02 - logprior: -4.3032e+00
Epoch 5/10
10/10 - 2s - loss: 236.1644 - loglik: -2.3261e+02 - logprior: -3.1938e+00
Epoch 6/10
10/10 - 2s - loss: 233.4251 - loglik: -2.3093e+02 - logprior: -2.1290e+00
Epoch 7/10
10/10 - 2s - loss: 230.9453 - loglik: -2.2915e+02 - logprior: -1.4648e+00
Epoch 8/10
10/10 - 2s - loss: 229.9376 - loglik: -2.2846e+02 - logprior: -1.1534e+00
Epoch 9/10
10/10 - 2s - loss: 229.2253 - loglik: -2.2801e+02 - logprior: -9.0594e-01
Epoch 10/10
10/10 - 2s - loss: 228.9505 - loglik: -2.2797e+02 - logprior: -6.5581e-01
Fitted a model with MAP estimate = -228.3515
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (87, 4), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 304.8885 - loglik: -2.3514e+02 - logprior: -6.9646e+01
Epoch 2/2
10/10 - 2s - loss: 240.4404 - loglik: -2.1384e+02 - logprior: -2.6318e+01
Fitted a model with MAP estimate = -227.9512
expansions: [(0, 3)]
discards: [  0  13 107]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 267.7518 - loglik: -2.1328e+02 - logprior: -5.4437e+01
Epoch 2/2
10/10 - 2s - loss: 218.1115 - loglik: -2.0641e+02 - logprior: -1.1660e+01
Fitted a model with MAP estimate = -209.2631
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 276.4041 - loglik: -2.1073e+02 - logprior: -6.5639e+01
Epoch 2/10
10/10 - 2s - loss: 224.1035 - loglik: -2.0678e+02 - logprior: -1.7271e+01
Epoch 3/10
10/10 - 2s - loss: 207.3953 - loglik: -2.0335e+02 - logprior: -3.8154e+00
Epoch 4/10
10/10 - 2s - loss: 200.5771 - loglik: -2.0096e+02 - logprior: 0.7794
Epoch 5/10
10/10 - 2s - loss: 197.9260 - loglik: -2.0051e+02 - logprior: 2.9603
Epoch 6/10
10/10 - 2s - loss: 195.5923 - loglik: -1.9938e+02 - logprior: 4.1189
Epoch 7/10
10/10 - 2s - loss: 194.8227 - loglik: -1.9931e+02 - logprior: 4.8261
Epoch 8/10
10/10 - 2s - loss: 193.9856 - loglik: -1.9907e+02 - logprior: 5.4137
Epoch 9/10
10/10 - 2s - loss: 193.1872 - loglik: -1.9883e+02 - logprior: 5.9653
Epoch 10/10
10/10 - 2s - loss: 193.2226 - loglik: -1.9934e+02 - logprior: 6.4268
Fitted a model with MAP estimate = -192.4886
Time for alignment: 60.0149
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.8680 - loglik: -3.4880e+02 - logprior: -6.2023e+01
Epoch 2/10
10/10 - 2s - loss: 321.1609 - loglik: -3.0671e+02 - logprior: -1.4431e+01
Epoch 3/10
10/10 - 2s - loss: 274.0015 - loglik: -2.6758e+02 - logprior: -6.3315e+00
Epoch 4/10
10/10 - 2s - loss: 247.1062 - loglik: -2.4296e+02 - logprior: -3.9163e+00
Epoch 5/10
10/10 - 2s - loss: 236.7440 - loglik: -2.3375e+02 - logprior: -2.6366e+00
Epoch 6/10
10/10 - 2s - loss: 233.9990 - loglik: -2.3194e+02 - logprior: -1.7469e+00
Epoch 7/10
10/10 - 2s - loss: 231.4194 - loglik: -2.2987e+02 - logprior: -1.2652e+00
Epoch 8/10
10/10 - 2s - loss: 230.0069 - loglik: -2.2875e+02 - logprior: -9.8206e-01
Epoch 9/10
10/10 - 2s - loss: 230.1843 - loglik: -2.2925e+02 - logprior: -6.7476e-01
Fitted a model with MAP estimate = -229.1290
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 2), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 303.4350 - loglik: -2.3335e+02 - logprior: -6.9737e+01
Epoch 2/2
10/10 - 2s - loss: 240.0692 - loglik: -2.1309e+02 - logprior: -2.6610e+01
Fitted a model with MAP estimate = -227.3575
expansions: [(0, 3)]
discards: [  0  97 108]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.5228 - loglik: -2.1195e+02 - logprior: -5.4532e+01
Epoch 2/2
10/10 - 2s - loss: 217.4718 - loglik: -2.0565e+02 - logprior: -1.1796e+01
Fitted a model with MAP estimate = -208.3744
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.0505 - loglik: -2.1044e+02 - logprior: -6.5598e+01
Epoch 2/10
10/10 - 2s - loss: 223.1952 - loglik: -2.0589e+02 - logprior: -1.7239e+01
Epoch 3/10
10/10 - 2s - loss: 206.8322 - loglik: -2.0282e+02 - logprior: -3.7919e+00
Epoch 4/10
10/10 - 2s - loss: 200.5011 - loglik: -2.0096e+02 - logprior: 0.8038
Epoch 5/10
10/10 - 2s - loss: 197.0538 - loglik: -1.9965e+02 - logprior: 2.9687
Epoch 6/10
10/10 - 2s - loss: 195.9179 - loglik: -1.9968e+02 - logprior: 4.1336
Epoch 7/10
10/10 - 2s - loss: 194.0641 - loglik: -1.9861e+02 - logprior: 4.8831
Epoch 8/10
10/10 - 2s - loss: 193.6433 - loglik: -1.9880e+02 - logprior: 5.4805
Epoch 9/10
10/10 - 2s - loss: 193.6656 - loglik: -1.9933e+02 - logprior: 5.9850
Fitted a model with MAP estimate = -192.8169
Time for alignment: 57.2097
Computed alignments with likelihoods: ['-193.2381', '-192.4886', '-192.8169']
Best model has likelihood: -192.4886
time for generating output: 0.1842
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9378029079159935
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac5d9ce2e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab300a19d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faae3370880>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba148ffa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba2501d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaf4357340>, <__main__.SimpleDirichletPrior object at 0x7faadaba5d60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.5926 - loglik: -1.9333e+02 - logprior: -2.2007e+00
Epoch 2/10
22/22 - 2s - loss: 162.6872 - loglik: -1.6098e+02 - logprior: -1.3386e+00
Epoch 3/10
22/22 - 2s - loss: 154.9990 - loglik: -1.5294e+02 - logprior: -1.4465e+00
Epoch 4/10
22/22 - 2s - loss: 153.7157 - loglik: -1.5193e+02 - logprior: -1.3438e+00
Epoch 5/10
22/22 - 2s - loss: 152.8175 - loglik: -1.5110e+02 - logprior: -1.3373e+00
Epoch 6/10
22/22 - 2s - loss: 152.4639 - loglik: -1.5082e+02 - logprior: -1.3129e+00
Epoch 7/10
22/22 - 2s - loss: 152.6761 - loglik: -1.5106e+02 - logprior: -1.3009e+00
Fitted a model with MAP estimate = -152.5792
expansions: [(8, 1), (9, 2), (11, 2), (14, 2), (17, 2), (21, 2), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 157.2094 - loglik: -1.5412e+02 - logprior: -2.9221e+00
Epoch 2/2
22/22 - 2s - loss: 147.3841 - loglik: -1.4547e+02 - logprior: -1.5479e+00
Fitted a model with MAP estimate = -144.7425
expansions: [(0, 2)]
discards: [ 0  9 14 19 23 30 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 147.5970 - loglik: -1.4540e+02 - logprior: -2.1171e+00
Epoch 2/2
22/22 - 2s - loss: 144.1976 - loglik: -1.4296e+02 - logprior: -1.0173e+00
Fitted a model with MAP estimate = -143.6287
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.9737 - loglik: -1.4359e+02 - logprior: -1.2159e+00
Epoch 2/10
32/32 - 3s - loss: 142.1081 - loglik: -1.4081e+02 - logprior: -8.9990e-01
Epoch 3/10
32/32 - 3s - loss: 141.5850 - loglik: -1.4020e+02 - logprior: -8.8456e-01
Epoch 4/10
32/32 - 3s - loss: 140.9365 - loglik: -1.3951e+02 - logprior: -8.7897e-01
Epoch 5/10
32/32 - 3s - loss: 140.5529 - loglik: -1.3915e+02 - logprior: -8.7681e-01
Epoch 6/10
32/32 - 3s - loss: 140.3468 - loglik: -1.3898e+02 - logprior: -8.7375e-01
Epoch 7/10
32/32 - 3s - loss: 140.0007 - loglik: -1.3860e+02 - logprior: -8.6633e-01
Epoch 8/10
32/32 - 3s - loss: 139.9507 - loglik: -1.3861e+02 - logprior: -8.6306e-01
Epoch 9/10
32/32 - 3s - loss: 139.3713 - loglik: -1.3802e+02 - logprior: -8.5572e-01
Epoch 10/10
32/32 - 3s - loss: 139.3860 - loglik: -1.3801e+02 - logprior: -8.5231e-01
Fitted a model with MAP estimate = -138.6135
Time for alignment: 86.6370
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.3563 - loglik: -1.9309e+02 - logprior: -2.1982e+00
Epoch 2/10
22/22 - 2s - loss: 161.6457 - loglik: -1.5991e+02 - logprior: -1.3337e+00
Epoch 3/10
22/22 - 2s - loss: 154.4508 - loglik: -1.5239e+02 - logprior: -1.4420e+00
Epoch 4/10
22/22 - 2s - loss: 152.5600 - loglik: -1.5078e+02 - logprior: -1.3288e+00
Epoch 5/10
22/22 - 2s - loss: 152.5034 - loglik: -1.5078e+02 - logprior: -1.3242e+00
Epoch 6/10
22/22 - 2s - loss: 152.0185 - loglik: -1.5037e+02 - logprior: -1.2952e+00
Epoch 7/10
22/22 - 2s - loss: 151.7702 - loglik: -1.5016e+02 - logprior: -1.2856e+00
Epoch 8/10
22/22 - 2s - loss: 151.5278 - loglik: -1.4994e+02 - logprior: -1.2743e+00
Epoch 9/10
22/22 - 2s - loss: 151.3429 - loglik: -1.4975e+02 - logprior: -1.2716e+00
Epoch 10/10
22/22 - 2s - loss: 151.4736 - loglik: -1.4988e+02 - logprior: -1.2676e+00
Fitted a model with MAP estimate = -153.4444
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 158.8987 - loglik: -1.5588e+02 - logprior: -2.9227e+00
Epoch 2/2
22/22 - 2s - loss: 147.8243 - loglik: -1.4599e+02 - logprior: -1.5809e+00
Fitted a model with MAP estimate = -145.2000
expansions: [(0, 2)]
discards: [ 0  9 17 26 29 32 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 148.2886 - loglik: -1.4613e+02 - logprior: -2.1167e+00
Epoch 2/2
22/22 - 2s - loss: 144.6918 - loglik: -1.4358e+02 - logprior: -1.0060e+00
Fitted a model with MAP estimate = -143.8811
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 7s - loss: 144.9209 - loglik: -1.4355e+02 - logprior: -1.2084e+00
Epoch 2/10
32/32 - 3s - loss: 142.3947 - loglik: -1.4104e+02 - logprior: -9.0287e-01
Epoch 3/10
32/32 - 3s - loss: 141.6125 - loglik: -1.4021e+02 - logprior: -8.8045e-01
Epoch 4/10
32/32 - 3s - loss: 140.6613 - loglik: -1.3928e+02 - logprior: -8.7731e-01
Epoch 5/10
32/32 - 3s - loss: 140.8641 - loglik: -1.3948e+02 - logprior: -8.7137e-01
Fitted a model with MAP estimate = -139.8554
Time for alignment: 78.2110
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.3768 - loglik: -1.9312e+02 - logprior: -2.1989e+00
Epoch 2/10
22/22 - 2s - loss: 162.2501 - loglik: -1.6055e+02 - logprior: -1.3414e+00
Epoch 3/10
22/22 - 2s - loss: 154.9900 - loglik: -1.5289e+02 - logprior: -1.4571e+00
Epoch 4/10
22/22 - 2s - loss: 153.1368 - loglik: -1.5135e+02 - logprior: -1.3526e+00
Epoch 5/10
22/22 - 2s - loss: 152.7561 - loglik: -1.5101e+02 - logprior: -1.3455e+00
Epoch 6/10
22/22 - 2s - loss: 152.4100 - loglik: -1.5073e+02 - logprior: -1.3203e+00
Epoch 7/10
22/22 - 2s - loss: 152.2555 - loglik: -1.5062e+02 - logprior: -1.3050e+00
Epoch 8/10
22/22 - 2s - loss: 152.1162 - loglik: -1.5049e+02 - logprior: -1.3002e+00
Epoch 9/10
22/22 - 2s - loss: 152.0178 - loglik: -1.5039e+02 - logprior: -1.2938e+00
Epoch 10/10
22/22 - 2s - loss: 151.6888 - loglik: -1.5004e+02 - logprior: -1.2910e+00
Fitted a model with MAP estimate = -153.8140
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 159.3049 - loglik: -1.5629e+02 - logprior: -2.9183e+00
Epoch 2/2
22/22 - 2s - loss: 148.0969 - loglik: -1.4629e+02 - logprior: -1.5458e+00
Fitted a model with MAP estimate = -145.5391
expansions: [(0, 2)]
discards: [ 0  9 17 22 31 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.2568 - loglik: -1.4499e+02 - logprior: -2.1094e+00
Epoch 2/2
22/22 - 2s - loss: 144.1060 - loglik: -1.4274e+02 - logprior: -1.0145e+00
Fitted a model with MAP estimate = -143.4489
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.4722 - loglik: -1.4418e+02 - logprior: -1.1994e+00
Epoch 2/10
32/32 - 3s - loss: 142.6965 - loglik: -1.4159e+02 - logprior: -8.9784e-01
Epoch 3/10
32/32 - 3s - loss: 141.3472 - loglik: -1.4009e+02 - logprior: -8.7531e-01
Epoch 4/10
32/32 - 3s - loss: 141.0370 - loglik: -1.3968e+02 - logprior: -8.7357e-01
Epoch 5/10
32/32 - 3s - loss: 140.6326 - loglik: -1.3929e+02 - logprior: -8.7252e-01
Epoch 6/10
32/32 - 3s - loss: 140.5067 - loglik: -1.3920e+02 - logprior: -8.6447e-01
Epoch 7/10
32/32 - 3s - loss: 140.0265 - loglik: -1.3867e+02 - logprior: -8.5913e-01
Epoch 8/10
32/32 - 3s - loss: 139.8225 - loglik: -1.3851e+02 - logprior: -8.5883e-01
Epoch 9/10
32/32 - 3s - loss: 139.5767 - loglik: -1.3823e+02 - logprior: -8.5884e-01
Epoch 10/10
32/32 - 3s - loss: 139.5943 - loglik: -1.3823e+02 - logprior: -8.4708e-01
Fitted a model with MAP estimate = -138.6577
Time for alignment: 89.8339
Computed alignments with likelihoods: ['-138.6135', '-139.8554', '-138.6577']
Best model has likelihood: -138.6135
time for generating output: 0.1656
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8245392754537173
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac08887850>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafcb933d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faba1db2040>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3bf74970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafcbb7b80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaaf670490>, <__main__.SimpleDirichletPrior object at 0x7fab2ee92190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 461.8237 - loglik: -4.4876e+02 - logprior: -1.3023e+01
Epoch 2/10
17/17 - 5s - loss: 313.5943 - loglik: -3.1160e+02 - logprior: -1.8913e+00
Epoch 3/10
17/17 - 5s - loss: 260.3417 - loglik: -2.5869e+02 - logprior: -1.5993e+00
Epoch 4/10
17/17 - 5s - loss: 249.1988 - loglik: -2.4764e+02 - logprior: -1.3426e+00
Epoch 5/10
17/17 - 5s - loss: 245.1660 - loglik: -2.4346e+02 - logprior: -1.3850e+00
Epoch 6/10
17/17 - 5s - loss: 239.8951 - loglik: -2.3819e+02 - logprior: -1.3875e+00
Epoch 7/10
17/17 - 5s - loss: 243.9918 - loglik: -2.4221e+02 - logprior: -1.4465e+00
Fitted a model with MAP estimate = -241.4252
expansions: [(25, 1), (50, 1), (61, 1), (83, 1), (91, 1), (108, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 270.7279 - loglik: -2.5301e+02 - logprior: -1.7687e+01
Epoch 2/2
17/17 - 5s - loss: 249.8494 - loglik: -2.4361e+02 - logprior: -6.1408e+00
Fitted a model with MAP estimate = -245.2472
expansions: [(0, 16)]
discards: [  0 139]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 258.7600 - loglik: -2.4516e+02 - logprior: -1.3447e+01
Epoch 2/2
17/17 - 5s - loss: 237.9995 - loglik: -2.3585e+02 - logprior: -1.9528e+00
Fitted a model with MAP estimate = -234.3272
expansions: [(154, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 261.6451 - loglik: -2.4490e+02 - logprior: -1.6634e+01
Epoch 2/10
17/17 - 5s - loss: 242.7448 - loglik: -2.3975e+02 - logprior: -2.8274e+00
Epoch 3/10
17/17 - 5s - loss: 239.2362 - loglik: -2.3931e+02 - logprior: 0.3563
Epoch 4/10
17/17 - 5s - loss: 234.6813 - loglik: -2.3514e+02 - logprior: 0.7618
Epoch 5/10
17/17 - 6s - loss: 235.8996 - loglik: -2.3645e+02 - logprior: 0.8736
Fitted a model with MAP estimate = -233.2680
Time for alignment: 117.7520
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 463.3260 - loglik: -4.5018e+02 - logprior: -1.2958e+01
Epoch 2/10
17/17 - 5s - loss: 322.1098 - loglik: -3.2084e+02 - logprior: -1.1766e+00
Epoch 3/10
17/17 - 5s - loss: 268.1437 - loglik: -2.6745e+02 - logprior: -6.0473e-01
Epoch 4/10
17/17 - 5s - loss: 255.9096 - loglik: -2.5527e+02 - logprior: -3.3375e-01
Epoch 5/10
17/17 - 5s - loss: 250.7484 - loglik: -2.5014e+02 - logprior: -2.5590e-01
Epoch 6/10
17/17 - 5s - loss: 252.7670 - loglik: -2.5219e+02 - logprior: -2.1979e-01
Fitted a model with MAP estimate = -249.9455
expansions: [(0, 34), (20, 1), (23, 2), (24, 1), (59, 1), (80, 1), (91, 1), (108, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 265.2141 - loglik: -2.4875e+02 - logprior: -1.6313e+01
Epoch 2/2
17/17 - 6s - loss: 221.6715 - loglik: -2.1967e+02 - logprior: -1.8586e+00
Fitted a model with MAP estimate = -212.2493
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 253.1873 - loglik: -2.3958e+02 - logprior: -1.3403e+01
Epoch 2/2
17/17 - 6s - loss: 224.4568 - loglik: -2.2367e+02 - logprior: -5.4550e-01
Fitted a model with MAP estimate = -219.6771
expansions: [(0, 25)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 251.3565 - loglik: -2.3930e+02 - logprior: -1.1908e+01
Epoch 2/10
17/17 - 6s - loss: 222.8742 - loglik: -2.2276e+02 - logprior: -3.4608e-02
Epoch 3/10
17/17 - 6s - loss: 213.5482 - loglik: -2.1461e+02 - logprior: 1.2681
Epoch 4/10
17/17 - 6s - loss: 208.4557 - loglik: -2.0987e+02 - logprior: 1.7377
Epoch 5/10
17/17 - 6s - loss: 207.3689 - loglik: -2.0893e+02 - logprior: 1.9023
Epoch 6/10
17/17 - 6s - loss: 205.1172 - loglik: -2.0686e+02 - logprior: 2.1247
Epoch 7/10
17/17 - 6s - loss: 205.0791 - loglik: -2.0701e+02 - logprior: 2.3171
Epoch 8/10
17/17 - 6s - loss: 204.2715 - loglik: -2.0642e+02 - logprior: 2.5349
Epoch 9/10
17/17 - 6s - loss: 203.8516 - loglik: -2.0620e+02 - logprior: 2.7368
Epoch 10/10
17/17 - 6s - loss: 203.6809 - loglik: -2.0625e+02 - logprior: 2.9514
Fitted a model with MAP estimate = -203.1191
Time for alignment: 149.2985
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 462.4876 - loglik: -4.4947e+02 - logprior: -1.2986e+01
Epoch 2/10
17/17 - 5s - loss: 313.2043 - loglik: -3.1144e+02 - logprior: -1.6559e+00
Epoch 3/10
17/17 - 5s - loss: 257.8636 - loglik: -2.5634e+02 - logprior: -1.4695e+00
Epoch 4/10
17/17 - 5s - loss: 247.9541 - loglik: -2.4643e+02 - logprior: -1.2603e+00
Epoch 5/10
17/17 - 5s - loss: 246.6154 - loglik: -2.4502e+02 - logprior: -1.2773e+00
Epoch 6/10
17/17 - 5s - loss: 242.6525 - loglik: -2.4106e+02 - logprior: -1.2553e+00
Epoch 7/10
17/17 - 5s - loss: 243.0251 - loglik: -2.4139e+02 - logprior: -1.2950e+00
Fitted a model with MAP estimate = -241.9280
expansions: [(50, 1), (59, 1), (80, 1), (91, 1), (108, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 271.5369 - loglik: -2.5375e+02 - logprior: -1.7689e+01
Epoch 2/2
17/17 - 5s - loss: 249.2533 - loglik: -2.4290e+02 - logprior: -6.1497e+00
Fitted a model with MAP estimate = -246.7365
expansions: [(0, 16)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 260.9037 - loglik: -2.4710e+02 - logprior: -1.3654e+01
Epoch 2/2
17/17 - 6s - loss: 238.5735 - loglik: -2.3657e+02 - logprior: -1.7818e+00
Fitted a model with MAP estimate = -235.6738
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 263.5267 - loglik: -2.4671e+02 - logprior: -1.6622e+01
Epoch 2/10
17/17 - 5s - loss: 246.3197 - loglik: -2.4348e+02 - logprior: -2.6389e+00
Epoch 3/10
17/17 - 5s - loss: 239.3080 - loglik: -2.3936e+02 - logprior: 0.3367
Epoch 4/10
17/17 - 5s - loss: 238.0980 - loglik: -2.3844e+02 - logprior: 0.6938
Epoch 5/10
17/17 - 5s - loss: 237.2901 - loglik: -2.3771e+02 - logprior: 0.7991
Epoch 6/10
17/17 - 5s - loss: 237.3079 - loglik: -2.3791e+02 - logprior: 1.0023
Fitted a model with MAP estimate = -234.7196
Time for alignment: 120.0584
Computed alignments with likelihoods: ['-233.2680', '-203.1191', '-234.7196']
Best model has likelihood: -203.1191
time for generating output: 0.3529
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.5284007545587927
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabf7975670>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa9694d520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabf7e16850>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2247e370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2ff432b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabc480d190>, <__main__.SimpleDirichletPrior object at 0x7fabef71d3d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.1778 - loglik: -2.2575e+02 - logprior: -2.0280e+01
Epoch 2/10
10/10 - 1s - loss: 215.4548 - loglik: -2.0986e+02 - logprior: -5.4389e+00
Epoch 3/10
10/10 - 1s - loss: 198.0442 - loglik: -1.9489e+02 - logprior: -2.8703e+00
Epoch 4/10
10/10 - 1s - loss: 189.6989 - loglik: -1.8707e+02 - logprior: -2.2699e+00
Epoch 5/10
10/10 - 1s - loss: 186.3789 - loglik: -1.8396e+02 - logprior: -2.1302e+00
Epoch 6/10
10/10 - 1s - loss: 184.7150 - loglik: -1.8254e+02 - logprior: -1.9503e+00
Epoch 7/10
10/10 - 1s - loss: 183.8208 - loglik: -1.8193e+02 - logprior: -1.6887e+00
Epoch 8/10
10/10 - 1s - loss: 183.5354 - loglik: -1.8177e+02 - logprior: -1.5748e+00
Epoch 9/10
10/10 - 1s - loss: 182.9558 - loglik: -1.8120e+02 - logprior: -1.5625e+00
Epoch 10/10
10/10 - 1s - loss: 182.7683 - loglik: -1.8103e+02 - logprior: -1.5323e+00
Fitted a model with MAP estimate = -182.3861
expansions: [(0, 2), (7, 2), (8, 2), (34, 1), (41, 2), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 211.8359 - loglik: -1.8503e+02 - logprior: -2.6783e+01
Epoch 2/2
10/10 - 1s - loss: 188.2869 - loglik: -1.7998e+02 - logprior: -8.2108e+00
Fitted a model with MAP estimate = -183.3830
expansions: []
discards: [ 0 10 12]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.1555 - loglik: -1.8089e+02 - logprior: -2.3176e+01
Epoch 2/2
10/10 - 1s - loss: 188.7855 - loglik: -1.7966e+02 - logprior: -9.0816e+00
Fitted a model with MAP estimate = -185.2154
expansions: [(0, 2)]
discards: [ 0 48]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 198.8999 - loglik: -1.7863e+02 - logprior: -2.0234e+01
Epoch 2/10
10/10 - 1s - loss: 182.4169 - loglik: -1.7692e+02 - logprior: -5.3274e+00
Epoch 3/10
10/10 - 1s - loss: 179.1654 - loglik: -1.7664e+02 - logprior: -2.2513e+00
Epoch 4/10
10/10 - 1s - loss: 177.3013 - loglik: -1.7573e+02 - logprior: -1.2623e+00
Epoch 5/10
10/10 - 1s - loss: 176.3703 - loglik: -1.7519e+02 - logprior: -8.9861e-01
Epoch 6/10
10/10 - 1s - loss: 175.8858 - loglik: -1.7486e+02 - logprior: -7.6301e-01
Epoch 7/10
10/10 - 1s - loss: 175.8008 - loglik: -1.7495e+02 - logprior: -5.8904e-01
Epoch 8/10
10/10 - 1s - loss: 175.1606 - loglik: -1.7450e+02 - logprior: -4.0324e-01
Epoch 9/10
10/10 - 1s - loss: 175.0308 - loglik: -1.7446e+02 - logprior: -2.9866e-01
Epoch 10/10
10/10 - 1s - loss: 174.9022 - loglik: -1.7437e+02 - logprior: -2.7137e-01
Fitted a model with MAP estimate = -174.5231
Time for alignment: 48.7413
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 245.8801 - loglik: -2.2542e+02 - logprior: -2.0281e+01
Epoch 2/10
10/10 - 1s - loss: 215.9311 - loglik: -2.1033e+02 - logprior: -5.4426e+00
Epoch 3/10
10/10 - 1s - loss: 198.1969 - loglik: -1.9503e+02 - logprior: -2.8909e+00
Epoch 4/10
10/10 - 1s - loss: 189.6723 - loglik: -1.8704e+02 - logprior: -2.2665e+00
Epoch 5/10
10/10 - 1s - loss: 186.6162 - loglik: -1.8420e+02 - logprior: -2.1182e+00
Epoch 6/10
10/10 - 1s - loss: 184.8737 - loglik: -1.8271e+02 - logprior: -1.9288e+00
Epoch 7/10
10/10 - 1s - loss: 184.0767 - loglik: -1.8223e+02 - logprior: -1.6209e+00
Epoch 8/10
10/10 - 1s - loss: 183.6485 - loglik: -1.8194e+02 - logprior: -1.4785e+00
Epoch 9/10
10/10 - 1s - loss: 183.0367 - loglik: -1.8132e+02 - logprior: -1.4931e+00
Epoch 10/10
10/10 - 1s - loss: 182.8777 - loglik: -1.8118e+02 - logprior: -1.4747e+00
Fitted a model with MAP estimate = -182.5667
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (22, 1), (41, 2), (42, 3), (44, 2), (46, 1), (48, 1), (49, 1), (50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.9770 - loglik: -1.8512e+02 - logprior: -2.6722e+01
Epoch 2/2
10/10 - 1s - loss: 188.1076 - loglik: -1.7971e+02 - logprior: -8.1898e+00
Fitted a model with MAP estimate = -183.0286
expansions: []
discards: [ 0 50 62]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 202.8884 - loglik: -1.7964e+02 - logprior: -2.3202e+01
Epoch 2/2
10/10 - 1s - loss: 188.0922 - loglik: -1.7878e+02 - logprior: -9.1468e+00
Fitted a model with MAP estimate = -184.4872
expansions: [(0, 2)]
discards: [ 0 11 54]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.7139 - loglik: -1.7840e+02 - logprior: -2.0207e+01
Epoch 2/10
10/10 - 1s - loss: 182.6956 - loglik: -1.7711e+02 - logprior: -5.3501e+00
Epoch 3/10
10/10 - 1s - loss: 179.3672 - loglik: -1.7672e+02 - logprior: -2.3132e+00
Epoch 4/10
10/10 - 1s - loss: 177.8471 - loglik: -1.7617e+02 - logprior: -1.3071e+00
Epoch 5/10
10/10 - 1s - loss: 176.8939 - loglik: -1.7562e+02 - logprior: -9.3732e-01
Epoch 6/10
10/10 - 1s - loss: 176.6339 - loglik: -1.7554e+02 - logprior: -7.9370e-01
Epoch 7/10
10/10 - 1s - loss: 176.2423 - loglik: -1.7534e+02 - logprior: -6.1853e-01
Epoch 8/10
10/10 - 1s - loss: 175.5757 - loglik: -1.7487e+02 - logprior: -4.1781e-01
Epoch 9/10
10/10 - 1s - loss: 175.5680 - loglik: -1.7495e+02 - logprior: -3.2231e-01
Epoch 10/10
10/10 - 1s - loss: 175.3500 - loglik: -1.7477e+02 - logprior: -2.8559e-01
Fitted a model with MAP estimate = -174.9128
Time for alignment: 45.9440
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.1627 - loglik: -2.2585e+02 - logprior: -2.0282e+01
Epoch 2/10
10/10 - 1s - loss: 215.5695 - loglik: -2.1004e+02 - logprior: -5.4395e+00
Epoch 3/10
10/10 - 1s - loss: 198.4913 - loglik: -1.9539e+02 - logprior: -2.8946e+00
Epoch 4/10
10/10 - 1s - loss: 189.9511 - loglik: -1.8739e+02 - logprior: -2.2355e+00
Epoch 5/10
10/10 - 1s - loss: 186.7640 - loglik: -1.8441e+02 - logprior: -2.0268e+00
Epoch 6/10
10/10 - 1s - loss: 184.8039 - loglik: -1.8270e+02 - logprior: -1.8636e+00
Epoch 7/10
10/10 - 1s - loss: 184.0172 - loglik: -1.8222e+02 - logprior: -1.5750e+00
Epoch 8/10
10/10 - 1s - loss: 183.7075 - loglik: -1.8207e+02 - logprior: -1.4283e+00
Epoch 9/10
10/10 - 1s - loss: 183.4288 - loglik: -1.8179e+02 - logprior: -1.4465e+00
Epoch 10/10
10/10 - 1s - loss: 182.9964 - loglik: -1.8134e+02 - logprior: -1.4530e+00
Fitted a model with MAP estimate = -182.5279
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 3), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 212.5776 - loglik: -1.8576e+02 - logprior: -2.6782e+01
Epoch 2/2
10/10 - 1s - loss: 188.0528 - loglik: -1.7976e+02 - logprior: -8.2777e+00
Fitted a model with MAP estimate = -183.5237
expansions: []
discards: [ 0 10 12 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.0348 - loglik: -1.7970e+02 - logprior: -2.3285e+01
Epoch 2/2
10/10 - 1s - loss: 188.2242 - loglik: -1.7889e+02 - logprior: -9.1721e+00
Fitted a model with MAP estimate = -184.7123
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.0990 - loglik: -1.7858e+02 - logprior: -2.0372e+01
Epoch 2/10
10/10 - 1s - loss: 182.8291 - loglik: -1.7730e+02 - logprior: -5.4479e+00
Epoch 3/10
10/10 - 1s - loss: 179.5907 - loglik: -1.7711e+02 - logprior: -2.3745e+00
Epoch 4/10
10/10 - 1s - loss: 178.2166 - loglik: -1.7670e+02 - logprior: -1.3624e+00
Epoch 5/10
10/10 - 1s - loss: 176.7494 - loglik: -1.7555e+02 - logprior: -9.9229e-01
Epoch 6/10
10/10 - 1s - loss: 176.4511 - loglik: -1.7537e+02 - logprior: -8.5128e-01
Epoch 7/10
10/10 - 1s - loss: 176.1311 - loglik: -1.7521e+02 - logprior: -7.0085e-01
Epoch 8/10
10/10 - 1s - loss: 175.8218 - loglik: -1.7508e+02 - logprior: -5.1328e-01
Epoch 9/10
10/10 - 1s - loss: 175.6236 - loglik: -1.7500e+02 - logprior: -3.8544e-01
Epoch 10/10
10/10 - 1s - loss: 175.2904 - loglik: -1.7472e+02 - logprior: -3.2670e-01
Fitted a model with MAP estimate = -174.9628
Time for alignment: 45.9944
Computed alignments with likelihoods: ['-174.5231', '-174.9128', '-174.9628']
Best model has likelihood: -174.5231
time for generating output: 0.1658
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6746268656716418
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faba1d4c280>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac11096bb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fab052aad60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab052aaca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba235b0d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac11048820>, <__main__.SimpleDirichletPrior object at 0x7faa8bff2280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 198.8773 - loglik: -1.8730e+02 - logprior: -1.1497e+01
Epoch 2/10
11/11 - 1s - loss: 159.1502 - loglik: -1.5592e+02 - logprior: -3.2101e+00
Epoch 3/10
11/11 - 1s - loss: 126.5438 - loglik: -1.2415e+02 - logprior: -2.3376e+00
Epoch 4/10
11/11 - 1s - loss: 109.6417 - loglik: -1.0728e+02 - logprior: -2.2853e+00
Epoch 5/10
11/11 - 1s - loss: 104.4767 - loglik: -1.0229e+02 - logprior: -1.9675e+00
Epoch 6/10
11/11 - 1s - loss: 102.8978 - loglik: -1.0063e+02 - logprior: -1.9269e+00
Epoch 7/10
11/11 - 1s - loss: 101.9314 - loglik: -9.9674e+01 - logprior: -1.9436e+00
Epoch 8/10
11/11 - 1s - loss: 101.2331 - loglik: -9.9045e+01 - logprior: -1.8862e+00
Epoch 9/10
11/11 - 1s - loss: 101.4188 - loglik: -9.9218e+01 - logprior: -1.8820e+00
Fitted a model with MAP estimate = -100.6376
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.8881 - loglik: -9.9139e+01 - logprior: -1.3733e+01
Epoch 2/2
11/11 - 1s - loss: 95.6185 - loglik: -9.1261e+01 - logprior: -4.2535e+00
Fitted a model with MAP estimate = -92.5531
expansions: []
discards: [ 0 35 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.9965 - loglik: -9.2793e+01 - logprior: -1.3129e+01
Epoch 2/2
11/11 - 1s - loss: 95.8872 - loglik: -9.0404e+01 - logprior: -5.3894e+00
Fitted a model with MAP estimate = -93.8931
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6403 - loglik: -9.0773e+01 - logprior: -1.0788e+01
Epoch 2/10
11/11 - 1s - loss: 92.7973 - loglik: -8.9677e+01 - logprior: -2.9381e+00
Epoch 3/10
11/11 - 1s - loss: 91.0487 - loglik: -8.9038e+01 - logprior: -1.7734e+00
Epoch 4/10
11/11 - 1s - loss: 90.1696 - loglik: -8.8300e+01 - logprior: -1.5856e+00
Epoch 5/10
11/11 - 1s - loss: 89.3419 - loglik: -8.7491e+01 - logprior: -1.5192e+00
Epoch 6/10
11/11 - 1s - loss: 89.1333 - loglik: -8.7423e+01 - logprior: -1.3601e+00
Epoch 7/10
11/11 - 1s - loss: 88.4935 - loglik: -8.6886e+01 - logprior: -1.2578e+00
Epoch 8/10
11/11 - 1s - loss: 88.5157 - loglik: -8.6920e+01 - logprior: -1.2457e+00
Fitted a model with MAP estimate = -88.0883
Time for alignment: 40.3861
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.9063 - loglik: -1.8719e+02 - logprior: -1.1495e+01
Epoch 2/10
11/11 - 1s - loss: 158.2127 - loglik: -1.5490e+02 - logprior: -3.2205e+00
Epoch 3/10
11/11 - 1s - loss: 123.7319 - loglik: -1.2126e+02 - logprior: -2.3921e+00
Epoch 4/10
11/11 - 1s - loss: 108.1294 - loglik: -1.0574e+02 - logprior: -2.2968e+00
Epoch 5/10
11/11 - 1s - loss: 103.7272 - loglik: -1.0157e+02 - logprior: -2.0337e+00
Epoch 6/10
11/11 - 1s - loss: 101.8731 - loglik: -9.9601e+01 - logprior: -2.0035e+00
Epoch 7/10
11/11 - 1s - loss: 101.4483 - loglik: -9.9085e+01 - logprior: -1.9897e+00
Epoch 8/10
11/11 - 1s - loss: 100.7968 - loglik: -9.8493e+01 - logprior: -1.9418e+00
Epoch 9/10
11/11 - 1s - loss: 100.6171 - loglik: -9.8313e+01 - logprior: -1.9605e+00
Epoch 10/10
11/11 - 1s - loss: 100.7106 - loglik: -9.8399e+01 - logprior: -1.9544e+00
Fitted a model with MAP estimate = -99.9480
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 112.6894 - loglik: -9.8776e+01 - logprior: -1.3752e+01
Epoch 2/2
11/11 - 1s - loss: 95.2451 - loglik: -9.0782e+01 - logprior: -4.2643e+00
Fitted a model with MAP estimate = -92.4996
expansions: []
discards: [36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 100.6774 - loglik: -9.0075e+01 - logprior: -1.0571e+01
Epoch 2/2
11/11 - 1s - loss: 92.9581 - loglik: -8.9898e+01 - logprior: -2.9745e+00
Fitted a model with MAP estimate = -91.0240
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 104.9999 - loglik: -9.1797e+01 - logprior: -1.3060e+01
Epoch 2/10
11/11 - 1s - loss: 96.0471 - loglik: -9.0314e+01 - logprior: -5.5415e+00
Epoch 3/10
11/11 - 1s - loss: 93.0767 - loglik: -8.9257e+01 - logprior: -3.5773e+00
Epoch 4/10
11/11 - 1s - loss: 91.0716 - loglik: -8.8951e+01 - logprior: -1.8472e+00
Epoch 5/10
11/11 - 1s - loss: 89.7137 - loglik: -8.8027e+01 - logprior: -1.3701e+00
Epoch 6/10
11/11 - 1s - loss: 89.6175 - loglik: -8.7957e+01 - logprior: -1.3111e+00
Epoch 7/10
11/11 - 1s - loss: 88.9606 - loglik: -8.7374e+01 - logprior: -1.2443e+00
Epoch 8/10
11/11 - 1s - loss: 88.7277 - loglik: -8.7109e+01 - logprior: -1.2675e+00
Epoch 9/10
11/11 - 1s - loss: 88.7797 - loglik: -8.7190e+01 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -88.2103
Time for alignment: 41.5590
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 199.2245 - loglik: -1.8772e+02 - logprior: -1.1496e+01
Epoch 2/10
11/11 - 1s - loss: 158.1190 - loglik: -1.5491e+02 - logprior: -3.2030e+00
Epoch 3/10
11/11 - 1s - loss: 125.1288 - loglik: -1.2273e+02 - logprior: -2.3356e+00
Epoch 4/10
11/11 - 1s - loss: 109.4216 - loglik: -1.0704e+02 - logprior: -2.2829e+00
Epoch 5/10
11/11 - 1s - loss: 104.4476 - loglik: -1.0225e+02 - logprior: -2.0199e+00
Epoch 6/10
11/11 - 1s - loss: 102.4930 - loglik: -1.0015e+02 - logprior: -2.0250e+00
Epoch 7/10
11/11 - 1s - loss: 101.8281 - loglik: -9.9476e+01 - logprior: -2.0223e+00
Epoch 8/10
11/11 - 1s - loss: 101.8167 - loglik: -9.9560e+01 - logprior: -1.9533e+00
Epoch 9/10
11/11 - 1s - loss: 101.1323 - loglik: -9.8857e+01 - logprior: -1.9562e+00
Epoch 10/10
11/11 - 1s - loss: 100.6807 - loglik: -9.8422e+01 - logprior: -1.9459e+00
Fitted a model with MAP estimate = -100.6366
expansions: [(0, 3), (15, 1), (26, 1), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 113.3961 - loglik: -9.9646e+01 - logprior: -1.3739e+01
Epoch 2/2
11/11 - 1s - loss: 95.5579 - loglik: -9.1241e+01 - logprior: -4.2540e+00
Fitted a model with MAP estimate = -92.6517
expansions: []
discards: [ 0 35 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.5122 - loglik: -9.2353e+01 - logprior: -1.3123e+01
Epoch 2/2
11/11 - 1s - loss: 96.6388 - loglik: -9.1177e+01 - logprior: -5.3925e+00
Fitted a model with MAP estimate = -93.9557
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.5443 - loglik: -9.0649e+01 - logprior: -1.0802e+01
Epoch 2/10
11/11 - 1s - loss: 92.9639 - loglik: -8.9848e+01 - logprior: -2.9321e+00
Epoch 3/10
11/11 - 1s - loss: 91.0789 - loglik: -8.9075e+01 - logprior: -1.7767e+00
Epoch 4/10
11/11 - 1s - loss: 89.9072 - loglik: -8.8035e+01 - logprior: -1.5821e+00
Epoch 5/10
11/11 - 1s - loss: 89.4782 - loglik: -8.7622e+01 - logprior: -1.5205e+00
Epoch 6/10
11/11 - 1s - loss: 89.1183 - loglik: -8.7405e+01 - logprior: -1.3672e+00
Epoch 7/10
11/11 - 1s - loss: 88.5183 - loglik: -8.6902e+01 - logprior: -1.2619e+00
Epoch 8/10
11/11 - 1s - loss: 88.7218 - loglik: -8.7118e+01 - logprior: -1.2493e+00
Fitted a model with MAP estimate = -88.0734
Time for alignment: 39.3281
Computed alignments with likelihoods: ['-88.0883', '-88.2103', '-88.0734']
Best model has likelihood: -88.0734
time for generating output: 0.1304
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac088f7580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa95d44340>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa95f195e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac11385ee0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafc526910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faba18202b0>, <__main__.SimpleDirichletPrior object at 0x7fac3341cf10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 171.4650 - loglik: -1.7060e+02 - logprior: -7.2542e-01
Epoch 2/10
42/42 - 4s - loss: 81.2029 - loglik: -8.0345e+01 - logprior: -6.3289e-01
Epoch 3/10
42/42 - 5s - loss: 78.6279 - loglik: -7.7774e+01 - logprior: -6.1801e-01
Epoch 4/10
42/42 - 5s - loss: 78.5196 - loglik: -7.7700e+01 - logprior: -5.9799e-01
Epoch 5/10
42/42 - 4s - loss: 77.5476 - loglik: -7.6730e+01 - logprior: -6.0088e-01
Epoch 6/10
42/42 - 5s - loss: 77.8075 - loglik: -7.7019e+01 - logprior: -5.8022e-01
Fitted a model with MAP estimate = -76.7710
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 10s - loss: 46.2946 - loglik: -4.5229e+01 - logprior: -8.1562e-01
Epoch 2/2
42/42 - 5s - loss: 33.3165 - loglik: -3.2455e+01 - logprior: -6.0847e-01
Fitted a model with MAP estimate = -32.5877
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.4126 - loglik: -3.6321e+01 - logprior: -9.4123e-01
Epoch 2/2
42/42 - 5s - loss: 34.5071 - loglik: -3.3782e+01 - logprior: -4.7140e-01
Fitted a model with MAP estimate = -33.7835
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.8882 - loglik: -3.3207e+01 - logprior: -5.7899e-01
Epoch 2/10
59/59 - 7s - loss: 32.0006 - loglik: -3.1236e+01 - logprior: -5.5094e-01
Epoch 3/10
59/59 - 7s - loss: 32.8263 - loglik: -3.2060e+01 - logprior: -5.3868e-01
Fitted a model with MAP estimate = -31.7407
Time for alignment: 178.7670
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 171.2931 - loglik: -1.7033e+02 - logprior: -7.2693e-01
Epoch 2/10
42/42 - 5s - loss: 81.2234 - loglik: -8.0311e+01 - logprior: -6.3118e-01
Epoch 3/10
42/42 - 5s - loss: 78.6560 - loglik: -7.7797e+01 - logprior: -6.1995e-01
Epoch 4/10
42/42 - 5s - loss: 78.4782 - loglik: -7.7644e+01 - logprior: -6.0199e-01
Epoch 5/10
42/42 - 5s - loss: 77.8918 - loglik: -7.7082e+01 - logprior: -5.9455e-01
Epoch 6/10
42/42 - 4s - loss: 77.6856 - loglik: -7.6893e+01 - logprior: -5.8413e-01
Epoch 7/10
42/42 - 4s - loss: 77.6558 - loglik: -7.6875e+01 - logprior: -5.7858e-01
Epoch 8/10
42/42 - 4s - loss: 77.1327 - loglik: -7.6350e+01 - logprior: -5.8193e-01
Epoch 9/10
42/42 - 5s - loss: 77.7046 - loglik: -7.6937e+01 - logprior: -5.7327e-01
Fitted a model with MAP estimate = -76.6251
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.7599 - loglik: -4.5734e+01 - logprior: -8.1907e-01
Epoch 2/2
42/42 - 5s - loss: 33.2704 - loglik: -3.2413e+01 - logprior: -6.0604e-01
Fitted a model with MAP estimate = -32.6150
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 12s - loss: 33.2515 - loglik: -3.2706e+01 - logprior: -4.6050e-01
Epoch 2/10
59/59 - 7s - loss: 31.9244 - loglik: -3.1320e+01 - logprior: -3.9728e-01
Epoch 3/10
59/59 - 7s - loss: 32.0633 - loglik: -3.1450e+01 - logprior: -3.8320e-01
Fitted a model with MAP estimate = -31.3165
Time for alignment: 149.9086
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.4200 - loglik: -1.7046e+02 - logprior: -7.2286e-01
Epoch 2/10
42/42 - 5s - loss: 81.0158 - loglik: -8.0177e+01 - logprior: -6.3041e-01
Epoch 3/10
42/42 - 4s - loss: 78.1412 - loglik: -7.7312e+01 - logprior: -6.2189e-01
Epoch 4/10
42/42 - 4s - loss: 77.5529 - loglik: -7.6741e+01 - logprior: -6.0809e-01
Epoch 5/10
42/42 - 4s - loss: 77.5445 - loglik: -7.6769e+01 - logprior: -5.8823e-01
Epoch 6/10
42/42 - 4s - loss: 77.2496 - loglik: -7.6481e+01 - logprior: -5.8469e-01
Epoch 7/10
42/42 - 5s - loss: 76.9884 - loglik: -7.6229e+01 - logprior: -5.8087e-01
Epoch 8/10
42/42 - 4s - loss: 76.5429 - loglik: -7.5790e+01 - logprior: -5.8039e-01
Epoch 9/10
42/42 - 4s - loss: 77.0523 - loglik: -7.6309e+01 - logprior: -5.7606e-01
Fitted a model with MAP estimate = -76.1315
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.6376 - loglik: -4.5667e+01 - logprior: -8.2548e-01
Epoch 2/2
42/42 - 5s - loss: 33.1711 - loglik: -3.2307e+01 - logprior: -6.0708e-01
Fitted a model with MAP estimate = -32.6164
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 32.1817 - loglik: -3.1520e+01 - logprior: -4.5848e-01
Epoch 2/10
59/59 - 6s - loss: 32.1496 - loglik: -3.1521e+01 - logprior: -3.9325e-01
Epoch 3/10
59/59 - 7s - loss: 31.8806 - loglik: -3.1263e+01 - logprior: -3.8253e-01
Epoch 4/10
59/59 - 7s - loss: 31.5360 - loglik: -3.0989e+01 - logprior: -3.7536e-01
Epoch 5/10
59/59 - 7s - loss: 31.0048 - loglik: -3.0473e+01 - logprior: -3.6963e-01
Epoch 6/10
59/59 - 7s - loss: 31.5265 - loglik: -3.1002e+01 - logprior: -3.6266e-01
Fitted a model with MAP estimate = -30.7101
Time for alignment: 169.8984
Computed alignments with likelihoods: ['-31.7407', '-31.3165', '-30.7101']
Best model has likelihood: -30.7101
time for generating output: 0.1698
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.29093369418132614
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac19ef37f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faab892c940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faadab72940>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab052dd760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab15d969d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab2fc19130>, <__main__.SimpleDirichletPrior object at 0x7fab15d806a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.7755 - loglik: -4.5133e+02 - logprior: -2.3383e+01
Epoch 2/10
14/14 - 4s - loss: 409.4152 - loglik: -4.0574e+02 - logprior: -3.5409e+00
Epoch 3/10
14/14 - 4s - loss: 368.9082 - loglik: -3.6685e+02 - logprior: -1.6394e+00
Epoch 4/10
14/14 - 4s - loss: 358.0789 - loglik: -3.5614e+02 - logprior: -1.3157e+00
Epoch 5/10
14/14 - 4s - loss: 353.5608 - loglik: -3.5184e+02 - logprior: -1.1291e+00
Epoch 6/10
14/14 - 4s - loss: 350.8404 - loglik: -3.4936e+02 - logprior: -9.3577e-01
Epoch 7/10
14/14 - 4s - loss: 349.2144 - loglik: -3.4778e+02 - logprior: -8.5677e-01
Epoch 8/10
14/14 - 4s - loss: 350.0690 - loglik: -3.4870e+02 - logprior: -7.7933e-01
Fitted a model with MAP estimate = -348.4164
expansions: [(12, 2), (17, 7), (19, 1), (36, 1), (38, 2), (43, 1), (44, 1), (45, 1), (66, 2), (76, 1), (78, 2), (79, 4), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 380.9006 - loglik: -3.5284e+02 - logprior: -2.7950e+01
Epoch 2/2
14/14 - 4s - loss: 354.5067 - loglik: -3.4448e+02 - logprior: -9.7201e+00
Fitted a model with MAP estimate = -348.5856
expansions: [(0, 19), (58, 1), (83, 1)]
discards: [  0  11  12  13  14  15  48  60  61  97 100 101 140 141]
Re-initialized the encoder parameters.
Fitting a model of length 164 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 373.2942 - loglik: -3.5208e+02 - logprior: -2.1106e+01
Epoch 2/2
14/14 - 5s - loss: 349.6612 - loglik: -3.4639e+02 - logprior: -3.0765e+00
Fitted a model with MAP estimate = -345.6585
expansions: [(29, 3), (149, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 73]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 371.2720 - loglik: -3.4619e+02 - logprior: -2.5056e+01
Epoch 2/10
14/14 - 4s - loss: 346.5861 - loglik: -3.4319e+02 - logprior: -3.1940e+00
Epoch 3/10
14/14 - 4s - loss: 341.1129 - loglik: -3.4118e+02 - logprior: 0.5450
Epoch 4/10
14/14 - 4s - loss: 337.4584 - loglik: -3.3840e+02 - logprior: 1.5734
Epoch 5/10
14/14 - 4s - loss: 336.3748 - loglik: -3.3772e+02 - logprior: 1.9694
Epoch 6/10
14/14 - 4s - loss: 336.2414 - loglik: -3.3800e+02 - logprior: 2.3356
Epoch 7/10
14/14 - 4s - loss: 335.2089 - loglik: -3.3725e+02 - logprior: 2.6023
Epoch 8/10
14/14 - 4s - loss: 333.8469 - loglik: -3.3605e+02 - logprior: 2.7891
Epoch 9/10
14/14 - 4s - loss: 334.0944 - loglik: -3.3650e+02 - logprior: 2.9895
Fitted a model with MAP estimate = -332.6329
Time for alignment: 115.0435
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 475.2275 - loglik: -4.5180e+02 - logprior: -2.3375e+01
Epoch 2/10
14/14 - 4s - loss: 408.6701 - loglik: -4.0501e+02 - logprior: -3.5557e+00
Epoch 3/10
14/14 - 4s - loss: 370.6481 - loglik: -3.6854e+02 - logprior: -1.7148e+00
Epoch 4/10
14/14 - 4s - loss: 357.9007 - loglik: -3.5574e+02 - logprior: -1.5096e+00
Epoch 5/10
14/14 - 4s - loss: 355.4878 - loglik: -3.5358e+02 - logprior: -1.2885e+00
Epoch 6/10
14/14 - 4s - loss: 350.8032 - loglik: -3.4909e+02 - logprior: -1.1098e+00
Epoch 7/10
14/14 - 4s - loss: 350.3101 - loglik: -3.4860e+02 - logprior: -1.0855e+00
Epoch 8/10
14/14 - 4s - loss: 350.5415 - loglik: -3.4885e+02 - logprior: -1.0755e+00
Fitted a model with MAP estimate = -349.3695
expansions: [(10, 1), (11, 1), (15, 1), (16, 5), (17, 1), (18, 1), (36, 1), (42, 1), (43, 1), (45, 1), (66, 1), (67, 2), (79, 2), (80, 4), (81, 1), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 382.9428 - loglik: -3.5482e+02 - logprior: -2.7995e+01
Epoch 2/2
14/14 - 5s - loss: 353.3151 - loglik: -3.4342e+02 - logprior: -9.6913e+00
Fitted a model with MAP estimate = -348.6212
expansions: [(0, 23)]
discards: [ 0 17 83 84 85]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 368.9189 - loglik: -3.4789e+02 - logprior: -2.0987e+01
Epoch 2/2
14/14 - 5s - loss: 347.0828 - loglik: -3.4396e+02 - logprior: -2.9137e+00
Fitted a model with MAP estimate = -342.9986
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  79  80 104]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 374.8020 - loglik: -3.4986e+02 - logprior: -2.4845e+01
Epoch 2/10
14/14 - 4s - loss: 350.9163 - loglik: -3.4757e+02 - logprior: -3.2231e+00
Epoch 3/10
14/14 - 4s - loss: 344.9495 - loglik: -3.4501e+02 - logprior: 0.3001
Epoch 4/10
14/14 - 4s - loss: 342.4000 - loglik: -3.4331e+02 - logprior: 1.2907
Epoch 5/10
14/14 - 4s - loss: 340.5587 - loglik: -3.4182e+02 - logprior: 1.7650
Epoch 6/10
14/14 - 4s - loss: 340.2488 - loglik: -3.4182e+02 - logprior: 2.1244
Epoch 7/10
14/14 - 5s - loss: 339.1971 - loglik: -3.4102e+02 - logprior: 2.3774
Epoch 8/10
14/14 - 4s - loss: 338.3923 - loglik: -3.4048e+02 - logprior: 2.6344
Epoch 9/10
14/14 - 4s - loss: 337.3516 - loglik: -3.3964e+02 - logprior: 2.8342
Epoch 10/10
14/14 - 4s - loss: 338.8639 - loglik: -3.4136e+02 - logprior: 3.0454
Fitted a model with MAP estimate = -336.9928
Time for alignment: 120.8285
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.7397 - loglik: -4.5122e+02 - logprior: -2.3396e+01
Epoch 2/10
14/14 - 4s - loss: 409.4001 - loglik: -4.0573e+02 - logprior: -3.5848e+00
Epoch 3/10
14/14 - 4s - loss: 370.0406 - loglik: -3.6803e+02 - logprior: -1.6914e+00
Epoch 4/10
14/14 - 4s - loss: 357.1467 - loglik: -3.5526e+02 - logprior: -1.3330e+00
Epoch 5/10
14/14 - 4s - loss: 355.3687 - loglik: -3.5362e+02 - logprior: -1.1480e+00
Epoch 6/10
14/14 - 4s - loss: 350.9328 - loglik: -3.4937e+02 - logprior: -9.6271e-01
Epoch 7/10
14/14 - 4s - loss: 350.6974 - loglik: -3.4926e+02 - logprior: -8.2264e-01
Epoch 8/10
14/14 - 4s - loss: 350.9764 - loglik: -3.4961e+02 - logprior: -7.6181e-01
Fitted a model with MAP estimate = -349.6336
expansions: [(12, 1), (16, 6), (17, 1), (19, 1), (36, 1), (44, 1), (46, 1), (55, 1), (66, 3), (79, 6), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 385.0975 - loglik: -3.5690e+02 - logprior: -2.8174e+01
Epoch 2/2
14/14 - 4s - loss: 353.8037 - loglik: -3.4382e+02 - logprior: -9.8411e+00
Fitted a model with MAP estimate = -349.9655
expansions: [(0, 26)]
discards: [ 0 12 13 54 55]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 370.3586 - loglik: -3.4908e+02 - logprior: -2.1079e+01
Epoch 2/2
14/14 - 5s - loss: 348.3691 - loglik: -3.4516e+02 - logprior: -2.8698e+00
Fitted a model with MAP estimate = -344.0142
expansions: [(37, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 369.2215 - loglik: -3.4401e+02 - logprior: -2.5149e+01
Epoch 2/10
14/14 - 4s - loss: 346.6159 - loglik: -3.4310e+02 - logprior: -3.2876e+00
Epoch 3/10
14/14 - 4s - loss: 340.6985 - loglik: -3.4090e+02 - logprior: 0.5507
Epoch 4/10
14/14 - 4s - loss: 339.6196 - loglik: -3.4073e+02 - logprior: 1.6065
Epoch 5/10
14/14 - 4s - loss: 336.4068 - loglik: -3.3783e+02 - logprior: 2.0218
Epoch 6/10
14/14 - 4s - loss: 335.9197 - loglik: -3.3767e+02 - logprior: 2.3760
Epoch 7/10
14/14 - 4s - loss: 335.9509 - loglik: -3.3803e+02 - logprior: 2.6871
Fitted a model with MAP estimate = -333.9976
Time for alignment: 106.6421
Computed alignments with likelihoods: ['-332.6329', '-336.9928', '-333.9976']
Best model has likelihood: -332.6329
time for generating output: 0.2176
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.973097940311055
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa96c15160>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac22511760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabe68b1af0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaad3dd60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab30252a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faae2ee3d30>, <__main__.SimpleDirichletPrior object at 0x7faba171ac70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 858.9109 - loglik: -8.5591e+02 - logprior: -2.7305e+00
Epoch 2/10
29/29 - 21s - loss: 669.6874 - loglik: -6.6721e+02 - logprior: -1.7691e+00
Epoch 3/10
29/29 - 21s - loss: 643.7871 - loglik: -6.4104e+02 - logprior: -2.0800e+00
Epoch 4/10
29/29 - 22s - loss: 638.6751 - loglik: -6.3606e+02 - logprior: -2.0642e+00
Epoch 5/10
29/29 - 21s - loss: 634.1718 - loglik: -6.3160e+02 - logprior: -2.0854e+00
Epoch 6/10
29/29 - 22s - loss: 635.0147 - loglik: -6.3244e+02 - logprior: -2.1114e+00
Fitted a model with MAP estimate = -632.9822
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 2), (38, 2), (48, 2), (49, 2), (52, 1), (64, 1), (73, 1), (86, 1), (87, 1), (88, 1), (89, 1), (94, 2), (119, 2), (121, 2), (123, 1), (124, 1), (128, 1), (142, 1), (144, 1), (152, 1), (155, 1), (156, 1), (163, 1), (164, 1), (183, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (255, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 344 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 622.1376 - loglik: -6.1683e+02 - logprior: -5.0722e+00
Epoch 2/2
29/29 - 29s - loss: 591.8611 - loglik: -5.8911e+02 - logprior: -2.3196e+00
Fitted a model with MAP estimate = -587.2342
expansions: [(0, 2)]
discards: [  0  37  63  64 265 304 330 331]
Re-initialized the encoder parameters.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 598.0385 - loglik: -5.9498e+02 - logprior: -2.7632e+00
Epoch 2/2
29/29 - 28s - loss: 587.8136 - loglik: -5.8708e+02 - logprior: -2.2630e-01
Fitted a model with MAP estimate = -585.4075
expansions: [(326, 2), (328, 2)]
discards: [ 0 47]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 597.4143 - loglik: -5.9261e+02 - logprior: -4.5482e+00
Epoch 2/10
29/29 - 29s - loss: 588.1125 - loglik: -5.8706e+02 - logprior: -6.1922e-01
Epoch 3/10
29/29 - 28s - loss: 586.3999 - loglik: -5.8601e+02 - logprior: 0.1855
Epoch 4/10
29/29 - 29s - loss: 582.2930 - loglik: -5.8201e+02 - logprior: 0.3514
Epoch 5/10
29/29 - 29s - loss: 581.7111 - loglik: -5.8167e+02 - logprior: 0.6020
Epoch 6/10
29/29 - 29s - loss: 582.2770 - loglik: -5.8232e+02 - logprior: 0.6520
Fitted a model with MAP estimate = -580.1913
Time for alignment: 540.4531
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 855.4492 - loglik: -8.5258e+02 - logprior: -2.7570e+00
Epoch 2/10
29/29 - 22s - loss: 675.1613 - loglik: -6.7288e+02 - logprior: -1.8263e+00
Epoch 3/10
29/29 - 22s - loss: 642.4392 - loglik: -6.3969e+02 - logprior: -2.1789e+00
Epoch 4/10
29/29 - 21s - loss: 633.9896 - loglik: -6.3120e+02 - logprior: -2.2114e+00
Epoch 5/10
29/29 - 22s - loss: 630.4299 - loglik: -6.2774e+02 - logprior: -2.1692e+00
Epoch 6/10
29/29 - 21s - loss: 629.3416 - loglik: -6.2666e+02 - logprior: -2.1851e+00
Epoch 7/10
29/29 - 22s - loss: 628.2504 - loglik: -6.2553e+02 - logprior: -2.2352e+00
Epoch 8/10
29/29 - 22s - loss: 626.2310 - loglik: -6.2347e+02 - logprior: -2.2663e+00
Epoch 9/10
29/29 - 21s - loss: 628.4817 - loglik: -6.2572e+02 - logprior: -2.2631e+00
Fitted a model with MAP estimate = -626.4329
expansions: [(15, 1), (17, 1), (24, 2), (27, 1), (28, 1), (29, 1), (30, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 2), (65, 1), (76, 1), (87, 1), (88, 2), (89, 2), (94, 2), (121, 3), (123, 1), (124, 1), (128, 1), (142, 1), (144, 1), (149, 1), (154, 1), (155, 1), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 617.2410 - loglik: -6.1204e+02 - logprior: -5.0320e+00
Epoch 2/2
29/29 - 29s - loss: 587.8329 - loglik: -5.8514e+02 - logprior: -2.2034e+00
Fitted a model with MAP estimate = -584.7936
expansions: [(0, 2)]
discards: [  0  61 105 108 144 263 302 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 595.6619 - loglik: -5.9282e+02 - logprior: -2.6224e+00
Epoch 2/2
29/29 - 28s - loss: 584.5866 - loglik: -5.8422e+02 - logprior: -4.0730e-03
Fitted a model with MAP estimate = -583.3321
expansions: [(323, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 594.4959 - loglik: -5.8983e+02 - logprior: -4.3321e+00
Epoch 2/10
29/29 - 28s - loss: 585.8069 - loglik: -5.8476e+02 - logprior: -4.1567e-01
Epoch 3/10
29/29 - 28s - loss: 583.5746 - loglik: -5.8307e+02 - logprior: 0.1977
Epoch 4/10
29/29 - 28s - loss: 582.2445 - loglik: -5.8225e+02 - logprior: 0.6850
Epoch 5/10
29/29 - 28s - loss: 579.5532 - loglik: -5.7946e+02 - logprior: 0.5277
Epoch 6/10
29/29 - 28s - loss: 578.7014 - loglik: -5.7920e+02 - logprior: 1.0870
Epoch 7/10
29/29 - 28s - loss: 580.3190 - loglik: -5.8068e+02 - logprior: 0.9238
Fitted a model with MAP estimate = -578.1339
Time for alignment: 628.4777
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 858.4275 - loglik: -8.5543e+02 - logprior: -2.7243e+00
Epoch 2/10
29/29 - 22s - loss: 669.3821 - loglik: -6.6688e+02 - logprior: -1.9367e+00
Epoch 3/10
29/29 - 22s - loss: 639.4492 - loglik: -6.3661e+02 - logprior: -2.2601e+00
Epoch 4/10
29/29 - 22s - loss: 633.1946 - loglik: -6.3048e+02 - logprior: -2.2079e+00
Epoch 5/10
29/29 - 22s - loss: 629.7245 - loglik: -6.2708e+02 - logprior: -2.1700e+00
Epoch 6/10
29/29 - 22s - loss: 630.2303 - loglik: -6.2756e+02 - logprior: -2.1909e+00
Fitted a model with MAP estimate = -627.9417
expansions: [(16, 1), (22, 1), (24, 1), (26, 1), (28, 1), (29, 1), (36, 1), (38, 1), (48, 2), (49, 3), (76, 1), (87, 1), (88, 2), (89, 2), (94, 1), (119, 2), (120, 2), (123, 1), (124, 2), (127, 1), (141, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (262, 1), (263, 1), (264, 1), (269, 2), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 620.1566 - loglik: -6.1488e+02 - logprior: -5.1029e+00
Epoch 2/2
29/29 - 29s - loss: 590.8105 - loglik: -5.8811e+02 - logprior: -2.3325e+00
Fitted a model with MAP estimate = -585.3698
expansions: [(0, 2), (35, 1), (106, 1)]
discards: [  0  59  60 103 139 148 261 314 330]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 593.8146 - loglik: -5.9048e+02 - logprior: -3.0406e+00
Epoch 2/2
29/29 - 28s - loss: 587.4760 - loglik: -5.8665e+02 - logprior: -2.9553e-01
Fitted a model with MAP estimate = -583.4993
expansions: [(322, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 33s - loss: 594.8007 - loglik: -5.9028e+02 - logprior: -4.3097e+00
Epoch 2/10
29/29 - 28s - loss: 587.9322 - loglik: -5.8671e+02 - logprior: -7.7890e-01
Epoch 3/10
29/29 - 29s - loss: 583.4755 - loglik: -5.8334e+02 - logprior: 0.4227
Epoch 4/10
29/29 - 28s - loss: 581.2678 - loglik: -5.8088e+02 - logprior: 0.2230
Epoch 5/10
29/29 - 28s - loss: 580.7734 - loglik: -5.8055e+02 - logprior: 0.3856
Epoch 6/10
29/29 - 29s - loss: 579.8376 - loglik: -5.8020e+02 - logprior: 0.9494
Epoch 7/10
29/29 - 28s - loss: 579.3243 - loglik: -5.7999e+02 - logprior: 1.2331
Epoch 8/10
29/29 - 28s - loss: 578.8577 - loglik: -5.7973e+02 - logprior: 1.4255
Epoch 9/10
29/29 - 29s - loss: 579.3574 - loglik: -5.7962e+02 - logprior: 0.7991
Fitted a model with MAP estimate = -578.5201
Time for alignment: 622.2091
Computed alignments with likelihoods: ['-580.1913', '-578.1339', '-578.5201']
Best model has likelihood: -578.1339
time for generating output: 0.4359
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8795947901591896
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac55218700>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabd5e52d00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faafc526b50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3ba768e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4439e5e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab0522f910>, <__main__.SimpleDirichletPrior object at 0x7fac3bbdba30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.4930 - loglik: -1.2826e+02 - logprior: -4.8900e+02
Epoch 2/10
10/10 - 0s - loss: 242.6948 - loglik: -1.0737e+02 - logprior: -1.3522e+02
Epoch 3/10
10/10 - 1s - loss: 150.8854 - loglik: -8.8307e+01 - logprior: -6.2531e+01
Epoch 4/10
10/10 - 0s - loss: 110.3120 - loglik: -7.5315e+01 - logprior: -3.4927e+01
Epoch 5/10
10/10 - 0s - loss: 90.4933 - loglik: -7.0135e+01 - logprior: -2.0278e+01
Epoch 6/10
10/10 - 0s - loss: 80.1698 - loglik: -6.9313e+01 - logprior: -1.0691e+01
Epoch 7/10
10/10 - 1s - loss: 74.3248 - loglik: -6.9667e+01 - logprior: -4.4371e+00
Epoch 8/10
10/10 - 1s - loss: 70.7557 - loglik: -6.9906e+01 - logprior: -6.4854e-01
Epoch 9/10
10/10 - 0s - loss: 68.3985 - loglik: -7.0105e+01 - logprior: 1.9030
Epoch 10/10
10/10 - 0s - loss: 66.7182 - loglik: -7.0290e+01 - logprior: 3.7832
Fitted a model with MAP estimate = -65.7501
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 715.3195 - loglik: -6.2903e+01 - logprior: -6.5236e+02
Epoch 2/2
10/10 - 0s - loss: 258.5049 - loglik: -5.4878e+01 - logprior: -2.0362e+02
Fitted a model with MAP estimate = -171.9032
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 505.6549 - loglik: -4.9829e+01 - logprior: -4.5566e+02
Epoch 2/2
10/10 - 1s - loss: 171.2256 - loglik: -4.9722e+01 - logprior: -1.2145e+02
Fitted a model with MAP estimate = -121.2328
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 477.0063 - loglik: -4.8866e+01 - logprior: -4.2808e+02
Epoch 2/10
10/10 - 1s - loss: 163.6898 - loglik: -4.9521e+01 - logprior: -1.1405e+02
Epoch 3/10
10/10 - 1s - loss: 97.8011 - loglik: -5.0253e+01 - logprior: -4.7459e+01
Epoch 4/10
10/10 - 1s - loss: 69.7658 - loglik: -5.0815e+01 - logprior: -1.8886e+01
Epoch 5/10
10/10 - 0s - loss: 54.3259 - loglik: -5.1268e+01 - logprior: -2.9826e+00
Epoch 6/10
10/10 - 0s - loss: 45.3072 - loglik: -5.1678e+01 - logprior: 6.4578
Epoch 7/10
10/10 - 1s - loss: 39.6884 - loglik: -5.2029e+01 - logprior: 12.4215
Epoch 8/10
10/10 - 0s - loss: 35.8619 - loglik: -5.2297e+01 - logprior: 16.5217
Epoch 9/10
10/10 - 1s - loss: 33.0135 - loglik: -5.2516e+01 - logprior: 19.5887
Epoch 10/10
10/10 - 1s - loss: 30.7272 - loglik: -5.2696e+01 - logprior: 22.0562
Fitted a model with MAP estimate = -29.5280
Time for alignment: 28.4150
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 617.4034 - loglik: -1.2831e+02 - logprior: -4.8901e+02
Epoch 2/10
10/10 - 1s - loss: 242.3611 - loglik: -1.0709e+02 - logprior: -1.3523e+02
Epoch 3/10
10/10 - 1s - loss: 149.5677 - loglik: -8.7004e+01 - logprior: -6.2530e+01
Epoch 4/10
10/10 - 1s - loss: 108.6748 - loglik: -7.3593e+01 - logprior: -3.5006e+01
Epoch 5/10
10/10 - 0s - loss: 89.9301 - loglik: -6.9458e+01 - logprior: -2.0353e+01
Epoch 6/10
10/10 - 0s - loss: 80.0363 - loglik: -6.9090e+01 - logprior: -1.0747e+01
Epoch 7/10
10/10 - 1s - loss: 74.2621 - loglik: -6.9605e+01 - logprior: -4.4457e+00
Epoch 8/10
10/10 - 1s - loss: 70.7374 - loglik: -6.9936e+01 - logprior: -6.1078e-01
Epoch 9/10
10/10 - 1s - loss: 68.4017 - loglik: -7.0099e+01 - logprior: 1.8982
Epoch 10/10
10/10 - 1s - loss: 66.7352 - loglik: -7.0277e+01 - logprior: 3.7578
Fitted a model with MAP estimate = -65.7706
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 714.7107 - loglik: -6.2242e+01 - logprior: -6.5237e+02
Epoch 2/2
10/10 - 1s - loss: 258.4130 - loglik: -5.4645e+01 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -171.8103
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.6816 - loglik: -4.9870e+01 - logprior: -4.5574e+02
Epoch 2/2
10/10 - 1s - loss: 171.2998 - loglik: -4.9716e+01 - logprior: -1.2149e+02
Fitted a model with MAP estimate = -121.2891
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 477.0364 - loglik: -4.8891e+01 - logprior: -4.2810e+02
Epoch 2/10
10/10 - 1s - loss: 163.7236 - loglik: -4.9537e+01 - logprior: -1.1407e+02
Epoch 3/10
10/10 - 0s - loss: 97.8373 - loglik: -5.0240e+01 - logprior: -4.7483e+01
Epoch 4/10
10/10 - 0s - loss: 69.8259 - loglik: -5.0835e+01 - logprior: -1.8907e+01
Epoch 5/10
10/10 - 1s - loss: 54.3718 - loglik: -5.1314e+01 - logprior: -2.9912e+00
Epoch 6/10
10/10 - 0s - loss: 45.3386 - loglik: -5.1701e+01 - logprior: 6.4463
Epoch 7/10
10/10 - 0s - loss: 39.7156 - loglik: -5.2031e+01 - logprior: 12.4040
Epoch 8/10
10/10 - 0s - loss: 35.8869 - loglik: -5.2313e+01 - logprior: 16.5106
Epoch 9/10
10/10 - 1s - loss: 33.0350 - loglik: -5.2528e+01 - logprior: 19.5802
Epoch 10/10
10/10 - 1s - loss: 30.7469 - loglik: -5.2705e+01 - logprior: 22.0471
Fitted a model with MAP estimate = -29.5457
Time for alignment: 28.0947
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 617.4203 - loglik: -1.2829e+02 - logprior: -4.8901e+02
Epoch 2/10
10/10 - 0s - loss: 242.4372 - loglik: -1.0712e+02 - logprior: -1.3526e+02
Epoch 3/10
10/10 - 0s - loss: 149.6280 - loglik: -8.7044e+01 - logprior: -6.2533e+01
Epoch 4/10
10/10 - 1s - loss: 108.6862 - loglik: -7.3602e+01 - logprior: -3.4993e+01
Epoch 5/10
10/10 - 0s - loss: 90.0642 - loglik: -6.9651e+01 - logprior: -2.0307e+01
Epoch 6/10
10/10 - 0s - loss: 80.0751 - loglik: -6.9229e+01 - logprior: -1.0677e+01
Epoch 7/10
10/10 - 0s - loss: 74.2895 - loglik: -6.9666e+01 - logprior: -4.4242e+00
Epoch 8/10
10/10 - 1s - loss: 70.7507 - loglik: -6.9959e+01 - logprior: -6.0647e-01
Epoch 9/10
10/10 - 0s - loss: 68.4043 - loglik: -7.0128e+01 - logprior: 1.9056
Epoch 10/10
10/10 - 0s - loss: 66.7329 - loglik: -7.0303e+01 - logprior: 3.7618
Fitted a model with MAP estimate = -65.7856
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 714.5947 - loglik: -6.2082e+01 - logprior: -6.5238e+02
Epoch 2/2
10/10 - 0s - loss: 258.3909 - loglik: -5.4624e+01 - logprior: -2.0370e+02
Fitted a model with MAP estimate = -171.8320
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 506.2177 - loglik: -5.0333e+01 - logprior: -4.5577e+02
Epoch 2/2
10/10 - 1s - loss: 171.3737 - loglik: -4.9891e+01 - logprior: -1.2147e+02
Fitted a model with MAP estimate = -121.4242
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 477.3505 - loglik: -4.9148e+01 - logprior: -4.2810e+02
Epoch 2/10
10/10 - 1s - loss: 163.7392 - loglik: -4.9587e+01 - logprior: -1.1408e+02
Epoch 3/10
10/10 - 1s - loss: 97.8788 - loglik: -5.0329e+01 - logprior: -4.7477e+01
Epoch 4/10
10/10 - 0s - loss: 69.8800 - loglik: -5.0909e+01 - logprior: -1.8906e+01
Epoch 5/10
10/10 - 0s - loss: 54.4127 - loglik: -5.1343e+01 - logprior: -3.0050e+00
Epoch 6/10
10/10 - 1s - loss: 45.3748 - loglik: -5.1725e+01 - logprior: 6.4325
Epoch 7/10
10/10 - 1s - loss: 39.7451 - loglik: -5.2048e+01 - logprior: 12.3972
Epoch 8/10
10/10 - 1s - loss: 35.9117 - loglik: -5.2321e+01 - logprior: 16.5002
Epoch 9/10
10/10 - 1s - loss: 33.0580 - loglik: -5.2542e+01 - logprior: 19.5731
Epoch 10/10
10/10 - 1s - loss: 30.7681 - loglik: -5.2722e+01 - logprior: 22.0438
Fitted a model with MAP estimate = -29.5633
Time for alignment: 29.0850
Computed alignments with likelihoods: ['-29.5280', '-29.5457', '-29.5633']
Best model has likelihood: -29.5280
time for generating output: 0.1086
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.8078703703703703
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faac97807f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf7ba0400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faab889ef10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac1131e5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faac955bd00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac5dcbabe0>, <__main__.SimpleDirichletPrior object at 0x7fab161b1820>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.0872 - loglik: -4.7278e+02 - logprior: -4.2004e+00
Epoch 2/10
16/16 - 5s - loss: 438.7414 - loglik: -4.3731e+02 - logprior: -1.0425e+00
Epoch 3/10
16/16 - 5s - loss: 411.1270 - loglik: -4.0896e+02 - logprior: -1.3950e+00
Epoch 4/10
16/16 - 5s - loss: 402.3456 - loglik: -3.9982e+02 - logprior: -1.4503e+00
Epoch 5/10
16/16 - 5s - loss: 399.5012 - loglik: -3.9700e+02 - logprior: -1.3942e+00
Epoch 6/10
16/16 - 5s - loss: 397.5939 - loglik: -3.9512e+02 - logprior: -1.4263e+00
Epoch 7/10
16/16 - 5s - loss: 397.8688 - loglik: -3.9554e+02 - logprior: -1.4027e+00
Fitted a model with MAP estimate = -395.5464
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 3), (42, 1), (48, 3), (50, 2), (56, 1), (58, 1), (72, 1), (73, 2), (74, 3), (95, 3), (96, 1), (105, 1), (116, 3), (122, 1), (125, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 417.1482 - loglik: -4.1292e+02 - logprior: -4.0308e+00
Epoch 2/2
33/33 - 8s - loss: 395.0554 - loglik: -3.9314e+02 - logprior: -1.2266e+00
Fitted a model with MAP estimate = -389.0217
expansions: [(35, 1), (151, 1), (159, 1), (172, 2)]
discards: [  0  32  33  60  88 143 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 14s - loss: 406.4720 - loglik: -4.0252e+02 - logprior: -3.6767e+00
Epoch 2/2
33/33 - 8s - loss: 394.3082 - loglik: -3.9210e+02 - logprior: -1.0207e+00
Fitted a model with MAP estimate = -388.9301
expansions: [(0, 1), (30, 2), (111, 2), (168, 2)]
discards: [ 89 166 167]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 402.4510 - loglik: -3.9996e+02 - logprior: -2.3894e+00
Epoch 2/10
33/33 - 8s - loss: 392.1964 - loglik: -3.9121e+02 - logprior: -6.0315e-01
Epoch 3/10
33/33 - 8s - loss: 387.5178 - loglik: -3.8631e+02 - logprior: -4.6758e-01
Epoch 4/10
33/33 - 9s - loss: 384.9665 - loglik: -3.8352e+02 - logprior: -4.4135e-01
Epoch 5/10
33/33 - 8s - loss: 383.9301 - loglik: -3.8238e+02 - logprior: -4.0581e-01
Epoch 6/10
33/33 - 8s - loss: 382.4110 - loglik: -3.8088e+02 - logprior: -3.8162e-01
Epoch 7/10
33/33 - 8s - loss: 381.1580 - loglik: -3.7972e+02 - logprior: -3.2740e-01
Epoch 8/10
33/33 - 9s - loss: 380.8343 - loglik: -3.7954e+02 - logprior: -2.8151e-01
Epoch 9/10
33/33 - 9s - loss: 379.7068 - loglik: -3.7858e+02 - logprior: -2.1628e-01
Epoch 10/10
33/33 - 9s - loss: 380.6646 - loglik: -3.7968e+02 - logprior: -1.5483e-01
Fitted a model with MAP estimate = -378.7980
Time for alignment: 205.2328
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 477.0968 - loglik: -4.7277e+02 - logprior: -4.1959e+00
Epoch 2/10
16/16 - 5s - loss: 437.6479 - loglik: -4.3620e+02 - logprior: -1.0411e+00
Epoch 3/10
16/16 - 5s - loss: 414.0061 - loglik: -4.1181e+02 - logprior: -1.3285e+00
Epoch 4/10
16/16 - 5s - loss: 404.7859 - loglik: -4.0227e+02 - logprior: -1.4141e+00
Epoch 5/10
16/16 - 5s - loss: 400.5019 - loglik: -3.9809e+02 - logprior: -1.3408e+00
Epoch 6/10
16/16 - 5s - loss: 398.7883 - loglik: -3.9643e+02 - logprior: -1.3794e+00
Epoch 7/10
16/16 - 5s - loss: 399.3908 - loglik: -3.9708e+02 - logprior: -1.3902e+00
Fitted a model with MAP estimate = -396.8998
expansions: [(13, 1), (14, 1), (23, 1), (24, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (50, 1), (56, 3), (69, 1), (72, 1), (74, 3), (94, 6), (102, 1), (113, 1), (116, 2), (119, 1), (122, 1), (125, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 411.9615 - loglik: -4.0778e+02 - logprior: -3.9190e+00
Epoch 2/2
33/33 - 9s - loss: 393.4665 - loglik: -3.9163e+02 - logprior: -1.2040e+00
Fitted a model with MAP estimate = -388.2275
expansions: [(162, 1), (175, 2)]
discards: [  0  32  33  69  94 115 145 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 405.6374 - loglik: -4.0184e+02 - logprior: -3.6494e+00
Epoch 2/2
33/33 - 8s - loss: 394.8850 - loglik: -3.9351e+02 - logprior: -9.2313e-01
Fitted a model with MAP estimate = -390.2449
expansions: [(0, 1), (31, 1), (33, 1), (110, 2), (168, 2)]
discards: [166 167]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 401.3746 - loglik: -3.9890e+02 - logprior: -2.3405e+00
Epoch 2/10
33/33 - 9s - loss: 392.1183 - loglik: -3.9117e+02 - logprior: -5.6940e-01
Epoch 3/10
33/33 - 8s - loss: 387.0563 - loglik: -3.8589e+02 - logprior: -4.2791e-01
Epoch 4/10
33/33 - 8s - loss: 385.2505 - loglik: -3.8384e+02 - logprior: -3.9799e-01
Epoch 5/10
33/33 - 8s - loss: 382.2370 - loglik: -3.8076e+02 - logprior: -3.5856e-01
Epoch 6/10
33/33 - 9s - loss: 382.3084 - loglik: -3.8084e+02 - logprior: -3.2287e-01
Fitted a model with MAP estimate = -379.8419
Time for alignment: 173.8940
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 476.7242 - loglik: -4.7241e+02 - logprior: -4.2219e+00
Epoch 2/10
16/16 - 5s - loss: 438.2775 - loglik: -4.3686e+02 - logprior: -1.0664e+00
Epoch 3/10
16/16 - 5s - loss: 412.9007 - loglik: -4.1073e+02 - logprior: -1.3794e+00
Epoch 4/10
16/16 - 5s - loss: 402.8245 - loglik: -4.0029e+02 - logprior: -1.4874e+00
Epoch 5/10
16/16 - 5s - loss: 400.6439 - loglik: -3.9824e+02 - logprior: -1.3851e+00
Epoch 6/10
16/16 - 5s - loss: 400.8182 - loglik: -3.9845e+02 - logprior: -1.4149e+00
Fitted a model with MAP estimate = -397.8040
expansions: [(13, 1), (14, 1), (22, 1), (23, 1), (28, 3), (29, 1), (42, 3), (48, 2), (49, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (74, 2), (75, 2), (93, 1), (94, 4), (95, 2), (105, 1), (116, 3), (122, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 414.5048 - loglik: -4.1033e+02 - logprior: -3.9457e+00
Epoch 2/2
33/33 - 9s - loss: 393.1422 - loglik: -3.9156e+02 - logprior: -1.1805e+00
Fitted a model with MAP estimate = -389.0602
expansions: [(157, 1), (179, 2)]
discards: [  0  32  33  51  73 119 120 149 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 403.2745 - loglik: -3.9925e+02 - logprior: -3.6686e+00
Epoch 2/2
33/33 - 8s - loss: 393.1659 - loglik: -3.9138e+02 - logprior: -9.3812e-01
Fitted a model with MAP estimate = -388.7609
expansions: [(0, 1), (114, 1), (171, 2)]
discards: [ 85 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 11s - loss: 405.9471 - loglik: -4.0347e+02 - logprior: -2.4065e+00
Epoch 2/10
33/33 - 8s - loss: 393.2721 - loglik: -3.9210e+02 - logprior: -6.3514e-01
Epoch 3/10
33/33 - 8s - loss: 387.8107 - loglik: -3.8601e+02 - logprior: -5.3300e-01
Epoch 4/10
33/33 - 8s - loss: 384.9319 - loglik: -3.8312e+02 - logprior: -5.0850e-01
Epoch 5/10
33/33 - 8s - loss: 384.1879 - loglik: -3.8253e+02 - logprior: -4.4839e-01
Epoch 6/10
33/33 - 8s - loss: 382.6779 - loglik: -3.8117e+02 - logprior: -4.1448e-01
Epoch 7/10
33/33 - 8s - loss: 381.9337 - loglik: -3.8060e+02 - logprior: -3.4451e-01
Epoch 8/10
33/33 - 8s - loss: 380.7846 - loglik: -3.7957e+02 - logprior: -2.8946e-01
Epoch 9/10
33/33 - 8s - loss: 381.4037 - loglik: -3.8034e+02 - logprior: -2.2448e-01
Fitted a model with MAP estimate = -379.5124
Time for alignment: 192.3323
Computed alignments with likelihoods: ['-378.7980', '-379.8419', '-379.5124']
Best model has likelihood: -378.7980
time for generating output: 0.2231
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7928730512249443
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabbc66c670>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa81094940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabf7a45f10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf7a45ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf7a45b20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac22443370>, <__main__.SimpleDirichletPrior object at 0x7fab16326f70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 291.9494 - loglik: -2.7694e+02 - logprior: -1.4945e+01
Epoch 2/10
10/10 - 2s - loss: 253.8357 - loglik: -2.4991e+02 - logprior: -3.9074e+00
Epoch 3/10
10/10 - 2s - loss: 228.2255 - loglik: -2.2591e+02 - logprior: -2.2839e+00
Epoch 4/10
10/10 - 2s - loss: 212.6014 - loglik: -2.1039e+02 - logprior: -2.0404e+00
Epoch 5/10
10/10 - 2s - loss: 204.9267 - loglik: -2.0253e+02 - logprior: -2.0357e+00
Epoch 6/10
10/10 - 2s - loss: 203.0812 - loglik: -2.0062e+02 - logprior: -2.1294e+00
Epoch 7/10
10/10 - 2s - loss: 201.1490 - loglik: -1.9889e+02 - logprior: -2.0437e+00
Epoch 8/10
10/10 - 2s - loss: 201.7865 - loglik: -1.9967e+02 - logprior: -1.9598e+00
Fitted a model with MAP estimate = -200.4433
expansions: [(21, 1), (24, 1), (26, 1), (39, 1), (45, 2), (46, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 224.3015 - loglik: -2.0682e+02 - logprior: -1.7459e+01
Epoch 2/2
10/10 - 2s - loss: 207.2147 - loglik: -1.9953e+02 - logprior: -7.5716e+00
Fitted a model with MAP estimate = -204.3876
expansions: []
discards: [ 0 27]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 217.9135 - loglik: -2.0085e+02 - logprior: -1.7051e+01
Epoch 2/2
10/10 - 2s - loss: 205.7323 - loglik: -1.9950e+02 - logprior: -6.1781e+00
Fitted a model with MAP estimate = -201.5474
expansions: [(0, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 213.5996 - loglik: -1.9922e+02 - logprior: -1.4362e+01
Epoch 2/10
10/10 - 2s - loss: 197.0194 - loglik: -1.9250e+02 - logprior: -4.4337e+00
Epoch 3/10
10/10 - 2s - loss: 192.1678 - loglik: -1.8949e+02 - logprior: -2.5145e+00
Epoch 4/10
10/10 - 2s - loss: 190.3287 - loglik: -1.8819e+02 - logprior: -1.9359e+00
Epoch 5/10
10/10 - 2s - loss: 189.1898 - loglik: -1.8719e+02 - logprior: -1.7635e+00
Epoch 6/10
10/10 - 2s - loss: 188.2159 - loglik: -1.8629e+02 - logprior: -1.6827e+00
Epoch 7/10
10/10 - 2s - loss: 188.3914 - loglik: -1.8661e+02 - logprior: -1.5590e+00
Fitted a model with MAP estimate = -187.2344
Time for alignment: 54.5361
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 291.6751 - loglik: -2.7670e+02 - logprior: -1.4945e+01
Epoch 2/10
10/10 - 2s - loss: 254.3190 - loglik: -2.5042e+02 - logprior: -3.8941e+00
Epoch 3/10
10/10 - 2s - loss: 227.4973 - loglik: -2.2535e+02 - logprior: -2.1245e+00
Epoch 4/10
10/10 - 2s - loss: 213.9407 - loglik: -2.1205e+02 - logprior: -1.7416e+00
Epoch 5/10
10/10 - 2s - loss: 208.1220 - loglik: -2.0605e+02 - logprior: -1.7855e+00
Epoch 6/10
10/10 - 2s - loss: 204.6757 - loglik: -2.0255e+02 - logprior: -1.8863e+00
Epoch 7/10
10/10 - 2s - loss: 203.7838 - loglik: -2.0178e+02 - logprior: -1.8131e+00
Epoch 8/10
10/10 - 2s - loss: 202.8934 - loglik: -2.0101e+02 - logprior: -1.7140e+00
Epoch 9/10
10/10 - 2s - loss: 201.8471 - loglik: -1.9998e+02 - logprior: -1.7281e+00
Epoch 10/10
10/10 - 2s - loss: 202.2955 - loglik: -2.0041e+02 - logprior: -1.7547e+00
Fitted a model with MAP estimate = -201.6588
expansions: [(23, 2), (25, 1), (27, 2), (36, 1), (47, 1), (48, 1), (56, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 225.0405 - loglik: -2.0769e+02 - logprior: -1.7328e+01
Epoch 2/2
10/10 - 2s - loss: 205.7240 - loglik: -1.9822e+02 - logprior: -7.4615e+00
Fitted a model with MAP estimate = -202.3961
expansions: [(0, 13)]
discards: [ 0 22 23 29]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 213.7401 - loglik: -1.9891e+02 - logprior: -1.4583e+01
Epoch 2/2
10/10 - 2s - loss: 198.9921 - loglik: -1.9429e+02 - logprior: -4.5417e+00
Fitted a model with MAP estimate = -192.8226
expansions: [(21, 1), (34, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 213.4440 - loglik: -1.9696e+02 - logprior: -1.6425e+01
Epoch 2/10
10/10 - 2s - loss: 198.5981 - loglik: -1.9337e+02 - logprior: -5.1129e+00
Epoch 3/10
10/10 - 1s - loss: 195.8413 - loglik: -1.9321e+02 - logprior: -2.4580e+00
Epoch 4/10
10/10 - 2s - loss: 193.3874 - loglik: -1.9158e+02 - logprior: -1.6545e+00
Epoch 5/10
10/10 - 2s - loss: 193.4530 - loglik: -1.9198e+02 - logprior: -1.3414e+00
Fitted a model with MAP estimate = -191.9818
Time for alignment: 50.5236
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.6592 - loglik: -2.7662e+02 - logprior: -1.4946e+01
Epoch 2/10
10/10 - 2s - loss: 255.0486 - loglik: -2.5110e+02 - logprior: -3.9121e+00
Epoch 3/10
10/10 - 2s - loss: 225.3466 - loglik: -2.2301e+02 - logprior: -2.3004e+00
Epoch 4/10
10/10 - 2s - loss: 210.1965 - loglik: -2.0795e+02 - logprior: -2.0675e+00
Epoch 5/10
10/10 - 2s - loss: 204.5821 - loglik: -2.0218e+02 - logprior: -2.0959e+00
Epoch 6/10
10/10 - 2s - loss: 201.8219 - loglik: -1.9936e+02 - logprior: -2.1851e+00
Epoch 7/10
10/10 - 2s - loss: 199.8817 - loglik: -1.9764e+02 - logprior: -2.0216e+00
Epoch 8/10
10/10 - 1s - loss: 201.0849 - loglik: -1.9900e+02 - logprior: -1.9219e+00
Fitted a model with MAP estimate = -199.7626
expansions: [(20, 1), (21, 1), (24, 1), (36, 1), (40, 1), (46, 1), (47, 1), (48, 2), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 225.4969 - loglik: -2.0803e+02 - logprior: -1.7412e+01
Epoch 2/2
10/10 - 2s - loss: 205.0112 - loglik: -1.9733e+02 - logprior: -7.5329e+00
Fitted a model with MAP estimate = -201.6830
expansions: [(0, 7)]
discards: [ 0 20 38 55]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.5878 - loglik: -2.0021e+02 - logprior: -1.4283e+01
Epoch 2/2
10/10 - 2s - loss: 198.0844 - loglik: -1.9368e+02 - logprior: -4.3341e+00
Fitted a model with MAP estimate = -194.9388
expansions: [(40, 1)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 215.1401 - loglik: -1.9869e+02 - logprior: -1.6391e+01
Epoch 2/10
10/10 - 2s - loss: 199.4634 - loglik: -1.9428e+02 - logprior: -5.1059e+00
Epoch 3/10
10/10 - 2s - loss: 196.5688 - loglik: -1.9397e+02 - logprior: -2.4679e+00
Epoch 4/10
10/10 - 2s - loss: 193.8157 - loglik: -1.9187e+02 - logprior: -1.7243e+00
Epoch 5/10
10/10 - 2s - loss: 191.7438 - loglik: -1.9010e+02 - logprior: -1.3658e+00
Epoch 6/10
10/10 - 2s - loss: 192.2734 - loglik: -1.9092e+02 - logprior: -1.0991e+00
Fitted a model with MAP estimate = -190.9012
Time for alignment: 49.5216
Computed alignments with likelihoods: ['-187.2344', '-191.9818', '-190.9012']
Best model has likelihood: -187.2344
time for generating output: 0.2216
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.5588124410933082
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabcd2f0a90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc49d44f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa820ef790>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3b9de160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6f7e6a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faae2cdaa00>, <__main__.SimpleDirichletPrior object at 0x7faa8cc0ce20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 613.9714 - loglik: -5.6099e+02 - logprior: -5.2960e+01
Epoch 2/10
10/10 - 3s - loss: 516.6027 - loglik: -5.0761e+02 - logprior: -8.9486e+00
Epoch 3/10
10/10 - 3s - loss: 452.8282 - loglik: -4.5073e+02 - logprior: -1.9457e+00
Epoch 4/10
10/10 - 3s - loss: 421.8309 - loglik: -4.2132e+02 - logprior: -1.3839e-01
Epoch 5/10
10/10 - 3s - loss: 408.0095 - loglik: -4.0839e+02 - logprior: 0.8599
Epoch 6/10
10/10 - 3s - loss: 405.2109 - loglik: -4.0622e+02 - logprior: 1.5127
Epoch 7/10
10/10 - 3s - loss: 399.2415 - loglik: -4.0038e+02 - logprior: 1.6686
Epoch 8/10
10/10 - 3s - loss: 398.5036 - loglik: -3.9991e+02 - logprior: 1.9387
Epoch 9/10
10/10 - 3s - loss: 398.1850 - loglik: -3.9980e+02 - logprior: 2.1485
Epoch 10/10
10/10 - 3s - loss: 398.4767 - loglik: -4.0025e+02 - logprior: 2.3102
Fitted a model with MAP estimate = -396.5902
expansions: [(7, 3), (9, 1), (19, 1), (22, 1), (23, 1), (25, 1), (29, 3), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (52, 1), (74, 1), (80, 1), (82, 1), (104, 1), (107, 3), (113, 2), (117, 2), (119, 2), (145, 4), (155, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 484.5529 - loglik: -4.2384e+02 - logprior: -6.0671e+01
Epoch 2/2
10/10 - 3s - loss: 415.6550 - loglik: -3.9409e+02 - logprior: -2.1432e+01
Fitted a model with MAP estimate = -403.2246
expansions: [(0, 13)]
discards: [  0  11 128 129 130 137 147 176 177 178 191]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 449.5734 - loglik: -4.0225e+02 - logprior: -4.7251e+01
Epoch 2/2
10/10 - 3s - loss: 398.6111 - loglik: -3.9028e+02 - logprior: -8.1261e+00
Fitted a model with MAP estimate = -389.0994
expansions: [(23, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 42 43]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 452.9097 - loglik: -4.0652e+02 - logprior: -4.6365e+01
Epoch 2/10
10/10 - 3s - loss: 408.1479 - loglik: -4.0088e+02 - logprior: -7.2507e+00
Epoch 3/10
10/10 - 3s - loss: 394.7300 - loglik: -3.9565e+02 - logprior: 1.0462
Epoch 4/10
10/10 - 3s - loss: 387.7104 - loglik: -3.9185e+02 - logprior: 4.4833
Epoch 5/10
10/10 - 3s - loss: 382.0989 - loglik: -3.8792e+02 - logprior: 6.3285
Epoch 6/10
10/10 - 3s - loss: 378.1589 - loglik: -3.8493e+02 - logprior: 7.3380
Epoch 7/10
10/10 - 3s - loss: 376.7518 - loglik: -3.8417e+02 - logprior: 8.0256
Epoch 8/10
10/10 - 3s - loss: 375.0813 - loglik: -3.8308e+02 - logprior: 8.6272
Epoch 9/10
10/10 - 3s - loss: 374.2509 - loglik: -3.8281e+02 - logprior: 9.2026
Epoch 10/10
10/10 - 3s - loss: 371.9564 - loglik: -3.8104e+02 - logprior: 9.7299
Fitted a model with MAP estimate = -371.7619
Time for alignment: 93.6530
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 613.4076 - loglik: -5.6034e+02 - logprior: -5.2962e+01
Epoch 2/10
10/10 - 3s - loss: 517.6954 - loglik: -5.0860e+02 - logprior: -9.0164e+00
Epoch 3/10
10/10 - 3s - loss: 455.0125 - loglik: -4.5284e+02 - logprior: -1.9971e+00
Epoch 4/10
10/10 - 3s - loss: 417.2782 - loglik: -4.1700e+02 - logprior: 0.0608
Epoch 5/10
10/10 - 3s - loss: 406.3181 - loglik: -4.0676e+02 - logprior: 0.8813
Epoch 6/10
10/10 - 3s - loss: 398.9676 - loglik: -4.0002e+02 - logprior: 1.4749
Epoch 7/10
10/10 - 3s - loss: 397.7610 - loglik: -3.9923e+02 - logprior: 1.8732
Epoch 8/10
10/10 - 3s - loss: 394.9986 - loglik: -3.9668e+02 - logprior: 2.1492
Epoch 9/10
10/10 - 3s - loss: 395.4208 - loglik: -3.9728e+02 - logprior: 2.3428
Fitted a model with MAP estimate = -393.7620
expansions: [(9, 2), (19, 1), (22, 4), (23, 1), (30, 2), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (81, 2), (94, 1), (102, 2), (105, 3), (106, 1), (116, 4), (149, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 479.5058 - loglik: -4.1825e+02 - logprior: -6.1083e+01
Epoch 2/2
10/10 - 3s - loss: 413.3137 - loglik: -3.9127e+02 - logprior: -2.1809e+01
Fitted a model with MAP estimate = -400.6775
expansions: [(0, 10)]
discards: [  0  26  65  97 121]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 443.8072 - loglik: -3.9640e+02 - logprior: -4.7397e+01
Epoch 2/2
10/10 - 3s - loss: 391.0733 - loglik: -3.8299e+02 - logprior: -7.9872e+00
Fitted a model with MAP estimate = -383.2583
expansions: [(18, 1), (179, 3)]
discards: [  1   2   3   4   5   6   7   8   9  45  63 135 147]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 436.9122 - loglik: -3.9107e+02 - logprior: -4.5754e+01
Epoch 2/10
10/10 - 3s - loss: 388.5784 - loglik: -3.8152e+02 - logprior: -6.8949e+00
Epoch 3/10
10/10 - 3s - loss: 376.8510 - loglik: -3.7770e+02 - logprior: 1.1464
Epoch 4/10
10/10 - 3s - loss: 369.2553 - loglik: -3.7314e+02 - logprior: 4.3768
Epoch 5/10
10/10 - 3s - loss: 366.6337 - loglik: -3.7220e+02 - logprior: 6.1805
Epoch 6/10
10/10 - 3s - loss: 359.6031 - loglik: -3.6623e+02 - logprior: 7.2928
Epoch 7/10
10/10 - 3s - loss: 360.6324 - loglik: -3.6801e+02 - logprior: 8.0450
Fitted a model with MAP estimate = -358.2510
Time for alignment: 83.2750
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 613.6204 - loglik: -5.6060e+02 - logprior: -5.2975e+01
Epoch 2/10
10/10 - 3s - loss: 517.9324 - loglik: -5.0886e+02 - logprior: -8.9961e+00
Epoch 3/10
10/10 - 3s - loss: 456.9500 - loglik: -4.5472e+02 - logprior: -2.0345e+00
Epoch 4/10
10/10 - 3s - loss: 418.1066 - loglik: -4.1732e+02 - logprior: -4.4637e-01
Epoch 5/10
10/10 - 3s - loss: 404.9872 - loglik: -4.0456e+02 - logprior: 1.7296e-04
Epoch 6/10
10/10 - 3s - loss: 399.7435 - loglik: -4.0000e+02 - logprior: 0.6824
Epoch 7/10
10/10 - 3s - loss: 396.2252 - loglik: -3.9695e+02 - logprior: 1.1375
Epoch 8/10
10/10 - 3s - loss: 394.5132 - loglik: -3.9541e+02 - logprior: 1.3400
Epoch 9/10
10/10 - 3s - loss: 395.6470 - loglik: -3.9675e+02 - logprior: 1.5465
Fitted a model with MAP estimate = -393.6845
expansions: [(9, 1), (10, 1), (16, 1), (19, 1), (20, 1), (22, 2), (23, 1), (32, 1), (45, 3), (46, 1), (52, 2), (81, 2), (94, 1), (98, 1), (102, 2), (103, 1), (104, 2), (106, 1), (113, 2), (116, 2), (144, 4), (150, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 477.1065 - loglik: -4.1582e+02 - logprior: -6.1159e+01
Epoch 2/2
10/10 - 3s - loss: 412.7095 - loglik: -3.9103e+02 - logprior: -2.1474e+01
Fitted a model with MAP estimate = -398.5885
expansions: [(0, 10)]
discards: [  0  27  64 120 129 137]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 442.2297 - loglik: -3.9507e+02 - logprior: -4.7077e+01
Epoch 2/2
10/10 - 3s - loss: 392.0419 - loglik: -3.8416e+02 - logprior: -7.6973e+00
Fitted a model with MAP estimate = -383.1097
expansions: [(176, 1)]
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 435.0771 - loglik: -3.8929e+02 - logprior: -4.5678e+01
Epoch 2/10
10/10 - 3s - loss: 390.7446 - loglik: -3.8403e+02 - logprior: -6.6158e+00
Epoch 3/10
10/10 - 3s - loss: 378.1129 - loglik: -3.7930e+02 - logprior: 1.4210
Epoch 4/10
10/10 - 3s - loss: 369.9797 - loglik: -3.7424e+02 - logprior: 4.6815
Epoch 5/10
10/10 - 3s - loss: 364.9042 - loglik: -3.7092e+02 - logprior: 6.5599
Epoch 6/10
10/10 - 3s - loss: 364.0678 - loglik: -3.7121e+02 - logprior: 7.7117
Epoch 7/10
10/10 - 3s - loss: 360.9167 - loglik: -3.6890e+02 - logprior: 8.5398
Epoch 8/10
10/10 - 3s - loss: 360.2523 - loglik: -3.6882e+02 - logprior: 9.1256
Epoch 9/10
10/10 - 3s - loss: 358.7350 - loglik: -3.6782e+02 - logprior: 9.6424
Epoch 10/10
10/10 - 3s - loss: 357.8408 - loglik: -3.6738e+02 - logprior: 10.0966
Fitted a model with MAP estimate = -357.1420
Time for alignment: 89.6818
Computed alignments with likelihoods: ['-371.7619', '-358.2510', '-357.1420']
Best model has likelihood: -357.1420
time for generating output: 0.2681
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7601641907740422
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fabde3fd3d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faac96fc3d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaeb4fb970>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac08af0670>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4878610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa80d2aac0>, <__main__.SimpleDirichletPrior object at 0x7faad1e61c10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.5960 - loglik: -2.3542e+02 - logprior: -3.1278e+00
Epoch 2/10
19/19 - 2s - loss: 208.0420 - loglik: -2.0654e+02 - logprior: -1.1989e+00
Epoch 3/10
19/19 - 2s - loss: 195.8485 - loglik: -1.9393e+02 - logprior: -1.3689e+00
Epoch 4/10
19/19 - 2s - loss: 193.4602 - loglik: -1.9175e+02 - logprior: -1.2795e+00
Epoch 5/10
19/19 - 2s - loss: 192.7077 - loglik: -1.9108e+02 - logprior: -1.2443e+00
Epoch 6/10
19/19 - 2s - loss: 192.5232 - loglik: -1.9099e+02 - logprior: -1.2101e+00
Epoch 7/10
19/19 - 2s - loss: 192.0778 - loglik: -1.9057e+02 - logprior: -1.2050e+00
Epoch 8/10
19/19 - 2s - loss: 191.8179 - loglik: -1.9034e+02 - logprior: -1.1910e+00
Epoch 9/10
19/19 - 2s - loss: 192.0479 - loglik: -1.9057e+02 - logprior: -1.1906e+00
Fitted a model with MAP estimate = -185.4872
expansions: [(0, 2), (3, 1), (6, 1), (18, 5), (23, 1), (42, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 197.2327 - loglik: -1.9291e+02 - logprior: -4.2359e+00
Epoch 2/2
19/19 - 2s - loss: 187.5368 - loglik: -1.8581e+02 - logprior: -1.4281e+00
Fitted a model with MAP estimate = -178.6597
expansions: [(7, 1)]
discards: [ 0  1 58]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 189.8792 - loglik: -1.8682e+02 - logprior: -2.8617e+00
Epoch 2/2
19/19 - 2s - loss: 186.0213 - loglik: -1.8465e+02 - logprior: -1.0343e+00
Fitted a model with MAP estimate = -178.2167
expansions: [(0, 2)]
discards: [8 9]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 182.9915 - loglik: -1.8063e+02 - logprior: -2.3075e+00
Epoch 2/10
23/23 - 3s - loss: 179.1782 - loglik: -1.7802e+02 - logprior: -1.0253e+00
Epoch 3/10
23/23 - 3s - loss: 177.2401 - loglik: -1.7590e+02 - logprior: -9.5196e-01
Epoch 4/10
23/23 - 3s - loss: 177.2625 - loglik: -1.7588e+02 - logprior: -9.0868e-01
Fitted a model with MAP estimate = -176.0220
Time for alignment: 73.2951
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.2553 - loglik: -2.3508e+02 - logprior: -3.1259e+00
Epoch 2/10
19/19 - 2s - loss: 206.7239 - loglik: -2.0525e+02 - logprior: -1.1996e+00
Epoch 3/10
19/19 - 2s - loss: 195.4344 - loglik: -1.9363e+02 - logprior: -1.3482e+00
Epoch 4/10
19/19 - 2s - loss: 193.7962 - loglik: -1.9217e+02 - logprior: -1.2490e+00
Epoch 5/10
19/19 - 2s - loss: 192.6738 - loglik: -1.9111e+02 - logprior: -1.2199e+00
Epoch 6/10
19/19 - 2s - loss: 192.4349 - loglik: -1.9095e+02 - logprior: -1.1943e+00
Epoch 7/10
19/19 - 2s - loss: 191.9955 - loglik: -1.9054e+02 - logprior: -1.1863e+00
Epoch 8/10
19/19 - 2s - loss: 191.8228 - loglik: -1.9038e+02 - logprior: -1.1844e+00
Epoch 9/10
19/19 - 2s - loss: 191.7671 - loglik: -1.9035e+02 - logprior: -1.1737e+00
Epoch 10/10
19/19 - 2s - loss: 191.6900 - loglik: -1.9027e+02 - logprior: -1.1844e+00
Fitted a model with MAP estimate = -185.6630
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (22, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 194.9997 - loglik: -1.9209e+02 - logprior: -2.8261e+00
Epoch 2/2
19/19 - 2s - loss: 186.5826 - loglik: -1.8515e+02 - logprior: -1.1500e+00
Fitted a model with MAP estimate = -178.3458
expansions: [(20, 1), (30, 1)]
discards: [ 0 23 24 25 26 61]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 193.5784 - loglik: -1.8976e+02 - logprior: -3.7808e+00
Epoch 2/2
19/19 - 2s - loss: 188.8251 - loglik: -1.8669e+02 - logprior: -1.9789e+00
Fitted a model with MAP estimate = -180.0480
expansions: [(0, 4), (23, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 180.7574 - loglik: -1.7868e+02 - logprior: -1.8907e+00
Epoch 2/10
23/23 - 3s - loss: 177.1622 - loglik: -1.7572e+02 - logprior: -1.0196e+00
Epoch 3/10
23/23 - 3s - loss: 175.8032 - loglik: -1.7422e+02 - logprior: -1.0030e+00
Epoch 4/10
23/23 - 3s - loss: 175.3423 - loglik: -1.7382e+02 - logprior: -9.6682e-01
Epoch 5/10
23/23 - 3s - loss: 174.4902 - loglik: -1.7302e+02 - logprior: -9.7100e-01
Epoch 6/10
23/23 - 3s - loss: 174.5823 - loglik: -1.7316e+02 - logprior: -9.4995e-01
Fitted a model with MAP estimate = -173.6926
Time for alignment: 81.2461
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.6800 - loglik: -2.3551e+02 - logprior: -3.1227e+00
Epoch 2/10
19/19 - 2s - loss: 206.7557 - loglik: -2.0527e+02 - logprior: -1.2024e+00
Epoch 3/10
19/19 - 2s - loss: 195.3462 - loglik: -1.9349e+02 - logprior: -1.3477e+00
Epoch 4/10
19/19 - 2s - loss: 193.1471 - loglik: -1.9151e+02 - logprior: -1.2620e+00
Epoch 5/10
19/19 - 2s - loss: 192.2703 - loglik: -1.9070e+02 - logprior: -1.2321e+00
Epoch 6/10
19/19 - 2s - loss: 191.8981 - loglik: -1.9039e+02 - logprior: -1.2098e+00
Epoch 7/10
19/19 - 2s - loss: 191.4645 - loglik: -1.8999e+02 - logprior: -1.2071e+00
Epoch 8/10
19/19 - 2s - loss: 191.6069 - loglik: -1.9017e+02 - logprior: -1.1929e+00
Fitted a model with MAP estimate = -184.6434
expansions: [(0, 2), (3, 1), (6, 1), (16, 1), (17, 3), (22, 2), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.6264 - loglik: -1.9228e+02 - logprior: -4.2162e+00
Epoch 2/2
19/19 - 2s - loss: 187.4841 - loglik: -1.8571e+02 - logprior: -1.4606e+00
Fitted a model with MAP estimate = -178.5016
expansions: [(7, 1), (26, 2)]
discards: [ 0 31 58]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 191.9287 - loglik: -1.8802e+02 - logprior: -3.8670e+00
Epoch 2/2
19/19 - 2s - loss: 186.7513 - loglik: -1.8510e+02 - logprior: -1.4944e+00
Fitted a model with MAP estimate = -178.4594
expansions: [(26, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 181.1659 - loglik: -1.7883e+02 - logprior: -2.1693e+00
Epoch 2/10
23/23 - 3s - loss: 177.8474 - loglik: -1.7651e+02 - logprior: -9.0060e-01
Epoch 3/10
23/23 - 3s - loss: 176.7340 - loglik: -1.7531e+02 - logprior: -8.3387e-01
Epoch 4/10
23/23 - 3s - loss: 176.3452 - loglik: -1.7500e+02 - logprior: -7.9437e-01
Epoch 5/10
23/23 - 3s - loss: 175.5506 - loglik: -1.7429e+02 - logprior: -7.7906e-01
Epoch 6/10
23/23 - 3s - loss: 175.5266 - loglik: -1.7433e+02 - logprior: -7.6312e-01
Epoch 7/10
23/23 - 3s - loss: 175.2098 - loglik: -1.7403e+02 - logprior: -7.4906e-01
Epoch 8/10
23/23 - 3s - loss: 175.0318 - loglik: -1.7385e+02 - logprior: -7.3271e-01
Epoch 9/10
23/23 - 3s - loss: 174.7027 - loglik: -1.7352e+02 - logprior: -7.2968e-01
Epoch 10/10
23/23 - 3s - loss: 174.5717 - loglik: -1.7342e+02 - logprior: -7.1672e-01
Fitted a model with MAP estimate = -173.8976
Time for alignment: 84.9034
Computed alignments with likelihoods: ['-176.0220', '-173.6926', '-173.8976']
Best model has likelihood: -173.6926
time for generating output: 0.1597
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8402457757296466
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa8bcc5e20>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2edbcd30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faad2284a00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa8cfb4df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac08a69430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac08af0070>, <__main__.SimpleDirichletPrior object at 0x7faaafcc86d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 797.1395 - loglik: -7.9093e+02 - logprior: -5.7983e+00
Epoch 2/10
22/22 - 14s - loss: 683.7297 - loglik: -6.8210e+02 - logprior: -7.7855e-01
Epoch 3/10
22/22 - 14s - loss: 644.7771 - loglik: -6.4159e+02 - logprior: -2.0831e+00
Epoch 4/10
22/22 - 14s - loss: 635.3637 - loglik: -6.3246e+02 - logprior: -1.9456e+00
Epoch 5/10
22/22 - 14s - loss: 636.0645 - loglik: -6.3331e+02 - logprior: -1.9262e+00
Fitted a model with MAP estimate = -632.7499
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (48, 1), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 1), (77, 1), (82, 1), (98, 1), (99, 1), (104, 1), (105, 2), (108, 1), (109, 1), (118, 1), (120, 2), (132, 1), (143, 1), (144, 2), (148, 1), (149, 1), (155, 3), (157, 1), (174, 1), (177, 3), (178, 1), (179, 1), (182, 2), (183, 1), (184, 2), (192, 2), (206, 1), (207, 1), (208, 1), (212, 1), (216, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 646.9402 - loglik: -6.3829e+02 - logprior: -8.4696e+00
Epoch 2/2
22/22 - 19s - loss: 618.2755 - loglik: -6.1529e+02 - logprior: -2.4893e+00
Fitted a model with MAP estimate = -612.8450
expansions: [(0, 3), (104, 1), (192, 1), (246, 1)]
discards: [  0 176 219 229 274]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 624.8504 - loglik: -6.1980e+02 - logprior: -4.9282e+00
Epoch 2/2
22/22 - 19s - loss: 608.8696 - loglik: -6.0943e+02 - logprior: 0.9614
Fitted a model with MAP estimate = -607.8378
expansions: []
discards: [  0   1   2 153]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 23s - loss: 633.1356 - loglik: -6.2565e+02 - logprior: -7.4382e+00
Epoch 2/10
22/22 - 19s - loss: 618.4968 - loglik: -6.1677e+02 - logprior: -1.4880e+00
Epoch 3/10
22/22 - 19s - loss: 609.6412 - loglik: -6.0899e+02 - logprior: 0.0265
Epoch 4/10
22/22 - 19s - loss: 606.9917 - loglik: -6.0834e+02 - logprior: 2.2972
Epoch 5/10
22/22 - 19s - loss: 603.7730 - loglik: -6.0520e+02 - logprior: 2.4591
Epoch 6/10
22/22 - 19s - loss: 603.5891 - loglik: -6.0517e+02 - logprior: 2.6134
Epoch 7/10
22/22 - 19s - loss: 602.6340 - loglik: -6.0454e+02 - logprior: 2.8932
Epoch 8/10
22/22 - 19s - loss: 601.0920 - loglik: -6.0320e+02 - logprior: 3.0593
Epoch 9/10
22/22 - 19s - loss: 600.6003 - loglik: -6.0297e+02 - logprior: 3.2969
Epoch 10/10
22/22 - 19s - loss: 599.7063 - loglik: -6.0233e+02 - logprior: 3.5102
Fitted a model with MAP estimate = -598.3376
Time for alignment: 403.6842
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 797.9695 - loglik: -7.9183e+02 - logprior: -5.8016e+00
Epoch 2/10
22/22 - 14s - loss: 683.0440 - loglik: -6.8154e+02 - logprior: -6.8654e-01
Epoch 3/10
22/22 - 14s - loss: 642.8187 - loglik: -6.3969e+02 - logprior: -2.0506e+00
Epoch 4/10
22/22 - 14s - loss: 637.2136 - loglik: -6.3429e+02 - logprior: -1.9765e+00
Epoch 5/10
22/22 - 14s - loss: 634.7787 - loglik: -6.3195e+02 - logprior: -1.9718e+00
Epoch 6/10
22/22 - 14s - loss: 634.4527 - loglik: -6.3168e+02 - logprior: -1.9563e+00
Epoch 7/10
22/22 - 14s - loss: 631.6066 - loglik: -6.2882e+02 - logprior: -1.9688e+00
Epoch 8/10
22/22 - 14s - loss: 629.5561 - loglik: -6.2677e+02 - logprior: -1.9935e+00
Epoch 9/10
22/22 - 14s - loss: 633.1519 - loglik: -6.3035e+02 - logprior: -2.0015e+00
Fitted a model with MAP estimate = -629.9599
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (47, 2), (48, 1), (49, 1), (69, 1), (70, 2), (71, 1), (76, 1), (79, 1), (81, 1), (83, 1), (97, 1), (98, 1), (103, 1), (104, 2), (107, 1), (108, 1), (117, 1), (119, 2), (142, 2), (143, 1), (148, 1), (149, 1), (155, 3), (175, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (192, 1), (193, 1), (206, 1), (207, 1), (208, 1), (210, 1), (213, 1), (215, 1), (222, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 651.5472 - loglik: -6.4286e+02 - logprior: -8.5858e+00
Epoch 2/2
22/22 - 19s - loss: 615.5115 - loglik: -6.1258e+02 - logprior: -2.5771e+00
Fitted a model with MAP estimate = -612.0130
expansions: [(0, 3), (275, 2)]
discards: [  0  34 175 220 230 282]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 622.4734 - loglik: -6.1742e+02 - logprior: -4.8244e+00
Epoch 2/2
22/22 - 19s - loss: 614.7727 - loglik: -6.1531e+02 - logprior: 1.1141
Fitted a model with MAP estimate = -607.1483
expansions: []
discards: [  0   1   2 152 195 274]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 630.4190 - loglik: -6.2272e+02 - logprior: -7.5461e+00
Epoch 2/10
22/22 - 18s - loss: 615.6960 - loglik: -6.1351e+02 - logprior: -1.7406e+00
Epoch 3/10
22/22 - 19s - loss: 609.1192 - loglik: -6.0818e+02 - logprior: -2.2672e-01
Epoch 4/10
22/22 - 19s - loss: 606.7307 - loglik: -6.0820e+02 - logprior: 2.3426
Epoch 5/10
22/22 - 19s - loss: 604.9795 - loglik: -6.0664e+02 - logprior: 2.6221
Epoch 6/10
22/22 - 19s - loss: 600.4875 - loglik: -6.0226e+02 - logprior: 2.7677
Epoch 7/10
22/22 - 19s - loss: 603.4078 - loglik: -6.0543e+02 - logprior: 3.0098
Fitted a model with MAP estimate = -600.2309
Time for alignment: 400.9855
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 797.6330 - loglik: -7.9165e+02 - logprior: -5.8006e+00
Epoch 2/10
22/22 - 14s - loss: 683.1547 - loglik: -6.8187e+02 - logprior: -7.0148e-01
Epoch 3/10
22/22 - 14s - loss: 644.0400 - loglik: -6.4121e+02 - logprior: -1.8943e+00
Epoch 4/10
22/22 - 14s - loss: 635.9670 - loglik: -6.3330e+02 - logprior: -1.7454e+00
Epoch 5/10
22/22 - 14s - loss: 634.1827 - loglik: -6.3157e+02 - logprior: -1.7800e+00
Epoch 6/10
22/22 - 14s - loss: 632.9501 - loglik: -6.3032e+02 - logprior: -1.8123e+00
Epoch 7/10
22/22 - 14s - loss: 630.6326 - loglik: -6.2799e+02 - logprior: -1.8185e+00
Epoch 8/10
22/22 - 14s - loss: 627.2090 - loglik: -6.2463e+02 - logprior: -1.7936e+00
Epoch 9/10
22/22 - 14s - loss: 631.9174 - loglik: -6.2939e+02 - logprior: -1.7643e+00
Fitted a model with MAP estimate = -628.5942
expansions: [(12, 1), (13, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 8), (48, 1), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (80, 1), (81, 1), (83, 1), (97, 1), (99, 1), (105, 2), (106, 1), (109, 1), (118, 1), (120, 2), (138, 1), (142, 2), (143, 1), (148, 1), (149, 1), (155, 3), (157, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 2), (205, 1), (206, 1), (209, 1), (214, 2), (215, 1), (216, 2), (222, 1), (224, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 653.8766 - loglik: -6.4491e+02 - logprior: -8.7184e+00
Epoch 2/2
22/22 - 20s - loss: 616.2841 - loglik: -6.1293e+02 - logprior: -2.7260e+00
Fitted a model with MAP estimate = -611.5341
expansions: [(0, 3), (252, 1)]
discards: [  0  34  55  56  57  58  59 180 225 226 235 281]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 628.9874 - loglik: -6.2381e+02 - logprior: -4.9426e+00
Epoch 2/2
22/22 - 19s - loss: 611.7026 - loglik: -6.1189e+02 - logprior: 0.9273
Fitted a model with MAP estimate = -606.9454
expansions: [(220, 1), (283, 1)]
discards: [  0   1   2 281]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 629.1880 - loglik: -6.2162e+02 - logprior: -7.4099e+00
Epoch 2/10
22/22 - 19s - loss: 615.4368 - loglik: -6.1352e+02 - logprior: -1.4897e+00
Epoch 3/10
22/22 - 19s - loss: 610.2964 - loglik: -6.0952e+02 - logprior: -1.1544e-01
Epoch 4/10
22/22 - 19s - loss: 605.6494 - loglik: -6.0717e+02 - logprior: 2.3580
Epoch 5/10
22/22 - 19s - loss: 603.8174 - loglik: -6.0546e+02 - logprior: 2.5929
Epoch 6/10
22/22 - 19s - loss: 601.7454 - loglik: -6.0356e+02 - logprior: 2.8132
Epoch 7/10
22/22 - 19s - loss: 602.4467 - loglik: -6.0445e+02 - logprior: 3.0235
Fitted a model with MAP estimate = -599.6114
Time for alignment: 406.4888
Computed alignments with likelihoods: ['-598.3376', '-600.2309', '-599.6114']
Best model has likelihood: -598.3376
time for generating output: 0.3705
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9076235710835614
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac2a9e7d90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa83342250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaafa23460>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabf7e264c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa8cfb4700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa95dc7550>, <__main__.SimpleDirichletPrior object at 0x7faba1993af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.3459 - loglik: -7.9860e+01 - logprior: -9.2419e+01
Epoch 2/10
10/10 - 1s - loss: 95.2890 - loglik: -6.8645e+01 - logprior: -2.6609e+01
Epoch 3/10
10/10 - 1s - loss: 73.2078 - loglik: -5.9859e+01 - logprior: -1.3337e+01
Epoch 4/10
10/10 - 1s - loss: 63.9006 - loglik: -5.5611e+01 - logprior: -8.2176e+00
Epoch 5/10
10/10 - 1s - loss: 58.9852 - loglik: -5.3183e+01 - logprior: -5.6062e+00
Epoch 6/10
10/10 - 1s - loss: 56.8738 - loglik: -5.2448e+01 - logprior: -4.1527e+00
Epoch 7/10
10/10 - 1s - loss: 55.7859 - loglik: -5.2238e+01 - logprior: -3.2984e+00
Epoch 8/10
10/10 - 1s - loss: 55.1224 - loglik: -5.2135e+01 - logprior: -2.7579e+00
Epoch 9/10
10/10 - 1s - loss: 54.7759 - loglik: -5.2106e+01 - logprior: -2.4268e+00
Epoch 10/10
10/10 - 1s - loss: 54.5538 - loglik: -5.2101e+01 - logprior: -2.1917e+00
Fitted a model with MAP estimate = -54.1980
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.2490 - loglik: -4.9486e+01 - logprior: -1.2372e+02
Epoch 2/2
10/10 - 1s - loss: 86.1883 - loglik: -4.5910e+01 - logprior: -4.0169e+01
Fitted a model with MAP estimate = -69.1094
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 130.4664 - loglik: -4.3411e+01 - logprior: -8.6966e+01
Epoch 2/10
10/10 - 1s - loss: 68.7858 - loglik: -4.3666e+01 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 56.2526 - loglik: -4.3805e+01 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 1s - loss: 51.2868 - loglik: -4.3883e+01 - logprior: -7.1778e+00
Epoch 5/10
10/10 - 1s - loss: 48.6880 - loglik: -4.4042e+01 - logprior: -4.4300e+00
Epoch 6/10
10/10 - 1s - loss: 46.7624 - loglik: -4.3623e+01 - logprior: -2.8965e+00
Epoch 7/10
10/10 - 1s - loss: 45.6546 - loglik: -4.3424e+01 - logprior: -1.9805e+00
Epoch 8/10
10/10 - 1s - loss: 45.0466 - loglik: -4.3364e+01 - logprior: -1.4405e+00
Epoch 9/10
10/10 - 1s - loss: 44.6767 - loglik: -4.3327e+01 - logprior: -1.1094e+00
Epoch 10/10
10/10 - 1s - loss: 44.4161 - loglik: -4.3339e+01 - logprior: -8.2640e-01
Fitted a model with MAP estimate = -44.0415
Time for alignment: 31.1143
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5948 - loglik: -7.9866e+01 - logprior: -9.2420e+01
Epoch 2/10
10/10 - 1s - loss: 95.3077 - loglik: -6.8554e+01 - logprior: -2.6612e+01
Epoch 3/10
10/10 - 1s - loss: 73.2303 - loglik: -5.9840e+01 - logprior: -1.3340e+01
Epoch 4/10
10/10 - 1s - loss: 64.1418 - loglik: -5.5859e+01 - logprior: -8.2085e+00
Epoch 5/10
10/10 - 1s - loss: 59.2657 - loglik: -5.3503e+01 - logprior: -5.5851e+00
Epoch 6/10
10/10 - 1s - loss: 57.0448 - loglik: -5.2632e+01 - logprior: -4.1322e+00
Epoch 7/10
10/10 - 1s - loss: 55.9515 - loglik: -5.2370e+01 - logprior: -3.2833e+00
Epoch 8/10
10/10 - 1s - loss: 55.2251 - loglik: -5.2206e+01 - logprior: -2.7467e+00
Epoch 9/10
10/10 - 1s - loss: 54.8329 - loglik: -5.2152e+01 - logprior: -2.4209e+00
Epoch 10/10
10/10 - 1s - loss: 54.5892 - loglik: -5.2133e+01 - logprior: -2.1938e+00
Fitted a model with MAP estimate = -54.2232
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.1979 - loglik: -4.9419e+01 - logprior: -1.2372e+02
Epoch 2/2
10/10 - 1s - loss: 86.2190 - loglik: -4.5970e+01 - logprior: -4.0170e+01
Fitted a model with MAP estimate = -69.1764
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 130.5475 - loglik: -4.3462e+01 - logprior: -8.6962e+01
Epoch 2/10
10/10 - 1s - loss: 68.8608 - loglik: -4.3763e+01 - logprior: -2.5060e+01
Epoch 3/10
10/10 - 1s - loss: 56.3065 - loglik: -4.3877e+01 - logprior: -1.2321e+01
Epoch 4/10
10/10 - 1s - loss: 51.3503 - loglik: -4.3947e+01 - logprior: -7.1866e+00
Epoch 5/10
10/10 - 1s - loss: 48.8019 - loglik: -4.4153e+01 - logprior: -4.4409e+00
Epoch 6/10
10/10 - 1s - loss: 46.9757 - loglik: -4.3855e+01 - logprior: -2.9028e+00
Epoch 7/10
10/10 - 1s - loss: 45.7345 - loglik: -4.3445e+01 - logprior: -2.0382e+00
Epoch 8/10
10/10 - 1s - loss: 45.0627 - loglik: -4.3327e+01 - logprior: -1.4810e+00
Epoch 9/10
10/10 - 1s - loss: 44.6915 - loglik: -4.3310e+01 - logprior: -1.1373e+00
Epoch 10/10
10/10 - 1s - loss: 44.4321 - loglik: -4.3343e+01 - logprior: -8.4007e-01
Fitted a model with MAP estimate = -44.0590
Time for alignment: 30.3602
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.3062 - loglik: -7.9832e+01 - logprior: -9.2418e+01
Epoch 2/10
10/10 - 1s - loss: 95.3296 - loglik: -6.8684e+01 - logprior: -2.6606e+01
Epoch 3/10
10/10 - 1s - loss: 73.2153 - loglik: -5.9868e+01 - logprior: -1.3332e+01
Epoch 4/10
10/10 - 1s - loss: 63.9532 - loglik: -5.5668e+01 - logprior: -8.2085e+00
Epoch 5/10
10/10 - 1s - loss: 59.0027 - loglik: -5.3211e+01 - logprior: -5.5987e+00
Epoch 6/10
10/10 - 1s - loss: 56.8603 - loglik: -5.2447e+01 - logprior: -4.1475e+00
Epoch 7/10
10/10 - 1s - loss: 55.7709 - loglik: -5.2224e+01 - logprior: -3.2934e+00
Epoch 8/10
10/10 - 1s - loss: 55.1059 - loglik: -5.2097e+01 - logprior: -2.7542e+00
Epoch 9/10
10/10 - 1s - loss: 54.7675 - loglik: -5.2072e+01 - logprior: -2.4269e+00
Epoch 10/10
10/10 - 1s - loss: 54.5464 - loglik: -5.2081e+01 - logprior: -2.1920e+00
Fitted a model with MAP estimate = -54.1788
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.7028 - loglik: -4.9792e+01 - logprior: -1.2373e+02
Epoch 2/2
10/10 - 1s - loss: 86.4595 - loglik: -4.6186e+01 - logprior: -4.0202e+01
Fitted a model with MAP estimate = -69.5747
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 131.0317 - loglik: -4.3934e+01 - logprior: -8.6980e+01
Epoch 2/10
10/10 - 1s - loss: 68.9060 - loglik: -4.3837e+01 - logprior: -2.5051e+01
Epoch 3/10
10/10 - 1s - loss: 56.5233 - loglik: -4.4215e+01 - logprior: -1.2292e+01
Epoch 4/10
10/10 - 1s - loss: 51.5935 - loglik: -4.4411e+01 - logprior: -7.1358e+00
Epoch 5/10
10/10 - 1s - loss: 48.9590 - loglik: -4.4434e+01 - logprior: -4.4099e+00
Epoch 6/10
10/10 - 1s - loss: 47.4857 - loglik: -4.4422e+01 - logprior: -2.9045e+00
Epoch 7/10
10/10 - 1s - loss: 46.0742 - loglik: -4.3847e+01 - logprior: -2.0455e+00
Epoch 8/10
10/10 - 1s - loss: 45.3143 - loglik: -4.3596e+01 - logprior: -1.5341e+00
Epoch 9/10
10/10 - 1s - loss: 44.9193 - loglik: -4.3597e+01 - logprior: -1.1427e+00
Epoch 10/10
10/10 - 1s - loss: 44.6506 - loglik: -4.3654e+01 - logprior: -8.1014e-01
Fitted a model with MAP estimate = -44.3262
Time for alignment: 30.0862
Computed alignments with likelihoods: ['-44.0415', '-44.0590', '-44.3262']
Best model has likelihood: -44.0415
time for generating output: 0.1256
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9418604651162791
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac5dce5d30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab05228d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac336f2c10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab0d4fd6d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2252efd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa8cb92df0>, <__main__.SimpleDirichletPrior object at 0x7fa8f0585790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.9465 - loglik: -3.5045e+02 - logprior: -1.2329e+01
Epoch 2/10
11/11 - 2s - loss: 309.8921 - loglik: -3.0681e+02 - logprior: -2.9713e+00
Epoch 3/10
11/11 - 2s - loss: 269.1232 - loglik: -2.6702e+02 - logprior: -1.9979e+00
Epoch 4/10
11/11 - 3s - loss: 247.1314 - loglik: -2.4466e+02 - logprior: -2.1595e+00
Epoch 5/10
11/11 - 2s - loss: 242.3969 - loglik: -2.3961e+02 - logprior: -2.2525e+00
Epoch 6/10
11/11 - 3s - loss: 239.3711 - loglik: -2.3657e+02 - logprior: -2.2140e+00
Epoch 7/10
11/11 - 2s - loss: 237.1578 - loglik: -2.3447e+02 - logprior: -2.1503e+00
Epoch 8/10
11/11 - 3s - loss: 236.5628 - loglik: -2.3387e+02 - logprior: -2.1852e+00
Epoch 9/10
11/11 - 3s - loss: 235.6280 - loglik: -2.3294e+02 - logprior: -2.2016e+00
Epoch 10/10
11/11 - 3s - loss: 235.2718 - loglik: -2.3260e+02 - logprior: -2.2133e+00
Fitted a model with MAP estimate = -234.3695
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (17, 1), (33, 1), (34, 1), (38, 1), (39, 2), (61, 2), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 256.6730 - loglik: -2.4235e+02 - logprior: -1.4275e+01
Epoch 2/2
11/11 - 3s - loss: 229.7092 - loglik: -2.2361e+02 - logprior: -6.0185e+00
Fitted a model with MAP estimate = -224.6222
expansions: [(0, 24)]
discards: [ 0  7  8 17 74 85 88]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 10s - loss: 230.3364 - loglik: -2.2265e+02 - logprior: -7.6040e+00
Epoch 2/2
22/22 - 5s - loss: 217.3947 - loglik: -2.1523e+02 - logprior: -1.8386e+00
Fitted a model with MAP estimate = -214.1728
expansions: [(30, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 232.1580 - loglik: -2.1836e+02 - logprior: -1.3711e+01
Epoch 2/10
11/11 - 3s - loss: 218.2954 - loglik: -2.1405e+02 - logprior: -4.0180e+00
Epoch 3/10
11/11 - 3s - loss: 214.7632 - loglik: -2.1299e+02 - logprior: -1.4060e+00
Epoch 4/10
11/11 - 3s - loss: 211.9349 - loglik: -2.1075e+02 - logprior: -7.0401e-01
Epoch 5/10
11/11 - 3s - loss: 210.3799 - loglik: -2.0928e+02 - logprior: -5.4772e-01
Epoch 6/10
11/11 - 3s - loss: 210.8493 - loglik: -2.0984e+02 - logprior: -4.4853e-01
Fitted a model with MAP estimate = -209.1849
Time for alignment: 86.0855
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.7177 - loglik: -3.5017e+02 - logprior: -1.2329e+01
Epoch 2/10
11/11 - 2s - loss: 309.5988 - loglik: -3.0650e+02 - logprior: -2.9741e+00
Epoch 3/10
11/11 - 3s - loss: 271.3575 - loglik: -2.6921e+02 - logprior: -2.0204e+00
Epoch 4/10
11/11 - 3s - loss: 247.8278 - loglik: -2.4529e+02 - logprior: -2.2248e+00
Epoch 5/10
11/11 - 2s - loss: 242.1332 - loglik: -2.3920e+02 - logprior: -2.4037e+00
Epoch 6/10
11/11 - 3s - loss: 237.0798 - loglik: -2.3403e+02 - logprior: -2.4211e+00
Epoch 7/10
11/11 - 2s - loss: 236.4451 - loglik: -2.3347e+02 - logprior: -2.3861e+00
Epoch 8/10
11/11 - 2s - loss: 234.5438 - loglik: -2.3168e+02 - logprior: -2.3587e+00
Epoch 9/10
11/11 - 3s - loss: 234.5950 - loglik: -2.3179e+02 - logprior: -2.3462e+00
Fitted a model with MAP estimate = -233.4493
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 254.2601 - loglik: -2.3996e+02 - logprior: -1.4265e+01
Epoch 2/2
11/11 - 3s - loss: 229.2440 - loglik: -2.2307e+02 - logprior: -6.0348e+00
Fitted a model with MAP estimate = -222.5664
expansions: [(0, 20)]
discards: [ 0  8 75 76 84 87]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 233.7706 - loglik: -2.2178e+02 - logprior: -1.1887e+01
Epoch 2/2
11/11 - 4s - loss: 219.9559 - loglik: -2.1624e+02 - logprior: -3.5331e+00
Fitted a model with MAP estimate = -216.6043
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 230.8624 - loglik: -2.1813e+02 - logprior: -1.2603e+01
Epoch 2/10
11/11 - 3s - loss: 217.9354 - loglik: -2.1486e+02 - logprior: -3.0057e+00
Epoch 3/10
11/11 - 3s - loss: 215.1673 - loglik: -2.1374e+02 - logprior: -1.2532e+00
Epoch 4/10
11/11 - 3s - loss: 212.1473 - loglik: -2.1097e+02 - logprior: -7.7621e-01
Epoch 5/10
11/11 - 3s - loss: 212.9577 - loglik: -2.1174e+02 - logprior: -6.3671e-01
Fitted a model with MAP estimate = -209.8665
Time for alignment: 75.2702
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 362.4977 - loglik: -3.5012e+02 - logprior: -1.2321e+01
Epoch 2/10
11/11 - 2s - loss: 308.4008 - loglik: -3.0540e+02 - logprior: -2.9725e+00
Epoch 3/10
11/11 - 3s - loss: 265.0852 - loglik: -2.6305e+02 - logprior: -1.9820e+00
Epoch 4/10
11/11 - 3s - loss: 247.6200 - loglik: -2.4532e+02 - logprior: -2.0886e+00
Epoch 5/10
11/11 - 3s - loss: 240.5552 - loglik: -2.3797e+02 - logprior: -2.1463e+00
Epoch 6/10
11/11 - 3s - loss: 238.4129 - loglik: -2.3578e+02 - logprior: -2.1242e+00
Epoch 7/10
11/11 - 3s - loss: 237.2436 - loglik: -2.3472e+02 - logprior: -2.0627e+00
Epoch 8/10
11/11 - 3s - loss: 235.9196 - loglik: -2.3343e+02 - logprior: -2.0669e+00
Epoch 9/10
11/11 - 2s - loss: 235.3530 - loglik: -2.3285e+02 - logprior: -2.1033e+00
Epoch 10/10
11/11 - 3s - loss: 235.7537 - loglik: -2.3326e+02 - logprior: -2.1272e+00
Fitted a model with MAP estimate = -234.7261
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 2), (61, 2), (62, 2), (63, 1), (65, 1), (66, 1), (67, 1), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 254.6074 - loglik: -2.4035e+02 - logprior: -1.4218e+01
Epoch 2/2
11/11 - 3s - loss: 228.3800 - loglik: -2.2237e+02 - logprior: -5.9234e+00
Fitted a model with MAP estimate = -223.1616
expansions: [(0, 24)]
discards: [  0   9  10  73  76 108]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 8s - loss: 228.2746 - loglik: -2.2054e+02 - logprior: -7.6378e+00
Epoch 2/2
22/22 - 4s - loss: 217.8254 - loglik: -2.1549e+02 - logprior: -2.0022e+00
Fitted a model with MAP estimate = -214.2787
expansions: [(32, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 229.1009 - loglik: -2.1642e+02 - logprior: -1.2557e+01
Epoch 2/10
11/11 - 3s - loss: 216.5218 - loglik: -2.1339e+02 - logprior: -2.9227e+00
Epoch 3/10
11/11 - 3s - loss: 214.0968 - loglik: -2.1254e+02 - logprior: -1.2647e+00
Epoch 4/10
11/11 - 3s - loss: 212.7011 - loglik: -2.1141e+02 - logprior: -8.6791e-01
Epoch 5/10
11/11 - 3s - loss: 213.6351 - loglik: -2.1243e+02 - logprior: -6.9486e-01
Fitted a model with MAP estimate = -210.5425
Time for alignment: 83.4396
Computed alignments with likelihoods: ['-209.1849', '-209.8665', '-210.5425']
Best model has likelihood: -209.1849
time for generating output: 0.2978
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.5492248062015503
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faad21b10d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa833c8640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa81ad3e50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1b6e7c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac336f2c10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab0d4fd940>, <__main__.SimpleDirichletPrior object at 0x7faab8794b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 753.4875 - loglik: -7.5149e+02 - logprior: -1.7350e+00
Epoch 2/10
39/39 - 19s - loss: 632.9481 - loglik: -6.3011e+02 - logprior: -1.8182e+00
Epoch 3/10
39/39 - 19s - loss: 621.4598 - loglik: -6.1829e+02 - logprior: -1.9175e+00
Epoch 4/10
39/39 - 19s - loss: 619.2296 - loglik: -6.1614e+02 - logprior: -1.8753e+00
Epoch 5/10
39/39 - 19s - loss: 617.8999 - loglik: -6.1492e+02 - logprior: -1.8661e+00
Epoch 6/10
39/39 - 19s - loss: 616.9942 - loglik: -6.1405e+02 - logprior: -1.8681e+00
Epoch 7/10
39/39 - 19s - loss: 616.5582 - loglik: -6.1361e+02 - logprior: -1.8749e+00
Epoch 8/10
39/39 - 19s - loss: 615.6132 - loglik: -6.1266e+02 - logprior: -1.8826e+00
Epoch 9/10
39/39 - 19s - loss: 615.4141 - loglik: -6.1244e+02 - logprior: -1.8949e+00
Epoch 10/10
39/39 - 19s - loss: 614.5908 - loglik: -6.1161e+02 - logprior: -1.9083e+00
Fitted a model with MAP estimate = -576.1736
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (45, 3), (46, 1), (59, 1), (60, 1), (62, 5), (63, 1), (66, 1), (67, 1), (68, 1), (70, 1), (118, 1), (119, 2), (124, 2), (125, 5), (127, 1), (128, 1), (135, 1), (137, 2), (138, 1), (139, 1), (140, 3), (141, 2), (142, 2), (146, 2), (152, 1), (157, 1), (158, 2), (159, 2), (165, 3), (166, 1), (167, 3), (168, 3), (169, 1), (170, 1), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (202, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 625.1349 - loglik: -6.2195e+02 - logprior: -2.9321e+00
Epoch 2/2
39/39 - 29s - loss: 592.2414 - loglik: -5.9005e+02 - logprior: -1.4040e+00
Fitted a model with MAP estimate = -546.7934
expansions: [(0, 2), (72, 1), (153, 2), (233, 1), (234, 1)]
discards: [  0  11  12  58  73 145 156 157 158 175 180 187 189 195 214 223 229 250
 289]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 602.0150 - loglik: -6.0026e+02 - logprior: -1.6494e+00
Epoch 2/2
39/39 - 27s - loss: 590.2609 - loglik: -5.8922e+02 - logprior: -6.3074e-01
Fitted a model with MAP estimate = -547.4304
expansions: []
discards: [  0 151 205 206 207 278]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 34s - loss: 553.4313 - loglik: -5.5159e+02 - logprior: -1.6641e+00
Epoch 2/10
45/45 - 29s - loss: 546.0355 - loglik: -5.4486e+02 - logprior: -5.8619e-01
Epoch 3/10
45/45 - 30s - loss: 541.4811 - loglik: -5.3998e+02 - logprior: -6.0262e-01
Epoch 4/10
45/45 - 30s - loss: 542.8369 - loglik: -5.4125e+02 - logprior: -5.0434e-01
Fitted a model with MAP estimate = -539.0232
Time for alignment: 580.2806
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 759.2200 - loglik: -7.5722e+02 - logprior: -1.6789e+00
Epoch 2/10
39/39 - 19s - loss: 636.4985 - loglik: -6.3372e+02 - logprior: -1.6633e+00
Epoch 3/10
39/39 - 19s - loss: 623.7314 - loglik: -6.2068e+02 - logprior: -1.7251e+00
Epoch 4/10
39/39 - 19s - loss: 621.0830 - loglik: -6.1819e+02 - logprior: -1.6638e+00
Epoch 5/10
39/39 - 19s - loss: 620.0353 - loglik: -6.1723e+02 - logprior: -1.6472e+00
Epoch 6/10
39/39 - 19s - loss: 619.2963 - loglik: -6.1658e+02 - logprior: -1.6504e+00
Epoch 7/10
39/39 - 19s - loss: 618.8099 - loglik: -6.1610e+02 - logprior: -1.6570e+00
Epoch 8/10
39/39 - 19s - loss: 618.2416 - loglik: -6.1554e+02 - logprior: -1.6623e+00
Epoch 9/10
39/39 - 19s - loss: 617.7705 - loglik: -6.1510e+02 - logprior: -1.6633e+00
Epoch 10/10
39/39 - 19s - loss: 617.3322 - loglik: -6.1469e+02 - logprior: -1.6710e+00
Fitted a model with MAP estimate = -578.4303
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (23, 2), (35, 1), (36, 1), (45, 3), (46, 1), (58, 5), (59, 4), (60, 1), (64, 1), (90, 1), (119, 4), (120, 14), (121, 2), (128, 1), (132, 1), (136, 1), (142, 2), (144, 2), (157, 2), (158, 2), (164, 4), (166, 1), (168, 4), (169, 2), (179, 1), (180, 2), (181, 1), (193, 1), (194, 1), (202, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [  0 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 622.7756 - loglik: -6.1957e+02 - logprior: -2.8875e+00
Epoch 2/2
39/39 - 28s - loss: 591.7552 - loglik: -5.8954e+02 - logprior: -1.2742e+00
Fitted a model with MAP estimate = -546.3499
expansions: [(0, 2), (231, 1)]
discards: [  0  11  12  29  59 152 153 154 190 193 211 212 222 247 286 288]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 601.6541 - loglik: -5.9962e+02 - logprior: -1.5507e+00
Epoch 2/2
39/39 - 27s - loss: 589.5483 - loglik: -5.8768e+02 - logprior: -6.1240e-01
Fitted a model with MAP estimate = -545.2934
expansions: [(203, 2)]
discards: [  0 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 34s - loss: 558.7822 - loglik: -5.5706e+02 - logprior: -1.6300e+00
Epoch 2/10
45/45 - 30s - loss: 546.7255 - loglik: -5.4570e+02 - logprior: -6.1651e-01
Epoch 3/10
45/45 - 30s - loss: 541.5997 - loglik: -5.4038e+02 - logprior: -5.1331e-01
Epoch 4/10
45/45 - 30s - loss: 541.5884 - loglik: -5.4023e+02 - logprior: -4.6286e-01
Epoch 5/10
45/45 - 31s - loss: 541.0850 - loglik: -5.3974e+02 - logprior: -3.4340e-01
Epoch 6/10
45/45 - 29s - loss: 538.3697 - loglik: -5.3699e+02 - logprior: -3.4651e-01
Epoch 7/10
45/45 - 30s - loss: 538.9760 - loglik: -5.3772e+02 - logprior: -1.9145e-01
Fitted a model with MAP estimate = -537.0652
Time for alignment: 670.8987
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 753.8325 - loglik: -7.5173e+02 - logprior: -1.6741e+00
Epoch 2/10
39/39 - 19s - loss: 636.1491 - loglik: -6.3342e+02 - logprior: -1.5339e+00
Epoch 3/10
39/39 - 19s - loss: 622.3134 - loglik: -6.1926e+02 - logprior: -1.6724e+00
Epoch 4/10
39/39 - 19s - loss: 619.4478 - loglik: -6.1653e+02 - logprior: -1.6489e+00
Epoch 5/10
39/39 - 19s - loss: 618.0964 - loglik: -6.1533e+02 - logprior: -1.6459e+00
Epoch 6/10
39/39 - 19s - loss: 617.0077 - loglik: -6.1429e+02 - logprior: -1.6575e+00
Epoch 7/10
39/39 - 19s - loss: 616.4072 - loglik: -6.1368e+02 - logprior: -1.6670e+00
Epoch 8/10
39/39 - 19s - loss: 615.8865 - loglik: -6.1319e+02 - logprior: -1.6799e+00
Epoch 9/10
39/39 - 19s - loss: 615.2842 - loglik: -6.1259e+02 - logprior: -1.6913e+00
Epoch 10/10
39/39 - 19s - loss: 615.1376 - loglik: -6.1247e+02 - logprior: -1.6927e+00
Fitted a model with MAP estimate = -575.5066
expansions: [(11, 5), (13, 1), (15, 1), (19, 1), (35, 1), (36, 1), (46, 2), (47, 2), (58, 5), (60, 4), (65, 1), (67, 1), (70, 1), (120, 2), (126, 2), (127, 5), (128, 2), (129, 2), (138, 3), (139, 1), (140, 1), (141, 3), (142, 2), (143, 2), (145, 2), (146, 1), (151, 1), (159, 3), (164, 1), (165, 1), (166, 1), (167, 5), (168, 3), (169, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 622.2122 - loglik: -6.1897e+02 - logprior: -2.8878e+00
Epoch 2/2
39/39 - 29s - loss: 591.0182 - loglik: -5.8873e+02 - logprior: -1.3354e+00
Fitted a model with MAP estimate = -546.8654
expansions: [(0, 2)]
discards: [  0  12  13  58  76 146 157 158 159 162 177 182 189 191 196 211 212 213
 214 235 251 290 292]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 603.1445 - loglik: -6.0142e+02 - logprior: -1.5991e+00
Epoch 2/2
39/39 - 26s - loss: 591.9519 - loglik: -5.9094e+02 - logprior: -5.4775e-01
Fitted a model with MAP estimate = -548.6723
expansions: [(199, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 552.8134 - loglik: -5.5090e+02 - logprior: -1.6111e+00
Epoch 2/10
45/45 - 29s - loss: 544.1801 - loglik: -5.4281e+02 - logprior: -6.2217e-01
Epoch 3/10
45/45 - 30s - loss: 542.5162 - loglik: -5.4090e+02 - logprior: -5.7501e-01
Epoch 4/10
45/45 - 31s - loss: 541.7917 - loglik: -5.4009e+02 - logprior: -4.8206e-01
Epoch 5/10
45/45 - 29s - loss: 539.6776 - loglik: -5.3802e+02 - logprior: -4.4353e-01
Epoch 6/10
45/45 - 30s - loss: 539.4620 - loglik: -5.3795e+02 - logprior: -3.3032e-01
Epoch 7/10
45/45 - 30s - loss: 538.9723 - loglik: -5.3760e+02 - logprior: -2.1145e-01
Epoch 8/10
45/45 - 29s - loss: 538.0609 - loglik: -5.3676e+02 - logprior: -1.6730e-01
Epoch 9/10
45/45 - 30s - loss: 538.5992 - loglik: -5.3741e+02 - logprior: -6.8314e-02
Fitted a model with MAP estimate = -536.1320
Time for alignment: 727.4526
Computed alignments with likelihoods: ['-539.0232', '-537.0652', '-536.1320']
Best model has likelihood: -536.1320
time for generating output: 0.3233
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8939142001995344
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac331ed910>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa8d2fdf10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa81c849a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6f1bdc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd2f0460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa82230eb0>, <__main__.SimpleDirichletPrior object at 0x7fabe691e580>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 445.1371 - loglik: -4.4386e+02 - logprior: -1.0516e+00
Epoch 2/10
30/30 - 8s - loss: 371.9012 - loglik: -3.6974e+02 - logprior: -1.1579e+00
Epoch 3/10
30/30 - 8s - loss: 360.3237 - loglik: -3.5816e+02 - logprior: -1.1598e+00
Epoch 4/10
30/30 - 8s - loss: 357.9666 - loglik: -3.5594e+02 - logprior: -1.1468e+00
Epoch 5/10
30/30 - 8s - loss: 357.6849 - loglik: -3.5574e+02 - logprior: -1.1158e+00
Epoch 6/10
30/30 - 8s - loss: 356.7052 - loglik: -3.5481e+02 - logprior: -1.1025e+00
Epoch 7/10
30/30 - 8s - loss: 356.2911 - loglik: -3.5442e+02 - logprior: -1.0938e+00
Epoch 8/10
30/30 - 8s - loss: 356.1397 - loglik: -3.5436e+02 - logprior: -1.0911e+00
Epoch 9/10
30/30 - 8s - loss: 355.5323 - loglik: -3.5379e+02 - logprior: -1.0899e+00
Epoch 10/10
30/30 - 8s - loss: 355.7891 - loglik: -3.5407e+02 - logprior: -1.0867e+00
Fitted a model with MAP estimate = -347.1274
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 353.7858 - loglik: -3.5217e+02 - logprior: -1.2176e+00
Epoch 2/2
61/61 - 14s - loss: 342.7695 - loglik: -3.4116e+02 - logprior: -9.1110e-01
Fitted a model with MAP estimate = -330.3525
expansions: []
discards: [ 18  30  48  52  93  95 133 148 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 346.7988 - loglik: -3.4538e+02 - logprior: -1.0718e+00
Epoch 2/2
61/61 - 14s - loss: 343.0356 - loglik: -3.4101e+02 - logprior: -8.1797e-01
Fitted a model with MAP estimate = -330.4294
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 333.0248 - loglik: -3.3208e+02 - logprior: -7.2301e-01
Epoch 2/10
87/87 - 18s - loss: 329.4545 - loglik: -3.2799e+02 - logprior: -6.2008e-01
Epoch 3/10
87/87 - 19s - loss: 328.5291 - loglik: -3.2685e+02 - logprior: -5.9797e-01
Epoch 4/10
87/87 - 19s - loss: 327.4057 - loglik: -3.2586e+02 - logprior: -5.7510e-01
Epoch 5/10
87/87 - 19s - loss: 327.5588 - loglik: -3.2604e+02 - logprior: -5.5324e-01
Fitted a model with MAP estimate = -326.0204
Time for alignment: 409.5843
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.3374 - loglik: -4.4396e+02 - logprior: -1.0578e+00
Epoch 2/10
30/30 - 8s - loss: 371.7495 - loglik: -3.6950e+02 - logprior: -1.1486e+00
Epoch 3/10
30/30 - 8s - loss: 360.4846 - loglik: -3.5825e+02 - logprior: -1.1383e+00
Epoch 4/10
30/30 - 8s - loss: 358.3384 - loglik: -3.5618e+02 - logprior: -1.1272e+00
Epoch 5/10
30/30 - 8s - loss: 357.3192 - loglik: -3.5529e+02 - logprior: -1.1047e+00
Epoch 6/10
30/30 - 8s - loss: 356.6764 - loglik: -3.5475e+02 - logprior: -1.0895e+00
Epoch 7/10
30/30 - 8s - loss: 356.6950 - loglik: -3.5479e+02 - logprior: -1.0813e+00
Fitted a model with MAP estimate = -345.9900
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (36, 1), (38, 1), (39, 2), (40, 3), (41, 1), (43, 1), (48, 1), (51, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (92, 1), (93, 1), (97, 1), (102, 2), (113, 3), (114, 1), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 352.1165 - loglik: -3.5026e+02 - logprior: -1.2024e+00
Epoch 2/2
61/61 - 14s - loss: 342.3600 - loglik: -3.4008e+02 - logprior: -8.9740e-01
Fitted a model with MAP estimate = -329.8955
expansions: []
discards: [ 25  48  51  92  94 132 146 153]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 346.9676 - loglik: -3.4550e+02 - logprior: -1.0686e+00
Epoch 2/2
61/61 - 14s - loss: 342.3324 - loglik: -3.4037e+02 - logprior: -7.9356e-01
Fitted a model with MAP estimate = -330.6505
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 331.2169 - loglik: -3.2991e+02 - logprior: -7.3118e-01
Epoch 2/10
87/87 - 19s - loss: 329.2153 - loglik: -3.2732e+02 - logprior: -6.1812e-01
Epoch 3/10
87/87 - 19s - loss: 328.3284 - loglik: -3.2642e+02 - logprior: -5.9977e-01
Epoch 4/10
87/87 - 19s - loss: 327.5244 - loglik: -3.2592e+02 - logprior: -5.7321e-01
Epoch 5/10
87/87 - 19s - loss: 327.7776 - loglik: -3.2622e+02 - logprior: -5.4861e-01
Fitted a model with MAP estimate = -325.7976
Time for alignment: 380.6898
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 445.1238 - loglik: -4.4391e+02 - logprior: -1.0548e+00
Epoch 2/10
30/30 - 8s - loss: 372.0114 - loglik: -3.6998e+02 - logprior: -1.1554e+00
Epoch 3/10
30/30 - 8s - loss: 360.1526 - loglik: -3.5804e+02 - logprior: -1.1504e+00
Epoch 4/10
30/30 - 8s - loss: 358.2071 - loglik: -3.5617e+02 - logprior: -1.1390e+00
Epoch 5/10
30/30 - 8s - loss: 357.5950 - loglik: -3.5564e+02 - logprior: -1.1159e+00
Epoch 6/10
30/30 - 8s - loss: 356.6418 - loglik: -3.5475e+02 - logprior: -1.1012e+00
Epoch 7/10
30/30 - 8s - loss: 356.2754 - loglik: -3.5438e+02 - logprior: -1.0916e+00
Epoch 8/10
30/30 - 8s - loss: 356.0841 - loglik: -3.5429e+02 - logprior: -1.0887e+00
Epoch 9/10
30/30 - 8s - loss: 355.8900 - loglik: -3.5413e+02 - logprior: -1.0862e+00
Epoch 10/10
30/30 - 8s - loss: 355.4671 - loglik: -3.5372e+02 - logprior: -1.0850e+00
Fitted a model with MAP estimate = -347.2818
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (22, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 355.7183 - loglik: -3.5413e+02 - logprior: -1.2388e+00
Epoch 2/2
61/61 - 14s - loss: 342.6235 - loglik: -3.4069e+02 - logprior: -9.3729e-01
Fitted a model with MAP estimate = -330.3553
expansions: []
discards: [ 18  26  48  52  93  95 104 134 148 152 156]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 19s - loss: 346.6757 - loglik: -3.4542e+02 - logprior: -1.0709e+00
Epoch 2/2
61/61 - 14s - loss: 343.3448 - loglik: -3.4198e+02 - logprior: -7.9468e-01
Fitted a model with MAP estimate = -330.8670
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 23s - loss: 332.7431 - loglik: -3.3183e+02 - logprior: -7.1565e-01
Epoch 2/10
87/87 - 19s - loss: 329.9145 - loglik: -3.2847e+02 - logprior: -6.1539e-01
Epoch 3/10
87/87 - 19s - loss: 328.4817 - loglik: -3.2680e+02 - logprior: -6.0280e-01
Epoch 4/10
87/87 - 19s - loss: 327.5674 - loglik: -3.2605e+02 - logprior: -5.7583e-01
Epoch 5/10
87/87 - 19s - loss: 327.6279 - loglik: -3.2613e+02 - logprior: -5.5615e-01
Fitted a model with MAP estimate = -326.1405
Time for alignment: 405.2381
Computed alignments with likelihoods: ['-326.0204', '-325.7976', '-326.1405']
Best model has likelihood: -325.7976
time for generating output: 0.2559
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.638681900976983
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa8cfa1ca0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaeb9c5c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabde684610>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaaa4f100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa81b7c4f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabaaed1820>, <__main__.SimpleDirichletPrior object at 0x7fabd5d38e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 1226.8984 - loglik: -1.2249e+03 - logprior: -1.4199e+00
Epoch 2/10
40/40 - 44s - loss: 1090.7330 - loglik: -1.0866e+03 - logprior: -2.4539e+00
Epoch 3/10
40/40 - 44s - loss: 1069.1237 - loglik: -1.0634e+03 - logprior: -2.7897e+00
Epoch 4/10
40/40 - 44s - loss: 1058.8450 - loglik: -1.0521e+03 - logprior: -2.9201e+00
Epoch 5/10
40/40 - 44s - loss: 1054.3507 - loglik: -1.0472e+03 - logprior: -3.0057e+00
Epoch 6/10
40/40 - 44s - loss: 1050.7709 - loglik: -1.0438e+03 - logprior: -3.0426e+00
Epoch 7/10
40/40 - 45s - loss: 1047.7871 - loglik: -1.0411e+03 - logprior: -3.0860e+00
Epoch 8/10
40/40 - 44s - loss: 1046.2263 - loglik: -1.0398e+03 - logprior: -3.1222e+00
Epoch 9/10
40/40 - 45s - loss: 1044.7725 - loglik: -1.0387e+03 - logprior: -3.1303e+00
Epoch 10/10
40/40 - 45s - loss: 1043.8964 - loglik: -1.0380e+03 - logprior: -3.1511e+00
Fitted a model with MAP estimate = -807.4041
expansions: [(125, 1), (171, 1), (330, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 109 123 162
 163 180 181 182 183 184 185 186 187 211 212 213 214 215 216 217 218 219
 239 247 248 249 250 251 252 253 254 255 256 258 259 260 261 262 263 264
 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282
 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300
 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318
 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 22s - loss: 1328.7375 - loglik: -1.3249e+03 - logprior: -3.4748e+00
Epoch 2/2
40/40 - 17s - loss: 1248.2468 - loglik: -1.2447e+03 - logprior: -1.7646e+00
Fitted a model with MAP estimate = -897.3155
expansions: [(0, 314), (19, 1), (22, 2), (23, 9), (56, 3), (61, 1), (62, 6), (63, 3), (73, 3), (115, 1), (118, 1), (120, 1)]
discards: [  0  17  64  65 101 116 122]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 83s - loss: 1079.8160 - loglik: -1.0774e+03 - logprior: -1.0298e+00
Epoch 2/2
80/80 - 79s - loss: 1011.2617 - loglik: -1.0069e+03 - logprior: -5.5119e-01
Fitted a model with MAP estimate = -749.4071
expansions: [(256, 1), (265, 1), (286, 1), (294, 1), (301, 1), (304, 1)]
discards: [  0  19  20  80  81  82  83 135 203 341 342 382 389 390 391]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 114s - loss: 770.8342 - loglik: -7.6809e+02 - logprior: -7.8486e-01
Epoch 2/10
113/113 - 108s - loss: 744.1230 - loglik: -7.3885e+02 - logprior: -4.0385e-01
Epoch 3/10
113/113 - 108s - loss: 741.5455 - loglik: -7.3677e+02 - logprior: -4.2472e-01
Epoch 4/10
113/113 - 109s - loss: 744.4022 - loglik: -7.3995e+02 - logprior: -1.7040e-01
Fitted a model with MAP estimate = -734.2594
Time for alignment: 1437.9934
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1222.4410 - loglik: -1.2201e+03 - logprior: -1.6433e+00
Epoch 2/10
40/40 - 44s - loss: 1088.3611 - loglik: -1.0839e+03 - logprior: -2.5224e+00
Epoch 3/10
40/40 - 44s - loss: 1068.7634 - loglik: -1.0627e+03 - logprior: -2.7819e+00
Epoch 4/10
40/40 - 44s - loss: 1059.3093 - loglik: -1.0523e+03 - logprior: -2.9274e+00
Epoch 5/10
40/40 - 44s - loss: 1055.7166 - loglik: -1.0485e+03 - logprior: -2.9850e+00
Epoch 6/10
40/40 - 44s - loss: 1052.2986 - loglik: -1.0454e+03 - logprior: -3.0231e+00
Epoch 7/10
40/40 - 44s - loss: 1050.3987 - loglik: -1.0438e+03 - logprior: -3.0753e+00
Epoch 8/10
40/40 - 45s - loss: 1047.3759 - loglik: -1.0412e+03 - logprior: -3.0794e+00
Epoch 9/10
40/40 - 44s - loss: 1047.2264 - loglik: -1.0413e+03 - logprior: -3.1114e+00
Epoch 10/10
40/40 - 44s - loss: 1045.1895 - loglik: -1.0394e+03 - logprior: -3.1583e+00
Fitted a model with MAP estimate = -807.5895
expansions: [(113, 1), (330, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 123 162 163 164 180 181 182 183 184 185 186 187 188 212
 213 214 215 216 217 238 246 247 248 249 250 251 252 253 254 255 256 257
 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275
 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293
 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 22s - loss: 1328.1799 - loglik: -1.3246e+03 - logprior: -3.0521e+00
Epoch 2/2
40/40 - 17s - loss: 1254.2610 - loglik: -1.2512e+03 - logprior: -7.0753e-01
Fitted a model with MAP estimate = -905.3600
expansions: [(0, 318), (12, 1), (16, 1), (17, 1), (18, 5), (19, 5), (50, 3), (56, 1), (58, 2), (62, 2), (63, 7), (64, 2), (65, 5), (108, 1)]
discards: [51 52 53 54 94 95 98 99]
Re-initialized the encoder parameters.
Fitting a model of length 462 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 83s - loss: 1085.5380 - loglik: -1.0828e+03 - logprior: -1.4400e+00
Epoch 2/2
80/80 - 80s - loss: 1012.6743 - loglik: -1.0090e+03 - logprior: -5.6481e-01
Fitted a model with MAP estimate = -750.7767
expansions: [(266, 1), (282, 1), (289, 1), (301, 1), (302, 1), (307, 1), (406, 1), (409, 1), (442, 2), (443, 1)]
discards: [  0  22  23  24  83  90  91 164 165 239 337 342 344 387 388 389 390 391
 392 393 398]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 113s - loss: 766.2028 - loglik: -7.6348e+02 - logprior: -7.6792e-01
Epoch 2/10
113/113 - 108s - loss: 747.0945 - loglik: -7.4165e+02 - logprior: -4.1022e-01
Epoch 3/10
113/113 - 108s - loss: 742.2808 - loglik: -7.3735e+02 - logprior: -2.9718e-01
Epoch 4/10
113/113 - 109s - loss: 743.4081 - loglik: -7.3877e+02 - logprior: -2.5238e-01
Fitted a model with MAP estimate = -734.0281
Time for alignment: 1436.1624
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1240.2990 - loglik: -1.2388e+03 - logprior: -1.0529e+00
Epoch 2/10
40/40 - 44s - loss: 1121.7198 - loglik: -1.1201e+03 - logprior: -6.9581e-02
Epoch 3/10
40/40 - 44s - loss: 1105.5131 - loglik: -1.1028e+03 - logprior: -7.3801e-02
Epoch 4/10
40/40 - 44s - loss: 1097.7673 - loglik: -1.0944e+03 - logprior: -1.3119e-01
Epoch 5/10
40/40 - 44s - loss: 1094.4639 - loglik: -1.0908e+03 - logprior: -2.0893e-01
Epoch 6/10
40/40 - 44s - loss: 1090.9381 - loglik: -1.0874e+03 - logprior: -2.7745e-01
Epoch 7/10
40/40 - 44s - loss: 1089.4851 - loglik: -1.0862e+03 - logprior: -3.0705e-01
Epoch 8/10
40/40 - 44s - loss: 1088.4395 - loglik: -1.0855e+03 - logprior: -3.5395e-01
Epoch 9/10
40/40 - 44s - loss: 1086.7209 - loglik: -1.0841e+03 - logprior: -3.6845e-01
Epoch 10/10
40/40 - 44s - loss: 1085.4720 - loglik: -1.0831e+03 - logprior: -3.9016e-01
Fitted a model with MAP estimate = -828.4274
expansions: [(18, 2), (42, 2), (118, 2), (125, 1), (138, 1), (201, 1), (289, 2), (290, 2), (291, 8), (292, 4), (315, 2), (316, 2), (317, 3), (329, 10), (330, 88)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 83s - loss: 1087.8330 - loglik: -1.0843e+03 - logprior: -1.4580e+00
Epoch 2/2
80/80 - 79s - loss: 1008.9601 - loglik: -1.0033e+03 - logprior: -3.3804e-01
Fitted a model with MAP estimate = -747.2544
expansions: [(77, 1), (164, 1), (250, 2), (447, 2), (459, 2)]
discards: [ 43  78 121 246 300 301 307 340 344 387 388]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 82s - loss: 1045.2584 - loglik: -1.0444e+03 - logprior: -3.7239e-01
Epoch 2/2
80/80 - 78s - loss: 1011.4794 - loglik: -1.0106e+03 - logprior: 0.3615
Fitted a model with MAP estimate = -754.7389
expansions: []
discards: [381 454 455]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 112s - loss: 764.5350 - loglik: -7.6273e+02 - logprior: -1.5716e-01
Epoch 2/10
113/113 - 108s - loss: 748.1531 - loglik: -7.4353e+02 - logprior: 0.0919
Epoch 3/10
113/113 - 109s - loss: 743.1782 - loglik: -7.3932e+02 - logprior: 0.4086
Epoch 4/10
113/113 - 109s - loss: 740.6508 - loglik: -7.3732e+02 - logprior: 0.7061
Epoch 5/10
113/113 - 108s - loss: 740.5027 - loglik: -7.3735e+02 - logprior: 0.7963
Epoch 6/10
113/113 - 109s - loss: 733.6639 - loglik: -7.3061e+02 - logprior: 1.0516
Epoch 7/10
113/113 - 109s - loss: 729.2293 - loglik: -7.2705e+02 - logprior: 1.1794
Epoch 8/10
113/113 - 109s - loss: 730.6715 - loglik: -7.2939e+02 - logprior: 1.5870
Fitted a model with MAP estimate = -724.1572
Time for alignment: 2107.1483
Computed alignments with likelihoods: ['-734.2594', '-734.0281', '-724.1572']
Best model has likelihood: -724.1572
time for generating output: 0.5158
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.6694632942628007
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faad241d5e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5df240a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa8e05dddf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8e0741b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2a9b70d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa8e8060970>, <__main__.SimpleDirichletPrior object at 0x7fac3bad3430>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.7022 - loglik: -4.4678e+02 - logprior: -2.8899e+00
Epoch 2/10
19/19 - 5s - loss: 284.5962 - loglik: -2.8308e+02 - logprior: -1.3325e+00
Epoch 3/10
19/19 - 5s - loss: 214.6381 - loglik: -2.1282e+02 - logprior: -1.7273e+00
Epoch 4/10
19/19 - 5s - loss: 201.0904 - loglik: -1.9887e+02 - logprior: -1.9868e+00
Epoch 5/10
19/19 - 5s - loss: 196.0436 - loglik: -1.9341e+02 - logprior: -2.2309e+00
Epoch 6/10
19/19 - 5s - loss: 192.7643 - loglik: -1.9010e+02 - logprior: -2.2805e+00
Epoch 7/10
19/19 - 5s - loss: 191.8776 - loglik: -1.8928e+02 - logprior: -2.2243e+00
Epoch 8/10
19/19 - 5s - loss: 190.5753 - loglik: -1.8802e+02 - logprior: -2.2068e+00
Epoch 9/10
19/19 - 5s - loss: 190.6398 - loglik: -1.8809e+02 - logprior: -2.2018e+00
Fitted a model with MAP estimate = -184.8695
expansions: [(7, 1), (8, 2), (12, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (111, 2), (114, 2), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 180.6298 - loglik: -1.7764e+02 - logprior: -2.8062e+00
Epoch 2/2
19/19 - 6s - loss: 144.8968 - loglik: -1.4384e+02 - logprior: -9.4602e-01
Fitted a model with MAP estimate = -143.2880
expansions: []
discards: [ 50  76 138 143 154]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 145.9245 - loglik: -1.4303e+02 - logprior: -2.6750e+00
Epoch 2/2
19/19 - 6s - loss: 141.2879 - loglik: -1.4035e+02 - logprior: -7.6697e-01
Fitted a model with MAP estimate = -141.7179
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 145.1330 - loglik: -1.4322e+02 - logprior: -1.8941e+00
Epoch 2/10
22/22 - 7s - loss: 139.3434 - loglik: -1.3853e+02 - logprior: -7.0513e-01
Epoch 3/10
22/22 - 7s - loss: 137.9196 - loglik: -1.3691e+02 - logprior: -7.4445e-01
Epoch 4/10
22/22 - 7s - loss: 133.2434 - loglik: -1.3185e+02 - logprior: -1.0167e+00
Epoch 5/10
22/22 - 7s - loss: 130.5757 - loglik: -1.2910e+02 - logprior: -1.0196e+00
Epoch 6/10
22/22 - 7s - loss: 128.6686 - loglik: -1.2727e+02 - logprior: -9.8420e-01
Epoch 7/10
22/22 - 7s - loss: 127.0505 - loglik: -1.2571e+02 - logprior: -9.4683e-01
Epoch 8/10
22/22 - 7s - loss: 126.4920 - loglik: -1.2519e+02 - logprior: -9.2365e-01
Epoch 9/10
22/22 - 7s - loss: 127.2842 - loglik: -1.2603e+02 - logprior: -8.8572e-01
Fitted a model with MAP estimate = -125.9053
Time for alignment: 189.8644
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.1157 - loglik: -4.4608e+02 - logprior: -2.9027e+00
Epoch 2/10
19/19 - 5s - loss: 280.7853 - loglik: -2.7914e+02 - logprior: -1.3352e+00
Epoch 3/10
19/19 - 5s - loss: 212.3350 - loglik: -2.1053e+02 - logprior: -1.7249e+00
Epoch 4/10
19/19 - 5s - loss: 200.3082 - loglik: -1.9815e+02 - logprior: -1.9559e+00
Epoch 5/10
19/19 - 5s - loss: 193.6315 - loglik: -1.9116e+02 - logprior: -2.0233e+00
Epoch 6/10
19/19 - 5s - loss: 191.9067 - loglik: -1.8934e+02 - logprior: -2.1183e+00
Epoch 7/10
19/19 - 5s - loss: 190.3642 - loglik: -1.8791e+02 - logprior: -2.0563e+00
Epoch 8/10
19/19 - 5s - loss: 190.9100 - loglik: -1.8849e+02 - logprior: -2.0593e+00
Fitted a model with MAP estimate = -183.7226
expansions: [(0, 2), (6, 1), (7, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 1), (61, 1), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (111, 2), (114, 2), (121, 1), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 182.2402 - loglik: -1.7826e+02 - logprior: -3.9416e+00
Epoch 2/2
19/19 - 6s - loss: 143.1916 - loglik: -1.4188e+02 - logprior: -1.1517e+00
Fitted a model with MAP estimate = -142.0267
expansions: []
discards: [  0  51 138 143 154 159]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 149.1535 - loglik: -1.4532e+02 - logprior: -3.7511e+00
Epoch 2/2
19/19 - 6s - loss: 142.4577 - loglik: -1.4106e+02 - logprior: -1.1513e+00
Fitted a model with MAP estimate = -141.6620
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 144.2145 - loglik: -1.4222e+02 - logprior: -1.9404e+00
Epoch 2/10
22/22 - 7s - loss: 140.3954 - loglik: -1.3945e+02 - logprior: -7.4499e-01
Epoch 3/10
22/22 - 7s - loss: 135.9165 - loglik: -1.3491e+02 - logprior: -7.6042e-01
Epoch 4/10
22/22 - 7s - loss: 135.5044 - loglik: -1.3418e+02 - logprior: -1.0003e+00
Epoch 5/10
22/22 - 7s - loss: 130.6369 - loglik: -1.2925e+02 - logprior: -1.0138e+00
Epoch 6/10
22/22 - 7s - loss: 128.6118 - loglik: -1.2722e+02 - logprior: -9.9662e-01
Epoch 7/10
22/22 - 7s - loss: 127.5481 - loglik: -1.2620e+02 - logprior: -9.4282e-01
Epoch 8/10
22/22 - 7s - loss: 126.7183 - loglik: -1.2542e+02 - logprior: -9.0692e-01
Epoch 9/10
22/22 - 7s - loss: 126.6644 - loglik: -1.2541e+02 - logprior: -8.8264e-01
Epoch 10/10
22/22 - 7s - loss: 125.3194 - loglik: -1.2413e+02 - logprior: -8.3541e-01
Fitted a model with MAP estimate = -125.7853
Time for alignment: 191.6262
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 449.1850 - loglik: -4.4619e+02 - logprior: -2.8966e+00
Epoch 2/10
19/19 - 5s - loss: 280.9500 - loglik: -2.7935e+02 - logprior: -1.3311e+00
Epoch 3/10
19/19 - 5s - loss: 213.4813 - loglik: -2.1167e+02 - logprior: -1.7204e+00
Epoch 4/10
19/19 - 5s - loss: 200.9420 - loglik: -1.9878e+02 - logprior: -1.9848e+00
Epoch 5/10
19/19 - 5s - loss: 195.6980 - loglik: -1.9302e+02 - logprior: -2.2338e+00
Epoch 6/10
19/19 - 5s - loss: 192.3520 - loglik: -1.8958e+02 - logprior: -2.3053e+00
Epoch 7/10
19/19 - 5s - loss: 192.2185 - loglik: -1.8954e+02 - logprior: -2.2548e+00
Epoch 8/10
19/19 - 5s - loss: 190.0265 - loglik: -1.8741e+02 - logprior: -2.2348e+00
Epoch 9/10
19/19 - 5s - loss: 190.1640 - loglik: -1.8757e+02 - logprior: -2.2310e+00
Fitted a model with MAP estimate = -184.9921
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (111, 2), (114, 1), (120, 2), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 180.9070 - loglik: -1.7806e+02 - logprior: -2.7557e+00
Epoch 2/2
19/19 - 6s - loss: 145.5147 - loglik: -1.4434e+02 - logprior: -9.8315e-01
Fitted a model with MAP estimate = -142.7913
expansions: []
discards: [ 50  76 138 150 155]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 146.5902 - loglik: -1.4384e+02 - logprior: -2.6752e+00
Epoch 2/2
19/19 - 6s - loss: 141.3422 - loglik: -1.4042e+02 - logprior: -7.5981e-01
Fitted a model with MAP estimate = -141.2065
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 143.7321 - loglik: -1.4171e+02 - logprior: -1.8892e+00
Epoch 2/10
22/22 - 7s - loss: 141.5803 - loglik: -1.4064e+02 - logprior: -7.8716e-01
Epoch 3/10
22/22 - 7s - loss: 135.1996 - loglik: -1.3405e+02 - logprior: -9.5980e-01
Epoch 4/10
22/22 - 7s - loss: 133.3514 - loglik: -1.3198e+02 - logprior: -1.0478e+00
Epoch 5/10
22/22 - 7s - loss: 131.6845 - loglik: -1.3022e+02 - logprior: -1.0278e+00
Epoch 6/10
22/22 - 7s - loss: 127.7732 - loglik: -1.2634e+02 - logprior: -9.8131e-01
Epoch 7/10
22/22 - 7s - loss: 127.0976 - loglik: -1.2573e+02 - logprior: -9.3783e-01
Epoch 8/10
22/22 - 7s - loss: 128.0760 - loglik: -1.2677e+02 - logprior: -9.0717e-01
Fitted a model with MAP estimate = -126.0525
Time for alignment: 182.7159
Computed alignments with likelihoods: ['-125.9053', '-125.7853', '-126.0525']
Best model has likelihood: -125.7853
time for generating output: 0.2065
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa8d64d8e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2b5569f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa2b4d0b400>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2c45ebac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86c7d8310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabf7b1ee20>, <__main__.SimpleDirichletPrior object at 0x7fac110b2160>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 607.4568 - loglik: -5.7516e+02 - logprior: -3.2212e+01
Epoch 2/10
12/12 - 4s - loss: 524.8399 - loglik: -5.2119e+02 - logprior: -3.5261e+00
Epoch 3/10
12/12 - 4s - loss: 467.2616 - loglik: -4.6611e+02 - logprior: -8.3258e-01
Epoch 4/10
12/12 - 4s - loss: 437.2834 - loglik: -4.3675e+02 - logprior: -2.6578e-02
Epoch 5/10
12/12 - 4s - loss: 426.6819 - loglik: -4.2640e+02 - logprior: 0.2261
Epoch 6/10
12/12 - 4s - loss: 424.8434 - loglik: -4.2498e+02 - logprior: 0.5668
Epoch 7/10
12/12 - 4s - loss: 423.9422 - loglik: -4.2432e+02 - logprior: 0.7851
Epoch 8/10
12/12 - 4s - loss: 421.1864 - loglik: -4.2169e+02 - logprior: 0.9094
Epoch 9/10
12/12 - 4s - loss: 421.5715 - loglik: -4.2218e+02 - logprior: 1.0194
Fitted a model with MAP estimate = -420.5586
expansions: [(11, 3), (19, 1), (30, 1), (31, 2), (39, 1), (41, 1), (44, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (90, 1), (92, 1), (101, 1), (102, 2), (126, 7), (129, 1), (130, 1), (145, 1), (146, 2), (148, 2), (153, 2), (173, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 460.0407 - loglik: -4.2264e+02 - logprior: -3.7351e+01
Epoch 2/2
12/12 - 4s - loss: 421.6342 - loglik: -4.1054e+02 - logprior: -1.1047e+01
Fitted a model with MAP estimate = -413.1159
expansions: [(0, 4)]
discards: [  0 127 152 153 186]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 436.9388 - loglik: -4.0946e+02 - logprior: -2.7375e+01
Epoch 2/2
12/12 - 4s - loss: 407.8674 - loglik: -4.0645e+02 - logprior: -1.3968e+00
Fitted a model with MAP estimate = -402.4985
expansions: [(108, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 432.1779 - loglik: -4.0555e+02 - logprior: -2.6544e+01
Epoch 2/10
12/12 - 4s - loss: 401.9491 - loglik: -4.0077e+02 - logprior: -8.7622e-01
Epoch 3/10
12/12 - 4s - loss: 397.2632 - loglik: -4.0053e+02 - logprior: 3.7943
Epoch 4/10
12/12 - 4s - loss: 393.9072 - loglik: -3.9913e+02 - logprior: 5.8040
Epoch 5/10
12/12 - 4s - loss: 390.6359 - loglik: -3.9696e+02 - logprior: 6.8621
Epoch 6/10
12/12 - 4s - loss: 391.3523 - loglik: -3.9837e+02 - logprior: 7.4792
Fitted a model with MAP estimate = -389.7981
Time for alignment: 105.0995
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 608.0660 - loglik: -5.7577e+02 - logprior: -3.2231e+01
Epoch 2/10
12/12 - 4s - loss: 522.4389 - loglik: -5.1883e+02 - logprior: -3.5310e+00
Epoch 3/10
12/12 - 4s - loss: 464.5941 - loglik: -4.6343e+02 - logprior: -8.9256e-01
Epoch 4/10
12/12 - 4s - loss: 434.0253 - loglik: -4.3324e+02 - logprior: -2.9203e-01
Epoch 5/10
12/12 - 4s - loss: 425.5103 - loglik: -4.2512e+02 - logprior: 0.1220
Epoch 6/10
12/12 - 4s - loss: 421.9260 - loglik: -4.2189e+02 - logprior: 0.4378
Epoch 7/10
12/12 - 4s - loss: 420.2030 - loglik: -4.2035e+02 - logprior: 0.5803
Epoch 8/10
12/12 - 4s - loss: 418.7475 - loglik: -4.1907e+02 - logprior: 0.7453
Epoch 9/10
12/12 - 4s - loss: 418.7248 - loglik: -4.1920e+02 - logprior: 0.8946
Epoch 10/10
12/12 - 4s - loss: 416.6109 - loglik: -4.1716e+02 - logprior: 0.9779
Fitted a model with MAP estimate = -416.3867
expansions: [(11, 3), (25, 1), (31, 4), (32, 2), (45, 1), (48, 1), (60, 2), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (87, 1), (90, 2), (92, 1), (100, 1), (101, 1), (118, 1), (126, 5), (127, 1), (129, 2), (130, 2), (137, 1), (145, 1), (146, 3), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 457.5318 - loglik: -4.2031e+02 - logprior: -3.7178e+01
Epoch 2/2
12/12 - 4s - loss: 414.7154 - loglik: -4.0347e+02 - logprior: -1.1233e+01
Fitted a model with MAP estimate = -408.6785
expansions: [(0, 4), (190, 1)]
discards: [  0  36  39  72 131 156 157 158 167 191 192 194 223]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 432.4728 - loglik: -4.0499e+02 - logprior: -2.7345e+01
Epoch 2/2
12/12 - 4s - loss: 401.7675 - loglik: -4.0019e+02 - logprior: -1.3448e+00
Fitted a model with MAP estimate = -396.3719
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 428.5274 - loglik: -4.0201e+02 - logprior: -2.6504e+01
Epoch 2/10
12/12 - 4s - loss: 401.4041 - loglik: -4.0052e+02 - logprior: -7.6480e-01
Epoch 3/10
12/12 - 4s - loss: 394.6604 - loglik: -3.9832e+02 - logprior: 3.9768
Epoch 4/10
12/12 - 4s - loss: 392.2431 - loglik: -3.9775e+02 - logprior: 5.9720
Epoch 5/10
12/12 - 4s - loss: 390.1005 - loglik: -3.9660e+02 - logprior: 7.0305
Epoch 6/10
12/12 - 4s - loss: 389.0565 - loglik: -3.9615e+02 - logprior: 7.6072
Epoch 7/10
12/12 - 4s - loss: 388.2852 - loglik: -3.9588e+02 - logprior: 8.0615
Epoch 8/10
12/12 - 4s - loss: 387.7035 - loglik: -3.9573e+02 - logprior: 8.4549
Epoch 9/10
12/12 - 4s - loss: 387.8707 - loglik: -3.9626e+02 - logprior: 8.8108
Fitted a model with MAP estimate = -386.4936
Time for alignment: 121.2855
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 608.3990 - loglik: -5.7585e+02 - logprior: -3.2238e+01
Epoch 2/10
12/12 - 4s - loss: 522.9384 - loglik: -5.1924e+02 - logprior: -3.5310e+00
Epoch 3/10
12/12 - 4s - loss: 457.8335 - loglik: -4.5671e+02 - logprior: -8.5665e-01
Epoch 4/10
12/12 - 4s - loss: 434.5816 - loglik: -4.3395e+02 - logprior: -1.5503e-01
Epoch 5/10
12/12 - 4s - loss: 424.0462 - loglik: -4.2367e+02 - logprior: 0.2235
Epoch 6/10
12/12 - 4s - loss: 421.6189 - loglik: -4.2159e+02 - logprior: 0.5006
Epoch 7/10
12/12 - 4s - loss: 419.3833 - loglik: -4.1955e+02 - logprior: 0.6195
Epoch 8/10
12/12 - 4s - loss: 418.5453 - loglik: -4.1890e+02 - logprior: 0.7808
Epoch 9/10
12/12 - 4s - loss: 417.6975 - loglik: -4.1815e+02 - logprior: 0.8824
Epoch 10/10
12/12 - 4s - loss: 419.0508 - loglik: -4.1961e+02 - logprior: 1.0168
Fitted a model with MAP estimate = -416.5962
expansions: [(11, 3), (19, 1), (30, 1), (31, 3), (42, 1), (45, 1), (61, 1), (62, 1), (63, 1), (64, 1), (76, 3), (77, 1), (78, 1), (87, 1), (88, 1), (90, 2), (92, 1), (102, 1), (127, 5), (128, 1), (130, 2), (131, 2), (138, 2), (139, 2), (146, 3), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 455.4176 - loglik: -4.1786e+02 - logprior: -3.7411e+01
Epoch 2/2
12/12 - 4s - loss: 415.6819 - loglik: -4.0386e+02 - logprior: -1.1518e+01
Fitted a model with MAP estimate = -407.7639
expansions: [(0, 4), (112, 1), (186, 1)]
discards: [  0  36 151 152 153 162 175 190 219]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 431.8960 - loglik: -4.0453e+02 - logprior: -2.7332e+01
Epoch 2/2
12/12 - 4s - loss: 399.9953 - loglik: -3.9867e+02 - logprior: -1.1609e+00
Fitted a model with MAP estimate = -395.9455
expansions: []
discards: [  1   2   3 186]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 427.9257 - loglik: -4.0133e+02 - logprior: -2.6460e+01
Epoch 2/10
12/12 - 4s - loss: 399.4287 - loglik: -3.9826e+02 - logprior: -8.4030e-01
Epoch 3/10
12/12 - 4s - loss: 392.7284 - loglik: -3.9613e+02 - logprior: 3.9101
Epoch 4/10
12/12 - 4s - loss: 392.6699 - loglik: -3.9806e+02 - logprior: 5.9641
Epoch 5/10
12/12 - 4s - loss: 389.0786 - loglik: -3.9559e+02 - logprior: 7.0571
Epoch 6/10
12/12 - 4s - loss: 388.5237 - loglik: -3.9571e+02 - logprior: 7.6886
Epoch 7/10
12/12 - 4s - loss: 387.4460 - loglik: -3.9511e+02 - logprior: 8.1377
Epoch 8/10
12/12 - 4s - loss: 387.5599 - loglik: -3.9562e+02 - logprior: 8.5238
Fitted a model with MAP estimate = -386.3345
Time for alignment: 114.6231
Computed alignments with likelihoods: ['-389.7981', '-386.4936', '-386.3345']
Best model has likelihood: -386.3345
time for generating output: 0.2581
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9352317520728558
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa8f046ffd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabd5aafe80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faac9a66190>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabd5cd8e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86d4a1190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faad2476550>, <__main__.SimpleDirichletPrior object at 0x7fab0d585730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 605.7369 - loglik: -5.9823e+02 - logprior: -7.3206e+00
Epoch 2/10
21/21 - 8s - loss: 478.0267 - loglik: -4.7584e+02 - logprior: -1.8133e+00
Epoch 3/10
21/21 - 8s - loss: 437.1967 - loglik: -4.3415e+02 - logprior: -2.3389e+00
Epoch 4/10
21/21 - 9s - loss: 431.0764 - loglik: -4.2827e+02 - logprior: -2.1928e+00
Epoch 5/10
21/21 - 8s - loss: 428.7994 - loglik: -4.2613e+02 - logprior: -2.1042e+00
Epoch 6/10
21/21 - 8s - loss: 428.2430 - loglik: -4.2554e+02 - logprior: -2.1230e+00
Epoch 7/10
21/21 - 8s - loss: 426.5352 - loglik: -4.2385e+02 - logprior: -2.1080e+00
Epoch 8/10
21/21 - 9s - loss: 426.0501 - loglik: -4.2333e+02 - logprior: -2.1574e+00
Epoch 9/10
21/21 - 8s - loss: 425.5405 - loglik: -4.2279e+02 - logprior: -2.1997e+00
Epoch 10/10
21/21 - 9s - loss: 425.1003 - loglik: -4.2235e+02 - logprior: -2.1984e+00
Fitted a model with MAP estimate = -424.5266
expansions: [(16, 2), (17, 1), (20, 3), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (77, 1), (79, 2), (97, 1), (100, 2), (119, 2), (120, 1), (121, 4), (143, 1), (146, 1), (147, 1), (150, 1), (158, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 245 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 420.4584 - loglik: -4.1326e+02 - logprior: -7.0233e+00
Epoch 2/2
21/21 - 11s - loss: 395.3039 - loglik: -3.9407e+02 - logprior: -7.5237e-01
Fitted a model with MAP estimate = -389.0913
expansions: []
discards: [ 17  24  28  81  86 100 101 108 134 154 159 221]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 406.5860 - loglik: -4.0004e+02 - logprior: -6.4416e+00
Epoch 2/2
21/21 - 10s - loss: 394.5119 - loglik: -3.9429e+02 - logprior: 0.1163
Fitted a model with MAP estimate = -391.2896
expansions: []
discards: [101]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 405.8945 - loglik: -3.9966e+02 - logprior: -6.0766e+00
Epoch 2/10
21/21 - 10s - loss: 394.3492 - loglik: -3.9464e+02 - logprior: 0.5849
Epoch 3/10
21/21 - 10s - loss: 391.0233 - loglik: -3.9191e+02 - logprior: 1.2805
Epoch 4/10
21/21 - 10s - loss: 390.4089 - loglik: -3.9154e+02 - logprior: 1.6202
Epoch 5/10
21/21 - 9s - loss: 387.6202 - loglik: -3.8879e+02 - logprior: 1.7142
Epoch 6/10
21/21 - 10s - loss: 388.2804 - loglik: -3.8951e+02 - logprior: 1.7997
Fitted a model with MAP estimate = -386.3625
Time for alignment: 232.2737
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 605.0054 - loglik: -5.9760e+02 - logprior: -7.3408e+00
Epoch 2/10
21/21 - 9s - loss: 479.0174 - loglik: -4.7662e+02 - logprior: -1.9663e+00
Epoch 3/10
21/21 - 8s - loss: 440.6741 - loglik: -4.3712e+02 - logprior: -2.9057e+00
Epoch 4/10
21/21 - 9s - loss: 434.7301 - loglik: -4.3153e+02 - logprior: -2.6595e+00
Epoch 5/10
21/21 - 8s - loss: 430.8537 - loglik: -4.2786e+02 - logprior: -2.5058e+00
Epoch 6/10
21/21 - 9s - loss: 431.0373 - loglik: -4.2805e+02 - logprior: -2.5141e+00
Fitted a model with MAP estimate = -429.5347
expansions: [(7, 1), (16, 1), (17, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (50, 1), (64, 4), (66, 1), (75, 8), (78, 1), (97, 1), (98, 1), (100, 1), (118, 2), (119, 1), (120, 1), (121, 1), (146, 1), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 432.4766 - loglik: -4.2235e+02 - logprior: -1.0000e+01
Epoch 2/2
21/21 - 10s - loss: 406.3898 - loglik: -4.0282e+02 - logprior: -3.2846e+00
Fitted a model with MAP estimate = -399.9573
expansions: []
discards: [  0  24  49  51  82  86 152 216]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 419.8221 - loglik: -4.1020e+02 - logprior: -9.5908e+00
Epoch 2/2
21/21 - 10s - loss: 401.6629 - loglik: -3.9997e+02 - logprior: -1.4844e+00
Fitted a model with MAP estimate = -398.5292
expansions: [(0, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 408.9749 - loglik: -4.0251e+02 - logprior: -6.3338e+00
Epoch 2/10
21/21 - 11s - loss: 396.4428 - loglik: -3.9621e+02 - logprior: 0.1329
Epoch 3/10
21/21 - 10s - loss: 394.6567 - loglik: -3.9478e+02 - logprior: 0.6271
Epoch 4/10
21/21 - 10s - loss: 392.8515 - loglik: -3.9333e+02 - logprior: 1.0019
Epoch 5/10
21/21 - 10s - loss: 390.1341 - loglik: -3.9077e+02 - logprior: 1.1525
Epoch 6/10
21/21 - 10s - loss: 390.1471 - loglik: -3.9099e+02 - logprior: 1.3765
Fitted a model with MAP estimate = -389.2685
Time for alignment: 203.0874
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 606.7687 - loglik: -5.9922e+02 - logprior: -7.2745e+00
Epoch 2/10
21/21 - 8s - loss: 481.5905 - loglik: -4.7922e+02 - logprior: -1.9278e+00
Epoch 3/10
21/21 - 9s - loss: 440.9697 - loglik: -4.3741e+02 - logprior: -2.8914e+00
Epoch 4/10
21/21 - 8s - loss: 432.5081 - loglik: -4.2927e+02 - logprior: -2.7030e+00
Epoch 5/10
21/21 - 9s - loss: 430.8700 - loglik: -4.2771e+02 - logprior: -2.6257e+00
Epoch 6/10
21/21 - 8s - loss: 429.3671 - loglik: -4.2626e+02 - logprior: -2.6028e+00
Epoch 7/10
21/21 - 9s - loss: 428.9578 - loglik: -4.2586e+02 - logprior: -2.5999e+00
Epoch 8/10
21/21 - 8s - loss: 428.1893 - loglik: -4.2511e+02 - logprior: -2.5843e+00
Epoch 9/10
21/21 - 8s - loss: 428.1369 - loglik: -4.2506e+02 - logprior: -2.5746e+00
Epoch 10/10
21/21 - 9s - loss: 428.4849 - loglik: -4.2540e+02 - logprior: -2.5724e+00
Fitted a model with MAP estimate = -427.0735
expansions: [(16, 1), (17, 1), (19, 2), (20, 2), (21, 4), (23, 1), (36, 1), (37, 1), (38, 1), (39, 1), (45, 1), (48, 1), (62, 3), (65, 1), (70, 1), (75, 3), (76, 1), (78, 2), (97, 1), (100, 2), (119, 2), (120, 1), (121, 1), (122, 1), (147, 1), (148, 2), (155, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 430.3294 - loglik: -4.2027e+02 - logprior: -9.9936e+00
Epoch 2/2
21/21 - 10s - loss: 404.1396 - loglik: -4.0059e+02 - logprior: -3.2806e+00
Fitted a model with MAP estimate = -398.9599
expansions: [(0, 5), (13, 1), (98, 1)]
discards: [  0  21  80 130 150 184 215]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 409.4860 - loglik: -4.0295e+02 - logprior: -6.4779e+00
Epoch 2/2
21/21 - 10s - loss: 393.7892 - loglik: -3.9343e+02 - logprior: -7.2510e-02
Fitted a model with MAP estimate = -391.1100
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 403.8377 - loglik: -3.9757e+02 - logprior: -6.0188e+00
Epoch 2/10
21/21 - 10s - loss: 391.8928 - loglik: -3.9217e+02 - logprior: 0.6214
Epoch 3/10
21/21 - 11s - loss: 391.0617 - loglik: -3.9190e+02 - logprior: 1.3249
Epoch 4/10
21/21 - 10s - loss: 388.3882 - loglik: -3.8957e+02 - logprior: 1.7021
Epoch 5/10
21/21 - 10s - loss: 387.6320 - loglik: -3.8899e+02 - logprior: 1.8731
Epoch 6/10
21/21 - 10s - loss: 386.7053 - loglik: -3.8819e+02 - logprior: 2.0002
Epoch 7/10
21/21 - 10s - loss: 386.2006 - loglik: -3.8776e+02 - logprior: 2.1008
Epoch 8/10
21/21 - 10s - loss: 386.1103 - loglik: -3.8775e+02 - logprior: 2.2242
Epoch 9/10
21/21 - 10s - loss: 385.8068 - loglik: -3.8765e+02 - logprior: 2.4288
Epoch 10/10
21/21 - 10s - loss: 383.1932 - loglik: -3.8522e+02 - logprior: 2.6121
Fitted a model with MAP estimate = -383.6741
Time for alignment: 274.3482
Computed alignments with likelihoods: ['-386.3625', '-389.2685', '-383.6741']
Best model has likelihood: -383.6741
time for generating output: 0.3548
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5787045754095274
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac2aaa5cd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86c0ddd30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac660dfe80>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1ed48b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba2770a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faac9a66190>, <__main__.SimpleDirichletPrior object at 0x7fac665af2e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.4864 - loglik: -2.8038e+02 - logprior: -3.0153e+00
Epoch 2/10
19/19 - 3s - loss: 254.2180 - loglik: -2.5292e+02 - logprior: -9.0315e-01
Epoch 3/10
19/19 - 3s - loss: 241.2213 - loglik: -2.3946e+02 - logprior: -9.5935e-01
Epoch 4/10
19/19 - 3s - loss: 238.0177 - loglik: -2.3617e+02 - logprior: -8.8586e-01
Epoch 5/10
19/19 - 3s - loss: 236.8631 - loglik: -2.3508e+02 - logprior: -8.6244e-01
Epoch 6/10
19/19 - 3s - loss: 235.5891 - loglik: -2.3392e+02 - logprior: -8.3659e-01
Epoch 7/10
19/19 - 3s - loss: 235.4146 - loglik: -2.3388e+02 - logprior: -8.1978e-01
Epoch 8/10
19/19 - 3s - loss: 234.9396 - loglik: -2.3345e+02 - logprior: -8.0475e-01
Epoch 9/10
19/19 - 3s - loss: 234.0479 - loglik: -2.3257e+02 - logprior: -8.0522e-01
Epoch 10/10
19/19 - 3s - loss: 233.5957 - loglik: -2.3208e+02 - logprior: -8.1938e-01
Fitted a model with MAP estimate = -231.8173
expansions: [(0, 11), (12, 1), (13, 3), (20, 1), (31, 2), (55, 1), (57, 2), (58, 2), (60, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 262.3317 - loglik: -2.5795e+02 - logprior: -3.9842e+00
Epoch 2/2
19/19 - 4s - loss: 240.1667 - loglik: -2.3812e+02 - logprior: -1.2618e+00
Fitted a model with MAP estimate = -234.6944
expansions: [(0, 9), (77, 1)]
discards: [10 11 12 13 14 15 16 17 18 19 20 24 48 78 83]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 246.0286 - loglik: -2.4216e+02 - logprior: -3.8500e+00
Epoch 2/2
19/19 - 4s - loss: 238.6943 - loglik: -2.3731e+02 - logprior: -1.2625e+00
Fitted a model with MAP estimate = -235.2232
expansions: [(0, 9), (20, 2)]
discards: [1 2 3 4 5 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.0092 - loglik: -2.3600e+02 - logprior: -3.6808e+00
Epoch 2/10
19/19 - 4s - loss: 234.6013 - loglik: -2.3257e+02 - logprior: -1.3310e+00
Epoch 3/10
19/19 - 4s - loss: 232.3533 - loglik: -2.3027e+02 - logprior: -1.0390e+00
Epoch 4/10
19/19 - 4s - loss: 230.8593 - loglik: -2.2874e+02 - logprior: -9.4203e-01
Epoch 5/10
19/19 - 4s - loss: 229.4575 - loglik: -2.2748e+02 - logprior: -8.7014e-01
Epoch 6/10
19/19 - 4s - loss: 228.4448 - loglik: -2.2660e+02 - logprior: -8.3480e-01
Epoch 7/10
19/19 - 3s - loss: 228.1252 - loglik: -2.2634e+02 - logprior: -8.3325e-01
Epoch 8/10
19/19 - 4s - loss: 227.0040 - loglik: -2.2528e+02 - logprior: -8.1861e-01
Epoch 9/10
19/19 - 4s - loss: 226.6297 - loglik: -2.2490e+02 - logprior: -8.1116e-01
Epoch 10/10
19/19 - 3s - loss: 225.7071 - loglik: -2.2396e+02 - logprior: -8.1302e-01
Fitted a model with MAP estimate = -224.4504
Time for alignment: 120.8353
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 283.4685 - loglik: -2.8019e+02 - logprior: -3.0153e+00
Epoch 2/10
19/19 - 3s - loss: 251.7902 - loglik: -2.5035e+02 - logprior: -8.8556e-01
Epoch 3/10
19/19 - 3s - loss: 241.0936 - loglik: -2.3920e+02 - logprior: -9.2536e-01
Epoch 4/10
19/19 - 3s - loss: 238.1409 - loglik: -2.3620e+02 - logprior: -8.4531e-01
Epoch 5/10
19/19 - 3s - loss: 236.4079 - loglik: -2.3458e+02 - logprior: -8.4925e-01
Epoch 6/10
19/19 - 3s - loss: 235.1033 - loglik: -2.3338e+02 - logprior: -8.3945e-01
Epoch 7/10
19/19 - 3s - loss: 235.1522 - loglik: -2.3352e+02 - logprior: -8.2988e-01
Fitted a model with MAP estimate = -232.5657
expansions: [(0, 11), (11, 3), (12, 3), (13, 1), (14, 1), (54, 1), (56, 2), (57, 1), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 258.3842 - loglik: -2.5443e+02 - logprior: -3.8836e+00
Epoch 2/2
19/19 - 4s - loss: 239.3573 - loglik: -2.3783e+02 - logprior: -1.1800e+00
Fitted a model with MAP estimate = -234.9049
expansions: [(0, 9)]
discards: [ 9 10 11 12 13 14 15 16 17 18 30]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 243.3686 - loglik: -2.3950e+02 - logprior: -3.7999e+00
Epoch 2/2
19/19 - 4s - loss: 237.2565 - loglik: -2.3565e+02 - logprior: -1.3251e+00
Fitted a model with MAP estimate = -233.8776
expansions: [(0, 6), (17, 1), (18, 1), (22, 1)]
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 242.3317 - loglik: -2.3865e+02 - logprior: -3.6444e+00
Epoch 2/10
19/19 - 4s - loss: 236.6385 - loglik: -2.3521e+02 - logprior: -1.2771e+00
Epoch 3/10
19/19 - 4s - loss: 233.9790 - loglik: -2.3216e+02 - logprior: -1.1347e+00
Epoch 4/10
19/19 - 4s - loss: 230.8951 - loglik: -2.2882e+02 - logprior: -9.7293e-01
Epoch 5/10
19/19 - 4s - loss: 230.1503 - loglik: -2.2823e+02 - logprior: -8.8220e-01
Epoch 6/10
19/19 - 4s - loss: 228.8079 - loglik: -2.2704e+02 - logprior: -8.6845e-01
Epoch 7/10
19/19 - 4s - loss: 228.1799 - loglik: -2.2652e+02 - logprior: -8.5388e-01
Epoch 8/10
19/19 - 4s - loss: 227.5555 - loglik: -2.2595e+02 - logprior: -8.5243e-01
Epoch 9/10
19/19 - 4s - loss: 227.0824 - loglik: -2.2548e+02 - logprior: -8.3550e-01
Epoch 10/10
19/19 - 4s - loss: 226.5128 - loglik: -2.2491e+02 - logprior: -8.1535e-01
Fitted a model with MAP estimate = -225.1117
Time for alignment: 111.3694
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 282.8953 - loglik: -2.7969e+02 - logprior: -3.0176e+00
Epoch 2/10
19/19 - 3s - loss: 251.5323 - loglik: -2.5000e+02 - logprior: -8.9391e-01
Epoch 3/10
19/19 - 3s - loss: 240.7769 - loglik: -2.3876e+02 - logprior: -9.4498e-01
Epoch 4/10
19/19 - 3s - loss: 237.9737 - loglik: -2.3596e+02 - logprior: -8.4939e-01
Epoch 5/10
19/19 - 3s - loss: 236.4476 - loglik: -2.3465e+02 - logprior: -8.2212e-01
Epoch 6/10
19/19 - 3s - loss: 235.9785 - loglik: -2.3433e+02 - logprior: -7.9873e-01
Epoch 7/10
19/19 - 3s - loss: 235.3234 - loglik: -2.3379e+02 - logprior: -7.8147e-01
Epoch 8/10
19/19 - 3s - loss: 234.7970 - loglik: -2.3328e+02 - logprior: -7.7959e-01
Epoch 9/10
19/19 - 3s - loss: 233.8533 - loglik: -2.3230e+02 - logprior: -7.7922e-01
Epoch 10/10
19/19 - 3s - loss: 233.7493 - loglik: -2.3217e+02 - logprior: -7.8234e-01
Fitted a model with MAP estimate = -231.2223
expansions: [(0, 11), (11, 3), (12, 2), (15, 2), (30, 2), (57, 4), (59, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 263.3715 - loglik: -2.5918e+02 - logprior: -3.9785e+00
Epoch 2/2
19/19 - 4s - loss: 240.0384 - loglik: -2.3836e+02 - logprior: -1.2222e+00
Fitted a model with MAP estimate = -234.7230
expansions: [(0, 10)]
discards: [ 9 10 11 12 13 14 15 16 17 18 23 31 49 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 244.1075 - loglik: -2.4017e+02 - logprior: -3.8146e+00
Epoch 2/2
19/19 - 4s - loss: 237.2118 - loglik: -2.3558e+02 - logprior: -1.3172e+00
Fitted a model with MAP estimate = -233.9235
expansions: [(0, 5), (23, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 80]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 239.8666 - loglik: -2.3622e+02 - logprior: -3.2814e+00
Epoch 2/10
19/19 - 4s - loss: 235.0849 - loglik: -2.3345e+02 - logprior: -1.0484e+00
Epoch 3/10
19/19 - 3s - loss: 232.7533 - loglik: -2.3092e+02 - logprior: -9.3846e-01
Epoch 4/10
19/19 - 4s - loss: 231.4064 - loglik: -2.2950e+02 - logprior: -8.1977e-01
Epoch 5/10
19/19 - 3s - loss: 229.8349 - loglik: -2.2795e+02 - logprior: -7.9862e-01
Epoch 6/10
19/19 - 4s - loss: 229.3066 - loglik: -2.2752e+02 - logprior: -7.8771e-01
Epoch 7/10
19/19 - 4s - loss: 228.2234 - loglik: -2.2650e+02 - logprior: -7.8189e-01
Epoch 8/10
19/19 - 3s - loss: 227.6918 - loglik: -2.2604e+02 - logprior: -7.5603e-01
Epoch 9/10
19/19 - 4s - loss: 227.1212 - loglik: -2.2549e+02 - logprior: -7.3875e-01
Epoch 10/10
19/19 - 4s - loss: 226.7616 - loglik: -2.2510e+02 - logprior: -7.3849e-01
Fitted a model with MAP estimate = -224.9443
Time for alignment: 118.2676
Computed alignments with likelihoods: ['-224.4504', '-225.1117', '-224.9443']
Best model has likelihood: -224.4504
time for generating output: 0.2523
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.5177894736842106
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaf42e1a60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaf3fb1820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa81aad6d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2ac787370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa95f2b610>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabe69c6970>, <__main__.SimpleDirichletPrior object at 0x7fabe6ab1220>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 730.5814 - loglik: -7.1862e+02 - logprior: -1.1942e+01
Epoch 2/10
17/17 - 8s - loss: 545.5795 - loglik: -5.4370e+02 - logprior: -1.8139e+00
Epoch 3/10
17/17 - 8s - loss: 448.1998 - loglik: -4.4450e+02 - logprior: -3.4851e+00
Epoch 4/10
17/17 - 8s - loss: 431.7888 - loglik: -4.2744e+02 - logprior: -3.8708e+00
Epoch 5/10
17/17 - 8s - loss: 425.3551 - loglik: -4.2134e+02 - logprior: -3.5123e+00
Epoch 6/10
17/17 - 8s - loss: 426.0262 - loglik: -4.2217e+02 - logprior: -3.3244e+00
Fitted a model with MAP estimate = -423.4499
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (24, 1), (26, 1), (32, 1), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (87, 4), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (181, 1), (182, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 431.0832 - loglik: -4.1496e+02 - logprior: -1.5979e+01
Epoch 2/2
17/17 - 10s - loss: 389.3704 - loglik: -3.8441e+02 - logprior: -4.6137e+00
Fitted a model with MAP estimate = -384.3259
expansions: [(0, 10)]
discards: [  0 104 105 142 157]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 17s - loss: 399.1913 - loglik: -3.8820e+02 - logprior: -1.0955e+01
Epoch 2/2
17/17 - 10s - loss: 380.9069 - loglik: -3.8090e+02 - logprior: 0.1806
Fitted a model with MAP estimate = -377.7852
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 394.2885 - loglik: -3.8410e+02 - logprior: -1.0026e+01
Epoch 2/10
17/17 - 10s - loss: 379.3019 - loglik: -3.8014e+02 - logprior: 1.1424
Epoch 3/10
17/17 - 10s - loss: 375.2977 - loglik: -3.7756e+02 - logprior: 2.6440
Epoch 4/10
17/17 - 10s - loss: 374.3777 - loglik: -3.7733e+02 - logprior: 3.3597
Epoch 5/10
17/17 - 10s - loss: 370.3148 - loglik: -3.7364e+02 - logprior: 3.7433
Epoch 6/10
17/17 - 9s - loss: 371.0273 - loglik: -3.7460e+02 - logprior: 4.0015
Fitted a model with MAP estimate = -368.9244
Time for alignment: 192.6846
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 732.2708 - loglik: -7.2027e+02 - logprior: -1.1943e+01
Epoch 2/10
17/17 - 8s - loss: 552.8614 - loglik: -5.5088e+02 - logprior: -1.9155e+00
Epoch 3/10
17/17 - 8s - loss: 455.8464 - loglik: -4.5212e+02 - logprior: -3.5553e+00
Epoch 4/10
17/17 - 8s - loss: 434.5399 - loglik: -4.3020e+02 - logprior: -3.9015e+00
Epoch 5/10
17/17 - 8s - loss: 429.0111 - loglik: -4.2494e+02 - logprior: -3.5843e+00
Epoch 6/10
17/17 - 8s - loss: 427.8838 - loglik: -4.2408e+02 - logprior: -3.3316e+00
Epoch 7/10
17/17 - 8s - loss: 424.7386 - loglik: -4.2094e+02 - logprior: -3.3177e+00
Epoch 8/10
17/17 - 8s - loss: 424.6611 - loglik: -4.2087e+02 - logprior: -3.2960e+00
Epoch 9/10
17/17 - 8s - loss: 424.5852 - loglik: -4.2080e+02 - logprior: -3.2682e+00
Epoch 10/10
17/17 - 7s - loss: 424.8097 - loglik: -4.2105e+02 - logprior: -3.2381e+00
Fitted a model with MAP estimate = -423.1932
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (15, 1), (24, 1), (28, 1), (32, 1), (42, 1), (45, 1), (46, 1), (55, 1), (57, 1), (58, 1), (60, 1), (63, 1), (87, 3), (95, 2), (97, 1), (100, 1), (114, 1), (115, 2), (116, 1), (117, 1), (129, 2), (131, 1), (142, 1), (156, 1), (161, 1), (162, 1), (163, 2), (176, 1), (178, 1), (180, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 435.8959 - loglik: -4.1972e+02 - logprior: -1.5991e+01
Epoch 2/2
17/17 - 10s - loss: 391.4597 - loglik: -3.8669e+02 - logprior: -4.6791e+00
Fitted a model with MAP estimate = -387.0037
expansions: [(0, 10)]
discards: [  0 103 138 156]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 398.3146 - loglik: -3.8732e+02 - logprior: -1.0922e+01
Epoch 2/2
17/17 - 10s - loss: 381.2771 - loglik: -3.8129e+02 - logprior: 0.3031
Fitted a model with MAP estimate = -377.5867
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 393.7332 - loglik: -3.8360e+02 - logprior: -9.9786e+00
Epoch 2/10
17/17 - 10s - loss: 379.6525 - loglik: -3.8042e+02 - logprior: 1.1731
Epoch 3/10
17/17 - 10s - loss: 377.0498 - loglik: -3.7919e+02 - logprior: 2.7017
Epoch 4/10
17/17 - 10s - loss: 372.1437 - loglik: -3.7501e+02 - logprior: 3.4412
Epoch 5/10
17/17 - 10s - loss: 372.1595 - loglik: -3.7543e+02 - logprior: 3.8276
Fitted a model with MAP estimate = -370.0914
Time for alignment: 214.4093
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 732.4646 - loglik: -7.2047e+02 - logprior: -1.1935e+01
Epoch 2/10
17/17 - 8s - loss: 556.3834 - loglik: -5.5457e+02 - logprior: -1.7661e+00
Epoch 3/10
17/17 - 8s - loss: 458.0951 - loglik: -4.5482e+02 - logprior: -3.0917e+00
Epoch 4/10
17/17 - 8s - loss: 433.3711 - loglik: -4.2926e+02 - logprior: -3.6442e+00
Epoch 5/10
17/17 - 8s - loss: 426.4402 - loglik: -4.2252e+02 - logprior: -3.3842e+00
Epoch 6/10
17/17 - 8s - loss: 425.6463 - loglik: -4.2198e+02 - logprior: -3.1417e+00
Epoch 7/10
17/17 - 8s - loss: 422.2393 - loglik: -4.1859e+02 - logprior: -3.1178e+00
Epoch 8/10
17/17 - 8s - loss: 423.6757 - loglik: -4.2006e+02 - logprior: -3.1083e+00
Fitted a model with MAP estimate = -422.1248
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (33, 1), (43, 1), (44, 1), (45, 1), (47, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (71, 2), (86, 4), (94, 2), (97, 1), (114, 2), (115, 3), (129, 3), (130, 2), (141, 1), (156, 1), (160, 1), (163, 1), (173, 1), (179, 1), (180, 1), (181, 1), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 433.7946 - loglik: -4.1779e+02 - logprior: -1.5938e+01
Epoch 2/2
17/17 - 10s - loss: 392.6536 - loglik: -3.8775e+02 - logprior: -4.7794e+00
Fitted a model with MAP estimate = -386.0552
expansions: [(0, 10)]
discards: [  0  86 105 106 138 159 162]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 398.0766 - loglik: -3.8691e+02 - logprior: -1.0950e+01
Epoch 2/2
17/17 - 10s - loss: 381.2756 - loglik: -3.8123e+02 - logprior: 0.2265
Fitted a model with MAP estimate = -378.0185
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 395.2159 - loglik: -3.8506e+02 - logprior: -1.0058e+01
Epoch 2/10
17/17 - 10s - loss: 379.8914 - loglik: -3.8067e+02 - logprior: 1.0938
Epoch 3/10
17/17 - 10s - loss: 376.3167 - loglik: -3.7851e+02 - logprior: 2.6638
Epoch 4/10
17/17 - 10s - loss: 373.7368 - loglik: -3.7667e+02 - logprior: 3.4176
Epoch 5/10
17/17 - 10s - loss: 373.2338 - loglik: -3.7655e+02 - logprior: 3.8121
Epoch 6/10
17/17 - 10s - loss: 371.3015 - loglik: -3.7495e+02 - logprior: 4.1380
Epoch 7/10
17/17 - 10s - loss: 367.2973 - loglik: -3.7122e+02 - logprior: 4.3811
Epoch 8/10
17/17 - 10s - loss: 369.8801 - loglik: -3.7406e+02 - logprior: 4.6256
Fitted a model with MAP estimate = -367.8196
Time for alignment: 227.3516
Computed alignments with likelihoods: ['-368.9244', '-370.0914', '-367.8196']
Best model has likelihood: -367.8196
time for generating output: 0.3507
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9452317119877441
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac443ea730>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6fe28b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faba1862160>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2f831dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa830bae20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab15e532e0>, <__main__.SimpleDirichletPrior object at 0x7fa9087d3790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5206 - loglik: -1.5946e+02 - logprior: -2.0030e+01
Epoch 2/10
10/10 - 1s - loss: 148.1807 - loglik: -1.4244e+02 - logprior: -5.7266e+00
Epoch 3/10
10/10 - 1s - loss: 130.9515 - loglik: -1.2764e+02 - logprior: -3.2801e+00
Epoch 4/10
10/10 - 1s - loss: 119.0289 - loglik: -1.1611e+02 - logprior: -2.7808e+00
Epoch 5/10
10/10 - 1s - loss: 112.9537 - loglik: -1.0987e+02 - logprior: -2.7572e+00
Epoch 6/10
10/10 - 1s - loss: 111.1219 - loglik: -1.0795e+02 - logprior: -2.7158e+00
Epoch 7/10
10/10 - 1s - loss: 110.4050 - loglik: -1.0741e+02 - logprior: -2.5854e+00
Epoch 8/10
10/10 - 1s - loss: 109.9440 - loglik: -1.0718e+02 - logprior: -2.4264e+00
Epoch 9/10
10/10 - 1s - loss: 109.6117 - loglik: -1.0693e+02 - logprior: -2.3374e+00
Epoch 10/10
10/10 - 1s - loss: 109.7178 - loglik: -1.0705e+02 - logprior: -2.3261e+00
Fitted a model with MAP estimate = -109.1076
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.8102 - loglik: -1.1325e+02 - logprior: -2.2492e+01
Epoch 2/2
10/10 - 1s - loss: 115.9920 - loglik: -1.0622e+02 - logprior: -9.6465e+00
Fitted a model with MAP estimate = -112.3578
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 121.9234 - loglik: -1.0374e+02 - logprior: -1.7810e+01
Epoch 2/2
10/10 - 1s - loss: 107.7218 - loglik: -1.0260e+02 - logprior: -4.9047e+00
Fitted a model with MAP estimate = -105.4985
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.6207 - loglik: -1.0431e+02 - logprior: -2.0254e+01
Epoch 2/10
10/10 - 1s - loss: 110.0398 - loglik: -1.0407e+02 - logprior: -5.8053e+00
Epoch 3/10
10/10 - 1s - loss: 106.1059 - loglik: -1.0286e+02 - logprior: -2.9292e+00
Epoch 4/10
10/10 - 1s - loss: 104.8695 - loglik: -1.0238e+02 - logprior: -2.0932e+00
Epoch 5/10
10/10 - 1s - loss: 103.8793 - loglik: -1.0198e+02 - logprior: -1.5025e+00
Epoch 6/10
10/10 - 1s - loss: 103.9431 - loglik: -1.0233e+02 - logprior: -1.2477e+00
Fitted a model with MAP estimate = -103.1082
Time for alignment: 33.2851
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5860 - loglik: -1.5937e+02 - logprior: -2.0028e+01
Epoch 2/10
10/10 - 1s - loss: 148.6624 - loglik: -1.4286e+02 - logprior: -5.7198e+00
Epoch 3/10
10/10 - 1s - loss: 131.0353 - loglik: -1.2775e+02 - logprior: -3.2506e+00
Epoch 4/10
10/10 - 1s - loss: 119.8887 - loglik: -1.1711e+02 - logprior: -2.6765e+00
Epoch 5/10
10/10 - 1s - loss: 114.0264 - loglik: -1.1109e+02 - logprior: -2.6575e+00
Epoch 6/10
10/10 - 1s - loss: 111.5232 - loglik: -1.0845e+02 - logprior: -2.6500e+00
Epoch 7/10
10/10 - 1s - loss: 110.7904 - loglik: -1.0787e+02 - logprior: -2.5346e+00
Epoch 8/10
10/10 - 1s - loss: 110.2446 - loglik: -1.0749e+02 - logprior: -2.4238e+00
Epoch 9/10
10/10 - 1s - loss: 109.9568 - loglik: -1.0726e+02 - logprior: -2.3482e+00
Epoch 10/10
10/10 - 1s - loss: 109.4911 - loglik: -1.0683e+02 - logprior: -2.3189e+00
Fitted a model with MAP estimate = -109.1698
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 136.0426 - loglik: -1.1352e+02 - logprior: -2.2490e+01
Epoch 2/2
10/10 - 1s - loss: 115.9250 - loglik: -1.0615e+02 - logprior: -9.6456e+00
Fitted a model with MAP estimate = -112.3672
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.7840 - loglik: -1.0384e+02 - logprior: -1.7797e+01
Epoch 2/2
10/10 - 1s - loss: 107.9384 - loglik: -1.0297e+02 - logprior: -4.8890e+00
Fitted a model with MAP estimate = -105.6937
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.7671 - loglik: -1.0437e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 110.4757 - loglik: -1.0450e+02 - logprior: -5.8226e+00
Epoch 3/10
10/10 - 1s - loss: 106.1364 - loglik: -1.0293e+02 - logprior: -2.9464e+00
Epoch 4/10
10/10 - 1s - loss: 104.9055 - loglik: -1.0248e+02 - logprior: -2.1255e+00
Epoch 5/10
10/10 - 1s - loss: 104.0854 - loglik: -1.0224e+02 - logprior: -1.5369e+00
Epoch 6/10
10/10 - 1s - loss: 103.7677 - loglik: -1.0222e+02 - logprior: -1.2469e+00
Epoch 7/10
10/10 - 1s - loss: 103.4690 - loglik: -1.0205e+02 - logprior: -1.1235e+00
Epoch 8/10
10/10 - 1s - loss: 103.4165 - loglik: -1.0212e+02 - logprior: -9.9305e-01
Epoch 9/10
10/10 - 1s - loss: 103.1823 - loglik: -1.0194e+02 - logprior: -9.2893e-01
Epoch 10/10
10/10 - 1s - loss: 102.9705 - loglik: -1.0175e+02 - logprior: -9.0055e-01
Fitted a model with MAP estimate = -102.6786
Time for alignment: 36.1813
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.4065 - loglik: -1.5925e+02 - logprior: -2.0028e+01
Epoch 2/10
10/10 - 1s - loss: 148.3188 - loglik: -1.4254e+02 - logprior: -5.7216e+00
Epoch 3/10
10/10 - 1s - loss: 131.0916 - loglik: -1.2781e+02 - logprior: -3.2475e+00
Epoch 4/10
10/10 - 1s - loss: 120.0267 - loglik: -1.1727e+02 - logprior: -2.6268e+00
Epoch 5/10
10/10 - 1s - loss: 115.1570 - loglik: -1.1238e+02 - logprior: -2.4877e+00
Epoch 6/10
10/10 - 1s - loss: 113.0667 - loglik: -1.1024e+02 - logprior: -2.4280e+00
Epoch 7/10
10/10 - 1s - loss: 112.0386 - loglik: -1.0933e+02 - logprior: -2.3233e+00
Epoch 8/10
10/10 - 1s - loss: 111.7711 - loglik: -1.0925e+02 - logprior: -2.1866e+00
Epoch 9/10
10/10 - 1s - loss: 111.4929 - loglik: -1.0906e+02 - logprior: -2.1082e+00
Epoch 10/10
10/10 - 1s - loss: 111.4136 - loglik: -1.0898e+02 - logprior: -2.0929e+00
Fitted a model with MAP estimate = -110.9592
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 1), (36, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 135.3911 - loglik: -1.1292e+02 - logprior: -2.2445e+01
Epoch 2/2
10/10 - 1s - loss: 115.7782 - loglik: -1.0614e+02 - logprior: -9.5733e+00
Fitted a model with MAP estimate = -112.3372
expansions: [(0, 2)]
discards: [ 0  6 13 15 18]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.6270 - loglik: -1.0382e+02 - logprior: -1.7791e+01
Epoch 2/2
10/10 - 1s - loss: 107.7208 - loglik: -1.0276e+02 - logprior: -4.8855e+00
Fitted a model with MAP estimate = -105.6638
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.5908 - loglik: -1.0425e+02 - logprior: -2.0236e+01
Epoch 2/10
10/10 - 1s - loss: 109.8371 - loglik: -1.0384e+02 - logprior: -5.8024e+00
Epoch 3/10
10/10 - 1s - loss: 106.1945 - loglik: -1.0295e+02 - logprior: -2.9392e+00
Epoch 4/10
10/10 - 1s - loss: 104.8133 - loglik: -1.0233e+02 - logprior: -2.1082e+00
Epoch 5/10
10/10 - 1s - loss: 104.0603 - loglik: -1.0218e+02 - logprior: -1.5162e+00
Epoch 6/10
10/10 - 1s - loss: 103.7470 - loglik: -1.0216e+02 - logprior: -1.2509e+00
Epoch 7/10
10/10 - 1s - loss: 103.1325 - loglik: -1.0165e+02 - logprior: -1.1447e+00
Epoch 8/10
10/10 - 1s - loss: 103.3930 - loglik: -1.0206e+02 - logprior: -9.9965e-01
Fitted a model with MAP estimate = -102.7802
Time for alignment: 33.9475
Computed alignments with likelihoods: ['-103.1082', '-102.6786', '-102.7802']
Best model has likelihood: -102.6786
time for generating output: 0.1185
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9928712871287129
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faae2f9c220>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8f0699040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa86e226f10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faab0112970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faad248c3a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faab89438b0>, <__main__.SimpleDirichletPrior object at 0x7fa86d1e0280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 470.4526 - loglik: -4.6849e+02 - logprior: -1.8012e+00
Epoch 2/10
39/39 - 11s - loss: 387.4441 - loglik: -3.8580e+02 - logprior: -1.1796e+00
Epoch 3/10
39/39 - 11s - loss: 378.6738 - loglik: -3.7696e+02 - logprior: -1.1565e+00
Epoch 4/10
39/39 - 11s - loss: 376.1181 - loglik: -3.7436e+02 - logprior: -1.1566e+00
Epoch 5/10
39/39 - 11s - loss: 374.6775 - loglik: -3.7292e+02 - logprior: -1.1508e+00
Epoch 6/10
39/39 - 11s - loss: 373.9453 - loglik: -3.7219e+02 - logprior: -1.1514e+00
Epoch 7/10
39/39 - 10s - loss: 373.4316 - loglik: -3.7169e+02 - logprior: -1.1486e+00
Epoch 8/10
39/39 - 10s - loss: 372.7863 - loglik: -3.7103e+02 - logprior: -1.1442e+00
Epoch 9/10
39/39 - 10s - loss: 372.8441 - loglik: -3.7108e+02 - logprior: -1.1460e+00
Fitted a model with MAP estimate = -306.9561
expansions: [(0, 15), (10, 1), (11, 1), (18, 1), (28, 2), (29, 3), (30, 1), (33, 1), (34, 1), (37, 1), (42, 1), (43, 1), (44, 1), (71, 1), (88, 4), (89, 1), (90, 1), (102, 1), (104, 1), (107, 1), (123, 6), (125, 2), (129, 2), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 362.1281 - loglik: -3.5905e+02 - logprior: -2.7287e+00
Epoch 2/2
39/39 - 14s - loss: 340.2719 - loglik: -3.3853e+02 - logprior: -1.3783e+00
Fitted a model with MAP estimate = -281.6243
expansions: [(151, 1), (165, 1), (166, 3)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  49  50  51 121 178
 181]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 345.7566 - loglik: -3.4348e+02 - logprior: -1.9632e+00
Epoch 2/2
39/39 - 13s - loss: 340.1442 - loglik: -3.3895e+02 - logprior: -8.0077e-01
Fitted a model with MAP estimate = -284.3358
expansions: [(0, 15)]
discards: [152]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 21s - loss: 282.1498 - loglik: -2.8034e+02 - logprior: -1.5795e+00
Epoch 2/10
52/52 - 16s - loss: 277.2300 - loglik: -2.7575e+02 - logprior: -1.1893e+00
Epoch 3/10
52/52 - 16s - loss: 273.8334 - loglik: -2.7245e+02 - logprior: -1.0052e+00
Epoch 4/10
52/52 - 16s - loss: 274.2718 - loglik: -2.7303e+02 - logprior: -8.1875e-01
Fitted a model with MAP estimate = -272.1801
Time for alignment: 316.3060
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 468.8202 - loglik: -4.6690e+02 - logprior: -1.7981e+00
Epoch 2/10
39/39 - 11s - loss: 388.0885 - loglik: -3.8649e+02 - logprior: -1.1552e+00
Epoch 3/10
39/39 - 10s - loss: 377.9073 - loglik: -3.7627e+02 - logprior: -1.1469e+00
Epoch 4/10
39/39 - 10s - loss: 375.4750 - loglik: -3.7385e+02 - logprior: -1.1075e+00
Epoch 5/10
39/39 - 11s - loss: 374.4996 - loglik: -3.7290e+02 - logprior: -1.0961e+00
Epoch 6/10
39/39 - 11s - loss: 373.6643 - loglik: -3.7208e+02 - logprior: -1.0931e+00
Epoch 7/10
39/39 - 11s - loss: 373.6750 - loglik: -3.7210e+02 - logprior: -1.0995e+00
Fitted a model with MAP estimate = -308.5477
expansions: [(0, 15), (10, 1), (11, 1), (18, 1), (28, 2), (29, 3), (34, 3), (43, 2), (44, 1), (45, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 2), (108, 1), (112, 1), (125, 7), (126, 2), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 360.0835 - loglik: -3.5728e+02 - logprior: -2.6697e+00
Epoch 2/2
39/39 - 14s - loss: 342.7880 - loglik: -3.4109e+02 - logprior: -1.3496e+00
Fitted a model with MAP estimate = -283.6635
expansions: [(169, 2), (170, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  49  50  51  57  58
 143 151]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 351.0091 - loglik: -3.4875e+02 - logprior: -1.9574e+00
Epoch 2/2
39/39 - 13s - loss: 346.1861 - loglik: -3.4491e+02 - logprior: -8.4116e-01
Fitted a model with MAP estimate = -288.9554
expansions: [(0, 15), (36, 2)]
discards: [152]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 285.4562 - loglik: -2.8367e+02 - logprior: -1.6203e+00
Epoch 2/10
52/52 - 15s - loss: 279.0979 - loglik: -2.7776e+02 - logprior: -1.0078e+00
Epoch 3/10
52/52 - 16s - loss: 276.0728 - loglik: -2.7480e+02 - logprior: -9.1680e-01
Epoch 4/10
52/52 - 16s - loss: 274.8776 - loglik: -2.7361e+02 - logprior: -8.8008e-01
Epoch 5/10
52/52 - 17s - loss: 274.1304 - loglik: -2.7291e+02 - logprior: -8.3262e-01
Epoch 6/10
52/52 - 16s - loss: 271.4208 - loglik: -2.7025e+02 - logprior: -7.8223e-01
Epoch 7/10
52/52 - 15s - loss: 272.0527 - loglik: -2.7092e+02 - logprior: -7.2836e-01
Fitted a model with MAP estimate = -271.2907
Time for alignment: 342.8719
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 468.0430 - loglik: -4.6617e+02 - logprior: -1.7966e+00
Epoch 2/10
39/39 - 10s - loss: 384.9487 - loglik: -3.8341e+02 - logprior: -1.1741e+00
Epoch 3/10
39/39 - 11s - loss: 376.2774 - loglik: -3.7467e+02 - logprior: -1.1718e+00
Epoch 4/10
39/39 - 11s - loss: 374.2361 - loglik: -3.7258e+02 - logprior: -1.1545e+00
Epoch 5/10
39/39 - 11s - loss: 372.4473 - loglik: -3.7075e+02 - logprior: -1.1789e+00
Epoch 6/10
39/39 - 11s - loss: 372.2380 - loglik: -3.7056e+02 - logprior: -1.1789e+00
Epoch 7/10
39/39 - 11s - loss: 371.3141 - loglik: -3.6967e+02 - logprior: -1.1890e+00
Epoch 8/10
39/39 - 10s - loss: 371.3758 - loglik: -3.6976e+02 - logprior: -1.1814e+00
Fitted a model with MAP estimate = -306.3193
expansions: [(0, 14), (10, 1), (15, 1), (18, 1), (28, 2), (29, 1), (31, 1), (32, 2), (33, 1), (34, 1), (37, 2), (44, 2), (56, 1), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (102, 1), (104, 1), (107, 1), (112, 1), (126, 11), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 355.6367 - loglik: -3.5275e+02 - logprior: -2.7565e+00
Epoch 2/2
39/39 - 14s - loss: 337.1412 - loglik: -3.3533e+02 - logprior: -1.4913e+00
Fitted a model with MAP estimate = -279.8270
expansions: [(172, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  45  53  71 182]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 346.6902 - loglik: -3.4372e+02 - logprior: -2.8299e+00
Epoch 2/2
39/39 - 13s - loss: 340.7247 - loglik: -3.3959e+02 - logprior: -7.9340e-01
Fitted a model with MAP estimate = -284.9213
expansions: [(0, 16), (156, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 19s - loss: 279.0294 - loglik: -2.7742e+02 - logprior: -1.4367e+00
Epoch 2/10
52/52 - 17s - loss: 278.3842 - loglik: -2.7703e+02 - logprior: -1.0815e+00
Epoch 3/10
52/52 - 19s - loss: 273.2878 - loglik: -2.7191e+02 - logprior: -1.0460e+00
Epoch 4/10
52/52 - 15s - loss: 272.5419 - loglik: -2.7120e+02 - logprior: -9.7732e-01
Epoch 5/10
52/52 - 17s - loss: 272.7106 - loglik: -2.7146e+02 - logprior: -8.9218e-01
Fitted a model with MAP estimate = -270.0273
Time for alignment: 324.5006
Computed alignments with likelihoods: ['-272.1801', '-271.2907', '-270.0273']
Best model has likelihood: -270.0273
time for generating output: 0.7138
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.20430517192113534
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa2ad75a3a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabef401790>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa965e5070>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4cf59880>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabcd4d5cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa8e878d7c0>, <__main__.SimpleDirichletPrior object at 0x7faba17d45b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 812.4230 - loglik: -8.1054e+02 - logprior: -1.5434e+00
Epoch 2/10
37/37 - 23s - loss: 722.6778 - loglik: -7.2095e+02 - logprior: -6.8039e-01
Epoch 3/10
37/37 - 23s - loss: 709.0225 - loglik: -7.0684e+02 - logprior: -7.5006e-01
Epoch 4/10
37/37 - 23s - loss: 704.1361 - loglik: -7.0186e+02 - logprior: -6.8142e-01
Epoch 5/10
37/37 - 23s - loss: 700.4128 - loglik: -6.9812e+02 - logprior: -7.4276e-01
Epoch 6/10
37/37 - 23s - loss: 701.5992 - loglik: -6.9936e+02 - logprior: -8.0085e-01
Fitted a model with MAP estimate = -697.2822
expansions: [(0, 4), (30, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (66, 1), (92, 5), (93, 11), (94, 4), (100, 2), (101, 1), (143, 3), (197, 12), (205, 4), (239, 1)]
discards: [104 105]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 33s - loss: 728.5045 - loglik: -7.2559e+02 - logprior: -2.5475e+00
Epoch 2/2
37/37 - 30s - loss: 696.9774 - loglik: -6.9525e+02 - logprior: -9.0406e-01
Fitted a model with MAP estimate = -690.6646
expansions: [(0, 2), (39, 1), (44, 1), (262, 2)]
discards: [  0 105 106 107 179]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 33s - loss: 711.7802 - loglik: -7.0957e+02 - logprior: -2.0168e+00
Epoch 2/2
37/37 - 30s - loss: 697.0549 - loglik: -6.9547e+02 - logprior: -6.0798e-01
Fitted a model with MAP estimate = -688.7117
expansions: [(0, 2), (180, 5), (262, 1)]
discards: [  1 256 257 258 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 709.4021 - loglik: -7.0699e+02 - logprior: -2.1739e+00
Epoch 2/10
37/37 - 30s - loss: 695.5887 - loglik: -6.9391e+02 - logprior: -4.8252e-01
Epoch 3/10
37/37 - 30s - loss: 686.5976 - loglik: -6.8419e+02 - logprior: -3.9649e-01
Epoch 4/10
37/37 - 30s - loss: 683.7158 - loglik: -6.8126e+02 - logprior: -2.5580e-01
Epoch 5/10
37/37 - 30s - loss: 678.5930 - loglik: -6.7630e+02 - logprior: -2.7158e-01
Epoch 6/10
37/37 - 30s - loss: 678.6030 - loglik: -6.7675e+02 - logprior: -1.1351e-01
Fitted a model with MAP estimate = -675.5662
Time for alignment: 587.7466
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 816.9594 - loglik: -8.1525e+02 - logprior: -1.5599e+00
Epoch 2/10
37/37 - 23s - loss: 723.5606 - loglik: -7.2209e+02 - logprior: -6.4743e-01
Epoch 3/10
37/37 - 23s - loss: 708.5703 - loglik: -7.0665e+02 - logprior: -6.2107e-01
Epoch 4/10
37/37 - 24s - loss: 706.3690 - loglik: -7.0425e+02 - logprior: -5.7003e-01
Epoch 5/10
37/37 - 23s - loss: 701.7975 - loglik: -6.9963e+02 - logprior: -6.1048e-01
Epoch 6/10
37/37 - 23s - loss: 698.9918 - loglik: -6.9673e+02 - logprior: -8.1643e-01
Epoch 7/10
37/37 - 23s - loss: 699.9893 - loglik: -6.9801e+02 - logprior: -6.4577e-01
Fitted a model with MAP estimate = -697.1926
expansions: [(0, 4), (30, 1), (32, 3), (34, 4), (35, 1), (73, 1), (91, 16), (92, 5), (133, 1), (197, 12), (205, 4), (239, 1)]
discards: [ 99 100 102 104 121]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 34s - loss: 745.2322 - loglik: -7.4226e+02 - logprior: -2.7138e+00
Epoch 2/2
37/37 - 29s - loss: 702.4626 - loglik: -7.0051e+02 - logprior: -9.4396e-01
Fitted a model with MAP estimate = -692.9644
expansions: [(0, 2), (38, 1), (39, 1), (113, 1), (117, 1), (241, 2), (249, 1), (250, 2), (251, 3)]
discards: [  0  40 183 184 185 186 187 188 189 190 191 252 253 254 255 256 257]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 714.5801 - loglik: -7.1240e+02 - logprior: -2.0452e+00
Epoch 2/2
37/37 - 29s - loss: 699.4190 - loglik: -6.9836e+02 - logprior: -5.9633e-01
Fitted a model with MAP estimate = -693.8431
expansions: [(0, 2), (187, 3), (188, 1), (189, 6), (237, 2), (252, 2)]
discards: [  1 245]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 710.5300 - loglik: -7.0829e+02 - logprior: -2.1408e+00
Epoch 2/10
37/37 - 31s - loss: 694.8233 - loglik: -6.9333e+02 - logprior: -4.5391e-01
Epoch 3/10
37/37 - 30s - loss: 687.3470 - loglik: -6.8491e+02 - logprior: -4.0950e-01
Epoch 4/10
37/37 - 31s - loss: 680.1307 - loglik: -6.7757e+02 - logprior: -4.5313e-01
Epoch 5/10
37/37 - 31s - loss: 681.6719 - loglik: -6.7939e+02 - logprior: -2.7115e-01
Fitted a model with MAP estimate = -675.7507
Time for alignment: 578.3236
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 813.1072 - loglik: -8.1130e+02 - logprior: -1.5422e+00
Epoch 2/10
37/37 - 23s - loss: 726.1665 - loglik: -7.2462e+02 - logprior: -5.7584e-01
Epoch 3/10
37/37 - 23s - loss: 711.2363 - loglik: -7.0904e+02 - logprior: -6.2271e-01
Epoch 4/10
37/37 - 23s - loss: 706.8235 - loglik: -7.0451e+02 - logprior: -5.7628e-01
Epoch 5/10
37/37 - 23s - loss: 705.8535 - loglik: -7.0365e+02 - logprior: -6.0379e-01
Epoch 6/10
37/37 - 23s - loss: 704.5504 - loglik: -7.0247e+02 - logprior: -6.1781e-01
Epoch 7/10
37/37 - 23s - loss: 701.5670 - loglik: -6.9963e+02 - logprior: -6.2640e-01
Epoch 8/10
37/37 - 23s - loss: 701.5787 - loglik: -6.9972e+02 - logprior: -6.3559e-01
Fitted a model with MAP estimate = -699.9682
expansions: [(0, 4), (30, 1), (32, 6), (73, 4), (117, 1), (123, 1), (134, 5), (135, 1), (144, 2), (197, 11), (205, 4), (242, 1)]
discards: [ 99 100 101 102 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 742.6779 - loglik: -7.3964e+02 - logprior: -2.7129e+00
Epoch 2/2
37/37 - 27s - loss: 704.4946 - loglik: -7.0260e+02 - logprior: -9.7463e-01
Fitted a model with MAP estimate = -698.0440
expansions: [(0, 2), (38, 2), (39, 1), (150, 1), (168, 3), (169, 8), (228, 2), (238, 3)]
discards: [ 86  87 239 240 241 242 243]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 33s - loss: 713.9789 - loglik: -7.1093e+02 - logprior: -2.8203e+00
Epoch 2/2
37/37 - 30s - loss: 700.3373 - loglik: -6.9904e+02 - logprior: -6.7675e-01
Fitted a model with MAP estimate = -693.2240
expansions: [(111, 6), (179, 1), (241, 4), (253, 1)]
discards: [  1   2   3 174]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 36s - loss: 709.1432 - loglik: -7.0718e+02 - logprior: -1.7072e+00
Epoch 2/10
37/37 - 31s - loss: 696.6997 - loglik: -6.9574e+02 - logprior: -3.3726e-01
Epoch 3/10
37/37 - 31s - loss: 689.6868 - loglik: -6.8836e+02 - logprior: -1.7899e-01
Epoch 4/10
37/37 - 31s - loss: 684.7278 - loglik: -6.8309e+02 - logprior: -9.1972e-02
Epoch 5/10
37/37 - 31s - loss: 682.1380 - loglik: -6.8041e+02 - logprior: -5.0028e-02
Epoch 6/10
37/37 - 30s - loss: 678.9822 - loglik: -6.7737e+02 - logprior: 0.0322
Epoch 7/10
37/37 - 31s - loss: 681.0546 - loglik: -6.7970e+02 - logprior: 0.2015
Fitted a model with MAP estimate = -676.6267
Time for alignment: 659.2774
Computed alignments with likelihoods: ['-675.5662', '-675.7507', '-676.6267']
Best model has likelihood: -675.5662
time for generating output: 0.3106
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.9323590814196242
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa2b4f14fa0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5de97d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac33024fa0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac11098ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86f1313a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa965e5070>, <__main__.SimpleDirichletPrior object at 0x7faab0117e20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 385.9946 - loglik: -2.7258e+02 - logprior: -1.1329e+02
Epoch 2/10
10/10 - 1s - loss: 288.7044 - loglik: -2.6103e+02 - logprior: -2.7436e+01
Epoch 3/10
10/10 - 1s - loss: 260.2138 - loglik: -2.4995e+02 - logprior: -9.9163e+00
Epoch 4/10
10/10 - 1s - loss: 244.0968 - loglik: -2.4002e+02 - logprior: -3.5395e+00
Epoch 5/10
10/10 - 1s - loss: 235.0338 - loglik: -2.3370e+02 - logprior: -6.3784e-01
Epoch 6/10
10/10 - 1s - loss: 229.7796 - loglik: -2.3018e+02 - logprior: 1.0954
Epoch 7/10
10/10 - 1s - loss: 227.0752 - loglik: -2.2842e+02 - logprior: 2.0649
Epoch 8/10
10/10 - 1s - loss: 225.3633 - loglik: -2.2742e+02 - logprior: 2.8114
Epoch 9/10
10/10 - 1s - loss: 224.0349 - loglik: -2.2660e+02 - logprior: 3.3318
Epoch 10/10
10/10 - 1s - loss: 223.0887 - loglik: -2.2612e+02 - logprior: 3.7775
Fitted a model with MAP estimate = -221.9459
expansions: [(0, 6), (37, 5), (52, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 390.7034 - loglik: -2.3974e+02 - logprior: -1.5092e+02
Epoch 2/2
10/10 - 1s - loss: 272.2541 - loglik: -2.2929e+02 - logprior: -4.2880e+01
Fitted a model with MAP estimate = -249.6041
expansions: [(0, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.5761 - loglik: -2.2948e+02 - logprior: -1.1902e+02
Epoch 2/2
10/10 - 1s - loss: 257.9582 - loglik: -2.2692e+02 - logprior: -3.1034e+01
Fitted a model with MAP estimate = -241.8812
expansions: [(0, 4), (42, 2), (61, 1), (76, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 350.9149 - loglik: -2.2494e+02 - logprior: -1.2590e+02
Epoch 2/10
10/10 - 1s - loss: 259.2947 - loglik: -2.2309e+02 - logprior: -3.5979e+01
Epoch 3/10
10/10 - 1s - loss: 233.9084 - loglik: -2.2228e+02 - logprior: -1.1129e+01
Epoch 4/10
10/10 - 1s - loss: 223.1767 - loglik: -2.2220e+02 - logprior: -3.4998e-01
Epoch 5/10
10/10 - 1s - loss: 218.1293 - loglik: -2.2206e+02 - logprior: 4.4927
Epoch 6/10
10/10 - 1s - loss: 215.3172 - loglik: -2.2177e+02 - logprior: 7.0233
Epoch 7/10
10/10 - 1s - loss: 213.5064 - loglik: -2.2147e+02 - logprior: 8.5610
Epoch 8/10
10/10 - 1s - loss: 212.1975 - loglik: -2.2112e+02 - logprior: 9.5810
Epoch 9/10
10/10 - 1s - loss: 211.1358 - loglik: -2.2079e+02 - logprior: 10.3406
Epoch 10/10
10/10 - 1s - loss: 210.2272 - loglik: -2.2048e+02 - logprior: 10.9685
Fitted a model with MAP estimate = -209.0577
Time for alignment: 45.9649
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0885 - loglik: -2.7266e+02 - logprior: -1.1328e+02
Epoch 2/10
10/10 - 1s - loss: 288.8665 - loglik: -2.6121e+02 - logprior: -2.7423e+01
Epoch 3/10
10/10 - 1s - loss: 259.9523 - loglik: -2.4968e+02 - logprior: -9.9284e+00
Epoch 4/10
10/10 - 1s - loss: 243.2219 - loglik: -2.3895e+02 - logprior: -3.6957e+00
Epoch 5/10
10/10 - 1s - loss: 234.2389 - loglik: -2.3279e+02 - logprior: -7.0622e-01
Epoch 6/10
10/10 - 1s - loss: 229.0633 - loglik: -2.2945e+02 - logprior: 1.1169
Epoch 7/10
10/10 - 1s - loss: 226.1126 - loglik: -2.2747e+02 - logprior: 2.0795
Epoch 8/10
10/10 - 1s - loss: 224.3592 - loglik: -2.2641e+02 - logprior: 2.7867
Epoch 9/10
10/10 - 1s - loss: 223.2053 - loglik: -2.2575e+02 - logprior: 3.2716
Epoch 10/10
10/10 - 1s - loss: 222.4211 - loglik: -2.2533e+02 - logprior: 3.6319
Fitted a model with MAP estimate = -221.3597
expansions: [(0, 6), (37, 6), (50, 4), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 386.2554 - loglik: -2.3617e+02 - logprior: -1.4998e+02
Epoch 2/2
10/10 - 1s - loss: 268.9572 - loglik: -2.2619e+02 - logprior: -4.2421e+01
Fitted a model with MAP estimate = -246.9528
expansions: [(0, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 347.0252 - loglik: -2.2818e+02 - logprior: -1.1864e+02
Epoch 2/2
10/10 - 1s - loss: 256.3528 - loglik: -2.2536e+02 - logprior: -3.0763e+01
Fitted a model with MAP estimate = -239.6210
expansions: [(0, 4), (41, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 351.2796 - loglik: -2.2518e+02 - logprior: -1.2604e+02
Epoch 2/10
10/10 - 1s - loss: 259.1395 - loglik: -2.2297e+02 - logprior: -3.5987e+01
Epoch 3/10
10/10 - 1s - loss: 233.4210 - loglik: -2.2186e+02 - logprior: -1.1138e+01
Epoch 4/10
10/10 - 1s - loss: 222.3557 - loglik: -2.2150e+02 - logprior: -3.2151e-01
Epoch 5/10
10/10 - 1s - loss: 217.1616 - loglik: -2.2108e+02 - logprior: 4.4872
Epoch 6/10
10/10 - 1s - loss: 214.2638 - loglik: -2.2059e+02 - logprior: 6.9819
Epoch 7/10
10/10 - 1s - loss: 212.3472 - loglik: -2.2008e+02 - logprior: 8.4831
Epoch 8/10
10/10 - 1s - loss: 210.9304 - loglik: -2.1965e+02 - logprior: 9.5010
Epoch 9/10
10/10 - 1s - loss: 209.7848 - loglik: -2.1928e+02 - logprior: 10.3117
Epoch 10/10
10/10 - 1s - loss: 208.9438 - loglik: -2.1907e+02 - logprior: 10.9643
Fitted a model with MAP estimate = -207.6920
Time for alignment: 45.9429
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 385.6640 - loglik: -2.7196e+02 - logprior: -1.1326e+02
Epoch 2/10
10/10 - 1s - loss: 288.9450 - loglik: -2.6096e+02 - logprior: -2.7318e+01
Epoch 3/10
10/10 - 1s - loss: 261.1858 - loglik: -2.5094e+02 - logprior: -9.7420e+00
Epoch 4/10
10/10 - 1s - loss: 244.9404 - loglik: -2.4123e+02 - logprior: -3.2610e+00
Epoch 5/10
10/10 - 1s - loss: 235.8163 - loglik: -2.3490e+02 - logprior: -2.8356e-01
Epoch 6/10
10/10 - 1s - loss: 230.4187 - loglik: -2.3079e+02 - logprior: 1.1262
Epoch 7/10
10/10 - 1s - loss: 227.1064 - loglik: -2.2832e+02 - logprior: 1.9581
Epoch 8/10
10/10 - 1s - loss: 225.1388 - loglik: -2.2696e+02 - logprior: 2.5668
Epoch 9/10
10/10 - 1s - loss: 223.8863 - loglik: -2.2627e+02 - logprior: 3.1283
Epoch 10/10
10/10 - 1s - loss: 223.1015 - loglik: -2.2589e+02 - logprior: 3.5437
Fitted a model with MAP estimate = -221.9794
expansions: [(0, 6), (52, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 392.6094 - loglik: -2.3951e+02 - logprior: -1.5303e+02
Epoch 2/2
10/10 - 1s - loss: 273.5468 - loglik: -2.2893e+02 - logprior: -4.4377e+01
Fitted a model with MAP estimate = -250.7162
expansions: [(0, 3), (78, 4)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 352.2959 - loglik: -2.3210e+02 - logprior: -1.2018e+02
Epoch 2/2
10/10 - 1s - loss: 259.1622 - loglik: -2.2749e+02 - logprior: -3.1575e+01
Fitted a model with MAP estimate = -242.1452
expansions: [(0, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 354.0262 - loglik: -2.2714e+02 - logprior: -1.2683e+02
Epoch 2/10
10/10 - 1s - loss: 262.1887 - loglik: -2.2522e+02 - logprior: -3.6813e+01
Epoch 3/10
10/10 - 1s - loss: 236.6969 - loglik: -2.2432e+02 - logprior: -1.1998e+01
Epoch 4/10
10/10 - 1s - loss: 225.9018 - loglik: -2.2386e+02 - logprior: -1.4995e+00
Epoch 5/10
10/10 - 1s - loss: 220.8901 - loglik: -2.2355e+02 - logprior: 3.2269
Epoch 6/10
10/10 - 1s - loss: 218.0672 - loglik: -2.2321e+02 - logprior: 5.7166
Epoch 7/10
10/10 - 1s - loss: 216.3487 - loglik: -2.2296e+02 - logprior: 7.2195
Epoch 8/10
10/10 - 1s - loss: 215.2030 - loglik: -2.2283e+02 - logprior: 8.2508
Epoch 9/10
10/10 - 1s - loss: 214.3409 - loglik: -2.2273e+02 - logprior: 9.0286
Epoch 10/10
10/10 - 1s - loss: 213.6297 - loglik: -2.2264e+02 - logprior: 9.6635
Fitted a model with MAP estimate = -212.6224
Time for alignment: 45.1504
Computed alignments with likelihoods: ['-209.0577', '-207.6920', '-212.6224']
Best model has likelihood: -207.6920
time for generating output: 0.1759
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7445830597504924
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac33717cd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa963dc2e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa29c3f2370>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8e86cc6d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2b547b940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab04e545b0>, <__main__.SimpleDirichletPrior object at 0x7fabaae65a60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 997.1011 - loglik: -9.8946e+02 - logprior: -7.5945e+00
Epoch 2/10
19/19 - 19s - loss: 868.1418 - loglik: -8.6882e+02 - logprior: 0.9749
Epoch 3/10
19/19 - 20s - loss: 810.9880 - loglik: -8.1027e+02 - logprior: -7.2116e-02
Epoch 4/10
19/19 - 20s - loss: 787.0388 - loglik: -7.8585e+02 - logprior: -3.8010e-01
Epoch 5/10
19/19 - 20s - loss: 788.9116 - loglik: -7.8773e+02 - logprior: -3.9479e-01
Fitted a model with MAP estimate = -785.2546
expansions: [(14, 1), (30, 1), (32, 1), (68, 1), (99, 2), (106, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (122, 8), (124, 2), (142, 1), (156, 1), (166, 3), (167, 1), (169, 1), (176, 2), (177, 1), (178, 1), (180, 1), (189, 1), (190, 2), (205, 2), (206, 1), (212, 1), (220, 2), (221, 1), (223, 1), (224, 2), (237, 2), (238, 2), (259, 1), (262, 1), (300, 1), (301, 2), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 818.6486 - loglik: -8.0728e+02 - logprior: -1.1334e+01
Epoch 2/2
19/19 - 25s - loss: 771.9383 - loglik: -7.6899e+02 - logprior: -2.7552e+00
Fitted a model with MAP estimate = -761.2731
expansions: [(0, 2), (59, 1), (101, 1), (176, 1)]
discards: [  0  30 145 193 204 244 261]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 775.4221 - loglik: -7.6841e+02 - logprior: -6.8236e+00
Epoch 2/2
19/19 - 25s - loss: 754.5504 - loglik: -7.5601e+02 - logprior: 1.8723
Fitted a model with MAP estimate = -749.5269
expansions: []
discards: [  0 105 146 350]
Re-initialized the encoder parameters.
Fitting a model of length 373 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 30s - loss: 780.9468 - loglik: -7.7106e+02 - logprior: -9.7845e+00
Epoch 2/10
19/19 - 24s - loss: 754.4899 - loglik: -7.5423e+02 - logprior: 0.0713
Epoch 3/10
19/19 - 24s - loss: 754.1717 - loglik: -7.5681e+02 - logprior: 3.1619
Epoch 4/10
19/19 - 24s - loss: 742.9732 - loglik: -7.4616e+02 - logprior: 3.8539
Epoch 5/10
19/19 - 24s - loss: 746.6372 - loglik: -7.5003e+02 - logprior: 4.1325
Fitted a model with MAP estimate = -742.4307
Time for alignment: 403.6625
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 995.4625 - loglik: -9.8745e+02 - logprior: -7.5359e+00
Epoch 2/10
19/19 - 20s - loss: 871.9402 - loglik: -8.7203e+02 - logprior: 0.9200
Epoch 3/10
19/19 - 20s - loss: 807.9637 - loglik: -8.0642e+02 - logprior: -5.8197e-01
Epoch 4/10
19/19 - 20s - loss: 786.9388 - loglik: -7.8493e+02 - logprior: -1.0672e+00
Epoch 5/10
19/19 - 20s - loss: 783.4656 - loglik: -7.8155e+02 - logprior: -1.0264e+00
Epoch 6/10
19/19 - 20s - loss: 779.6190 - loglik: -7.7766e+02 - logprior: -1.1049e+00
Epoch 7/10
19/19 - 20s - loss: 778.0728 - loglik: -7.7618e+02 - logprior: -1.0810e+00
Epoch 8/10
19/19 - 20s - loss: 777.5109 - loglik: -7.7560e+02 - logprior: -1.1241e+00
Epoch 9/10
19/19 - 20s - loss: 774.8323 - loglik: -7.7305e+02 - logprior: -1.0341e+00
Epoch 10/10
19/19 - 20s - loss: 776.9758 - loglik: -7.7514e+02 - logprior: -1.1112e+00
Fitted a model with MAP estimate = -774.2249
expansions: [(14, 1), (32, 1), (61, 1), (66, 2), (97, 1), (98, 3), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (121, 2), (122, 3), (125, 3), (143, 1), (145, 1), (156, 1), (157, 1), (168, 1), (170, 1), (171, 1), (174, 1), (175, 1), (177, 1), (178, 2), (191, 1), (192, 1), (197, 1), (199, 1), (200, 1), (205, 1), (219, 1), (221, 1), (222, 1), (223, 2), (224, 2), (234, 1), (237, 3), (257, 1), (258, 1), (263, 2), (267, 6), (300, 2), (301, 1), (302, 1), (309, 1), (310, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 822.8463 - loglik: -8.1131e+02 - logprior: -1.1457e+01
Epoch 2/2
19/19 - 26s - loss: 759.7040 - loglik: -7.5696e+02 - logprior: -2.3148e+00
Fitted a model with MAP estimate = -749.6154
expansions: [(0, 2), (287, 1), (353, 4)]
discards: [  0 104 105 144 146 270 316 317 322 323]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 766.4406 - loglik: -7.6016e+02 - logprior: -6.2061e+00
Epoch 2/2
19/19 - 26s - loss: 743.6981 - loglik: -7.4584e+02 - logprior: 2.4131
Fitted a model with MAP estimate = -736.6949
expansions: [(313, 2), (348, 1), (349, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 761.1982 - loglik: -7.5178e+02 - logprior: -9.2529e+00
Epoch 2/10
19/19 - 26s - loss: 740.3472 - loglik: -7.4060e+02 - logprior: 0.5997
Epoch 3/10
19/19 - 26s - loss: 732.7142 - loglik: -7.3586e+02 - logprior: 3.7026
Epoch 4/10
19/19 - 26s - loss: 724.5730 - loglik: -7.2845e+02 - logprior: 4.5952
Epoch 5/10
19/19 - 26s - loss: 728.0152 - loglik: -7.3207e+02 - logprior: 4.8480
Fitted a model with MAP estimate = -723.4090
Time for alignment: 513.9196
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 997.1387 - loglik: -9.8943e+02 - logprior: -7.6025e+00
Epoch 2/10
19/19 - 20s - loss: 868.5936 - loglik: -8.6908e+02 - logprior: 0.9608
Epoch 3/10
19/19 - 19s - loss: 806.3853 - loglik: -8.0520e+02 - logprior: -3.8634e-01
Epoch 4/10
19/19 - 20s - loss: 786.6548 - loglik: -7.8507e+02 - logprior: -7.0300e-01
Epoch 5/10
19/19 - 20s - loss: 783.0078 - loglik: -7.8167e+02 - logprior: -5.3487e-01
Epoch 6/10
19/19 - 20s - loss: 779.3044 - loglik: -7.7797e+02 - logprior: -5.9540e-01
Epoch 7/10
19/19 - 20s - loss: 778.7337 - loglik: -7.7735e+02 - logprior: -6.7013e-01
Epoch 8/10
19/19 - 20s - loss: 779.7253 - loglik: -7.7835e+02 - logprior: -6.9118e-01
Fitted a model with MAP estimate = -775.9001
expansions: [(28, 1), (32, 1), (61, 1), (96, 1), (97, 1), (98, 3), (106, 1), (111, 1), (112, 3), (113, 1), (114, 1), (115, 1), (119, 3), (120, 3), (141, 1), (153, 1), (162, 2), (165, 2), (166, 1), (168, 1), (171, 1), (175, 1), (176, 2), (189, 4), (194, 1), (204, 1), (218, 2), (219, 1), (220, 1), (221, 1), (223, 1), (226, 1), (236, 1), (237, 1), (241, 2), (259, 1), (264, 8), (290, 7), (300, 1), (301, 1), (302, 2), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 808.8148 - loglik: -7.9730e+02 - logprior: -1.1344e+01
Epoch 2/2
19/19 - 26s - loss: 755.4766 - loglik: -7.5204e+02 - logprior: -3.1112e+00
Fitted a model with MAP estimate = -747.0675
expansions: [(0, 2)]
discards: [  0 103 185 207 290 350 351 352 353]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 761.9609 - loglik: -7.5509e+02 - logprior: -6.5997e+00
Epoch 2/2
19/19 - 26s - loss: 739.4713 - loglik: -7.4118e+02 - logprior: 2.2413
Fitted a model with MAP estimate = -734.7116
expansions: [(177, 1), (347, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 31s - loss: 761.1472 - loglik: -7.5090e+02 - logprior: -1.0015e+01
Epoch 2/10
19/19 - 26s - loss: 741.0767 - loglik: -7.4047e+02 - logprior: -1.1477e-01
Epoch 3/10
19/19 - 26s - loss: 730.3524 - loglik: -7.3313e+02 - logprior: 3.4767
Epoch 4/10
19/19 - 26s - loss: 729.3323 - loglik: -7.3256e+02 - logprior: 4.0509
Epoch 5/10
19/19 - 26s - loss: 726.2415 - loglik: -7.2989e+02 - logprior: 4.5025
Epoch 6/10
19/19 - 26s - loss: 725.5387 - loglik: -7.2954e+02 - logprior: 4.8411
Epoch 7/10
19/19 - 26s - loss: 722.6561 - loglik: -7.2695e+02 - logprior: 5.1110
Epoch 8/10
19/19 - 26s - loss: 721.8787 - loglik: -7.2653e+02 - logprior: 5.4384
Epoch 9/10
19/19 - 26s - loss: 722.3162 - loglik: -7.2742e+02 - logprior: 5.8767
Fitted a model with MAP estimate = -720.0921
Time for alignment: 581.9361
Computed alignments with likelihoods: ['-742.4307', '-723.4090', '-720.0921']
Best model has likelihood: -720.0921
time for generating output: 0.4891
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7405348557692307
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faadacab760>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaeba3f640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa8e86cc6d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2c42f3c10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa963dc2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faaaf5c0040>, <__main__.SimpleDirichletPrior object at 0x7fabf7cb7a30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 368.7882 - loglik: -3.1238e+02 - logprior: -5.6344e+01
Epoch 2/10
10/10 - 1s - loss: 291.0978 - loglik: -2.7760e+02 - logprior: -1.3458e+01
Epoch 3/10
10/10 - 1s - loss: 241.3761 - loglik: -2.3529e+02 - logprior: -6.0131e+00
Epoch 4/10
10/10 - 1s - loss: 211.9104 - loglik: -2.0786e+02 - logprior: -3.8995e+00
Epoch 5/10
10/10 - 1s - loss: 201.7700 - loglik: -1.9870e+02 - logprior: -2.8572e+00
Epoch 6/10
10/10 - 1s - loss: 196.9054 - loglik: -1.9437e+02 - logprior: -2.2662e+00
Epoch 7/10
10/10 - 1s - loss: 195.9130 - loglik: -1.9378e+02 - logprior: -1.8453e+00
Epoch 8/10
10/10 - 1s - loss: 194.9081 - loglik: -1.9302e+02 - logprior: -1.6038e+00
Epoch 9/10
10/10 - 1s - loss: 194.0710 - loglik: -1.9239e+02 - logprior: -1.4083e+00
Epoch 10/10
10/10 - 1s - loss: 193.6584 - loglik: -1.9213e+02 - logprior: -1.2406e+00
Fitted a model with MAP estimate = -193.2990
expansions: [(11, 4), (17, 2), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (55, 1), (56, 1), (57, 1), (62, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.0449 - loglik: -1.8753e+02 - logprior: -6.3494e+01
Epoch 2/2
10/10 - 2s - loss: 197.6879 - loglik: -1.7290e+02 - logprior: -2.4694e+01
Fitted a model with MAP estimate = -188.9098
expansions: [(0, 2), (12, 1)]
discards: [ 0 20 21 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.7594 - loglik: -1.7075e+02 - logprior: -4.9995e+01
Epoch 2/2
10/10 - 2s - loss: 178.4837 - loglik: -1.6736e+02 - logprior: -1.1092e+01
Fitted a model with MAP estimate = -172.3751
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 230.2327 - loglik: -1.6969e+02 - logprior: -6.0499e+01
Epoch 2/10
10/10 - 2s - loss: 185.9324 - loglik: -1.6868e+02 - logprior: -1.7149e+01
Epoch 3/10
10/10 - 2s - loss: 172.5862 - loglik: -1.6816e+02 - logprior: -4.2473e+00
Epoch 4/10
10/10 - 2s - loss: 167.6896 - loglik: -1.6772e+02 - logprior: 0.2247
Epoch 5/10
10/10 - 2s - loss: 165.6707 - loglik: -1.6762e+02 - logprior: 2.1662
Epoch 6/10
10/10 - 2s - loss: 164.4092 - loglik: -1.6737e+02 - logprior: 3.1923
Epoch 7/10
10/10 - 2s - loss: 163.8373 - loglik: -1.6746e+02 - logprior: 3.8846
Epoch 8/10
10/10 - 2s - loss: 163.2748 - loglik: -1.6746e+02 - logprior: 4.4538
Epoch 9/10
10/10 - 2s - loss: 162.8702 - loglik: -1.6753e+02 - logprior: 4.9401
Epoch 10/10
10/10 - 2s - loss: 162.3896 - loglik: -1.6744e+02 - logprior: 5.3329
Fitted a model with MAP estimate = -162.0811
Time for alignment: 53.2436
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 368.6662 - loglik: -3.1227e+02 - logprior: -5.6344e+01
Epoch 2/10
10/10 - 1s - loss: 291.4088 - loglik: -2.7792e+02 - logprior: -1.3450e+01
Epoch 3/10
10/10 - 1s - loss: 242.4246 - loglik: -2.3632e+02 - logprior: -6.0571e+00
Epoch 4/10
10/10 - 1s - loss: 215.9807 - loglik: -2.1194e+02 - logprior: -3.9060e+00
Epoch 5/10
10/10 - 1s - loss: 205.3591 - loglik: -2.0215e+02 - logprior: -2.9855e+00
Epoch 6/10
10/10 - 1s - loss: 200.3899 - loglik: -1.9764e+02 - logprior: -2.4898e+00
Epoch 7/10
10/10 - 1s - loss: 197.1493 - loglik: -1.9491e+02 - logprior: -1.9856e+00
Epoch 8/10
10/10 - 1s - loss: 195.5666 - loglik: -1.9357e+02 - logprior: -1.7256e+00
Epoch 9/10
10/10 - 1s - loss: 194.7637 - loglik: -1.9294e+02 - logprior: -1.5297e+00
Epoch 10/10
10/10 - 1s - loss: 194.8402 - loglik: -1.9319e+02 - logprior: -1.3525e+00
Fitted a model with MAP estimate = -193.9227
expansions: [(13, 3), (18, 4), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (58, 1), (62, 1), (76, 1), (77, 3), (78, 1), (80, 1), (83, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.6889 - loglik: -1.8898e+02 - logprior: -6.3595e+01
Epoch 2/2
10/10 - 2s - loss: 199.8871 - loglik: -1.7486e+02 - logprior: -2.4812e+01
Fitted a model with MAP estimate = -189.6408
expansions: [(0, 2), (22, 1), (70, 1)]
discards: [ 0 98]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 217.5951 - loglik: -1.6749e+02 - logprior: -4.9998e+01
Epoch 2/2
10/10 - 2s - loss: 174.9649 - loglik: -1.6383e+02 - logprior: -1.0973e+01
Fitted a model with MAP estimate = -168.6354
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 226.9586 - loglik: -1.6642e+02 - logprior: -6.0525e+01
Epoch 2/10
10/10 - 2s - loss: 183.0565 - loglik: -1.6555e+02 - logprior: -1.7437e+01
Epoch 3/10
10/10 - 2s - loss: 169.4266 - loglik: -1.6504e+02 - logprior: -4.2402e+00
Epoch 4/10
10/10 - 2s - loss: 164.1820 - loglik: -1.6438e+02 - logprior: 0.3722
Epoch 5/10
10/10 - 2s - loss: 162.0577 - loglik: -1.6418e+02 - logprior: 2.3332
Epoch 6/10
10/10 - 2s - loss: 160.9626 - loglik: -1.6410e+02 - logprior: 3.3785
Epoch 7/10
10/10 - 2s - loss: 160.5832 - loglik: -1.6437e+02 - logprior: 4.0556
Epoch 8/10
10/10 - 2s - loss: 159.6070 - loglik: -1.6395e+02 - logprior: 4.6212
Epoch 9/10
10/10 - 2s - loss: 159.4254 - loglik: -1.6424e+02 - logprior: 5.1124
Epoch 10/10
10/10 - 2s - loss: 159.0944 - loglik: -1.6429e+02 - logprior: 5.4975
Fitted a model with MAP estimate = -158.5619
Time for alignment: 53.9143
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.0839 - loglik: -3.1254e+02 - logprior: -5.6353e+01
Epoch 2/10
10/10 - 1s - loss: 290.7909 - loglik: -2.7723e+02 - logprior: -1.3488e+01
Epoch 3/10
10/10 - 1s - loss: 241.3616 - loglik: -2.3522e+02 - logprior: -6.0447e+00
Epoch 4/10
10/10 - 1s - loss: 212.6894 - loglik: -2.0882e+02 - logprior: -3.7000e+00
Epoch 5/10
10/10 - 1s - loss: 202.5220 - loglik: -1.9966e+02 - logprior: -2.6062e+00
Epoch 6/10
10/10 - 1s - loss: 196.9845 - loglik: -1.9461e+02 - logprior: -2.1289e+00
Epoch 7/10
10/10 - 1s - loss: 194.8297 - loglik: -1.9276e+02 - logprior: -1.8088e+00
Epoch 8/10
10/10 - 1s - loss: 193.4716 - loglik: -1.9171e+02 - logprior: -1.4838e+00
Epoch 9/10
10/10 - 1s - loss: 192.9323 - loglik: -1.9143e+02 - logprior: -1.2259e+00
Epoch 10/10
10/10 - 1s - loss: 192.2326 - loglik: -1.9088e+02 - logprior: -1.0653e+00
Fitted a model with MAP estimate = -192.0075
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (56, 5), (62, 1), (78, 4), (81, 1), (83, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.5414 - loglik: -1.8689e+02 - logprior: -6.3640e+01
Epoch 2/2
10/10 - 2s - loss: 199.9039 - loglik: -1.7502e+02 - logprior: -2.4795e+01
Fitted a model with MAP estimate = -191.0557
expansions: [(0, 2), (66, 1)]
discards: [ 0 20 97]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 221.6201 - loglik: -1.7151e+02 - logprior: -5.0102e+01
Epoch 2/2
10/10 - 2s - loss: 179.7644 - loglik: -1.6840e+02 - logprior: -1.1285e+01
Fitted a model with MAP estimate = -173.4825
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.3434 - loglik: -1.7081e+02 - logprior: -6.0436e+01
Epoch 2/10
10/10 - 2s - loss: 186.8159 - loglik: -1.6979e+02 - logprior: -1.6914e+01
Epoch 3/10
10/10 - 2s - loss: 173.9120 - loglik: -1.6944e+02 - logprior: -4.2899e+00
Epoch 4/10
10/10 - 1s - loss: 168.9680 - loglik: -1.6883e+02 - logprior: 0.0632
Epoch 5/10
10/10 - 1s - loss: 167.1113 - loglik: -1.6890e+02 - logprior: 2.0024
Epoch 6/10
10/10 - 2s - loss: 165.3804 - loglik: -1.6820e+02 - logprior: 3.0520
Epoch 7/10
10/10 - 2s - loss: 165.1110 - loglik: -1.6861e+02 - logprior: 3.7536
Epoch 8/10
10/10 - 2s - loss: 164.3539 - loglik: -1.6840e+02 - logprior: 4.3164
Epoch 9/10
10/10 - 2s - loss: 163.6691 - loglik: -1.6820e+02 - logprior: 4.8095
Epoch 10/10
10/10 - 2s - loss: 163.7538 - loglik: -1.6867e+02 - logprior: 5.2023
Fitted a model with MAP estimate = -162.9391
Time for alignment: 51.9846
Computed alignments with likelihoods: ['-162.0811', '-158.5619', '-162.9391']
Best model has likelihood: -158.5619
time for generating output: 0.1767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9311943393924607
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab161d0e50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa8cdfaf10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa908587f70>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86c29b190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faac1184dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac33717cd0>, <__main__.SimpleDirichletPrior object at 0x7fab0513daf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.6681 - loglik: -3.8855e+02 - logprior: -8.0828e+00
Epoch 2/10
13/13 - 3s - loss: 353.3172 - loglik: -3.5128e+02 - logprior: -1.8574e+00
Epoch 3/10
13/13 - 3s - loss: 321.9280 - loglik: -3.1984e+02 - logprior: -1.6328e+00
Epoch 4/10
13/13 - 3s - loss: 310.2740 - loglik: -3.0772e+02 - logprior: -1.8974e+00
Epoch 5/10
13/13 - 3s - loss: 305.1858 - loglik: -3.0279e+02 - logprior: -1.8019e+00
Epoch 6/10
13/13 - 3s - loss: 303.7061 - loglik: -3.0148e+02 - logprior: -1.6927e+00
Epoch 7/10
13/13 - 3s - loss: 302.9579 - loglik: -3.0079e+02 - logprior: -1.7110e+00
Epoch 8/10
13/13 - 3s - loss: 302.2530 - loglik: -3.0014e+02 - logprior: -1.6871e+00
Epoch 9/10
13/13 - 3s - loss: 301.9734 - loglik: -2.9990e+02 - logprior: -1.6583e+00
Epoch 10/10
13/13 - 3s - loss: 301.4890 - loglik: -2.9942e+02 - logprior: -1.6611e+00
Fitted a model with MAP estimate = -301.0775
expansions: [(12, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (32, 1), (52, 1), (55, 1), (59, 3), (64, 2), (80, 1), (82, 1), (92, 7), (99, 1), (100, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 323.6973 - loglik: -3.1405e+02 - logprior: -9.5083e+00
Epoch 2/2
13/13 - 3s - loss: 302.2330 - loglik: -2.9753e+02 - logprior: -4.4171e+00
Fitted a model with MAP estimate = -296.7188
expansions: [(0, 2)]
discards: [  0  22  30  34  40  77  84 119 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 303.8005 - loglik: -2.9663e+02 - logprior: -7.1062e+00
Epoch 2/2
13/13 - 3s - loss: 292.1612 - loglik: -2.9018e+02 - logprior: -1.8017e+00
Fitted a model with MAP estimate = -289.8862
expansions: []
discards: [ 0 74]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.4167 - loglik: -2.9555e+02 - logprior: -8.7775e+00
Epoch 2/10
13/13 - 3s - loss: 294.4430 - loglik: -2.9173e+02 - logprior: -2.5100e+00
Epoch 3/10
13/13 - 3s - loss: 290.2439 - loglik: -2.8879e+02 - logprior: -1.0651e+00
Epoch 4/10
13/13 - 3s - loss: 288.5639 - loglik: -2.8731e+02 - logprior: -7.5529e-01
Epoch 5/10
13/13 - 3s - loss: 287.4862 - loglik: -2.8643e+02 - logprior: -5.6162e-01
Epoch 6/10
13/13 - 3s - loss: 287.7119 - loglik: -2.8678e+02 - logprior: -4.9947e-01
Fitted a model with MAP estimate = -286.5072
Time for alignment: 88.4085
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 396.7314 - loglik: -3.8838e+02 - logprior: -8.0801e+00
Epoch 2/10
13/13 - 3s - loss: 354.6110 - loglik: -3.5249e+02 - logprior: -1.8365e+00
Epoch 3/10
13/13 - 3s - loss: 325.5912 - loglik: -3.2349e+02 - logprior: -1.5379e+00
Epoch 4/10
13/13 - 3s - loss: 312.3142 - loglik: -3.0974e+02 - logprior: -1.8146e+00
Epoch 5/10
13/13 - 3s - loss: 307.7910 - loglik: -3.0527e+02 - logprior: -1.7681e+00
Epoch 6/10
13/13 - 3s - loss: 304.6317 - loglik: -3.0237e+02 - logprior: -1.6779e+00
Epoch 7/10
13/13 - 3s - loss: 304.7233 - loglik: -3.0253e+02 - logprior: -1.6925e+00
Fitted a model with MAP estimate = -303.2812
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (82, 2), (83, 1), (84, 2), (93, 3), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 326.9015 - loglik: -3.1738e+02 - logprior: -9.4853e+00
Epoch 2/2
13/13 - 3s - loss: 303.8782 - loglik: -2.9941e+02 - logprior: -4.4038e+00
Fitted a model with MAP estimate = -298.5274
expansions: [(0, 2), (130, 1)]
discards: [ 0 22 30 34 40 76 77 84]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 302.0451 - loglik: -2.9477e+02 - logprior: -7.1402e+00
Epoch 2/2
13/13 - 3s - loss: 292.3051 - loglik: -2.9010e+02 - logprior: -1.8382e+00
Fitted a model with MAP estimate = -289.0501
expansions: []
discards: [  0 101 116]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.4281 - loglik: -2.9552e+02 - logprior: -8.8342e+00
Epoch 2/10
13/13 - 3s - loss: 294.5607 - loglik: -2.9177e+02 - logprior: -2.5942e+00
Epoch 3/10
13/13 - 3s - loss: 289.3791 - loglik: -2.8788e+02 - logprior: -1.1278e+00
Epoch 4/10
13/13 - 3s - loss: 288.2839 - loglik: -2.8699e+02 - logprior: -8.0129e-01
Epoch 5/10
13/13 - 3s - loss: 287.5256 - loglik: -2.8637e+02 - logprior: -6.2128e-01
Epoch 6/10
13/13 - 3s - loss: 286.4144 - loglik: -2.8535e+02 - logprior: -5.4659e-01
Epoch 7/10
13/13 - 3s - loss: 286.5013 - loglik: -2.8549e+02 - logprior: -5.1788e-01
Fitted a model with MAP estimate = -285.7671
Time for alignment: 83.3729
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.9594 - loglik: -3.8886e+02 - logprior: -8.0874e+00
Epoch 2/10
13/13 - 3s - loss: 354.2930 - loglik: -3.5236e+02 - logprior: -1.8433e+00
Epoch 3/10
13/13 - 3s - loss: 326.1840 - loglik: -3.2433e+02 - logprior: -1.5323e+00
Epoch 4/10
13/13 - 3s - loss: 312.2849 - loglik: -3.0992e+02 - logprior: -1.7903e+00
Epoch 5/10
13/13 - 3s - loss: 308.1176 - loglik: -3.0580e+02 - logprior: -1.7098e+00
Epoch 6/10
13/13 - 3s - loss: 305.0492 - loglik: -3.0287e+02 - logprior: -1.6198e+00
Epoch 7/10
13/13 - 3s - loss: 304.6744 - loglik: -3.0252e+02 - logprior: -1.6545e+00
Epoch 8/10
13/13 - 3s - loss: 304.2617 - loglik: -3.0214e+02 - logprior: -1.6601e+00
Epoch 9/10
13/13 - 3s - loss: 303.3284 - loglik: -3.0125e+02 - logprior: -1.6336e+00
Epoch 10/10
13/13 - 3s - loss: 302.7816 - loglik: -3.0074e+02 - logprior: -1.6179e+00
Fitted a model with MAP estimate = -302.5730
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 2), (26, 2), (27, 2), (28, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (82, 2), (83, 1), (84, 2), (93, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 324.6968 - loglik: -3.1517e+02 - logprior: -9.4950e+00
Epoch 2/2
13/13 - 3s - loss: 302.8790 - loglik: -2.9830e+02 - logprior: -4.3829e+00
Fitted a model with MAP estimate = -298.0176
expansions: [(0, 2)]
discards: [ 0 22 32 35 82]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 304.0766 - loglik: -2.9686e+02 - logprior: -7.1304e+00
Epoch 2/2
13/13 - 3s - loss: 293.4069 - loglik: -2.9150e+02 - logprior: -1.7722e+00
Fitted a model with MAP estimate = -291.4622
expansions: []
discards: [  0  73 102]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 304.9067 - loglik: -2.9602e+02 - logprior: -8.7721e+00
Epoch 2/10
13/13 - 3s - loss: 294.9362 - loglik: -2.9221e+02 - logprior: -2.5124e+00
Epoch 3/10
13/13 - 3s - loss: 291.3980 - loglik: -2.8995e+02 - logprior: -1.0590e+00
Epoch 4/10
13/13 - 3s - loss: 288.9149 - loglik: -2.8766e+02 - logprior: -7.4518e-01
Epoch 5/10
13/13 - 3s - loss: 289.0003 - loglik: -2.8791e+02 - logprior: -5.4527e-01
Fitted a model with MAP estimate = -287.8113
Time for alignment: 85.9365
Computed alignments with likelihoods: ['-286.5072', '-285.7671', '-287.8113']
Best model has likelihood: -285.7671
time for generating output: 0.2218
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8580329327992879
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaf3ccbf40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa9f385a30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faab8707d30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faae2eadcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac444ec5e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabcd642340>, <__main__.SimpleDirichletPrior object at 0x7fa9087e41c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.5298 - loglik: -2.1387e+02 - logprior: -8.5991e+00
Epoch 2/10
13/13 - 2s - loss: 173.1238 - loglik: -1.7082e+02 - logprior: -2.2442e+00
Epoch 3/10
13/13 - 2s - loss: 143.4733 - loglik: -1.4155e+02 - logprior: -1.8556e+00
Epoch 4/10
13/13 - 2s - loss: 135.4332 - loglik: -1.3352e+02 - logprior: -1.7888e+00
Epoch 5/10
13/13 - 2s - loss: 132.9629 - loglik: -1.3103e+02 - logprior: -1.6791e+00
Epoch 6/10
13/13 - 2s - loss: 132.5361 - loglik: -1.3066e+02 - logprior: -1.6587e+00
Epoch 7/10
13/13 - 2s - loss: 131.8379 - loglik: -1.3002e+02 - logprior: -1.6125e+00
Epoch 8/10
13/13 - 2s - loss: 131.6566 - loglik: -1.2984e+02 - logprior: -1.6068e+00
Epoch 9/10
13/13 - 2s - loss: 131.4521 - loglik: -1.2963e+02 - logprior: -1.5973e+00
Epoch 10/10
13/13 - 2s - loss: 131.2553 - loglik: -1.2946e+02 - logprior: -1.5726e+00
Fitted a model with MAP estimate = -131.0061
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 138.3345 - loglik: -1.2804e+02 - logprior: -1.0231e+01
Epoch 2/2
13/13 - 2s - loss: 121.7643 - loglik: -1.1845e+02 - logprior: -3.1120e+00
Fitted a model with MAP estimate = -118.5778
expansions: [(0, 2)]
discards: [43 47 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 127.5559 - loglik: -1.1747e+02 - logprior: -1.0059e+01
Epoch 2/2
13/13 - 2s - loss: 118.4607 - loglik: -1.1517e+02 - logprior: -3.2030e+00
Fitted a model with MAP estimate = -116.1513
expansions: [(63, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 122.2935 - loglik: -1.1430e+02 - logprior: -7.9294e+00
Epoch 2/10
13/13 - 2s - loss: 114.1611 - loglik: -1.1167e+02 - logprior: -2.2993e+00
Epoch 3/10
13/13 - 2s - loss: 112.3211 - loglik: -1.1029e+02 - logprior: -1.8054e+00
Epoch 4/10
13/13 - 2s - loss: 111.8136 - loglik: -1.1013e+02 - logprior: -1.4776e+00
Epoch 5/10
13/13 - 2s - loss: 111.1425 - loglik: -1.0951e+02 - logprior: -1.4122e+00
Epoch 6/10
13/13 - 2s - loss: 110.9146 - loglik: -1.0936e+02 - logprior: -1.3426e+00
Epoch 7/10
13/13 - 2s - loss: 110.3986 - loglik: -1.0886e+02 - logprior: -1.3198e+00
Epoch 8/10
13/13 - 2s - loss: 110.3602 - loglik: -1.0886e+02 - logprior: -1.2723e+00
Epoch 9/10
13/13 - 2s - loss: 110.2172 - loglik: -1.0875e+02 - logprior: -1.2316e+00
Epoch 10/10
13/13 - 2s - loss: 110.3692 - loglik: -1.0893e+02 - logprior: -1.1933e+00
Fitted a model with MAP estimate = -109.8590
Time for alignment: 70.7271
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.7589 - loglik: -2.1412e+02 - logprior: -8.6004e+00
Epoch 2/10
13/13 - 2s - loss: 174.2752 - loglik: -1.7197e+02 - logprior: -2.2505e+00
Epoch 3/10
13/13 - 2s - loss: 145.6453 - loglik: -1.4371e+02 - logprior: -1.8646e+00
Epoch 4/10
13/13 - 2s - loss: 136.0037 - loglik: -1.3414e+02 - logprior: -1.7680e+00
Epoch 5/10
13/13 - 2s - loss: 133.6291 - loglik: -1.3175e+02 - logprior: -1.6466e+00
Epoch 6/10
13/13 - 2s - loss: 132.5758 - loglik: -1.3070e+02 - logprior: -1.6721e+00
Epoch 7/10
13/13 - 2s - loss: 132.0469 - loglik: -1.3024e+02 - logprior: -1.6119e+00
Epoch 8/10
13/13 - 2s - loss: 131.9418 - loglik: -1.3013e+02 - logprior: -1.6018e+00
Epoch 9/10
13/13 - 2s - loss: 131.8111 - loglik: -1.3002e+02 - logprior: -1.5747e+00
Epoch 10/10
13/13 - 2s - loss: 131.6149 - loglik: -1.2982e+02 - logprior: -1.5749e+00
Fitted a model with MAP estimate = -131.3689
expansions: [(0, 4), (13, 1), (16, 1), (35, 1), (36, 1), (37, 3), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 138.4824 - loglik: -1.2810e+02 - logprior: -1.0253e+01
Epoch 2/2
13/13 - 2s - loss: 122.9791 - loglik: -1.1966e+02 - logprior: -3.0825e+00
Fitted a model with MAP estimate = -119.8554
expansions: [(0, 2)]
discards: [46 49]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 128.3187 - loglik: -1.1809e+02 - logprior: -1.0066e+01
Epoch 2/2
13/13 - 2s - loss: 119.2588 - loglik: -1.1580e+02 - logprior: -3.2528e+00
Fitted a model with MAP estimate = -117.0096
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 123.6726 - loglik: -1.1558e+02 - logprior: -7.9722e+00
Epoch 2/10
13/13 - 2s - loss: 117.1619 - loglik: -1.1470e+02 - logprior: -2.2742e+00
Epoch 3/10
13/13 - 2s - loss: 116.2462 - loglik: -1.1428e+02 - logprior: -1.7482e+00
Epoch 4/10
13/13 - 2s - loss: 115.2021 - loglik: -1.1354e+02 - logprior: -1.3960e+00
Epoch 5/10
13/13 - 2s - loss: 115.2234 - loglik: -1.1364e+02 - logprior: -1.3225e+00
Fitted a model with MAP estimate = -114.5742
Time for alignment: 60.6616
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.7721 - loglik: -2.1415e+02 - logprior: -8.6012e+00
Epoch 2/10
13/13 - 2s - loss: 172.8896 - loglik: -1.7059e+02 - logprior: -2.2420e+00
Epoch 3/10
13/13 - 2s - loss: 144.9897 - loglik: -1.4308e+02 - logprior: -1.8199e+00
Epoch 4/10
13/13 - 2s - loss: 136.4847 - loglik: -1.3467e+02 - logprior: -1.7087e+00
Epoch 5/10
13/13 - 2s - loss: 133.6949 - loglik: -1.3185e+02 - logprior: -1.6208e+00
Epoch 6/10
13/13 - 2s - loss: 132.5275 - loglik: -1.3068e+02 - logprior: -1.6341e+00
Epoch 7/10
13/13 - 2s - loss: 132.0216 - loglik: -1.3024e+02 - logprior: -1.5829e+00
Epoch 8/10
13/13 - 2s - loss: 132.0934 - loglik: -1.3030e+02 - logprior: -1.5778e+00
Fitted a model with MAP estimate = -131.6136
expansions: [(0, 4), (13, 1), (16, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 138.1405 - loglik: -1.2796e+02 - logprior: -1.0136e+01
Epoch 2/2
13/13 - 2s - loss: 122.3151 - loglik: -1.1915e+02 - logprior: -2.9952e+00
Fitted a model with MAP estimate = -119.2131
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 127.9228 - loglik: -1.1774e+02 - logprior: -1.0053e+01
Epoch 2/2
13/13 - 2s - loss: 118.3413 - loglik: -1.1507e+02 - logprior: -3.1611e+00
Fitted a model with MAP estimate = -116.2386
expansions: [(64, 3)]
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 122.0141 - loglik: -1.1401e+02 - logprior: -7.9226e+00
Epoch 2/10
13/13 - 2s - loss: 114.7782 - loglik: -1.1226e+02 - logprior: -2.3140e+00
Epoch 3/10
13/13 - 2s - loss: 112.5480 - loglik: -1.1052e+02 - logprior: -1.8124e+00
Epoch 4/10
13/13 - 2s - loss: 111.7142 - loglik: -1.1002e+02 - logprior: -1.4915e+00
Epoch 5/10
13/13 - 2s - loss: 110.9750 - loglik: -1.0934e+02 - logprior: -1.4249e+00
Epoch 6/10
13/13 - 2s - loss: 110.8222 - loglik: -1.0925e+02 - logprior: -1.3643e+00
Epoch 7/10
13/13 - 2s - loss: 110.6175 - loglik: -1.0907e+02 - logprior: -1.3335e+00
Epoch 8/10
13/13 - 2s - loss: 110.2496 - loglik: -1.0873e+02 - logprior: -1.2882e+00
Epoch 9/10
13/13 - 2s - loss: 110.5487 - loglik: -1.0906e+02 - logprior: -1.2545e+00
Fitted a model with MAP estimate = -109.9487
Time for alignment: 64.1733
Computed alignments with likelihoods: ['-109.8590', '-114.5742', '-109.9487']
Best model has likelihood: -109.8590
time for generating output: 0.2403
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5069627399322544
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa86d5c79a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac1154d430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac223fcdf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa29c417c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8f0199490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa82a9b7f0>, <__main__.SimpleDirichletPrior object at 0x7fabcd224670>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.7729 - loglik: -5.8880e+01 - logprior: -8.6223e-01
Epoch 2/10
41/41 - 2s - loss: 46.2632 - loglik: -4.5211e+01 - logprior: -9.0330e-01
Epoch 3/10
41/41 - 2s - loss: 45.4570 - loglik: -4.4450e+01 - logprior: -8.8527e-01
Epoch 4/10
41/41 - 2s - loss: 45.4568 - loglik: -4.4439e+01 - logprior: -8.7582e-01
Epoch 5/10
41/41 - 2s - loss: 45.0633 - loglik: -4.4024e+01 - logprior: -8.7259e-01
Epoch 6/10
41/41 - 2s - loss: 45.0373 - loglik: -4.3986e+01 - logprior: -8.7110e-01
Epoch 7/10
41/41 - 2s - loss: 44.9088 - loglik: -4.3847e+01 - logprior: -8.6903e-01
Epoch 8/10
41/41 - 2s - loss: 44.8281 - loglik: -4.3764e+01 - logprior: -8.6739e-01
Epoch 9/10
41/41 - 2s - loss: 44.7756 - loglik: -4.3708e+01 - logprior: -8.6828e-01
Epoch 10/10
41/41 - 2s - loss: 44.8347 - loglik: -4.3767e+01 - logprior: -8.6543e-01
Fitted a model with MAP estimate = -44.7783
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 45.0797 - loglik: -4.3859e+01 - logprior: -1.0701e+00
Epoch 2/2
41/41 - 2s - loss: 43.5541 - loglik: -4.2589e+01 - logprior: -8.5279e-01
Fitted a model with MAP estimate = -42.7006
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1390 - loglik: -4.2971e+01 - logprior: -1.0497e+00
Epoch 2/2
41/41 - 2s - loss: 43.4142 - loglik: -4.2465e+01 - logprior: -8.3547e-01
Fitted a model with MAP estimate = -42.7029
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7663 - loglik: -4.1983e+01 - logprior: -6.7950e-01
Epoch 2/10
58/58 - 2s - loss: 42.2609 - loglik: -4.1564e+01 - logprior: -5.8265e-01
Epoch 3/10
58/58 - 2s - loss: 42.2739 - loglik: -4.1583e+01 - logprior: -5.7439e-01
Fitted a model with MAP estimate = -41.7843
Time for alignment: 79.4077
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 6s - loss: 59.7060 - loglik: -5.8853e+01 - logprior: -8.1494e-01
Epoch 2/10
41/41 - 2s - loss: 46.3799 - loglik: -4.5538e+01 - logprior: -7.3988e-01
Epoch 3/10
41/41 - 2s - loss: 45.4461 - loglik: -4.4651e+01 - logprior: -6.9088e-01
Epoch 4/10
41/41 - 2s - loss: 45.4105 - loglik: -4.4600e+01 - logprior: -6.8913e-01
Epoch 5/10
41/41 - 2s - loss: 45.0668 - loglik: -4.4241e+01 - logprior: -6.8438e-01
Epoch 6/10
41/41 - 2s - loss: 45.0763 - loglik: -4.4233e+01 - logprior: -6.8343e-01
Fitted a model with MAP estimate = -44.5714
expansions: [(4, 1), (8, 2), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 46.9165 - loglik: -4.5674e+01 - logprior: -1.1143e+00
Epoch 2/2
41/41 - 1s - loss: 43.5968 - loglik: -4.2628e+01 - logprior: -8.6239e-01
Fitted a model with MAP estimate = -42.6372
expansions: []
discards: [ 8 11]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.0787 - loglik: -4.2949e+01 - logprior: -1.0508e+00
Epoch 2/2
41/41 - 2s - loss: 43.5962 - loglik: -4.2648e+01 - logprior: -8.3601e-01
Fitted a model with MAP estimate = -42.6663
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 7s - loss: 42.7627 - loglik: -4.1973e+01 - logprior: -6.8031e-01
Epoch 2/10
58/58 - 2s - loss: 42.3042 - loglik: -4.1607e+01 - logprior: -5.8281e-01
Epoch 3/10
58/58 - 2s - loss: 42.1902 - loglik: -4.1502e+01 - logprior: -5.7476e-01
Epoch 4/10
58/58 - 2s - loss: 41.8030 - loglik: -4.1067e+01 - logprior: -5.7284e-01
Epoch 5/10
58/58 - 2s - loss: 41.8498 - loglik: -4.1106e+01 - logprior: -5.7046e-01
Fitted a model with MAP estimate = -41.5286
Time for alignment: 78.4211
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.9688 - loglik: -5.9111e+01 - logprior: -8.2542e-01
Epoch 2/10
41/41 - 2s - loss: 46.5542 - loglik: -4.5700e+01 - logprior: -7.4511e-01
Epoch 3/10
41/41 - 2s - loss: 45.4210 - loglik: -4.4613e+01 - logprior: -6.9284e-01
Epoch 4/10
41/41 - 2s - loss: 45.4219 - loglik: -4.4602e+01 - logprior: -6.8927e-01
Fitted a model with MAP estimate = -44.6251
expansions: [(4, 1), (8, 2), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 47.0668 - loglik: -4.5879e+01 - logprior: -1.1280e+00
Epoch 2/2
41/41 - 1s - loss: 43.5817 - loglik: -4.2599e+01 - logprior: -8.7694e-01
Fitted a model with MAP estimate = -42.6375
expansions: []
discards: [ 8 12 14]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.1849 - loglik: -4.3064e+01 - logprior: -1.0518e+00
Epoch 2/2
41/41 - 2s - loss: 43.5245 - loglik: -4.2587e+01 - logprior: -8.3660e-01
Fitted a model with MAP estimate = -42.6331
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 7s - loss: 42.7062 - loglik: -4.1945e+01 - logprior: -6.8110e-01
Epoch 2/10
58/58 - 2s - loss: 42.3626 - loglik: -4.1672e+01 - logprior: -5.8226e-01
Epoch 3/10
58/58 - 2s - loss: 42.2236 - loglik: -4.1539e+01 - logprior: -5.7651e-01
Epoch 4/10
58/58 - 3s - loss: 41.8850 - loglik: -4.1152e+01 - logprior: -5.7342e-01
Epoch 5/10
58/58 - 2s - loss: 41.8065 - loglik: -4.1060e+01 - logprior: -5.7088e-01
Epoch 6/10
58/58 - 2s - loss: 41.8339 - loglik: -4.1085e+01 - logprior: -5.7079e-01
Fitted a model with MAP estimate = -41.4287
Time for alignment: 75.0737
Computed alignments with likelihoods: ['-41.7843', '-41.5286', '-41.4287']
Best model has likelihood: -41.4287
time for generating output: 0.1005
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faac97a6a30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86e11ae50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa2ad100ee0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa963240d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa83489c40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faba1af1100>, <__main__.SimpleDirichletPrior object at 0x7fa86e23e1f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.1916 - loglik: -1.5686e+02 - logprior: -1.2426e+02
Epoch 2/10
10/10 - 1s - loss: 169.0318 - loglik: -1.3511e+02 - logprior: -3.3889e+01
Epoch 3/10
10/10 - 1s - loss: 134.1340 - loglik: -1.1797e+02 - logprior: -1.6135e+01
Epoch 4/10
10/10 - 1s - loss: 118.1915 - loglik: -1.0846e+02 - logprior: -9.6982e+00
Epoch 5/10
10/10 - 1s - loss: 109.8362 - loglik: -1.0360e+02 - logprior: -6.1530e+00
Epoch 6/10
10/10 - 1s - loss: 105.8277 - loglik: -1.0149e+02 - logprior: -4.0998e+00
Epoch 7/10
10/10 - 1s - loss: 104.0522 - loglik: -1.0099e+02 - logprior: -2.7886e+00
Epoch 8/10
10/10 - 1s - loss: 103.0738 - loglik: -1.0092e+02 - logprior: -1.9223e+00
Epoch 9/10
10/10 - 1s - loss: 102.4812 - loglik: -1.0090e+02 - logprior: -1.3408e+00
Epoch 10/10
10/10 - 1s - loss: 102.0889 - loglik: -1.0092e+02 - logprior: -8.9633e-01
Fitted a model with MAP estimate = -101.6567
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 2), (25, 1), (31, 2), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.5703 - loglik: -1.0147e+02 - logprior: -1.3906e+02
Epoch 2/2
10/10 - 1s - loss: 152.5436 - loglik: -9.5072e+01 - logprior: -5.7391e+01
Fitted a model with MAP estimate = -137.8559
expansions: [(0, 2)]
discards: [ 0  8 18 21]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.1819 - loglik: -9.2295e+01 - logprior: -1.1177e+02
Epoch 2/2
10/10 - 1s - loss: 121.0520 - loglik: -9.0825e+01 - logprior: -3.0104e+01
Fitted a model with MAP estimate = -108.6016
expansions: []
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 222.9479 - loglik: -9.3395e+01 - logprior: -1.2949e+02
Epoch 2/10
10/10 - 1s - loss: 129.1836 - loglik: -9.1867e+01 - logprior: -3.7223e+01
Epoch 3/10
10/10 - 1s - loss: 105.8660 - loglik: -9.1221e+01 - logprior: -1.4452e+01
Epoch 4/10
10/10 - 1s - loss: 97.7042 - loglik: -9.1220e+01 - logprior: -6.2389e+00
Epoch 5/10
10/10 - 1s - loss: 93.6893 - loglik: -9.1335e+01 - logprior: -2.0961e+00
Epoch 6/10
10/10 - 1s - loss: 91.4772 - loglik: -9.1440e+01 - logprior: 0.2492
Epoch 7/10
10/10 - 1s - loss: 90.1813 - loglik: -9.1519e+01 - logprior: 1.6503
Epoch 8/10
10/10 - 1s - loss: 89.3387 - loglik: -9.1582e+01 - logprior: 2.5664
Epoch 9/10
10/10 - 1s - loss: 88.7364 - loglik: -9.1657e+01 - logprior: 3.2464
Epoch 10/10
10/10 - 1s - loss: 88.2557 - loglik: -9.1732e+01 - logprior: 3.8031
Fitted a model with MAP estimate = -87.6943
Time for alignment: 32.9700
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.1998 - loglik: -1.5681e+02 - logprior: -1.2426e+02
Epoch 2/10
10/10 - 1s - loss: 169.1376 - loglik: -1.3518e+02 - logprior: -3.3891e+01
Epoch 3/10
10/10 - 1s - loss: 134.1717 - loglik: -1.1800e+02 - logprior: -1.6136e+01
Epoch 4/10
10/10 - 1s - loss: 118.2789 - loglik: -1.0853e+02 - logprior: -9.7176e+00
Epoch 5/10
10/10 - 1s - loss: 109.9532 - loglik: -1.0371e+02 - logprior: -6.1703e+00
Epoch 6/10
10/10 - 1s - loss: 105.7268 - loglik: -1.0145e+02 - logprior: -4.0604e+00
Epoch 7/10
10/10 - 1s - loss: 103.5525 - loglik: -1.0051e+02 - logprior: -2.7303e+00
Epoch 8/10
10/10 - 1s - loss: 101.8036 - loglik: -9.9644e+01 - logprior: -1.9005e+00
Epoch 9/10
10/10 - 1s - loss: 100.8651 - loglik: -9.9318e+01 - logprior: -1.3014e+00
Epoch 10/10
10/10 - 1s - loss: 100.4097 - loglik: -9.9258e+01 - logprior: -8.7714e-01
Fitted a model with MAP estimate = -99.9358
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 239.7697 - loglik: -1.0045e+02 - logprior: -1.3917e+02
Epoch 2/2
10/10 - 1s - loss: 151.8628 - loglik: -9.4030e+01 - logprior: -5.7727e+01
Fitted a model with MAP estimate = -137.2909
expansions: [(0, 2)]
discards: [ 0  8 18 21 30 41]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.7230 - loglik: -9.1756e+01 - logprior: -1.1185e+02
Epoch 2/2
10/10 - 1s - loss: 120.3631 - loglik: -9.0172e+01 - logprior: -3.0103e+01
Fitted a model with MAP estimate = -107.8905
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.6886 - loglik: -9.1855e+01 - logprior: -1.2972e+02
Epoch 2/10
10/10 - 1s - loss: 128.7401 - loglik: -9.1269e+01 - logprior: -3.7344e+01
Epoch 3/10
10/10 - 1s - loss: 105.7523 - loglik: -9.1156e+01 - logprior: -1.4396e+01
Epoch 4/10
10/10 - 1s - loss: 97.6262 - loglik: -9.1246e+01 - logprior: -6.1333e+00
Epoch 5/10
10/10 - 1s - loss: 93.6490 - loglik: -9.1405e+01 - logprior: -1.9804e+00
Epoch 6/10
10/10 - 1s - loss: 91.4402 - loglik: -9.1499e+01 - logprior: 0.3576
Epoch 7/10
10/10 - 1s - loss: 90.1393 - loglik: -9.1575e+01 - logprior: 1.7553
Epoch 8/10
10/10 - 1s - loss: 89.3025 - loglik: -9.1661e+01 - logprior: 2.6810
Epoch 9/10
10/10 - 1s - loss: 88.7012 - loglik: -9.1745e+01 - logprior: 3.3678
Epoch 10/10
10/10 - 1s - loss: 88.2257 - loglik: -9.1825e+01 - logprior: 3.9264
Fitted a model with MAP estimate = -87.6676
Time for alignment: 32.2467
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.1552 - loglik: -1.5686e+02 - logprior: -1.2426e+02
Epoch 2/10
10/10 - 1s - loss: 169.0211 - loglik: -1.3513e+02 - logprior: -3.3874e+01
Epoch 3/10
10/10 - 1s - loss: 134.1014 - loglik: -1.1797e+02 - logprior: -1.6110e+01
Epoch 4/10
10/10 - 1s - loss: 118.1703 - loglik: -1.0845e+02 - logprior: -9.6831e+00
Epoch 5/10
10/10 - 1s - loss: 109.7192 - loglik: -1.0343e+02 - logprior: -6.1682e+00
Epoch 6/10
10/10 - 1s - loss: 105.7837 - loglik: -1.0142e+02 - logprior: -4.1036e+00
Epoch 7/10
10/10 - 1s - loss: 104.0133 - loglik: -1.0098e+02 - logprior: -2.7693e+00
Epoch 8/10
10/10 - 1s - loss: 103.0017 - loglik: -1.0085e+02 - logprior: -1.9161e+00
Epoch 9/10
10/10 - 1s - loss: 102.3923 - loglik: -1.0080e+02 - logprior: -1.3318e+00
Epoch 10/10
10/10 - 1s - loss: 101.9734 - loglik: -1.0079e+02 - logprior: -8.9243e-01
Fitted a model with MAP estimate = -101.5192
expansions: [(9, 1), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 240.4133 - loglik: -1.0115e+02 - logprior: -1.3915e+02
Epoch 2/2
10/10 - 1s - loss: 152.3671 - loglik: -9.4859e+01 - logprior: -5.7410e+01
Fitted a model with MAP estimate = -137.2390
expansions: [(0, 2)]
discards: [ 0 17 20 29]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.0664 - loglik: -9.1233e+01 - logprior: -1.1180e+02
Epoch 2/2
10/10 - 1s - loss: 119.5611 - loglik: -8.9229e+01 - logprior: -3.0208e+01
Fitted a model with MAP estimate = -107.0501
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 221.5386 - loglik: -9.2320e+01 - logprior: -1.2919e+02
Epoch 2/10
10/10 - 1s - loss: 128.4148 - loglik: -9.1415e+01 - logprior: -3.6958e+01
Epoch 3/10
10/10 - 1s - loss: 105.8043 - loglik: -9.1298e+01 - logprior: -1.4374e+01
Epoch 4/10
10/10 - 1s - loss: 97.7600 - loglik: -9.1380e+01 - logprior: -6.1828e+00
Epoch 5/10
10/10 - 1s - loss: 93.7813 - loglik: -9.1484e+01 - logprior: -2.0707e+00
Epoch 6/10
10/10 - 1s - loss: 91.5386 - loglik: -9.1530e+01 - logprior: 0.2517
Epoch 7/10
10/10 - 1s - loss: 90.2089 - loglik: -9.1565e+01 - logprior: 1.6456
Epoch 8/10
10/10 - 1s - loss: 89.3585 - loglik: -9.1619e+01 - logprior: 2.5641
Epoch 9/10
10/10 - 1s - loss: 88.7429 - loglik: -9.1664e+01 - logprior: 3.2369
Epoch 10/10
10/10 - 1s - loss: 88.2608 - loglik: -9.1727e+01 - logprior: 3.7906
Fitted a model with MAP estimate = -87.7065
Time for alignment: 31.9492
Computed alignments with likelihoods: ['-87.6943', '-87.6676', '-87.7065']
Best model has likelihood: -87.6676
time for generating output: 0.1302
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8782660332541568
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab1eac2d90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa29c799100>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabc4cc7250>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabc4cc7e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5dccbfa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa2c4420a30>, <__main__.SimpleDirichletPrior object at 0x7fa2b496ecd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.4645 - loglik: -1.9233e+02 - logprior: -3.1244e+00
Epoch 2/10
19/19 - 2s - loss: 156.3438 - loglik: -1.5508e+02 - logprior: -1.2332e+00
Epoch 3/10
19/19 - 2s - loss: 143.0861 - loglik: -1.4160e+02 - logprior: -1.2933e+00
Epoch 4/10
19/19 - 2s - loss: 141.0187 - loglik: -1.3962e+02 - logprior: -1.2599e+00
Epoch 5/10
19/19 - 2s - loss: 140.2892 - loglik: -1.3897e+02 - logprior: -1.2067e+00
Epoch 6/10
19/19 - 2s - loss: 140.0096 - loglik: -1.3871e+02 - logprior: -1.1790e+00
Epoch 7/10
19/19 - 2s - loss: 139.8981 - loglik: -1.3862e+02 - logprior: -1.1575e+00
Epoch 8/10
19/19 - 2s - loss: 139.8696 - loglik: -1.3860e+02 - logprior: -1.1421e+00
Epoch 9/10
19/19 - 2s - loss: 139.6102 - loglik: -1.3834e+02 - logprior: -1.1337e+00
Epoch 10/10
19/19 - 2s - loss: 139.5311 - loglik: -1.3827e+02 - logprior: -1.1268e+00
Fitted a model with MAP estimate = -140.2142
expansions: [(0, 3), (13, 2), (14, 1), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (49, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 143.4952 - loglik: -1.3899e+02 - logprior: -4.3676e+00
Epoch 2/2
19/19 - 2s - loss: 132.2524 - loglik: -1.3052e+02 - logprior: -1.5761e+00
Fitted a model with MAP estimate = -131.9187
expansions: [(0, 2)]
discards: [25 28 50 61 66]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 134.5925 - loglik: -1.3066e+02 - logprior: -3.8974e+00
Epoch 2/2
19/19 - 2s - loss: 129.2798 - loglik: -1.2771e+02 - logprior: -1.4352e+00
Fitted a model with MAP estimate = -128.8947
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.8694 - loglik: -1.2933e+02 - logprior: -3.4415e+00
Epoch 2/10
21/21 - 2s - loss: 129.7162 - loglik: -1.2766e+02 - logprior: -1.8555e+00
Epoch 3/10
21/21 - 2s - loss: 127.8919 - loglik: -1.2643e+02 - logprior: -1.2308e+00
Epoch 4/10
21/21 - 2s - loss: 127.3483 - loglik: -1.2601e+02 - logprior: -1.1024e+00
Epoch 5/10
21/21 - 2s - loss: 126.8962 - loglik: -1.2557e+02 - logprior: -1.0990e+00
Epoch 6/10
21/21 - 2s - loss: 127.1109 - loglik: -1.2580e+02 - logprior: -1.0876e+00
Fitted a model with MAP estimate = -126.4313
Time for alignment: 61.4734
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.3246 - loglik: -1.9218e+02 - logprior: -3.1216e+00
Epoch 2/10
19/19 - 2s - loss: 155.4161 - loglik: -1.5416e+02 - logprior: -1.2285e+00
Epoch 3/10
19/19 - 2s - loss: 142.0944 - loglik: -1.4060e+02 - logprior: -1.3105e+00
Epoch 4/10
19/19 - 2s - loss: 139.8361 - loglik: -1.3844e+02 - logprior: -1.2864e+00
Epoch 5/10
19/19 - 2s - loss: 139.0131 - loglik: -1.3768e+02 - logprior: -1.2313e+00
Epoch 6/10
19/19 - 2s - loss: 138.9805 - loglik: -1.3769e+02 - logprior: -1.2073e+00
Epoch 7/10
19/19 - 2s - loss: 138.6571 - loglik: -1.3737e+02 - logprior: -1.1842e+00
Epoch 8/10
19/19 - 2s - loss: 138.5782 - loglik: -1.3730e+02 - logprior: -1.1699e+00
Epoch 9/10
19/19 - 2s - loss: 138.4405 - loglik: -1.3716e+02 - logprior: -1.1636e+00
Epoch 10/10
19/19 - 2s - loss: 138.3514 - loglik: -1.3707e+02 - logprior: -1.1566e+00
Fitted a model with MAP estimate = -138.8937
expansions: [(0, 3), (13, 2), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 142.6777 - loglik: -1.3828e+02 - logprior: -4.3586e+00
Epoch 2/2
19/19 - 2s - loss: 132.1029 - loglik: -1.3048e+02 - logprior: -1.5361e+00
Fitted a model with MAP estimate = -131.5468
expansions: [(0, 2)]
discards: [27 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 133.9173 - loglik: -1.2987e+02 - logprior: -3.9848e+00
Epoch 2/2
19/19 - 2s - loss: 128.2246 - loglik: -1.2676e+02 - logprior: -1.3072e+00
Fitted a model with MAP estimate = -128.3000
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.0530 - loglik: -1.2960e+02 - logprior: -3.4108e+00
Epoch 2/10
21/21 - 2s - loss: 129.2589 - loglik: -1.2758e+02 - logprior: -1.5877e+00
Epoch 3/10
21/21 - 2s - loss: 127.9924 - loglik: -1.2666e+02 - logprior: -1.1628e+00
Epoch 4/10
21/21 - 2s - loss: 127.3197 - loglik: -1.2601e+02 - logprior: -1.1153e+00
Epoch 5/10
21/21 - 2s - loss: 127.0753 - loglik: -1.2577e+02 - logprior: -1.1108e+00
Epoch 6/10
21/21 - 2s - loss: 126.9176 - loglik: -1.2563e+02 - logprior: -1.0941e+00
Epoch 7/10
21/21 - 2s - loss: 126.4769 - loglik: -1.2520e+02 - logprior: -1.0688e+00
Epoch 8/10
21/21 - 2s - loss: 126.7535 - loglik: -1.2549e+02 - logprior: -1.0540e+00
Fitted a model with MAP estimate = -126.2695
Time for alignment: 63.8718
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.2640 - loglik: -1.9211e+02 - logprior: -3.1251e+00
Epoch 2/10
19/19 - 2s - loss: 155.9963 - loglik: -1.5472e+02 - logprior: -1.2372e+00
Epoch 3/10
19/19 - 2s - loss: 141.3557 - loglik: -1.3986e+02 - logprior: -1.3159e+00
Epoch 4/10
19/19 - 2s - loss: 139.0467 - loglik: -1.3766e+02 - logprior: -1.2902e+00
Epoch 5/10
19/19 - 2s - loss: 138.4215 - loglik: -1.3708e+02 - logprior: -1.2378e+00
Epoch 6/10
19/19 - 2s - loss: 138.2302 - loglik: -1.3692e+02 - logprior: -1.2119e+00
Epoch 7/10
19/19 - 2s - loss: 138.0971 - loglik: -1.3680e+02 - logprior: -1.1879e+00
Epoch 8/10
19/19 - 2s - loss: 137.9359 - loglik: -1.3666e+02 - logprior: -1.1721e+00
Epoch 9/10
19/19 - 2s - loss: 137.7352 - loglik: -1.3646e+02 - logprior: -1.1628e+00
Epoch 10/10
19/19 - 2s - loss: 137.9249 - loglik: -1.3665e+02 - logprior: -1.1599e+00
Fitted a model with MAP estimate = -138.4599
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 143.1235 - loglik: -1.3866e+02 - logprior: -4.3708e+00
Epoch 2/2
19/19 - 2s - loss: 132.3463 - loglik: -1.3063e+02 - logprior: -1.5904e+00
Fitted a model with MAP estimate = -132.1345
expansions: [(0, 2)]
discards: [19 26 29 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 134.9652 - loglik: -1.3101e+02 - logprior: -3.9271e+00
Epoch 2/2
19/19 - 2s - loss: 129.5573 - loglik: -1.2801e+02 - logprior: -1.4390e+00
Fitted a model with MAP estimate = -129.3241
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.0893 - loglik: -1.2945e+02 - logprior: -3.4452e+00
Epoch 2/10
21/21 - 2s - loss: 129.6382 - loglik: -1.2762e+02 - logprior: -1.8097e+00
Epoch 3/10
21/21 - 2s - loss: 127.9379 - loglik: -1.2650e+02 - logprior: -1.2037e+00
Epoch 4/10
21/21 - 2s - loss: 127.5407 - loglik: -1.2620e+02 - logprior: -1.0988e+00
Epoch 5/10
21/21 - 2s - loss: 126.8926 - loglik: -1.2555e+02 - logprior: -1.1008e+00
Epoch 6/10
21/21 - 2s - loss: 126.9785 - loglik: -1.2565e+02 - logprior: -1.0888e+00
Fitted a model with MAP estimate = -126.4984
Time for alignment: 59.9665
Computed alignments with likelihoods: ['-126.4313', '-126.2695', '-126.4984']
Best model has likelihood: -126.2695
time for generating output: 0.1492
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9488459139114162
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faaafa31550>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac11026ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaeb931eb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5544d490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5544d5e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac5544d2e0>, <__main__.SimpleDirichletPrior object at 0x7fa2b5696b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.7894 - loglik: -3.8254e+02 - logprior: -8.1284e+00
Epoch 2/10
13/13 - 2s - loss: 338.3511 - loglik: -3.3623e+02 - logprior: -1.8929e+00
Epoch 3/10
13/13 - 2s - loss: 294.3423 - loglik: -2.9201e+02 - logprior: -1.8110e+00
Epoch 4/10
13/13 - 2s - loss: 280.9246 - loglik: -2.7802e+02 - logprior: -2.1935e+00
Epoch 5/10
13/13 - 2s - loss: 278.2389 - loglik: -2.7551e+02 - logprior: -2.1916e+00
Epoch 6/10
13/13 - 2s - loss: 275.2848 - loglik: -2.7280e+02 - logprior: -2.0610e+00
Epoch 7/10
13/13 - 3s - loss: 275.9030 - loglik: -2.7349e+02 - logprior: -2.0356e+00
Fitted a model with MAP estimate = -274.9651
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 9s - loss: 286.9770 - loglik: -2.7741e+02 - logprior: -9.5481e+00
Epoch 2/2
13/13 - 3s - loss: 265.3292 - loglik: -2.6132e+02 - logprior: -3.9344e+00
Fitted a model with MAP estimate = -261.2850
expansions: [(0, 3)]
discards: [  0  86  87  94 127]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.7979 - loglik: -2.5933e+02 - logprior: -7.3305e+00
Epoch 2/2
13/13 - 3s - loss: 257.6442 - loglik: -2.5559e+02 - logprior: -1.7659e+00
Fitted a model with MAP estimate = -254.7659
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 268.7631 - loglik: -2.5945e+02 - logprior: -9.2086e+00
Epoch 2/10
13/13 - 3s - loss: 260.0997 - loglik: -2.5719e+02 - logprior: -2.7296e+00
Epoch 3/10
13/13 - 3s - loss: 255.7134 - loglik: -2.5436e+02 - logprior: -1.0359e+00
Epoch 4/10
13/13 - 3s - loss: 253.1317 - loglik: -2.5216e+02 - logprior: -5.1878e-01
Epoch 5/10
13/13 - 3s - loss: 252.7088 - loglik: -2.5180e+02 - logprior: -4.0074e-01
Epoch 6/10
13/13 - 3s - loss: 252.6011 - loglik: -2.5172e+02 - logprior: -3.8281e-01
Epoch 7/10
13/13 - 3s - loss: 252.0754 - loglik: -2.5121e+02 - logprior: -4.0063e-01
Epoch 8/10
13/13 - 3s - loss: 251.7773 - loglik: -2.5097e+02 - logprior: -3.8015e-01
Epoch 9/10
13/13 - 3s - loss: 251.5037 - loglik: -2.5076e+02 - logprior: -3.4328e-01
Epoch 10/10
13/13 - 3s - loss: 251.4865 - loglik: -2.5079e+02 - logprior: -3.1468e-01
Fitted a model with MAP estimate = -250.8730
Time for alignment: 89.5732
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 390.8262 - loglik: -3.8265e+02 - logprior: -8.1310e+00
Epoch 2/10
13/13 - 2s - loss: 336.3108 - loglik: -3.3428e+02 - logprior: -1.9120e+00
Epoch 3/10
13/13 - 2s - loss: 295.3933 - loglik: -2.9318e+02 - logprior: -1.8247e+00
Epoch 4/10
13/13 - 2s - loss: 282.9835 - loglik: -2.8024e+02 - logprior: -2.1280e+00
Epoch 5/10
13/13 - 2s - loss: 277.9186 - loglik: -2.7527e+02 - logprior: -2.1444e+00
Epoch 6/10
13/13 - 2s - loss: 275.9782 - loglik: -2.7354e+02 - logprior: -2.0415e+00
Epoch 7/10
13/13 - 2s - loss: 275.5399 - loglik: -2.7316e+02 - logprior: -2.0339e+00
Epoch 8/10
13/13 - 2s - loss: 274.8935 - loglik: -2.7253e+02 - logprior: -2.0345e+00
Epoch 9/10
13/13 - 2s - loss: 274.2413 - loglik: -2.7189e+02 - logprior: -2.0220e+00
Epoch 10/10
13/13 - 2s - loss: 274.5318 - loglik: -2.7221e+02 - logprior: -2.0137e+00
Fitted a model with MAP estimate = -273.8082
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 287.8406 - loglik: -2.7824e+02 - logprior: -9.5555e+00
Epoch 2/2
13/13 - 3s - loss: 265.1999 - loglik: -2.6114e+02 - logprior: -3.9332e+00
Fitted a model with MAP estimate = -261.2566
expansions: [(0, 3)]
discards: [  0  77  88  89 128]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.5822 - loglik: -2.6018e+02 - logprior: -7.3143e+00
Epoch 2/2
13/13 - 3s - loss: 257.5689 - loglik: -2.5563e+02 - logprior: -1.7317e+00
Fitted a model with MAP estimate = -254.8671
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 269.9353 - loglik: -2.6074e+02 - logprior: -9.1679e+00
Epoch 2/10
13/13 - 3s - loss: 261.1857 - loglik: -2.5840e+02 - logprior: -2.7079e+00
Epoch 3/10
13/13 - 3s - loss: 255.3696 - loglik: -2.5406e+02 - logprior: -1.0054e+00
Epoch 4/10
13/13 - 3s - loss: 254.0291 - loglik: -2.5301e+02 - logprior: -4.6277e-01
Epoch 5/10
13/13 - 3s - loss: 253.1081 - loglik: -2.5218e+02 - logprior: -3.7526e-01
Epoch 6/10
13/13 - 3s - loss: 252.2910 - loglik: -2.5148e+02 - logprior: -3.3351e-01
Epoch 7/10
13/13 - 3s - loss: 252.2581 - loglik: -2.5151e+02 - logprior: -3.5050e-01
Epoch 8/10
13/13 - 3s - loss: 251.6882 - loglik: -2.5098e+02 - logprior: -3.4882e-01
Epoch 9/10
13/13 - 3s - loss: 250.7228 - loglik: -2.5006e+02 - logprior: -3.2201e-01
Epoch 10/10
13/13 - 3s - loss: 251.8407 - loglik: -2.5123e+02 - logprior: -2.8701e-01
Fitted a model with MAP estimate = -250.8864
Time for alignment: 98.4073
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 391.1061 - loglik: -3.8277e+02 - logprior: -8.1361e+00
Epoch 2/10
13/13 - 2s - loss: 338.8892 - loglik: -3.3679e+02 - logprior: -1.8960e+00
Epoch 3/10
13/13 - 2s - loss: 297.5128 - loglik: -2.9531e+02 - logprior: -1.8007e+00
Epoch 4/10
13/13 - 2s - loss: 282.4696 - loglik: -2.7964e+02 - logprior: -2.1559e+00
Epoch 5/10
13/13 - 2s - loss: 277.6675 - loglik: -2.7493e+02 - logprior: -2.1884e+00
Epoch 6/10
13/13 - 2s - loss: 276.6913 - loglik: -2.7416e+02 - logprior: -2.0868e+00
Epoch 7/10
13/13 - 2s - loss: 276.2007 - loglik: -2.7374e+02 - logprior: -2.0714e+00
Epoch 8/10
13/13 - 2s - loss: 275.4655 - loglik: -2.7303e+02 - logprior: -2.0722e+00
Epoch 9/10
13/13 - 3s - loss: 275.4210 - loglik: -2.7302e+02 - logprior: -2.0615e+00
Epoch 10/10
13/13 - 3s - loss: 274.6599 - loglik: -2.7227e+02 - logprior: -2.0610e+00
Fitted a model with MAP estimate = -274.5741
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 287.0918 - loglik: -2.7750e+02 - logprior: -9.5542e+00
Epoch 2/2
13/13 - 3s - loss: 264.3712 - loglik: -2.6031e+02 - logprior: -3.9037e+00
Fitted a model with MAP estimate = -261.0238
expansions: [(0, 3)]
discards: [ 0 65 87 88]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 269.3132 - loglik: -2.6179e+02 - logprior: -7.3498e+00
Epoch 2/2
13/13 - 3s - loss: 259.2116 - loglik: -2.5744e+02 - logprior: -1.7230e+00
Fitted a model with MAP estimate = -256.5684
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.6552 - loglik: -2.6037e+02 - logprior: -9.1800e+00
Epoch 2/10
13/13 - 3s - loss: 259.6884 - loglik: -2.5686e+02 - logprior: -2.7090e+00
Epoch 3/10
13/13 - 3s - loss: 256.0764 - loglik: -2.5474e+02 - logprior: -1.0395e+00
Epoch 4/10
13/13 - 3s - loss: 253.2716 - loglik: -2.5233e+02 - logprior: -4.9339e-01
Epoch 5/10
13/13 - 3s - loss: 253.0351 - loglik: -2.5216e+02 - logprior: -3.9498e-01
Epoch 6/10
13/13 - 3s - loss: 252.2196 - loglik: -2.5140e+02 - logprior: -3.6443e-01
Epoch 7/10
13/13 - 3s - loss: 251.7339 - loglik: -2.5094e+02 - logprior: -3.8233e-01
Epoch 8/10
13/13 - 3s - loss: 251.3314 - loglik: -2.5060e+02 - logprior: -3.6481e-01
Epoch 9/10
13/13 - 3s - loss: 251.8799 - loglik: -2.5119e+02 - logprior: -3.4193e-01
Fitted a model with MAP estimate = -250.9702
Time for alignment: 92.9718
Computed alignments with likelihoods: ['-250.8730', '-250.8864', '-250.9702']
Best model has likelihood: -250.8730
time for generating output: 0.2166
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9432506887052342
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faada6a8a90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa8c182be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fab300e6af0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabb382c8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa81eff1c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa86f583b20>, <__main__.SimpleDirichletPrior object at 0x7fab2ed859d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1080.0531 - loglik: -1.0763e+03 - logprior: -3.3961e+00
Epoch 2/10
25/25 - 32s - loss: 821.8162 - loglik: -8.2030e+02 - logprior: -9.1128e-01
Epoch 3/10
25/25 - 32s - loss: 766.3459 - loglik: -7.6352e+02 - logprior: -1.8221e+00
Epoch 4/10
25/25 - 32s - loss: 758.3286 - loglik: -7.5537e+02 - logprior: -1.9827e+00
Epoch 5/10
25/25 - 32s - loss: 749.5416 - loglik: -7.4651e+02 - logprior: -2.1101e+00
Epoch 6/10
25/25 - 32s - loss: 749.0513 - loglik: -7.4615e+02 - logprior: -2.0406e+00
Epoch 7/10
25/25 - 32s - loss: 748.1435 - loglik: -7.4503e+02 - logprior: -2.2940e+00
Epoch 8/10
25/25 - 32s - loss: 745.8008 - loglik: -7.4299e+02 - logprior: -2.0095e+00
Epoch 9/10
25/25 - 32s - loss: 745.2742 - loglik: -7.4251e+02 - logprior: -2.0019e+00
Epoch 10/10
25/25 - 32s - loss: 745.4531 - loglik: -7.4264e+02 - logprior: -2.0572e+00
Fitted a model with MAP estimate = -744.0101
expansions: [(0, 2), (43, 1), (86, 1), (124, 1), (142, 3), (162, 1), (163, 1), (167, 2), (173, 4), (174, 2), (175, 2), (190, 2), (191, 6), (192, 2), (194, 1), (195, 1), (196, 2), (197, 1), (199, 1), (200, 1), (202, 1), (204, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (212, 1), (214, 1), (220, 1), (222, 1), (223, 1), (226, 1), (228, 1), (229, 1), (230, 1), (237, 2), (238, 1), (249, 1), (253, 2), (255, 3), (256, 4), (258, 5), (266, 1), (280, 1), (281, 1), (282, 1), (299, 2), (300, 1), (301, 1), (304, 1), (318, 5), (320, 2), (325, 1), (327, 2), (350, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 464 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 737.5189 - loglik: -7.3119e+02 - logprior: -6.0921e+00
Epoch 2/2
25/25 - 46s - loss: 698.2455 - loglik: -6.9691e+02 - logprior: -7.2316e-01
Fitted a model with MAP estimate = -691.2145
expansions: [(444, 1)]
discards: [  3 177 187 188 215 315 316 317 318 324 325 375 405]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 50s - loss: 710.0706 - loglik: -7.0668e+02 - logprior: -3.3140e+00
Epoch 2/2
25/25 - 44s - loss: 695.2883 - loglik: -6.9600e+02 - logprior: 1.0526
Fitted a model with MAP estimate = -690.3425
expansions: []
discards: [432]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 709.5433 - loglik: -7.0687e+02 - logprior: -2.4960e+00
Epoch 2/10
25/25 - 44s - loss: 694.9470 - loglik: -6.9619e+02 - logprior: 1.5879
Epoch 3/10
25/25 - 44s - loss: 689.4398 - loglik: -6.9111e+02 - logprior: 2.3191
Epoch 4/10
25/25 - 44s - loss: 686.9352 - loglik: -6.8866e+02 - logprior: 2.5538
Epoch 5/10
25/25 - 44s - loss: 686.5593 - loglik: -6.8847e+02 - logprior: 2.8203
Epoch 6/10
25/25 - 44s - loss: 684.3844 - loglik: -6.8653e+02 - logprior: 3.0848
Epoch 7/10
25/25 - 44s - loss: 684.4914 - loglik: -6.8694e+02 - logprior: 3.3735
Fitted a model with MAP estimate = -681.8560
Time for alignment: 971.2119
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 38s - loss: 1083.2122 - loglik: -1.0794e+03 - logprior: -3.4441e+00
Epoch 2/10
25/25 - 32s - loss: 821.3536 - loglik: -8.1968e+02 - logprior: -1.0370e+00
Epoch 3/10
25/25 - 32s - loss: 762.6613 - loglik: -7.5948e+02 - logprior: -2.1900e+00
Epoch 4/10
25/25 - 32s - loss: 754.4646 - loglik: -7.5108e+02 - logprior: -2.4217e+00
Epoch 5/10
25/25 - 32s - loss: 746.2992 - loglik: -7.4310e+02 - logprior: -2.3258e+00
Epoch 6/10
25/25 - 32s - loss: 747.1106 - loglik: -7.4387e+02 - logprior: -2.3630e+00
Fitted a model with MAP estimate = -744.0389
expansions: [(52, 1), (125, 1), (134, 1), (140, 1), (163, 2), (164, 1), (171, 1), (175, 4), (176, 2), (177, 2), (180, 1), (193, 1), (194, 4), (195, 1), (196, 1), (199, 1), (200, 1), (201, 1), (202, 1), (204, 1), (205, 1), (207, 1), (209, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (217, 1), (219, 1), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (230, 2), (231, 3), (238, 2), (239, 1), (250, 1), (254, 2), (255, 1), (256, 5), (257, 1), (259, 3), (267, 1), (281, 1), (282, 1), (283, 1), (287, 1), (299, 1), (300, 2), (301, 1), (304, 1), (318, 2), (319, 2), (321, 2), (326, 1), (327, 3), (352, 2), (354, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 462 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 737.0413 - loglik: -7.3219e+02 - logprior: -4.7345e+00
Epoch 2/2
25/25 - 46s - loss: 696.7512 - loglik: -6.9495e+02 - logprior: -1.3596e+00
Fitted a model with MAP estimate = -690.4206
expansions: [(437, 1)]
discards: [168 185 186 214 309 310 311 373 401 438 439]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 705.4682 - loglik: -7.0192e+02 - logprior: -3.3283e+00
Epoch 2/2
25/25 - 44s - loss: 694.4696 - loglik: -6.9485e+02 - logprior: 0.9812
Fitted a model with MAP estimate = -689.4850
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 50s - loss: 710.2198 - loglik: -7.0743e+02 - logprior: -2.7272e+00
Epoch 2/10
25/25 - 44s - loss: 694.9342 - loglik: -6.9629e+02 - logprior: 1.6359
Epoch 3/10
25/25 - 44s - loss: 689.1543 - loglik: -6.9049e+02 - logprior: 2.1255
Epoch 4/10
25/25 - 44s - loss: 686.9732 - loglik: -6.8824e+02 - logprior: 2.2851
Epoch 5/10
25/25 - 44s - loss: 685.3408 - loglik: -6.8688e+02 - logprior: 2.5689
Epoch 6/10
25/25 - 44s - loss: 682.9476 - loglik: -6.8485e+02 - logprior: 2.8716
Epoch 7/10
25/25 - 44s - loss: 683.3234 - loglik: -6.8553e+02 - logprior: 3.1194
Fitted a model with MAP estimate = -681.5127
Time for alignment: 848.9962
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1080.6654 - loglik: -1.0772e+03 - logprior: -3.4492e+00
Epoch 2/10
25/25 - 32s - loss: 812.8747 - loglik: -8.1109e+02 - logprior: -1.2788e+00
Epoch 3/10
25/25 - 32s - loss: 760.4211 - loglik: -7.5744e+02 - logprior: -2.2046e+00
Epoch 4/10
25/25 - 32s - loss: 751.0714 - loglik: -7.4790e+02 - logprior: -2.3759e+00
Epoch 5/10
25/25 - 32s - loss: 748.4282 - loglik: -7.4528e+02 - logprior: -2.3518e+00
Epoch 6/10
25/25 - 32s - loss: 744.0404 - loglik: -7.4091e+02 - logprior: -2.3319e+00
Epoch 7/10
25/25 - 32s - loss: 747.0896 - loglik: -7.4398e+02 - logprior: -2.3440e+00
Fitted a model with MAP estimate = -743.5368
expansions: [(46, 1), (132, 1), (144, 3), (162, 1), (163, 3), (169, 1), (173, 4), (174, 2), (175, 2), (176, 1), (189, 1), (190, 1), (191, 4), (192, 1), (193, 1), (195, 1), (196, 1), (197, 2), (198, 1), (200, 1), (201, 2), (204, 1), (206, 1), (207, 1), (210, 1), (211, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (235, 1), (237, 1), (238, 1), (249, 1), (250, 1), (252, 1), (254, 3), (255, 1), (256, 1), (258, 4), (266, 1), (280, 1), (282, 1), (297, 1), (299, 1), (300, 4), (302, 1), (312, 1), (315, 1), (316, 1), (317, 1), (318, 1), (324, 1), (327, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 456 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 732.1210 - loglik: -7.2810e+02 - logprior: -3.8557e+00
Epoch 2/2
25/25 - 45s - loss: 699.6617 - loglik: -6.9925e+02 - logprior: 0.0119
Fitted a model with MAP estimate = -693.6645
expansions: [(247, 1), (314, 1)]
discards: [169 185 186 214 320]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 707.8984 - loglik: -7.0493e+02 - logprior: -2.8229e+00
Epoch 2/2
25/25 - 44s - loss: 696.8438 - loglik: -6.9749e+02 - logprior: 1.0618
Fitted a model with MAP estimate = -691.9908
expansions: []
discards: [305]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 704.7520 - loglik: -7.0209e+02 - logprior: -2.4368e+00
Epoch 2/10
25/25 - 44s - loss: 695.1193 - loglik: -6.9597e+02 - logprior: 1.4827
Epoch 3/10
25/25 - 44s - loss: 688.5952 - loglik: -6.8955e+02 - logprior: 1.9349
Epoch 4/10
25/25 - 44s - loss: 687.8812 - loglik: -6.8898e+02 - logprior: 2.2270
Epoch 5/10
25/25 - 44s - loss: 684.7738 - loglik: -6.8608e+02 - logprior: 2.4247
Epoch 6/10
25/25 - 44s - loss: 687.8686 - loglik: -6.8956e+02 - logprior: 2.7333
Fitted a model with MAP estimate = -682.7701
Time for alignment: 829.1404
Computed alignments with likelihoods: ['-681.8560', '-681.5127', '-682.7701']
Best model has likelihood: -681.5127
time for generating output: 0.5074
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.8979500089911886
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faba19fce80>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2b42dd7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faba17d4040>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faafcbb7c40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8e82bfbe0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa90843bfa0>, <__main__.SimpleDirichletPrior object at 0x7faba1802dc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.5005 - loglik: -3.2199e+02 - logprior: -4.4958e+00
Epoch 2/10
16/16 - 5s - loss: 252.7199 - loglik: -2.5121e+02 - logprior: -1.4684e+00
Epoch 3/10
16/16 - 4s - loss: 223.6861 - loglik: -2.2159e+02 - logprior: -1.8505e+00
Epoch 4/10
16/16 - 4s - loss: 208.8399 - loglik: -2.0650e+02 - logprior: -1.8182e+00
Epoch 5/10
16/16 - 5s - loss: 203.2069 - loglik: -2.0091e+02 - logprior: -1.7405e+00
Epoch 6/10
16/16 - 4s - loss: 200.8296 - loglik: -1.9860e+02 - logprior: -1.7273e+00
Epoch 7/10
16/16 - 5s - loss: 199.7003 - loglik: -1.9751e+02 - logprior: -1.7061e+00
Epoch 8/10
16/16 - 5s - loss: 199.8256 - loglik: -1.9764e+02 - logprior: -1.7050e+00
Fitted a model with MAP estimate = -198.4011
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (69, 1), (86, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 218.0942 - loglik: -2.1229e+02 - logprior: -5.6824e+00
Epoch 2/2
16/16 - 4s - loss: 202.1248 - loglik: -1.9896e+02 - logprior: -2.8409e+00
Fitted a model with MAP estimate = -198.8910
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 199.5766 - loglik: -1.9640e+02 - logprior: -3.0593e+00
Epoch 2/2
33/33 - 6s - loss: 194.2288 - loglik: -1.9256e+02 - logprior: -1.4785e+00
Fitted a model with MAP estimate = -192.3305
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 198.3002 - loglik: -1.9516e+02 - logprior: -3.0354e+00
Epoch 2/10
33/33 - 7s - loss: 194.1315 - loglik: -1.9252e+02 - logprior: -1.3446e+00
Epoch 3/10
33/33 - 6s - loss: 191.0981 - loglik: -1.8949e+02 - logprior: -1.2542e+00
Epoch 4/10
33/33 - 7s - loss: 191.2230 - loglik: -1.8970e+02 - logprior: -1.1433e+00
Fitted a model with MAP estimate = -189.3105
Time for alignment: 125.4728
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 325.9875 - loglik: -3.2143e+02 - logprior: -4.5000e+00
Epoch 2/10
16/16 - 4s - loss: 254.7758 - loglik: -2.5324e+02 - logprior: -1.4752e+00
Epoch 3/10
16/16 - 4s - loss: 222.5054 - loglik: -2.2041e+02 - logprior: -1.8332e+00
Epoch 4/10
16/16 - 4s - loss: 212.8335 - loglik: -2.1065e+02 - logprior: -1.7697e+00
Epoch 5/10
16/16 - 4s - loss: 207.4887 - loglik: -2.0525e+02 - logprior: -1.7004e+00
Epoch 6/10
16/16 - 5s - loss: 203.3523 - loglik: -2.0099e+02 - logprior: -1.7054e+00
Epoch 7/10
16/16 - 4s - loss: 200.8412 - loglik: -1.9856e+02 - logprior: -1.7168e+00
Epoch 8/10
16/16 - 5s - loss: 197.8415 - loglik: -1.9558e+02 - logprior: -1.7108e+00
Epoch 9/10
16/16 - 5s - loss: 197.0253 - loglik: -1.9481e+02 - logprior: -1.7089e+00
Epoch 10/10
16/16 - 4s - loss: 196.8136 - loglik: -1.9455e+02 - logprior: -1.7179e+00
Fitted a model with MAP estimate = -196.2483
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (72, 1), (97, 1), (98, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 10s - loss: 220.3223 - loglik: -2.1451e+02 - logprior: -5.6852e+00
Epoch 2/2
16/16 - 4s - loss: 200.8186 - loglik: -1.9766e+02 - logprior: -2.7755e+00
Fitted a model with MAP estimate = -197.2594
expansions: [(0, 2), (19, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 198.7702 - loglik: -1.9563e+02 - logprior: -3.0423e+00
Epoch 2/2
33/33 - 6s - loss: 192.1426 - loglik: -1.9049e+02 - logprior: -1.4551e+00
Fitted a model with MAP estimate = -190.7763
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 196.5558 - loglik: -1.9345e+02 - logprior: -3.0033e+00
Epoch 2/10
33/33 - 7s - loss: 192.7407 - loglik: -1.9119e+02 - logprior: -1.3160e+00
Epoch 3/10
33/33 - 7s - loss: 190.3157 - loglik: -1.8877e+02 - logprior: -1.2086e+00
Epoch 4/10
33/33 - 6s - loss: 189.5252 - loglik: -1.8803e+02 - logprior: -1.1080e+00
Epoch 5/10
33/33 - 7s - loss: 187.7072 - loglik: -1.8629e+02 - logprior: -1.0206e+00
Epoch 6/10
33/33 - 6s - loss: 186.5950 - loglik: -1.8523e+02 - logprior: -9.5870e-01
Epoch 7/10
33/33 - 7s - loss: 187.3293 - loglik: -1.8601e+02 - logprior: -8.8077e-01
Fitted a model with MAP estimate = -186.2320
Time for alignment: 153.7273
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.6926 - loglik: -3.2217e+02 - logprior: -4.4966e+00
Epoch 2/10
16/16 - 4s - loss: 254.9634 - loglik: -2.5344e+02 - logprior: -1.4606e+00
Epoch 3/10
16/16 - 4s - loss: 223.3943 - loglik: -2.2131e+02 - logprior: -1.7884e+00
Epoch 4/10
16/16 - 4s - loss: 208.4622 - loglik: -2.0597e+02 - logprior: -1.8585e+00
Epoch 5/10
16/16 - 5s - loss: 202.7060 - loglik: -2.0039e+02 - logprior: -1.7197e+00
Epoch 6/10
16/16 - 4s - loss: 198.5505 - loglik: -1.9632e+02 - logprior: -1.7041e+00
Epoch 7/10
16/16 - 4s - loss: 197.9912 - loglik: -1.9583e+02 - logprior: -1.6954e+00
Epoch 8/10
16/16 - 5s - loss: 197.0050 - loglik: -1.9485e+02 - logprior: -1.6931e+00
Epoch 9/10
16/16 - 5s - loss: 197.2498 - loglik: -1.9509e+02 - logprior: -1.6890e+00
Fitted a model with MAP estimate = -196.0408
expansions: [(10, 1), (12, 1), (15, 1), (16, 3), (69, 1), (86, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 220.3974 - loglik: -2.1457e+02 - logprior: -5.6979e+00
Epoch 2/2
16/16 - 5s - loss: 202.8268 - loglik: -1.9978e+02 - logprior: -2.7722e+00
Fitted a model with MAP estimate = -197.8009
expansions: [(0, 2), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 198.3152 - loglik: -1.9521e+02 - logprior: -3.0390e+00
Epoch 2/2
33/33 - 7s - loss: 192.9518 - loglik: -1.9128e+02 - logprior: -1.4453e+00
Fitted a model with MAP estimate = -190.8662
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 196.1370 - loglik: -1.9304e+02 - logprior: -2.9974e+00
Epoch 2/10
33/33 - 6s - loss: 193.7248 - loglik: -1.9223e+02 - logprior: -1.3095e+00
Epoch 3/10
33/33 - 7s - loss: 189.7418 - loglik: -1.8821e+02 - logprior: -1.2068e+00
Epoch 4/10
33/33 - 7s - loss: 189.1974 - loglik: -1.8770e+02 - logprior: -1.1060e+00
Epoch 5/10
33/33 - 6s - loss: 187.1828 - loglik: -1.8574e+02 - logprior: -1.0375e+00
Epoch 6/10
33/33 - 7s - loss: 187.5211 - loglik: -1.8614e+02 - logprior: -9.7059e-01
Fitted a model with MAP estimate = -186.7811
Time for alignment: 142.2962
Computed alignments with likelihoods: ['-189.3105', '-186.2320', '-186.7811']
Best model has likelihood: -186.2320
time for generating output: 0.3333
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.542419366634974
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa96b477c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faac1332970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa9efbfdc0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa82f25df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8e86ccd60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fabe6bef2e0>, <__main__.SimpleDirichletPrior object at 0x7fac2aa48e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 308.7234 - loglik: -3.0543e+02 - logprior: -3.1379e+00
Epoch 2/10
19/19 - 3s - loss: 276.3624 - loglik: -2.7458e+02 - logprior: -1.1660e+00
Epoch 3/10
19/19 - 3s - loss: 260.1162 - loglik: -2.5756e+02 - logprior: -1.5469e+00
Epoch 4/10
19/19 - 3s - loss: 255.8798 - loglik: -2.5342e+02 - logprior: -1.4973e+00
Epoch 5/10
19/19 - 3s - loss: 254.8168 - loglik: -2.5262e+02 - logprior: -1.4291e+00
Epoch 6/10
19/19 - 3s - loss: 253.9939 - loglik: -2.5192e+02 - logprior: -1.4059e+00
Epoch 7/10
19/19 - 3s - loss: 253.9439 - loglik: -2.5193e+02 - logprior: -1.3834e+00
Epoch 8/10
19/19 - 3s - loss: 252.6322 - loglik: -2.5065e+02 - logprior: -1.3744e+00
Epoch 9/10
19/19 - 3s - loss: 253.3154 - loglik: -2.5137e+02 - logprior: -1.3713e+00
Fitted a model with MAP estimate = -240.3779
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 10), (38, 2), (43, 2), (55, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 268.0503 - loglik: -2.6401e+02 - logprior: -3.9641e+00
Epoch 2/2
19/19 - 3s - loss: 250.8016 - loglik: -2.4824e+02 - logprior: -2.2417e+00
Fitted a model with MAP estimate = -233.9689
expansions: [(0, 2)]
discards: [ 0 27 44 45 46 47 48 55 77 82 88 90]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.2814 - loglik: -2.4908e+02 - logprior: -2.9157e+00
Epoch 2/2
19/19 - 3s - loss: 247.4901 - loglik: -2.4579e+02 - logprior: -1.1377e+00
Fitted a model with MAP estimate = -231.8402
expansions: [(44, 6)]
discards: [ 0 10]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 238.2449 - loglik: -2.3558e+02 - logprior: -2.6144e+00
Epoch 2/10
23/23 - 4s - loss: 232.0527 - loglik: -2.3064e+02 - logprior: -1.1693e+00
Epoch 3/10
23/23 - 4s - loss: 229.2581 - loglik: -2.2777e+02 - logprior: -1.0512e+00
Epoch 4/10
23/23 - 4s - loss: 227.7276 - loglik: -2.2606e+02 - logprior: -1.0483e+00
Epoch 5/10
23/23 - 4s - loss: 227.6956 - loglik: -2.2592e+02 - logprior: -1.0291e+00
Epoch 6/10
23/23 - 4s - loss: 226.3754 - loglik: -2.2454e+02 - logprior: -1.0282e+00
Epoch 7/10
23/23 - 4s - loss: 226.0839 - loglik: -2.2424e+02 - logprior: -1.0124e+00
Epoch 8/10
23/23 - 4s - loss: 224.9315 - loglik: -2.2311e+02 - logprior: -1.0019e+00
Epoch 9/10
23/23 - 4s - loss: 225.5273 - loglik: -2.2373e+02 - logprior: -9.9068e-01
Fitted a model with MAP estimate = -223.8507
Time for alignment: 112.9459
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.4835 - loglik: -3.0505e+02 - logprior: -3.1383e+00
Epoch 2/10
19/19 - 3s - loss: 275.8604 - loglik: -2.7384e+02 - logprior: -1.1824e+00
Epoch 3/10
19/19 - 3s - loss: 259.9980 - loglik: -2.5731e+02 - logprior: -1.5165e+00
Epoch 4/10
19/19 - 3s - loss: 256.4492 - loglik: -2.5401e+02 - logprior: -1.4507e+00
Epoch 5/10
19/19 - 3s - loss: 254.6333 - loglik: -2.5240e+02 - logprior: -1.4172e+00
Epoch 6/10
19/19 - 3s - loss: 254.0965 - loglik: -2.5200e+02 - logprior: -1.4043e+00
Epoch 7/10
19/19 - 3s - loss: 253.6046 - loglik: -2.5159e+02 - logprior: -1.3903e+00
Epoch 8/10
19/19 - 3s - loss: 252.9936 - loglik: -2.5104e+02 - logprior: -1.3766e+00
Epoch 9/10
19/19 - 3s - loss: 253.3395 - loglik: -2.5142e+02 - logprior: -1.3709e+00
Fitted a model with MAP estimate = -240.9974
expansions: [(6, 3), (7, 2), (10, 2), (33, 2), (34, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (63, 2), (65, 2), (66, 1), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 272.8534 - loglik: -2.6885e+02 - logprior: -3.9457e+00
Epoch 2/2
19/19 - 3s - loss: 254.3323 - loglik: -2.5203e+02 - logprior: -2.2185e+00
Fitted a model with MAP estimate = -236.2472
expansions: [(0, 2)]
discards: [ 0  8 14 46 47 48 49 51 56 81 93]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 253.1246 - loglik: -2.5014e+02 - logprior: -2.9013e+00
Epoch 2/2
19/19 - 3s - loss: 248.3757 - loglik: -2.4681e+02 - logprior: -1.1288e+00
Fitted a model with MAP estimate = -232.7780
expansions: []
discards: [ 0 81]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 236.9581 - loglik: -2.3414e+02 - logprior: -2.5814e+00
Epoch 2/10
23/23 - 4s - loss: 232.2843 - loglik: -2.3054e+02 - logprior: -1.1036e+00
Epoch 3/10
23/23 - 4s - loss: 230.6235 - loglik: -2.2866e+02 - logprior: -1.0479e+00
Epoch 4/10
23/23 - 3s - loss: 229.7900 - loglik: -2.2779e+02 - logprior: -1.0209e+00
Epoch 5/10
23/23 - 3s - loss: 228.5444 - loglik: -2.2659e+02 - logprior: -1.0110e+00
Epoch 6/10
23/23 - 4s - loss: 228.0544 - loglik: -2.2618e+02 - logprior: -9.9913e-01
Epoch 7/10
23/23 - 3s - loss: 227.1245 - loglik: -2.2530e+02 - logprior: -9.8198e-01
Epoch 8/10
23/23 - 4s - loss: 226.5260 - loglik: -2.2474e+02 - logprior: -9.7479e-01
Epoch 9/10
23/23 - 4s - loss: 226.8249 - loglik: -2.2510e+02 - logprior: -9.6588e-01
Fitted a model with MAP estimate = -225.1681
Time for alignment: 108.1140
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.4207 - loglik: -3.0521e+02 - logprior: -3.1379e+00
Epoch 2/10
19/19 - 3s - loss: 275.8284 - loglik: -2.7428e+02 - logprior: -1.1771e+00
Epoch 3/10
19/19 - 3s - loss: 260.1154 - loglik: -2.5776e+02 - logprior: -1.5404e+00
Epoch 4/10
19/19 - 3s - loss: 256.1463 - loglik: -2.5383e+02 - logprior: -1.4871e+00
Epoch 5/10
19/19 - 3s - loss: 255.2097 - loglik: -2.5307e+02 - logprior: -1.4239e+00
Epoch 6/10
19/19 - 3s - loss: 253.8718 - loglik: -2.5183e+02 - logprior: -1.4030e+00
Epoch 7/10
19/19 - 3s - loss: 253.6090 - loglik: -2.5163e+02 - logprior: -1.3832e+00
Epoch 8/10
19/19 - 3s - loss: 253.2610 - loglik: -2.5131e+02 - logprior: -1.3727e+00
Epoch 9/10
19/19 - 3s - loss: 253.0055 - loglik: -2.5108e+02 - logprior: -1.3675e+00
Epoch 10/10
19/19 - 3s - loss: 252.5009 - loglik: -2.5059e+02 - logprior: -1.3641e+00
Fitted a model with MAP estimate = -240.6567
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 10), (38, 2), (43, 2), (55, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 270.7946 - loglik: -2.6671e+02 - logprior: -3.9412e+00
Epoch 2/2
19/19 - 3s - loss: 252.5100 - loglik: -2.4987e+02 - logprior: -2.2721e+00
Fitted a model with MAP estimate = -234.7515
expansions: [(0, 2)]
discards: [ 0  8 14 28 47 56 78 83 89 91]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 252.5788 - loglik: -2.4958e+02 - logprior: -2.9007e+00
Epoch 2/2
19/19 - 3s - loss: 247.1064 - loglik: -2.4556e+02 - logprior: -1.1283e+00
Fitted a model with MAP estimate = -231.4566
expansions: []
discards: [ 0 43 46]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 236.8697 - loglik: -2.3420e+02 - logprior: -2.5646e+00
Epoch 2/10
23/23 - 4s - loss: 231.3995 - loglik: -2.2996e+02 - logprior: -1.1034e+00
Epoch 3/10
23/23 - 3s - loss: 229.9036 - loglik: -2.2824e+02 - logprior: -1.0630e+00
Epoch 4/10
23/23 - 4s - loss: 228.8961 - loglik: -2.2708e+02 - logprior: -1.0258e+00
Epoch 5/10
23/23 - 4s - loss: 228.5211 - loglik: -2.2664e+02 - logprior: -1.0154e+00
Epoch 6/10
23/23 - 4s - loss: 227.1887 - loglik: -2.2530e+02 - logprior: -1.0065e+00
Epoch 7/10
23/23 - 4s - loss: 226.7851 - loglik: -2.2493e+02 - logprior: -9.9428e-01
Epoch 8/10
23/23 - 4s - loss: 225.9651 - loglik: -2.2414e+02 - logprior: -9.8574e-01
Epoch 9/10
23/23 - 4s - loss: 225.3148 - loglik: -2.2354e+02 - logprior: -9.6576e-01
Epoch 10/10
23/23 - 4s - loss: 225.8697 - loglik: -2.2414e+02 - logprior: -9.6373e-01
Fitted a model with MAP estimate = -224.2439
Time for alignment: 115.3601
Computed alignments with likelihoods: ['-223.8507', '-225.1681', '-224.2439']
Best model has likelihood: -223.8507
time for generating output: 0.2024
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.78470715835141
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fac0060fc10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabbc776880>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa86c35ce50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faaafb87d00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa8e85b0df0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa86ed9a0a0>, <__main__.SimpleDirichletPrior object at 0x7faaf411c5b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.3640 - loglik: -3.8675e+02 - logprior: -4.5561e+01
Epoch 2/10
10/10 - 2s - loss: 370.2793 - loglik: -3.5974e+02 - logprior: -1.0381e+01
Epoch 3/10
10/10 - 2s - loss: 335.6099 - loglik: -3.3108e+02 - logprior: -4.2462e+00
Epoch 4/10
10/10 - 2s - loss: 311.4934 - loglik: -3.0857e+02 - logprior: -2.4314e+00
Epoch 5/10
10/10 - 2s - loss: 301.4358 - loglik: -2.9935e+02 - logprior: -1.6140e+00
Epoch 6/10
10/10 - 2s - loss: 297.8336 - loglik: -2.9641e+02 - logprior: -1.0801e+00
Epoch 7/10
10/10 - 2s - loss: 295.7621 - loglik: -2.9478e+02 - logprior: -6.6713e-01
Epoch 8/10
10/10 - 2s - loss: 294.8620 - loglik: -2.9409e+02 - logprior: -4.3422e-01
Epoch 9/10
10/10 - 2s - loss: 294.0217 - loglik: -2.9340e+02 - logprior: -2.4854e-01
Epoch 10/10
10/10 - 2s - loss: 293.6115 - loglik: -2.9308e+02 - logprior: -1.3166e-01
Fitted a model with MAP estimate = -293.1180
expansions: [(10, 4), (17, 1), (18, 1), (19, 1), (26, 1), (42, 1), (48, 1), (58, 1), (59, 2), (61, 2), (73, 1), (82, 1), (90, 2), (91, 3), (92, 3), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 351.8075 - loglik: -3.0077e+02 - logprior: -5.1029e+01
Epoch 2/2
10/10 - 3s - loss: 307.7563 - loglik: -2.8821e+02 - logprior: -1.9465e+01
Fitted a model with MAP estimate = -299.3582
expansions: [(10, 2), (34, 3)]
discards: [ 69  74 107 114]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 332.8502 - loglik: -2.8393e+02 - logprior: -4.8794e+01
Epoch 2/2
10/10 - 3s - loss: 293.2177 - loglik: -2.7910e+02 - logprior: -1.3833e+01
Fitted a model with MAP estimate = -283.6789
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 319.8528 - loglik: -2.7967e+02 - logprior: -4.0145e+01
Epoch 2/10
10/10 - 3s - loss: 285.8287 - loglik: -2.7790e+02 - logprior: -7.8023e+00
Epoch 3/10
10/10 - 3s - loss: 278.1459 - loglik: -2.7665e+02 - logprior: -1.2650e+00
Epoch 4/10
10/10 - 3s - loss: 274.9527 - loglik: -2.7604e+02 - logprior: 1.3808
Epoch 5/10
10/10 - 3s - loss: 273.6346 - loglik: -2.7606e+02 - logprior: 2.7455
Epoch 6/10
10/10 - 3s - loss: 272.0043 - loglik: -2.7516e+02 - logprior: 3.4835
Epoch 7/10
10/10 - 3s - loss: 271.7774 - loglik: -2.7543e+02 - logprior: 3.9822
Epoch 8/10
10/10 - 3s - loss: 271.2330 - loglik: -2.7533e+02 - logprior: 4.4330
Epoch 9/10
10/10 - 3s - loss: 270.9069 - loglik: -2.7538e+02 - logprior: 4.8054
Epoch 10/10
10/10 - 3s - loss: 270.2236 - loglik: -2.7496e+02 - logprior: 5.0821
Fitted a model with MAP estimate = -269.9158
Time for alignment: 79.6772
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 432.1119 - loglik: -3.8647e+02 - logprior: -4.5566e+01
Epoch 2/10
10/10 - 2s - loss: 370.1255 - loglik: -3.5956e+02 - logprior: -1.0395e+01
Epoch 3/10
10/10 - 2s - loss: 334.4964 - loglik: -3.2991e+02 - logprior: -4.2764e+00
Epoch 4/10
10/10 - 2s - loss: 311.7557 - loglik: -3.0905e+02 - logprior: -2.2491e+00
Epoch 5/10
10/10 - 2s - loss: 302.0491 - loglik: -3.0035e+02 - logprior: -1.3180e+00
Epoch 6/10
10/10 - 2s - loss: 298.9956 - loglik: -2.9783e+02 - logprior: -8.4168e-01
Epoch 7/10
10/10 - 2s - loss: 296.8640 - loglik: -2.9611e+02 - logprior: -4.5013e-01
Epoch 8/10
10/10 - 2s - loss: 295.4991 - loglik: -2.9496e+02 - logprior: -2.0468e-01
Epoch 9/10
10/10 - 2s - loss: 295.1972 - loglik: -2.9482e+02 - logprior: -6.8646e-03
Epoch 10/10
10/10 - 2s - loss: 295.1194 - loglik: -2.9484e+02 - logprior: 0.1247
Fitted a model with MAP estimate = -293.9993
expansions: [(10, 4), (17, 1), (18, 1), (26, 2), (48, 2), (58, 2), (59, 1), (62, 2), (73, 1), (82, 1), (90, 2), (91, 3), (92, 2), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.1539 - loglik: -2.9802e+02 - logprior: -5.0960e+01
Epoch 2/2
10/10 - 3s - loss: 305.8603 - loglik: -2.8621e+02 - logprior: -1.9357e+01
Fitted a model with MAP estimate = -298.0434
expansions: [(10, 2), (34, 3)]
discards: [ 56  68 107]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 334.7178 - loglik: -2.8592e+02 - logprior: -4.8776e+01
Epoch 2/2
10/10 - 3s - loss: 294.1896 - loglik: -2.8038e+02 - logprior: -1.3753e+01
Fitted a model with MAP estimate = -284.7986
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 320.2325 - loglik: -2.8000e+02 - logprior: -4.0173e+01
Epoch 2/10
10/10 - 3s - loss: 285.1421 - loglik: -2.7722e+02 - logprior: -7.7804e+00
Epoch 3/10
10/10 - 3s - loss: 278.2312 - loglik: -2.7678e+02 - logprior: -1.1889e+00
Epoch 4/10
10/10 - 3s - loss: 274.8185 - loglik: -2.7594e+02 - logprior: 1.4473
Epoch 5/10
10/10 - 3s - loss: 273.5530 - loglik: -2.7605e+02 - logprior: 2.8172
Epoch 6/10
10/10 - 3s - loss: 271.8885 - loglik: -2.7514e+02 - logprior: 3.5560
Epoch 7/10
10/10 - 3s - loss: 271.6224 - loglik: -2.7539e+02 - logprior: 4.0624
Epoch 8/10
10/10 - 3s - loss: 271.1286 - loglik: -2.7534e+02 - logprior: 4.5075
Epoch 9/10
10/10 - 3s - loss: 271.0193 - loglik: -2.7559e+02 - logprior: 4.8802
Epoch 10/10
10/10 - 3s - loss: 270.5223 - loglik: -2.7537e+02 - logprior: 5.1621
Fitted a model with MAP estimate = -269.9423
Time for alignment: 78.8321
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.5541 - loglik: -3.8692e+02 - logprior: -4.5562e+01
Epoch 2/10
10/10 - 2s - loss: 370.3331 - loglik: -3.5976e+02 - logprior: -1.0384e+01
Epoch 3/10
10/10 - 2s - loss: 333.5168 - loglik: -3.2891e+02 - logprior: -4.2759e+00
Epoch 4/10
10/10 - 2s - loss: 309.3222 - loglik: -3.0638e+02 - logprior: -2.4522e+00
Epoch 5/10
10/10 - 2s - loss: 300.2650 - loglik: -2.9822e+02 - logprior: -1.6628e+00
Epoch 6/10
10/10 - 2s - loss: 296.9675 - loglik: -2.9557e+02 - logprior: -1.0679e+00
Epoch 7/10
10/10 - 2s - loss: 295.7283 - loglik: -2.9475e+02 - logprior: -6.4735e-01
Epoch 8/10
10/10 - 2s - loss: 294.0314 - loglik: -2.9333e+02 - logprior: -3.4251e-01
Epoch 9/10
10/10 - 2s - loss: 293.7603 - loglik: -2.9322e+02 - logprior: -1.6911e-01
Epoch 10/10
10/10 - 2s - loss: 293.3168 - loglik: -2.9286e+02 - logprior: -7.3465e-02
Fitted a model with MAP estimate = -292.5476
expansions: [(10, 4), (17, 1), (18, 1), (27, 1), (28, 3), (42, 1), (58, 2), (59, 2), (62, 1), (64, 1), (73, 1), (85, 1), (90, 2), (91, 3), (92, 2), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 349.7679 - loglik: -2.9885e+02 - logprior: -5.0856e+01
Epoch 2/2
10/10 - 3s - loss: 306.1897 - loglik: -2.8680e+02 - logprior: -1.9233e+01
Fitted a model with MAP estimate = -298.2865
expansions: [(10, 1)]
discards: [  0  69  71 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 336.5895 - loglik: -2.8685e+02 - logprior: -4.9693e+01
Epoch 2/2
10/10 - 3s - loss: 300.0335 - loglik: -2.8297e+02 - logprior: -1.6894e+01
Fitted a model with MAP estimate = -290.8859
expansions: [(2, 2), (9, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 331.2763 - loglik: -2.8115e+02 - logprior: -4.9965e+01
Epoch 2/10
10/10 - 3s - loss: 297.3087 - loglik: -2.7875e+02 - logprior: -1.8224e+01
Epoch 3/10
10/10 - 3s - loss: 288.7773 - loglik: -2.7720e+02 - logprior: -1.1163e+01
Epoch 4/10
10/10 - 3s - loss: 280.8773 - loglik: -2.7597e+02 - logprior: -4.5406e+00
Epoch 5/10
10/10 - 3s - loss: 274.5056 - loglik: -2.7593e+02 - logprior: 1.7286
Epoch 6/10
10/10 - 3s - loss: 272.7926 - loglik: -2.7605e+02 - logprior: 3.5318
Epoch 7/10
10/10 - 3s - loss: 271.9083 - loglik: -2.7580e+02 - logprior: 4.1758
Epoch 8/10
10/10 - 3s - loss: 271.4448 - loglik: -2.7570e+02 - logprior: 4.5607
Epoch 9/10
10/10 - 3s - loss: 270.8226 - loglik: -2.7534e+02 - logprior: 4.8206
Epoch 10/10
10/10 - 3s - loss: 270.4339 - loglik: -2.7516e+02 - logprior: 5.0466
Fitted a model with MAP estimate = -270.1336
Time for alignment: 78.8143
Computed alignments with likelihoods: ['-269.9158', '-269.9423', '-270.1336']
Best model has likelihood: -269.9158
time for generating output: 0.1746
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9149265274555298
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa2b4edf9d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2945359a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faac1442f10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba1b280a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faa82e2f130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa86e588e50>, <__main__.SimpleDirichletPrior object at 0x7faac118cfa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.2700 - loglik: -1.8609e+02 - logprior: -2.6713e+02
Epoch 2/10
10/10 - 1s - loss: 230.6604 - loglik: -1.6048e+02 - logprior: -7.0165e+01
Epoch 3/10
10/10 - 1s - loss: 170.9765 - loglik: -1.3998e+02 - logprior: -3.0990e+01
Epoch 4/10
10/10 - 1s - loss: 142.8549 - loglik: -1.2587e+02 - logprior: -1.6950e+01
Epoch 5/10
10/10 - 1s - loss: 128.3828 - loglik: -1.1884e+02 - logprior: -9.3222e+00
Epoch 6/10
10/10 - 1s - loss: 120.7596 - loglik: -1.1618e+02 - logprior: -4.2290e+00
Epoch 7/10
10/10 - 1s - loss: 115.8225 - loglik: -1.1458e+02 - logprior: -9.3797e-01
Epoch 8/10
10/10 - 1s - loss: 113.1264 - loglik: -1.1408e+02 - logprior: 1.2465
Epoch 9/10
10/10 - 1s - loss: 111.5290 - loglik: -1.1411e+02 - logprior: 2.9017
Epoch 10/10
10/10 - 1s - loss: 110.4474 - loglik: -1.1423e+02 - logprior: 4.1252
Fitted a model with MAP estimate = -109.6182
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 465.1214 - loglik: -1.0959e+02 - logprior: -3.5551e+02
Epoch 2/2
10/10 - 1s - loss: 203.8958 - loglik: -9.7179e+01 - logprior: -1.0661e+02
Fitted a model with MAP estimate = -154.8683
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 398.4953 - loglik: -9.7775e+01 - logprior: -3.0056e+02
Epoch 2/2
10/10 - 1s - loss: 210.3052 - loglik: -9.5716e+01 - logprior: -1.1452e+02
Fitted a model with MAP estimate = -180.1811
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 370.0047 - loglik: -9.4909e+01 - logprior: -2.7502e+02
Epoch 2/10
10/10 - 1s - loss: 167.3949 - loglik: -9.4052e+01 - logprior: -7.3273e+01
Epoch 3/10
10/10 - 1s - loss: 116.3898 - loglik: -9.3719e+01 - logprior: -2.2505e+01
Epoch 4/10
10/10 - 1s - loss: 97.9812 - loglik: -9.3704e+01 - logprior: -4.0418e+00
Epoch 5/10
10/10 - 1s - loss: 88.6103 - loglik: -9.3847e+01 - logprior: 5.4841
Epoch 6/10
10/10 - 1s - loss: 83.2203 - loglik: -9.4073e+01 - logprior: 11.1143
Epoch 7/10
10/10 - 1s - loss: 79.8432 - loglik: -9.4229e+01 - logprior: 14.6874
Epoch 8/10
10/10 - 1s - loss: 77.5049 - loglik: -9.4375e+01 - logprior: 17.1859
Epoch 9/10
10/10 - 1s - loss: 75.7180 - loglik: -9.4522e+01 - logprior: 19.1182
Epoch 10/10
10/10 - 1s - loss: 74.2408 - loglik: -9.4642e+01 - logprior: 20.7166
Fitted a model with MAP estimate = -73.1896
Time for alignment: 36.8246
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.2475 - loglik: -1.8611e+02 - logprior: -2.6713e+02
Epoch 2/10
10/10 - 1s - loss: 230.5828 - loglik: -1.6042e+02 - logprior: -7.0155e+01
Epoch 3/10
10/10 - 1s - loss: 170.7617 - loglik: -1.3978e+02 - logprior: -3.0973e+01
Epoch 4/10
10/10 - 1s - loss: 142.2449 - loglik: -1.2518e+02 - logprior: -1.6957e+01
Epoch 5/10
10/10 - 1s - loss: 128.2434 - loglik: -1.1851e+02 - logprior: -9.4234e+00
Epoch 6/10
10/10 - 1s - loss: 120.9143 - loglik: -1.1632e+02 - logprior: -4.2452e+00
Epoch 7/10
10/10 - 1s - loss: 116.0280 - loglik: -1.1476e+02 - logprior: -9.6539e-01
Epoch 8/10
10/10 - 1s - loss: 113.1510 - loglik: -1.1409e+02 - logprior: 1.2441
Epoch 9/10
10/10 - 1s - loss: 111.5340 - loglik: -1.1408e+02 - logprior: 2.8779
Epoch 10/10
10/10 - 1s - loss: 110.4559 - loglik: -1.1420e+02 - logprior: 4.0953
Fitted a model with MAP estimate = -109.6165
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 465.5993 - loglik: -1.1000e+02 - logprior: -3.5554e+02
Epoch 2/2
10/10 - 1s - loss: 204.1902 - loglik: -9.7572e+01 - logprior: -1.0658e+02
Fitted a model with MAP estimate = -155.2852
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 398.1568 - loglik: -9.7414e+01 - logprior: -3.0059e+02
Epoch 2/2
10/10 - 1s - loss: 210.1659 - loglik: -9.5522e+01 - logprior: -1.1458e+02
Fitted a model with MAP estimate = -180.0786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.9595 - loglik: -9.4869e+01 - logprior: -2.7506e+02
Epoch 2/10
10/10 - 1s - loss: 167.2950 - loglik: -9.3895e+01 - logprior: -7.3292e+01
Epoch 3/10
10/10 - 1s - loss: 116.3303 - loglik: -9.3609e+01 - logprior: -2.2518e+01
Epoch 4/10
10/10 - 1s - loss: 97.8921 - loglik: -9.3695e+01 - logprior: -3.9951e+00
Epoch 5/10
10/10 - 1s - loss: 88.5187 - loglik: -9.3824e+01 - logprior: 5.5394
Epoch 6/10
10/10 - 1s - loss: 83.1400 - loglik: -9.3994e+01 - logprior: 11.1517
Epoch 7/10
10/10 - 1s - loss: 79.7741 - loglik: -9.4206e+01 - logprior: 14.7401
Epoch 8/10
10/10 - 1s - loss: 77.4363 - loglik: -9.4385e+01 - logprior: 17.2588
Epoch 9/10
10/10 - 1s - loss: 75.6432 - loglik: -9.4526e+01 - logprior: 19.2002
Epoch 10/10
10/10 - 1s - loss: 74.1574 - loglik: -9.4648e+01 - logprior: 20.8086
Fitted a model with MAP estimate = -73.1002
Time for alignment: 36.8420
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.2891 - loglik: -1.8609e+02 - logprior: -2.6713e+02
Epoch 2/10
10/10 - 1s - loss: 230.6083 - loglik: -1.6043e+02 - logprior: -7.0164e+01
Epoch 3/10
10/10 - 1s - loss: 170.9261 - loglik: -1.3992e+02 - logprior: -3.1001e+01
Epoch 4/10
10/10 - 1s - loss: 142.8706 - loglik: -1.2587e+02 - logprior: -1.6971e+01
Epoch 5/10
10/10 - 1s - loss: 128.5184 - loglik: -1.1908e+02 - logprior: -9.2397e+00
Epoch 6/10
10/10 - 1s - loss: 120.8004 - loglik: -1.1632e+02 - logprior: -4.1473e+00
Epoch 7/10
10/10 - 1s - loss: 115.8588 - loglik: -1.1465e+02 - logprior: -9.0913e-01
Epoch 8/10
10/10 - 1s - loss: 113.1465 - loglik: -1.1412e+02 - logprior: 1.2579
Epoch 9/10
10/10 - 1s - loss: 111.5452 - loglik: -1.1412e+02 - logprior: 2.8936
Epoch 10/10
10/10 - 1s - loss: 110.4540 - loglik: -1.1424e+02 - logprior: 4.1301
Fitted a model with MAP estimate = -109.6233
expansions: [(0, 3), (10, 1), (17, 1), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 465.1320 - loglik: -1.0953e+02 - logprior: -3.5547e+02
Epoch 2/2
10/10 - 1s - loss: 203.9495 - loglik: -9.7222e+01 - logprior: -1.0661e+02
Fitted a model with MAP estimate = -154.9876
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.9664 - loglik: -9.7194e+01 - logprior: -3.0064e+02
Epoch 2/2
10/10 - 1s - loss: 210.1426 - loglik: -9.5446e+01 - logprior: -1.1461e+02
Fitted a model with MAP estimate = -180.0247
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.9178 - loglik: -9.4720e+01 - logprior: -2.7510e+02
Epoch 2/10
10/10 - 1s - loss: 167.2955 - loglik: -9.3820e+01 - logprior: -7.3364e+01
Epoch 3/10
10/10 - 1s - loss: 116.3068 - loglik: -9.3497e+01 - logprior: -2.2582e+01
Epoch 4/10
10/10 - 1s - loss: 97.8567 - loglik: -9.3552e+01 - logprior: -4.0754e+00
Epoch 5/10
10/10 - 1s - loss: 88.5061 - loglik: -9.3677e+01 - logprior: 5.4579
Epoch 6/10
10/10 - 1s - loss: 83.1699 - loglik: -9.3929e+01 - logprior: 11.0596
Epoch 7/10
10/10 - 1s - loss: 79.8290 - loglik: -9.4151e+01 - logprior: 14.6398
Epoch 8/10
10/10 - 1s - loss: 77.5088 - loglik: -9.4342e+01 - logprior: 17.1533
Epoch 9/10
10/10 - 1s - loss: 75.7300 - loglik: -9.4494e+01 - logprior: 19.0862
Epoch 10/10
10/10 - 1s - loss: 74.2576 - loglik: -9.4613e+01 - logprior: 20.6794
Fitted a model with MAP estimate = -73.2016
Time for alignment: 35.9277
Computed alignments with likelihoods: ['-73.1896', '-73.1002', '-73.2016']
Best model has likelihood: -73.1002
time for generating output: 0.1365
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8474320241691843
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa86dfaa2b0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa86dfa0cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fab2ed94700>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2ed94e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fab2ed94a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab2ed94e20>, <__main__.SimpleDirichletPrior object at 0x7fa86d40ffa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 691.4476 - loglik: -6.8732e+02 - logprior: -4.0555e+00
Epoch 2/10
26/26 - 11s - loss: 588.9103 - loglik: -5.8711e+02 - logprior: -1.4644e+00
Epoch 3/10
26/26 - 11s - loss: 573.5471 - loglik: -5.7154e+02 - logprior: -1.5097e+00
Epoch 4/10
26/26 - 11s - loss: 570.4866 - loglik: -5.6835e+02 - logprior: -1.5645e+00
Epoch 5/10
26/26 - 11s - loss: 568.5470 - loglik: -5.6638e+02 - logprior: -1.5756e+00
Epoch 6/10
26/26 - 11s - loss: 566.5687 - loglik: -5.6440e+02 - logprior: -1.5368e+00
Epoch 7/10
26/26 - 11s - loss: 565.3790 - loglik: -5.6321e+02 - logprior: -1.5193e+00
Epoch 8/10
26/26 - 11s - loss: 566.9455 - loglik: -5.6478e+02 - logprior: -1.5149e+00
Fitted a model with MAP estimate = -564.4574
expansions: [(51, 1), (174, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 579.1517 - loglik: -5.7409e+02 - logprior: -4.6544e+00
Epoch 2/2
26/26 - 11s - loss: 571.4025 - loglik: -5.6942e+02 - logprior: -1.3400e+00
Fitted a model with MAP estimate = -567.2958
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 576.4744 - loglik: -5.7175e+02 - logprior: -4.4546e+00
Epoch 2/10
26/26 - 11s - loss: 570.3668 - loglik: -5.6880e+02 - logprior: -1.0857e+00
Epoch 3/10
26/26 - 11s - loss: 566.6689 - loglik: -5.6512e+02 - logprior: -8.7162e-01
Epoch 4/10
26/26 - 11s - loss: 565.8689 - loglik: -5.6427e+02 - logprior: -8.1474e-01
Epoch 5/10
26/26 - 11s - loss: 564.6342 - loglik: -5.6313e+02 - logprior: -7.1788e-01
Epoch 6/10
26/26 - 11s - loss: 564.0580 - loglik: -5.6272e+02 - logprior: -5.8671e-01
Epoch 7/10
26/26 - 11s - loss: 563.1405 - loglik: -5.6194e+02 - logprior: -4.9166e-01
Epoch 8/10
26/26 - 11s - loss: 563.1693 - loglik: -5.6212e+02 - logprior: -3.4693e-01
Fitted a model with MAP estimate = -561.5746
Time for alignment: 243.9676
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 691.2923 - loglik: -6.8695e+02 - logprior: -4.0935e+00
Epoch 2/10
26/26 - 11s - loss: 584.1430 - loglik: -5.8245e+02 - logprior: -1.3678e+00
Epoch 3/10
26/26 - 11s - loss: 565.6056 - loglik: -5.6375e+02 - logprior: -1.3323e+00
Epoch 4/10
26/26 - 11s - loss: 561.4166 - loglik: -5.5944e+02 - logprior: -1.3333e+00
Epoch 5/10
26/26 - 11s - loss: 559.8437 - loglik: -5.5773e+02 - logprior: -1.3913e+00
Epoch 6/10
26/26 - 11s - loss: 559.0598 - loglik: -5.5688e+02 - logprior: -1.3957e+00
Epoch 7/10
26/26 - 11s - loss: 557.9844 - loglik: -5.5579e+02 - logprior: -1.4143e+00
Epoch 8/10
26/26 - 11s - loss: 555.9079 - loglik: -5.5371e+02 - logprior: -1.4265e+00
Epoch 9/10
26/26 - 11s - loss: 557.4962 - loglik: -5.5533e+02 - logprior: -1.4032e+00
Fitted a model with MAP estimate = -555.6681
expansions: [(0, 3), (9, 1), (57, 2), (77, 1), (108, 5), (129, 1), (158, 1), (173, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 575.3986 - loglik: -5.7053e+02 - logprior: -4.8217e+00
Epoch 2/2
26/26 - 12s - loss: 558.3231 - loglik: -5.5672e+02 - logprior: -1.3680e+00
Fitted a model with MAP estimate = -554.4436
expansions: []
discards: [ 1  2 61]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 563.1839 - loglik: -5.5870e+02 - logprior: -4.4097e+00
Epoch 2/2
26/26 - 11s - loss: 558.9903 - loglik: -5.5775e+02 - logprior: -9.9992e-01
Fitted a model with MAP estimate = -554.2630
expansions: [(113, 1), (114, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 566.3458 - loglik: -5.5999e+02 - logprior: -6.2463e+00
Epoch 2/10
26/26 - 11s - loss: 558.0239 - loglik: -5.5501e+02 - logprior: -2.6041e+00
Epoch 3/10
26/26 - 11s - loss: 555.2299 - loglik: -5.5230e+02 - logprior: -2.2434e+00
Epoch 4/10
26/26 - 11s - loss: 548.9259 - loglik: -5.4749e+02 - logprior: -5.7400e-01
Epoch 5/10
26/26 - 11s - loss: 548.8553 - loglik: -5.4768e+02 - logprior: -2.6568e-01
Epoch 6/10
26/26 - 11s - loss: 547.4605 - loglik: -5.4645e+02 - logprior: -1.3470e-01
Epoch 7/10
26/26 - 11s - loss: 545.5701 - loglik: -5.4477e+02 - logprior: 0.0217
Epoch 8/10
26/26 - 11s - loss: 547.1301 - loglik: -5.4649e+02 - logprior: 0.1698
Fitted a model with MAP estimate = -544.5291
Time for alignment: 297.1076
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 17s - loss: 690.9214 - loglik: -6.8671e+02 - logprior: -4.0732e+00
Epoch 2/10
26/26 - 11s - loss: 583.1573 - loglik: -5.8144e+02 - logprior: -1.4317e+00
Epoch 3/10
26/26 - 11s - loss: 567.7864 - loglik: -5.6586e+02 - logprior: -1.4392e+00
Epoch 4/10
26/26 - 11s - loss: 565.5536 - loglik: -5.6344e+02 - logprior: -1.5249e+00
Epoch 5/10
26/26 - 11s - loss: 562.5258 - loglik: -5.6032e+02 - logprior: -1.5633e+00
Epoch 6/10
26/26 - 11s - loss: 560.7808 - loglik: -5.5855e+02 - logprior: -1.5845e+00
Epoch 7/10
26/26 - 11s - loss: 562.1744 - loglik: -5.5993e+02 - logprior: -1.5895e+00
Fitted a model with MAP estimate = -559.6712
expansions: [(0, 3), (11, 1), (129, 1), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 573.8649 - loglik: -5.6825e+02 - logprior: -4.8974e+00
Epoch 2/2
26/26 - 11s - loss: 564.3477 - loglik: -5.6211e+02 - logprior: -1.4045e+00
Fitted a model with MAP estimate = -561.6433
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 576.3826 - loglik: -5.6958e+02 - logprior: -6.6948e+00
Epoch 2/2
26/26 - 11s - loss: 568.1559 - loglik: -5.6479e+02 - logprior: -2.9591e+00
Fitted a model with MAP estimate = -564.4258
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 16s - loss: 571.4698 - loglik: -5.6690e+02 - logprior: -4.3220e+00
Epoch 2/10
26/26 - 12s - loss: 563.8287 - loglik: -5.6243e+02 - logprior: -9.5843e-01
Epoch 3/10
26/26 - 11s - loss: 560.5533 - loglik: -5.5900e+02 - logprior: -8.4096e-01
Epoch 4/10
26/26 - 12s - loss: 559.1479 - loglik: -5.5755e+02 - logprior: -7.9887e-01
Epoch 5/10
26/26 - 11s - loss: 556.7977 - loglik: -5.5536e+02 - logprior: -6.7521e-01
Epoch 6/10
26/26 - 11s - loss: 555.8447 - loglik: -5.5454e+02 - logprior: -5.7246e-01
Epoch 7/10
26/26 - 11s - loss: 554.6161 - loglik: -5.5349e+02 - logprior: -4.3423e-01
Epoch 8/10
26/26 - 11s - loss: 557.0103 - loglik: -5.5603e+02 - logprior: -3.1588e-01
Fitted a model with MAP estimate = -554.2510
Time for alignment: 275.0509
Computed alignments with likelihoods: ['-561.5746', '-544.5291', '-554.2510']
Best model has likelihood: -544.5291
time for generating output: 0.3762
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.49483870967741933
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fab15cdd460>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2c4114f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faae31dc4f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faae31dc9d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faae31dcaf0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa2b411d910>, <__main__.SimpleDirichletPrior object at 0x7fab0d6ff490>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.0425 - loglik: -3.4497e+02 - logprior: -7.6843e+01
Epoch 2/10
10/10 - 2s - loss: 336.5860 - loglik: -3.1867e+02 - logprior: -1.7735e+01
Epoch 3/10
10/10 - 2s - loss: 296.4919 - loglik: -2.8948e+02 - logprior: -6.6824e+00
Epoch 4/10
10/10 - 2s - loss: 275.6649 - loglik: -2.7216e+02 - logprior: -2.9896e+00
Epoch 5/10
10/10 - 2s - loss: 267.3759 - loglik: -2.6572e+02 - logprior: -1.1323e+00
Epoch 6/10
10/10 - 2s - loss: 262.6012 - loglik: -2.6209e+02 - logprior: -1.4551e-02
Epoch 7/10
10/10 - 2s - loss: 259.6951 - loglik: -2.5981e+02 - logprior: 0.5391
Epoch 8/10
10/10 - 2s - loss: 258.7409 - loglik: -2.5923e+02 - logprior: 0.9075
Epoch 9/10
10/10 - 2s - loss: 257.6834 - loglik: -2.5857e+02 - logprior: 1.3074
Epoch 10/10
10/10 - 2s - loss: 256.8213 - loglik: -2.5797e+02 - logprior: 1.5965
Fitted a model with MAP estimate = -255.9810
expansions: [(6, 1), (8, 1), (10, 2), (11, 2), (12, 1), (19, 2), (30, 1), (36, 4), (45, 1), (46, 1), (48, 2), (55, 1), (64, 3), (88, 3), (89, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 353.9899 - loglik: -2.6834e+02 - logprior: -8.5621e+01
Epoch 2/2
10/10 - 2s - loss: 280.9150 - loglik: -2.4859e+02 - logprior: -3.2255e+01
Fitted a model with MAP estimate = -267.5467
expansions: [(0, 3)]
discards: [  0  47 114]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 314.0386 - loglik: -2.4665e+02 - logprior: -6.7356e+01
Epoch 2/2
10/10 - 2s - loss: 254.3598 - loglik: -2.4011e+02 - logprior: -1.4188e+01
Fitted a model with MAP estimate = -244.2392
expansions: [(111, 2)]
discards: [ 0  1 41]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 325.1152 - loglik: -2.4227e+02 - logprior: -8.2783e+01
Epoch 2/10
10/10 - 2s - loss: 264.0860 - loglik: -2.3808e+02 - logprior: -2.5795e+01
Epoch 3/10
10/10 - 2s - loss: 242.2729 - loglik: -2.3542e+02 - logprior: -6.4608e+00
Epoch 4/10
10/10 - 2s - loss: 232.8246 - loglik: -2.3378e+02 - logprior: 1.4275
Epoch 5/10
10/10 - 2s - loss: 228.4619 - loglik: -2.3260e+02 - logprior: 4.5794
Epoch 6/10
10/10 - 2s - loss: 225.7872 - loglik: -2.3162e+02 - logprior: 6.2427
Epoch 7/10
10/10 - 2s - loss: 225.0196 - loglik: -2.3186e+02 - logprior: 7.2421
Epoch 8/10
10/10 - 2s - loss: 224.3914 - loglik: -2.3195e+02 - logprior: 7.9536
Epoch 9/10
10/10 - 2s - loss: 223.5457 - loglik: -2.3165e+02 - logprior: 8.5079
Epoch 10/10
10/10 - 2s - loss: 222.9136 - loglik: -2.3155e+02 - logprior: 9.0425
Fitted a model with MAP estimate = -222.3237
Time for alignment: 61.1455
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.1574 - loglik: -3.4511e+02 - logprior: -7.6862e+01
Epoch 2/10
10/10 - 2s - loss: 336.2634 - loglik: -3.1826e+02 - logprior: -1.7767e+01
Epoch 3/10
10/10 - 2s - loss: 298.3907 - loglik: -2.9129e+02 - logprior: -6.7261e+00
Epoch 4/10
10/10 - 2s - loss: 280.0678 - loglik: -2.7647e+02 - logprior: -3.0688e+00
Epoch 5/10
10/10 - 2s - loss: 269.6336 - loglik: -2.6787e+02 - logprior: -1.2866e+00
Epoch 6/10
10/10 - 2s - loss: 263.4407 - loglik: -2.6262e+02 - logprior: -4.0706e-01
Epoch 7/10
10/10 - 2s - loss: 260.0301 - loglik: -2.5975e+02 - logprior: 0.1191
Epoch 8/10
10/10 - 2s - loss: 258.3771 - loglik: -2.5861e+02 - logprior: 0.5934
Epoch 9/10
10/10 - 2s - loss: 257.4281 - loglik: -2.5800e+02 - logprior: 0.9617
Epoch 10/10
10/10 - 2s - loss: 256.3528 - loglik: -2.5710e+02 - logprior: 1.1357
Fitted a model with MAP estimate = -255.3080
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 2), (13, 1), (15, 1), (18, 1), (19, 2), (30, 1), (36, 4), (45, 2), (48, 2), (59, 1), (63, 3), (80, 1), (86, 4), (88, 3), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.2665 - loglik: -2.6273e+02 - logprior: -8.5482e+01
Epoch 2/2
10/10 - 2s - loss: 276.0416 - loglik: -2.4358e+02 - logprior: -3.2300e+01
Fitted a model with MAP estimate = -262.9071
expansions: [(0, 3), (63, 1)]
discards: [ 0  5 26 50 87]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 312.7095 - loglik: -2.4547e+02 - logprior: -6.7199e+01
Epoch 2/2
10/10 - 2s - loss: 251.3983 - loglik: -2.3743e+02 - logprior: -1.3880e+01
Fitted a model with MAP estimate = -240.9452
expansions: []
discards: [ 1  2 42 79]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 306.1008 - loglik: -2.4052e+02 - logprior: -6.5567e+01
Epoch 2/10
10/10 - 2s - loss: 249.9951 - loglik: -2.3650e+02 - logprior: -1.3421e+01
Epoch 3/10
10/10 - 2s - loss: 236.3328 - loglik: -2.3350e+02 - logprior: -2.6096e+00
Epoch 4/10
10/10 - 2s - loss: 231.2597 - loglik: -2.3305e+02 - logprior: 2.1249
Epoch 5/10
10/10 - 2s - loss: 228.3457 - loglik: -2.3274e+02 - logprior: 4.7943
Epoch 6/10
10/10 - 2s - loss: 226.5983 - loglik: -2.3258e+02 - logprior: 6.4146
Epoch 7/10
10/10 - 2s - loss: 225.0289 - loglik: -2.3204e+02 - logprior: 7.4507
Epoch 8/10
10/10 - 2s - loss: 224.9415 - loglik: -2.3268e+02 - logprior: 8.1786
Epoch 9/10
10/10 - 2s - loss: 223.9414 - loglik: -2.3224e+02 - logprior: 8.7291
Epoch 10/10
10/10 - 2s - loss: 223.2928 - loglik: -2.3203e+02 - logprior: 9.1689
Fitted a model with MAP estimate = -222.6094
Time for alignment: 62.0480
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.0793 - loglik: -3.4506e+02 - logprior: -7.6855e+01
Epoch 2/10
10/10 - 2s - loss: 336.3030 - loglik: -3.1832e+02 - logprior: -1.7744e+01
Epoch 3/10
10/10 - 2s - loss: 296.3026 - loglik: -2.8926e+02 - logprior: -6.6608e+00
Epoch 4/10
10/10 - 2s - loss: 276.1155 - loglik: -2.7237e+02 - logprior: -3.2201e+00
Epoch 5/10
10/10 - 2s - loss: 267.5613 - loglik: -2.6529e+02 - logprior: -1.8182e+00
Epoch 6/10
10/10 - 2s - loss: 262.2562 - loglik: -2.6095e+02 - logprior: -9.1328e-01
Epoch 7/10
10/10 - 2s - loss: 259.7533 - loglik: -2.5906e+02 - logprior: -3.0976e-01
Epoch 8/10
10/10 - 2s - loss: 258.0695 - loglik: -2.5779e+02 - logprior: 0.0799
Epoch 9/10
10/10 - 2s - loss: 257.0357 - loglik: -2.5707e+02 - logprior: 0.4131
Epoch 10/10
10/10 - 2s - loss: 256.4611 - loglik: -2.5685e+02 - logprior: 0.7870
Fitted a model with MAP estimate = -255.6179
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 2), (13, 1), (15, 1), (19, 2), (36, 4), (45, 1), (46, 1), (49, 3), (63, 3), (78, 1), (82, 1), (86, 4), (88, 3), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.2733 - loglik: -2.6364e+02 - logprior: -8.5596e+01
Epoch 2/2
10/10 - 2s - loss: 277.9159 - loglik: -2.4544e+02 - logprior: -3.2351e+01
Fitted a model with MAP estimate = -264.7042
expansions: [(0, 3), (65, 1)]
discards: [ 0  4 48]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 309.8290 - loglik: -2.4238e+02 - logprior: -6.7350e+01
Epoch 2/2
10/10 - 2s - loss: 249.9850 - loglik: -2.3560e+02 - logprior: -1.4110e+01
Fitted a model with MAP estimate = -239.6664
expansions: [(83, 2)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 303.6743 - loglik: -2.3803e+02 - logprior: -6.5592e+01
Epoch 2/10
10/10 - 2s - loss: 247.7915 - loglik: -2.3453e+02 - logprior: -1.3149e+01
Epoch 3/10
10/10 - 2s - loss: 234.2068 - loglik: -2.3159e+02 - logprior: -2.3685e+00
Epoch 4/10
10/10 - 2s - loss: 228.5725 - loglik: -2.3047e+02 - logprior: 2.2645
Epoch 5/10
10/10 - 2s - loss: 225.8194 - loglik: -2.3037e+02 - logprior: 4.9515
Epoch 6/10
10/10 - 2s - loss: 224.1590 - loglik: -2.3039e+02 - logprior: 6.6224
Epoch 7/10
10/10 - 2s - loss: 223.1001 - loglik: -2.3040e+02 - logprior: 7.7020
Epoch 8/10
10/10 - 2s - loss: 221.6612 - loglik: -2.2968e+02 - logprior: 8.4217
Epoch 9/10
10/10 - 2s - loss: 221.3450 - loglik: -2.2991e+02 - logprior: 8.9692
Epoch 10/10
10/10 - 2s - loss: 220.7699 - loglik: -2.2977e+02 - logprior: 9.4127
Fitted a model with MAP estimate = -219.9421
Time for alignment: 61.4478
Computed alignments with likelihoods: ['-222.3237', '-222.6094', '-219.9421']
Best model has likelihood: -219.9421
time for generating output: 0.1840
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7992805755395683
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faac103fac0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac4c9a7a60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac5db64ee0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa29c04e7c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2b41fec40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faac10312e0>, <__main__.SimpleDirichletPrior object at 0x7faaeb4c95b0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2018 - loglik: -9.2726e+01 - logprior: -4.4235e+00
Epoch 2/10
17/17 - 1s - loss: 74.5001 - loglik: -7.2950e+01 - logprior: -1.5352e+00
Epoch 3/10
17/17 - 1s - loss: 64.8343 - loglik: -6.3103e+01 - logprior: -1.6626e+00
Epoch 4/10
17/17 - 1s - loss: 63.5093 - loglik: -6.1764e+01 - logprior: -1.6214e+00
Epoch 5/10
17/17 - 1s - loss: 62.9099 - loglik: -6.1236e+01 - logprior: -1.5397e+00
Epoch 6/10
17/17 - 1s - loss: 62.7147 - loglik: -6.1011e+01 - logprior: -1.5505e+00
Epoch 7/10
17/17 - 1s - loss: 62.7903 - loglik: -6.1089e+01 - logprior: -1.5338e+00
Fitted a model with MAP estimate = -62.4486
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.1362 - loglik: -6.4546e+01 - logprior: -5.5173e+00
Epoch 2/2
17/17 - 1s - loss: 61.5229 - loglik: -5.8863e+01 - logprior: -2.5261e+00
Fitted a model with MAP estimate = -59.1368
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.1658 - loglik: -5.7714e+01 - logprior: -4.3400e+00
Epoch 2/2
17/17 - 1s - loss: 58.5546 - loglik: -5.6792e+01 - logprior: -1.6623e+00
Fitted a model with MAP estimate = -57.9994
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.5204 - loglik: -5.7246e+01 - logprior: -4.2335e+00
Epoch 2/10
17/17 - 1s - loss: 58.5521 - loglik: -5.6794e+01 - logprior: -1.6500e+00
Epoch 3/10
17/17 - 1s - loss: 58.0034 - loglik: -5.6465e+01 - logprior: -1.4128e+00
Epoch 4/10
17/17 - 1s - loss: 57.7104 - loglik: -5.6185e+01 - logprior: -1.3590e+00
Epoch 5/10
17/17 - 1s - loss: 57.7694 - loglik: -5.6262e+01 - logprior: -1.3213e+00
Fitted a model with MAP estimate = -57.3551
Time for alignment: 36.7482
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1396 - loglik: -9.2678e+01 - logprior: -4.4209e+00
Epoch 2/10
17/17 - 1s - loss: 74.9284 - loglik: -7.3375e+01 - logprior: -1.5331e+00
Epoch 3/10
17/17 - 1s - loss: 64.9324 - loglik: -6.3224e+01 - logprior: -1.6547e+00
Epoch 4/10
17/17 - 1s - loss: 63.2907 - loglik: -6.1521e+01 - logprior: -1.6267e+00
Epoch 5/10
17/17 - 1s - loss: 62.8554 - loglik: -6.1169e+01 - logprior: -1.5420e+00
Epoch 6/10
17/17 - 1s - loss: 62.6046 - loglik: -6.0877e+01 - logprior: -1.5598e+00
Epoch 7/10
17/17 - 1s - loss: 62.6220 - loglik: -6.0905e+01 - logprior: -1.5387e+00
Fitted a model with MAP estimate = -62.2608
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.7705 - loglik: -6.5194e+01 - logprior: -5.5243e+00
Epoch 2/2
17/17 - 1s - loss: 61.6742 - loglik: -5.8996e+01 - logprior: -2.5711e+00
Fitted a model with MAP estimate = -59.2998
expansions: []
discards: [13 16 26]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 62.2582 - loglik: -5.7837e+01 - logprior: -4.3475e+00
Epoch 2/2
17/17 - 1s - loss: 58.5547 - loglik: -5.6780e+01 - logprior: -1.6582e+00
Fitted a model with MAP estimate = -58.0123
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.6729 - loglik: -5.7308e+01 - logprior: -4.2368e+00
Epoch 2/10
17/17 - 1s - loss: 58.4806 - loglik: -5.6726e+01 - logprior: -1.6453e+00
Epoch 3/10
17/17 - 1s - loss: 58.0891 - loglik: -5.6527e+01 - logprior: -1.4187e+00
Epoch 4/10
17/17 - 1s - loss: 57.6620 - loglik: -5.6152e+01 - logprior: -1.3554e+00
Epoch 5/10
17/17 - 1s - loss: 57.7429 - loglik: -5.6242e+01 - logprior: -1.3182e+00
Fitted a model with MAP estimate = -57.3731
Time for alignment: 36.3240
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.2090 - loglik: -9.2698e+01 - logprior: -4.4206e+00
Epoch 2/10
17/17 - 1s - loss: 75.6173 - loglik: -7.4077e+01 - logprior: -1.5252e+00
Epoch 3/10
17/17 - 1s - loss: 65.7920 - loglik: -6.4087e+01 - logprior: -1.6634e+00
Epoch 4/10
17/17 - 1s - loss: 63.5269 - loglik: -6.1747e+01 - logprior: -1.6407e+00
Epoch 5/10
17/17 - 1s - loss: 62.9725 - loglik: -6.1293e+01 - logprior: -1.5489e+00
Epoch 6/10
17/17 - 1s - loss: 62.8215 - loglik: -6.1107e+01 - logprior: -1.5656e+00
Epoch 7/10
17/17 - 1s - loss: 62.7375 - loglik: -6.1029e+01 - logprior: -1.5454e+00
Epoch 8/10
17/17 - 1s - loss: 62.6576 - loglik: -6.0951e+01 - logprior: -1.5309e+00
Epoch 9/10
17/17 - 1s - loss: 62.5177 - loglik: -6.0807e+01 - logprior: -1.5202e+00
Epoch 10/10
17/17 - 1s - loss: 62.5349 - loglik: -6.0831e+01 - logprior: -1.5126e+00
Fitted a model with MAP estimate = -62.3250
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.6176 - loglik: -6.4990e+01 - logprior: -5.5080e+00
Epoch 2/2
17/17 - 1s - loss: 62.0718 - loglik: -5.9390e+01 - logprior: -2.5934e+00
Fitted a model with MAP estimate = -59.6220
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 62.5057 - loglik: -5.8103e+01 - logprior: -4.3683e+00
Epoch 2/2
17/17 - 1s - loss: 58.4739 - loglik: -5.6712e+01 - logprior: -1.6624e+00
Fitted a model with MAP estimate = -58.0471
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.9375 - loglik: -5.7528e+01 - logprior: -4.2385e+00
Epoch 2/10
17/17 - 1s - loss: 58.6219 - loglik: -5.6911e+01 - logprior: -1.6413e+00
Epoch 3/10
17/17 - 1s - loss: 58.2113 - loglik: -5.6734e+01 - logprior: -1.4159e+00
Epoch 4/10
17/17 - 1s - loss: 57.9457 - loglik: -5.6516e+01 - logprior: -1.3566e+00
Epoch 5/10
17/17 - 1s - loss: 57.8059 - loglik: -5.6389e+01 - logprior: -1.3170e+00
Epoch 6/10
17/17 - 1s - loss: 57.6881 - loglik: -5.6261e+01 - logprior: -1.3011e+00
Epoch 7/10
17/17 - 1s - loss: 57.6543 - loglik: -5.6237e+01 - logprior: -1.2718e+00
Epoch 8/10
17/17 - 1s - loss: 57.5206 - loglik: -5.6086e+01 - logprior: -1.2652e+00
Epoch 9/10
17/17 - 1s - loss: 57.4979 - loglik: -5.6070e+01 - logprior: -1.2476e+00
Epoch 10/10
17/17 - 1s - loss: 57.3584 - loglik: -5.5922e+01 - logprior: -1.2406e+00
Fitted a model with MAP estimate = -57.1906
Time for alignment: 43.0952
Computed alignments with likelihoods: ['-57.3551', '-57.3731', '-57.1906']
Best model has likelihood: -57.1906
time for generating output: 0.1287
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7587803936703975
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faab0159160>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac5569ca60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fac3bce8520>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faadab26b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac66040070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa968d9070>, <__main__.SimpleDirichletPrior object at 0x7faa82dcb2e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.6938 - loglik: -1.8554e+02 - logprior: -8.0972e+00
Epoch 2/10
13/13 - 1s - loss: 163.6865 - loglik: -1.6147e+02 - logprior: -2.2150e+00
Epoch 3/10
13/13 - 1s - loss: 147.3686 - loglik: -1.4539e+02 - logprior: -1.9045e+00
Epoch 4/10
13/13 - 1s - loss: 138.8432 - loglik: -1.3653e+02 - logprior: -2.0344e+00
Epoch 5/10
13/13 - 1s - loss: 136.7381 - loglik: -1.3451e+02 - logprior: -1.9412e+00
Epoch 6/10
13/13 - 1s - loss: 135.3351 - loglik: -1.3328e+02 - logprior: -1.8450e+00
Epoch 7/10
13/13 - 1s - loss: 135.4096 - loglik: -1.3335e+02 - logprior: -1.8769e+00
Fitted a model with MAP estimate = -134.5534
expansions: [(12, 1), (17, 2), (18, 1), (21, 5), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.1817 - loglik: -1.3458e+02 - logprior: -9.5450e+00
Epoch 2/2
13/13 - 1s - loss: 128.9460 - loglik: -1.2440e+02 - logprior: -4.4867e+00
Fitted a model with MAP estimate = -126.5130
expansions: [(0, 2), (14, 1)]
discards: [ 0 18 56]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 129.6582 - loglik: -1.2215e+02 - logprior: -7.3050e+00
Epoch 2/2
13/13 - 1s - loss: 122.2378 - loglik: -1.1988e+02 - logprior: -2.1754e+00
Fitted a model with MAP estimate = -120.5883
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 132.5306 - loglik: -1.2320e+02 - logprior: -9.2722e+00
Epoch 2/10
13/13 - 1s - loss: 124.0154 - loglik: -1.2040e+02 - logprior: -3.4958e+00
Epoch 3/10
13/13 - 1s - loss: 121.3038 - loglik: -1.1936e+02 - logprior: -1.7291e+00
Epoch 4/10
13/13 - 1s - loss: 119.7443 - loglik: -1.1808e+02 - logprior: -1.3790e+00
Epoch 5/10
13/13 - 1s - loss: 119.8036 - loglik: -1.1828e+02 - logprior: -1.2502e+00
Fitted a model with MAP estimate = -118.7915
Time for alignment: 38.9480
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7168 - loglik: -1.8561e+02 - logprior: -8.0989e+00
Epoch 2/10
13/13 - 1s - loss: 163.5703 - loglik: -1.6134e+02 - logprior: -2.2219e+00
Epoch 3/10
13/13 - 1s - loss: 145.6342 - loglik: -1.4360e+02 - logprior: -1.9570e+00
Epoch 4/10
13/13 - 1s - loss: 138.1479 - loglik: -1.3579e+02 - logprior: -2.0924e+00
Epoch 5/10
13/13 - 1s - loss: 135.5660 - loglik: -1.3332e+02 - logprior: -1.9567e+00
Epoch 6/10
13/13 - 1s - loss: 134.8682 - loglik: -1.3279e+02 - logprior: -1.8604e+00
Epoch 7/10
13/13 - 1s - loss: 133.9538 - loglik: -1.3188e+02 - logprior: -1.8829e+00
Epoch 8/10
13/13 - 1s - loss: 134.0865 - loglik: -1.3204e+02 - logprior: -1.8692e+00
Fitted a model with MAP estimate = -133.6811
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 144.1102 - loglik: -1.3451e+02 - logprior: -9.5593e+00
Epoch 2/2
13/13 - 1s - loss: 129.0742 - loglik: -1.2451e+02 - logprior: -4.4412e+00
Fitted a model with MAP estimate = -126.9740
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6249 - loglik: -1.2326e+02 - logprior: -7.3266e+00
Epoch 2/2
13/13 - 1s - loss: 123.7740 - loglik: -1.2142e+02 - logprior: -2.2125e+00
Fitted a model with MAP estimate = -121.9736
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.2012 - loglik: -1.2386e+02 - logprior: -9.2875e+00
Epoch 2/10
13/13 - 1s - loss: 124.7919 - loglik: -1.2111e+02 - logprior: -3.5275e+00
Epoch 3/10
13/13 - 1s - loss: 121.4468 - loglik: -1.1944e+02 - logprior: -1.7340e+00
Epoch 4/10
13/13 - 1s - loss: 119.9110 - loglik: -1.1823e+02 - logprior: -1.3780e+00
Epoch 5/10
13/13 - 1s - loss: 119.4357 - loglik: -1.1792e+02 - logprior: -1.2540e+00
Epoch 6/10
13/13 - 1s - loss: 119.0701 - loglik: -1.1757e+02 - logprior: -1.2627e+00
Epoch 7/10
13/13 - 1s - loss: 118.4076 - loglik: -1.1691e+02 - logprior: -1.2852e+00
Epoch 8/10
13/13 - 1s - loss: 118.7467 - loglik: -1.1731e+02 - logprior: -1.2409e+00
Fitted a model with MAP estimate = -118.1923
Time for alignment: 42.0746
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.7934 - loglik: -1.8564e+02 - logprior: -8.0966e+00
Epoch 2/10
13/13 - 1s - loss: 163.2274 - loglik: -1.6099e+02 - logprior: -2.2248e+00
Epoch 3/10
13/13 - 1s - loss: 145.5041 - loglik: -1.4346e+02 - logprior: -1.9612e+00
Epoch 4/10
13/13 - 1s - loss: 138.6529 - loglik: -1.3629e+02 - logprior: -2.0916e+00
Epoch 5/10
13/13 - 1s - loss: 136.1058 - loglik: -1.3390e+02 - logprior: -1.9459e+00
Epoch 6/10
13/13 - 1s - loss: 134.6824 - loglik: -1.3260e+02 - logprior: -1.8704e+00
Epoch 7/10
13/13 - 1s - loss: 134.3622 - loglik: -1.3229e+02 - logprior: -1.8852e+00
Epoch 8/10
13/13 - 1s - loss: 133.7894 - loglik: -1.3175e+02 - logprior: -1.8687e+00
Epoch 9/10
13/13 - 1s - loss: 133.2575 - loglik: -1.3124e+02 - logprior: -1.8489e+00
Epoch 10/10
13/13 - 1s - loss: 133.5696 - loglik: -1.3155e+02 - logprior: -1.8488e+00
Fitted a model with MAP estimate = -133.2045
expansions: [(12, 1), (13, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 144.9241 - loglik: -1.3531e+02 - logprior: -9.5590e+00
Epoch 2/2
13/13 - 1s - loss: 128.7191 - loglik: -1.2418e+02 - logprior: -4.4891e+00
Fitted a model with MAP estimate = -125.8971
expansions: [(0, 2)]
discards: [ 0 24 57]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 129.8000 - loglik: -1.2241e+02 - logprior: -7.2930e+00
Epoch 2/2
13/13 - 1s - loss: 122.5474 - loglik: -1.2034e+02 - logprior: -2.1510e+00
Fitted a model with MAP estimate = -121.2214
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 132.4550 - loglik: -1.2318e+02 - logprior: -9.2274e+00
Epoch 2/10
13/13 - 1s - loss: 124.1867 - loglik: -1.2060e+02 - logprior: -3.4255e+00
Epoch 3/10
13/13 - 1s - loss: 121.2896 - loglik: -1.1934e+02 - logprior: -1.7025e+00
Epoch 4/10
13/13 - 1s - loss: 120.2468 - loglik: -1.1860e+02 - logprior: -1.3705e+00
Epoch 5/10
13/13 - 1s - loss: 119.1066 - loglik: -1.1762e+02 - logprior: -1.2505e+00
Epoch 6/10
13/13 - 1s - loss: 119.1228 - loglik: -1.1765e+02 - logprior: -1.2608e+00
Fitted a model with MAP estimate = -118.5349
Time for alignment: 42.9994
Computed alignments with likelihoods: ['-118.7915', '-118.1923', '-118.5349']
Best model has likelihood: -118.1923
time for generating output: 0.1326
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faba1a97160>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabe6c10070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa2b56221f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa29dfe9070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac089c0a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faa82144940>, <__main__.SimpleDirichletPrior object at 0x7fabb35ce940>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9829 - loglik: -2.2842e+02 - logprior: -1.2553e+01
Epoch 2/10
11/11 - 1s - loss: 208.4142 - loglik: -2.0514e+02 - logprior: -3.2650e+00
Epoch 3/10
11/11 - 1s - loss: 182.3409 - loglik: -1.8016e+02 - logprior: -2.0807e+00
Epoch 4/10
11/11 - 1s - loss: 165.4632 - loglik: -1.6329e+02 - logprior: -1.8560e+00
Epoch 5/10
11/11 - 1s - loss: 160.3388 - loglik: -1.5835e+02 - logprior: -1.6191e+00
Epoch 6/10
11/11 - 1s - loss: 157.8611 - loglik: -1.5608e+02 - logprior: -1.4643e+00
Epoch 7/10
11/11 - 1s - loss: 157.1008 - loglik: -1.5561e+02 - logprior: -1.2339e+00
Epoch 8/10
11/11 - 1s - loss: 156.3243 - loglik: -1.5501e+02 - logprior: -1.0964e+00
Epoch 9/10
11/11 - 1s - loss: 156.1947 - loglik: -1.5496e+02 - logprior: -1.0079e+00
Epoch 10/10
11/11 - 1s - loss: 155.7136 - loglik: -1.5452e+02 - logprior: -9.6034e-01
Fitted a model with MAP estimate = -155.4186
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 174.3169 - loglik: -1.5844e+02 - logprior: -1.5714e+01
Epoch 2/2
11/11 - 1s - loss: 151.5313 - loglik: -1.4685e+02 - logprior: -4.5337e+00
Fitted a model with MAP estimate = -147.4817
expansions: []
discards: [ 0 30 39 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 162.8712 - loglik: -1.4873e+02 - logprior: -1.4128e+01
Epoch 2/2
11/11 - 1s - loss: 151.4048 - loglik: -1.4584e+02 - logprior: -5.4576e+00
Fitted a model with MAP estimate = -148.3725
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.4576 - loglik: -1.4616e+02 - logprior: -1.2275e+01
Epoch 2/10
11/11 - 1s - loss: 147.3238 - loglik: -1.4414e+02 - logprior: -3.0912e+00
Epoch 3/10
11/11 - 1s - loss: 145.1678 - loglik: -1.4316e+02 - logprior: -1.8082e+00
Epoch 4/10
11/11 - 1s - loss: 143.7098 - loglik: -1.4227e+02 - logprior: -1.1839e+00
Epoch 5/10
11/11 - 1s - loss: 143.2047 - loglik: -1.4209e+02 - logprior: -8.6246e-01
Epoch 6/10
11/11 - 1s - loss: 142.5247 - loglik: -1.4148e+02 - logprior: -7.9805e-01
Epoch 7/10
11/11 - 1s - loss: 142.3339 - loglik: -1.4144e+02 - logprior: -6.6565e-01
Epoch 8/10
11/11 - 1s - loss: 142.2765 - loglik: -1.4141e+02 - logprior: -6.3919e-01
Epoch 9/10
11/11 - 1s - loss: 141.9576 - loglik: -1.4113e+02 - logprior: -5.9403e-01
Epoch 10/10
11/11 - 1s - loss: 141.9815 - loglik: -1.4119e+02 - logprior: -5.5811e-01
Fitted a model with MAP estimate = -141.4985
Time for alignment: 47.4309
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9949 - loglik: -2.2838e+02 - logprior: -1.2554e+01
Epoch 2/10
11/11 - 1s - loss: 207.5473 - loglik: -2.0427e+02 - logprior: -3.2638e+00
Epoch 3/10
11/11 - 1s - loss: 180.1848 - loglik: -1.7799e+02 - logprior: -2.0705e+00
Epoch 4/10
11/11 - 1s - loss: 164.6943 - loglik: -1.6248e+02 - logprior: -1.8392e+00
Epoch 5/10
11/11 - 1s - loss: 159.7876 - loglik: -1.5777e+02 - logprior: -1.6024e+00
Epoch 6/10
11/11 - 1s - loss: 157.3483 - loglik: -1.5558e+02 - logprior: -1.4670e+00
Epoch 7/10
11/11 - 1s - loss: 156.5886 - loglik: -1.5506e+02 - logprior: -1.2543e+00
Epoch 8/10
11/11 - 1s - loss: 156.0410 - loglik: -1.5469e+02 - logprior: -1.1199e+00
Epoch 9/10
11/11 - 1s - loss: 155.8877 - loglik: -1.5462e+02 - logprior: -1.0275e+00
Epoch 10/10
11/11 - 1s - loss: 155.6222 - loglik: -1.5441e+02 - logprior: -9.8209e-01
Fitted a model with MAP estimate = -155.1982
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 173.1767 - loglik: -1.5742e+02 - logprior: -1.5695e+01
Epoch 2/2
11/11 - 1s - loss: 150.7995 - loglik: -1.4642e+02 - logprior: -4.3661e+00
Fitted a model with MAP estimate = -147.2138
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.5314 - loglik: -1.4912e+02 - logprior: -1.4093e+01
Epoch 2/2
11/11 - 1s - loss: 151.5321 - loglik: -1.4596e+02 - logprior: -5.4604e+00
Fitted a model with MAP estimate = -148.9769
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.0287 - loglik: -1.4568e+02 - logprior: -1.2284e+01
Epoch 2/10
11/11 - 1s - loss: 147.8812 - loglik: -1.4472e+02 - logprior: -3.1010e+00
Epoch 3/10
11/11 - 1s - loss: 145.3537 - loglik: -1.4337e+02 - logprior: -1.8154e+00
Epoch 4/10
11/11 - 1s - loss: 143.9355 - loglik: -1.4251e+02 - logprior: -1.1773e+00
Epoch 5/10
11/11 - 1s - loss: 143.0825 - loglik: -1.4195e+02 - logprior: -8.7293e-01
Epoch 6/10
11/11 - 1s - loss: 142.8494 - loglik: -1.4181e+02 - logprior: -8.0065e-01
Epoch 7/10
11/11 - 1s - loss: 142.2017 - loglik: -1.4130e+02 - logprior: -6.6970e-01
Epoch 8/10
11/11 - 1s - loss: 142.4992 - loglik: -1.4163e+02 - logprior: -6.5062e-01
Fitted a model with MAP estimate = -141.8374
Time for alignment: 44.7880
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 240.8867 - loglik: -2.2831e+02 - logprior: -1.2556e+01
Epoch 2/10
11/11 - 1s - loss: 207.7556 - loglik: -2.0448e+02 - logprior: -3.2726e+00
Epoch 3/10
11/11 - 1s - loss: 180.1798 - loglik: -1.7797e+02 - logprior: -2.1042e+00
Epoch 4/10
11/11 - 1s - loss: 164.5715 - loglik: -1.6235e+02 - logprior: -1.8564e+00
Epoch 5/10
11/11 - 1s - loss: 159.7194 - loglik: -1.5771e+02 - logprior: -1.6116e+00
Epoch 6/10
11/11 - 1s - loss: 158.0130 - loglik: -1.5625e+02 - logprior: -1.4745e+00
Epoch 7/10
11/11 - 1s - loss: 156.6994 - loglik: -1.5520e+02 - logprior: -1.2588e+00
Epoch 8/10
11/11 - 1s - loss: 156.3107 - loglik: -1.5497e+02 - logprior: -1.1264e+00
Epoch 9/10
11/11 - 1s - loss: 155.8837 - loglik: -1.5463e+02 - logprior: -1.0393e+00
Epoch 10/10
11/11 - 1s - loss: 155.3710 - loglik: -1.5416e+02 - logprior: -9.9591e-01
Fitted a model with MAP estimate = -155.2133
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 171.7607 - loglik: -1.5596e+02 - logprior: -1.5713e+01
Epoch 2/2
11/11 - 1s - loss: 150.4985 - loglik: -1.4598e+02 - logprior: -4.3802e+00
Fitted a model with MAP estimate = -146.7716
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 161.9039 - loglik: -1.4773e+02 - logprior: -1.4102e+01
Epoch 2/2
11/11 - 1s - loss: 151.1996 - loglik: -1.4554e+02 - logprior: -5.4624e+00
Fitted a model with MAP estimate = -148.1016
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.5217 - loglik: -1.4614e+02 - logprior: -1.2279e+01
Epoch 2/10
11/11 - 1s - loss: 147.7086 - loglik: -1.4450e+02 - logprior: -3.1093e+00
Epoch 3/10
11/11 - 1s - loss: 145.1677 - loglik: -1.4317e+02 - logprior: -1.8189e+00
Epoch 4/10
11/11 - 1s - loss: 144.0936 - loglik: -1.4266e+02 - logprior: -1.1793e+00
Epoch 5/10
11/11 - 1s - loss: 143.0018 - loglik: -1.4188e+02 - logprior: -8.6120e-01
Epoch 6/10
11/11 - 1s - loss: 142.6940 - loglik: -1.4166e+02 - logprior: -7.9235e-01
Epoch 7/10
11/11 - 1s - loss: 142.2909 - loglik: -1.4140e+02 - logprior: -6.6055e-01
Epoch 8/10
11/11 - 1s - loss: 142.2713 - loglik: -1.4142e+02 - logprior: -6.2296e-01
Epoch 9/10
11/11 - 1s - loss: 142.1642 - loglik: -1.4135e+02 - logprior: -5.8841e-01
Epoch 10/10
11/11 - 1s - loss: 141.8758 - loglik: -1.4110e+02 - logprior: -5.4643e-01
Fitted a model with MAP estimate = -141.5613
Time for alignment: 46.5796
Computed alignments with likelihoods: ['-141.4985', '-141.8374', '-141.5613']
Best model has likelihood: -141.4985
time for generating output: 0.1394
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7faa8c1eaa90>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa295d14ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fabf7a1ee50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba17d4dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac2260b250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa2b525cfd0>, <__main__.SimpleDirichletPrior object at 0x7fa86d97fd00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 14s - loss: 527.0790 - loglik: -5.2166e+02 - logprior: -5.3562e+00
Epoch 2/10
24/24 - 8s - loss: 381.5052 - loglik: -3.7904e+02 - logprior: -2.2600e+00
Epoch 3/10
24/24 - 7s - loss: 347.7300 - loglik: -3.4457e+02 - logprior: -2.6944e+00
Epoch 4/10
24/24 - 7s - loss: 344.7329 - loglik: -3.4170e+02 - logprior: -2.5837e+00
Epoch 5/10
24/24 - 7s - loss: 341.9490 - loglik: -3.3898e+02 - logprior: -2.5456e+00
Epoch 6/10
24/24 - 7s - loss: 339.5540 - loglik: -3.3659e+02 - logprior: -2.5615e+00
Epoch 7/10
24/24 - 7s - loss: 342.8379 - loglik: -3.3986e+02 - logprior: -2.5712e+00
Fitted a model with MAP estimate = -340.3996
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (34, 2), (35, 1), (36, 1), (37, 1), (39, 2), (47, 1), (48, 1), (49, 1), (61, 1), (65, 1), (83, 1), (87, 1), (88, 1), (91, 1), (101, 1), (110, 1), (113, 1), (116, 1), (117, 1), (119, 1), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 335.9264 - loglik: -3.2835e+02 - logprior: -7.4433e+00
Epoch 2/2
24/24 - 9s - loss: 314.5601 - loglik: -3.1133e+02 - logprior: -2.7893e+00
Fitted a model with MAP estimate = -310.5994
expansions: [(0, 2), (192, 1), (194, 1)]
discards: [  0  12  25  53 219]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 317.8153 - loglik: -3.1312e+02 - logprior: -4.5851e+00
Epoch 2/2
24/24 - 9s - loss: 307.1699 - loglik: -3.0667e+02 - logprior: -1.3083e-01
Fitted a model with MAP estimate = -304.4618
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 15s - loss: 315.2277 - loglik: -3.1092e+02 - logprior: -4.1909e+00
Epoch 2/10
24/24 - 9s - loss: 305.2281 - loglik: -3.0507e+02 - logprior: 0.2492
Epoch 3/10
24/24 - 9s - loss: 304.2414 - loglik: -3.0438e+02 - logprior: 0.7171
Epoch 4/10
24/24 - 9s - loss: 301.0479 - loglik: -3.0135e+02 - logprior: 0.9010
Epoch 5/10
24/24 - 9s - loss: 301.6847 - loglik: -3.0220e+02 - logprior: 1.0759
Fitted a model with MAP estimate = -300.3090
Time for alignment: 186.5093
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 525.8856 - loglik: -5.2047e+02 - logprior: -5.3912e+00
Epoch 2/10
24/24 - 8s - loss: 379.1063 - loglik: -3.7644e+02 - logprior: -2.4181e+00
Epoch 3/10
24/24 - 8s - loss: 346.6128 - loglik: -3.4331e+02 - logprior: -2.8422e+00
Epoch 4/10
24/24 - 7s - loss: 343.7554 - loglik: -3.4073e+02 - logprior: -2.5918e+00
Epoch 5/10
24/24 - 7s - loss: 342.0822 - loglik: -3.3912e+02 - logprior: -2.5413e+00
Epoch 6/10
24/24 - 8s - loss: 340.0380 - loglik: -3.3708e+02 - logprior: -2.5551e+00
Epoch 7/10
24/24 - 8s - loss: 341.2645 - loglik: -3.3830e+02 - logprior: -2.5711e+00
Fitted a model with MAP estimate = -339.9048
expansions: [(11, 1), (12, 3), (15, 1), (18, 1), (19, 1), (33, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (153, 2), (154, 3), (155, 1), (156, 1), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 337.8985 - loglik: -3.3048e+02 - logprior: -7.3252e+00
Epoch 2/2
24/24 - 9s - loss: 315.0271 - loglik: -3.1194e+02 - logprior: -2.7016e+00
Fitted a model with MAP estimate = -312.3088
expansions: [(0, 2), (12, 1), (13, 1), (188, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 315.8040 - loglik: -3.1093e+02 - logprior: -4.4868e+00
Epoch 2/2
24/24 - 9s - loss: 306.4790 - loglik: -3.0581e+02 - logprior: -6.9559e-02
Fitted a model with MAP estimate = -303.7451
expansions: [(151, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 315.5544 - loglik: -3.1124e+02 - logprior: -4.1650e+00
Epoch 2/10
24/24 - 9s - loss: 306.0362 - loglik: -3.0592e+02 - logprior: 0.2515
Epoch 3/10
24/24 - 9s - loss: 302.9400 - loglik: -3.0311e+02 - logprior: 0.7369
Epoch 4/10
24/24 - 9s - loss: 301.6549 - loglik: -3.0199e+02 - logprior: 0.9383
Epoch 5/10
24/24 - 9s - loss: 302.2305 - loglik: -3.0277e+02 - logprior: 1.1038
Fitted a model with MAP estimate = -299.8962
Time for alignment: 183.1071
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 527.7191 - loglik: -5.2229e+02 - logprior: -5.3682e+00
Epoch 2/10
24/24 - 8s - loss: 378.7137 - loglik: -3.7593e+02 - logprior: -2.4022e+00
Epoch 3/10
24/24 - 7s - loss: 345.8531 - loglik: -3.4243e+02 - logprior: -2.8542e+00
Epoch 4/10
24/24 - 8s - loss: 341.9236 - loglik: -3.3883e+02 - logprior: -2.6163e+00
Epoch 5/10
24/24 - 8s - loss: 340.3918 - loglik: -3.3736e+02 - logprior: -2.6155e+00
Epoch 6/10
24/24 - 7s - loss: 339.7633 - loglik: -3.3674e+02 - logprior: -2.6127e+00
Epoch 7/10
24/24 - 7s - loss: 339.9856 - loglik: -3.3698e+02 - logprior: -2.5963e+00
Fitted a model with MAP estimate = -338.8770
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 1), (18, 1), (21, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (83, 1), (87, 1), (88, 1), (91, 1), (103, 1), (108, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 336.9857 - loglik: -3.2942e+02 - logprior: -7.3748e+00
Epoch 2/2
24/24 - 9s - loss: 315.9440 - loglik: -3.1293e+02 - logprior: -2.6705e+00
Fitted a model with MAP estimate = -311.8523
expansions: [(0, 2), (192, 1), (194, 1), (215, 1)]
discards: [ 0 12 29]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 316.2578 - loglik: -3.1168e+02 - logprior: -4.4792e+00
Epoch 2/2
24/24 - 9s - loss: 307.7155 - loglik: -3.0727e+02 - logprior: -5.5648e-02
Fitted a model with MAP estimate = -303.8559
expansions: [(151, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 315.9999 - loglik: -3.1183e+02 - logprior: -4.1367e+00
Epoch 2/10
24/24 - 9s - loss: 306.8430 - loglik: -3.0698e+02 - logprior: 0.3250
Epoch 3/10
24/24 - 9s - loss: 303.6332 - loglik: -3.0401e+02 - logprior: 0.7813
Epoch 4/10
24/24 - 9s - loss: 301.2708 - loglik: -3.0173e+02 - logprior: 0.9620
Epoch 5/10
24/24 - 9s - loss: 300.9492 - loglik: -3.0155e+02 - logprior: 1.1138
Epoch 6/10
24/24 - 9s - loss: 300.4966 - loglik: -3.0127e+02 - logprior: 1.2847
Epoch 7/10
24/24 - 9s - loss: 299.4853 - loglik: -3.0043e+02 - logprior: 1.4553
Epoch 8/10
24/24 - 9s - loss: 298.9041 - loglik: -3.0006e+02 - logprior: 1.6540
Epoch 9/10
24/24 - 9s - loss: 299.9964 - loglik: -3.0132e+02 - logprior: 1.8259
Fitted a model with MAP estimate = -298.3798
Time for alignment: 219.4712
Computed alignments with likelihoods: ['-300.3090', '-299.8962', '-298.3798']
Best model has likelihood: -298.3798
time for generating output: 0.2828
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.989053736204089
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa86f4c9580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2b4837520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa86f1dbaf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fabaadb3e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2c40356d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fab2ff8a8b0>, <__main__.SimpleDirichletPrior object at 0x7fa27d7b6700>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.7471 - loglik: -2.7660e+02 - logprior: -2.7107e+01
Epoch 2/10
10/10 - 1s - loss: 246.5284 - loglik: -2.3958e+02 - logprior: -6.9268e+00
Epoch 3/10
10/10 - 1s - loss: 209.7641 - loglik: -2.0612e+02 - logprior: -3.6202e+00
Epoch 4/10
10/10 - 1s - loss: 188.4455 - loglik: -1.8575e+02 - logprior: -2.6384e+00
Epoch 5/10
10/10 - 1s - loss: 179.3018 - loglik: -1.7664e+02 - logprior: -2.4322e+00
Epoch 6/10
10/10 - 1s - loss: 175.3782 - loglik: -1.7267e+02 - logprior: -2.3348e+00
Epoch 7/10
10/10 - 1s - loss: 173.7694 - loglik: -1.7133e+02 - logprior: -2.0801e+00
Epoch 8/10
10/10 - 1s - loss: 172.8113 - loglik: -1.7055e+02 - logprior: -1.9475e+00
Epoch 9/10
10/10 - 1s - loss: 172.8328 - loglik: -1.7058e+02 - logprior: -1.9161e+00
Fitted a model with MAP estimate = -172.1071
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 4), (36, 1), (49, 3), (58, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.5174 - loglik: -1.7140e+02 - logprior: -3.4088e+01
Epoch 2/2
10/10 - 1s - loss: 170.3195 - loglik: -1.6004e+02 - logprior: -1.0143e+01
Fitted a model with MAP estimate = -162.8968
expansions: [(60, 1)]
discards: [ 0 90]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 190.1489 - loglik: -1.5913e+02 - logprior: -3.0895e+01
Epoch 2/2
10/10 - 1s - loss: 168.1639 - loglik: -1.5579e+02 - logprior: -1.2166e+01
Fitted a model with MAP estimate = -163.5047
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 185.4069 - loglik: -1.5634e+02 - logprior: -2.8982e+01
Epoch 2/10
10/10 - 1s - loss: 161.7270 - loglik: -1.5332e+02 - logprior: -8.1845e+00
Epoch 3/10
10/10 - 1s - loss: 155.9736 - loglik: -1.5272e+02 - logprior: -2.9467e+00
Epoch 4/10
10/10 - 1s - loss: 153.0196 - loglik: -1.5140e+02 - logprior: -1.2554e+00
Epoch 5/10
10/10 - 1s - loss: 151.8457 - loglik: -1.5094e+02 - logprior: -5.1577e-01
Epoch 6/10
10/10 - 1s - loss: 150.8039 - loglik: -1.5025e+02 - logprior: -1.7297e-01
Epoch 7/10
10/10 - 1s - loss: 150.5656 - loglik: -1.5021e+02 - logprior: 0.0301
Epoch 8/10
10/10 - 1s - loss: 150.1575 - loglik: -1.4995e+02 - logprior: 0.1925
Epoch 9/10
10/10 - 1s - loss: 149.9154 - loglik: -1.4987e+02 - logprior: 0.3527
Epoch 10/10
10/10 - 1s - loss: 149.6434 - loglik: -1.4973e+02 - logprior: 0.4884
Fitted a model with MAP estimate = -149.1222
Time for alignment: 47.5601
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.6768 - loglik: -2.7631e+02 - logprior: -2.7099e+01
Epoch 2/10
10/10 - 1s - loss: 249.0135 - loglik: -2.4194e+02 - logprior: -6.9178e+00
Epoch 3/10
10/10 - 1s - loss: 210.4572 - loglik: -2.0673e+02 - logprior: -3.6672e+00
Epoch 4/10
10/10 - 1s - loss: 188.8632 - loglik: -1.8614e+02 - logprior: -2.6598e+00
Epoch 5/10
10/10 - 1s - loss: 180.2915 - loglik: -1.7778e+02 - logprior: -2.3341e+00
Epoch 6/10
10/10 - 1s - loss: 176.2142 - loglik: -1.7359e+02 - logprior: -2.2731e+00
Epoch 7/10
10/10 - 1s - loss: 175.1313 - loglik: -1.7270e+02 - logprior: -2.0402e+00
Epoch 8/10
10/10 - 1s - loss: 174.4397 - loglik: -1.7222e+02 - logprior: -1.8464e+00
Epoch 9/10
10/10 - 1s - loss: 173.7177 - loglik: -1.7154e+02 - logprior: -1.7969e+00
Epoch 10/10
10/10 - 1s - loss: 173.9578 - loglik: -1.7181e+02 - logprior: -1.7816e+00
Fitted a model with MAP estimate = -172.9827
expansions: [(0, 3), (11, 1), (12, 1), (33, 1), (34, 2), (35, 4), (47, 3), (49, 1), (58, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 205.2343 - loglik: -1.7092e+02 - logprior: -3.4211e+01
Epoch 2/2
10/10 - 1s - loss: 169.3824 - loglik: -1.5885e+02 - logprior: -1.0274e+01
Fitted a model with MAP estimate = -162.2155
expansions: []
discards: [ 0 40 92]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.2971 - loglik: -1.6037e+02 - logprior: -3.0893e+01
Epoch 2/2
10/10 - 1s - loss: 169.7036 - loglik: -1.5754e+02 - logprior: -1.2155e+01
Fitted a model with MAP estimate = -165.1650
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.2124 - loglik: -1.5719e+02 - logprior: -2.8997e+01
Epoch 2/10
10/10 - 1s - loss: 162.9487 - loglik: -1.5472e+02 - logprior: -8.1714e+00
Epoch 3/10
10/10 - 1s - loss: 156.0461 - loglik: -1.5296e+02 - logprior: -2.9089e+00
Epoch 4/10
10/10 - 1s - loss: 153.2649 - loglik: -1.5172e+02 - logprior: -1.2227e+00
Epoch 5/10
10/10 - 1s - loss: 152.1549 - loglik: -1.5131e+02 - logprior: -5.0435e-01
Epoch 6/10
10/10 - 1s - loss: 151.0392 - loglik: -1.5053e+02 - logprior: -1.8181e-01
Epoch 7/10
10/10 - 1s - loss: 150.9734 - loglik: -1.5065e+02 - logprior: 0.0156
Epoch 8/10
10/10 - 1s - loss: 150.1460 - loglik: -1.4998e+02 - logprior: 0.1870
Epoch 9/10
10/10 - 1s - loss: 150.2967 - loglik: -1.5026e+02 - logprior: 0.3408
Fitted a model with MAP estimate = -149.4827
Time for alignment: 46.7849
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8557 - loglik: -2.7664e+02 - logprior: -2.7099e+01
Epoch 2/10
10/10 - 1s - loss: 247.9838 - loglik: -2.4100e+02 - logprior: -6.9113e+00
Epoch 3/10
10/10 - 1s - loss: 209.8475 - loglik: -2.0621e+02 - logprior: -3.6257e+00
Epoch 4/10
10/10 - 1s - loss: 190.5884 - loglik: -1.8793e+02 - logprior: -2.6033e+00
Epoch 5/10
10/10 - 1s - loss: 181.8288 - loglik: -1.7936e+02 - logprior: -2.2845e+00
Epoch 6/10
10/10 - 1s - loss: 178.9153 - loglik: -1.7637e+02 - logprior: -2.1832e+00
Epoch 7/10
10/10 - 1s - loss: 176.4145 - loglik: -1.7402e+02 - logprior: -1.9991e+00
Epoch 8/10
10/10 - 1s - loss: 174.9296 - loglik: -1.7272e+02 - logprior: -1.8315e+00
Epoch 9/10
10/10 - 1s - loss: 173.8671 - loglik: -1.7170e+02 - logprior: -1.7934e+00
Epoch 10/10
10/10 - 1s - loss: 173.6793 - loglik: -1.7155e+02 - logprior: -1.7657e+00
Fitted a model with MAP estimate = -172.8700
expansions: [(0, 3), (5, 1), (12, 1), (34, 1), (36, 5), (48, 3), (49, 2), (58, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 207.0590 - loglik: -1.7269e+02 - logprior: -3.4242e+01
Epoch 2/2
10/10 - 1s - loss: 170.9463 - loglik: -1.6073e+02 - logprior: -1.0165e+01
Fitted a model with MAP estimate = -163.0689
expansions: []
discards: [ 0 92]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 190.1789 - loglik: -1.5935e+02 - logprior: -3.0788e+01
Epoch 2/2
10/10 - 1s - loss: 169.3337 - loglik: -1.5718e+02 - logprior: -1.2131e+01
Fitted a model with MAP estimate = -165.2418
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.5971 - loglik: -1.5648e+02 - logprior: -2.8975e+01
Epoch 2/10
10/10 - 1s - loss: 162.6028 - loglik: -1.5432e+02 - logprior: -8.1951e+00
Epoch 3/10
10/10 - 1s - loss: 156.1291 - loglik: -1.5306e+02 - logprior: -2.9016e+00
Epoch 4/10
10/10 - 1s - loss: 152.7659 - loglik: -1.5125e+02 - logprior: -1.2267e+00
Epoch 5/10
10/10 - 1s - loss: 151.6453 - loglik: -1.5078e+02 - logprior: -5.0832e-01
Epoch 6/10
10/10 - 1s - loss: 150.9242 - loglik: -1.5039e+02 - logprior: -1.8203e-01
Epoch 7/10
10/10 - 1s - loss: 150.2746 - loglik: -1.4993e+02 - logprior: 0.0163
Epoch 8/10
10/10 - 1s - loss: 149.9701 - loglik: -1.4975e+02 - logprior: 0.1587
Epoch 9/10
10/10 - 1s - loss: 149.3660 - loglik: -1.4926e+02 - logprior: 0.2747
Epoch 10/10
10/10 - 1s - loss: 149.5026 - loglik: -1.4952e+02 - logprior: 0.4140
Fitted a model with MAP estimate = -148.7657
Time for alignment: 48.3642
Computed alignments with likelihoods: ['-149.1222', '-149.4827', '-148.7657']
Best model has likelihood: -148.7657
time for generating output: 0.1676
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7886075949367088
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa908677550>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2ad602eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faaf40d4400>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2942bc0a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa2ac3e6f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fa29cc70d30>, <__main__.SimpleDirichletPrior object at 0x7fa2b528a5e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.0100 - loglik: -1.4586e+02 - logprior: -5.0956e+00
Epoch 2/10
16/16 - 2s - loss: 126.3484 - loglik: -1.2469e+02 - logprior: -1.5687e+00
Epoch 3/10
16/16 - 2s - loss: 114.8874 - loglik: -1.1295e+02 - logprior: -1.6679e+00
Epoch 4/10
16/16 - 2s - loss: 110.4036 - loglik: -1.0840e+02 - logprior: -1.7503e+00
Epoch 5/10
16/16 - 2s - loss: 108.7076 - loglik: -1.0674e+02 - logprior: -1.7063e+00
Epoch 6/10
16/16 - 2s - loss: 107.9182 - loglik: -1.0600e+02 - logprior: -1.6646e+00
Epoch 7/10
16/16 - 2s - loss: 108.1758 - loglik: -1.0630e+02 - logprior: -1.6270e+00
Fitted a model with MAP estimate = -107.5182
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (17, 1), (23, 6), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 115.3999 - loglik: -1.0911e+02 - logprior: -6.2504e+00
Epoch 2/2
16/16 - 2s - loss: 105.5397 - loglik: -1.0240e+02 - logprior: -3.0579e+00
Fitted a model with MAP estimate = -103.9820
expansions: [(0, 1)]
discards: [ 0 31 36]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.3622 - loglik: -1.0162e+02 - logprior: -4.6046e+00
Epoch 2/2
16/16 - 2s - loss: 102.9079 - loglik: -1.0099e+02 - logprior: -1.6377e+00
Fitted a model with MAP estimate = -101.3318
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.2664 - loglik: -1.0293e+02 - logprior: -6.2014e+00
Epoch 2/10
16/16 - 2s - loss: 103.9632 - loglik: -1.0096e+02 - logprior: -2.7789e+00
Epoch 3/10
16/16 - 2s - loss: 101.9046 - loglik: -9.9986e+01 - logprior: -1.5686e+00
Epoch 4/10
16/16 - 2s - loss: 101.1080 - loglik: -9.9367e+01 - logprior: -1.3357e+00
Epoch 5/10
16/16 - 2s - loss: 100.3852 - loglik: -9.8665e+01 - logprior: -1.3206e+00
Epoch 6/10
16/16 - 2s - loss: 99.4490 - loglik: -9.7737e+01 - logprior: -1.3099e+00
Epoch 7/10
16/16 - 2s - loss: 100.0227 - loglik: -9.8344e+01 - logprior: -1.2908e+00
Fitted a model with MAP estimate = -98.9668
Time for alignment: 56.3190
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.2581 - loglik: -1.4615e+02 - logprior: -5.0967e+00
Epoch 2/10
16/16 - 2s - loss: 128.3334 - loglik: -1.2671e+02 - logprior: -1.5738e+00
Epoch 3/10
16/16 - 2s - loss: 116.4186 - loglik: -1.1437e+02 - logprior: -1.7389e+00
Epoch 4/10
16/16 - 2s - loss: 110.7711 - loglik: -1.0860e+02 - logprior: -1.7949e+00
Epoch 5/10
16/16 - 2s - loss: 109.8078 - loglik: -1.0777e+02 - logprior: -1.7539e+00
Epoch 6/10
16/16 - 2s - loss: 109.1821 - loglik: -1.0716e+02 - logprior: -1.7437e+00
Epoch 7/10
16/16 - 2s - loss: 108.0523 - loglik: -1.0610e+02 - logprior: -1.6738e+00
Epoch 8/10
16/16 - 2s - loss: 107.4278 - loglik: -1.0545e+02 - logprior: -1.6683e+00
Epoch 9/10
16/16 - 2s - loss: 107.1057 - loglik: -1.0513e+02 - logprior: -1.6626e+00
Epoch 10/10
16/16 - 2s - loss: 107.3130 - loglik: -1.0534e+02 - logprior: -1.6584e+00
Fitted a model with MAP estimate = -106.8533
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (19, 1), (23, 5), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 115.2918 - loglik: -1.0880e+02 - logprior: -6.2653e+00
Epoch 2/2
16/16 - 2s - loss: 104.8527 - loglik: -1.0142e+02 - logprior: -3.0672e+00
Fitted a model with MAP estimate = -103.4664
expansions: [(0, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.5614 - loglik: -1.0187e+02 - logprior: -4.6010e+00
Epoch 2/2
16/16 - 2s - loss: 102.9148 - loglik: -1.0110e+02 - logprior: -1.6189e+00
Fitted a model with MAP estimate = -101.5204
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.4541 - loglik: -1.0324e+02 - logprior: -6.1791e+00
Epoch 2/10
16/16 - 2s - loss: 104.4604 - loglik: -1.0156e+02 - logprior: -2.7729e+00
Epoch 3/10
16/16 - 2s - loss: 101.9462 - loglik: -1.0012e+02 - logprior: -1.5679e+00
Epoch 4/10
16/16 - 2s - loss: 101.2400 - loglik: -9.9589e+01 - logprior: -1.3044e+00
Epoch 5/10
16/16 - 2s - loss: 100.7261 - loglik: -9.9086e+01 - logprior: -1.2801e+00
Epoch 6/10
16/16 - 2s - loss: 100.1452 - loglik: -9.8507e+01 - logprior: -1.2766e+00
Epoch 7/10
16/16 - 2s - loss: 99.4590 - loglik: -9.7840e+01 - logprior: -1.2548e+00
Epoch 8/10
16/16 - 2s - loss: 99.6129 - loglik: -9.8015e+01 - logprior: -1.2395e+00
Fitted a model with MAP estimate = -98.9333
Time for alignment: 62.2996
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.2816 - loglik: -1.4602e+02 - logprior: -5.0967e+00
Epoch 2/10
16/16 - 2s - loss: 127.4664 - loglik: -1.2583e+02 - logprior: -1.5729e+00
Epoch 3/10
16/16 - 2s - loss: 115.5854 - loglik: -1.1365e+02 - logprior: -1.7217e+00
Epoch 4/10
16/16 - 2s - loss: 110.1514 - loglik: -1.0809e+02 - logprior: -1.7526e+00
Epoch 5/10
16/16 - 2s - loss: 109.2655 - loglik: -1.0725e+02 - logprior: -1.7153e+00
Epoch 6/10
16/16 - 2s - loss: 107.8627 - loglik: -1.0588e+02 - logprior: -1.6986e+00
Epoch 7/10
16/16 - 2s - loss: 107.7428 - loglik: -1.0581e+02 - logprior: -1.6450e+00
Epoch 8/10
16/16 - 2s - loss: 107.2203 - loglik: -1.0529e+02 - logprior: -1.6369e+00
Epoch 9/10
16/16 - 2s - loss: 107.2993 - loglik: -1.0537e+02 - logprior: -1.6279e+00
Fitted a model with MAP estimate = -106.8104
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (19, 1), (22, 5), (23, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 115.3752 - loglik: -1.0906e+02 - logprior: -6.2538e+00
Epoch 2/2
16/16 - 2s - loss: 105.4951 - loglik: -1.0227e+02 - logprior: -3.0917e+00
Fitted a model with MAP estimate = -103.7429
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.3460 - loglik: -1.0156e+02 - logprior: -4.6329e+00
Epoch 2/2
16/16 - 2s - loss: 102.1749 - loglik: -1.0028e+02 - logprior: -1.6506e+00
Fitted a model with MAP estimate = -101.1766
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.2422 - loglik: -1.0298e+02 - logprior: -6.2198e+00
Epoch 2/10
16/16 - 2s - loss: 104.3269 - loglik: -1.0142e+02 - logprior: -2.8335e+00
Epoch 3/10
16/16 - 2s - loss: 101.9927 - loglik: -1.0020e+02 - logprior: -1.6185e+00
Epoch 4/10
16/16 - 2s - loss: 101.4367 - loglik: -9.9802e+01 - logprior: -1.3390e+00
Epoch 5/10
16/16 - 2s - loss: 100.3269 - loglik: -9.8654e+01 - logprior: -1.3203e+00
Epoch 6/10
16/16 - 2s - loss: 100.1264 - loglik: -9.8433e+01 - logprior: -1.3223e+00
Epoch 7/10
16/16 - 2s - loss: 99.2373 - loglik: -9.7577e+01 - logprior: -1.3002e+00
Epoch 8/10
16/16 - 2s - loss: 99.3786 - loglik: -9.7726e+01 - logprior: -1.2867e+00
Fitted a model with MAP estimate = -98.7877
Time for alignment: 60.1499
Computed alignments with likelihoods: ['-98.9668', '-98.9333', '-98.7877']
Best model has likelihood: -98.7877
time for generating output: 0.1335
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9143780290791599
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa2b4ca1b80>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa294412be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fab2f068130>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac443794f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fac3b9934c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7faba16b46d0>, <__main__.SimpleDirichletPrior object at 0x7fa89016d790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 585.3917 - loglik: -5.8341e+02 - logprior: -1.6954e+00
Epoch 2/10
39/39 - 12s - loss: 530.4891 - loglik: -5.2821e+02 - logprior: -1.0056e+00
Epoch 3/10
39/39 - 12s - loss: 522.0827 - loglik: -5.1918e+02 - logprior: -9.6976e-01
Epoch 4/10
39/39 - 12s - loss: 518.4722 - loglik: -5.1530e+02 - logprior: -9.5255e-01
Epoch 5/10
39/39 - 12s - loss: 515.9341 - loglik: -5.1278e+02 - logprior: -9.6377e-01
Epoch 6/10
39/39 - 12s - loss: 514.2841 - loglik: -5.1136e+02 - logprior: -9.8208e-01
Epoch 7/10
39/39 - 12s - loss: 513.1354 - loglik: -5.1043e+02 - logprior: -1.0017e+00
Epoch 8/10
39/39 - 12s - loss: 512.3334 - loglik: -5.0979e+02 - logprior: -1.0136e+00
Epoch 9/10
39/39 - 12s - loss: 511.7083 - loglik: -5.0929e+02 - logprior: -1.0302e+00
Epoch 10/10
39/39 - 12s - loss: 511.0983 - loglik: -5.0877e+02 - logprior: -1.0324e+00
Fitted a model with MAP estimate = -491.9905
expansions: [(7, 1), (11, 1), (13, 1), (21, 3), (25, 1), (27, 1), (54, 1), (55, 1), (56, 2), (77, 5), (78, 1), (107, 1), (109, 1), (122, 6), (126, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 565.6675 - loglik: -5.6320e+02 - logprior: -2.2561e+00
Epoch 2/2
39/39 - 14s - loss: 522.4000 - loglik: -5.2008e+02 - logprior: -1.2605e+00
Fitted a model with MAP estimate = -472.2956
expansions: [(6, 1), (15, 1), (35, 1), (95, 1), (127, 1), (141, 4), (146, 2), (147, 3), (150, 1)]
discards: [  0  16  27  65 153 154 155 156 157 158 159 160 161 162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 532.4028 - loglik: -5.2962e+02 - logprior: -2.6102e+00
Epoch 2/2
39/39 - 14s - loss: 520.3646 - loglik: -5.1890e+02 - logprior: -8.7825e-01
Fitted a model with MAP estimate = -472.6300
expansions: [(0, 2), (28, 5), (163, 1)]
discards: [  0 144 145]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 21s - loss: 475.1165 - loglik: -4.7369e+02 - logprior: -1.0639e+00
Epoch 2/10
51/51 - 19s - loss: 464.9273 - loglik: -4.6249e+02 - logprior: -6.5545e-01
Epoch 3/10
51/51 - 18s - loss: 460.1714 - loglik: -4.5679e+02 - logprior: -6.4258e-01
Epoch 4/10
51/51 - 18s - loss: 457.5454 - loglik: -4.5427e+02 - logprior: -6.3015e-01
Epoch 5/10
51/51 - 18s - loss: 454.3852 - loglik: -4.5137e+02 - logprior: -5.9004e-01
Epoch 6/10
51/51 - 18s - loss: 454.0810 - loglik: -4.5125e+02 - logprior: -5.4989e-01
Epoch 7/10
51/51 - 18s - loss: 451.1115 - loglik: -4.4854e+02 - logprior: -5.1757e-01
Epoch 8/10
51/51 - 18s - loss: 450.6340 - loglik: -4.4836e+02 - logprior: -4.7404e-01
Epoch 9/10
51/51 - 18s - loss: 449.3735 - loglik: -4.4745e+02 - logprior: -4.3049e-01
Epoch 10/10
51/51 - 18s - loss: 449.4133 - loglik: -4.4765e+02 - logprior: -3.9396e-01
Fitted a model with MAP estimate = -446.9811
Time for alignment: 476.0810
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 585.8901 - loglik: -5.8382e+02 - logprior: -1.6909e+00
Epoch 2/10
39/39 - 12s - loss: 532.4033 - loglik: -5.3004e+02 - logprior: -9.7656e-01
Epoch 3/10
39/39 - 12s - loss: 523.9839 - loglik: -5.2099e+02 - logprior: -9.5123e-01
Epoch 4/10
39/39 - 12s - loss: 520.2614 - loglik: -5.1704e+02 - logprior: -9.4653e-01
Epoch 5/10
39/39 - 12s - loss: 517.7700 - loglik: -5.1461e+02 - logprior: -9.6713e-01
Epoch 6/10
39/39 - 12s - loss: 515.8624 - loglik: -5.1290e+02 - logprior: -9.9820e-01
Epoch 7/10
39/39 - 12s - loss: 514.4988 - loglik: -5.1177e+02 - logprior: -1.0209e+00
Epoch 8/10
39/39 - 12s - loss: 513.4683 - loglik: -5.1088e+02 - logprior: -1.0381e+00
Epoch 9/10
39/39 - 12s - loss: 513.0333 - loglik: -5.1058e+02 - logprior: -1.0460e+00
Epoch 10/10
39/39 - 12s - loss: 512.2261 - loglik: -5.0987e+02 - logprior: -1.0602e+00
Fitted a model with MAP estimate = -492.4091
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 3), (25, 1), (55, 2), (71, 2), (74, 13), (77, 2), (95, 2), (96, 2), (108, 1), (122, 10)]
discards: [135 136 137]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 563.0548 - loglik: -5.6048e+02 - logprior: -2.2766e+00
Epoch 2/2
39/39 - 15s - loss: 522.8969 - loglik: -5.2048e+02 - logprior: -1.3145e+00
Fitted a model with MAP estimate = -473.4814
expansions: [(16, 1), (92, 2), (126, 1), (158, 2), (159, 5), (177, 1)]
discards: [  0  17  28  65  98  99 100 101 102 103 104 105 106 107 108 109 110 124
 127 128 163 164 165 166 167 168 169 170 171 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 536.4474 - loglik: -5.3353e+02 - logprior: -2.6313e+00
Epoch 2/2
39/39 - 14s - loss: 522.2527 - loglik: -5.2021e+02 - logprior: -8.0652e-01
Fitted a model with MAP estimate = -473.6624
expansions: [(0, 2), (32, 2), (152, 2), (154, 5)]
discards: [  0 146]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 23s - loss: 475.5107 - loglik: -4.7393e+02 - logprior: -1.0666e+00
Epoch 2/10
51/51 - 18s - loss: 463.8423 - loglik: -4.6131e+02 - logprior: -6.7095e-01
Epoch 3/10
51/51 - 18s - loss: 460.3531 - loglik: -4.5707e+02 - logprior: -6.3801e-01
Epoch 4/10
51/51 - 18s - loss: 458.0824 - loglik: -4.5474e+02 - logprior: -6.2690e-01
Epoch 5/10
51/51 - 18s - loss: 454.2881 - loglik: -4.5114e+02 - logprior: -6.0920e-01
Epoch 6/10
51/51 - 18s - loss: 453.1142 - loglik: -4.5023e+02 - logprior: -5.7163e-01
Epoch 7/10
51/51 - 18s - loss: 451.8857 - loglik: -4.4934e+02 - logprior: -5.4274e-01
Epoch 8/10
51/51 - 18s - loss: 451.2071 - loglik: -4.4893e+02 - logprior: -5.0821e-01
Epoch 9/10
51/51 - 18s - loss: 449.6302 - loglik: -4.4765e+02 - logprior: -4.8333e-01
Epoch 10/10
51/51 - 18s - loss: 449.1355 - loglik: -4.4731e+02 - logprior: -4.4817e-01
Fitted a model with MAP estimate = -447.2259
Time for alignment: 469.0260
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 585.1625 - loglik: -5.8285e+02 - logprior: -1.6656e+00
Epoch 2/10
39/39 - 13s - loss: 533.6810 - loglik: -5.3091e+02 - logprior: -9.4555e-01
Epoch 3/10
39/39 - 12s - loss: 525.4155 - loglik: -5.2213e+02 - logprior: -9.5132e-01
Epoch 4/10
39/39 - 13s - loss: 521.8980 - loglik: -5.1853e+02 - logprior: -9.4701e-01
Epoch 5/10
39/39 - 13s - loss: 519.2043 - loglik: -5.1595e+02 - logprior: -9.4824e-01
Epoch 6/10
39/39 - 12s - loss: 517.2404 - loglik: -5.1423e+02 - logprior: -9.7118e-01
Epoch 7/10
39/39 - 12s - loss: 515.7787 - loglik: -5.1302e+02 - logprior: -9.9272e-01
Epoch 8/10
39/39 - 12s - loss: 515.0450 - loglik: -5.1250e+02 - logprior: -9.9956e-01
Epoch 9/10
39/39 - 12s - loss: 513.0596 - loglik: -5.1063e+02 - logprior: -1.0185e+00
Epoch 10/10
39/39 - 13s - loss: 511.6115 - loglik: -5.0930e+02 - logprior: -1.0315e+00
Fitted a model with MAP estimate = -492.6073
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (21, 2), (23, 7), (55, 1), (56, 2), (70, 1), (77, 3), (93, 1), (94, 1), (95, 1), (113, 1), (122, 5), (123, 1), (124, 1), (126, 1), (146, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 563.6940 - loglik: -5.6120e+02 - logprior: -2.2578e+00
Epoch 2/2
39/39 - 15s - loss: 522.1591 - loglik: -5.2030e+02 - logprior: -1.1955e+00
Fitted a model with MAP estimate = -473.2985
expansions: [(97, 1), (116, 1), (134, 1), (137, 1), (147, 4), (149, 1), (150, 2), (151, 2), (155, 1), (179, 2)]
discards: [  0  18  28  29  71 117 138 159 160 161 162 163 164 165 166 167 168 169
 170 181 182]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 532.9108 - loglik: -5.3011e+02 - logprior: -2.6533e+00
Epoch 2/2
39/39 - 14s - loss: 519.4312 - loglik: -5.1790e+02 - logprior: -9.3508e-01
Fitted a model with MAP estimate = -472.0122
expansions: [(0, 2), (27, 1), (147, 4)]
discards: [  0 163 165]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 24s - loss: 469.5386 - loglik: -4.6795e+02 - logprior: -1.0630e+00
Epoch 2/10
51/51 - 19s - loss: 462.8487 - loglik: -4.6090e+02 - logprior: -6.4809e-01
Epoch 3/10
51/51 - 19s - loss: 458.4680 - loglik: -4.5591e+02 - logprior: -5.8312e-01
Epoch 4/10
51/51 - 19s - loss: 456.0961 - loglik: -4.5317e+02 - logprior: -5.2940e-01
Epoch 5/10
51/51 - 18s - loss: 451.9151 - loglik: -4.4891e+02 - logprior: -4.9773e-01
Epoch 6/10
51/51 - 18s - loss: 450.8449 - loglik: -4.4792e+02 - logprior: -4.6186e-01
Epoch 7/10
51/51 - 18s - loss: 450.2406 - loglik: -4.4763e+02 - logprior: -4.3287e-01
Epoch 8/10
51/51 - 18s - loss: 448.1675 - loglik: -4.4589e+02 - logprior: -3.9976e-01
Epoch 9/10
51/51 - 18s - loss: 448.2213 - loglik: -4.4623e+02 - logprior: -3.5860e-01
Fitted a model with MAP estimate = -445.2765
Time for alignment: 456.0561
Computed alignments with likelihoods: ['-446.9811', '-447.2259', '-445.2765']
Best model has likelihood: -445.2765
time for generating output: 0.2903
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6985006518904824
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7faa200c2490>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7faa200c2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e10a0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1580>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1c70>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e12e0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7faa200e1e20>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e16d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1b20>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1130>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200e1340>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025c70>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025670>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025ac0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025130>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025910>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025fa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7faa20025be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fa86e9a3fd0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa28c34baf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fa294c92f10>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fa294c92a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7faa20118ca0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa200255e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7faa20025580>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7faba2376f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7fac76df8d30> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7fab336461f0>, <function make_default_emission_matrix at 0x7fab336461f0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fac11739b50>, <__main__.SimpleDirichletPrior object at 0x7fa27df29910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.4670 - loglik: -4.8549e+02 - logprior: -5.9582e+00
Epoch 2/10
14/14 - 4s - loss: 436.1656 - loglik: -4.3477e+02 - logprior: -1.2903e+00
Epoch 3/10
14/14 - 4s - loss: 396.7126 - loglik: -3.9499e+02 - logprior: -1.4022e+00
Epoch 4/10
14/14 - 4s - loss: 385.4391 - loglik: -3.8346e+02 - logprior: -1.4387e+00
Epoch 5/10
14/14 - 4s - loss: 381.7977 - loglik: -3.7982e+02 - logprior: -1.4064e+00
Epoch 6/10
14/14 - 4s - loss: 380.1909 - loglik: -3.7825e+02 - logprior: -1.3799e+00
Epoch 7/10
14/14 - 4s - loss: 378.6526 - loglik: -3.7676e+02 - logprior: -1.3344e+00
Epoch 8/10
14/14 - 4s - loss: 378.3308 - loglik: -3.7647e+02 - logprior: -1.3276e+00
Epoch 9/10
14/14 - 4s - loss: 377.9176 - loglik: -3.7606e+02 - logprior: -1.3415e+00
Epoch 10/10
14/14 - 4s - loss: 377.7799 - loglik: -3.7591e+02 - logprior: -1.3761e+00
Fitted a model with MAP estimate = -376.4621
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (70, 4), (74, 1), (96, 1), (103, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 385.9622 - loglik: -3.8143e+02 - logprior: -4.3515e+00
Epoch 2/2
29/29 - 7s - loss: 363.5201 - loglik: -3.6202e+02 - logprior: -1.0068e+00
Fitted a model with MAP estimate = -359.6646
expansions: [(127, 1)]
discards: [ 41 150]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 368.6846 - loglik: -3.6527e+02 - logprior: -3.1885e+00
Epoch 2/2
29/29 - 7s - loss: 361.8176 - loglik: -3.6071e+02 - logprior: -6.6868e-01
Fitted a model with MAP estimate = -358.8895
expansions: [(2, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 13s - loss: 371.5189 - loglik: -3.6676e+02 - logprior: -4.6409e+00
Epoch 2/10
29/29 - 7s - loss: 362.8954 - loglik: -3.6085e+02 - logprior: -1.7365e+00
Epoch 3/10
29/29 - 7s - loss: 360.2741 - loglik: -3.5949e+02 - logprior: -2.3849e-01
Epoch 4/10
29/29 - 7s - loss: 356.8365 - loglik: -3.5588e+02 - logprior: -2.7094e-01
Epoch 5/10
29/29 - 7s - loss: 355.8013 - loglik: -3.5495e+02 - logprior: -1.4933e-01
Epoch 6/10
29/29 - 7s - loss: 355.2939 - loglik: -3.5453e+02 - logprior: -7.6488e-02
Epoch 7/10
29/29 - 7s - loss: 355.1967 - loglik: -3.5457e+02 - logprior: 0.0245
Epoch 8/10
29/29 - 7s - loss: 353.5160 - loglik: -3.5303e+02 - logprior: 0.1258
Epoch 9/10
29/29 - 7s - loss: 354.8343 - loglik: -3.5448e+02 - logprior: 0.2217
Fitted a model with MAP estimate = -353.2131
Time for alignment: 183.4494
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.7860 - loglik: -4.8571e+02 - logprior: -5.9614e+00
Epoch 2/10
14/14 - 4s - loss: 433.9078 - loglik: -4.3227e+02 - logprior: -1.2821e+00
Epoch 3/10
14/14 - 4s - loss: 396.3878 - loglik: -3.9433e+02 - logprior: -1.3932e+00
Epoch 4/10
14/14 - 4s - loss: 384.9962 - loglik: -3.8282e+02 - logprior: -1.3781e+00
Epoch 5/10
14/14 - 4s - loss: 382.0863 - loglik: -3.8003e+02 - logprior: -1.3326e+00
Epoch 6/10
14/14 - 4s - loss: 380.7003 - loglik: -3.7876e+02 - logprior: -1.3075e+00
Epoch 7/10
14/14 - 4s - loss: 379.4108 - loglik: -3.7757e+02 - logprior: -1.2678e+00
Epoch 8/10
14/14 - 4s - loss: 378.6599 - loglik: -3.7686e+02 - logprior: -1.2686e+00
Epoch 9/10
14/14 - 4s - loss: 378.1216 - loglik: -3.7637e+02 - logprior: -1.2542e+00
Epoch 10/10
14/14 - 4s - loss: 377.6280 - loglik: -3.7590e+02 - logprior: -1.2567e+00
Fitted a model with MAP estimate = -377.2779
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (51, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (70, 3), (96, 1), (103, 5), (117, 1), (120, 2), (127, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 388.1057 - loglik: -3.8342e+02 - logprior: -4.4069e+00
Epoch 2/2
29/29 - 7s - loss: 363.2297 - loglik: -3.6160e+02 - logprior: -1.0514e+00
Fitted a model with MAP estimate = -359.0260
expansions: [(90, 2), (125, 1), (158, 1)]
discards: [ 41  91 148]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 369.3937 - loglik: -3.6620e+02 - logprior: -3.0972e+00
Epoch 2/2
29/29 - 7s - loss: 360.5284 - loglik: -3.5942e+02 - logprior: -6.8765e-01
Fitted a model with MAP estimate = -357.3228
expansions: []
discards: [ 0 90 91]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 372.5587 - loglik: -3.6794e+02 - logprior: -4.5685e+00
Epoch 2/10
29/29 - 7s - loss: 362.7090 - loglik: -3.6077e+02 - logprior: -1.5615e+00
Epoch 3/10
29/29 - 7s - loss: 359.9567 - loglik: -3.5902e+02 - logprior: -1.8355e-01
Epoch 4/10
29/29 - 7s - loss: 357.6579 - loglik: -3.5682e+02 - logprior: -5.6981e-02
Epoch 5/10
29/29 - 7s - loss: 356.1644 - loglik: -3.5544e+02 - logprior: 1.2602e-04
Epoch 6/10
29/29 - 7s - loss: 355.4280 - loglik: -3.5485e+02 - logprior: 0.0842
Epoch 7/10
29/29 - 7s - loss: 354.8457 - loglik: -3.5442e+02 - logprior: 0.1899
Epoch 8/10
29/29 - 7s - loss: 354.6513 - loglik: -3.5439e+02 - logprior: 0.2876
Epoch 9/10
29/29 - 7s - loss: 353.8811 - loglik: -3.5376e+02 - logprior: 0.3797
Epoch 10/10
29/29 - 7s - loss: 354.3466 - loglik: -3.5434e+02 - logprior: 0.4717
Fitted a model with MAP estimate = -353.3225
Time for alignment: 189.2603
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.1493 - loglik: -4.8515e+02 - logprior: -5.9615e+00
Epoch 2/10
14/14 - 4s - loss: 436.0107 - loglik: -4.3451e+02 - logprior: -1.2680e+00
Epoch 3/10
14/14 - 4s - loss: 398.4741 - loglik: -3.9661e+02 - logprior: -1.3950e+00
Epoch 4/10
14/14 - 4s - loss: 386.1354 - loglik: -3.8408e+02 - logprior: -1.3788e+00
Epoch 5/10
14/14 - 4s - loss: 381.5020 - loglik: -3.7947e+02 - logprior: -1.3281e+00
Epoch 6/10
14/14 - 4s - loss: 381.0246 - loglik: -3.7902e+02 - logprior: -1.3314e+00
Epoch 7/10
14/14 - 4s - loss: 379.7372 - loglik: -3.7782e+02 - logprior: -1.2832e+00
Epoch 8/10
14/14 - 4s - loss: 378.2300 - loglik: -3.7633e+02 - logprior: -1.2804e+00
Epoch 9/10
14/14 - 4s - loss: 378.0294 - loglik: -3.7618e+02 - logprior: -1.2817e+00
Epoch 10/10
14/14 - 4s - loss: 378.5362 - loglik: -3.7672e+02 - logprior: -1.2720e+00
Fitted a model with MAP estimate = -377.2790
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (53, 2), (54, 1), (57, 1), (58, 1), (66, 1), (71, 4), (72, 1), (89, 1), (97, 1), (103, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (140, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 394.4768 - loglik: -3.8954e+02 - logprior: -4.5127e+00
Epoch 2/2
29/29 - 7s - loss: 365.2367 - loglik: -3.6365e+02 - logprior: -1.1376e+00
Fitted a model with MAP estimate = -359.6802
expansions: [(127, 1)]
discards: [ 41 151]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 368.7516 - loglik: -3.6537e+02 - logprior: -3.2269e+00
Epoch 2/2
29/29 - 7s - loss: 361.2387 - loglik: -3.6010e+02 - logprior: -7.4389e-01
Fitted a model with MAP estimate = -357.7574
expansions: [(167, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 368.6017 - loglik: -3.6554e+02 - logprior: -3.0021e+00
Epoch 2/10
29/29 - 7s - loss: 362.3152 - loglik: -3.6168e+02 - logprior: -4.8673e-01
Epoch 3/10
29/29 - 7s - loss: 357.1654 - loglik: -3.5645e+02 - logprior: -3.7778e-01
Epoch 4/10
29/29 - 7s - loss: 355.4919 - loglik: -3.5468e+02 - logprior: -3.0382e-01
Epoch 5/10
29/29 - 7s - loss: 353.2960 - loglik: -3.5247e+02 - logprior: -2.1609e-01
Epoch 6/10
29/29 - 7s - loss: 354.6175 - loglik: -3.5383e+02 - logprior: -1.2871e-01
Fitted a model with MAP estimate = -352.1877
Time for alignment: 161.0004
Computed alignments with likelihoods: ['-353.2131', '-353.3225', '-352.1877']
Best model has likelihood: -352.1877
time for generating output: 0.2826
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.8210081152131672
