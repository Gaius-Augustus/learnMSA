Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2df5af430>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2df55ffd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df21ceb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df3c4eb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df7d7a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2defccd30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df230cd0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2df754670>, <__main__.SimpleDirichletPrior object at 0x7ff2def62f10>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 1634.2734 - loglik: -1.6320e+03 - logprior: -2.2246e+00
Epoch 2/10
39/39 - 35s - loss: 1510.5221 - loglik: -1.5086e+03 - logprior: -1.9115e+00
Epoch 3/10
39/39 - 36s - loss: 1501.3513 - loglik: -1.4994e+03 - logprior: -1.9173e+00
Epoch 4/10
39/39 - 36s - loss: 1497.3708 - loglik: -1.4955e+03 - logprior: -1.9169e+00
Epoch 5/10
39/39 - 38s - loss: 1496.4010 - loglik: -1.4943e+03 - logprior: -2.0592e+00
Epoch 6/10
39/39 - 39s - loss: 1495.5328 - loglik: -1.4933e+03 - logprior: -2.2537e+00
Epoch 7/10
39/39 - 38s - loss: 1495.3391 - loglik: -1.4929e+03 - logprior: -2.4196e+00
Epoch 8/10
39/39 - 39s - loss: 1494.9210 - loglik: -1.4923e+03 - logprior: -2.6037e+00
Epoch 9/10
39/39 - 40s - loss: 1495.6210 - loglik: -1.4929e+03 - logprior: -2.7365e+00
Fitted a model with MAP estimate = -1493.9600
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (50, 1), (51, 3), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 2), (85, 1), (90, 1), (91, 1), (94, 1), (97, 1), (100, 1), (106, 1), (116, 2), (118, 1), (119, 1), (120, 1), (139, 1), (141, 1), (142, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 2), (165, 1), (173, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (202, 1), (203, 1), (213, 1), (217, 1), (219, 1), (229, 1), (233, 1), (236, 2), (247, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1485.6888 - loglik: -1.4838e+03 - logprior: -1.9291e+00
Epoch 2/2
39/39 - 59s - loss: 1473.4155 - loglik: -1.4730e+03 - logprior: -4.0813e-01
Fitted a model with MAP estimate = -1470.0418
expansions: [(16, 1), (202, 1)]
discards: [ 55 248]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 1475.5111 - loglik: -1.4743e+03 - logprior: -1.2311e+00
Epoch 2/2
39/39 - 62s - loss: 1472.2422 - loglik: -1.4721e+03 - logprior: -1.0518e-01
Fitted a model with MAP estimate = -1469.3530
expansions: [(56, 2)]
discards: [144 334 335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 1475.4956 - loglik: -1.4745e+03 - logprior: -9.4810e-01
Epoch 2/10
39/39 - 61s - loss: 1471.8894 - loglik: -1.4720e+03 - logprior: 0.1135
Epoch 3/10
39/39 - 60s - loss: 1469.0564 - loglik: -1.4692e+03 - logprior: 0.1727
Epoch 4/10
39/39 - 59s - loss: 1467.0023 - loglik: -1.4672e+03 - logprior: 0.2457
Epoch 5/10
39/39 - 61s - loss: 1465.3018 - loglik: -1.4655e+03 - logprior: 0.1902
Epoch 6/10
39/39 - 59s - loss: 1464.0665 - loglik: -1.4642e+03 - logprior: 0.1518
Epoch 7/10
39/39 - 59s - loss: 1461.9167 - loglik: -1.4620e+03 - logprior: 0.0554
Epoch 8/10
39/39 - 57s - loss: 1464.3787 - loglik: -1.4643e+03 - logprior: -5.2533e-02
Fitted a model with MAP estimate = -1461.9884
Time for alignment: 1295.5798
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1635.3234 - loglik: -1.6331e+03 - logprior: -2.2256e+00
Epoch 2/10
39/39 - 40s - loss: 1510.1158 - loglik: -1.5081e+03 - logprior: -2.0189e+00
Epoch 3/10
39/39 - 40s - loss: 1498.5762 - loglik: -1.4966e+03 - logprior: -2.0022e+00
Epoch 4/10
39/39 - 40s - loss: 1495.9288 - loglik: -1.4939e+03 - logprior: -2.0311e+00
Epoch 5/10
39/39 - 42s - loss: 1494.9451 - loglik: -1.4928e+03 - logprior: -2.1723e+00
Epoch 6/10
39/39 - 43s - loss: 1493.6929 - loglik: -1.4913e+03 - logprior: -2.3686e+00
Epoch 7/10
39/39 - 43s - loss: 1493.4978 - loglik: -1.4909e+03 - logprior: -2.5695e+00
Epoch 8/10
39/39 - 44s - loss: 1494.4856 - loglik: -1.4917e+03 - logprior: -2.7495e+00
Fitted a model with MAP estimate = -1492.1652
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (99, 1), (100, 1), (101, 1), (107, 1), (119, 1), (120, 1), (121, 1), (122, 1), (136, 1), (141, 1), (142, 1), (143, 1), (159, 1), (160, 1), (164, 2), (166, 1), (167, 1), (175, 1), (185, 1), (189, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (201, 1), (202, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 1), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (265, 2), (266, 3), (267, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 1486.7380 - loglik: -1.4848e+03 - logprior: -1.9470e+00
Epoch 2/2
39/39 - 66s - loss: 1474.0840 - loglik: -1.4736e+03 - logprior: -4.6863e-01
Fitted a model with MAP estimate = -1470.9644
expansions: [(202, 1), (226, 1)]
discards: [332 333]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1476.5847 - loglik: -1.4753e+03 - logprior: -1.2461e+00
Epoch 2/2
39/39 - 67s - loss: 1472.2651 - loglik: -1.4721e+03 - logprior: -1.3713e-01
Fitted a model with MAP estimate = -1470.0242
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 363 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 1475.0779 - loglik: -1.4741e+03 - logprior: -1.0174e+00
Epoch 2/10
39/39 - 68s - loss: 1472.3757 - loglik: -1.4725e+03 - logprior: 0.0872
Epoch 3/10
39/39 - 69s - loss: 1469.1029 - loglik: -1.4693e+03 - logprior: 0.2151
Epoch 4/10
39/39 - 69s - loss: 1467.9136 - loglik: -1.4681e+03 - logprior: 0.2359
Epoch 5/10
39/39 - 67s - loss: 1464.4945 - loglik: -1.4646e+03 - logprior: 0.1372
Epoch 6/10
39/39 - 58s - loss: 1464.5038 - loglik: -1.4647e+03 - logprior: 0.1482
Fitted a model with MAP estimate = -1463.3710
Time for alignment: 1282.6274
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 1636.8447 - loglik: -1.6346e+03 - logprior: -2.2208e+00
Epoch 2/10
39/39 - 39s - loss: 1510.1498 - loglik: -1.5082e+03 - logprior: -1.9821e+00
Epoch 3/10
39/39 - 39s - loss: 1497.8927 - loglik: -1.4959e+03 - logprior: -1.9916e+00
Epoch 4/10
39/39 - 38s - loss: 1496.9415 - loglik: -1.4949e+03 - logprior: -2.0290e+00
Epoch 5/10
39/39 - 40s - loss: 1494.8104 - loglik: -1.4926e+03 - logprior: -2.1625e+00
Epoch 6/10
39/39 - 42s - loss: 1494.4753 - loglik: -1.4921e+03 - logprior: -2.3723e+00
Epoch 7/10
39/39 - 42s - loss: 1493.0789 - loglik: -1.4905e+03 - logprior: -2.5604e+00
Epoch 8/10
39/39 - 44s - loss: 1493.3080 - loglik: -1.4906e+03 - logprior: -2.7294e+00
Fitted a model with MAP estimate = -1492.4123
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (53, 1), (57, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (88, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (106, 1), (112, 1), (117, 1), (119, 1), (121, 1), (134, 1), (140, 1), (143, 1), (146, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 2), (165, 1), (171, 1), (183, 1), (184, 1), (187, 1), (190, 1), (192, 1), (193, 1), (194, 1), (196, 1), (197, 2), (198, 1), (212, 1), (213, 1), (216, 1), (218, 1), (222, 1), (227, 1), (234, 3), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 1485.5220 - loglik: -1.4836e+03 - logprior: -1.9420e+00
Epoch 2/2
39/39 - 67s - loss: 1473.2540 - loglik: -1.4728e+03 - logprior: -4.5263e-01
Fitted a model with MAP estimate = -1470.0947
expansions: [(16, 1), (55, 2), (201, 1)]
discards: [247]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 1475.3229 - loglik: -1.4741e+03 - logprior: -1.2683e+00
Epoch 2/2
39/39 - 70s - loss: 1472.3380 - loglik: -1.4723e+03 - logprior: -8.5167e-02
Fitted a model with MAP estimate = -1468.9348
expansions: [(144, 1)]
discards: [56 57]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1474.4440 - loglik: -1.4735e+03 - logprior: -9.7892e-01
Epoch 2/10
39/39 - 61s - loss: 1470.7938 - loglik: -1.4709e+03 - logprior: 0.1277
Epoch 3/10
39/39 - 58s - loss: 1467.6821 - loglik: -1.4679e+03 - logprior: 0.2482
Epoch 4/10
39/39 - 57s - loss: 1466.5154 - loglik: -1.4667e+03 - logprior: 0.2142
Epoch 5/10
39/39 - 57s - loss: 1463.2310 - loglik: -1.4634e+03 - logprior: 0.2155
Epoch 6/10
39/39 - 57s - loss: 1463.9008 - loglik: -1.4640e+03 - logprior: 0.1007
Fitted a model with MAP estimate = -1462.0730
Time for alignment: 1235.6408
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 1634.9149 - loglik: -1.6327e+03 - logprior: -2.2210e+00
Epoch 2/10
39/39 - 39s - loss: 1510.2357 - loglik: -1.5082e+03 - logprior: -1.9875e+00
Epoch 3/10
39/39 - 39s - loss: 1499.8401 - loglik: -1.4978e+03 - logprior: -2.0159e+00
Epoch 4/10
39/39 - 39s - loss: 1496.6620 - loglik: -1.4946e+03 - logprior: -2.0345e+00
Epoch 5/10
39/39 - 39s - loss: 1494.1108 - loglik: -1.4920e+03 - logprior: -2.1593e+00
Epoch 6/10
39/39 - 39s - loss: 1494.0492 - loglik: -1.4917e+03 - logprior: -2.3679e+00
Epoch 7/10
39/39 - 39s - loss: 1494.4403 - loglik: -1.4919e+03 - logprior: -2.5609e+00
Fitted a model with MAP estimate = -1492.6778
expansions: [(13, 1), (14, 1), (15, 1), (46, 1), (52, 5), (55, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (76, 1), (77, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (94, 1), (97, 1), (98, 1), (99, 1), (105, 1), (111, 1), (116, 1), (119, 1), (134, 1), (139, 1), (141, 1), (142, 1), (159, 1), (160, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (185, 1), (186, 1), (189, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (215, 1), (218, 1), (219, 1), (220, 1), (224, 1), (234, 1), (235, 1), (236, 2), (238, 1), (242, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 1484.8342 - loglik: -1.4829e+03 - logprior: -1.9393e+00
Epoch 2/2
39/39 - 57s - loss: 1472.5027 - loglik: -1.4720e+03 - logprior: -5.2911e-01
Fitted a model with MAP estimate = -1469.5614
expansions: [(16, 1)]
discards: [250]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 1475.4244 - loglik: -1.4741e+03 - logprior: -1.3139e+00
Epoch 2/2
39/39 - 56s - loss: 1472.2107 - loglik: -1.4721e+03 - logprior: -1.4528e-01
Fitted a model with MAP estimate = -1468.9446
expansions: [(144, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1473.8027 - loglik: -1.4728e+03 - logprior: -1.0051e+00
Epoch 2/10
39/39 - 58s - loss: 1470.2051 - loglik: -1.4703e+03 - logprior: 0.1040
Epoch 3/10
39/39 - 59s - loss: 1469.0829 - loglik: -1.4693e+03 - logprior: 0.1744
Epoch 4/10
39/39 - 63s - loss: 1465.3727 - loglik: -1.4656e+03 - logprior: 0.1886
Epoch 5/10
39/39 - 62s - loss: 1462.5278 - loglik: -1.4627e+03 - logprior: 0.1489
Epoch 6/10
39/39 - 62s - loss: 1462.7820 - loglik: -1.4629e+03 - logprior: 0.1010
Fitted a model with MAP estimate = -1461.7691
Time for alignment: 1080.5681
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 1634.4830 - loglik: -1.6323e+03 - logprior: -2.2145e+00
Epoch 2/10
39/39 - 41s - loss: 1509.2784 - loglik: -1.5073e+03 - logprior: -1.9591e+00
Epoch 3/10
39/39 - 42s - loss: 1499.6107 - loglik: -1.4977e+03 - logprior: -1.9212e+00
Epoch 4/10
39/39 - 42s - loss: 1496.7654 - loglik: -1.4948e+03 - logprior: -1.9569e+00
Epoch 5/10
39/39 - 42s - loss: 1495.4904 - loglik: -1.4934e+03 - logprior: -2.1143e+00
Epoch 6/10
39/39 - 42s - loss: 1493.5261 - loglik: -1.4912e+03 - logprior: -2.3261e+00
Epoch 7/10
39/39 - 42s - loss: 1494.1940 - loglik: -1.4916e+03 - logprior: -2.5438e+00
Fitted a model with MAP estimate = -1492.5063
expansions: [(12, 2), (13, 1), (14, 1), (46, 1), (52, 3), (56, 1), (61, 1), (62, 1), (63, 2), (64, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (94, 1), (97, 1), (98, 1), (99, 1), (105, 1), (117, 1), (120, 1), (135, 1), (139, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (159, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (174, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (220, 1), (227, 1), (228, 2), (236, 2), (247, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1485.0912 - loglik: -1.4832e+03 - logprior: -1.8734e+00
Epoch 2/2
39/39 - 61s - loss: 1472.1942 - loglik: -1.4717e+03 - logprior: -4.9423e-01
Fitted a model with MAP estimate = -1469.2675
expansions: []
discards: [250 287 338]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 1475.8711 - loglik: -1.4746e+03 - logprior: -1.2252e+00
Epoch 2/2
39/39 - 58s - loss: 1471.1738 - loglik: -1.4711e+03 - logprior: -1.0460e-01
Fitted a model with MAP estimate = -1469.3193
expansions: []
discards: [335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1474.6353 - loglik: -1.4736e+03 - logprior: -1.0198e+00
Epoch 2/10
39/39 - 57s - loss: 1472.0660 - loglik: -1.4722e+03 - logprior: 0.1382
Epoch 3/10
39/39 - 57s - loss: 1468.6193 - loglik: -1.4688e+03 - logprior: 0.1887
Epoch 4/10
39/39 - 57s - loss: 1467.9049 - loglik: -1.4681e+03 - logprior: 0.2263
Epoch 5/10
39/39 - 57s - loss: 1463.6058 - loglik: -1.4638e+03 - logprior: 0.1663
Epoch 6/10
39/39 - 57s - loss: 1463.4037 - loglik: -1.4635e+03 - logprior: 0.1250
Epoch 7/10
39/39 - 58s - loss: 1463.5905 - loglik: -1.4636e+03 - logprior: -6.6296e-03
Fitted a model with MAP estimate = -1462.3306
Time for alignment: 1164.2113
Computed alignments with likelihoods: ['-1461.9884', '-1463.3710', '-1462.0730', '-1461.7691', '-1462.3306']
Best model has likelihood: -1461.7691  (prior= 0.1790 )
time for generating output: 0.3467
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9444444444444444
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2df21ceb0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2df3c4460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df5af430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2df55ffd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2dee667c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11800b1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff12120c070>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c5f73fd0>, <__main__.SimpleDirichletPrior object at 0x7fef1c0f8ca0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.5352 - loglik: -2.9914e+02 - logprior: -3.3960e+00
Epoch 2/10
19/19 - 1s - loss: 280.6292 - loglik: -2.7927e+02 - logprior: -1.3604e+00
Epoch 3/10
19/19 - 1s - loss: 272.0008 - loglik: -2.7049e+02 - logprior: -1.5139e+00
Epoch 4/10
19/19 - 1s - loss: 270.3800 - loglik: -2.6896e+02 - logprior: -1.4219e+00
Epoch 5/10
19/19 - 1s - loss: 270.0402 - loglik: -2.6864e+02 - logprior: -1.3967e+00
Epoch 6/10
19/19 - 1s - loss: 269.6891 - loglik: -2.6829e+02 - logprior: -1.3955e+00
Epoch 7/10
19/19 - 1s - loss: 269.7181 - loglik: -2.6832e+02 - logprior: -1.4016e+00
Fitted a model with MAP estimate = -269.1256
expansions: [(6, 3), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 273.3529 - loglik: -2.6919e+02 - logprior: -4.1605e+00
Epoch 2/2
19/19 - 1s - loss: 265.8903 - loglik: -2.6391e+02 - logprior: -1.9763e+00
Fitted a model with MAP estimate = -264.2853
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.0584 - loglik: -2.6304e+02 - logprior: -3.0157e+00
Epoch 2/2
19/19 - 1s - loss: 262.9031 - loglik: -2.6170e+02 - logprior: -1.2062e+00
Fitted a model with MAP estimate = -262.1438
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.3080 - loglik: -2.6356e+02 - logprior: -3.7462e+00
Epoch 2/10
19/19 - 1s - loss: 263.0676 - loglik: -2.6174e+02 - logprior: -1.3294e+00
Epoch 3/10
19/19 - 1s - loss: 262.1946 - loglik: -2.6104e+02 - logprior: -1.1553e+00
Epoch 4/10
19/19 - 1s - loss: 262.0956 - loglik: -2.6097e+02 - logprior: -1.1268e+00
Epoch 5/10
19/19 - 1s - loss: 261.5380 - loglik: -2.6042e+02 - logprior: -1.1170e+00
Epoch 6/10
19/19 - 1s - loss: 261.2497 - loglik: -2.6013e+02 - logprior: -1.1230e+00
Epoch 7/10
19/19 - 1s - loss: 261.0437 - loglik: -2.5992e+02 - logprior: -1.1274e+00
Epoch 8/10
19/19 - 1s - loss: 261.2064 - loglik: -2.6006e+02 - logprior: -1.1418e+00
Fitted a model with MAP estimate = -260.8991
Time for alignment: 46.3349
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.3047 - loglik: -2.9891e+02 - logprior: -3.3966e+00
Epoch 2/10
19/19 - 1s - loss: 280.1194 - loglik: -2.7876e+02 - logprior: -1.3557e+00
Epoch 3/10
19/19 - 1s - loss: 272.4539 - loglik: -2.7095e+02 - logprior: -1.5000e+00
Epoch 4/10
19/19 - 1s - loss: 270.9955 - loglik: -2.6958e+02 - logprior: -1.4111e+00
Epoch 5/10
19/19 - 1s - loss: 270.2666 - loglik: -2.6889e+02 - logprior: -1.3812e+00
Epoch 6/10
19/19 - 1s - loss: 270.4638 - loglik: -2.6908e+02 - logprior: -1.3862e+00
Fitted a model with MAP estimate = -269.6926
expansions: [(6, 3), (15, 1), (18, 2), (19, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 273.8936 - loglik: -2.6972e+02 - logprior: -4.1687e+00
Epoch 2/2
19/19 - 1s - loss: 266.1107 - loglik: -2.6412e+02 - logprior: -1.9921e+00
Fitted a model with MAP estimate = -264.2645
expansions: [(0, 2)]
discards: [ 0 22 24 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.9938 - loglik: -2.6297e+02 - logprior: -3.0195e+00
Epoch 2/2
19/19 - 1s - loss: 262.9561 - loglik: -2.6175e+02 - logprior: -1.2070e+00
Fitted a model with MAP estimate = -262.1547
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.2709 - loglik: -2.6355e+02 - logprior: -3.7257e+00
Epoch 2/10
19/19 - 1s - loss: 262.9992 - loglik: -2.6167e+02 - logprior: -1.3245e+00
Epoch 3/10
19/19 - 1s - loss: 262.3648 - loglik: -2.6120e+02 - logprior: -1.1607e+00
Epoch 4/10
19/19 - 1s - loss: 261.9602 - loglik: -2.6084e+02 - logprior: -1.1206e+00
Epoch 5/10
19/19 - 1s - loss: 261.4524 - loglik: -2.6033e+02 - logprior: -1.1213e+00
Epoch 6/10
19/19 - 1s - loss: 261.2258 - loglik: -2.6010e+02 - logprior: -1.1251e+00
Epoch 7/10
19/19 - 1s - loss: 261.4072 - loglik: -2.6027e+02 - logprior: -1.1325e+00
Fitted a model with MAP estimate = -261.0293
Time for alignment: 43.0774
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 302.4492 - loglik: -2.9905e+02 - logprior: -3.3960e+00
Epoch 2/10
19/19 - 1s - loss: 280.7294 - loglik: -2.7939e+02 - logprior: -1.3424e+00
Epoch 3/10
19/19 - 1s - loss: 272.3426 - loglik: -2.7084e+02 - logprior: -1.5030e+00
Epoch 4/10
19/19 - 1s - loss: 271.0450 - loglik: -2.6964e+02 - logprior: -1.4009e+00
Epoch 5/10
19/19 - 1s - loss: 270.6571 - loglik: -2.6930e+02 - logprior: -1.3618e+00
Epoch 6/10
19/19 - 1s - loss: 270.2949 - loglik: -2.6893e+02 - logprior: -1.3603e+00
Epoch 7/10
19/19 - 1s - loss: 270.2295 - loglik: -2.6886e+02 - logprior: -1.3693e+00
Epoch 8/10
19/19 - 1s - loss: 269.9802 - loglik: -2.6860e+02 - logprior: -1.3810e+00
Epoch 9/10
19/19 - 1s - loss: 269.9231 - loglik: -2.6852e+02 - logprior: -1.3988e+00
Epoch 10/10
19/19 - 1s - loss: 270.0723 - loglik: -2.6866e+02 - logprior: -1.4140e+00
Fitted a model with MAP estimate = -269.5007
expansions: [(6, 3), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 274.5475 - loglik: -2.7035e+02 - logprior: -4.2010e+00
Epoch 2/2
19/19 - 1s - loss: 266.3355 - loglik: -2.6427e+02 - logprior: -2.0693e+00
Fitted a model with MAP estimate = -264.5509
expansions: [(0, 2)]
discards: [ 0 23 34 36 40 46]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.3823 - loglik: -2.6337e+02 - logprior: -3.0146e+00
Epoch 2/2
19/19 - 1s - loss: 262.9406 - loglik: -2.6174e+02 - logprior: -1.2040e+00
Fitted a model with MAP estimate = -262.1585
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.3698 - loglik: -2.6362e+02 - logprior: -3.7473e+00
Epoch 2/10
19/19 - 1s - loss: 263.1072 - loglik: -2.6179e+02 - logprior: -1.3210e+00
Epoch 3/10
19/19 - 1s - loss: 262.2445 - loglik: -2.6109e+02 - logprior: -1.1519e+00
Epoch 4/10
19/19 - 1s - loss: 261.9173 - loglik: -2.6080e+02 - logprior: -1.1222e+00
Epoch 5/10
19/19 - 1s - loss: 261.5193 - loglik: -2.6041e+02 - logprior: -1.1115e+00
Epoch 6/10
19/19 - 1s - loss: 261.2929 - loglik: -2.6018e+02 - logprior: -1.1140e+00
Epoch 7/10
19/19 - 1s - loss: 261.1443 - loglik: -2.6002e+02 - logprior: -1.1286e+00
Epoch 8/10
19/19 - 1s - loss: 261.0354 - loglik: -2.5990e+02 - logprior: -1.1328e+00
Epoch 9/10
19/19 - 1s - loss: 260.9934 - loglik: -2.5985e+02 - logprior: -1.1435e+00
Epoch 10/10
19/19 - 1s - loss: 260.9326 - loglik: -2.5978e+02 - logprior: -1.1509e+00
Fitted a model with MAP estimate = -260.7244
Time for alignment: 51.5367
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.3763 - loglik: -2.9898e+02 - logprior: -3.3969e+00
Epoch 2/10
19/19 - 1s - loss: 281.3328 - loglik: -2.7997e+02 - logprior: -1.3632e+00
Epoch 3/10
19/19 - 1s - loss: 272.8988 - loglik: -2.7135e+02 - logprior: -1.5454e+00
Epoch 4/10
19/19 - 1s - loss: 270.3652 - loglik: -2.6889e+02 - logprior: -1.4753e+00
Epoch 5/10
19/19 - 1s - loss: 269.3542 - loglik: -2.6790e+02 - logprior: -1.4572e+00
Epoch 6/10
19/19 - 1s - loss: 269.3392 - loglik: -2.6789e+02 - logprior: -1.4524e+00
Epoch 7/10
19/19 - 1s - loss: 268.9273 - loglik: -2.6747e+02 - logprior: -1.4604e+00
Epoch 8/10
19/19 - 1s - loss: 268.9685 - loglik: -2.6749e+02 - logprior: -1.4756e+00
Fitted a model with MAP estimate = -268.5038
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 273.8203 - loglik: -2.6964e+02 - logprior: -4.1838e+00
Epoch 2/2
19/19 - 1s - loss: 265.9845 - loglik: -2.6401e+02 - logprior: -1.9728e+00
Fitted a model with MAP estimate = -264.2932
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.2423 - loglik: -2.6323e+02 - logprior: -3.0132e+00
Epoch 2/2
19/19 - 1s - loss: 262.8824 - loglik: -2.6168e+02 - logprior: -1.2043e+00
Fitted a model with MAP estimate = -262.1536
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.1616 - loglik: -2.6342e+02 - logprior: -3.7375e+00
Epoch 2/10
19/19 - 1s - loss: 263.0990 - loglik: -2.6177e+02 - logprior: -1.3266e+00
Epoch 3/10
19/19 - 1s - loss: 262.1666 - loglik: -2.6101e+02 - logprior: -1.1589e+00
Epoch 4/10
19/19 - 1s - loss: 262.1352 - loglik: -2.6101e+02 - logprior: -1.1205e+00
Epoch 5/10
19/19 - 1s - loss: 261.5226 - loglik: -2.6041e+02 - logprior: -1.1083e+00
Epoch 6/10
19/19 - 1s - loss: 261.3429 - loglik: -2.6021e+02 - logprior: -1.1342e+00
Epoch 7/10
19/19 - 1s - loss: 261.0953 - loglik: -2.5997e+02 - logprior: -1.1261e+00
Epoch 8/10
19/19 - 1s - loss: 261.0544 - loglik: -2.5992e+02 - logprior: -1.1385e+00
Epoch 9/10
19/19 - 1s - loss: 261.0761 - loglik: -2.5993e+02 - logprior: -1.1459e+00
Fitted a model with MAP estimate = -260.8176
Time for alignment: 48.2687
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 302.3983 - loglik: -2.9901e+02 - logprior: -3.3915e+00
Epoch 2/10
19/19 - 1s - loss: 281.0871 - loglik: -2.7980e+02 - logprior: -1.2918e+00
Epoch 3/10
19/19 - 1s - loss: 273.0534 - loglik: -2.7162e+02 - logprior: -1.4381e+00
Epoch 4/10
19/19 - 1s - loss: 271.5570 - loglik: -2.7015e+02 - logprior: -1.4036e+00
Epoch 5/10
19/19 - 1s - loss: 270.8895 - loglik: -2.6951e+02 - logprior: -1.3752e+00
Epoch 6/10
19/19 - 1s - loss: 270.6645 - loglik: -2.6929e+02 - logprior: -1.3774e+00
Epoch 7/10
19/19 - 1s - loss: 270.2611 - loglik: -2.6888e+02 - logprior: -1.3842e+00
Epoch 8/10
19/19 - 1s - loss: 270.2487 - loglik: -2.6885e+02 - logprior: -1.3981e+00
Epoch 9/10
19/19 - 1s - loss: 270.3065 - loglik: -2.6889e+02 - logprior: -1.4127e+00
Fitted a model with MAP estimate = -269.7583
expansions: [(6, 3), (7, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 273.8477 - loglik: -2.6967e+02 - logprior: -4.1774e+00
Epoch 2/2
19/19 - 1s - loss: 266.0582 - loglik: -2.6408e+02 - logprior: -1.9828e+00
Fitted a model with MAP estimate = -264.3590
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 265.9910 - loglik: -2.6298e+02 - logprior: -3.0147e+00
Epoch 2/2
19/19 - 1s - loss: 262.9296 - loglik: -2.6173e+02 - logprior: -1.2002e+00
Fitted a model with MAP estimate = -262.1496
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.1574 - loglik: -2.6343e+02 - logprior: -3.7271e+00
Epoch 2/10
19/19 - 1s - loss: 263.1143 - loglik: -2.6179e+02 - logprior: -1.3281e+00
Epoch 3/10
19/19 - 1s - loss: 262.2647 - loglik: -2.6111e+02 - logprior: -1.1501e+00
Epoch 4/10
19/19 - 1s - loss: 261.8555 - loglik: -2.6073e+02 - logprior: -1.1226e+00
Epoch 5/10
19/19 - 1s - loss: 261.6460 - loglik: -2.6054e+02 - logprior: -1.1105e+00
Epoch 6/10
19/19 - 1s - loss: 261.4420 - loglik: -2.6032e+02 - logprior: -1.1236e+00
Epoch 7/10
19/19 - 1s - loss: 261.1616 - loglik: -2.6004e+02 - logprior: -1.1239e+00
Epoch 8/10
19/19 - 1s - loss: 261.0638 - loglik: -2.5992e+02 - logprior: -1.1407e+00
Epoch 9/10
19/19 - 1s - loss: 260.7766 - loglik: -2.5963e+02 - logprior: -1.1439e+00
Epoch 10/10
19/19 - 1s - loss: 260.8647 - loglik: -2.5971e+02 - logprior: -1.1521e+00
Fitted a model with MAP estimate = -260.7223
Time for alignment: 51.2290
Computed alignments with likelihoods: ['-260.8991', '-261.0293', '-260.7244', '-260.8176', '-260.7223']
Best model has likelihood: -260.7223  (prior= -1.1800 )
time for generating output: 0.1146
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6825916042581275
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff115149c10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff11537f460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11349b040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1134a20d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff115153730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1154d53a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef9c06a940>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c5d75b20>, <__main__.SimpleDirichletPrior object at 0x7ff14bb64f10>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.2906 - loglik: -7.3482e+02 - logprior: -3.4681e+00
Epoch 2/10
19/19 - 3s - loss: 680.1252 - loglik: -6.7896e+02 - logprior: -1.1673e+00
Epoch 3/10
19/19 - 3s - loss: 655.4547 - loglik: -6.5415e+02 - logprior: -1.3005e+00
Epoch 4/10
19/19 - 3s - loss: 648.9503 - loglik: -6.4758e+02 - logprior: -1.3663e+00
Epoch 5/10
19/19 - 3s - loss: 647.2186 - loglik: -6.4588e+02 - logprior: -1.3424e+00
Epoch 6/10
19/19 - 3s - loss: 646.4232 - loglik: -6.4504e+02 - logprior: -1.3811e+00
Epoch 7/10
19/19 - 3s - loss: 644.7440 - loglik: -6.4338e+02 - logprior: -1.3599e+00
Epoch 8/10
19/19 - 3s - loss: 645.0666 - loglik: -6.4366e+02 - logprior: -1.4038e+00
Fitted a model with MAP estimate = -644.7411
expansions: [(12, 2), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (69, 2), (70, 1), (71, 1), (94, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 647.5902 - loglik: -6.4348e+02 - logprior: -4.1146e+00
Epoch 2/2
19/19 - 4s - loss: 637.4324 - loglik: -6.3552e+02 - logprior: -1.9139e+00
Fitted a model with MAP estimate = -635.3862
expansions: [(0, 3)]
discards: [ 0 12 17 38 88]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 638.7133 - loglik: -6.3586e+02 - logprior: -2.8560e+00
Epoch 2/2
19/19 - 4s - loss: 635.2031 - loglik: -6.3420e+02 - logprior: -1.0060e+00
Fitted a model with MAP estimate = -634.2339
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 641.0691 - loglik: -6.3735e+02 - logprior: -3.7167e+00
Epoch 2/10
19/19 - 4s - loss: 637.5884 - loglik: -6.3601e+02 - logprior: -1.5794e+00
Epoch 3/10
19/19 - 4s - loss: 635.0152 - loglik: -6.3425e+02 - logprior: -7.6816e-01
Epoch 4/10
19/19 - 4s - loss: 634.4714 - loglik: -6.3395e+02 - logprior: -5.2160e-01
Epoch 5/10
19/19 - 4s - loss: 634.1065 - loglik: -6.3359e+02 - logprior: -5.1766e-01
Epoch 6/10
19/19 - 4s - loss: 632.8804 - loglik: -6.3236e+02 - logprior: -5.2262e-01
Epoch 7/10
19/19 - 4s - loss: 632.7763 - loglik: -6.3225e+02 - logprior: -5.2215e-01
Epoch 8/10
19/19 - 4s - loss: 633.2687 - loglik: -6.3274e+02 - logprior: -5.3093e-01
Fitted a model with MAP estimate = -632.5778
Time for alignment: 119.3902
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 738.4781 - loglik: -7.3501e+02 - logprior: -3.4694e+00
Epoch 2/10
19/19 - 3s - loss: 681.3243 - loglik: -6.8016e+02 - logprior: -1.1607e+00
Epoch 3/10
19/19 - 3s - loss: 655.8924 - loglik: -6.5463e+02 - logprior: -1.2619e+00
Epoch 4/10
19/19 - 3s - loss: 650.4259 - loglik: -6.4911e+02 - logprior: -1.3151e+00
Epoch 5/10
19/19 - 3s - loss: 648.4346 - loglik: -6.4713e+02 - logprior: -1.3091e+00
Epoch 6/10
19/19 - 3s - loss: 646.1597 - loglik: -6.4480e+02 - logprior: -1.3602e+00
Epoch 7/10
19/19 - 4s - loss: 646.8936 - loglik: -6.4550e+02 - logprior: -1.3956e+00
Fitted a model with MAP estimate = -645.7510
expansions: [(12, 2), (14, 4), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (52, 1), (56, 1), (57, 2), (70, 1), (71, 1), (93, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 647.6177 - loglik: -6.4348e+02 - logprior: -4.1329e+00
Epoch 2/2
19/19 - 5s - loss: 637.5094 - loglik: -6.3559e+02 - logprior: -1.9220e+00
Fitted a model with MAP estimate = -635.8565
expansions: [(0, 3)]
discards: [  0  12  17  38  75 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 640.0890 - loglik: -6.3719e+02 - logprior: -2.9027e+00
Epoch 2/2
19/19 - 4s - loss: 636.6766 - loglik: -6.3563e+02 - logprior: -1.0460e+00
Fitted a model with MAP estimate = -635.3697
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 641.7134 - loglik: -6.3795e+02 - logprior: -3.7678e+00
Epoch 2/10
19/19 - 4s - loss: 639.1212 - loglik: -6.3753e+02 - logprior: -1.5946e+00
Epoch 3/10
19/19 - 4s - loss: 635.6729 - loglik: -6.3492e+02 - logprior: -7.5739e-01
Epoch 4/10
19/19 - 4s - loss: 634.9363 - loglik: -6.3441e+02 - logprior: -5.2235e-01
Epoch 5/10
19/19 - 4s - loss: 634.3837 - loglik: -6.3388e+02 - logprior: -5.0627e-01
Epoch 6/10
19/19 - 4s - loss: 634.4362 - loglik: -6.3398e+02 - logprior: -4.5252e-01
Fitted a model with MAP estimate = -633.3145
Time for alignment: 108.6440
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.2258 - loglik: -7.3475e+02 - logprior: -3.4734e+00
Epoch 2/10
19/19 - 3s - loss: 680.4254 - loglik: -6.7926e+02 - logprior: -1.1667e+00
Epoch 3/10
19/19 - 4s - loss: 654.8196 - loglik: -6.5353e+02 - logprior: -1.2869e+00
Epoch 4/10
19/19 - 4s - loss: 649.0744 - loglik: -6.4770e+02 - logprior: -1.3694e+00
Epoch 5/10
19/19 - 4s - loss: 648.1877 - loglik: -6.4685e+02 - logprior: -1.3348e+00
Epoch 6/10
19/19 - 4s - loss: 647.0656 - loglik: -6.4573e+02 - logprior: -1.3401e+00
Epoch 7/10
19/19 - 4s - loss: 645.5309 - loglik: -6.4421e+02 - logprior: -1.3258e+00
Epoch 8/10
19/19 - 4s - loss: 647.0319 - loglik: -6.4566e+02 - logprior: -1.3676e+00
Fitted a model with MAP estimate = -645.6885
expansions: [(12, 2), (13, 2), (14, 2), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 2), (70, 1), (71, 1), (97, 1), (102, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 648.4756 - loglik: -6.4438e+02 - logprior: -4.0943e+00
Epoch 2/2
19/19 - 5s - loss: 637.8869 - loglik: -6.3599e+02 - logprior: -1.8973e+00
Fitted a model with MAP estimate = -635.9687
expansions: [(0, 3)]
discards: [ 0 17 38 76 89]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 638.7607 - loglik: -6.3593e+02 - logprior: -2.8354e+00
Epoch 2/2
19/19 - 4s - loss: 635.3964 - loglik: -6.3436e+02 - logprior: -1.0403e+00
Fitted a model with MAP estimate = -634.1213
expansions: []
discards: [  0   1   2  17 138]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 641.4829 - loglik: -6.3775e+02 - logprior: -3.7360e+00
Epoch 2/10
19/19 - 4s - loss: 637.9693 - loglik: -6.3638e+02 - logprior: -1.5852e+00
Epoch 3/10
19/19 - 4s - loss: 636.1442 - loglik: -6.3537e+02 - logprior: -7.7381e-01
Epoch 4/10
19/19 - 4s - loss: 634.5090 - loglik: -6.3401e+02 - logprior: -4.9413e-01
Epoch 5/10
19/19 - 4s - loss: 634.9554 - loglik: -6.3452e+02 - logprior: -4.3360e-01
Fitted a model with MAP estimate = -633.8796
Time for alignment: 108.3089
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.0540 - loglik: -7.3459e+02 - logprior: -3.4687e+00
Epoch 2/10
19/19 - 3s - loss: 682.3163 - loglik: -6.8114e+02 - logprior: -1.1725e+00
Epoch 3/10
19/19 - 4s - loss: 655.1548 - loglik: -6.5390e+02 - logprior: -1.2528e+00
Epoch 4/10
19/19 - 4s - loss: 651.0572 - loglik: -6.4972e+02 - logprior: -1.3418e+00
Epoch 5/10
19/19 - 4s - loss: 647.7524 - loglik: -6.4644e+02 - logprior: -1.3090e+00
Epoch 6/10
19/19 - 4s - loss: 646.7601 - loglik: -6.4544e+02 - logprior: -1.3223e+00
Epoch 7/10
19/19 - 4s - loss: 645.9539 - loglik: -6.4460e+02 - logprior: -1.3497e+00
Epoch 8/10
19/19 - 4s - loss: 645.6724 - loglik: -6.4429e+02 - logprior: -1.3842e+00
Epoch 9/10
19/19 - 4s - loss: 644.7576 - loglik: -6.4336e+02 - logprior: -1.4018e+00
Epoch 10/10
19/19 - 4s - loss: 646.7302 - loglik: -6.4529e+02 - logprior: -1.4355e+00
Fitted a model with MAP estimate = -645.2019
expansions: [(12, 2), (14, 3), (16, 1), (19, 1), (26, 1), (27, 2), (28, 3), (29, 2), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (94, 1), (97, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 648.2382 - loglik: -6.4408e+02 - logprior: -4.1569e+00
Epoch 2/2
19/19 - 5s - loss: 637.3508 - loglik: -6.3545e+02 - logprior: -1.9011e+00
Fitted a model with MAP estimate = -636.0108
expansions: [(0, 3)]
discards: [ 0 12 17 38 39]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 639.4100 - loglik: -6.3657e+02 - logprior: -2.8401e+00
Epoch 2/2
19/19 - 4s - loss: 635.2419 - loglik: -6.3423e+02 - logprior: -1.0152e+00
Fitted a model with MAP estimate = -634.6139
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 639.8961 - loglik: -6.3613e+02 - logprior: -3.7656e+00
Epoch 2/10
19/19 - 4s - loss: 636.3940 - loglik: -6.3499e+02 - logprior: -1.4076e+00
Epoch 3/10
19/19 - 5s - loss: 634.9127 - loglik: -6.3426e+02 - logprior: -6.5558e-01
Epoch 4/10
19/19 - 5s - loss: 634.1287 - loglik: -6.3353e+02 - logprior: -5.9732e-01
Epoch 5/10
19/19 - 5s - loss: 633.3395 - loglik: -6.3271e+02 - logprior: -6.2557e-01
Epoch 6/10
19/19 - 5s - loss: 631.9758 - loglik: -6.3139e+02 - logprior: -5.8689e-01
Epoch 7/10
19/19 - 4s - loss: 631.9470 - loglik: -6.3139e+02 - logprior: -5.5420e-01
Epoch 8/10
19/19 - 5s - loss: 632.4741 - loglik: -6.3190e+02 - logprior: -5.7518e-01
Fitted a model with MAP estimate = -631.8980
Time for alignment: 129.1420
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 738.3163 - loglik: -7.3484e+02 - logprior: -3.4719e+00
Epoch 2/10
19/19 - 4s - loss: 679.9695 - loglik: -6.7878e+02 - logprior: -1.1930e+00
Epoch 3/10
19/19 - 3s - loss: 654.5024 - loglik: -6.5318e+02 - logprior: -1.3200e+00
Epoch 4/10
19/19 - 4s - loss: 649.1436 - loglik: -6.4775e+02 - logprior: -1.3891e+00
Epoch 5/10
19/19 - 4s - loss: 647.7186 - loglik: -6.4637e+02 - logprior: -1.3509e+00
Epoch 6/10
19/19 - 4s - loss: 646.4899 - loglik: -6.4513e+02 - logprior: -1.3575e+00
Epoch 7/10
19/19 - 4s - loss: 645.5645 - loglik: -6.4418e+02 - logprior: -1.3817e+00
Epoch 8/10
19/19 - 4s - loss: 646.6445 - loglik: -6.4524e+02 - logprior: -1.4007e+00
Fitted a model with MAP estimate = -645.3975
expansions: [(12, 2), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 2), (70, 1), (71, 1), (94, 1), (97, 1), (101, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 648.0897 - loglik: -6.4396e+02 - logprior: -4.1283e+00
Epoch 2/2
19/19 - 5s - loss: 637.7686 - loglik: -6.3586e+02 - logprior: -1.9119e+00
Fitted a model with MAP estimate = -635.5101
expansions: [(0, 3)]
discards: [ 0 12 17 38 76 89]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 639.1768 - loglik: -6.3634e+02 - logprior: -2.8382e+00
Epoch 2/2
19/19 - 5s - loss: 635.3508 - loglik: -6.3435e+02 - logprior: -1.0026e+00
Fitted a model with MAP estimate = -634.5468
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 640.0665 - loglik: -6.3632e+02 - logprior: -3.7505e+00
Epoch 2/10
19/19 - 5s - loss: 636.4539 - loglik: -6.3504e+02 - logprior: -1.4090e+00
Epoch 3/10
19/19 - 5s - loss: 634.9556 - loglik: -6.3430e+02 - logprior: -6.5679e-01
Epoch 4/10
19/19 - 5s - loss: 633.1768 - loglik: -6.3257e+02 - logprior: -6.0443e-01
Epoch 5/10
19/19 - 5s - loss: 634.1252 - loglik: -6.3352e+02 - logprior: -6.0709e-01
Fitted a model with MAP estimate = -632.8139
Time for alignment: 110.1965
Computed alignments with likelihoods: ['-632.5778', '-633.3145', '-633.8796', '-631.8980', '-632.8139']
Best model has likelihood: -631.8980  (prior= -0.5831 )
time for generating output: 0.1793
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.7271523178807947
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9401112e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2c6055550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2efde3af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11385beb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113877d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c48c4250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff114b39220>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff1155d29d0>, <__main__.SimpleDirichletPrior object at 0x7ff121101d90>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 556.6727 - loglik: -5.5327e+02 - logprior: -3.4003e+00
Epoch 2/10
19/19 - 3s - loss: 518.5016 - loglik: -5.1723e+02 - logprior: -1.2699e+00
Epoch 3/10
19/19 - 3s - loss: 503.1818 - loglik: -5.0181e+02 - logprior: -1.3686e+00
Epoch 4/10
19/19 - 3s - loss: 499.5252 - loglik: -4.9814e+02 - logprior: -1.3822e+00
Epoch 5/10
19/19 - 3s - loss: 498.7340 - loglik: -4.9737e+02 - logprior: -1.3655e+00
Epoch 6/10
19/19 - 3s - loss: 497.4577 - loglik: -4.9612e+02 - logprior: -1.3401e+00
Epoch 7/10
19/19 - 3s - loss: 497.5820 - loglik: -4.9623e+02 - logprior: -1.3530e+00
Fitted a model with MAP estimate = -496.5353
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (28, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (69, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 499.3147 - loglik: -4.9534e+02 - logprior: -3.9715e+00
Epoch 2/2
19/19 - 3s - loss: 489.3012 - loglik: -4.8803e+02 - logprior: -1.2729e+00
Fitted a model with MAP estimate = -487.2645
expansions: []
discards: [ 0 45 74 79 82 84]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 494.0267 - loglik: -4.9004e+02 - logprior: -3.9829e+00
Epoch 2/2
19/19 - 3s - loss: 489.9247 - loglik: -4.8843e+02 - logprior: -1.4983e+00
Fitted a model with MAP estimate = -487.7996
expansions: [(0, 2), (24, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 490.3476 - loglik: -4.8745e+02 - logprior: -2.8939e+00
Epoch 2/10
19/19 - 3s - loss: 487.6041 - loglik: -4.8653e+02 - logprior: -1.0785e+00
Epoch 3/10
19/19 - 3s - loss: 487.1010 - loglik: -4.8608e+02 - logprior: -1.0235e+00
Epoch 4/10
19/19 - 3s - loss: 486.4424 - loglik: -4.8547e+02 - logprior: -9.7136e-01
Epoch 5/10
19/19 - 3s - loss: 485.9917 - loglik: -4.8503e+02 - logprior: -9.6255e-01
Epoch 6/10
19/19 - 3s - loss: 486.0349 - loglik: -4.8506e+02 - logprior: -9.7054e-01
Fitted a model with MAP estimate = -485.5151
Time for alignment: 83.0161
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 556.6091 - loglik: -5.5321e+02 - logprior: -3.3962e+00
Epoch 2/10
19/19 - 3s - loss: 517.7756 - loglik: -5.1652e+02 - logprior: -1.2561e+00
Epoch 3/10
19/19 - 3s - loss: 502.5209 - loglik: -5.0115e+02 - logprior: -1.3733e+00
Epoch 4/10
19/19 - 3s - loss: 498.9382 - loglik: -4.9756e+02 - logprior: -1.3819e+00
Epoch 5/10
19/19 - 3s - loss: 498.7138 - loglik: -4.9738e+02 - logprior: -1.3291e+00
Epoch 6/10
19/19 - 3s - loss: 496.9843 - loglik: -4.9567e+02 - logprior: -1.3192e+00
Epoch 7/10
19/19 - 3s - loss: 497.3100 - loglik: -4.9596e+02 - logprior: -1.3523e+00
Fitted a model with MAP estimate = -496.2123
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 498.6743 - loglik: -4.9471e+02 - logprior: -3.9683e+00
Epoch 2/2
19/19 - 3s - loss: 488.8307 - loglik: -4.8757e+02 - logprior: -1.2563e+00
Fitted a model with MAP estimate = -487.0920
expansions: []
discards: [ 0 74 78 94]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 492.9532 - loglik: -4.8897e+02 - logprior: -3.9859e+00
Epoch 2/2
19/19 - 3s - loss: 488.8564 - loglik: -4.8742e+02 - logprior: -1.4396e+00
Fitted a model with MAP estimate = -487.0174
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 489.6329 - loglik: -4.8676e+02 - logprior: -2.8696e+00
Epoch 2/10
19/19 - 3s - loss: 487.0200 - loglik: -4.8595e+02 - logprior: -1.0651e+00
Epoch 3/10
19/19 - 3s - loss: 486.5648 - loglik: -4.8555e+02 - logprior: -1.0104e+00
Epoch 4/10
19/19 - 3s - loss: 485.7537 - loglik: -4.8479e+02 - logprior: -9.5870e-01
Epoch 5/10
19/19 - 3s - loss: 485.8733 - loglik: -4.8493e+02 - logprior: -9.4390e-01
Fitted a model with MAP estimate = -485.2243
Time for alignment: 80.6440
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 556.4822 - loglik: -5.5309e+02 - logprior: -3.3927e+00
Epoch 2/10
19/19 - 3s - loss: 517.3306 - loglik: -5.1608e+02 - logprior: -1.2473e+00
Epoch 3/10
19/19 - 3s - loss: 502.4793 - loglik: -5.0113e+02 - logprior: -1.3497e+00
Epoch 4/10
19/19 - 3s - loss: 499.0759 - loglik: -4.9770e+02 - logprior: -1.3728e+00
Epoch 5/10
19/19 - 3s - loss: 498.1972 - loglik: -4.9684e+02 - logprior: -1.3547e+00
Epoch 6/10
19/19 - 3s - loss: 497.6348 - loglik: -4.9629e+02 - logprior: -1.3431e+00
Epoch 7/10
19/19 - 3s - loss: 497.1320 - loglik: -4.9577e+02 - logprior: -1.3652e+00
Epoch 8/10
19/19 - 3s - loss: 497.0968 - loglik: -4.9571e+02 - logprior: -1.3885e+00
Epoch 9/10
19/19 - 3s - loss: 496.7758 - loglik: -4.9535e+02 - logprior: -1.4283e+00
Epoch 10/10
19/19 - 3s - loss: 497.1204 - loglik: -4.9565e+02 - logprior: -1.4697e+00
Fitted a model with MAP estimate = -495.7999
expansions: [(0, 2), (4, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (32, 2), (46, 1), (59, 2), (61, 2), (63, 2), (64, 2), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 499.7423 - loglik: -4.9563e+02 - logprior: -4.1169e+00
Epoch 2/2
19/19 - 3s - loss: 489.2383 - loglik: -4.8792e+02 - logprior: -1.3225e+00
Fitted a model with MAP estimate = -487.0077
expansions: []
discards: [ 0 46 75 79 85 96]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 493.3508 - loglik: -4.8935e+02 - logprior: -4.0028e+00
Epoch 2/2
19/19 - 3s - loss: 489.2007 - loglik: -4.8773e+02 - logprior: -1.4743e+00
Fitted a model with MAP estimate = -487.2526
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 489.6841 - loglik: -4.8679e+02 - logprior: -2.8924e+00
Epoch 2/10
19/19 - 3s - loss: 487.2827 - loglik: -4.8619e+02 - logprior: -1.0928e+00
Epoch 3/10
19/19 - 3s - loss: 486.4291 - loglik: -4.8541e+02 - logprior: -1.0240e+00
Epoch 4/10
19/19 - 3s - loss: 485.8279 - loglik: -4.8485e+02 - logprior: -9.7513e-01
Epoch 5/10
19/19 - 3s - loss: 486.4879 - loglik: -4.8552e+02 - logprior: -9.6381e-01
Fitted a model with MAP estimate = -485.3842
Time for alignment: 89.1335
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 556.6812 - loglik: -5.5328e+02 - logprior: -3.3971e+00
Epoch 2/10
19/19 - 3s - loss: 517.5121 - loglik: -5.1627e+02 - logprior: -1.2415e+00
Epoch 3/10
19/19 - 3s - loss: 502.5740 - loglik: -5.0121e+02 - logprior: -1.3648e+00
Epoch 4/10
19/19 - 3s - loss: 498.4061 - loglik: -4.9706e+02 - logprior: -1.3497e+00
Epoch 5/10
19/19 - 3s - loss: 497.7695 - loglik: -4.9644e+02 - logprior: -1.3279e+00
Epoch 6/10
19/19 - 3s - loss: 497.0910 - loglik: -4.9576e+02 - logprior: -1.3316e+00
Epoch 7/10
19/19 - 3s - loss: 496.7979 - loglik: -4.9545e+02 - logprior: -1.3441e+00
Epoch 8/10
19/19 - 3s - loss: 496.6918 - loglik: -4.9531e+02 - logprior: -1.3864e+00
Epoch 9/10
19/19 - 3s - loss: 496.8504 - loglik: -4.9546e+02 - logprior: -1.3948e+00
Fitted a model with MAP estimate = -495.5464
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 498.8690 - loglik: -4.9481e+02 - logprior: -4.0593e+00
Epoch 2/2
19/19 - 3s - loss: 488.7191 - loglik: -4.8746e+02 - logprior: -1.2568e+00
Fitted a model with MAP estimate = -486.8501
expansions: []
discards: [ 0 46 75 79 95]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 492.4025 - loglik: -4.8845e+02 - logprior: -3.9478e+00
Epoch 2/2
19/19 - 3s - loss: 489.1697 - loglik: -4.8778e+02 - logprior: -1.3927e+00
Fitted a model with MAP estimate = -486.9232
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 489.4499 - loglik: -4.8658e+02 - logprior: -2.8706e+00
Epoch 2/10
19/19 - 3s - loss: 487.3688 - loglik: -4.8629e+02 - logprior: -1.0774e+00
Epoch 3/10
19/19 - 3s - loss: 486.0900 - loglik: -4.8508e+02 - logprior: -1.0055e+00
Epoch 4/10
19/19 - 3s - loss: 486.2806 - loglik: -4.8531e+02 - logprior: -9.6673e-01
Fitted a model with MAP estimate = -485.6210
Time for alignment: 83.2645
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 556.8495 - loglik: -5.5345e+02 - logprior: -3.4012e+00
Epoch 2/10
19/19 - 3s - loss: 517.9388 - loglik: -5.1671e+02 - logprior: -1.2302e+00
Epoch 3/10
19/19 - 3s - loss: 503.3613 - loglik: -5.0206e+02 - logprior: -1.3032e+00
Epoch 4/10
19/19 - 3s - loss: 500.1079 - loglik: -4.9877e+02 - logprior: -1.3332e+00
Epoch 5/10
19/19 - 3s - loss: 498.2842 - loglik: -4.9697e+02 - logprior: -1.3176e+00
Epoch 6/10
19/19 - 3s - loss: 498.4530 - loglik: -4.9713e+02 - logprior: -1.3237e+00
Fitted a model with MAP estimate = -497.1973
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (16, 1), (17, 1), (18, 1), (27, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 499.9592 - loglik: -4.9602e+02 - logprior: -3.9384e+00
Epoch 2/2
19/19 - 3s - loss: 489.5457 - loglik: -4.8826e+02 - logprior: -1.2843e+00
Fitted a model with MAP estimate = -487.3394
expansions: []
discards: [ 0 74 83 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 494.0610 - loglik: -4.9005e+02 - logprior: -4.0146e+00
Epoch 2/2
19/19 - 3s - loss: 489.9875 - loglik: -4.8851e+02 - logprior: -1.4817e+00
Fitted a model with MAP estimate = -487.7040
expansions: [(0, 2)]
discards: [ 0 79]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 490.5498 - loglik: -4.8765e+02 - logprior: -2.9010e+00
Epoch 2/10
19/19 - 3s - loss: 487.6011 - loglik: -4.8651e+02 - logprior: -1.0918e+00
Epoch 3/10
19/19 - 3s - loss: 487.2915 - loglik: -4.8625e+02 - logprior: -1.0406e+00
Epoch 4/10
19/19 - 3s - loss: 486.4224 - loglik: -4.8543e+02 - logprior: -9.9002e-01
Epoch 5/10
19/19 - 3s - loss: 486.1342 - loglik: -4.8516e+02 - logprior: -9.7440e-01
Epoch 6/10
19/19 - 3s - loss: 486.0197 - loglik: -4.8503e+02 - logprior: -9.9118e-01
Epoch 7/10
19/19 - 3s - loss: 485.4950 - loglik: -4.8450e+02 - logprior: -9.9462e-01
Epoch 8/10
19/19 - 3s - loss: 485.9885 - loglik: -4.8497e+02 - logprior: -1.0152e+00
Fitted a model with MAP estimate = -485.3533
Time for alignment: 87.2410
Computed alignments with likelihoods: ['-485.5151', '-485.2243', '-485.3842', '-485.6210', '-485.3533']
Best model has likelihood: -485.2243  (prior= -0.9430 )
time for generating output: 0.2088
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6577931750473955
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff14b8a1af0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff1432eef70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff15c07fb20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef4043e3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1432f2eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff129bd2f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113b558b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff113b7af70>, <__main__.SimpleDirichletPrior object at 0x7fef803bc940>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 365.4449 - loglik: -3.6200e+02 - logprior: -3.4402e+00
Epoch 2/10
19/19 - 1s - loss: 322.1023 - loglik: -3.2076e+02 - logprior: -1.3436e+00
Epoch 3/10
19/19 - 1s - loss: 307.0357 - loglik: -3.0571e+02 - logprior: -1.3280e+00
Epoch 4/10
19/19 - 1s - loss: 304.1207 - loglik: -3.0275e+02 - logprior: -1.3689e+00
Epoch 5/10
19/19 - 1s - loss: 303.1965 - loglik: -3.0186e+02 - logprior: -1.3337e+00
Epoch 6/10
19/19 - 1s - loss: 302.5526 - loglik: -3.0123e+02 - logprior: -1.3193e+00
Epoch 7/10
19/19 - 1s - loss: 302.6282 - loglik: -3.0129e+02 - logprior: -1.3364e+00
Fitted a model with MAP estimate = -302.2852
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.6871 - loglik: -2.9845e+02 - logprior: -4.2353e+00
Epoch 2/2
19/19 - 2s - loss: 293.8489 - loglik: -2.9256e+02 - logprior: -1.2906e+00
Fitted a model with MAP estimate = -292.6893
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 298.2367 - loglik: -2.9417e+02 - logprior: -4.0651e+00
Epoch 2/2
19/19 - 1s - loss: 293.8564 - loglik: -2.9240e+02 - logprior: -1.4519e+00
Fitted a model with MAP estimate = -292.8905
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.1931 - loglik: -2.9308e+02 - logprior: -3.1167e+00
Epoch 2/10
19/19 - 1s - loss: 293.2617 - loglik: -2.9191e+02 - logprior: -1.3480e+00
Epoch 3/10
19/19 - 2s - loss: 292.6609 - loglik: -2.9141e+02 - logprior: -1.2499e+00
Epoch 4/10
19/19 - 2s - loss: 291.8731 - loglik: -2.9067e+02 - logprior: -1.2016e+00
Epoch 5/10
19/19 - 1s - loss: 291.6893 - loglik: -2.9051e+02 - logprior: -1.1819e+00
Epoch 6/10
19/19 - 1s - loss: 291.0874 - loglik: -2.8990e+02 - logprior: -1.1824e+00
Epoch 7/10
19/19 - 1s - loss: 290.8966 - loglik: -2.8972e+02 - logprior: -1.1796e+00
Epoch 8/10
19/19 - 1s - loss: 290.7971 - loglik: -2.8961e+02 - logprior: -1.1887e+00
Epoch 9/10
19/19 - 1s - loss: 290.7340 - loglik: -2.8953e+02 - logprior: -1.2000e+00
Epoch 10/10
19/19 - 1s - loss: 290.6663 - loglik: -2.8946e+02 - logprior: -1.2039e+00
Fitted a model with MAP estimate = -290.5128
Time for alignment: 55.6220
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.3659 - loglik: -3.6193e+02 - logprior: -3.4369e+00
Epoch 2/10
19/19 - 1s - loss: 322.0851 - loglik: -3.2073e+02 - logprior: -1.3504e+00
Epoch 3/10
19/19 - 1s - loss: 306.7542 - loglik: -3.0540e+02 - logprior: -1.3511e+00
Epoch 4/10
19/19 - 1s - loss: 304.3144 - loglik: -3.0293e+02 - logprior: -1.3811e+00
Epoch 5/10
19/19 - 1s - loss: 303.4771 - loglik: -3.0213e+02 - logprior: -1.3450e+00
Epoch 6/10
19/19 - 1s - loss: 303.0013 - loglik: -3.0167e+02 - logprior: -1.3319e+00
Epoch 7/10
19/19 - 1s - loss: 302.4820 - loglik: -3.0114e+02 - logprior: -1.3402e+00
Epoch 8/10
19/19 - 1s - loss: 302.5122 - loglik: -3.0116e+02 - logprior: -1.3496e+00
Fitted a model with MAP estimate = -302.2884
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.8445 - loglik: -2.9858e+02 - logprior: -4.2633e+00
Epoch 2/2
19/19 - 1s - loss: 293.7492 - loglik: -2.9247e+02 - logprior: -1.2812e+00
Fitted a model with MAP estimate = -292.6589
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 298.1915 - loglik: -2.9413e+02 - logprior: -4.0630e+00
Epoch 2/2
19/19 - 1s - loss: 293.6678 - loglik: -2.9221e+02 - logprior: -1.4543e+00
Fitted a model with MAP estimate = -292.9026
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 296.1697 - loglik: -2.9297e+02 - logprior: -3.2029e+00
Epoch 2/10
19/19 - 2s - loss: 293.1234 - loglik: -2.9177e+02 - logprior: -1.3490e+00
Epoch 3/10
19/19 - 2s - loss: 292.5603 - loglik: -2.9133e+02 - logprior: -1.2262e+00
Epoch 4/10
19/19 - 1s - loss: 292.1503 - loglik: -2.9095e+02 - logprior: -1.1961e+00
Epoch 5/10
19/19 - 1s - loss: 291.3268 - loglik: -2.9016e+02 - logprior: -1.1712e+00
Epoch 6/10
19/19 - 1s - loss: 291.2628 - loglik: -2.9009e+02 - logprior: -1.1695e+00
Epoch 7/10
19/19 - 1s - loss: 291.0529 - loglik: -2.8988e+02 - logprior: -1.1713e+00
Epoch 8/10
19/19 - 1s - loss: 290.8106 - loglik: -2.8962e+02 - logprior: -1.1862e+00
Epoch 9/10
19/19 - 2s - loss: 290.7702 - loglik: -2.8958e+02 - logprior: -1.1884e+00
Epoch 10/10
19/19 - 1s - loss: 290.5985 - loglik: -2.8940e+02 - logprior: -1.1982e+00
Fitted a model with MAP estimate = -290.5044
Time for alignment: 57.2133
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 365.4415 - loglik: -3.6200e+02 - logprior: -3.4431e+00
Epoch 2/10
19/19 - 2s - loss: 322.8536 - loglik: -3.2147e+02 - logprior: -1.3797e+00
Epoch 3/10
19/19 - 1s - loss: 306.0539 - loglik: -3.0468e+02 - logprior: -1.3778e+00
Epoch 4/10
19/19 - 1s - loss: 303.5313 - loglik: -3.0215e+02 - logprior: -1.3833e+00
Epoch 5/10
19/19 - 1s - loss: 302.7683 - loglik: -3.0142e+02 - logprior: -1.3433e+00
Epoch 6/10
19/19 - 1s - loss: 302.2037 - loglik: -3.0087e+02 - logprior: -1.3357e+00
Epoch 7/10
19/19 - 1s - loss: 302.0792 - loglik: -3.0074e+02 - logprior: -1.3416e+00
Epoch 8/10
19/19 - 1s - loss: 301.7860 - loglik: -3.0044e+02 - logprior: -1.3459e+00
Epoch 9/10
19/19 - 1s - loss: 301.7968 - loglik: -3.0043e+02 - logprior: -1.3672e+00
Fitted a model with MAP estimate = -301.5666
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.9163 - loglik: -2.9862e+02 - logprior: -4.2973e+00
Epoch 2/2
19/19 - 2s - loss: 293.9432 - loglik: -2.9270e+02 - logprior: -1.2444e+00
Fitted a model with MAP estimate = -292.6224
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 298.0276 - loglik: -2.9397e+02 - logprior: -4.0605e+00
Epoch 2/2
19/19 - 1s - loss: 293.8633 - loglik: -2.9242e+02 - logprior: -1.4471e+00
Fitted a model with MAP estimate = -292.9804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.1184 - loglik: -2.9291e+02 - logprior: -3.2065e+00
Epoch 2/10
19/19 - 1s - loss: 293.2677 - loglik: -2.9193e+02 - logprior: -1.3383e+00
Epoch 3/10
19/19 - 2s - loss: 292.3932 - loglik: -2.9116e+02 - logprior: -1.2315e+00
Epoch 4/10
19/19 - 1s - loss: 292.1529 - loglik: -2.9097e+02 - logprior: -1.1874e+00
Epoch 5/10
19/19 - 2s - loss: 291.6270 - loglik: -2.9045e+02 - logprior: -1.1763e+00
Epoch 6/10
19/19 - 2s - loss: 290.9739 - loglik: -2.8981e+02 - logprior: -1.1597e+00
Epoch 7/10
19/19 - 2s - loss: 291.2203 - loglik: -2.9005e+02 - logprior: -1.1677e+00
Fitted a model with MAP estimate = -290.8197
Time for alignment: 53.5788
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 365.3053 - loglik: -3.6187e+02 - logprior: -3.4386e+00
Epoch 2/10
19/19 - 1s - loss: 321.7257 - loglik: -3.2036e+02 - logprior: -1.3627e+00
Epoch 3/10
19/19 - 1s - loss: 306.3185 - loglik: -3.0496e+02 - logprior: -1.3606e+00
Epoch 4/10
19/19 - 1s - loss: 303.9558 - loglik: -3.0257e+02 - logprior: -1.3906e+00
Epoch 5/10
19/19 - 1s - loss: 303.2873 - loglik: -3.0193e+02 - logprior: -1.3526e+00
Epoch 6/10
19/19 - 1s - loss: 302.6819 - loglik: -3.0135e+02 - logprior: -1.3356e+00
Epoch 7/10
19/19 - 1s - loss: 302.1038 - loglik: -3.0075e+02 - logprior: -1.3526e+00
Epoch 8/10
19/19 - 2s - loss: 302.2180 - loglik: -3.0086e+02 - logprior: -1.3576e+00
Fitted a model with MAP estimate = -301.9516
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.5603 - loglik: -2.9831e+02 - logprior: -4.2517e+00
Epoch 2/2
19/19 - 2s - loss: 293.9422 - loglik: -2.9269e+02 - logprior: -1.2514e+00
Fitted a model with MAP estimate = -292.6207
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 298.1013 - loglik: -2.9404e+02 - logprior: -4.0661e+00
Epoch 2/2
19/19 - 1s - loss: 293.8922 - loglik: -2.9245e+02 - logprior: -1.4404e+00
Fitted a model with MAP estimate = -292.9096
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.0362 - loglik: -2.9291e+02 - logprior: -3.1234e+00
Epoch 2/10
19/19 - 2s - loss: 293.2309 - loglik: -2.9188e+02 - logprior: -1.3490e+00
Epoch 3/10
19/19 - 1s - loss: 292.7154 - loglik: -2.9148e+02 - logprior: -1.2376e+00
Epoch 4/10
19/19 - 2s - loss: 292.0193 - loglik: -2.9082e+02 - logprior: -1.1991e+00
Epoch 5/10
19/19 - 1s - loss: 291.7133 - loglik: -2.9053e+02 - logprior: -1.1846e+00
Epoch 6/10
19/19 - 1s - loss: 291.0464 - loglik: -2.8987e+02 - logprior: -1.1747e+00
Epoch 7/10
19/19 - 2s - loss: 291.2545 - loglik: -2.9008e+02 - logprior: -1.1732e+00
Fitted a model with MAP estimate = -290.7950
Time for alignment: 52.7499
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 365.4979 - loglik: -3.6206e+02 - logprior: -3.4399e+00
Epoch 2/10
19/19 - 1s - loss: 321.9666 - loglik: -3.2062e+02 - logprior: -1.3444e+00
Epoch 3/10
19/19 - 2s - loss: 306.5445 - loglik: -3.0521e+02 - logprior: -1.3383e+00
Epoch 4/10
19/19 - 2s - loss: 304.1061 - loglik: -3.0273e+02 - logprior: -1.3720e+00
Epoch 5/10
19/19 - 1s - loss: 303.4045 - loglik: -3.0206e+02 - logprior: -1.3401e+00
Epoch 6/10
19/19 - 1s - loss: 303.1023 - loglik: -3.0178e+02 - logprior: -1.3269e+00
Epoch 7/10
19/19 - 1s - loss: 302.5168 - loglik: -3.0118e+02 - logprior: -1.3382e+00
Epoch 8/10
19/19 - 1s - loss: 302.5729 - loglik: -3.0123e+02 - logprior: -1.3466e+00
Fitted a model with MAP estimate = -302.2929
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 302.8537 - loglik: -2.9858e+02 - logprior: -4.2780e+00
Epoch 2/2
19/19 - 2s - loss: 293.9788 - loglik: -2.9270e+02 - logprior: -1.2779e+00
Fitted a model with MAP estimate = -292.6558
expansions: []
discards: [ 0 38 54]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.2170 - loglik: -2.9414e+02 - logprior: -4.0775e+00
Epoch 2/2
19/19 - 2s - loss: 293.8687 - loglik: -2.9242e+02 - logprior: -1.4474e+00
Fitted a model with MAP estimate = -292.8943
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 296.1301 - loglik: -2.9293e+02 - logprior: -3.1980e+00
Epoch 2/10
19/19 - 1s - loss: 293.2775 - loglik: -2.9192e+02 - logprior: -1.3528e+00
Epoch 3/10
19/19 - 1s - loss: 292.6337 - loglik: -2.9141e+02 - logprior: -1.2271e+00
Epoch 4/10
19/19 - 1s - loss: 291.7095 - loglik: -2.9051e+02 - logprior: -1.1968e+00
Epoch 5/10
19/19 - 1s - loss: 291.5552 - loglik: -2.9038e+02 - logprior: -1.1707e+00
Epoch 6/10
19/19 - 2s - loss: 291.1695 - loglik: -2.9000e+02 - logprior: -1.1702e+00
Epoch 7/10
19/19 - 2s - loss: 291.0331 - loglik: -2.8986e+02 - logprior: -1.1744e+00
Epoch 8/10
19/19 - 1s - loss: 290.9412 - loglik: -2.8976e+02 - logprior: -1.1846e+00
Epoch 9/10
19/19 - 1s - loss: 290.6567 - loglik: -2.8946e+02 - logprior: -1.1925e+00
Epoch 10/10
19/19 - 2s - loss: 290.5030 - loglik: -2.8931e+02 - logprior: -1.1961e+00
Fitted a model with MAP estimate = -290.4946
Time for alignment: 56.8644
Computed alignments with likelihoods: ['-290.5128', '-290.5044', '-290.8197', '-290.7950', '-290.4946']
Best model has likelihood: -290.4946  (prior= -1.2272 )
time for generating output: 0.1217
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8142414860681114
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2c5a43100>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef7834ee50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c5fc0a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2ded11730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff13a008cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff120eddb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11856e310>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff114d0bf70>, <__main__.SimpleDirichletPrior object at 0x7ff118436f70>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1213.8329 - loglik: -1.2115e+03 - logprior: -2.3147e+00
Epoch 2/10
39/39 - 16s - loss: 1130.6121 - loglik: -1.1293e+03 - logprior: -1.3240e+00
Epoch 3/10
39/39 - 16s - loss: 1122.3170 - loglik: -1.1209e+03 - logprior: -1.3977e+00
Epoch 4/10
39/39 - 16s - loss: 1120.0142 - loglik: -1.1186e+03 - logprior: -1.4281e+00
Epoch 5/10
39/39 - 17s - loss: 1118.7417 - loglik: -1.1172e+03 - logprior: -1.5190e+00
Epoch 6/10
39/39 - 17s - loss: 1119.0457 - loglik: -1.1174e+03 - logprior: -1.6371e+00
Fitted a model with MAP estimate = -1114.3822
expansions: [(25, 1), (30, 4), (55, 1), (56, 1), (76, 1), (78, 1), (93, 1), (101, 1), (103, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1119.7798 - loglik: -1.1177e+03 - logprior: -2.0993e+00
Epoch 2/2
39/39 - 18s - loss: 1114.4762 - loglik: -1.1135e+03 - logprior: -9.5780e-01
Fitted a model with MAP estimate = -1109.6273
expansions: [(41, 2)]
discards: [ 0 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1120.5840 - loglik: -1.1178e+03 - logprior: -2.8023e+00
Epoch 2/2
39/39 - 18s - loss: 1116.1146 - loglik: -1.1149e+03 - logprior: -1.1734e+00
Fitted a model with MAP estimate = -1111.0136
expansions: [(0, 2)]
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1114.2161 - loglik: -1.1124e+03 - logprior: -1.8007e+00
Epoch 2/10
39/39 - 18s - loss: 1111.4691 - loglik: -1.1108e+03 - logprior: -6.9020e-01
Epoch 3/10
39/39 - 18s - loss: 1108.7101 - loglik: -1.1081e+03 - logprior: -6.2318e-01
Epoch 4/10
39/39 - 18s - loss: 1109.4178 - loglik: -1.1088e+03 - logprior: -6.2811e-01
Fitted a model with MAP estimate = -1107.6402
Time for alignment: 338.3738
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1216.9177 - loglik: -1.2147e+03 - logprior: -2.2363e+00
Epoch 2/10
39/39 - 17s - loss: 1130.2626 - loglik: -1.1290e+03 - logprior: -1.2866e+00
Epoch 3/10
39/39 - 17s - loss: 1122.2354 - loglik: -1.1209e+03 - logprior: -1.3242e+00
Epoch 4/10
39/39 - 17s - loss: 1120.9227 - loglik: -1.1196e+03 - logprior: -1.3622e+00
Epoch 5/10
39/39 - 17s - loss: 1120.0229 - loglik: -1.1186e+03 - logprior: -1.4514e+00
Epoch 6/10
39/39 - 17s - loss: 1118.1481 - loglik: -1.1166e+03 - logprior: -1.5915e+00
Epoch 7/10
39/39 - 17s - loss: 1119.7252 - loglik: -1.1180e+03 - logprior: -1.7040e+00
Fitted a model with MAP estimate = -1114.7112
expansions: [(25, 1), (78, 2), (79, 2), (82, 1), (102, 1), (104, 1), (106, 1), (107, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1119.6940 - loglik: -1.1175e+03 - logprior: -2.1456e+00
Epoch 2/2
39/39 - 18s - loss: 1114.2819 - loglik: -1.1133e+03 - logprior: -9.9218e-01
Fitted a model with MAP estimate = -1109.5567
expansions: []
discards: [ 0 79]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1120.2861 - loglik: -1.1175e+03 - logprior: -2.8267e+00
Epoch 2/2
39/39 - 17s - loss: 1116.3898 - loglik: -1.1151e+03 - logprior: -1.2432e+00
Fitted a model with MAP estimate = -1110.8501
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1113.3518 - loglik: -1.1115e+03 - logprior: -1.8288e+00
Epoch 2/10
39/39 - 18s - loss: 1111.2051 - loglik: -1.1105e+03 - logprior: -7.1211e-01
Epoch 3/10
39/39 - 18s - loss: 1108.9253 - loglik: -1.1083e+03 - logprior: -6.4273e-01
Epoch 4/10
39/39 - 18s - loss: 1109.4362 - loglik: -1.1088e+03 - logprior: -6.4697e-01
Fitted a model with MAP estimate = -1107.6180
Time for alignment: 359.7836
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1219.6927 - loglik: -1.2174e+03 - logprior: -2.2707e+00
Epoch 2/10
39/39 - 17s - loss: 1138.4590 - loglik: -1.1372e+03 - logprior: -1.2988e+00
Epoch 3/10
39/39 - 17s - loss: 1127.8081 - loglik: -1.1265e+03 - logprior: -1.3409e+00
Epoch 4/10
39/39 - 17s - loss: 1126.6581 - loglik: -1.1252e+03 - logprior: -1.4103e+00
Epoch 5/10
39/39 - 17s - loss: 1124.9513 - loglik: -1.1235e+03 - logprior: -1.4752e+00
Epoch 6/10
39/39 - 17s - loss: 1121.3364 - loglik: -1.1197e+03 - logprior: -1.6173e+00
Epoch 7/10
39/39 - 17s - loss: 1121.2113 - loglik: -1.1195e+03 - logprior: -1.7361e+00
Epoch 8/10
39/39 - 17s - loss: 1120.5382 - loglik: -1.1187e+03 - logprior: -1.8493e+00
Epoch 9/10
39/39 - 17s - loss: 1118.8069 - loglik: -1.1169e+03 - logprior: -1.9168e+00
Epoch 10/10
39/39 - 17s - loss: 1119.9941 - loglik: -1.1180e+03 - logprior: -2.0028e+00
Fitted a model with MAP estimate = -1115.4048
expansions: [(25, 1), (34, 1), (57, 1), (80, 1), (81, 1), (82, 1), (83, 1), (92, 1), (102, 1), (105, 1), (106, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1124.6633 - loglik: -1.1216e+03 - logprior: -3.0575e+00
Epoch 2/2
39/39 - 18s - loss: 1117.8875 - loglik: -1.1168e+03 - logprior: -1.0782e+00
Fitted a model with MAP estimate = -1112.3718
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1119.5458 - loglik: -1.1177e+03 - logprior: -1.8637e+00
Epoch 2/2
39/39 - 18s - loss: 1115.5267 - loglik: -1.1147e+03 - logprior: -7.7853e-01
Fitted a model with MAP estimate = -1109.0801
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1112.2244 - loglik: -1.1105e+03 - logprior: -1.7301e+00
Epoch 2/10
39/39 - 19s - loss: 1110.2850 - loglik: -1.1096e+03 - logprior: -6.4913e-01
Epoch 3/10
39/39 - 19s - loss: 1108.4065 - loglik: -1.1078e+03 - logprior: -5.6841e-01
Epoch 4/10
39/39 - 19s - loss: 1106.5612 - loglik: -1.1060e+03 - logprior: -5.7366e-01
Epoch 5/10
39/39 - 18s - loss: 1107.1366 - loglik: -1.1065e+03 - logprior: -5.8733e-01
Fitted a model with MAP estimate = -1106.1590
Time for alignment: 431.1971
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1213.9557 - loglik: -1.2117e+03 - logprior: -2.2729e+00
Epoch 2/10
39/39 - 17s - loss: 1130.6975 - loglik: -1.1294e+03 - logprior: -1.2790e+00
Epoch 3/10
39/39 - 17s - loss: 1123.4595 - loglik: -1.1221e+03 - logprior: -1.3393e+00
Epoch 4/10
39/39 - 17s - loss: 1121.3864 - loglik: -1.1200e+03 - logprior: -1.4089e+00
Epoch 5/10
39/39 - 17s - loss: 1119.5343 - loglik: -1.1180e+03 - logprior: -1.5025e+00
Epoch 6/10
39/39 - 17s - loss: 1118.6476 - loglik: -1.1170e+03 - logprior: -1.6236e+00
Epoch 7/10
39/39 - 17s - loss: 1118.1005 - loglik: -1.1164e+03 - logprior: -1.7448e+00
Epoch 8/10
39/39 - 17s - loss: 1118.3990 - loglik: -1.1165e+03 - logprior: -1.8565e+00
Fitted a model with MAP estimate = -1114.3747
expansions: [(25, 1), (37, 1), (57, 1), (63, 1), (76, 1), (78, 1), (102, 1), (104, 1), (106, 1), (107, 1), (136, 2), (138, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1120.7861 - loglik: -1.1186e+03 - logprior: -2.1582e+00
Epoch 2/2
39/39 - 19s - loss: 1115.4404 - loglik: -1.1145e+03 - logprior: -9.8715e-01
Fitted a model with MAP estimate = -1110.0076
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1120.1788 - loglik: -1.1174e+03 - logprior: -2.8211e+00
Epoch 2/2
39/39 - 19s - loss: 1116.0190 - loglik: -1.1148e+03 - logprior: -1.2338e+00
Fitted a model with MAP estimate = -1110.9175
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1113.4963 - loglik: -1.1117e+03 - logprior: -1.8183e+00
Epoch 2/10
39/39 - 20s - loss: 1110.0963 - loglik: -1.1094e+03 - logprior: -7.1106e-01
Epoch 3/10
39/39 - 19s - loss: 1110.5046 - loglik: -1.1099e+03 - logprior: -6.4410e-01
Fitted a model with MAP estimate = -1108.4454
Time for alignment: 368.9606
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1216.1511 - loglik: -1.2139e+03 - logprior: -2.2767e+00
Epoch 2/10
39/39 - 17s - loss: 1131.9924 - loglik: -1.1307e+03 - logprior: -1.3105e+00
Epoch 3/10
39/39 - 17s - loss: 1125.2159 - loglik: -1.1239e+03 - logprior: -1.3098e+00
Epoch 4/10
39/39 - 17s - loss: 1123.1144 - loglik: -1.1218e+03 - logprior: -1.3395e+00
Epoch 5/10
39/39 - 17s - loss: 1121.4590 - loglik: -1.1201e+03 - logprior: -1.4020e+00
Epoch 6/10
39/39 - 17s - loss: 1121.2513 - loglik: -1.1197e+03 - logprior: -1.5030e+00
Epoch 7/10
39/39 - 17s - loss: 1121.1191 - loglik: -1.1195e+03 - logprior: -1.6013e+00
Epoch 8/10
39/39 - 17s - loss: 1121.6293 - loglik: -1.1199e+03 - logprior: -1.6930e+00
Fitted a model with MAP estimate = -1117.1299
expansions: [(25, 1), (30, 5), (58, 2), (61, 2), (81, 1), (82, 1), (83, 1), (103, 1), (105, 1), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1122.1904 - loglik: -1.1200e+03 - logprior: -2.1645e+00
Epoch 2/2
39/39 - 19s - loss: 1116.3606 - loglik: -1.1153e+03 - logprior: -1.0255e+00
Fitted a model with MAP estimate = -1111.7247
expansions: [(113, 1)]
discards: [ 0 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1121.7023 - loglik: -1.1188e+03 - logprior: -2.8568e+00
Epoch 2/2
39/39 - 18s - loss: 1117.5720 - loglik: -1.1163e+03 - logprior: -1.2611e+00
Fitted a model with MAP estimate = -1112.0764
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1114.6660 - loglik: -1.1128e+03 - logprior: -1.8582e+00
Epoch 2/10
39/39 - 19s - loss: 1112.6437 - loglik: -1.1119e+03 - logprior: -7.5376e-01
Epoch 3/10
39/39 - 19s - loss: 1109.7999 - loglik: -1.1091e+03 - logprior: -6.7778e-01
Epoch 4/10
39/39 - 19s - loss: 1110.0527 - loglik: -1.1094e+03 - logprior: -6.7087e-01
Fitted a model with MAP estimate = -1108.7743
Time for alignment: 388.5376
Computed alignments with likelihoods: ['-1107.6402', '-1107.6180', '-1106.1590', '-1108.4454', '-1108.7743']
Best model has likelihood: -1106.1590  (prior= -0.5923 )
time for generating output: 0.2610
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7394813541588819
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feefdc8ba00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff14b8ad8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefc42c400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe931624ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113e8dee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113e8daf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef7826c580>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feefc418cd0>, <__main__.SimpleDirichletPrior object at 0x7ff114619130>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.2983 - loglik: -2.8990e+02 - logprior: -3.3961e+00
Epoch 2/10
19/19 - 1s - loss: 264.1060 - loglik: -2.6266e+02 - logprior: -1.4486e+00
Epoch 3/10
19/19 - 1s - loss: 254.7353 - loglik: -2.5313e+02 - logprior: -1.6059e+00
Epoch 4/10
19/19 - 1s - loss: 252.5433 - loglik: -2.5108e+02 - logprior: -1.4656e+00
Epoch 5/10
19/19 - 1s - loss: 251.8171 - loglik: -2.5033e+02 - logprior: -1.4849e+00
Epoch 6/10
19/19 - 1s - loss: 251.0252 - loglik: -2.4955e+02 - logprior: -1.4766e+00
Epoch 7/10
19/19 - 1s - loss: 251.1603 - loglik: -2.4969e+02 - logprior: -1.4749e+00
Fitted a model with MAP estimate = -250.9943
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 257.9381 - loglik: -2.5371e+02 - logprior: -4.2296e+00
Epoch 2/2
19/19 - 1s - loss: 250.0399 - loglik: -2.4800e+02 - logprior: -2.0438e+00
Fitted a model with MAP estimate = -248.2780
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 250.9587 - loglik: -2.4768e+02 - logprior: -3.2737e+00
Epoch 2/2
19/19 - 1s - loss: 247.0882 - loglik: -2.4567e+02 - logprior: -1.4152e+00
Fitted a model with MAP estimate = -246.5308
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.8894 - loglik: -2.4664e+02 - logprior: -3.2539e+00
Epoch 2/10
19/19 - 1s - loss: 247.1938 - loglik: -2.4580e+02 - logprior: -1.3891e+00
Epoch 3/10
19/19 - 1s - loss: 246.8075 - loglik: -2.4551e+02 - logprior: -1.2986e+00
Epoch 4/10
19/19 - 1s - loss: 246.4396 - loglik: -2.4518e+02 - logprior: -1.2603e+00
Epoch 5/10
19/19 - 1s - loss: 246.3512 - loglik: -2.4511e+02 - logprior: -1.2421e+00
Epoch 6/10
19/19 - 1s - loss: 246.3168 - loglik: -2.4508e+02 - logprior: -1.2399e+00
Epoch 7/10
19/19 - 1s - loss: 246.0774 - loglik: -2.4483e+02 - logprior: -1.2474e+00
Epoch 8/10
19/19 - 1s - loss: 245.9397 - loglik: -2.4470e+02 - logprior: -1.2437e+00
Epoch 9/10
19/19 - 1s - loss: 245.9818 - loglik: -2.4472e+02 - logprior: -1.2583e+00
Fitted a model with MAP estimate = -245.9644
Time for alignment: 49.8220
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 293.2014 - loglik: -2.8980e+02 - logprior: -3.3976e+00
Epoch 2/10
19/19 - 1s - loss: 264.3690 - loglik: -2.6291e+02 - logprior: -1.4563e+00
Epoch 3/10
19/19 - 1s - loss: 254.1776 - loglik: -2.5256e+02 - logprior: -1.6140e+00
Epoch 4/10
19/19 - 1s - loss: 252.2539 - loglik: -2.5079e+02 - logprior: -1.4671e+00
Epoch 5/10
19/19 - 1s - loss: 251.4428 - loglik: -2.4996e+02 - logprior: -1.4856e+00
Epoch 6/10
19/19 - 1s - loss: 251.2261 - loglik: -2.4975e+02 - logprior: -1.4749e+00
Epoch 7/10
19/19 - 1s - loss: 251.0527 - loglik: -2.4958e+02 - logprior: -1.4730e+00
Epoch 8/10
19/19 - 1s - loss: 251.2187 - loglik: -2.4973e+02 - logprior: -1.4846e+00
Fitted a model with MAP estimate = -250.9359
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.9217 - loglik: -2.5369e+02 - logprior: -4.2357e+00
Epoch 2/2
19/19 - 1s - loss: 250.1155 - loglik: -2.4806e+02 - logprior: -2.0553e+00
Fitted a model with MAP estimate = -248.3316
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 250.9372 - loglik: -2.4767e+02 - logprior: -3.2686e+00
Epoch 2/2
19/19 - 1s - loss: 246.9736 - loglik: -2.4556e+02 - logprior: -1.4103e+00
Fitted a model with MAP estimate = -246.5358
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.8840 - loglik: -2.4663e+02 - logprior: -3.2503e+00
Epoch 2/10
19/19 - 1s - loss: 247.1941 - loglik: -2.4581e+02 - logprior: -1.3848e+00
Epoch 3/10
19/19 - 1s - loss: 246.7290 - loglik: -2.4544e+02 - logprior: -1.2925e+00
Epoch 4/10
19/19 - 1s - loss: 246.4876 - loglik: -2.4523e+02 - logprior: -1.2531e+00
Epoch 5/10
19/19 - 1s - loss: 246.4998 - loglik: -2.4526e+02 - logprior: -1.2395e+00
Fitted a model with MAP estimate = -246.2270
Time for alignment: 44.7621
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.2891 - loglik: -2.8990e+02 - logprior: -3.3926e+00
Epoch 2/10
19/19 - 1s - loss: 264.7715 - loglik: -2.6332e+02 - logprior: -1.4544e+00
Epoch 3/10
19/19 - 1s - loss: 253.9955 - loglik: -2.5238e+02 - logprior: -1.6136e+00
Epoch 4/10
19/19 - 1s - loss: 252.2915 - loglik: -2.5083e+02 - logprior: -1.4629e+00
Epoch 5/10
19/19 - 1s - loss: 251.6849 - loglik: -2.5020e+02 - logprior: -1.4816e+00
Epoch 6/10
19/19 - 1s - loss: 251.5149 - loglik: -2.5004e+02 - logprior: -1.4723e+00
Epoch 7/10
19/19 - 1s - loss: 251.2037 - loglik: -2.4973e+02 - logprior: -1.4716e+00
Epoch 8/10
19/19 - 1s - loss: 251.3571 - loglik: -2.4987e+02 - logprior: -1.4839e+00
Fitted a model with MAP estimate = -251.1032
expansions: [(6, 1), (7, 2), (12, 2), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 258.8165 - loglik: -2.5456e+02 - logprior: -4.2537e+00
Epoch 2/2
19/19 - 1s - loss: 250.1418 - loglik: -2.4805e+02 - logprior: -2.0919e+00
Fitted a model with MAP estimate = -248.4014
expansions: [(0, 1)]
discards: [ 0  8 14 17 27 28 45]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 250.7415 - loglik: -2.4746e+02 - logprior: -3.2798e+00
Epoch 2/2
19/19 - 1s - loss: 247.0985 - loglik: -2.4568e+02 - logprior: -1.4147e+00
Fitted a model with MAP estimate = -246.5196
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 249.8392 - loglik: -2.4658e+02 - logprior: -3.2546e+00
Epoch 2/10
19/19 - 1s - loss: 247.1997 - loglik: -2.4581e+02 - logprior: -1.3890e+00
Epoch 3/10
19/19 - 1s - loss: 246.7390 - loglik: -2.4544e+02 - logprior: -1.2964e+00
Epoch 4/10
19/19 - 1s - loss: 246.6165 - loglik: -2.4536e+02 - logprior: -1.2575e+00
Epoch 5/10
19/19 - 1s - loss: 246.2669 - loglik: -2.4503e+02 - logprior: -1.2408e+00
Epoch 6/10
19/19 - 1s - loss: 246.2652 - loglik: -2.4502e+02 - logprior: -1.2422e+00
Epoch 7/10
19/19 - 1s - loss: 246.1466 - loglik: -2.4490e+02 - logprior: -1.2455e+00
Epoch 8/10
19/19 - 1s - loss: 246.2016 - loglik: -2.4496e+02 - logprior: -1.2409e+00
Fitted a model with MAP estimate = -246.0332
Time for alignment: 48.9437
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.3928 - loglik: -2.9000e+02 - logprior: -3.3887e+00
Epoch 2/10
19/19 - 1s - loss: 264.7131 - loglik: -2.6326e+02 - logprior: -1.4496e+00
Epoch 3/10
19/19 - 1s - loss: 254.8660 - loglik: -2.5327e+02 - logprior: -1.6008e+00
Epoch 4/10
19/19 - 1s - loss: 252.1049 - loglik: -2.5065e+02 - logprior: -1.4588e+00
Epoch 5/10
19/19 - 1s - loss: 251.5222 - loglik: -2.5004e+02 - logprior: -1.4850e+00
Epoch 6/10
19/19 - 1s - loss: 251.3693 - loglik: -2.4989e+02 - logprior: -1.4771e+00
Epoch 7/10
19/19 - 1s - loss: 251.0927 - loglik: -2.4961e+02 - logprior: -1.4794e+00
Epoch 8/10
19/19 - 1s - loss: 251.3002 - loglik: -2.4982e+02 - logprior: -1.4850e+00
Fitted a model with MAP estimate = -250.9457
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 257.9283 - loglik: -2.5369e+02 - logprior: -4.2390e+00
Epoch 2/2
19/19 - 1s - loss: 250.0683 - loglik: -2.4800e+02 - logprior: -2.0669e+00
Fitted a model with MAP estimate = -248.3495
expansions: [(0, 1)]
discards: [ 0  8 17 44]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 250.7363 - loglik: -2.4744e+02 - logprior: -3.2918e+00
Epoch 2/2
19/19 - 1s - loss: 246.6854 - loglik: -2.4525e+02 - logprior: -1.4388e+00
Fitted a model with MAP estimate = -246.2336
expansions: []
discards: [26 27 38]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 250.0094 - loglik: -2.4675e+02 - logprior: -3.2606e+00
Epoch 2/10
19/19 - 1s - loss: 247.2909 - loglik: -2.4590e+02 - logprior: -1.3869e+00
Epoch 3/10
19/19 - 1s - loss: 246.6709 - loglik: -2.4538e+02 - logprior: -1.2956e+00
Epoch 4/10
19/19 - 1s - loss: 246.6304 - loglik: -2.4537e+02 - logprior: -1.2601e+00
Epoch 5/10
19/19 - 1s - loss: 246.3150 - loglik: -2.4507e+02 - logprior: -1.2406e+00
Epoch 6/10
19/19 - 1s - loss: 246.3012 - loglik: -2.4506e+02 - logprior: -1.2449e+00
Epoch 7/10
19/19 - 1s - loss: 245.9551 - loglik: -2.4471e+02 - logprior: -1.2453e+00
Epoch 8/10
19/19 - 1s - loss: 246.0852 - loglik: -2.4484e+02 - logprior: -1.2414e+00
Fitted a model with MAP estimate = -245.9880
Time for alignment: 47.3538
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.3337 - loglik: -2.8994e+02 - logprior: -3.3923e+00
Epoch 2/10
19/19 - 1s - loss: 264.4648 - loglik: -2.6302e+02 - logprior: -1.4473e+00
Epoch 3/10
19/19 - 1s - loss: 254.2422 - loglik: -2.5263e+02 - logprior: -1.6101e+00
Epoch 4/10
19/19 - 1s - loss: 252.1327 - loglik: -2.5067e+02 - logprior: -1.4629e+00
Epoch 5/10
19/19 - 1s - loss: 251.5996 - loglik: -2.5012e+02 - logprior: -1.4796e+00
Epoch 6/10
19/19 - 1s - loss: 251.2430 - loglik: -2.4977e+02 - logprior: -1.4711e+00
Epoch 7/10
19/19 - 1s - loss: 251.2749 - loglik: -2.4981e+02 - logprior: -1.4672e+00
Fitted a model with MAP estimate = -250.9995
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.9787 - loglik: -2.5375e+02 - logprior: -4.2283e+00
Epoch 2/2
19/19 - 1s - loss: 250.0139 - loglik: -2.4796e+02 - logprior: -2.0491e+00
Fitted a model with MAP estimate = -248.2734
expansions: [(0, 1)]
discards: [ 0  8 16 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 250.7563 - loglik: -2.4748e+02 - logprior: -3.2722e+00
Epoch 2/2
19/19 - 1s - loss: 247.0354 - loglik: -2.4562e+02 - logprior: -1.4130e+00
Fitted a model with MAP estimate = -246.5115
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.8066 - loglik: -2.4655e+02 - logprior: -3.2554e+00
Epoch 2/10
19/19 - 1s - loss: 247.2439 - loglik: -2.4586e+02 - logprior: -1.3875e+00
Epoch 3/10
19/19 - 1s - loss: 246.6480 - loglik: -2.4535e+02 - logprior: -1.2988e+00
Epoch 4/10
19/19 - 1s - loss: 246.7210 - loglik: -2.4546e+02 - logprior: -1.2602e+00
Fitted a model with MAP estimate = -246.3627
Time for alignment: 42.0996
Computed alignments with likelihoods: ['-245.9644', '-246.2270', '-246.0332', '-245.9880', '-246.3627']
Best model has likelihood: -245.9644  (prior= -1.2217 )
time for generating output: 0.1171
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7729610580455547
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff15c46c0a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feefc3dcd00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9402c8310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2eff799d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c6013f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef781f30d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff121418100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef4072cdf0>, <__main__.SimpleDirichletPrior object at 0x7feefdc5f640>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 940.6636 - loglik: -9.3720e+02 - logprior: -3.4667e+00
Epoch 2/10
19/19 - 6s - loss: 865.2863 - loglik: -8.6388e+02 - logprior: -1.4020e+00
Epoch 3/10
19/19 - 6s - loss: 834.2802 - loglik: -8.3257e+02 - logprior: -1.7143e+00
Epoch 4/10
19/19 - 6s - loss: 827.3153 - loglik: -8.2553e+02 - logprior: -1.7856e+00
Epoch 5/10
19/19 - 6s - loss: 823.0348 - loglik: -8.2129e+02 - logprior: -1.7425e+00
Epoch 6/10
19/19 - 6s - loss: 821.3338 - loglik: -8.1959e+02 - logprior: -1.7461e+00
Epoch 7/10
19/19 - 6s - loss: 821.3000 - loglik: -8.1956e+02 - logprior: -1.7385e+00
Epoch 8/10
19/19 - 6s - loss: 820.6795 - loglik: -8.1891e+02 - logprior: -1.7741e+00
Epoch 9/10
19/19 - 6s - loss: 820.5687 - loglik: -8.1878e+02 - logprior: -1.7860e+00
Epoch 10/10
19/19 - 6s - loss: 818.6865 - loglik: -8.1685e+02 - logprior: -1.8400e+00
Fitted a model with MAP estimate = -812.0547
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (33, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (65, 2), (66, 1), (68, 1), (71, 2), (80, 1), (87, 2), (88, 2), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 808.7181 - loglik: -8.0615e+02 - logprior: -2.5713e+00
Epoch 2/2
39/39 - 10s - loss: 796.6056 - loglik: -7.9556e+02 - logprior: -1.0452e+00
Fitted a model with MAP estimate = -787.7129
expansions: []
discards: [  0  55  85  94 115]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 801.5686 - loglik: -7.9875e+02 - logprior: -2.8228e+00
Epoch 2/2
39/39 - 10s - loss: 797.0666 - loglik: -7.9622e+02 - logprior: -8.4259e-01
Fitted a model with MAP estimate = -788.0486
expansions: []
discards: [112]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 792.3995 - loglik: -7.9068e+02 - logprior: -1.7182e+00
Epoch 2/10
39/39 - 10s - loss: 789.2370 - loglik: -7.8866e+02 - logprior: -5.7732e-01
Epoch 3/10
39/39 - 10s - loss: 787.5230 - loglik: -7.8705e+02 - logprior: -4.7761e-01
Epoch 4/10
39/39 - 10s - loss: 787.6475 - loglik: -7.8719e+02 - logprior: -4.5290e-01
Fitted a model with MAP estimate = -785.9696
Time for alignment: 208.0000
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 941.6401 - loglik: -9.3818e+02 - logprior: -3.4618e+00
Epoch 2/10
19/19 - 6s - loss: 862.6808 - loglik: -8.6130e+02 - logprior: -1.3798e+00
Epoch 3/10
19/19 - 6s - loss: 831.8541 - loglik: -8.3014e+02 - logprior: -1.7110e+00
Epoch 4/10
19/19 - 7s - loss: 826.5594 - loglik: -8.2474e+02 - logprior: -1.8194e+00
Epoch 5/10
19/19 - 6s - loss: 824.0767 - loglik: -8.2234e+02 - logprior: -1.7328e+00
Epoch 6/10
19/19 - 7s - loss: 821.8576 - loglik: -8.2014e+02 - logprior: -1.7212e+00
Epoch 7/10
19/19 - 7s - loss: 821.4356 - loglik: -8.1972e+02 - logprior: -1.7179e+00
Epoch 8/10
19/19 - 7s - loss: 823.0445 - loglik: -8.2130e+02 - logprior: -1.7467e+00
Fitted a model with MAP estimate = -813.6060
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (38, 1), (44, 1), (48, 1), (51, 1), (52, 1), (59, 1), (66, 1), (67, 1), (69, 1), (70, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (103, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 809.4532 - loglik: -8.0699e+02 - logprior: -2.4630e+00
Epoch 2/2
39/39 - 11s - loss: 799.1017 - loglik: -7.9816e+02 - logprior: -9.4342e-01
Fitted a model with MAP estimate = -790.1656
expansions: []
discards: [  0 111]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 803.8338 - loglik: -8.0104e+02 - logprior: -2.7920e+00
Epoch 2/2
39/39 - 10s - loss: 799.7849 - loglik: -7.9892e+02 - logprior: -8.6736e-01
Fitted a model with MAP estimate = -790.5181
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 794.0769 - loglik: -7.9235e+02 - logprior: -1.7249e+00
Epoch 2/10
39/39 - 10s - loss: 791.3722 - loglik: -7.9079e+02 - logprior: -5.8222e-01
Epoch 3/10
39/39 - 10s - loss: 790.2320 - loglik: -7.8973e+02 - logprior: -5.0290e-01
Epoch 4/10
39/39 - 10s - loss: 789.9279 - loglik: -7.8947e+02 - logprior: -4.5293e-01
Epoch 5/10
39/39 - 10s - loss: 787.9253 - loglik: -7.8750e+02 - logprior: -4.2523e-01
Epoch 6/10
39/39 - 10s - loss: 785.7923 - loglik: -7.8537e+02 - logprior: -4.2248e-01
Epoch 7/10
39/39 - 10s - loss: 787.8091 - loglik: -7.8739e+02 - logprior: -4.1511e-01
Fitted a model with MAP estimate = -786.3332
Time for alignment: 229.8125
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 940.9198 - loglik: -9.3745e+02 - logprior: -3.4662e+00
Epoch 2/10
19/19 - 6s - loss: 864.4803 - loglik: -8.6310e+02 - logprior: -1.3809e+00
Epoch 3/10
19/19 - 6s - loss: 836.3105 - loglik: -8.3460e+02 - logprior: -1.7094e+00
Epoch 4/10
19/19 - 6s - loss: 830.3257 - loglik: -8.2855e+02 - logprior: -1.7714e+00
Epoch 5/10
19/19 - 6s - loss: 826.7745 - loglik: -8.2506e+02 - logprior: -1.7162e+00
Epoch 6/10
19/19 - 6s - loss: 823.5129 - loglik: -8.2181e+02 - logprior: -1.6981e+00
Epoch 7/10
19/19 - 6s - loss: 823.3289 - loglik: -8.2161e+02 - logprior: -1.7198e+00
Epoch 8/10
19/19 - 6s - loss: 823.5275 - loglik: -8.2177e+02 - logprior: -1.7620e+00
Fitted a model with MAP estimate = -814.6675
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (39, 1), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 2), (67, 1), (69, 1), (70, 1), (80, 1), (87, 1), (88, 1), (89, 1), (101, 1), (113, 1), (114, 2), (116, 1), (118, 1), (123, 1), (136, 2), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 811.7825 - loglik: -8.0925e+02 - logprior: -2.5305e+00
Epoch 2/2
39/39 - 10s - loss: 801.2224 - loglik: -8.0021e+02 - logprior: -1.0148e+00
Fitted a model with MAP estimate = -792.4735
expansions: [(118, 1)]
discards: [  0  84 170]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 806.0647 - loglik: -8.0326e+02 - logprior: -2.8077e+00
Epoch 2/2
39/39 - 10s - loss: 800.4636 - loglik: -7.9962e+02 - logprior: -8.4334e-01
Fitted a model with MAP estimate = -791.7847
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 795.7601 - loglik: -7.9406e+02 - logprior: -1.7021e+00
Epoch 2/10
39/39 - 10s - loss: 792.3790 - loglik: -7.9181e+02 - logprior: -5.6909e-01
Epoch 3/10
39/39 - 10s - loss: 791.3557 - loglik: -7.9088e+02 - logprior: -4.7586e-01
Epoch 4/10
39/39 - 10s - loss: 790.6865 - loglik: -7.9024e+02 - logprior: -4.4644e-01
Epoch 5/10
39/39 - 10s - loss: 789.1167 - loglik: -7.8868e+02 - logprior: -4.3202e-01
Epoch 6/10
39/39 - 10s - loss: 788.0969 - loglik: -7.8766e+02 - logprior: -4.3493e-01
Epoch 7/10
39/39 - 10s - loss: 787.6249 - loglik: -7.8719e+02 - logprior: -4.3966e-01
Epoch 8/10
39/39 - 10s - loss: 787.9580 - loglik: -7.8753e+02 - logprior: -4.2297e-01
Fitted a model with MAP estimate = -787.2300
Time for alignment: 232.6200
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 941.3279 - loglik: -9.3786e+02 - logprior: -3.4704e+00
Epoch 2/10
19/19 - 6s - loss: 863.0847 - loglik: -8.6169e+02 - logprior: -1.3948e+00
Epoch 3/10
19/19 - 6s - loss: 828.9320 - loglik: -8.2724e+02 - logprior: -1.6902e+00
Epoch 4/10
19/19 - 6s - loss: 823.4084 - loglik: -8.2164e+02 - logprior: -1.7677e+00
Epoch 5/10
19/19 - 6s - loss: 821.2043 - loglik: -8.1950e+02 - logprior: -1.7051e+00
Epoch 6/10
19/19 - 6s - loss: 819.3766 - loglik: -8.1767e+02 - logprior: -1.7029e+00
Epoch 7/10
19/19 - 6s - loss: 819.4490 - loglik: -8.1772e+02 - logprior: -1.7251e+00
Fitted a model with MAP estimate = -811.3575
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (116, 1), (118, 1), (122, 1), (123, 1), (136, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 806.7831 - loglik: -8.0433e+02 - logprior: -2.4516e+00
Epoch 2/2
39/39 - 10s - loss: 795.8483 - loglik: -7.9491e+02 - logprior: -9.3380e-01
Fitted a model with MAP estimate = -786.9113
expansions: []
discards: [  0  55 113]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 800.7884 - loglik: -7.9801e+02 - logprior: -2.7761e+00
Epoch 2/2
39/39 - 10s - loss: 796.4482 - loglik: -7.9566e+02 - logprior: -7.8917e-01
Fitted a model with MAP estimate = -787.6304
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 791.1885 - loglik: -7.8952e+02 - logprior: -1.6674e+00
Epoch 2/10
39/39 - 10s - loss: 788.9230 - loglik: -7.8841e+02 - logprior: -5.1309e-01
Epoch 3/10
39/39 - 10s - loss: 786.6132 - loglik: -7.8618e+02 - logprior: -4.3373e-01
Epoch 4/10
39/39 - 11s - loss: 786.8411 - loglik: -7.8645e+02 - logprior: -3.9124e-01
Fitted a model with MAP estimate = -785.3056
Time for alignment: 192.0627
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 941.4707 - loglik: -9.3800e+02 - logprior: -3.4748e+00
Epoch 2/10
19/19 - 6s - loss: 863.3084 - loglik: -8.6191e+02 - logprior: -1.3992e+00
Epoch 3/10
19/19 - 7s - loss: 832.1415 - loglik: -8.3048e+02 - logprior: -1.6641e+00
Epoch 4/10
19/19 - 7s - loss: 825.4940 - loglik: -8.2378e+02 - logprior: -1.7113e+00
Epoch 5/10
19/19 - 7s - loss: 822.1674 - loglik: -8.2049e+02 - logprior: -1.6801e+00
Epoch 6/10
19/19 - 7s - loss: 822.9641 - loglik: -8.2127e+02 - logprior: -1.6928e+00
Fitted a model with MAP estimate = -813.9050
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (23, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (51, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 2), (88, 1), (89, 1), (101, 1), (113, 1), (114, 2), (115, 1), (116, 1), (123, 1), (136, 1), (137, 1), (138, 1), (139, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 809.4726 - loglik: -8.0699e+02 - logprior: -2.4797e+00
Epoch 2/2
39/39 - 10s - loss: 799.0432 - loglik: -7.9805e+02 - logprior: -9.9764e-01
Fitted a model with MAP estimate = -790.2244
expansions: []
discards: [  0  26  55 114]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 804.4819 - loglik: -8.0166e+02 - logprior: -2.8184e+00
Epoch 2/2
39/39 - 10s - loss: 799.9172 - loglik: -7.9906e+02 - logprior: -8.5955e-01
Fitted a model with MAP estimate = -791.1858
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 794.6875 - loglik: -7.9295e+02 - logprior: -1.7325e+00
Epoch 2/10
39/39 - 10s - loss: 791.4943 - loglik: -7.9091e+02 - logprior: -5.8438e-01
Epoch 3/10
39/39 - 10s - loss: 792.0264 - loglik: -7.9152e+02 - logprior: -5.1063e-01
Fitted a model with MAP estimate = -789.7810
Time for alignment: 174.2872
Computed alignments with likelihoods: ['-785.9696', '-786.3332', '-787.2300', '-785.3056', '-789.7810']
Best model has likelihood: -785.3056  (prior= -0.3443 )
time for generating output: 0.3822
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.24618293422807583
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2def86970>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feefd182280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefdafcd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefdafc460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef40304e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11849f8e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c5e01580>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c5d2d250>, <__main__.SimpleDirichletPrior object at 0x7ff15bd64a60>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 1955.6322 - loglik: -1.9535e+03 - logprior: -2.1178e+00
Epoch 2/10
39/39 - 50s - loss: 1773.3920 - loglik: -1.7714e+03 - logprior: -1.9730e+00
Epoch 3/10
39/39 - 49s - loss: 1758.1625 - loglik: -1.7561e+03 - logprior: -2.0595e+00
Epoch 4/10
39/39 - 50s - loss: 1752.3035 - loglik: -1.7502e+03 - logprior: -2.1362e+00
Epoch 5/10
39/39 - 54s - loss: 1750.9556 - loglik: -1.7487e+03 - logprior: -2.2283e+00
Epoch 6/10
39/39 - 53s - loss: 1748.4166 - loglik: -1.7460e+03 - logprior: -2.4604e+00
Epoch 7/10
39/39 - 52s - loss: 1747.3231 - loglik: -1.7446e+03 - logprior: -2.7534e+00
Epoch 8/10
39/39 - 54s - loss: 1747.3453 - loglik: -1.7444e+03 - logprior: -2.9509e+00
Fitted a model with MAP estimate = -1745.1556
expansions: [(0, 4), (12, 1), (35, 1), (45, 1), (60, 1), (62, 2), (66, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (141, 1), (145, 1), (147, 1), (149, 1), (151, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (180, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 4), (232, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 3), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 1729.5369 - loglik: -1.7268e+03 - logprior: -2.7609e+00
Epoch 2/2
39/39 - 77s - loss: 1710.3472 - loglik: -1.7094e+03 - logprior: -9.7666e-01
Fitted a model with MAP estimate = -1705.6720
expansions: []
discards: [  0 101 147 152 231 288 324 326 350]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 1717.7338 - loglik: -1.7152e+03 - logprior: -2.5204e+00
Epoch 2/2
39/39 - 76s - loss: 1711.4556 - loglik: -1.7109e+03 - logprior: -5.6898e-01
Fitted a model with MAP estimate = -1706.6179
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 1713.2810 - loglik: -1.7117e+03 - logprior: -1.6244e+00
Epoch 2/10
39/39 - 73s - loss: 1708.3479 - loglik: -1.7081e+03 - logprior: -2.2216e-01
Epoch 3/10
39/39 - 74s - loss: 1704.3840 - loglik: -1.7043e+03 - logprior: -7.8358e-02
Epoch 4/10
39/39 - 74s - loss: 1701.3441 - loglik: -1.7013e+03 - logprior: -4.3372e-02
Epoch 5/10
39/39 - 74s - loss: 1698.9825 - loglik: -1.6989e+03 - logprior: -7.9307e-02
Epoch 6/10
39/39 - 76s - loss: 1697.9675 - loglik: -1.6980e+03 - logprior: -4.3561e-03
Epoch 7/10
39/39 - 77s - loss: 1697.2949 - loglik: -1.6972e+03 - logprior: -4.7400e-02
Epoch 8/10
39/39 - 81s - loss: 1695.1302 - loglik: -1.6949e+03 - logprior: -2.1275e-01
Epoch 9/10
39/39 - 75s - loss: 1695.6678 - loglik: -1.6953e+03 - logprior: -3.5656e-01
Fitted a model with MAP estimate = -1694.4050
Time for alignment: 1762.7848
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1953.4110 - loglik: -1.9513e+03 - logprior: -2.0928e+00
Epoch 2/10
39/39 - 48s - loss: 1772.5306 - loglik: -1.7706e+03 - logprior: -1.9579e+00
Epoch 3/10
39/39 - 49s - loss: 1758.3329 - loglik: -1.7562e+03 - logprior: -2.0883e+00
Epoch 4/10
39/39 - 49s - loss: 1753.9860 - loglik: -1.7518e+03 - logprior: -2.2241e+00
Epoch 5/10
39/39 - 50s - loss: 1750.3832 - loglik: -1.7480e+03 - logprior: -2.4215e+00
Epoch 6/10
39/39 - 52s - loss: 1747.9835 - loglik: -1.7454e+03 - logprior: -2.6105e+00
Epoch 7/10
39/39 - 54s - loss: 1748.0010 - loglik: -1.7450e+03 - logprior: -2.9773e+00
Fitted a model with MAP estimate = -1745.7311
expansions: [(0, 5), (21, 1), (46, 1), (55, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (164, 2), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (197, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (232, 1), (256, 1), (257, 1), (258, 1), (259, 3), (260, 2), (261, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 382 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1730.2087 - loglik: -1.7276e+03 - logprior: -2.5787e+00
Epoch 2/2
39/39 - 60s - loss: 1710.1060 - loglik: -1.7091e+03 - logprior: -9.6282e-01
Fitted a model with MAP estimate = -1704.9776
expansions: []
discards: [  1   2 148 153 326]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1714.0042 - loglik: -1.7123e+03 - logprior: -1.7146e+00
Epoch 2/2
39/39 - 59s - loss: 1709.1125 - loglik: -1.7085e+03 - logprior: -6.0867e-01
Fitted a model with MAP estimate = -1704.6813
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 1712.6565 - loglik: -1.7111e+03 - logprior: -1.5676e+00
Epoch 2/10
39/39 - 59s - loss: 1707.8121 - loglik: -1.7075e+03 - logprior: -2.6389e-01
Epoch 3/10
39/39 - 59s - loss: 1703.9084 - loglik: -1.7038e+03 - logprior: -1.4688e-01
Epoch 4/10
39/39 - 59s - loss: 1702.2281 - loglik: -1.7021e+03 - logprior: -1.0742e-01
Epoch 5/10
39/39 - 59s - loss: 1698.4437 - loglik: -1.6983e+03 - logprior: -1.5519e-01
Epoch 6/10
39/39 - 59s - loss: 1696.6256 - loglik: -1.6965e+03 - logprior: -1.6871e-01
Epoch 7/10
39/39 - 58s - loss: 1695.9347 - loglik: -1.6958e+03 - logprior: -1.7136e-01
Epoch 8/10
39/39 - 59s - loss: 1696.5519 - loglik: -1.6963e+03 - logprior: -2.9548e-01
Fitted a model with MAP estimate = -1694.7897
Time for alignment: 1339.4280
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 1954.6838 - loglik: -1.9526e+03 - logprior: -2.0642e+00
Epoch 2/10
39/39 - 47s - loss: 1774.0715 - loglik: -1.7723e+03 - logprior: -1.7518e+00
Epoch 3/10
39/39 - 50s - loss: 1759.0671 - loglik: -1.7572e+03 - logprior: -1.9078e+00
Epoch 4/10
39/39 - 52s - loss: 1754.5322 - loglik: -1.7525e+03 - logprior: -2.0034e+00
Epoch 5/10
39/39 - 53s - loss: 1750.7748 - loglik: -1.7486e+03 - logprior: -2.1848e+00
Epoch 6/10
39/39 - 49s - loss: 1750.2772 - loglik: -1.7477e+03 - logprior: -2.5602e+00
Epoch 7/10
39/39 - 51s - loss: 1749.0198 - loglik: -1.7463e+03 - logprior: -2.6793e+00
Epoch 8/10
39/39 - 54s - loss: 1747.4753 - loglik: -1.7446e+03 - logprior: -2.8862e+00
Epoch 9/10
39/39 - 49s - loss: 1747.3263 - loglik: -1.7442e+03 - logprior: -3.0952e+00
Epoch 10/10
39/39 - 47s - loss: 1746.7384 - loglik: -1.7435e+03 - logprior: -3.2216e+00
Fitted a model with MAP estimate = -1745.3610
expansions: [(0, 4), (24, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (103, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (127, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 2), (189, 2), (190, 1), (191, 1), (201, 1), (206, 2), (208, 2), (209, 1), (210, 1), (228, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 1), (258, 1), (259, 5), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 1732.1403 - loglik: -1.7292e+03 - logprior: -2.9001e+00
Epoch 2/2
39/39 - 78s - loss: 1709.7172 - loglik: -1.7087e+03 - logprior: -1.0206e+00
Fitted a model with MAP estimate = -1704.8850
expansions: []
discards: [  0 147 152 223 232 289 324 350 366 367]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 1717.0416 - loglik: -1.7148e+03 - logprior: -2.2432e+00
Epoch 2/2
39/39 - 82s - loss: 1710.4878 - loglik: -1.7103e+03 - logprior: -1.9925e-01
Fitted a model with MAP estimate = -1706.3973
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 1713.3123 - loglik: -1.7120e+03 - logprior: -1.3289e+00
Epoch 2/10
39/39 - 72s - loss: 1708.1007 - loglik: -1.7081e+03 - logprior: 0.0197
Epoch 3/10
39/39 - 76s - loss: 1706.1381 - loglik: -1.7063e+03 - logprior: 0.1901
Epoch 4/10
39/39 - 77s - loss: 1701.7544 - loglik: -1.7020e+03 - logprior: 0.1979
Epoch 5/10
39/39 - 72s - loss: 1699.7744 - loglik: -1.6997e+03 - logprior: -1.0722e-01
Epoch 6/10
39/39 - 81s - loss: 1697.4175 - loglik: -1.6975e+03 - logprior: 0.1027
Epoch 7/10
39/39 - 86s - loss: 1695.9976 - loglik: -1.6960e+03 - logprior: -2.8045e-02
Epoch 8/10
39/39 - 85s - loss: 1695.9565 - loglik: -1.6959e+03 - logprior: -6.8567e-02
Epoch 9/10
39/39 - 84s - loss: 1695.0919 - loglik: -1.6950e+03 - logprior: -1.3749e-01
Epoch 10/10
39/39 - 80s - loss: 1695.3085 - loglik: -1.6951e+03 - logprior: -1.9069e-01
Fitted a model with MAP estimate = -1693.9075
Time for alignment: 2005.9382
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1955.9832 - loglik: -1.9539e+03 - logprior: -2.0943e+00
Epoch 2/10
39/39 - 53s - loss: 1775.7506 - loglik: -1.7739e+03 - logprior: -1.8438e+00
Epoch 3/10
39/39 - 50s - loss: 1761.1741 - loglik: -1.7592e+03 - logprior: -1.9749e+00
Epoch 4/10
39/39 - 54s - loss: 1757.2922 - loglik: -1.7553e+03 - logprior: -2.0199e+00
Epoch 5/10
39/39 - 54s - loss: 1753.2794 - loglik: -1.7510e+03 - logprior: -2.2581e+00
Epoch 6/10
39/39 - 47s - loss: 1751.9523 - loglik: -1.7495e+03 - logprior: -2.4172e+00
Epoch 7/10
39/39 - 49s - loss: 1750.9712 - loglik: -1.7482e+03 - logprior: -2.7563e+00
Epoch 8/10
39/39 - 54s - loss: 1748.7516 - loglik: -1.7458e+03 - logprior: -2.9221e+00
Epoch 9/10
39/39 - 52s - loss: 1750.0831 - loglik: -1.7471e+03 - logprior: -3.0268e+00
Fitted a model with MAP estimate = -1747.6420
expansions: [(0, 5), (20, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 2), (92, 1), (94, 1), (113, 3), (118, 1), (122, 2), (125, 2), (127, 1), (145, 1), (150, 3), (156, 1), (159, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 4), (232, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 1733.6957 - loglik: -1.7308e+03 - logprior: -2.9188e+00
Epoch 2/2
39/39 - 92s - loss: 1710.9181 - loglik: -1.7098e+03 - logprior: -1.1443e+00
Fitted a model with MAP estimate = -1705.9081
expansions: []
discards: [  1   2 148 153 183 201 234 291 326 353]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 1714.7841 - loglik: -1.7131e+03 - logprior: -1.7148e+00
Epoch 2/2
39/39 - 80s - loss: 1709.9734 - loglik: -1.7094e+03 - logprior: -5.8084e-01
Fitted a model with MAP estimate = -1705.8626
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 1713.3772 - loglik: -1.7119e+03 - logprior: -1.4478e+00
Epoch 2/10
39/39 - 74s - loss: 1708.3044 - loglik: -1.7082e+03 - logprior: -1.4183e-01
Epoch 3/10
39/39 - 79s - loss: 1705.7612 - loglik: -1.7056e+03 - logprior: -1.7358e-01
Epoch 4/10
39/39 - 70s - loss: 1701.4946 - loglik: -1.7014e+03 - logprior: -1.3078e-01
Epoch 5/10
39/39 - 79s - loss: 1700.0840 - loglik: -1.7000e+03 - logprior: -1.0279e-01
Epoch 6/10
39/39 - 78s - loss: 1697.7854 - loglik: -1.6977e+03 - logprior: -1.0839e-01
Epoch 7/10
39/39 - 82s - loss: 1695.4137 - loglik: -1.6952e+03 - logprior: -1.9046e-01
Epoch 8/10
39/39 - 80s - loss: 1696.3518 - loglik: -1.6961e+03 - logprior: -2.5342e-01
Fitted a model with MAP estimate = -1695.3245
Time for alignment: 1811.1832
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1956.1619 - loglik: -1.9541e+03 - logprior: -2.0795e+00
Epoch 2/10
39/39 - 54s - loss: 1775.2528 - loglik: -1.7734e+03 - logprior: -1.8062e+00
Epoch 3/10
39/39 - 51s - loss: 1758.9584 - loglik: -1.7571e+03 - logprior: -1.9028e+00
Epoch 4/10
39/39 - 47s - loss: 1755.4287 - loglik: -1.7535e+03 - logprior: -1.9581e+00
Epoch 5/10
39/39 - 51s - loss: 1751.5515 - loglik: -1.7494e+03 - logprior: -2.1975e+00
Epoch 6/10
39/39 - 54s - loss: 1750.2438 - loglik: -1.7477e+03 - logprior: -2.5654e+00
Epoch 7/10
39/39 - 48s - loss: 1748.5541 - loglik: -1.7458e+03 - logprior: -2.7119e+00
Epoch 8/10
39/39 - 48s - loss: 1747.8766 - loglik: -1.7450e+03 - logprior: -2.9115e+00
Epoch 9/10
39/39 - 52s - loss: 1748.3969 - loglik: -1.7453e+03 - logprior: -3.1191e+00
Fitted a model with MAP estimate = -1745.8807
expansions: [(0, 5), (20, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 2), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (144, 1), (146, 1), (149, 1), (156, 1), (158, 2), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 1), (189, 1), (190, 1), (191, 1), (194, 1), (197, 1), (206, 1), (208, 2), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 1733.9231 - loglik: -1.7309e+03 - logprior: -2.9781e+00
Epoch 2/2
39/39 - 66s - loss: 1709.7491 - loglik: -1.7085e+03 - logprior: -1.2285e+00
Fitted a model with MAP estimate = -1704.5632
expansions: []
discards: [  1   2   3 103 148 153 287 323 350 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1715.3838 - loglik: -1.7138e+03 - logprior: -1.5809e+00
Epoch 2/2
39/39 - 62s - loss: 1710.5881 - loglik: -1.7103e+03 - logprior: -2.7990e-01
Fitted a model with MAP estimate = -1706.0375
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 1712.3550 - loglik: -1.7110e+03 - logprior: -1.3339e+00
Epoch 2/10
39/39 - 63s - loss: 1707.3512 - loglik: -1.7072e+03 - logprior: -1.2434e-01
Epoch 3/10
39/39 - 62s - loss: 1704.7659 - loglik: -1.7046e+03 - logprior: -1.4298e-01
Epoch 4/10
39/39 - 62s - loss: 1701.3690 - loglik: -1.7013e+03 - logprior: -9.3761e-02
Epoch 5/10
39/39 - 62s - loss: 1697.7950 - loglik: -1.6978e+03 - logprior: -1.5465e-02
Epoch 6/10
39/39 - 62s - loss: 1696.5038 - loglik: -1.6965e+03 - logprior: 0.0349
Epoch 7/10
39/39 - 62s - loss: 1695.0715 - loglik: -1.6951e+03 - logprior: 0.0108
Epoch 8/10
39/39 - 62s - loss: 1695.0120 - loglik: -1.6950e+03 - logprior: -4.2145e-02
Epoch 9/10
39/39 - 62s - loss: 1694.2938 - loglik: -1.6940e+03 - logprior: -3.1249e-01
Epoch 10/10
39/39 - 61s - loss: 1694.7235 - loglik: -1.6945e+03 - logprior: -1.9580e-01
Fitted a model with MAP estimate = -1693.7727
Time for alignment: 1658.8496
Computed alignments with likelihoods: ['-1694.4050', '-1694.7897', '-1693.9075', '-1695.3245', '-1693.7727']
Best model has likelihood: -1693.7727  (prior= -0.3837 )
time for generating output: 0.4279
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9463961235614778
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe921daa730>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feefe969640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef9c089610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef8078ce50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe8c1ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c203fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff14299f460>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feefef73190>, <__main__.SimpleDirichletPrior object at 0x7fe9503b1220>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1359.5664 - loglik: -1.3572e+03 - logprior: -2.3183e+00
Epoch 2/10
39/39 - 18s - loss: 1270.4330 - loglik: -1.2689e+03 - logprior: -1.5147e+00
Epoch 3/10
39/39 - 18s - loss: 1261.2448 - loglik: -1.2596e+03 - logprior: -1.6627e+00
Epoch 4/10
39/39 - 18s - loss: 1260.6531 - loglik: -1.2589e+03 - logprior: -1.7915e+00
Epoch 5/10
39/39 - 18s - loss: 1258.8638 - loglik: -1.2570e+03 - logprior: -1.9013e+00
Epoch 6/10
39/39 - 18s - loss: 1258.2849 - loglik: -1.2562e+03 - logprior: -2.0370e+00
Epoch 7/10
39/39 - 18s - loss: 1258.6769 - loglik: -1.2565e+03 - logprior: -2.1572e+00
Fitted a model with MAP estimate = -1257.0380
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (81, 4), (84, 4), (87, 2), (88, 1), (89, 1), (113, 1), (114, 9), (115, 3), (116, 1), (125, 2), (127, 3), (128, 2), (133, 1), (135, 1), (138, 3), (139, 9), (140, 1), (142, 1), (144, 1), (154, 1), (157, 4), (159, 1), (170, 5), (184, 1)]
discards: [  0 146 147 148 149 160 161 162 163 164 165 166 167 168 186 187 188 189
 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1276.9349 - loglik: -1.2740e+03 - logprior: -2.8989e+00
Epoch 2/2
39/39 - 21s - loss: 1264.8948 - loglik: -1.2637e+03 - logprior: -1.2239e+00
Fitted a model with MAP estimate = -1260.5020
expansions: [(238, 20)]
discards: [  0  86  87 101 132 133 134 135 154 160 183 184 185 199 221 222 223 224
 225 226]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1263.2432 - loglik: -1.2605e+03 - logprior: -2.7228e+00
Epoch 2/2
39/39 - 21s - loss: 1254.2883 - loglik: -1.2535e+03 - logprior: -8.3110e-01
Fitted a model with MAP estimate = -1250.9060
expansions: [(0, 2), (79, 1), (85, 4), (187, 4), (188, 1), (197, 1), (217, 4), (238, 3)]
discards: [  0  81 173 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232
 233 234 235 236 237]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1260.3966 - loglik: -1.2587e+03 - logprior: -1.7371e+00
Epoch 2/10
39/39 - 21s - loss: 1253.5521 - loglik: -1.2531e+03 - logprior: -4.8348e-01
Epoch 3/10
39/39 - 21s - loss: 1252.6866 - loglik: -1.2523e+03 - logprior: -4.1109e-01
Epoch 4/10
39/39 - 20s - loss: 1250.7112 - loglik: -1.2503e+03 - logprior: -4.3209e-01
Epoch 5/10
39/39 - 20s - loss: 1249.4319 - loglik: -1.2490e+03 - logprior: -4.6900e-01
Epoch 6/10
39/39 - 20s - loss: 1249.2352 - loglik: -1.2487e+03 - logprior: -5.5681e-01
Epoch 7/10
39/39 - 20s - loss: 1247.6440 - loglik: -1.2470e+03 - logprior: -6.4188e-01
Epoch 8/10
39/39 - 20s - loss: 1249.3015 - loglik: -1.2486e+03 - logprior: -7.1192e-01
Fitted a model with MAP estimate = -1247.6670
Time for alignment: 483.9423
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1362.6719 - loglik: -1.3603e+03 - logprior: -2.3322e+00
Epoch 2/10
39/39 - 17s - loss: 1271.4921 - loglik: -1.2700e+03 - logprior: -1.4954e+00
Epoch 3/10
39/39 - 17s - loss: 1262.7963 - loglik: -1.2611e+03 - logprior: -1.6758e+00
Epoch 4/10
39/39 - 17s - loss: 1261.0521 - loglik: -1.2593e+03 - logprior: -1.8005e+00
Epoch 5/10
39/39 - 17s - loss: 1258.8295 - loglik: -1.2569e+03 - logprior: -1.9334e+00
Epoch 6/10
39/39 - 17s - loss: 1258.2087 - loglik: -1.2561e+03 - logprior: -2.0883e+00
Epoch 7/10
39/39 - 17s - loss: 1257.4830 - loglik: -1.2553e+03 - logprior: -2.2140e+00
Epoch 8/10
39/39 - 17s - loss: 1257.1432 - loglik: -1.2548e+03 - logprior: -2.3369e+00
Epoch 9/10
39/39 - 17s - loss: 1257.7529 - loglik: -1.2553e+03 - logprior: -2.4283e+00
Fitted a model with MAP estimate = -1256.0954
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (77, 1), (81, 8), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (114, 1), (115, 10), (116, 2), (117, 2), (126, 1), (128, 3), (129, 2), (132, 1), (136, 1), (139, 2), (140, 1), (141, 7), (142, 1), (144, 1), (146, 1), (156, 1), (158, 5), (170, 1), (171, 3), (173, 1), (183, 1), (184, 1)]
discards: [  0  78  79 148 149 150 151 152 159 160 161 162 163 164 165 166 167 175
 176 177 178 179 180 181 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 1282.1532 - loglik: -1.2792e+03 - logprior: -2.9096e+00
Epoch 2/2
39/39 - 19s - loss: 1266.0917 - loglik: -1.2648e+03 - logprior: -1.2447e+00
Fitted a model with MAP estimate = -1261.5635
expansions: [(201, 9), (228, 6), (229, 7)]
discards: [  0  82 133 134 135 142 162 189 212 213 214 218 219 220 221 222 223]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 1269.1443 - loglik: -1.2666e+03 - logprior: -2.5841e+00
Epoch 2/2
39/39 - 20s - loss: 1260.8820 - loglik: -1.2601e+03 - logprior: -7.9820e-01
Fitted a model with MAP estimate = -1257.8782
expansions: [(0, 2), (86, 2), (87, 2), (204, 9), (234, 7)]
discards: [  0  81  82  83  84 177 181 197 198 215 216 217 218 219 220 221 222 223
 225 226 227 228 229 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1265.4634 - loglik: -1.2638e+03 - logprior: -1.6933e+00
Epoch 2/10
39/39 - 19s - loss: 1257.7931 - loglik: -1.2574e+03 - logprior: -4.0796e-01
Epoch 3/10
39/39 - 19s - loss: 1256.4333 - loglik: -1.2561e+03 - logprior: -3.4976e-01
Epoch 4/10
39/39 - 19s - loss: 1254.2863 - loglik: -1.2539e+03 - logprior: -3.7631e-01
Epoch 5/10
39/39 - 19s - loss: 1253.7296 - loglik: -1.2533e+03 - logprior: -4.1315e-01
Epoch 6/10
39/39 - 19s - loss: 1252.6141 - loglik: -1.2522e+03 - logprior: -4.5947e-01
Epoch 7/10
39/39 - 19s - loss: 1252.5405 - loglik: -1.2520e+03 - logprior: -5.5516e-01
Epoch 8/10
39/39 - 20s - loss: 1252.8005 - loglik: -1.2522e+03 - logprior: -6.1120e-01
Fitted a model with MAP estimate = -1251.8015
Time for alignment: 496.6745
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1361.7319 - loglik: -1.3594e+03 - logprior: -2.3401e+00
Epoch 2/10
39/39 - 17s - loss: 1271.6655 - loglik: -1.2701e+03 - logprior: -1.5221e+00
Epoch 3/10
39/39 - 18s - loss: 1262.8225 - loglik: -1.2610e+03 - logprior: -1.7777e+00
Epoch 4/10
39/39 - 18s - loss: 1260.3368 - loglik: -1.2585e+03 - logprior: -1.8735e+00
Epoch 5/10
39/39 - 18s - loss: 1258.6980 - loglik: -1.2567e+03 - logprior: -1.9622e+00
Epoch 6/10
39/39 - 18s - loss: 1258.7408 - loglik: -1.2566e+03 - logprior: -2.0940e+00
Fitted a model with MAP estimate = -1257.4882
expansions: [(7, 1), (27, 1), (35, 1), (83, 4), (90, 1), (92, 1), (93, 1), (95, 1), (119, 1), (120, 8), (121, 2), (122, 1), (123, 1), (130, 1), (131, 1), (133, 3), (134, 2), (137, 1), (138, 1), (140, 1), (143, 3), (144, 9), (145, 1), (147, 1), (148, 2), (155, 2), (158, 1), (160, 2), (161, 2), (172, 5)]
discards: [  0 150 151 152 162 163 164 165 166 167 168 169 170 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1281.1292 - loglik: -1.2782e+03 - logprior: -2.8947e+00
Epoch 2/2
39/39 - 19s - loss: 1268.5695 - loglik: -1.2675e+03 - logprior: -1.0951e+00
Fitted a model with MAP estimate = -1264.8052
expansions: [(212, 1), (213, 1), (214, 1), (215, 2), (222, 4), (223, 19), (224, 6)]
discards: [  0 133 134 135 136 160 183 184 185 200 202 210]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1263.0831 - loglik: -1.2605e+03 - logprior: -2.6161e+00
Epoch 2/2
39/39 - 21s - loss: 1250.0958 - loglik: -1.2493e+03 - logprior: -7.9648e-01
Fitted a model with MAP estimate = -1247.2077
expansions: [(0, 2), (191, 1), (192, 1), (246, 5)]
discards: [  0 176 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234
 235 236 237 238 239 240 241 242 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1263.9293 - loglik: -1.2623e+03 - logprior: -1.6611e+00
Epoch 2/10
39/39 - 19s - loss: 1260.5813 - loglik: -1.2601e+03 - logprior: -4.3753e-01
Epoch 3/10
39/39 - 19s - loss: 1258.9569 - loglik: -1.2586e+03 - logprior: -3.9897e-01
Epoch 4/10
39/39 - 19s - loss: 1257.8284 - loglik: -1.2574e+03 - logprior: -4.2527e-01
Epoch 5/10
39/39 - 19s - loss: 1256.5741 - loglik: -1.2561e+03 - logprior: -4.5804e-01
Epoch 6/10
39/39 - 19s - loss: 1255.7292 - loglik: -1.2552e+03 - logprior: -5.5907e-01
Epoch 7/10
39/39 - 19s - loss: 1254.6716 - loglik: -1.2540e+03 - logprior: -6.7100e-01
Epoch 8/10
39/39 - 19s - loss: 1255.6606 - loglik: -1.2550e+03 - logprior: -7.1035e-01
Fitted a model with MAP estimate = -1254.8606
Time for alignment: 447.2949
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1360.6790 - loglik: -1.3583e+03 - logprior: -2.3348e+00
Epoch 2/10
39/39 - 18s - loss: 1270.1941 - loglik: -1.2687e+03 - logprior: -1.5003e+00
Epoch 3/10
39/39 - 18s - loss: 1262.4366 - loglik: -1.2608e+03 - logprior: -1.5947e+00
Epoch 4/10
39/39 - 18s - loss: 1260.5416 - loglik: -1.2589e+03 - logprior: -1.6851e+00
Epoch 5/10
39/39 - 18s - loss: 1259.1510 - loglik: -1.2573e+03 - logprior: -1.8148e+00
Epoch 6/10
39/39 - 18s - loss: 1258.3440 - loglik: -1.2564e+03 - logprior: -1.9569e+00
Epoch 7/10
39/39 - 18s - loss: 1257.5081 - loglik: -1.2554e+03 - logprior: -2.1032e+00
Epoch 8/10
39/39 - 18s - loss: 1256.9891 - loglik: -1.2548e+03 - logprior: -2.2015e+00
Epoch 9/10
39/39 - 18s - loss: 1257.6573 - loglik: -1.2554e+03 - logprior: -2.2984e+00
Fitted a model with MAP estimate = -1256.0958
expansions: [(4, 1), (6, 1), (33, 1), (52, 1), (82, 7), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (114, 1), (115, 8), (116, 2), (117, 1), (118, 2), (126, 2), (128, 3), (129, 2), (132, 1), (136, 1), (137, 1), (138, 3), (139, 9), (145, 1), (150, 1), (153, 2), (156, 1), (159, 6), (160, 1), (173, 1), (184, 1)]
discards: [  0 161 162 163 164 165 166 167 168 169 170 175 186 187 188 189 190 191
 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209
 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1273.2235 - loglik: -1.2703e+03 - logprior: -2.8736e+00
Epoch 2/2
39/39 - 22s - loss: 1261.1576 - loglik: -1.2600e+03 - logprior: -1.1968e+00
Fitted a model with MAP estimate = -1258.1143
expansions: [(4, 1), (87, 2), (89, 2), (226, 1), (240, 21)]
discards: [  0   1  81  82  83  84 132 133 134 135 136 161 183 184 185]
Re-initialized the encoder parameters.
Fitting a model of length 252 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1259.1733 - loglik: -1.2565e+03 - logprior: -2.6392e+00
Epoch 2/2
39/39 - 23s - loss: 1249.8225 - loglik: -1.2490e+03 - logprior: -8.3154e-01
Fitted a model with MAP estimate = -1246.9124
expansions: [(0, 2), (86, 2), (230, 4), (252, 3)]
discards: [  0   1  81  82  83  84 191 193 232 233 234 235 236 237 238 239 240 241
 242 243 244 245 246 247 248 249 250 251]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1262.0597 - loglik: -1.2604e+03 - logprior: -1.6716e+00
Epoch 2/10
39/39 - 21s - loss: 1255.8878 - loglik: -1.2554e+03 - logprior: -4.4961e-01
Epoch 3/10
39/39 - 21s - loss: 1254.3568 - loglik: -1.2540e+03 - logprior: -3.9104e-01
Epoch 4/10
39/39 - 21s - loss: 1253.6030 - loglik: -1.2532e+03 - logprior: -4.0819e-01
Epoch 5/10
39/39 - 21s - loss: 1251.6705 - loglik: -1.2512e+03 - logprior: -4.3529e-01
Epoch 6/10
39/39 - 21s - loss: 1251.8096 - loglik: -1.2513e+03 - logprior: -5.0989e-01
Fitted a model with MAP estimate = -1251.0265
Time for alignment: 495.0115
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1361.2986 - loglik: -1.3590e+03 - logprior: -2.3474e+00
Epoch 2/10
39/39 - 19s - loss: 1270.2158 - loglik: -1.2686e+03 - logprior: -1.5740e+00
Epoch 3/10
39/39 - 19s - loss: 1262.3066 - loglik: -1.2606e+03 - logprior: -1.6876e+00
Epoch 4/10
39/39 - 19s - loss: 1260.1802 - loglik: -1.2584e+03 - logprior: -1.8108e+00
Epoch 5/10
39/39 - 19s - loss: 1259.3807 - loglik: -1.2575e+03 - logprior: -1.9266e+00
Epoch 6/10
39/39 - 19s - loss: 1258.5393 - loglik: -1.2565e+03 - logprior: -2.0842e+00
Epoch 7/10
39/39 - 19s - loss: 1258.3514 - loglik: -1.2561e+03 - logprior: -2.2492e+00
Epoch 8/10
39/39 - 19s - loss: 1257.8843 - loglik: -1.2555e+03 - logprior: -2.3872e+00
Epoch 9/10
39/39 - 19s - loss: 1258.1340 - loglik: -1.2556e+03 - logprior: -2.4979e+00
Fitted a model with MAP estimate = -1256.7548
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (59, 1), (81, 6), (82, 1), (89, 3), (91, 2), (115, 1), (116, 8), (117, 2), (118, 1), (119, 2), (128, 1), (130, 2), (131, 2), (134, 1), (135, 1), (141, 3), (142, 8), (143, 1), (145, 1), (147, 1), (151, 2), (154, 1), (157, 1), (160, 1), (161, 3), (162, 1), (174, 1), (185, 1), (186, 1)]
discards: [  0 163 164 165 166 167 168 169 170 171 187 188 189 190 191 192 193 194
 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1273.3744 - loglik: -1.2704e+03 - logprior: -2.9523e+00
Epoch 2/2
39/39 - 23s - loss: 1260.8641 - loglik: -1.2595e+03 - logprior: -1.3150e+00
Fitted a model with MAP estimate = -1258.3268
expansions: [(4, 1), (216, 1), (241, 22)]
discards: [  0   1 134 135 136 137 138 184 185 186 202 221 222 223 224]
Re-initialized the encoder parameters.
Fitting a model of length 250 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1259.4653 - loglik: -1.2568e+03 - logprior: -2.6447e+00
Epoch 2/2
39/39 - 24s - loss: 1249.9485 - loglik: -1.2491e+03 - logprior: -8.9227e-01
Fitted a model with MAP estimate = -1246.7348
expansions: [(0, 2), (212, 9), (213, 1), (227, 1), (250, 3)]
discards: [  0  85  86 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245
 246 247 248 249]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1257.9391 - loglik: -1.2562e+03 - logprior: -1.7252e+00
Epoch 2/10
39/39 - 23s - loss: 1250.6119 - loglik: -1.2502e+03 - logprior: -3.9350e-01
Epoch 3/10
39/39 - 23s - loss: 1249.6237 - loglik: -1.2493e+03 - logprior: -3.3937e-01
Epoch 4/10
39/39 - 22s - loss: 1248.2170 - loglik: -1.2478e+03 - logprior: -3.7352e-01
Epoch 5/10
39/39 - 22s - loss: 1246.4031 - loglik: -1.2460e+03 - logprior: -4.1103e-01
Epoch 6/10
39/39 - 21s - loss: 1248.0042 - loglik: -1.2475e+03 - logprior: -4.8580e-01
Fitted a model with MAP estimate = -1246.0000
Time for alignment: 512.5997
Computed alignments with likelihoods: ['-1247.6670', '-1251.8015', '-1247.2077', '-1246.9124', '-1246.0000']
Best model has likelihood: -1246.0000  (prior= -0.5104 )
time for generating output: 0.3093
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.9224366250472947
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff14282f5e0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff118b9aca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe950324760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef9c199730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9503367f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11510feb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11510f970>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff11510fd60>, <__main__.SimpleDirichletPrior object at 0x7feefcc0c5e0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 563.9603 - loglik: -5.6048e+02 - logprior: -3.4757e+00
Epoch 2/10
19/19 - 3s - loss: 495.3268 - loglik: -4.9395e+02 - logprior: -1.3775e+00
Epoch 3/10
19/19 - 3s - loss: 472.5665 - loglik: -4.7088e+02 - logprior: -1.6887e+00
Epoch 4/10
19/19 - 3s - loss: 468.0568 - loglik: -4.6640e+02 - logprior: -1.6605e+00
Epoch 5/10
19/19 - 3s - loss: 466.6468 - loglik: -4.6503e+02 - logprior: -1.6124e+00
Epoch 6/10
19/19 - 3s - loss: 465.5735 - loglik: -4.6397e+02 - logprior: -1.6048e+00
Epoch 7/10
19/19 - 3s - loss: 466.1846 - loglik: -4.6459e+02 - logprior: -1.5967e+00
Fitted a model with MAP estimate = -465.2439
expansions: [(5, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.8570 - loglik: -4.5980e+02 - logprior: -4.0612e+00
Epoch 2/2
19/19 - 3s - loss: 451.4565 - loglik: -4.4948e+02 - logprior: -1.9787e+00
Fitted a model with MAP estimate = -448.9828
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 451.6824 - loglik: -4.4874e+02 - logprior: -2.9434e+00
Epoch 2/2
19/19 - 3s - loss: 447.0820 - loglik: -4.4595e+02 - logprior: -1.1352e+00
Fitted a model with MAP estimate = -446.1575
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 453.1570 - loglik: -4.4930e+02 - logprior: -3.8544e+00
Epoch 2/10
19/19 - 3s - loss: 448.4293 - loglik: -4.4700e+02 - logprior: -1.4300e+00
Epoch 3/10
19/19 - 3s - loss: 446.6034 - loglik: -4.4562e+02 - logprior: -9.8793e-01
Epoch 4/10
19/19 - 3s - loss: 445.8270 - loglik: -4.4486e+02 - logprior: -9.6331e-01
Epoch 5/10
19/19 - 3s - loss: 445.4984 - loglik: -4.4455e+02 - logprior: -9.4368e-01
Epoch 6/10
19/19 - 3s - loss: 444.5192 - loglik: -4.4357e+02 - logprior: -9.4470e-01
Epoch 7/10
19/19 - 3s - loss: 445.0904 - loglik: -4.4414e+02 - logprior: -9.5503e-01
Fitted a model with MAP estimate = -444.5434
Time for alignment: 87.2699
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 564.3726 - loglik: -5.6090e+02 - logprior: -3.4761e+00
Epoch 2/10
19/19 - 3s - loss: 495.4107 - loglik: -4.9402e+02 - logprior: -1.3954e+00
Epoch 3/10
19/19 - 3s - loss: 471.2532 - loglik: -4.6952e+02 - logprior: -1.7345e+00
Epoch 4/10
19/19 - 3s - loss: 467.3809 - loglik: -4.6572e+02 - logprior: -1.6560e+00
Epoch 5/10
19/19 - 3s - loss: 466.3138 - loglik: -4.6471e+02 - logprior: -1.6087e+00
Epoch 6/10
19/19 - 3s - loss: 466.0561 - loglik: -4.6446e+02 - logprior: -1.5970e+00
Epoch 7/10
19/19 - 3s - loss: 465.1541 - loglik: -4.6356e+02 - logprior: -1.5974e+00
Epoch 8/10
19/19 - 3s - loss: 464.8586 - loglik: -4.6325e+02 - logprior: -1.6050e+00
Epoch 9/10
19/19 - 3s - loss: 464.4681 - loglik: -4.6285e+02 - logprior: -1.6165e+00
Epoch 10/10
19/19 - 3s - loss: 464.5949 - loglik: -4.6296e+02 - logprior: -1.6323e+00
Fitted a model with MAP estimate = -464.3808
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.6647 - loglik: -4.5957e+02 - logprior: -4.0948e+00
Epoch 2/2
19/19 - 3s - loss: 451.2284 - loglik: -4.4921e+02 - logprior: -2.0155e+00
Fitted a model with MAP estimate = -449.3884
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 451.7439 - loglik: -4.4880e+02 - logprior: -2.9428e+00
Epoch 2/2
19/19 - 3s - loss: 447.3875 - loglik: -4.4625e+02 - logprior: -1.1395e+00
Fitted a model with MAP estimate = -446.1943
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 453.1973 - loglik: -4.4931e+02 - logprior: -3.8835e+00
Epoch 2/10
19/19 - 3s - loss: 448.2757 - loglik: -4.4682e+02 - logprior: -1.4548e+00
Epoch 3/10
19/19 - 3s - loss: 446.6727 - loglik: -4.4568e+02 - logprior: -9.9057e-01
Epoch 4/10
19/19 - 3s - loss: 446.2141 - loglik: -4.4526e+02 - logprior: -9.5820e-01
Epoch 5/10
19/19 - 3s - loss: 444.6357 - loglik: -4.4369e+02 - logprior: -9.4608e-01
Epoch 6/10
19/19 - 3s - loss: 445.4062 - loglik: -4.4447e+02 - logprior: -9.3648e-01
Fitted a model with MAP estimate = -444.7203
Time for alignment: 91.7013
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 564.4521 - loglik: -5.6097e+02 - logprior: -3.4791e+00
Epoch 2/10
19/19 - 3s - loss: 495.3359 - loglik: -4.9396e+02 - logprior: -1.3801e+00
Epoch 3/10
19/19 - 3s - loss: 471.3100 - loglik: -4.6960e+02 - logprior: -1.7070e+00
Epoch 4/10
19/19 - 3s - loss: 467.7404 - loglik: -4.6606e+02 - logprior: -1.6768e+00
Epoch 5/10
19/19 - 3s - loss: 466.3061 - loglik: -4.6471e+02 - logprior: -1.5990e+00
Epoch 6/10
19/19 - 3s - loss: 465.5246 - loglik: -4.6394e+02 - logprior: -1.5883e+00
Epoch 7/10
19/19 - 3s - loss: 465.1746 - loglik: -4.6359e+02 - logprior: -1.5893e+00
Epoch 8/10
19/19 - 3s - loss: 464.6080 - loglik: -4.6298e+02 - logprior: -1.6318e+00
Epoch 9/10
19/19 - 3s - loss: 464.1166 - loglik: -4.6246e+02 - logprior: -1.6528e+00
Epoch 10/10
19/19 - 3s - loss: 463.9296 - loglik: -4.6225e+02 - logprior: -1.6796e+00
Fitted a model with MAP estimate = -463.8049
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 2), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.9560 - loglik: -4.5985e+02 - logprior: -4.1047e+00
Epoch 2/2
19/19 - 3s - loss: 450.8759 - loglik: -4.4885e+02 - logprior: -2.0239e+00
Fitted a model with MAP estimate = -448.8519
expansions: [(0, 2)]
discards: [ 0 42 60 63]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 450.8008 - loglik: -4.4786e+02 - logprior: -2.9443e+00
Epoch 2/2
19/19 - 3s - loss: 446.6631 - loglik: -4.4554e+02 - logprior: -1.1220e+00
Fitted a model with MAP estimate = -445.6105
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 452.9569 - loglik: -4.4912e+02 - logprior: -3.8407e+00
Epoch 2/10
19/19 - 3s - loss: 448.0153 - loglik: -4.4658e+02 - logprior: -1.4400e+00
Epoch 3/10
19/19 - 3s - loss: 446.7530 - loglik: -4.4578e+02 - logprior: -9.7076e-01
Epoch 4/10
19/19 - 3s - loss: 445.9000 - loglik: -4.4494e+02 - logprior: -9.5507e-01
Epoch 5/10
19/19 - 3s - loss: 444.9200 - loglik: -4.4399e+02 - logprior: -9.3130e-01
Epoch 6/10
19/19 - 3s - loss: 444.6450 - loglik: -4.4372e+02 - logprior: -9.2326e-01
Epoch 7/10
19/19 - 3s - loss: 444.4744 - loglik: -4.4353e+02 - logprior: -9.4018e-01
Epoch 8/10
19/19 - 3s - loss: 444.2439 - loglik: -4.4328e+02 - logprior: -9.6459e-01
Epoch 9/10
19/19 - 3s - loss: 444.2072 - loglik: -4.4326e+02 - logprior: -9.5132e-01
Epoch 10/10
19/19 - 3s - loss: 444.3505 - loglik: -4.4338e+02 - logprior: -9.6784e-01
Fitted a model with MAP estimate = -444.0494
Time for alignment: 103.4170
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 564.4213 - loglik: -5.6094e+02 - logprior: -3.4765e+00
Epoch 2/10
19/19 - 3s - loss: 494.5763 - loglik: -4.9320e+02 - logprior: -1.3770e+00
Epoch 3/10
19/19 - 3s - loss: 471.6617 - loglik: -4.6996e+02 - logprior: -1.7027e+00
Epoch 4/10
19/19 - 3s - loss: 466.3834 - loglik: -4.6469e+02 - logprior: -1.6947e+00
Epoch 5/10
19/19 - 3s - loss: 465.9877 - loglik: -4.6437e+02 - logprior: -1.6184e+00
Epoch 6/10
19/19 - 3s - loss: 464.6282 - loglik: -4.6301e+02 - logprior: -1.6135e+00
Epoch 7/10
19/19 - 3s - loss: 464.4623 - loglik: -4.6286e+02 - logprior: -1.6056e+00
Epoch 8/10
19/19 - 3s - loss: 464.4400 - loglik: -4.6281e+02 - logprior: -1.6286e+00
Epoch 9/10
19/19 - 3s - loss: 463.9735 - loglik: -4.6233e+02 - logprior: -1.6397e+00
Epoch 10/10
19/19 - 3s - loss: 463.4451 - loglik: -4.6179e+02 - logprior: -1.6548e+00
Fitted a model with MAP estimate = -463.6028
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (14, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 459.3871 - loglik: -4.5628e+02 - logprior: -3.1117e+00
Epoch 2/2
19/19 - 3s - loss: 448.0094 - loglik: -4.4679e+02 - logprior: -1.2183e+00
Fitted a model with MAP estimate = -446.7419
expansions: []
discards: [61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 450.1226 - loglik: -4.4719e+02 - logprior: -2.9344e+00
Epoch 2/2
19/19 - 3s - loss: 447.5247 - loglik: -4.4639e+02 - logprior: -1.1339e+00
Fitted a model with MAP estimate = -446.4634
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 453.3359 - loglik: -4.4937e+02 - logprior: -3.9676e+00
Epoch 2/10
19/19 - 3s - loss: 449.0653 - loglik: -4.4687e+02 - logprior: -2.1918e+00
Epoch 3/10
19/19 - 3s - loss: 447.8006 - loglik: -4.4578e+02 - logprior: -2.0231e+00
Epoch 4/10
19/19 - 3s - loss: 447.2092 - loglik: -4.4558e+02 - logprior: -1.6327e+00
Epoch 5/10
19/19 - 3s - loss: 445.6750 - loglik: -4.4466e+02 - logprior: -1.0139e+00
Epoch 6/10
19/19 - 3s - loss: 445.6539 - loglik: -4.4464e+02 - logprior: -1.0090e+00
Epoch 7/10
19/19 - 3s - loss: 445.3932 - loglik: -4.4439e+02 - logprior: -9.9959e-01
Epoch 8/10
19/19 - 3s - loss: 445.1395 - loglik: -4.4412e+02 - logprior: -1.0198e+00
Epoch 9/10
19/19 - 3s - loss: 445.3477 - loglik: -4.4433e+02 - logprior: -1.0159e+00
Fitted a model with MAP estimate = -444.9616
Time for alignment: 102.1154
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 564.5997 - loglik: -5.6112e+02 - logprior: -3.4752e+00
Epoch 2/10
19/19 - 3s - loss: 495.4221 - loglik: -4.9405e+02 - logprior: -1.3759e+00
Epoch 3/10
19/19 - 3s - loss: 472.2608 - loglik: -4.7056e+02 - logprior: -1.7049e+00
Epoch 4/10
19/19 - 3s - loss: 467.5455 - loglik: -4.6587e+02 - logprior: -1.6792e+00
Epoch 5/10
19/19 - 3s - loss: 466.7088 - loglik: -4.6510e+02 - logprior: -1.6075e+00
Epoch 6/10
19/19 - 3s - loss: 464.8388 - loglik: -4.6325e+02 - logprior: -1.5901e+00
Epoch 7/10
19/19 - 3s - loss: 464.9672 - loglik: -4.6338e+02 - logprior: -1.5827e+00
Fitted a model with MAP estimate = -464.4875
expansions: [(6, 2), (7, 1), (8, 1), (9, 1), (14, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.3087 - loglik: -4.5924e+02 - logprior: -4.0715e+00
Epoch 2/2
19/19 - 3s - loss: 450.2369 - loglik: -4.4816e+02 - logprior: -2.0784e+00
Fitted a model with MAP estimate = -448.0021
expansions: [(2, 1)]
discards: [ 0  7 43 61]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 453.4311 - loglik: -4.4961e+02 - logprior: -3.8189e+00
Epoch 2/2
19/19 - 3s - loss: 448.4371 - loglik: -4.4711e+02 - logprior: -1.3228e+00
Fitted a model with MAP estimate = -447.3029
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 453.1777 - loglik: -4.4903e+02 - logprior: -4.1508e+00
Epoch 2/10
19/19 - 3s - loss: 448.2236 - loglik: -4.4653e+02 - logprior: -1.6943e+00
Epoch 3/10
19/19 - 3s - loss: 446.6463 - loglik: -4.4551e+02 - logprior: -1.1392e+00
Epoch 4/10
19/19 - 3s - loss: 446.5841 - loglik: -4.4555e+02 - logprior: -1.0375e+00
Epoch 5/10
19/19 - 3s - loss: 446.3685 - loglik: -4.4537e+02 - logprior: -9.9932e-01
Epoch 6/10
19/19 - 3s - loss: 445.5208 - loglik: -4.4452e+02 - logprior: -1.0001e+00
Epoch 7/10
19/19 - 3s - loss: 444.9628 - loglik: -4.4395e+02 - logprior: -1.0129e+00
Epoch 8/10
19/19 - 3s - loss: 445.2769 - loglik: -4.4427e+02 - logprior: -1.0094e+00
Fitted a model with MAP estimate = -444.9704
Time for alignment: 89.0729
Computed alignments with likelihoods: ['-444.5434', '-444.7203', '-444.0494', '-444.9616', '-444.9704']
Best model has likelihood: -444.0494  (prior= -0.9656 )
time for generating output: 0.1479
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9112056737588653
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9204cadf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff115811e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9204c5fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe93165d850>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1159818b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113eab070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9203aa310>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c49c54f0>, <__main__.SimpleDirichletPrior object at 0x7feefda6f190>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 475.2447 - loglik: -4.7184e+02 - logprior: -3.4046e+00
Epoch 2/10
19/19 - 2s - loss: 439.2529 - loglik: -4.3794e+02 - logprior: -1.3103e+00
Epoch 3/10
19/19 - 2s - loss: 428.2408 - loglik: -4.2695e+02 - logprior: -1.2945e+00
Epoch 4/10
19/19 - 2s - loss: 425.1973 - loglik: -4.2386e+02 - logprior: -1.3355e+00
Epoch 5/10
19/19 - 2s - loss: 423.9619 - loglik: -4.2262e+02 - logprior: -1.3447e+00
Epoch 6/10
19/19 - 2s - loss: 423.4109 - loglik: -4.2207e+02 - logprior: -1.3449e+00
Epoch 7/10
19/19 - 2s - loss: 422.8235 - loglik: -4.2144e+02 - logprior: -1.3853e+00
Epoch 8/10
19/19 - 2s - loss: 423.2271 - loglik: -4.2182e+02 - logprior: -1.4064e+00
Fitted a model with MAP estimate = -422.4469
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (30, 2), (31, 1), (34, 1), (35, 2), (37, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 426.6768 - loglik: -4.2259e+02 - logprior: -4.0830e+00
Epoch 2/2
19/19 - 2s - loss: 419.0771 - loglik: -4.1700e+02 - logprior: -2.0794e+00
Fitted a model with MAP estimate = -417.2756
expansions: [(0, 2)]
discards: [ 0 38 44 73]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 419.8101 - loglik: -4.1689e+02 - logprior: -2.9161e+00
Epoch 2/2
19/19 - 2s - loss: 416.4504 - loglik: -4.1530e+02 - logprior: -1.1472e+00
Fitted a model with MAP estimate = -415.7869
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 420.6049 - loglik: -4.1696e+02 - logprior: -3.6459e+00
Epoch 2/10
19/19 - 2s - loss: 416.6541 - loglik: -4.1547e+02 - logprior: -1.1855e+00
Epoch 3/10
19/19 - 2s - loss: 415.5231 - loglik: -4.1452e+02 - logprior: -1.0020e+00
Epoch 4/10
19/19 - 2s - loss: 415.4858 - loglik: -4.1451e+02 - logprior: -9.7665e-01
Epoch 5/10
19/19 - 2s - loss: 414.1668 - loglik: -4.1320e+02 - logprior: -9.6669e-01
Epoch 6/10
19/19 - 2s - loss: 413.8126 - loglik: -4.1283e+02 - logprior: -9.8107e-01
Epoch 7/10
19/19 - 2s - loss: 413.7393 - loglik: -4.1276e+02 - logprior: -9.8302e-01
Epoch 8/10
19/19 - 2s - loss: 413.3441 - loglik: -4.1233e+02 - logprior: -1.0099e+00
Epoch 9/10
19/19 - 2s - loss: 413.4268 - loglik: -4.1240e+02 - logprior: -1.0255e+00
Fitted a model with MAP estimate = -413.0863
Time for alignment: 66.7035
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 475.3392 - loglik: -4.7194e+02 - logprior: -3.4012e+00
Epoch 2/10
19/19 - 2s - loss: 439.2966 - loglik: -4.3799e+02 - logprior: -1.3020e+00
Epoch 3/10
19/19 - 2s - loss: 427.5399 - loglik: -4.2622e+02 - logprior: -1.3248e+00
Epoch 4/10
19/19 - 2s - loss: 424.9098 - loglik: -4.2357e+02 - logprior: -1.3372e+00
Epoch 5/10
19/19 - 2s - loss: 423.8257 - loglik: -4.2249e+02 - logprior: -1.3386e+00
Epoch 6/10
19/19 - 2s - loss: 423.1589 - loglik: -4.2182e+02 - logprior: -1.3396e+00
Epoch 7/10
19/19 - 2s - loss: 422.3931 - loglik: -4.2102e+02 - logprior: -1.3691e+00
Epoch 8/10
19/19 - 2s - loss: 422.6566 - loglik: -4.2127e+02 - logprior: -1.3882e+00
Fitted a model with MAP estimate = -422.2497
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (30, 2), (31, 1), (32, 1), (33, 2), (34, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 426.4636 - loglik: -4.2239e+02 - logprior: -4.0785e+00
Epoch 2/2
19/19 - 2s - loss: 419.2507 - loglik: -4.1718e+02 - logprior: -2.0751e+00
Fitted a model with MAP estimate = -417.3246
expansions: [(0, 2)]
discards: [ 0 38 44 73]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 419.7867 - loglik: -4.1687e+02 - logprior: -2.9172e+00
Epoch 2/2
19/19 - 2s - loss: 416.4956 - loglik: -4.1535e+02 - logprior: -1.1502e+00
Fitted a model with MAP estimate = -415.7846
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 420.5234 - loglik: -4.1686e+02 - logprior: -3.6647e+00
Epoch 2/10
19/19 - 2s - loss: 416.6975 - loglik: -4.1551e+02 - logprior: -1.1892e+00
Epoch 3/10
19/19 - 2s - loss: 415.5775 - loglik: -4.1457e+02 - logprior: -1.0032e+00
Epoch 4/10
19/19 - 2s - loss: 415.5364 - loglik: -4.1456e+02 - logprior: -9.7941e-01
Epoch 5/10
19/19 - 2s - loss: 414.2796 - loglik: -4.1331e+02 - logprior: -9.6796e-01
Epoch 6/10
19/19 - 2s - loss: 413.9019 - loglik: -4.1291e+02 - logprior: -9.8967e-01
Epoch 7/10
19/19 - 2s - loss: 413.7796 - loglik: -4.1278e+02 - logprior: -9.9524e-01
Epoch 8/10
19/19 - 2s - loss: 413.4534 - loglik: -4.1244e+02 - logprior: -1.0111e+00
Epoch 9/10
19/19 - 2s - loss: 412.8613 - loglik: -4.1184e+02 - logprior: -1.0259e+00
Epoch 10/10
19/19 - 2s - loss: 413.2964 - loglik: -4.1227e+02 - logprior: -1.0304e+00
Fitted a model with MAP estimate = -413.0235
Time for alignment: 67.7549
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 475.2386 - loglik: -4.7183e+02 - logprior: -3.4040e+00
Epoch 2/10
19/19 - 2s - loss: 439.6973 - loglik: -4.3839e+02 - logprior: -1.3118e+00
Epoch 3/10
19/19 - 2s - loss: 427.7858 - loglik: -4.2648e+02 - logprior: -1.3072e+00
Epoch 4/10
19/19 - 2s - loss: 424.8987 - loglik: -4.2355e+02 - logprior: -1.3534e+00
Epoch 5/10
19/19 - 2s - loss: 423.8267 - loglik: -4.2247e+02 - logprior: -1.3521e+00
Epoch 6/10
19/19 - 2s - loss: 423.1230 - loglik: -4.2178e+02 - logprior: -1.3462e+00
Epoch 7/10
19/19 - 2s - loss: 422.9272 - loglik: -4.2156e+02 - logprior: -1.3697e+00
Epoch 8/10
19/19 - 2s - loss: 422.9237 - loglik: -4.2154e+02 - logprior: -1.3887e+00
Epoch 9/10
19/19 - 2s - loss: 421.7934 - loglik: -4.2039e+02 - logprior: -1.4028e+00
Epoch 10/10
19/19 - 2s - loss: 422.6360 - loglik: -4.2121e+02 - logprior: -1.4218e+00
Fitted a model with MAP estimate = -422.1760
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.0976 - loglik: -4.2203e+02 - logprior: -4.0633e+00
Epoch 2/2
19/19 - 2s - loss: 419.0176 - loglik: -4.1698e+02 - logprior: -2.0340e+00
Fitted a model with MAP estimate = -417.2885
expansions: [(0, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 419.7273 - loglik: -4.1681e+02 - logprior: -2.9138e+00
Epoch 2/2
19/19 - 2s - loss: 416.3429 - loglik: -4.1519e+02 - logprior: -1.1503e+00
Fitted a model with MAP estimate = -415.8060
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 420.7910 - loglik: -4.1713e+02 - logprior: -3.6614e+00
Epoch 2/10
19/19 - 2s - loss: 417.1583 - loglik: -4.1598e+02 - logprior: -1.1812e+00
Epoch 3/10
19/19 - 2s - loss: 415.8755 - loglik: -4.1486e+02 - logprior: -1.0154e+00
Epoch 4/10
19/19 - 2s - loss: 415.2709 - loglik: -4.1430e+02 - logprior: -9.7423e-01
Epoch 5/10
19/19 - 2s - loss: 414.8665 - loglik: -4.1390e+02 - logprior: -9.6750e-01
Epoch 6/10
19/19 - 2s - loss: 414.0739 - loglik: -4.1309e+02 - logprior: -9.8482e-01
Epoch 7/10
19/19 - 2s - loss: 413.9335 - loglik: -4.1295e+02 - logprior: -9.8244e-01
Epoch 8/10
19/19 - 2s - loss: 413.7729 - loglik: -4.1276e+02 - logprior: -1.0146e+00
Epoch 9/10
19/19 - 2s - loss: 413.5378 - loglik: -4.1252e+02 - logprior: -1.0216e+00
Epoch 10/10
19/19 - 2s - loss: 413.4079 - loglik: -4.1239e+02 - logprior: -1.0192e+00
Fitted a model with MAP estimate = -413.3428
Time for alignment: 70.6769
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 475.2399 - loglik: -4.7184e+02 - logprior: -3.4023e+00
Epoch 2/10
19/19 - 2s - loss: 438.4590 - loglik: -4.3716e+02 - logprior: -1.2972e+00
Epoch 3/10
19/19 - 2s - loss: 427.6690 - loglik: -4.2636e+02 - logprior: -1.3110e+00
Epoch 4/10
19/19 - 2s - loss: 425.0847 - loglik: -4.2375e+02 - logprior: -1.3355e+00
Epoch 5/10
19/19 - 2s - loss: 423.9157 - loglik: -4.2257e+02 - logprior: -1.3478e+00
Epoch 6/10
19/19 - 2s - loss: 423.6837 - loglik: -4.2235e+02 - logprior: -1.3343e+00
Epoch 7/10
19/19 - 2s - loss: 422.8542 - loglik: -4.2149e+02 - logprior: -1.3659e+00
Epoch 8/10
19/19 - 2s - loss: 422.7151 - loglik: -4.2133e+02 - logprior: -1.3891e+00
Epoch 9/10
19/19 - 2s - loss: 422.9431 - loglik: -4.2154e+02 - logprior: -1.3982e+00
Fitted a model with MAP estimate = -422.4301
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (34, 1), (35, 2), (37, 1), (46, 1), (48, 1), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.5292 - loglik: -4.2245e+02 - logprior: -4.0762e+00
Epoch 2/2
19/19 - 2s - loss: 419.4169 - loglik: -4.1736e+02 - logprior: -2.0594e+00
Fitted a model with MAP estimate = -418.0784
expansions: [(0, 2), (10, 1)]
discards: [ 0 44 72]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 420.5036 - loglik: -4.1759e+02 - logprior: -2.9172e+00
Epoch 2/2
19/19 - 2s - loss: 417.1788 - loglik: -4.1604e+02 - logprior: -1.1431e+00
Fitted a model with MAP estimate = -416.3815
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 421.0316 - loglik: -4.1738e+02 - logprior: -3.6499e+00
Epoch 2/10
19/19 - 2s - loss: 417.4249 - loglik: -4.1624e+02 - logprior: -1.1832e+00
Epoch 3/10
19/19 - 2s - loss: 416.1739 - loglik: -4.1518e+02 - logprior: -9.9342e-01
Epoch 4/10
19/19 - 2s - loss: 415.2068 - loglik: -4.1425e+02 - logprior: -9.6093e-01
Epoch 5/10
19/19 - 2s - loss: 414.6486 - loglik: -4.1370e+02 - logprior: -9.4768e-01
Epoch 6/10
19/19 - 2s - loss: 414.4732 - loglik: -4.1350e+02 - logprior: -9.7541e-01
Epoch 7/10
19/19 - 2s - loss: 414.2003 - loglik: -4.1322e+02 - logprior: -9.8189e-01
Epoch 8/10
19/19 - 2s - loss: 413.4398 - loglik: -4.1244e+02 - logprior: -9.9536e-01
Epoch 9/10
19/19 - 2s - loss: 413.3887 - loglik: -4.1238e+02 - logprior: -1.0063e+00
Epoch 10/10
19/19 - 2s - loss: 413.6197 - loglik: -4.1260e+02 - logprior: -1.0178e+00
Fitted a model with MAP estimate = -413.2342
Time for alignment: 69.5042
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 475.3413 - loglik: -4.7194e+02 - logprior: -3.4057e+00
Epoch 2/10
19/19 - 2s - loss: 439.7174 - loglik: -4.3842e+02 - logprior: -1.3014e+00
Epoch 3/10
19/19 - 2s - loss: 427.9361 - loglik: -4.2666e+02 - logprior: -1.2803e+00
Epoch 4/10
19/19 - 2s - loss: 425.2645 - loglik: -4.2392e+02 - logprior: -1.3468e+00
Epoch 5/10
19/19 - 2s - loss: 423.6288 - loglik: -4.2229e+02 - logprior: -1.3360e+00
Epoch 6/10
19/19 - 2s - loss: 423.1607 - loglik: -4.2183e+02 - logprior: -1.3265e+00
Epoch 7/10
19/19 - 2s - loss: 422.5614 - loglik: -4.2121e+02 - logprior: -1.3523e+00
Epoch 8/10
19/19 - 2s - loss: 422.9568 - loglik: -4.2158e+02 - logprior: -1.3721e+00
Fitted a model with MAP estimate = -422.2455
expansions: [(7, 2), (8, 2), (9, 2), (23, 1), (31, 1), (32, 1), (35, 2), (37, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.7442 - loglik: -4.2267e+02 - logprior: -4.0738e+00
Epoch 2/2
19/19 - 2s - loss: 419.4462 - loglik: -4.1742e+02 - logprior: -2.0291e+00
Fitted a model with MAP estimate = -418.0190
expansions: [(0, 2)]
discards: [ 0 63 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 420.8559 - loglik: -4.1792e+02 - logprior: -2.9345e+00
Epoch 2/2
19/19 - 2s - loss: 417.4568 - loglik: -4.1630e+02 - logprior: -1.1609e+00
Fitted a model with MAP estimate = -416.6807
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 421.4283 - loglik: -4.1770e+02 - logprior: -3.7241e+00
Epoch 2/10
19/19 - 2s - loss: 417.2826 - loglik: -4.1606e+02 - logprior: -1.2238e+00
Epoch 3/10
19/19 - 2s - loss: 416.1804 - loglik: -4.1519e+02 - logprior: -9.9459e-01
Epoch 4/10
19/19 - 2s - loss: 415.4079 - loglik: -4.1444e+02 - logprior: -9.6849e-01
Epoch 5/10
19/19 - 2s - loss: 414.1862 - loglik: -4.1321e+02 - logprior: -9.7525e-01
Epoch 6/10
19/19 - 2s - loss: 413.7843 - loglik: -4.1280e+02 - logprior: -9.7967e-01
Epoch 7/10
19/19 - 2s - loss: 413.9762 - loglik: -4.1297e+02 - logprior: -1.0061e+00
Fitted a model with MAP estimate = -413.4167
Time for alignment: 60.7951
Computed alignments with likelihoods: ['-413.0863', '-413.0235', '-413.3428', '-413.2342', '-413.4167']
Best model has likelihood: -413.0235  (prior= -1.0503 )
time for generating output: 0.1420
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8040816326530612
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff11868aa30>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941c56730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff142ba0d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef9c127b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff129603fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1186bf5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefd1d9340>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff11891e580>, <__main__.SimpleDirichletPrior object at 0x7ff2c59002e0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1323.2108 - loglik: -1.3210e+03 - logprior: -2.1783e+00
Epoch 2/10
39/39 - 17s - loss: 1273.1959 - loglik: -1.2721e+03 - logprior: -1.0604e+00
Epoch 3/10
39/39 - 18s - loss: 1267.3909 - loglik: -1.2663e+03 - logprior: -1.0574e+00
Epoch 4/10
39/39 - 18s - loss: 1263.9410 - loglik: -1.2629e+03 - logprior: -1.0866e+00
Epoch 5/10
39/39 - 18s - loss: 1259.5527 - loglik: -1.2583e+03 - logprior: -1.2538e+00
Epoch 6/10
39/39 - 18s - loss: 1255.7960 - loglik: -1.2543e+03 - logprior: -1.4905e+00
Epoch 7/10
39/39 - 18s - loss: 1252.0579 - loglik: -1.2503e+03 - logprior: -1.7435e+00
Epoch 8/10
39/39 - 18s - loss: 1250.2581 - loglik: -1.2483e+03 - logprior: -2.0020e+00
Epoch 9/10
39/39 - 18s - loss: 1246.8875 - loglik: -1.2447e+03 - logprior: -2.2273e+00
Epoch 10/10
39/39 - 18s - loss: 1244.4948 - loglik: -1.2421e+03 - logprior: -2.3806e+00
Fitted a model with MAP estimate = -1243.8555
expansions: [(21, 1), (22, 1), (24, 5), (27, 1), (47, 1), (48, 6), (59, 1), (61, 1), (62, 2), (64, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (104, 3), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (135, 1), (145, 1), (155, 8), (160, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1309.9824 - loglik: -1.3072e+03 - logprior: -2.7777e+00
Epoch 2/2
39/39 - 25s - loss: 1267.9264 - loglik: -1.2670e+03 - logprior: -8.8069e-01
Fitted a model with MAP estimate = -1262.6594
expansions: [(24, 1), (33, 1), (59, 1), (103, 1), (130, 1), (178, 1), (190, 1), (192, 2), (195, 1), (242, 10)]
discards: [ 25  26  30  34  61  62 101 131 132 181 182 183 184 185 186 187 188 198
 199 200 201 202 239 240 241]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1268.2006 - loglik: -1.2665e+03 - logprior: -1.6653e+00
Epoch 2/2
39/39 - 25s - loss: 1262.6836 - loglik: -1.2622e+03 - logprior: -4.6561e-01
Fitted a model with MAP estimate = -1260.2056
expansions: [(177, 1), (182, 2), (186, 1), (187, 1)]
discards: [ 53  95 228 229 230 231 232 233 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1266.2062 - loglik: -1.2648e+03 - logprior: -1.4120e+00
Epoch 2/10
39/39 - 25s - loss: 1262.2565 - loglik: -1.2622e+03 - logprior: -1.0538e-01
Epoch 3/10
39/39 - 25s - loss: 1259.1532 - loglik: -1.2591e+03 - logprior: -1.6693e-02
Epoch 4/10
39/39 - 24s - loss: 1255.8423 - loglik: -1.2559e+03 - logprior: 0.0374
Epoch 5/10
39/39 - 24s - loss: 1249.9633 - loglik: -1.2500e+03 - logprior: -1.1002e-02
Epoch 6/10
39/39 - 24s - loss: 1243.5087 - loglik: -1.2434e+03 - logprior: -9.1369e-02
Epoch 7/10
39/39 - 24s - loss: 1239.6208 - loglik: -1.2393e+03 - logprior: -2.8097e-01
Epoch 8/10
39/39 - 24s - loss: 1235.5642 - loglik: -1.2351e+03 - logprior: -5.1154e-01
Epoch 9/10
39/39 - 24s - loss: 1233.1901 - loglik: -1.2324e+03 - logprior: -7.4497e-01
Epoch 10/10
39/39 - 24s - loss: 1228.5807 - loglik: -1.2277e+03 - logprior: -8.7035e-01
Fitted a model with MAP estimate = -1228.1201
Time for alignment: 637.0975
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1323.6733 - loglik: -1.3215e+03 - logprior: -2.1756e+00
Epoch 2/10
39/39 - 19s - loss: 1274.6332 - loglik: -1.2735e+03 - logprior: -1.1159e+00
Epoch 3/10
39/39 - 19s - loss: 1268.4846 - loglik: -1.2673e+03 - logprior: -1.2075e+00
Epoch 4/10
39/39 - 19s - loss: 1264.4785 - loglik: -1.2632e+03 - logprior: -1.2474e+00
Epoch 5/10
39/39 - 19s - loss: 1260.0079 - loglik: -1.2586e+03 - logprior: -1.4147e+00
Epoch 6/10
39/39 - 20s - loss: 1255.9880 - loglik: -1.2543e+03 - logprior: -1.6512e+00
Epoch 7/10
39/39 - 19s - loss: 1253.3485 - loglik: -1.2515e+03 - logprior: -1.8917e+00
Epoch 8/10
39/39 - 19s - loss: 1250.7350 - loglik: -1.2486e+03 - logprior: -2.1353e+00
Epoch 9/10
39/39 - 19s - loss: 1247.3966 - loglik: -1.2451e+03 - logprior: -2.3363e+00
Epoch 10/10
39/39 - 19s - loss: 1246.2903 - loglik: -1.2438e+03 - logprior: -2.4970e+00
Fitted a model with MAP estimate = -1244.3952
expansions: [(21, 1), (22, 1), (23, 4), (26, 1), (28, 1), (47, 5), (50, 1), (54, 1), (63, 1), (64, 2), (66, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (97, 1), (105, 3), (106, 2), (109, 1), (110, 1), (121, 1), (124, 1), (127, 1), (134, 1), (138, 1), (139, 1), (147, 1), (156, 7), (161, 1), (163, 1), (165, 1), (166, 1), (167, 1), (169, 1), (174, 1), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1309.8490 - loglik: -1.3070e+03 - logprior: -2.8262e+00
Epoch 2/2
39/39 - 28s - loss: 1267.1921 - loglik: -1.2663e+03 - logprior: -8.8550e-01
Fitted a model with MAP estimate = -1262.2746
expansions: [(26, 1), (57, 1), (104, 1), (106, 1), (194, 2), (207, 1), (246, 10)]
discards: [ 28  59  60 107 108 132 133 186 187 188 189 201 202 203 204 208 241 242
 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1266.6732 - loglik: -1.2650e+03 - logprior: -1.6428e+00
Epoch 2/2
39/39 - 26s - loss: 1262.0056 - loglik: -1.2616e+03 - logprior: -4.1681e-01
Fitted a model with MAP estimate = -1259.1095
expansions: [(32, 1), (76, 1), (187, 1), (192, 1), (193, 1), (194, 2)]
discards: [ 26  27  54  98 220 236 237 238 239 240 241]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1265.4832 - loglik: -1.2641e+03 - logprior: -1.3844e+00
Epoch 2/10
39/39 - 25s - loss: 1261.4819 - loglik: -1.2614e+03 - logprior: -7.6151e-02
Epoch 3/10
39/39 - 25s - loss: 1259.2172 - loglik: -1.2593e+03 - logprior: 0.0399
Epoch 4/10
39/39 - 25s - loss: 1253.6710 - loglik: -1.2538e+03 - logprior: 0.0971
Epoch 5/10
39/39 - 26s - loss: 1248.6494 - loglik: -1.2487e+03 - logprior: 0.0573
Epoch 6/10
39/39 - 26s - loss: 1242.8586 - loglik: -1.2428e+03 - logprior: -7.5075e-02
Epoch 7/10
39/39 - 26s - loss: 1238.8491 - loglik: -1.2386e+03 - logprior: -2.9799e-01
Epoch 8/10
39/39 - 26s - loss: 1234.2341 - loglik: -1.2337e+03 - logprior: -5.5200e-01
Epoch 9/10
39/39 - 26s - loss: 1230.2079 - loglik: -1.2294e+03 - logprior: -7.6821e-01
Epoch 10/10
39/39 - 26s - loss: 1227.2295 - loglik: -1.2263e+03 - logprior: -9.2773e-01
Fitted a model with MAP estimate = -1226.4174
Time for alignment: 674.2648
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1323.1879 - loglik: -1.3210e+03 - logprior: -2.1831e+00
Epoch 2/10
39/39 - 19s - loss: 1274.9034 - loglik: -1.2738e+03 - logprior: -1.0979e+00
Epoch 3/10
39/39 - 19s - loss: 1268.3474 - loglik: -1.2672e+03 - logprior: -1.1102e+00
Epoch 4/10
39/39 - 19s - loss: 1264.5579 - loglik: -1.2634e+03 - logprior: -1.1507e+00
Epoch 5/10
39/39 - 19s - loss: 1259.8660 - loglik: -1.2586e+03 - logprior: -1.3124e+00
Epoch 6/10
39/39 - 19s - loss: 1255.6896 - loglik: -1.2541e+03 - logprior: -1.5583e+00
Epoch 7/10
39/39 - 19s - loss: 1253.7119 - loglik: -1.2519e+03 - logprior: -1.8073e+00
Epoch 8/10
39/39 - 19s - loss: 1249.7899 - loglik: -1.2477e+03 - logprior: -2.0530e+00
Epoch 9/10
39/39 - 19s - loss: 1247.1804 - loglik: -1.2449e+03 - logprior: -2.2719e+00
Epoch 10/10
39/39 - 19s - loss: 1245.0126 - loglik: -1.2426e+03 - logprior: -2.4272e+00
Fitted a model with MAP estimate = -1243.9622
expansions: [(22, 1), (24, 6), (28, 1), (47, 1), (48, 6), (59, 1), (62, 1), (65, 1), (68, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (85, 1), (103, 3), (104, 2), (107, 1), (108, 1), (122, 1), (123, 1), (125, 1), (139, 1), (145, 1), (155, 5), (160, 1), (162, 1), (166, 1), (167, 1), (168, 1), (169, 1), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1310.8685 - loglik: -1.3081e+03 - logprior: -2.7826e+00
Epoch 2/2
39/39 - 26s - loss: 1267.3575 - loglik: -1.2665e+03 - logprior: -8.7847e-01
Fitted a model with MAP estimate = -1262.0372
expansions: [(76, 1), (77, 1), (102, 1), (103, 1), (104, 1), (184, 1), (202, 1), (241, 10)]
discards: [ 30  61  62  78 100 105 106 130 131 177 186 187 197 203 222 236 237 238
 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1267.4926 - loglik: -1.2658e+03 - logprior: -1.6724e+00
Epoch 2/2
39/39 - 25s - loss: 1262.0605 - loglik: -1.2616e+03 - logprior: -4.4013e-01
Fitted a model with MAP estimate = -1259.4891
expansions: [(31, 1), (59, 1), (190, 1), (216, 1)]
discards: [ 26  27  54  74  95 232 233 234 235 236 237]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1265.8132 - loglik: -1.2644e+03 - logprior: -1.4042e+00
Epoch 2/10
39/39 - 24s - loss: 1262.3102 - loglik: -1.2622e+03 - logprior: -8.2947e-02
Epoch 3/10
39/39 - 25s - loss: 1259.7251 - loglik: -1.2597e+03 - logprior: 0.0111
Epoch 4/10
39/39 - 24s - loss: 1255.4050 - loglik: -1.2555e+03 - logprior: 0.0674
Epoch 5/10
39/39 - 24s - loss: 1249.8054 - loglik: -1.2498e+03 - logprior: 0.0142
Epoch 6/10
39/39 - 25s - loss: 1244.5790 - loglik: -1.2445e+03 - logprior: -1.0632e-01
Epoch 7/10
39/39 - 25s - loss: 1238.6838 - loglik: -1.2383e+03 - logprior: -3.4293e-01
Epoch 8/10
39/39 - 25s - loss: 1236.6326 - loglik: -1.2361e+03 - logprior: -5.8136e-01
Epoch 9/10
39/39 - 26s - loss: 1231.1152 - loglik: -1.2303e+03 - logprior: -7.9201e-01
Epoch 10/10
39/39 - 26s - loss: 1229.0736 - loglik: -1.2281e+03 - logprior: -9.2700e-01
Fitted a model with MAP estimate = -1228.1204
Time for alignment: 662.8772
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1323.4404 - loglik: -1.3213e+03 - logprior: -2.1797e+00
Epoch 2/10
39/39 - 18s - loss: 1274.0748 - loglik: -1.2730e+03 - logprior: -1.0983e+00
Epoch 3/10
39/39 - 18s - loss: 1267.9492 - loglik: -1.2668e+03 - logprior: -1.1172e+00
Epoch 4/10
39/39 - 18s - loss: 1264.0698 - loglik: -1.2629e+03 - logprior: -1.1440e+00
Epoch 5/10
39/39 - 17s - loss: 1259.5627 - loglik: -1.2583e+03 - logprior: -1.3038e+00
Epoch 6/10
39/39 - 17s - loss: 1256.1433 - loglik: -1.2546e+03 - logprior: -1.5505e+00
Epoch 7/10
39/39 - 17s - loss: 1252.5846 - loglik: -1.2508e+03 - logprior: -1.7799e+00
Epoch 8/10
39/39 - 17s - loss: 1250.1519 - loglik: -1.2481e+03 - logprior: -2.0343e+00
Epoch 9/10
39/39 - 17s - loss: 1247.3339 - loglik: -1.2451e+03 - logprior: -2.2351e+00
Epoch 10/10
39/39 - 17s - loss: 1245.9299 - loglik: -1.2435e+03 - logprior: -2.4027e+00
Fitted a model with MAP estimate = -1244.1009
expansions: [(22, 1), (24, 6), (28, 1), (47, 5), (48, 1), (53, 1), (59, 1), (61, 2), (62, 2), (64, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (104, 3), (105, 2), (107, 1), (108, 1), (119, 1), (122, 1), (125, 1), (132, 1), (138, 1), (140, 2), (160, 1), (162, 1), (166, 1), (167, 1), (168, 1), (169, 1), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1310.7982 - loglik: -1.3080e+03 - logprior: -2.7706e+00
Epoch 2/2
39/39 - 22s - loss: 1266.6091 - loglik: -1.2657e+03 - logprior: -8.7230e-01
Fitted a model with MAP estimate = -1262.2381
expansions: [(28, 1), (104, 1), (105, 1), (106, 1), (188, 2), (194, 2), (196, 2), (200, 1), (222, 1), (239, 9)]
discards: [ 30  58  59  60  61 102 107 108 133 180 181 189 190 201 219 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1266.7291 - loglik: -1.2650e+03 - logprior: -1.7001e+00
Epoch 2/2
39/39 - 23s - loss: 1261.8748 - loglik: -1.2614e+03 - logprior: -4.3230e-01
Fitted a model with MAP estimate = -1259.1083
expansions: [(32, 2), (52, 1), (58, 1)]
discards: [ 26  27  28  75  95 234 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1266.0516 - loglik: -1.2647e+03 - logprior: -1.3861e+00
Epoch 2/10
39/39 - 22s - loss: 1262.1482 - loglik: -1.2621e+03 - logprior: -6.8633e-02
Epoch 3/10
39/39 - 22s - loss: 1259.7584 - loglik: -1.2598e+03 - logprior: 0.0330
Epoch 4/10
39/39 - 22s - loss: 1255.4507 - loglik: -1.2555e+03 - logprior: 0.0737
Epoch 5/10
39/39 - 22s - loss: 1249.1378 - loglik: -1.2492e+03 - logprior: 0.0170
Epoch 6/10
39/39 - 22s - loss: 1243.9933 - loglik: -1.2439e+03 - logprior: -5.6877e-02
Epoch 7/10
39/39 - 22s - loss: 1239.3707 - loglik: -1.2391e+03 - logprior: -2.6738e-01
Epoch 8/10
39/39 - 22s - loss: 1235.5885 - loglik: -1.2351e+03 - logprior: -5.1798e-01
Epoch 9/10
39/39 - 22s - loss: 1230.5382 - loglik: -1.2298e+03 - logprior: -7.1250e-01
Epoch 10/10
39/39 - 22s - loss: 1228.9586 - loglik: -1.2281e+03 - logprior: -8.5559e-01
Fitted a model with MAP estimate = -1227.6544
Time for alignment: 593.5120
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1322.7754 - loglik: -1.3206e+03 - logprior: -2.1766e+00
Epoch 2/10
39/39 - 17s - loss: 1272.9149 - loglik: -1.2718e+03 - logprior: -1.1214e+00
Epoch 3/10
39/39 - 17s - loss: 1267.4984 - loglik: -1.2664e+03 - logprior: -1.1176e+00
Epoch 4/10
39/39 - 17s - loss: 1263.6853 - loglik: -1.2625e+03 - logprior: -1.1546e+00
Epoch 5/10
39/39 - 17s - loss: 1259.3019 - loglik: -1.2580e+03 - logprior: -1.3072e+00
Epoch 6/10
39/39 - 17s - loss: 1255.2717 - loglik: -1.2537e+03 - logprior: -1.5451e+00
Epoch 7/10
39/39 - 17s - loss: 1252.1003 - loglik: -1.2503e+03 - logprior: -1.8090e+00
Epoch 8/10
39/39 - 17s - loss: 1249.5122 - loglik: -1.2475e+03 - logprior: -2.0563e+00
Epoch 9/10
39/39 - 17s - loss: 1247.4292 - loglik: -1.2452e+03 - logprior: -2.2710e+00
Epoch 10/10
39/39 - 17s - loss: 1244.7988 - loglik: -1.2424e+03 - logprior: -2.4017e+00
Fitted a model with MAP estimate = -1243.7361
expansions: [(23, 1), (24, 6), (28, 1), (47, 5), (49, 1), (60, 1), (62, 1), (63, 2), (65, 1), (81, 1), (82, 1), (84, 1), (85, 1), (105, 1), (106, 3), (107, 2), (111, 1), (122, 1), (124, 1), (125, 1), (127, 1), (136, 1), (156, 8), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (176, 4), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1310.1842 - loglik: -1.3074e+03 - logprior: -2.7963e+00
Epoch 2/2
39/39 - 23s - loss: 1266.3928 - loglik: -1.2655e+03 - logprior: -9.1391e-01
Fitted a model with MAP estimate = -1261.6708
expansions: [(28, 1), (53, 1), (104, 1), (130, 1), (182, 1), (195, 1), (203, 1), (245, 10)]
discards: [ 30  58  59  62 106 128 131 132 197 198 199 200 204 223 228 229 242 243
 244]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1266.8208 - loglik: -1.2651e+03 - logprior: -1.6840e+00
Epoch 2/2
39/39 - 22s - loss: 1262.0046 - loglik: -1.2616e+03 - logprior: -4.4450e-01
Fitted a model with MAP estimate = -1259.1938
expansions: [(32, 1), (190, 1)]
discards: [ 26  27  28 234 235 236 237 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1265.9890 - loglik: -1.2646e+03 - logprior: -1.4004e+00
Epoch 2/10
39/39 - 22s - loss: 1262.1571 - loglik: -1.2621e+03 - logprior: -9.0002e-02
Epoch 3/10
39/39 - 22s - loss: 1259.8000 - loglik: -1.2598e+03 - logprior: 0.0153
Epoch 4/10
39/39 - 23s - loss: 1255.3765 - loglik: -1.2554e+03 - logprior: 0.0579
Epoch 5/10
39/39 - 23s - loss: 1250.0308 - loglik: -1.2500e+03 - logprior: -2.9112e-03
Epoch 6/10
39/39 - 23s - loss: 1244.7612 - loglik: -1.2447e+03 - logprior: -7.1481e-02
Epoch 7/10
39/39 - 23s - loss: 1239.4427 - loglik: -1.2392e+03 - logprior: -2.7077e-01
Epoch 8/10
39/39 - 23s - loss: 1234.8213 - loglik: -1.2343e+03 - logprior: -5.1996e-01
Epoch 9/10
39/39 - 23s - loss: 1231.9037 - loglik: -1.2312e+03 - logprior: -7.1968e-01
Epoch 10/10
39/39 - 23s - loss: 1228.2728 - loglik: -1.2274e+03 - logprior: -8.5180e-01
Fitted a model with MAP estimate = -1227.7274
Time for alignment: 597.0174
Computed alignments with likelihoods: ['-1228.1201', '-1226.4174', '-1228.1204', '-1227.6544', '-1227.7274']
Best model has likelihood: -1226.4174  (prior= -0.8723 )
time for generating output: 0.3024
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.5899446494464945
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2efc0caf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff300e635b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1210f5dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9311da730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1214791c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11848bc70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff114ec1910>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff115193eb0>, <__main__.SimpleDirichletPrior object at 0x7feefda5a520>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.4243 - loglik: -3.1499e+02 - logprior: -3.4335e+00
Epoch 2/10
19/19 - 1s - loss: 288.1037 - loglik: -2.8653e+02 - logprior: -1.5699e+00
Epoch 3/10
19/19 - 1s - loss: 274.8242 - loglik: -2.7320e+02 - logprior: -1.6286e+00
Epoch 4/10
19/19 - 1s - loss: 271.3806 - loglik: -2.6976e+02 - logprior: -1.6251e+00
Epoch 5/10
19/19 - 1s - loss: 270.7991 - loglik: -2.6920e+02 - logprior: -1.6040e+00
Epoch 6/10
19/19 - 1s - loss: 270.3196 - loglik: -2.6873e+02 - logprior: -1.5916e+00
Epoch 7/10
19/19 - 1s - loss: 269.7021 - loglik: -2.6811e+02 - logprior: -1.5909e+00
Epoch 8/10
19/19 - 1s - loss: 270.3677 - loglik: -2.6877e+02 - logprior: -1.6002e+00
Fitted a model with MAP estimate = -269.6629
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 272.0271 - loglik: -2.6866e+02 - logprior: -3.3680e+00
Epoch 2/2
19/19 - 1s - loss: 264.2562 - loglik: -2.6289e+02 - logprior: -1.3710e+00
Fitted a model with MAP estimate = -263.3631
expansions: []
discards: [21 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 266.5613 - loglik: -2.6339e+02 - logprior: -3.1669e+00
Epoch 2/2
19/19 - 1s - loss: 263.6665 - loglik: -2.6239e+02 - logprior: -1.2715e+00
Fitted a model with MAP estimate = -262.9589
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 266.0281 - loglik: -2.6289e+02 - logprior: -3.1381e+00
Epoch 2/10
19/19 - 1s - loss: 263.3239 - loglik: -2.6206e+02 - logprior: -1.2614e+00
Epoch 3/10
19/19 - 1s - loss: 262.8424 - loglik: -2.6171e+02 - logprior: -1.1321e+00
Epoch 4/10
19/19 - 1s - loss: 262.4089 - loglik: -2.6131e+02 - logprior: -1.0978e+00
Epoch 5/10
19/19 - 1s - loss: 262.1627 - loglik: -2.6109e+02 - logprior: -1.0760e+00
Epoch 6/10
19/19 - 1s - loss: 261.7009 - loglik: -2.6062e+02 - logprior: -1.0831e+00
Epoch 7/10
19/19 - 1s - loss: 261.8082 - loglik: -2.6072e+02 - logprior: -1.0850e+00
Fitted a model with MAP estimate = -261.5459
Time for alignment: 50.4193
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 318.3277 - loglik: -3.1490e+02 - logprior: -3.4282e+00
Epoch 2/10
19/19 - 1s - loss: 287.4008 - loglik: -2.8583e+02 - logprior: -1.5677e+00
Epoch 3/10
19/19 - 1s - loss: 274.5454 - loglik: -2.7295e+02 - logprior: -1.5960e+00
Epoch 4/10
19/19 - 1s - loss: 271.0966 - loglik: -2.6953e+02 - logprior: -1.5621e+00
Epoch 5/10
19/19 - 1s - loss: 270.1180 - loglik: -2.6857e+02 - logprior: -1.5523e+00
Epoch 6/10
19/19 - 1s - loss: 269.9424 - loglik: -2.6840e+02 - logprior: -1.5450e+00
Epoch 7/10
19/19 - 1s - loss: 269.4781 - loglik: -2.6793e+02 - logprior: -1.5520e+00
Epoch 8/10
19/19 - 1s - loss: 269.5818 - loglik: -2.6802e+02 - logprior: -1.5601e+00
Fitted a model with MAP estimate = -269.2017
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.6633 - loglik: -2.6827e+02 - logprior: -3.3952e+00
Epoch 2/2
19/19 - 1s - loss: 264.2523 - loglik: -2.6286e+02 - logprior: -1.3960e+00
Fitted a model with MAP estimate = -263.3252
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.4759 - loglik: -2.6330e+02 - logprior: -3.1734e+00
Epoch 2/2
19/19 - 1s - loss: 263.6398 - loglik: -2.6236e+02 - logprior: -1.2841e+00
Fitted a model with MAP estimate = -262.9807
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 265.9600 - loglik: -2.6282e+02 - logprior: -3.1356e+00
Epoch 2/10
19/19 - 1s - loss: 263.4740 - loglik: -2.6222e+02 - logprior: -1.2575e+00
Epoch 3/10
19/19 - 1s - loss: 262.6512 - loglik: -2.6151e+02 - logprior: -1.1400e+00
Epoch 4/10
19/19 - 1s - loss: 262.3556 - loglik: -2.6127e+02 - logprior: -1.0861e+00
Epoch 5/10
19/19 - 1s - loss: 262.1109 - loglik: -2.6103e+02 - logprior: -1.0828e+00
Epoch 6/10
19/19 - 1s - loss: 261.8757 - loglik: -2.6080e+02 - logprior: -1.0796e+00
Epoch 7/10
19/19 - 1s - loss: 261.6612 - loglik: -2.6057e+02 - logprior: -1.0896e+00
Epoch 8/10
19/19 - 1s - loss: 261.5112 - loglik: -2.6043e+02 - logprior: -1.0852e+00
Epoch 9/10
19/19 - 1s - loss: 261.4472 - loglik: -2.6035e+02 - logprior: -1.1004e+00
Epoch 10/10
19/19 - 1s - loss: 261.6805 - loglik: -2.6057e+02 - logprior: -1.1130e+00
Fitted a model with MAP estimate = -261.3199
Time for alignment: 54.9372
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.3798 - loglik: -3.1495e+02 - logprior: -3.4315e+00
Epoch 2/10
19/19 - 1s - loss: 287.9063 - loglik: -2.8635e+02 - logprior: -1.5522e+00
Epoch 3/10
19/19 - 1s - loss: 275.4706 - loglik: -2.7393e+02 - logprior: -1.5373e+00
Epoch 4/10
19/19 - 1s - loss: 272.6310 - loglik: -2.7112e+02 - logprior: -1.5084e+00
Epoch 5/10
19/19 - 1s - loss: 271.6194 - loglik: -2.7013e+02 - logprior: -1.4847e+00
Epoch 6/10
19/19 - 1s - loss: 271.2558 - loglik: -2.6978e+02 - logprior: -1.4757e+00
Epoch 7/10
19/19 - 1s - loss: 270.4034 - loglik: -2.6891e+02 - logprior: -1.4919e+00
Epoch 8/10
19/19 - 1s - loss: 270.7027 - loglik: -2.6919e+02 - logprior: -1.5114e+00
Fitted a model with MAP estimate = -270.3617
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 271.7959 - loglik: -2.6842e+02 - logprior: -3.3805e+00
Epoch 2/2
19/19 - 1s - loss: 264.1744 - loglik: -2.6277e+02 - logprior: -1.4046e+00
Fitted a model with MAP estimate = -263.3280
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.6150 - loglik: -2.6344e+02 - logprior: -3.1748e+00
Epoch 2/2
19/19 - 1s - loss: 263.4324 - loglik: -2.6215e+02 - logprior: -1.2788e+00
Fitted a model with MAP estimate = -262.9505
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0293 - loglik: -2.6290e+02 - logprior: -3.1320e+00
Epoch 2/10
19/19 - 1s - loss: 263.2644 - loglik: -2.6200e+02 - logprior: -1.2642e+00
Epoch 3/10
19/19 - 1s - loss: 262.6504 - loglik: -2.6152e+02 - logprior: -1.1326e+00
Epoch 4/10
19/19 - 1s - loss: 262.6635 - loglik: -2.6157e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -262.1069
Time for alignment: 44.7057
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 318.3129 - loglik: -3.1488e+02 - logprior: -3.4312e+00
Epoch 2/10
19/19 - 1s - loss: 287.6789 - loglik: -2.8612e+02 - logprior: -1.5629e+00
Epoch 3/10
19/19 - 1s - loss: 275.0933 - loglik: -2.7350e+02 - logprior: -1.5963e+00
Epoch 4/10
19/19 - 1s - loss: 271.9279 - loglik: -2.7037e+02 - logprior: -1.5554e+00
Epoch 5/10
19/19 - 1s - loss: 270.1687 - loglik: -2.6862e+02 - logprior: -1.5450e+00
Epoch 6/10
19/19 - 1s - loss: 269.7463 - loglik: -2.6820e+02 - logprior: -1.5491e+00
Epoch 7/10
19/19 - 1s - loss: 269.6780 - loglik: -2.6813e+02 - logprior: -1.5523e+00
Epoch 8/10
19/19 - 1s - loss: 269.5488 - loglik: -2.6799e+02 - logprior: -1.5637e+00
Epoch 9/10
19/19 - 1s - loss: 269.3737 - loglik: -2.6779e+02 - logprior: -1.5864e+00
Epoch 10/10
19/19 - 1s - loss: 269.1613 - loglik: -2.6755e+02 - logprior: -1.6074e+00
Fitted a model with MAP estimate = -269.0590
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.8294 - loglik: -2.6842e+02 - logprior: -3.4131e+00
Epoch 2/2
19/19 - 1s - loss: 264.3284 - loglik: -2.6293e+02 - logprior: -1.4007e+00
Fitted a model with MAP estimate = -263.3149
expansions: []
discards: [36 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 266.4739 - loglik: -2.6327e+02 - logprior: -3.2021e+00
Epoch 2/2
19/19 - 1s - loss: 263.5963 - loglik: -2.6230e+02 - logprior: -1.2980e+00
Fitted a model with MAP estimate = -263.0020
expansions: []
discards: [21]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.1659 - loglik: -2.6302e+02 - logprior: -3.1428e+00
Epoch 2/10
19/19 - 1s - loss: 263.3566 - loglik: -2.6210e+02 - logprior: -1.2530e+00
Epoch 3/10
19/19 - 1s - loss: 262.6057 - loglik: -2.6147e+02 - logprior: -1.1318e+00
Epoch 4/10
19/19 - 1s - loss: 262.4656 - loglik: -2.6138e+02 - logprior: -1.0872e+00
Epoch 5/10
19/19 - 1s - loss: 262.3362 - loglik: -2.6125e+02 - logprior: -1.0858e+00
Epoch 6/10
19/19 - 1s - loss: 261.8141 - loglik: -2.6074e+02 - logprior: -1.0759e+00
Epoch 7/10
19/19 - 1s - loss: 261.5812 - loglik: -2.6050e+02 - logprior: -1.0831e+00
Epoch 8/10
19/19 - 1s - loss: 261.4701 - loglik: -2.6038e+02 - logprior: -1.0859e+00
Epoch 9/10
19/19 - 1s - loss: 261.5320 - loglik: -2.6044e+02 - logprior: -1.0900e+00
Fitted a model with MAP estimate = -261.3779
Time for alignment: 54.9259
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.3483 - loglik: -3.1492e+02 - logprior: -3.4311e+00
Epoch 2/10
19/19 - 1s - loss: 287.5777 - loglik: -2.8600e+02 - logprior: -1.5730e+00
Epoch 3/10
19/19 - 1s - loss: 274.6127 - loglik: -2.7301e+02 - logprior: -1.5997e+00
Epoch 4/10
19/19 - 1s - loss: 271.2144 - loglik: -2.6965e+02 - logprior: -1.5654e+00
Epoch 5/10
19/19 - 1s - loss: 270.0710 - loglik: -2.6851e+02 - logprior: -1.5575e+00
Epoch 6/10
19/19 - 1s - loss: 269.6483 - loglik: -2.6811e+02 - logprior: -1.5412e+00
Epoch 7/10
19/19 - 1s - loss: 269.8238 - loglik: -2.6827e+02 - logprior: -1.5502e+00
Fitted a model with MAP estimate = -269.2942
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 271.7179 - loglik: -2.6833e+02 - logprior: -3.3834e+00
Epoch 2/2
19/19 - 1s - loss: 264.1776 - loglik: -2.6277e+02 - logprior: -1.4060e+00
Fitted a model with MAP estimate = -263.3169
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.4859 - loglik: -2.6331e+02 - logprior: -3.1773e+00
Epoch 2/2
19/19 - 1s - loss: 263.5597 - loglik: -2.6228e+02 - logprior: -1.2809e+00
Fitted a model with MAP estimate = -262.9472
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.1052 - loglik: -2.6297e+02 - logprior: -3.1367e+00
Epoch 2/10
19/19 - 1s - loss: 263.3575 - loglik: -2.6210e+02 - logprior: -1.2618e+00
Epoch 3/10
19/19 - 1s - loss: 262.8569 - loglik: -2.6172e+02 - logprior: -1.1345e+00
Epoch 4/10
19/19 - 1s - loss: 262.3540 - loglik: -2.6126e+02 - logprior: -1.0943e+00
Epoch 5/10
19/19 - 1s - loss: 261.9493 - loglik: -2.6087e+02 - logprior: -1.0802e+00
Epoch 6/10
19/19 - 1s - loss: 262.1477 - loglik: -2.6106e+02 - logprior: -1.0841e+00
Fitted a model with MAP estimate = -261.6537
Time for alignment: 45.8351
Computed alignments with likelihoods: ['-261.5459', '-261.3199', '-262.1069', '-261.3779', '-261.6537']
Best model has likelihood: -261.3199  (prior= -1.1238 )
time for generating output: 0.1063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9849537037037037
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941f98af0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef1c3770a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1145cd160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefda91400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113714a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe941fbe580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11367f2b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef780a6c10>, <__main__.SimpleDirichletPrior object at 0x7fe9201e1340>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 739.3579 - loglik: -7.3593e+02 - logprior: -3.4260e+00
Epoch 2/10
19/19 - 3s - loss: 709.9105 - loglik: -7.0886e+02 - logprior: -1.0541e+00
Epoch 3/10
19/19 - 3s - loss: 698.6528 - loglik: -6.9740e+02 - logprior: -1.2481e+00
Epoch 4/10
19/19 - 3s - loss: 694.6551 - loglik: -6.9341e+02 - logprior: -1.2466e+00
Epoch 5/10
19/19 - 4s - loss: 691.8376 - loglik: -6.9050e+02 - logprior: -1.3389e+00
Epoch 6/10
19/19 - 3s - loss: 688.5732 - loglik: -6.8719e+02 - logprior: -1.3831e+00
Epoch 7/10
19/19 - 3s - loss: 687.3868 - loglik: -6.8591e+02 - logprior: -1.4808e+00
Epoch 8/10
19/19 - 4s - loss: 684.7614 - loglik: -6.8321e+02 - logprior: -1.5481e+00
Epoch 9/10
19/19 - 4s - loss: 683.4468 - loglik: -6.8183e+02 - logprior: -1.6177e+00
Epoch 10/10
19/19 - 4s - loss: 683.9765 - loglik: -6.8230e+02 - logprior: -1.6788e+00
Fitted a model with MAP estimate = -682.1551
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (25, 1), (28, 1), (31, 1), (53, 3), (68, 2), (75, 4), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 714.1788 - loglik: -7.1009e+02 - logprior: -4.0872e+00
Epoch 2/2
19/19 - 4s - loss: 693.1073 - loglik: -6.9107e+02 - logprior: -2.0334e+00
Fitted a model with MAP estimate = -689.1220
expansions: [(0, 2)]
discards: [  0  85 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 690.8749 - loglik: -6.8802e+02 - logprior: -2.8505e+00
Epoch 2/2
19/19 - 4s - loss: 686.9273 - loglik: -6.8590e+02 - logprior: -1.0252e+00
Fitted a model with MAP estimate = -685.7999
expansions: [(92, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 689.2381 - loglik: -6.8649e+02 - logprior: -2.7503e+00
Epoch 2/10
19/19 - 4s - loss: 686.2820 - loglik: -6.8533e+02 - logprior: -9.5440e-01
Epoch 3/10
19/19 - 4s - loss: 685.1804 - loglik: -6.8426e+02 - logprior: -9.1646e-01
Epoch 4/10
19/19 - 4s - loss: 683.7176 - loglik: -6.8283e+02 - logprior: -8.8718e-01
Epoch 5/10
19/19 - 4s - loss: 682.0648 - loglik: -6.8117e+02 - logprior: -8.9383e-01
Epoch 6/10
19/19 - 4s - loss: 679.7889 - loglik: -6.7888e+02 - logprior: -9.1034e-01
Epoch 7/10
19/19 - 4s - loss: 677.9415 - loglik: -6.7699e+02 - logprior: -9.5326e-01
Epoch 8/10
19/19 - 4s - loss: 675.5703 - loglik: -6.7457e+02 - logprior: -9.9697e-01
Epoch 9/10
19/19 - 4s - loss: 674.3979 - loglik: -6.7334e+02 - logprior: -1.0627e+00
Epoch 10/10
19/19 - 4s - loss: 672.5994 - loglik: -6.7150e+02 - logprior: -1.1035e+00
Fitted a model with MAP estimate = -672.0765
Time for alignment: 135.9729
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 739.2486 - loglik: -7.3583e+02 - logprior: -3.4232e+00
Epoch 2/10
19/19 - 3s - loss: 710.4455 - loglik: -7.0939e+02 - logprior: -1.0587e+00
Epoch 3/10
19/19 - 3s - loss: 698.3417 - loglik: -6.9708e+02 - logprior: -1.2649e+00
Epoch 4/10
19/19 - 3s - loss: 693.7604 - loglik: -6.9252e+02 - logprior: -1.2365e+00
Epoch 5/10
19/19 - 3s - loss: 691.9427 - loglik: -6.9062e+02 - logprior: -1.3218e+00
Epoch 6/10
19/19 - 3s - loss: 689.4844 - loglik: -6.8812e+02 - logprior: -1.3654e+00
Epoch 7/10
19/19 - 4s - loss: 687.6799 - loglik: -6.8622e+02 - logprior: -1.4650e+00
Epoch 8/10
19/19 - 3s - loss: 685.6354 - loglik: -6.8410e+02 - logprior: -1.5359e+00
Epoch 9/10
19/19 - 4s - loss: 683.7269 - loglik: -6.8212e+02 - logprior: -1.6039e+00
Epoch 10/10
19/19 - 4s - loss: 682.8038 - loglik: -6.8114e+02 - logprior: -1.6598e+00
Fitted a model with MAP estimate = -682.4731
expansions: [(15, 1), (18, 1), (19, 2), (21, 4), (23, 2), (26, 1), (27, 1), (28, 1), (31, 1), (54, 4), (75, 4), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 713.8604 - loglik: -7.0978e+02 - logprior: -4.0791e+00
Epoch 2/2
19/19 - 4s - loss: 692.6650 - loglik: -6.9068e+02 - logprior: -1.9868e+00
Fitted a model with MAP estimate = -689.2859
expansions: [(0, 2)]
discards: [  0 107]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 690.7518 - loglik: -6.8794e+02 - logprior: -2.8166e+00
Epoch 2/2
19/19 - 4s - loss: 687.2906 - loglik: -6.8628e+02 - logprior: -1.0150e+00
Fitted a model with MAP estimate = -685.9159
expansions: [(92, 1)]
discards: [ 1 27]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 690.2432 - loglik: -6.8757e+02 - logprior: -2.6779e+00
Epoch 2/10
19/19 - 4s - loss: 687.3657 - loglik: -6.8647e+02 - logprior: -8.9312e-01
Epoch 3/10
19/19 - 4s - loss: 685.9531 - loglik: -6.8508e+02 - logprior: -8.6807e-01
Epoch 4/10
19/19 - 4s - loss: 684.4836 - loglik: -6.8353e+02 - logprior: -9.5226e-01
Epoch 5/10
19/19 - 4s - loss: 682.3924 - loglik: -6.8146e+02 - logprior: -9.3512e-01
Epoch 6/10
19/19 - 4s - loss: 681.3522 - loglik: -6.8040e+02 - logprior: -9.5084e-01
Epoch 7/10
19/19 - 4s - loss: 678.3409 - loglik: -6.7734e+02 - logprior: -1.0012e+00
Epoch 8/10
19/19 - 4s - loss: 676.2133 - loglik: -6.7515e+02 - logprior: -1.0599e+00
Epoch 9/10
19/19 - 4s - loss: 675.8497 - loglik: -6.7474e+02 - logprior: -1.1130e+00
Epoch 10/10
19/19 - 4s - loss: 674.1927 - loglik: -6.7303e+02 - logprior: -1.1606e+00
Fitted a model with MAP estimate = -672.8568
Time for alignment: 133.5841
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 739.2538 - loglik: -7.3583e+02 - logprior: -3.4231e+00
Epoch 2/10
19/19 - 3s - loss: 711.4141 - loglik: -7.1036e+02 - logprior: -1.0535e+00
Epoch 3/10
19/19 - 3s - loss: 699.7762 - loglik: -6.9852e+02 - logprior: -1.2609e+00
Epoch 4/10
19/19 - 3s - loss: 694.1654 - loglik: -6.9293e+02 - logprior: -1.2378e+00
Epoch 5/10
19/19 - 4s - loss: 691.7499 - loglik: -6.9043e+02 - logprior: -1.3155e+00
Epoch 6/10
19/19 - 4s - loss: 689.8981 - loglik: -6.8855e+02 - logprior: -1.3436e+00
Epoch 7/10
19/19 - 3s - loss: 686.7747 - loglik: -6.8536e+02 - logprior: -1.4117e+00
Epoch 8/10
19/19 - 4s - loss: 684.5728 - loglik: -6.8308e+02 - logprior: -1.4881e+00
Epoch 9/10
19/19 - 4s - loss: 685.0291 - loglik: -6.8347e+02 - logprior: -1.5605e+00
Fitted a model with MAP estimate = -682.9218
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (23, 2), (24, 3), (25, 1), (29, 1), (55, 4), (73, 1), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 709.2697 - loglik: -7.0520e+02 - logprior: -4.0663e+00
Epoch 2/2
19/19 - 4s - loss: 691.9398 - loglik: -6.9005e+02 - logprior: -1.8870e+00
Fitted a model with MAP estimate = -688.5076
expansions: [(0, 2)]
discards: [  0 109]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 690.2922 - loglik: -6.8748e+02 - logprior: -2.8093e+00
Epoch 2/2
19/19 - 4s - loss: 686.6251 - loglik: -6.8561e+02 - logprior: -1.0136e+00
Fitted a model with MAP estimate = -685.5481
expansions: []
discards: [26 27]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 689.7452 - loglik: -6.8702e+02 - logprior: -2.7283e+00
Epoch 2/10
19/19 - 4s - loss: 686.7485 - loglik: -6.8580e+02 - logprior: -9.4810e-01
Epoch 3/10
19/19 - 4s - loss: 685.7021 - loglik: -6.8479e+02 - logprior: -9.0902e-01
Epoch 4/10
19/19 - 4s - loss: 684.3034 - loglik: -6.8341e+02 - logprior: -8.9421e-01
Epoch 5/10
19/19 - 4s - loss: 682.9411 - loglik: -6.8204e+02 - logprior: -8.9807e-01
Epoch 6/10
19/19 - 4s - loss: 680.5501 - loglik: -6.7963e+02 - logprior: -9.2335e-01
Epoch 7/10
19/19 - 4s - loss: 678.2721 - loglik: -6.7731e+02 - logprior: -9.6457e-01
Epoch 8/10
19/19 - 4s - loss: 675.9995 - loglik: -6.7497e+02 - logprior: -1.0322e+00
Epoch 9/10
19/19 - 4s - loss: 674.5319 - loglik: -6.7345e+02 - logprior: -1.0858e+00
Epoch 10/10
19/19 - 4s - loss: 673.8535 - loglik: -6.7272e+02 - logprior: -1.1312e+00
Fitted a model with MAP estimate = -672.4844
Time for alignment: 130.7037
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 739.4321 - loglik: -7.3601e+02 - logprior: -3.4216e+00
Epoch 2/10
19/19 - 3s - loss: 711.1674 - loglik: -7.1013e+02 - logprior: -1.0379e+00
Epoch 3/10
19/19 - 3s - loss: 698.9370 - loglik: -6.9767e+02 - logprior: -1.2637e+00
Epoch 4/10
19/19 - 4s - loss: 694.5897 - loglik: -6.9335e+02 - logprior: -1.2372e+00
Epoch 5/10
19/19 - 3s - loss: 691.7601 - loglik: -6.9044e+02 - logprior: -1.3224e+00
Epoch 6/10
19/19 - 3s - loss: 689.2780 - loglik: -6.8791e+02 - logprior: -1.3660e+00
Epoch 7/10
19/19 - 3s - loss: 687.0063 - loglik: -6.8556e+02 - logprior: -1.4453e+00
Epoch 8/10
19/19 - 4s - loss: 684.8284 - loglik: -6.8331e+02 - logprior: -1.5195e+00
Epoch 9/10
19/19 - 3s - loss: 684.8172 - loglik: -6.8322e+02 - logprior: -1.5981e+00
Epoch 10/10
19/19 - 3s - loss: 682.9301 - loglik: -6.8127e+02 - logprior: -1.6595e+00
Fitted a model with MAP estimate = -682.0673
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (24, 3), (25, 1), (32, 1), (55, 4), (70, 1), (75, 1), (76, 3), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 714.3817 - loglik: -7.1032e+02 - logprior: -4.0646e+00
Epoch 2/2
19/19 - 4s - loss: 693.4386 - loglik: -6.9144e+02 - logprior: -2.0014e+00
Fitted a model with MAP estimate = -689.5526
expansions: [(0, 2), (98, 1), (103, 3)]
discards: [  0 109]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 691.0250 - loglik: -6.8816e+02 - logprior: -2.8645e+00
Epoch 2/2
19/19 - 5s - loss: 686.9221 - loglik: -6.8589e+02 - logprior: -1.0346e+00
Fitted a model with MAP estimate = -685.4669
expansions: []
discards: [ 26  27  28  98 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 690.4341 - loglik: -6.8770e+02 - logprior: -2.7313e+00
Epoch 2/10
19/19 - 4s - loss: 687.7195 - loglik: -6.8676e+02 - logprior: -9.5615e-01
Epoch 3/10
19/19 - 4s - loss: 685.7170 - loglik: -6.8480e+02 - logprior: -9.1687e-01
Epoch 4/10
19/19 - 4s - loss: 684.7353 - loglik: -6.8384e+02 - logprior: -8.9408e-01
Epoch 5/10
19/19 - 4s - loss: 683.1453 - loglik: -6.8224e+02 - logprior: -9.0263e-01
Epoch 6/10
19/19 - 4s - loss: 681.2753 - loglik: -6.8035e+02 - logprior: -9.2256e-01
Epoch 7/10
19/19 - 4s - loss: 678.2455 - loglik: -6.7728e+02 - logprior: -9.6518e-01
Epoch 8/10
19/19 - 4s - loss: 676.2515 - loglik: -6.7523e+02 - logprior: -1.0244e+00
Epoch 9/10
19/19 - 4s - loss: 675.3474 - loglik: -6.7427e+02 - logprior: -1.0751e+00
Epoch 10/10
19/19 - 4s - loss: 674.0121 - loglik: -6.7290e+02 - logprior: -1.1111e+00
Fitted a model with MAP estimate = -673.0383
Time for alignment: 134.5152
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 739.3917 - loglik: -7.3597e+02 - logprior: -3.4252e+00
Epoch 2/10
19/19 - 4s - loss: 711.2218 - loglik: -7.1016e+02 - logprior: -1.0623e+00
Epoch 3/10
19/19 - 3s - loss: 698.8289 - loglik: -6.9754e+02 - logprior: -1.2890e+00
Epoch 4/10
19/19 - 3s - loss: 693.8893 - loglik: -6.9263e+02 - logprior: -1.2581e+00
Epoch 5/10
19/19 - 3s - loss: 691.8099 - loglik: -6.9044e+02 - logprior: -1.3701e+00
Epoch 6/10
19/19 - 4s - loss: 688.3309 - loglik: -6.8690e+02 - logprior: -1.4315e+00
Epoch 7/10
19/19 - 3s - loss: 685.6544 - loglik: -6.8412e+02 - logprior: -1.5334e+00
Epoch 8/10
19/19 - 4s - loss: 684.9688 - loglik: -6.8337e+02 - logprior: -1.5987e+00
Epoch 9/10
19/19 - 4s - loss: 682.4008 - loglik: -6.8073e+02 - logprior: -1.6658e+00
Epoch 10/10
19/19 - 4s - loss: 683.2283 - loglik: -6.8150e+02 - logprior: -1.7287e+00
Fitted a model with MAP estimate = -681.5136
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 2), (29, 1), (32, 1), (35, 2), (53, 2), (73, 1), (74, 1), (75, 1), (76, 3), (82, 2), (84, 1), (86, 1), (87, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 714.0555 - loglik: -7.0997e+02 - logprior: -4.0888e+00
Epoch 2/2
19/19 - 4s - loss: 693.3627 - loglik: -6.9134e+02 - logprior: -2.0247e+00
Fitted a model with MAP estimate = -689.6580
expansions: [(0, 2), (98, 1), (103, 3)]
discards: [  0  34  37  49 107]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 691.3057 - loglik: -6.8847e+02 - logprior: -2.8401e+00
Epoch 2/2
19/19 - 4s - loss: 687.2203 - loglik: -6.8620e+02 - logprior: -1.0191e+00
Fitted a model with MAP estimate = -685.5977
expansions: []
discards: [  1  26  98 102 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 690.7625 - loglik: -6.8808e+02 - logprior: -2.6819e+00
Epoch 2/10
19/19 - 4s - loss: 687.2936 - loglik: -6.8638e+02 - logprior: -9.1531e-01
Epoch 3/10
19/19 - 4s - loss: 686.1412 - loglik: -6.8530e+02 - logprior: -8.4341e-01
Epoch 4/10
19/19 - 4s - loss: 685.1667 - loglik: -6.8431e+02 - logprior: -8.5435e-01
Epoch 5/10
19/19 - 4s - loss: 682.0339 - loglik: -6.8121e+02 - logprior: -8.2051e-01
Epoch 6/10
19/19 - 4s - loss: 681.1486 - loglik: -6.8032e+02 - logprior: -8.2861e-01
Epoch 7/10
19/19 - 4s - loss: 678.2819 - loglik: -6.7739e+02 - logprior: -8.8873e-01
Epoch 8/10
19/19 - 4s - loss: 676.9612 - loglik: -6.7603e+02 - logprior: -9.2838e-01
Epoch 9/10
19/19 - 4s - loss: 675.0302 - loglik: -6.7402e+02 - logprior: -1.0092e+00
Epoch 10/10
19/19 - 4s - loss: 673.8519 - loglik: -6.7280e+02 - logprior: -1.0471e+00
Fitted a model with MAP estimate = -672.8884
Time for alignment: 134.2708
Computed alignments with likelihoods: ['-672.0765', '-672.8568', '-672.4844', '-673.0383', '-672.8884']
Best model has likelihood: -672.0765  (prior= -1.1229 )
time for generating output: 0.1725
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8589618021547503
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff1156586a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941ca9190>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1144623a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1296fc6a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef78504040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefee1bf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2dec819a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe950298520>, <__main__.SimpleDirichletPrior object at 0x7fe94165f370>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 1908.5940 - loglik: -1.9065e+03 - logprior: -2.0555e+00
Epoch 2/10
39/39 - 33s - loss: 1814.1469 - loglik: -1.8128e+03 - logprior: -1.3101e+00
Epoch 3/10
39/39 - 33s - loss: 1803.3281 - loglik: -1.8019e+03 - logprior: -1.4018e+00
Epoch 4/10
39/39 - 33s - loss: 1799.0996 - loglik: -1.7976e+03 - logprior: -1.4789e+00
Epoch 5/10
39/39 - 33s - loss: 1796.2253 - loglik: -1.7945e+03 - logprior: -1.7168e+00
Epoch 6/10
39/39 - 33s - loss: 1793.7504 - loglik: -1.7918e+03 - logprior: -1.9970e+00
Epoch 7/10
39/39 - 33s - loss: 1792.4399 - loglik: -1.7901e+03 - logprior: -2.3124e+00
Epoch 8/10
39/39 - 33s - loss: 1791.5745 - loglik: -1.7890e+03 - logprior: -2.5826e+00
Epoch 9/10
39/39 - 33s - loss: 1791.0140 - loglik: -1.7882e+03 - logprior: -2.8072e+00
Epoch 10/10
39/39 - 33s - loss: 1789.3580 - loglik: -1.7863e+03 - logprior: -3.0302e+00
Fitted a model with MAP estimate = -1771.2226
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (31, 1), (37, 1), (43, 4), (45, 2), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (88, 1), (90, 2), (94, 2), (103, 1), (120, 1), (121, 1), (132, 2), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (168, 1), (181, 1), (184, 1), (185, 1), (186, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (219, 1), (220, 1), (226, 2), (228, 1), (240, 2), (242, 1), (245, 1), (255, 1), (261, 2), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 1810.7463 - loglik: -1.8082e+03 - logprior: -2.5935e+00
Epoch 2/2
39/39 - 47s - loss: 1782.1302 - loglik: -1.7815e+03 - logprior: -6.0758e-01
Fitted a model with MAP estimate = -1761.9638
expansions: [(61, 1), (303, 2), (331, 1)]
discards: [  1  32  33  57 116 165 191 257 286 329 342]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 1786.2372 - loglik: -1.7850e+03 - logprior: -1.2158e+00
Epoch 2/2
39/39 - 48s - loss: 1781.5588 - loglik: -1.7813e+03 - logprior: -2.5533e-01
Fitted a model with MAP estimate = -1761.0420
expansions: [(30, 1), (54, 1), (152, 2)]
discards: [296 308]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1765.6810 - loglik: -1.7646e+03 - logprior: -1.1057e+00
Epoch 2/10
39/39 - 61s - loss: 1762.1362 - loglik: -1.7620e+03 - logprior: -9.2650e-02
Epoch 3/10
39/39 - 59s - loss: 1757.8156 - loglik: -1.7578e+03 - logprior: -5.8323e-02
Epoch 4/10
39/39 - 54s - loss: 1752.2512 - loglik: -1.7522e+03 - logprior: -7.9239e-02
Epoch 5/10
39/39 - 57s - loss: 1749.9746 - loglik: -1.7498e+03 - logprior: -1.8471e-01
Epoch 6/10
39/39 - 57s - loss: 1746.8932 - loglik: -1.7465e+03 - logprior: -4.0320e-01
Epoch 7/10
39/39 - 52s - loss: 1744.7638 - loglik: -1.7441e+03 - logprior: -6.2842e-01
Epoch 8/10
39/39 - 53s - loss: 1744.1057 - loglik: -1.7433e+03 - logprior: -8.0931e-01
Epoch 9/10
39/39 - 57s - loss: 1741.7263 - loglik: -1.7407e+03 - logprior: -1.0076e+00
Epoch 10/10
39/39 - 54s - loss: 1738.8824 - loglik: -1.7377e+03 - logprior: -1.2094e+00
Fitted a model with MAP estimate = -1738.7522
Time for alignment: 1320.3796
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1908.3479 - loglik: -1.9063e+03 - logprior: -2.0654e+00
Epoch 2/10
39/39 - 37s - loss: 1813.8607 - loglik: -1.8125e+03 - logprior: -1.3263e+00
Epoch 3/10
39/39 - 39s - loss: 1803.4669 - loglik: -1.8020e+03 - logprior: -1.4706e+00
Epoch 4/10
39/39 - 40s - loss: 1799.5128 - loglik: -1.7979e+03 - logprior: -1.5864e+00
Epoch 5/10
39/39 - 41s - loss: 1796.2562 - loglik: -1.7944e+03 - logprior: -1.8222e+00
Epoch 6/10
39/39 - 43s - loss: 1793.9435 - loglik: -1.7919e+03 - logprior: -2.0630e+00
Epoch 7/10
39/39 - 43s - loss: 1792.6515 - loglik: -1.7902e+03 - logprior: -2.4141e+00
Epoch 8/10
39/39 - 44s - loss: 1792.3856 - loglik: -1.7896e+03 - logprior: -2.7567e+00
Epoch 9/10
39/39 - 44s - loss: 1791.1707 - loglik: -1.7883e+03 - logprior: -2.9084e+00
Epoch 10/10
39/39 - 43s - loss: 1790.1024 - loglik: -1.7870e+03 - logprior: -3.1216e+00
Fitted a model with MAP estimate = -1771.6289
expansions: [(0, 2), (16, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (30, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (101, 1), (103, 2), (121, 1), (132, 2), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (162, 1), (181, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (211, 1), (220, 1), (223, 1), (227, 2), (228, 2), (240, 1), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (261, 1), (262, 1), (263, 1), (271, 3), (272, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1812.3179 - loglik: -1.8097e+03 - logprior: -2.5896e+00
Epoch 2/2
39/39 - 66s - loss: 1783.2950 - loglik: -1.7827e+03 - logprior: -6.0434e-01
Fitted a model with MAP estimate = -1762.5240
expansions: []
discards: [  1  32 106 134 166 192 257 286 304 318 344]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 1786.5529 - loglik: -1.7853e+03 - logprior: -1.2215e+00
Epoch 2/2
39/39 - 54s - loss: 1781.6699 - loglik: -1.7815e+03 - logprior: -2.0020e-01
Fitted a model with MAP estimate = -1761.2302
expansions: [(115, 1), (153, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 1765.9229 - loglik: -1.7648e+03 - logprior: -1.1504e+00
Epoch 2/10
39/39 - 57s - loss: 1760.6204 - loglik: -1.7605e+03 - logprior: -8.4889e-02
Epoch 3/10
39/39 - 57s - loss: 1758.2274 - loglik: -1.7582e+03 - logprior: 0.0032
Epoch 4/10
39/39 - 53s - loss: 1753.5065 - loglik: -1.7535e+03 - logprior: -2.7494e-02
Epoch 5/10
39/39 - 55s - loss: 1748.5465 - loglik: -1.7484e+03 - logprior: -1.6617e-01
Epoch 6/10
39/39 - 60s - loss: 1747.7432 - loglik: -1.7474e+03 - logprior: -3.4920e-01
Epoch 7/10
39/39 - 63s - loss: 1746.9700 - loglik: -1.7464e+03 - logprior: -6.0252e-01
Epoch 8/10
39/39 - 64s - loss: 1743.5853 - loglik: -1.7428e+03 - logprior: -8.2223e-01
Epoch 9/10
39/39 - 64s - loss: 1741.4210 - loglik: -1.7404e+03 - logprior: -1.0142e+00
Epoch 10/10
39/39 - 65s - loss: 1739.4509 - loglik: -1.7383e+03 - logprior: -1.1284e+00
Fitted a model with MAP estimate = -1738.8954
Time for alignment: 1555.8607
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 1908.7385 - loglik: -1.9067e+03 - logprior: -2.0635e+00
Epoch 2/10
39/39 - 44s - loss: 1813.0881 - loglik: -1.8117e+03 - logprior: -1.4089e+00
Epoch 3/10
39/39 - 44s - loss: 1800.2772 - loglik: -1.7986e+03 - logprior: -1.6322e+00
Epoch 4/10
39/39 - 44s - loss: 1796.3438 - loglik: -1.7946e+03 - logprior: -1.7235e+00
Epoch 5/10
39/39 - 43s - loss: 1793.1521 - loglik: -1.7912e+03 - logprior: -1.9240e+00
Epoch 6/10
39/39 - 40s - loss: 1790.9663 - loglik: -1.7887e+03 - logprior: -2.2279e+00
Epoch 7/10
39/39 - 41s - loss: 1789.7863 - loglik: -1.7872e+03 - logprior: -2.5463e+00
Epoch 8/10
39/39 - 42s - loss: 1788.8409 - loglik: -1.7860e+03 - logprior: -2.8232e+00
Epoch 9/10
39/39 - 43s - loss: 1788.2698 - loglik: -1.7852e+03 - logprior: -3.0439e+00
Epoch 10/10
39/39 - 43s - loss: 1787.6194 - loglik: -1.7843e+03 - logprior: -3.2833e+00
Fitted a model with MAP estimate = -1768.8510
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (40, 1), (41, 2), (42, 2), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (90, 1), (95, 1), (97, 1), (98, 1), (103, 1), (119, 1), (122, 1), (123, 1), (124, 1), (132, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (161, 1), (181, 1), (184, 1), (185, 1), (186, 1), (187, 2), (188, 1), (189, 1), (205, 4), (208, 1), (209, 1), (211, 1), (219, 1), (222, 1), (226, 1), (228, 1), (241, 2), (243, 1), (245, 1), (255, 1), (261, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 1805.7357 - loglik: -1.8032e+03 - logprior: -2.5558e+00
Epoch 2/2
39/39 - 57s - loss: 1778.0028 - loglik: -1.7775e+03 - logprior: -5.3432e-01
Fitted a model with MAP estimate = -1757.5415
expansions: [(334, 1)]
discards: [  1  30  56  57 195 240 345]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 1782.0737 - loglik: -1.7808e+03 - logprior: -1.3135e+00
Epoch 2/2
39/39 - 53s - loss: 1777.5834 - loglik: -1.7774e+03 - logprior: -2.2798e-01
Fitted a model with MAP estimate = -1757.1469
expansions: [(54, 1)]
discards: [255 256]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 1763.8859 - loglik: -1.7627e+03 - logprior: -1.1387e+00
Epoch 2/10
39/39 - 56s - loss: 1759.5116 - loglik: -1.7595e+03 - logprior: -3.3919e-02
Epoch 3/10
39/39 - 61s - loss: 1757.0934 - loglik: -1.7571e+03 - logprior: 0.0373
Epoch 4/10
39/39 - 64s - loss: 1753.8372 - loglik: -1.7539e+03 - logprior: 0.0149
Epoch 5/10
39/39 - 65s - loss: 1748.0056 - loglik: -1.7479e+03 - logprior: -1.4537e-01
Epoch 6/10
39/39 - 64s - loss: 1746.6344 - loglik: -1.7463e+03 - logprior: -3.3687e-01
Epoch 7/10
39/39 - 66s - loss: 1744.3756 - loglik: -1.7438e+03 - logprior: -5.8562e-01
Epoch 8/10
39/39 - 66s - loss: 1743.7438 - loglik: -1.7429e+03 - logprior: -8.1201e-01
Epoch 9/10
39/39 - 65s - loss: 1738.0690 - loglik: -1.7371e+03 - logprior: -1.0052e+00
Epoch 10/10
39/39 - 64s - loss: 1740.9581 - loglik: -1.7398e+03 - logprior: -1.1686e+00
Fitted a model with MAP estimate = -1737.9941
Time for alignment: 1588.3384
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1909.7094 - loglik: -1.9076e+03 - logprior: -2.0607e+00
Epoch 2/10
39/39 - 40s - loss: 1813.8079 - loglik: -1.8125e+03 - logprior: -1.3272e+00
Epoch 3/10
39/39 - 41s - loss: 1802.5304 - loglik: -1.8011e+03 - logprior: -1.4037e+00
Epoch 4/10
39/39 - 44s - loss: 1797.3547 - loglik: -1.7958e+03 - logprior: -1.5174e+00
Epoch 5/10
39/39 - 44s - loss: 1794.5465 - loglik: -1.7927e+03 - logprior: -1.8231e+00
Epoch 6/10
39/39 - 44s - loss: 1792.5450 - loglik: -1.7905e+03 - logprior: -2.0018e+00
Epoch 7/10
39/39 - 44s - loss: 1791.1418 - loglik: -1.7888e+03 - logprior: -2.3163e+00
Epoch 8/10
39/39 - 39s - loss: 1790.8594 - loglik: -1.7883e+03 - logprior: -2.5736e+00
Epoch 9/10
39/39 - 37s - loss: 1789.8016 - loglik: -1.7870e+03 - logprior: -2.7894e+00
Epoch 10/10
39/39 - 37s - loss: 1788.3680 - loglik: -1.7853e+03 - logprior: -3.0333e+00
Fitted a model with MAP estimate = -1770.2545
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (32, 1), (43, 4), (45, 2), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 2), (85, 1), (87, 1), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (125, 2), (130, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (197, 1), (205, 2), (207, 1), (208, 1), (209, 1), (220, 1), (223, 1), (227, 2), (228, 2), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (261, 1), (262, 1), (263, 1), (268, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 1808.1747 - loglik: -1.8057e+03 - logprior: -2.5086e+00
Epoch 2/2
39/39 - 54s - loss: 1779.8588 - loglik: -1.7793e+03 - logprior: -5.9236e-01
Fitted a model with MAP estimate = -1759.5983
expansions: [(61, 1), (160, 1)]
discards: [  1  31  32  57 104 122 134 194 288 319 345]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 1784.6010 - loglik: -1.7833e+03 - logprior: -1.2619e+00
Epoch 2/2
39/39 - 55s - loss: 1778.8862 - loglik: -1.7786e+03 - logprior: -2.5858e-01
Fitted a model with MAP estimate = -1758.6332
expansions: [(54, 1), (251, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1763.7843 - loglik: -1.7627e+03 - logprior: -1.0967e+00
Epoch 2/10
39/39 - 65s - loss: 1761.7172 - loglik: -1.7617e+03 - logprior: -6.2673e-02
Epoch 3/10
39/39 - 64s - loss: 1757.2581 - loglik: -1.7572e+03 - logprior: -2.9682e-02
Epoch 4/10
39/39 - 64s - loss: 1752.3953 - loglik: -1.7524e+03 - logprior: -1.0637e-02
Epoch 5/10
39/39 - 59s - loss: 1750.7330 - loglik: -1.7505e+03 - logprior: -2.1808e-01
Epoch 6/10
39/39 - 61s - loss: 1745.5615 - loglik: -1.7452e+03 - logprior: -3.6630e-01
Epoch 7/10
39/39 - 62s - loss: 1744.3278 - loglik: -1.7437e+03 - logprior: -5.8448e-01
Epoch 8/10
39/39 - 56s - loss: 1742.9540 - loglik: -1.7421e+03 - logprior: -8.3495e-01
Epoch 9/10
39/39 - 56s - loss: 1742.8634 - loglik: -1.7419e+03 - logprior: -1.0020e+00
Epoch 10/10
39/39 - 58s - loss: 1740.3055 - loglik: -1.7392e+03 - logprior: -1.1551e+00
Fitted a model with MAP estimate = -1738.8560
Time for alignment: 1545.1685
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1908.7444 - loglik: -1.9067e+03 - logprior: -2.0532e+00
Epoch 2/10
39/39 - 35s - loss: 1815.3630 - loglik: -1.8140e+03 - logprior: -1.3665e+00
Epoch 3/10
39/39 - 37s - loss: 1804.5569 - loglik: -1.8031e+03 - logprior: -1.4809e+00
Epoch 4/10
39/39 - 39s - loss: 1799.4091 - loglik: -1.7979e+03 - logprior: -1.5453e+00
Epoch 5/10
39/39 - 39s - loss: 1796.4093 - loglik: -1.7947e+03 - logprior: -1.7480e+00
Epoch 6/10
39/39 - 36s - loss: 1794.6895 - loglik: -1.7926e+03 - logprior: -2.0403e+00
Epoch 7/10
39/39 - 36s - loss: 1793.1368 - loglik: -1.7908e+03 - logprior: -2.3811e+00
Epoch 8/10
39/39 - 37s - loss: 1792.2681 - loglik: -1.7896e+03 - logprior: -2.6805e+00
Epoch 9/10
39/39 - 39s - loss: 1791.4729 - loglik: -1.7886e+03 - logprior: -2.8872e+00
Epoch 10/10
39/39 - 41s - loss: 1790.3300 - loglik: -1.7873e+03 - logprior: -3.0729e+00
Fitted a model with MAP estimate = -1772.2277
expansions: [(0, 2), (16, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (32, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (56, 1), (59, 2), (61, 1), (62, 1), (78, 1), (80, 1), (82, 1), (83, 2), (93, 1), (94, 1), (95, 2), (102, 1), (103, 1), (121, 1), (130, 1), (145, 1), (148, 2), (152, 1), (154, 1), (155, 3), (182, 1), (185, 1), (186, 1), (188, 1), (189, 2), (201, 1), (206, 3), (209, 1), (210, 1), (211, 1), (213, 1), (220, 1), (222, 1), (228, 2), (240, 2), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 354 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 1809.4440 - loglik: -1.8069e+03 - logprior: -2.5311e+00
Epoch 2/2
39/39 - 66s - loss: 1781.5291 - loglik: -1.7809e+03 - logprior: -5.8351e-01
Fitted a model with MAP estimate = -1760.9636
expansions: [(158, 2), (255, 1), (303, 1)]
discards: [  1  32  33  78  79 108 195 257 258 315 341]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 1784.8700 - loglik: -1.7836e+03 - logprior: -1.2283e+00
Epoch 2/2
39/39 - 64s - loss: 1779.6716 - loglik: -1.7794e+03 - logprior: -2.6963e-01
Fitted a model with MAP estimate = -1759.4684
expansions: [(30, 1), (75, 1), (154, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 1765.1879 - loglik: -1.7640e+03 - logprior: -1.1645e+00
Epoch 2/10
39/39 - 47s - loss: 1760.9590 - loglik: -1.7608e+03 - logprior: -1.3036e-01
Epoch 3/10
39/39 - 46s - loss: 1757.5247 - loglik: -1.7575e+03 - logprior: 0.0178
Epoch 4/10
39/39 - 46s - loss: 1753.1710 - loglik: -1.7532e+03 - logprior: 0.0292
Epoch 5/10
39/39 - 47s - loss: 1750.4778 - loglik: -1.7504e+03 - logprior: -1.0616e-01
Epoch 6/10
39/39 - 48s - loss: 1746.2106 - loglik: -1.7459e+03 - logprior: -2.8573e-01
Epoch 7/10
39/39 - 49s - loss: 1745.9235 - loglik: -1.7453e+03 - logprior: -5.7536e-01
Epoch 8/10
39/39 - 50s - loss: 1742.6981 - loglik: -1.7419e+03 - logprior: -7.7579e-01
Epoch 9/10
39/39 - 50s - loss: 1741.6515 - loglik: -1.7407e+03 - logprior: -9.6918e-01
Epoch 10/10
39/39 - 51s - loss: 1740.0740 - loglik: -1.7390e+03 - logprior: -1.1007e+00
Fitted a model with MAP estimate = -1738.6896
Time for alignment: 1422.4495
Computed alignments with likelihoods: ['-1738.7522', '-1738.8954', '-1737.9941', '-1738.8560', '-1738.6896']
Best model has likelihood: -1737.9941  (prior= -1.1361 )
time for generating output: 1.1911
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.4455922591148107
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9402cf3a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2efbcd2b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113d9dfa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff113d9dd90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c462820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9113ebf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1146414c0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef7850a040>, <__main__.SimpleDirichletPrior object at 0x7fe9311757c0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1898.5441 - loglik: -1.8963e+03 - logprior: -2.2616e+00
Epoch 2/10
39/39 - 53s - loss: 1759.6515 - loglik: -1.7581e+03 - logprior: -1.5583e+00
Epoch 3/10
39/39 - 51s - loss: 1744.5857 - loglik: -1.7428e+03 - logprior: -1.7858e+00
Epoch 4/10
39/39 - 50s - loss: 1744.1332 - loglik: -1.7422e+03 - logprior: -1.8905e+00
Epoch 5/10
39/39 - 49s - loss: 1740.3668 - loglik: -1.7383e+03 - logprior: -2.0647e+00
Epoch 6/10
39/39 - 49s - loss: 1739.9958 - loglik: -1.7378e+03 - logprior: -2.2321e+00
Epoch 7/10
39/39 - 50s - loss: 1741.1945 - loglik: -1.7388e+03 - logprior: -2.4030e+00
Fitted a model with MAP estimate = -1737.7157
expansions: [(6, 1), (20, 2), (21, 1), (31, 1), (63, 2), (66, 1), (68, 1), (100, 2), (103, 2), (105, 1), (106, 1), (107, 1), (114, 1), (119, 1), (135, 1), (137, 1), (139, 1), (140, 1), (141, 3), (142, 3), (157, 1), (159, 1), (160, 1), (161, 1), (165, 1), (174, 3), (175, 4), (178, 5), (179, 1), (181, 1), (183, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (201, 2), (202, 1), (209, 1), (217, 1), (218, 2), (219, 1), (224, 1), (226, 1), (231, 1), (232, 3), (234, 1), (237, 1), (239, 1), (241, 2), (245, 1), (249, 1), (251, 1), (254, 1), (266, 1), (274, 1), (275, 2), (276, 1), (280, 1), (281, 2), (287, 1), (307, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  0   1 191 192 193 194 195 196 197]
Re-initialized the encoder parameters.
Fitting a model of length 408 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 1733.1866 - loglik: -1.7304e+03 - logprior: -2.8249e+00
Epoch 2/2
39/39 - 70s - loss: 1718.4093 - loglik: -1.7171e+03 - logprior: -1.3065e+00
Fitted a model with MAP estimate = -1713.8844
expansions: [(4, 1), (5, 1), (347, 1)]
discards: [  0   1  67 107 163 208 209 220 243 244 245 246 247 248 268 288 303 348
 349 353]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1725.0597 - loglik: -1.7230e+03 - logprior: -2.0144e+00
Epoch 2/2
39/39 - 68s - loss: 1720.4810 - loglik: -1.7202e+03 - logprior: -3.2278e-01
Fitted a model with MAP estimate = -1716.0411
expansions: [(4, 1), (5, 1)]
discards: [  0   1 211 290]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1722.1246 - loglik: -1.7204e+03 - logprior: -1.7491e+00
Epoch 2/10
39/39 - 68s - loss: 1716.7628 - loglik: -1.7170e+03 - logprior: 0.1933
Epoch 3/10
39/39 - 71s - loss: 1715.1534 - loglik: -1.7154e+03 - logprior: 0.2476
Epoch 4/10
39/39 - 76s - loss: 1713.6672 - loglik: -1.7140e+03 - logprior: 0.3684
Epoch 5/10
39/39 - 76s - loss: 1714.1575 - loglik: -1.7145e+03 - logprior: 0.3292
Fitted a model with MAP estimate = -1712.1028
Time for alignment: 1289.2161
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 1894.5879 - loglik: -1.8923e+03 - logprior: -2.2702e+00
Epoch 2/10
39/39 - 55s - loss: 1757.8112 - loglik: -1.7562e+03 - logprior: -1.6148e+00
Epoch 3/10
39/39 - 57s - loss: 1744.9209 - loglik: -1.7432e+03 - logprior: -1.7311e+00
Epoch 4/10
39/39 - 60s - loss: 1743.4509 - loglik: -1.7416e+03 - logprior: -1.8534e+00
Epoch 5/10
39/39 - 59s - loss: 1741.8984 - loglik: -1.7399e+03 - logprior: -2.0117e+00
Epoch 6/10
39/39 - 60s - loss: 1741.4078 - loglik: -1.7392e+03 - logprior: -2.1904e+00
Epoch 7/10
39/39 - 59s - loss: 1740.6208 - loglik: -1.7383e+03 - logprior: -2.3327e+00
Epoch 8/10
39/39 - 57s - loss: 1740.5507 - loglik: -1.7380e+03 - logprior: -2.5348e+00
Epoch 9/10
39/39 - 56s - loss: 1741.3165 - loglik: -1.7387e+03 - logprior: -2.6213e+00
Fitted a model with MAP estimate = -1738.6166
expansions: [(20, 2), (21, 1), (31, 1), (66, 1), (68, 1), (100, 1), (103, 2), (104, 1), (105, 1), (113, 1), (118, 1), (119, 1), (134, 1), (136, 1), (138, 1), (139, 1), (140, 3), (141, 3), (156, 1), (158, 1), (159, 1), (160, 1), (164, 1), (171, 1), (173, 1), (174, 5), (177, 1), (178, 6), (179, 1), (181, 1), (182, 1), (183, 1), (184, 1), (186, 1), (187, 4), (209, 2), (217, 2), (218, 1), (219, 1), (227, 1), (232, 1), (233, 3), (239, 1), (241, 2), (242, 2), (246, 1), (250, 1), (252, 1), (255, 1), (276, 1), (278, 2), (281, 1), (282, 2), (283, 2), (289, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 195 196 197 198 199 200 201 202 203 204 205 206 207 214]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 1733.0360 - loglik: -1.7310e+03 - logprior: -2.0819e+00
Epoch 2/2
39/39 - 84s - loss: 1718.1150 - loglik: -1.7176e+03 - logprior: -5.4610e-01
Fitted a model with MAP estimate = -1714.0827
expansions: [(110, 1), (251, 2)]
discards: [159 206 207 218 219 220 221 222 223 279 280 293 294 345 346]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 1724.8365 - loglik: -1.7236e+03 - logprior: -1.2105e+00
Epoch 2/2
39/39 - 76s - loss: 1720.5393 - loglik: -1.7205e+03 - logprior: -3.1352e-02
Fitted a model with MAP estimate = -1716.7617
expansions: [(5, 1), (253, 2)]
discards: [216]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 1720.5846 - loglik: -1.7195e+03 - logprior: -1.0498e+00
Epoch 2/10
39/39 - 80s - loss: 1716.5966 - loglik: -1.7169e+03 - logprior: 0.2722
Epoch 3/10
39/39 - 77s - loss: 1717.3719 - loglik: -1.7177e+03 - logprior: 0.3265
Fitted a model with MAP estimate = -1714.2896
Time for alignment: 1442.5676
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 1895.2522 - loglik: -1.8930e+03 - logprior: -2.2539e+00
Epoch 2/10
39/39 - 57s - loss: 1757.9557 - loglik: -1.7564e+03 - logprior: -1.5305e+00
Epoch 3/10
39/39 - 57s - loss: 1747.0959 - loglik: -1.7454e+03 - logprior: -1.6834e+00
Epoch 4/10
39/39 - 57s - loss: 1745.2216 - loglik: -1.7435e+03 - logprior: -1.7431e+00
Epoch 5/10
39/39 - 58s - loss: 1742.7096 - loglik: -1.7408e+03 - logprior: -1.9184e+00
Epoch 6/10
39/39 - 57s - loss: 1742.9122 - loglik: -1.7408e+03 - logprior: -2.1185e+00
Fitted a model with MAP estimate = -1739.6542
expansions: [(9, 1), (19, 2), (34, 1), (67, 1), (69, 1), (100, 2), (103, 1), (106, 1), (107, 1), (115, 1), (121, 1), (140, 2), (141, 1), (142, 1), (143, 3), (144, 2), (145, 1), (160, 2), (161, 1), (162, 1), (163, 1), (168, 1), (177, 1), (178, 5), (181, 1), (182, 2), (189, 1), (190, 1), (191, 1), (192, 1), (193, 3), (205, 1), (212, 2), (220, 1), (221, 2), (222, 1), (227, 1), (229, 1), (234, 3), (237, 1), (240, 1), (242, 2), (243, 2), (245, 2), (255, 1), (276, 1), (277, 1), (278, 1), (280, 1), (281, 2), (282, 3), (296, 1), (306, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  0   1 194 195 196 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 1733.4974 - loglik: -1.7308e+03 - logprior: -2.6887e+00
Epoch 2/2
39/39 - 80s - loss: 1721.1825 - loglik: -1.7200e+03 - logprior: -1.1470e+00
Fitted a model with MAP estimate = -1715.8419
expansions: [(4, 1), (5, 1), (299, 2)]
discards: [  0   1 160 162 208 209 237 238 239 249 260 295 296 342 343]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 1726.8451 - loglik: -1.7248e+03 - logprior: -2.0093e+00
Epoch 2/2
39/39 - 66s - loss: 1720.5785 - loglik: -1.7202e+03 - logprior: -3.3427e-01
Fitted a model with MAP estimate = -1716.8925
expansions: [(4, 1), (285, 2)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 1724.1090 - loglik: -1.7223e+03 - logprior: -1.7676e+00
Epoch 2/10
39/39 - 63s - loss: 1719.6897 - loglik: -1.7191e+03 - logprior: -6.2227e-01
Epoch 3/10
39/39 - 63s - loss: 1718.6735 - loglik: -1.7187e+03 - logprior: 0.0682
Epoch 4/10
39/39 - 63s - loss: 1716.2537 - loglik: -1.7166e+03 - logprior: 0.3001
Epoch 5/10
39/39 - 63s - loss: 1714.4279 - loglik: -1.7149e+03 - logprior: 0.4595
Epoch 6/10
39/39 - 65s - loss: 1717.6324 - loglik: -1.7180e+03 - logprior: 0.3438
Fitted a model with MAP estimate = -1714.1033
Time for alignment: 1322.1168
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 1892.6031 - loglik: -1.8904e+03 - logprior: -2.2243e+00
Epoch 2/10
39/39 - 51s - loss: 1757.3516 - loglik: -1.7558e+03 - logprior: -1.5113e+00
Epoch 3/10
39/39 - 51s - loss: 1747.0360 - loglik: -1.7454e+03 - logprior: -1.6773e+00
Epoch 4/10
39/39 - 50s - loss: 1745.6829 - loglik: -1.7439e+03 - logprior: -1.7515e+00
Epoch 5/10
39/39 - 51s - loss: 1741.5409 - loglik: -1.7396e+03 - logprior: -1.9586e+00
Epoch 6/10
39/39 - 52s - loss: 1743.1650 - loglik: -1.7410e+03 - logprior: -2.1607e+00
Fitted a model with MAP estimate = -1739.9696
expansions: [(7, 1), (19, 2), (20, 1), (67, 1), (69, 1), (101, 2), (104, 2), (106, 1), (107, 1), (108, 1), (115, 1), (120, 1), (139, 2), (140, 1), (141, 1), (143, 2), (144, 2), (145, 1), (160, 2), (161, 1), (162, 1), (163, 1), (167, 1), (176, 2), (177, 1), (178, 3), (181, 6), (182, 1), (184, 1), (186, 1), (188, 1), (189, 3), (190, 2), (191, 1), (203, 2), (205, 2), (212, 1), (220, 1), (221, 1), (222, 1), (227, 1), (235, 1), (236, 3), (242, 1), (244, 1), (246, 2), (248, 1), (249, 1), (253, 1), (254, 2), (257, 1), (278, 1), (280, 2), (281, 1), (282, 1), (283, 5), (284, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 193 194 195 196 197 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 408 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1733.3966 - loglik: -1.7307e+03 - logprior: -2.7236e+00
Epoch 2/2
39/39 - 73s - loss: 1719.6835 - loglik: -1.7185e+03 - logprior: -1.1832e+00
Fitted a model with MAP estimate = -1715.4539
expansions: [(4, 1), (5, 1), (237, 4), (238, 1), (252, 2)]
discards: [  0   1 105 161 163 209 210 219 220 221 288 304 353 354]
Re-initialized the encoder parameters.
Fitting a model of length 403 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1723.7676 - loglik: -1.7217e+03 - logprior: -2.0715e+00
Epoch 2/2
39/39 - 67s - loss: 1717.9952 - loglik: -1.7177e+03 - logprior: -3.3701e-01
Fitted a model with MAP estimate = -1713.3799
expansions: [(213, 1), (247, 1)]
discards: [235 236 237 238 239 240 241 242 243 244 251 252 253 302]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 1722.2582 - loglik: -1.7212e+03 - logprior: -1.0159e+00
Epoch 2/10
39/39 - 64s - loss: 1717.0449 - loglik: -1.7172e+03 - logprior: 0.2000
Epoch 3/10
39/39 - 65s - loss: 1715.8253 - loglik: -1.7162e+03 - logprior: 0.3619
Epoch 4/10
39/39 - 66s - loss: 1713.2603 - loglik: -1.7136e+03 - logprior: 0.3189
Epoch 5/10
39/39 - 65s - loss: 1712.5100 - loglik: -1.7127e+03 - logprior: 0.2348
Epoch 6/10
39/39 - 66s - loss: 1711.4138 - loglik: -1.7117e+03 - logprior: 0.2628
Epoch 7/10
39/39 - 66s - loss: 1712.6722 - loglik: -1.7130e+03 - logprior: 0.2937
Fitted a model with MAP estimate = -1711.3088
Time for alignment: 1335.4141
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1895.0232 - loglik: -1.8928e+03 - logprior: -2.2514e+00
Epoch 2/10
39/39 - 50s - loss: 1759.0122 - loglik: -1.7575e+03 - logprior: -1.5601e+00
Epoch 3/10
39/39 - 50s - loss: 1746.3552 - loglik: -1.7446e+03 - logprior: -1.7541e+00
Epoch 4/10
39/39 - 50s - loss: 1746.0112 - loglik: -1.7442e+03 - logprior: -1.7813e+00
Epoch 5/10
39/39 - 50s - loss: 1742.9166 - loglik: -1.7410e+03 - logprior: -1.9653e+00
Epoch 6/10
39/39 - 49s - loss: 1743.2209 - loglik: -1.7411e+03 - logprior: -2.1337e+00
Fitted a model with MAP estimate = -1741.0348
expansions: [(9, 1), (19, 1), (21, 1), (31, 1), (66, 1), (68, 1), (100, 1), (103, 2), (104, 1), (105, 1), (113, 1), (118, 1), (119, 1), (137, 2), (138, 1), (139, 1), (140, 3), (141, 2), (142, 1), (157, 1), (159, 2), (160, 1), (161, 2), (174, 3), (175, 1), (176, 1), (179, 6), (180, 1), (181, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (202, 2), (203, 1), (210, 2), (218, 2), (219, 2), (220, 1), (225, 1), (233, 3), (236, 1), (239, 1), (241, 1), (245, 1), (251, 1), (253, 1), (256, 1), (276, 1), (278, 3), (281, 3), (282, 2), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 192 193 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 1735.4147 - loglik: -1.7326e+03 - logprior: -2.8224e+00
Epoch 2/2
39/39 - 70s - loss: 1720.8221 - loglik: -1.7195e+03 - logprior: -1.2811e+00
Fitted a model with MAP estimate = -1716.8695
expansions: [(4, 1), (5, 1), (109, 1), (296, 1)]
discards: [  0   1 157 160 187 203 213 214 215 216 239 240 241 242 243 253 263 265
 345 346]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 1726.8500 - loglik: -1.7249e+03 - logprior: -1.9946e+00
Epoch 2/2
39/39 - 66s - loss: 1721.3475 - loglik: -1.7209e+03 - logprior: -4.0004e-01
Fitted a model with MAP estimate = -1717.0809
expansions: [(4, 2)]
discards: [  0   1 101]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1723.8740 - loglik: -1.7221e+03 - logprior: -1.7604e+00
Epoch 2/10
39/39 - 72s - loss: 1718.5363 - loglik: -1.7187e+03 - logprior: 0.1221
Epoch 3/10
39/39 - 74s - loss: 1716.2944 - loglik: -1.7165e+03 - logprior: 0.2133
Epoch 4/10
39/39 - 76s - loss: 1715.4659 - loglik: -1.7156e+03 - logprior: 0.1154
Epoch 5/10
39/39 - 77s - loss: 1714.6672 - loglik: -1.7148e+03 - logprior: 0.1580
Epoch 6/10
39/39 - 75s - loss: 1713.1971 - loglik: -1.7133e+03 - logprior: 0.1526
Epoch 7/10
39/39 - 81s - loss: 1715.2174 - loglik: -1.7153e+03 - logprior: 0.1070
Fitted a model with MAP estimate = -1712.9309
Time for alignment: 1378.6975
Computed alignments with likelihoods: ['-1712.1028', '-1714.0827', '-1714.1033', '-1711.3088', '-1712.9309']
Best model has likelihood: -1711.3088  (prior= 0.2124 )
time for generating output: 0.4975
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8214914312181566
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe940312070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff142538310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefcbeecd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9041c91f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefcbd9df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9106f7280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe92006a370>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feefdf6ff40>, <__main__.SimpleDirichletPrior object at 0x7fe94186af10>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 511.3181 - loglik: -5.0794e+02 - logprior: -3.3738e+00
Epoch 2/10
19/19 - 2s - loss: 479.4011 - loglik: -4.7811e+02 - logprior: -1.2876e+00
Epoch 3/10
19/19 - 2s - loss: 466.3918 - loglik: -4.6496e+02 - logprior: -1.4360e+00
Epoch 4/10
19/19 - 2s - loss: 464.1769 - loglik: -4.6284e+02 - logprior: -1.3385e+00
Epoch 5/10
19/19 - 2s - loss: 463.5654 - loglik: -4.6217e+02 - logprior: -1.3918e+00
Epoch 6/10
19/19 - 2s - loss: 463.1345 - loglik: -4.6173e+02 - logprior: -1.4077e+00
Epoch 7/10
19/19 - 2s - loss: 463.1732 - loglik: -4.6174e+02 - logprior: -1.4374e+00
Fitted a model with MAP estimate = -462.5771
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (14, 1), (19, 1), (20, 1), (34, 1), (39, 1), (40, 1), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 465.5524 - loglik: -4.6153e+02 - logprior: -4.0180e+00
Epoch 2/2
19/19 - 2s - loss: 458.4755 - loglik: -4.5649e+02 - logprior: -1.9854e+00
Fitted a model with MAP estimate = -456.8990
expansions: [(0, 2)]
discards: [ 0 10 65]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 458.8770 - loglik: -4.5600e+02 - logprior: -2.8772e+00
Epoch 2/2
19/19 - 2s - loss: 455.7892 - loglik: -4.5468e+02 - logprior: -1.1090e+00
Fitted a model with MAP estimate = -455.0705
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.0765 - loglik: -4.5637e+02 - logprior: -3.7023e+00
Epoch 2/10
19/19 - 2s - loss: 455.7346 - loglik: -4.5456e+02 - logprior: -1.1702e+00
Epoch 3/10
19/19 - 2s - loss: 454.6946 - loglik: -4.5374e+02 - logprior: -9.5577e-01
Epoch 4/10
19/19 - 2s - loss: 454.2815 - loglik: -4.5334e+02 - logprior: -9.4112e-01
Epoch 5/10
19/19 - 2s - loss: 454.0866 - loglik: -4.5314e+02 - logprior: -9.4952e-01
Epoch 6/10
19/19 - 2s - loss: 453.4313 - loglik: -4.5246e+02 - logprior: -9.6781e-01
Epoch 7/10
19/19 - 2s - loss: 453.3065 - loglik: -4.5232e+02 - logprior: -9.8893e-01
Epoch 8/10
19/19 - 2s - loss: 452.9047 - loglik: -4.5191e+02 - logprior: -9.9932e-01
Epoch 9/10
19/19 - 2s - loss: 453.1414 - loglik: -4.5212e+02 - logprior: -1.0197e+00
Fitted a model with MAP estimate = -452.6960
Time for alignment: 76.1132
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 511.4762 - loglik: -5.0811e+02 - logprior: -3.3663e+00
Epoch 2/10
19/19 - 2s - loss: 480.9510 - loglik: -4.7968e+02 - logprior: -1.2743e+00
Epoch 3/10
19/19 - 2s - loss: 467.8296 - loglik: -4.6636e+02 - logprior: -1.4652e+00
Epoch 4/10
19/19 - 2s - loss: 464.3069 - loglik: -4.6292e+02 - logprior: -1.3916e+00
Epoch 5/10
19/19 - 2s - loss: 463.5032 - loglik: -4.6206e+02 - logprior: -1.4419e+00
Epoch 6/10
19/19 - 2s - loss: 462.9319 - loglik: -4.6148e+02 - logprior: -1.4548e+00
Epoch 7/10
19/19 - 2s - loss: 462.9940 - loglik: -4.6150e+02 - logprior: -1.4929e+00
Fitted a model with MAP estimate = -462.5435
expansions: [(10, 2), (11, 3), (13, 1), (14, 1), (15, 1), (32, 2), (37, 2), (39, 1), (40, 1), (43, 2), (47, 1), (48, 2), (49, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 466.6432 - loglik: -4.6259e+02 - logprior: -4.0485e+00
Epoch 2/2
19/19 - 2s - loss: 458.6143 - loglik: -4.5657e+02 - logprior: -2.0407e+00
Fitted a model with MAP estimate = -457.0627
expansions: [(0, 2), (56, 1)]
discards: [ 0 13 40 46 64]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.0406 - loglik: -4.5615e+02 - logprior: -2.8896e+00
Epoch 2/2
19/19 - 2s - loss: 455.5305 - loglik: -4.5443e+02 - logprior: -1.1040e+00
Fitted a model with MAP estimate = -454.8211
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 459.8773 - loglik: -4.5619e+02 - logprior: -3.6886e+00
Epoch 2/10
19/19 - 2s - loss: 455.7400 - loglik: -4.5458e+02 - logprior: -1.1630e+00
Epoch 3/10
19/19 - 2s - loss: 454.8046 - loglik: -4.5385e+02 - logprior: -9.5752e-01
Epoch 4/10
19/19 - 2s - loss: 454.2708 - loglik: -4.5333e+02 - logprior: -9.4429e-01
Epoch 5/10
19/19 - 2s - loss: 453.7596 - loglik: -4.5281e+02 - logprior: -9.4994e-01
Epoch 6/10
19/19 - 2s - loss: 453.7067 - loglik: -4.5273e+02 - logprior: -9.7676e-01
Epoch 7/10
19/19 - 2s - loss: 453.1487 - loglik: -4.5217e+02 - logprior: -9.8366e-01
Epoch 8/10
19/19 - 2s - loss: 452.8470 - loglik: -4.5184e+02 - logprior: -1.0071e+00
Epoch 9/10
19/19 - 2s - loss: 452.6775 - loglik: -4.5165e+02 - logprior: -1.0247e+00
Epoch 10/10
19/19 - 2s - loss: 453.0052 - loglik: -4.5194e+02 - logprior: -1.0641e+00
Fitted a model with MAP estimate = -452.5346
Time for alignment: 78.1374
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 511.2271 - loglik: -5.0786e+02 - logprior: -3.3714e+00
Epoch 2/10
19/19 - 2s - loss: 480.8021 - loglik: -4.7951e+02 - logprior: -1.2964e+00
Epoch 3/10
19/19 - 2s - loss: 466.7119 - loglik: -4.6525e+02 - logprior: -1.4665e+00
Epoch 4/10
19/19 - 2s - loss: 462.9833 - loglik: -4.6161e+02 - logprior: -1.3762e+00
Epoch 5/10
19/19 - 2s - loss: 462.7574 - loglik: -4.6133e+02 - logprior: -1.4282e+00
Epoch 6/10
19/19 - 2s - loss: 462.1411 - loglik: -4.6068e+02 - logprior: -1.4575e+00
Epoch 7/10
19/19 - 2s - loss: 461.9049 - loglik: -4.6043e+02 - logprior: -1.4787e+00
Epoch 8/10
19/19 - 2s - loss: 461.6727 - loglik: -4.6018e+02 - logprior: -1.4975e+00
Epoch 9/10
19/19 - 2s - loss: 461.8880 - loglik: -4.6036e+02 - logprior: -1.5246e+00
Fitted a model with MAP estimate = -461.2761
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (14, 1), (19, 1), (24, 1), (37, 2), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 466.0677 - loglik: -4.6202e+02 - logprior: -4.0472e+00
Epoch 2/2
19/19 - 2s - loss: 458.6196 - loglik: -4.5662e+02 - logprior: -1.9984e+00
Fitted a model with MAP estimate = -457.1172
expansions: [(0, 2)]
discards: [ 0 10 45 52 66]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 458.9981 - loglik: -4.5612e+02 - logprior: -2.8761e+00
Epoch 2/2
19/19 - 2s - loss: 455.8495 - loglik: -4.5474e+02 - logprior: -1.1088e+00
Fitted a model with MAP estimate = -455.0560
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.1077 - loglik: -4.5640e+02 - logprior: -3.7038e+00
Epoch 2/10
19/19 - 2s - loss: 455.7520 - loglik: -4.5457e+02 - logprior: -1.1792e+00
Epoch 3/10
19/19 - 2s - loss: 454.7993 - loglik: -4.5385e+02 - logprior: -9.4649e-01
Epoch 4/10
19/19 - 2s - loss: 454.2621 - loglik: -4.5333e+02 - logprior: -9.2919e-01
Epoch 5/10
19/19 - 2s - loss: 454.1147 - loglik: -4.5317e+02 - logprior: -9.4395e-01
Epoch 6/10
19/19 - 2s - loss: 453.3464 - loglik: -4.5238e+02 - logprior: -9.6954e-01
Epoch 7/10
19/19 - 2s - loss: 453.0498 - loglik: -4.5208e+02 - logprior: -9.7252e-01
Epoch 8/10
19/19 - 2s - loss: 453.3452 - loglik: -4.5235e+02 - logprior: -9.9179e-01
Fitted a model with MAP estimate = -452.8498
Time for alignment: 76.2323
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 511.5590 - loglik: -5.0819e+02 - logprior: -3.3711e+00
Epoch 2/10
19/19 - 2s - loss: 480.5439 - loglik: -4.7923e+02 - logprior: -1.3148e+00
Epoch 3/10
19/19 - 2s - loss: 467.4276 - loglik: -4.6598e+02 - logprior: -1.4452e+00
Epoch 4/10
19/19 - 2s - loss: 465.0854 - loglik: -4.6376e+02 - logprior: -1.3225e+00
Epoch 5/10
19/19 - 2s - loss: 464.3011 - loglik: -4.6293e+02 - logprior: -1.3669e+00
Epoch 6/10
19/19 - 2s - loss: 463.7982 - loglik: -4.6242e+02 - logprior: -1.3750e+00
Epoch 7/10
19/19 - 2s - loss: 463.8271 - loglik: -4.6242e+02 - logprior: -1.4058e+00
Fitted a model with MAP estimate = -463.2237
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (21, 1), (24, 1), (34, 1), (39, 1), (40, 1), (43, 2), (48, 1), (49, 2), (60, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 466.2851 - loglik: -4.6228e+02 - logprior: -4.0040e+00
Epoch 2/2
19/19 - 2s - loss: 458.6574 - loglik: -4.5668e+02 - logprior: -1.9780e+00
Fitted a model with MAP estimate = -456.9165
expansions: [(0, 2)]
discards: [ 0 10 64]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.0709 - loglik: -4.5619e+02 - logprior: -2.8784e+00
Epoch 2/2
19/19 - 2s - loss: 455.7791 - loglik: -4.5467e+02 - logprior: -1.1088e+00
Fitted a model with MAP estimate = -455.0402
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 459.9857 - loglik: -4.5629e+02 - logprior: -3.7005e+00
Epoch 2/10
19/19 - 2s - loss: 455.7673 - loglik: -4.5460e+02 - logprior: -1.1717e+00
Epoch 3/10
19/19 - 2s - loss: 454.8097 - loglik: -4.5386e+02 - logprior: -9.4598e-01
Epoch 4/10
19/19 - 2s - loss: 454.3861 - loglik: -4.5344e+02 - logprior: -9.4411e-01
Epoch 5/10
19/19 - 2s - loss: 453.9035 - loglik: -4.5296e+02 - logprior: -9.4727e-01
Epoch 6/10
19/19 - 2s - loss: 453.5307 - loglik: -4.5256e+02 - logprior: -9.6828e-01
Epoch 7/10
19/19 - 2s - loss: 452.9308 - loglik: -4.5195e+02 - logprior: -9.7801e-01
Epoch 8/10
19/19 - 2s - loss: 453.3884 - loglik: -4.5239e+02 - logprior: -9.9921e-01
Fitted a model with MAP estimate = -452.8615
Time for alignment: 70.9176
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 511.0595 - loglik: -5.0769e+02 - logprior: -3.3693e+00
Epoch 2/10
19/19 - 2s - loss: 477.4549 - loglik: -4.7619e+02 - logprior: -1.2600e+00
Epoch 3/10
19/19 - 2s - loss: 465.4643 - loglik: -4.6406e+02 - logprior: -1.4021e+00
Epoch 4/10
19/19 - 2s - loss: 463.1737 - loglik: -4.6187e+02 - logprior: -1.3068e+00
Epoch 5/10
19/19 - 2s - loss: 462.1990 - loglik: -4.6082e+02 - logprior: -1.3761e+00
Epoch 6/10
19/19 - 2s - loss: 462.2172 - loglik: -4.6082e+02 - logprior: -1.3972e+00
Fitted a model with MAP estimate = -461.5854
expansions: [(8, 4), (9, 3), (19, 1), (20, 1), (34, 1), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 465.9483 - loglik: -4.6191e+02 - logprior: -4.0411e+00
Epoch 2/2
19/19 - 2s - loss: 458.3948 - loglik: -4.5639e+02 - logprior: -2.0026e+00
Fitted a model with MAP estimate = -456.7986
expansions: [(0, 2)]
discards: [ 0 13 51 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 459.1030 - loglik: -4.5622e+02 - logprior: -2.8801e+00
Epoch 2/2
19/19 - 2s - loss: 455.8753 - loglik: -4.5477e+02 - logprior: -1.1050e+00
Fitted a model with MAP estimate = -455.0278
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 460.0299 - loglik: -4.5632e+02 - logprior: -3.7092e+00
Epoch 2/10
19/19 - 2s - loss: 455.8911 - loglik: -4.5471e+02 - logprior: -1.1835e+00
Epoch 3/10
19/19 - 2s - loss: 454.8262 - loglik: -4.5387e+02 - logprior: -9.5162e-01
Epoch 4/10
19/19 - 2s - loss: 454.1948 - loglik: -4.5326e+02 - logprior: -9.3881e-01
Epoch 5/10
19/19 - 2s - loss: 453.8878 - loglik: -4.5295e+02 - logprior: -9.3885e-01
Epoch 6/10
19/19 - 2s - loss: 453.5779 - loglik: -4.5261e+02 - logprior: -9.7261e-01
Epoch 7/10
19/19 - 2s - loss: 453.0620 - loglik: -4.5208e+02 - logprior: -9.8583e-01
Epoch 8/10
19/19 - 2s - loss: 453.1856 - loglik: -4.5219e+02 - logprior: -9.9682e-01
Fitted a model with MAP estimate = -452.8675
Time for alignment: 69.8886
Computed alignments with likelihoods: ['-452.6960', '-452.5346', '-452.8498', '-452.8615', '-452.8675']
Best model has likelihood: -452.5346  (prior= -1.0743 )
time for generating output: 0.1726
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.8255813953488372
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe910467be0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941b31e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1182ed880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff115d3ceb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff115d3c520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe94087dfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe950401670>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff1431b6670>, <__main__.SimpleDirichletPrior object at 0x7ff113e77a00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 610.1942 - loglik: -6.0681e+02 - logprior: -3.3887e+00
Epoch 2/10
19/19 - 2s - loss: 579.4845 - loglik: -5.7825e+02 - logprior: -1.2295e+00
Epoch 3/10
19/19 - 2s - loss: 567.4252 - loglik: -5.6633e+02 - logprior: -1.0997e+00
Epoch 4/10
19/19 - 2s - loss: 564.8517 - loglik: -5.6384e+02 - logprior: -1.0117e+00
Epoch 5/10
19/19 - 2s - loss: 562.7490 - loglik: -5.6172e+02 - logprior: -1.0318e+00
Epoch 6/10
19/19 - 2s - loss: 563.0569 - loglik: -5.6199e+02 - logprior: -1.0680e+00
Fitted a model with MAP estimate = -560.7901
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (41, 3), (42, 1), (44, 2), (53, 3), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 567.0315 - loglik: -5.6282e+02 - logprior: -4.2112e+00
Epoch 2/2
19/19 - 3s - loss: 560.7441 - loglik: -5.5952e+02 - logprior: -1.2277e+00
Fitted a model with MAP estimate = -558.4329
expansions: [(52, 1)]
discards: [ 1 58 98]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 562.9009 - loglik: -5.5997e+02 - logprior: -2.9290e+00
Epoch 2/2
19/19 - 3s - loss: 559.7440 - loglik: -5.5871e+02 - logprior: -1.0368e+00
Fitted a model with MAP estimate = -557.8386
expansions: [(0, 1)]
discards: [94]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 562.8369 - loglik: -5.5817e+02 - logprior: -4.6661e+00
Epoch 2/10
19/19 - 3s - loss: 559.0678 - loglik: -5.5776e+02 - logprior: -1.3092e+00
Epoch 3/10
19/19 - 3s - loss: 558.0342 - loglik: -5.5712e+02 - logprior: -9.1828e-01
Epoch 4/10
19/19 - 3s - loss: 557.3147 - loglik: -5.5637e+02 - logprior: -9.4520e-01
Epoch 5/10
19/19 - 3s - loss: 556.1170 - loglik: -5.5515e+02 - logprior: -9.7049e-01
Epoch 6/10
19/19 - 3s - loss: 554.9174 - loglik: -5.5393e+02 - logprior: -9.8299e-01
Epoch 7/10
19/19 - 3s - loss: 555.2220 - loglik: -5.5423e+02 - logprior: -9.9087e-01
Fitted a model with MAP estimate = -554.5207
Time for alignment: 81.8863
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 609.7534 - loglik: -6.0637e+02 - logprior: -3.3825e+00
Epoch 2/10
19/19 - 2s - loss: 578.8234 - loglik: -5.7758e+02 - logprior: -1.2392e+00
Epoch 3/10
19/19 - 2s - loss: 566.7137 - loglik: -5.6561e+02 - logprior: -1.1028e+00
Epoch 4/10
19/19 - 3s - loss: 564.3569 - loglik: -5.6332e+02 - logprior: -1.0361e+00
Epoch 5/10
19/19 - 2s - loss: 563.4092 - loglik: -5.6237e+02 - logprior: -1.0434e+00
Epoch 6/10
19/19 - 3s - loss: 562.6614 - loglik: -5.6156e+02 - logprior: -1.0980e+00
Epoch 7/10
19/19 - 2s - loss: 561.8779 - loglik: -5.6073e+02 - logprior: -1.1509e+00
Epoch 8/10
19/19 - 3s - loss: 562.4920 - loglik: -5.6132e+02 - logprior: -1.1718e+00
Fitted a model with MAP estimate = -560.1403
expansions: [(0, 2), (20, 3), (21, 2), (22, 1), (23, 1), (41, 3), (42, 1), (44, 1), (45, 1), (51, 1), (52, 2), (53, 1), (75, 3), (76, 2), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 568.1055 - loglik: -5.6378e+02 - logprior: -4.3288e+00
Epoch 2/2
19/19 - 3s - loss: 561.5371 - loglik: -5.6032e+02 - logprior: -1.2163e+00
Fitted a model with MAP estimate = -559.1198
expansions: []
discards: [ 1 26 69]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 563.5604 - loglik: -5.6068e+02 - logprior: -2.8796e+00
Epoch 2/2
19/19 - 3s - loss: 560.4744 - loglik: -5.5943e+02 - logprior: -1.0444e+00
Fitted a model with MAP estimate = -558.5716
expansions: [(22, 2), (49, 1)]
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 561.4843 - loglik: -5.5866e+02 - logprior: -2.8237e+00
Epoch 2/10
19/19 - 3s - loss: 558.9504 - loglik: -5.5797e+02 - logprior: -9.7956e-01
Epoch 3/10
19/19 - 3s - loss: 557.9404 - loglik: -5.5710e+02 - logprior: -8.3877e-01
Epoch 4/10
19/19 - 3s - loss: 557.7894 - loglik: -5.5695e+02 - logprior: -8.4434e-01
Epoch 5/10
19/19 - 3s - loss: 556.1432 - loglik: -5.5529e+02 - logprior: -8.5198e-01
Epoch 6/10
19/19 - 3s - loss: 555.7690 - loglik: -5.5493e+02 - logprior: -8.3447e-01
Epoch 7/10
19/19 - 3s - loss: 555.6389 - loglik: -5.5481e+02 - logprior: -8.3042e-01
Epoch 8/10
19/19 - 3s - loss: 553.7028 - loglik: -5.5284e+02 - logprior: -8.6760e-01
Epoch 9/10
19/19 - 3s - loss: 554.7192 - loglik: -5.5382e+02 - logprior: -8.9525e-01
Fitted a model with MAP estimate = -553.9061
Time for alignment: 93.2202
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 609.7618 - loglik: -6.0637e+02 - logprior: -3.3900e+00
Epoch 2/10
19/19 - 3s - loss: 577.8376 - loglik: -5.7661e+02 - logprior: -1.2243e+00
Epoch 3/10
19/19 - 3s - loss: 567.1511 - loglik: -5.6606e+02 - logprior: -1.0930e+00
Epoch 4/10
19/19 - 2s - loss: 563.9933 - loglik: -5.6295e+02 - logprior: -1.0461e+00
Epoch 5/10
19/19 - 3s - loss: 563.4683 - loglik: -5.6241e+02 - logprior: -1.0590e+00
Epoch 6/10
19/19 - 3s - loss: 562.3691 - loglik: -5.6126e+02 - logprior: -1.1060e+00
Epoch 7/10
19/19 - 3s - loss: 562.8514 - loglik: -5.6168e+02 - logprior: -1.1699e+00
Fitted a model with MAP estimate = -560.3880
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (32, 1), (39, 2), (42, 3), (44, 1), (45, 1), (53, 2), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 567.3274 - loglik: -5.6305e+02 - logprior: -4.2740e+00
Epoch 2/2
19/19 - 3s - loss: 561.4454 - loglik: -5.6024e+02 - logprior: -1.2033e+00
Fitted a model with MAP estimate = -558.6741
expansions: []
discards: [ 1  2 27 50 71 97]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 563.3394 - loglik: -5.6041e+02 - logprior: -2.9279e+00
Epoch 2/2
19/19 - 3s - loss: 560.1095 - loglik: -5.5909e+02 - logprior: -1.0177e+00
Fitted a model with MAP estimate = -558.3427
expansions: [(0, 1), (50, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 563.0590 - loglik: -5.5856e+02 - logprior: -4.4996e+00
Epoch 2/10
19/19 - 3s - loss: 558.9753 - loglik: -5.5771e+02 - logprior: -1.2662e+00
Epoch 3/10
19/19 - 3s - loss: 557.9950 - loglik: -5.5710e+02 - logprior: -8.9701e-01
Epoch 4/10
19/19 - 3s - loss: 556.4644 - loglik: -5.5563e+02 - logprior: -8.3737e-01
Epoch 5/10
19/19 - 3s - loss: 556.3857 - loglik: -5.5554e+02 - logprior: -8.4723e-01
Epoch 6/10
19/19 - 3s - loss: 555.0029 - loglik: -5.5416e+02 - logprior: -8.4401e-01
Epoch 7/10
19/19 - 3s - loss: 554.6924 - loglik: -5.5384e+02 - logprior: -8.5693e-01
Epoch 8/10
19/19 - 3s - loss: 554.2255 - loglik: -5.5333e+02 - logprior: -8.9374e-01
Epoch 9/10
19/19 - 3s - loss: 553.8376 - loglik: -5.5293e+02 - logprior: -9.0457e-01
Epoch 10/10
19/19 - 3s - loss: 553.9240 - loglik: -5.5298e+02 - logprior: -9.4721e-01
Fitted a model with MAP estimate = -553.2575
Time for alignment: 93.6885
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 609.8380 - loglik: -6.0644e+02 - logprior: -3.3971e+00
Epoch 2/10
19/19 - 2s - loss: 580.5934 - loglik: -5.7937e+02 - logprior: -1.2267e+00
Epoch 3/10
19/19 - 3s - loss: 567.4976 - loglik: -5.6638e+02 - logprior: -1.1135e+00
Epoch 4/10
19/19 - 2s - loss: 564.7281 - loglik: -5.6368e+02 - logprior: -1.0495e+00
Epoch 5/10
19/19 - 3s - loss: 562.8608 - loglik: -5.6180e+02 - logprior: -1.0571e+00
Epoch 6/10
19/19 - 3s - loss: 562.7192 - loglik: -5.6162e+02 - logprior: -1.0953e+00
Epoch 7/10
19/19 - 2s - loss: 561.9784 - loglik: -5.6083e+02 - logprior: -1.1464e+00
Epoch 8/10
19/19 - 3s - loss: 561.0807 - loglik: -5.5991e+02 - logprior: -1.1687e+00
Epoch 9/10
19/19 - 3s - loss: 561.1794 - loglik: -5.5996e+02 - logprior: -1.2154e+00
Fitted a model with MAP estimate = -559.6643
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (44, 1), (45, 1), (51, 1), (52, 2), (53, 1), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 568.5734 - loglik: -5.6419e+02 - logprior: -4.3838e+00
Epoch 2/2
19/19 - 3s - loss: 561.2375 - loglik: -5.6002e+02 - logprior: -1.2218e+00
Fitted a model with MAP estimate = -558.8025
expansions: []
discards: [ 1  2 71 97]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 563.2915 - loglik: -5.6035e+02 - logprior: -2.9460e+00
Epoch 2/2
19/19 - 3s - loss: 560.5038 - loglik: -5.5948e+02 - logprior: -1.0213e+00
Fitted a model with MAP estimate = -558.2883
expansions: [(0, 1)]
discards: [25 58]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 563.1174 - loglik: -5.5854e+02 - logprior: -4.5775e+00
Epoch 2/10
19/19 - 3s - loss: 558.8658 - loglik: -5.5757e+02 - logprior: -1.2916e+00
Epoch 3/10
19/19 - 3s - loss: 557.9792 - loglik: -5.5706e+02 - logprior: -9.1551e-01
Epoch 4/10
19/19 - 3s - loss: 556.6216 - loglik: -5.5577e+02 - logprior: -8.4764e-01
Epoch 5/10
19/19 - 3s - loss: 556.6536 - loglik: -5.5582e+02 - logprior: -8.3495e-01
Fitted a model with MAP estimate = -555.5670
Time for alignment: 83.4993
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 609.8430 - loglik: -6.0645e+02 - logprior: -3.3901e+00
Epoch 2/10
19/19 - 2s - loss: 583.0404 - loglik: -5.8181e+02 - logprior: -1.2263e+00
Epoch 3/10
19/19 - 2s - loss: 568.5041 - loglik: -5.6736e+02 - logprior: -1.1464e+00
Epoch 4/10
19/19 - 2s - loss: 565.0105 - loglik: -5.6395e+02 - logprior: -1.0555e+00
Epoch 5/10
19/19 - 3s - loss: 563.9455 - loglik: -5.6287e+02 - logprior: -1.0785e+00
Epoch 6/10
19/19 - 2s - loss: 562.9756 - loglik: -5.6183e+02 - logprior: -1.1493e+00
Epoch 7/10
19/19 - 3s - loss: 562.3603 - loglik: -5.6115e+02 - logprior: -1.2081e+00
Epoch 8/10
19/19 - 2s - loss: 561.5791 - loglik: -5.6036e+02 - logprior: -1.2223e+00
Epoch 9/10
19/19 - 2s - loss: 562.0926 - loglik: -5.6083e+02 - logprior: -1.2649e+00
Fitted a model with MAP estimate = -560.1375
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (41, 3), (42, 1), (44, 1), (45, 1), (51, 1), (52, 2), (53, 1), (55, 1), (74, 2), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 568.6148 - loglik: -5.6423e+02 - logprior: -4.3846e+00
Epoch 2/2
19/19 - 3s - loss: 561.3095 - loglik: -5.6009e+02 - logprior: -1.2168e+00
Fitted a model with MAP estimate = -558.9295
expansions: []
discards: [ 1  2 70 96]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 563.3597 - loglik: -5.6041e+02 - logprior: -2.9511e+00
Epoch 2/2
19/19 - 3s - loss: 560.4636 - loglik: -5.5944e+02 - logprior: -1.0242e+00
Fitted a model with MAP estimate = -558.2615
expansions: [(0, 1), (50, 1)]
discards: [26 57]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 563.0129 - loglik: -5.5852e+02 - logprior: -4.4920e+00
Epoch 2/10
19/19 - 3s - loss: 559.0044 - loglik: -5.5773e+02 - logprior: -1.2770e+00
Epoch 3/10
19/19 - 3s - loss: 557.5062 - loglik: -5.5659e+02 - logprior: -9.1695e-01
Epoch 4/10
19/19 - 3s - loss: 557.1850 - loglik: -5.5634e+02 - logprior: -8.4516e-01
Epoch 5/10
19/19 - 3s - loss: 556.2563 - loglik: -5.5541e+02 - logprior: -8.4583e-01
Epoch 6/10
19/19 - 3s - loss: 555.7110 - loglik: -5.5486e+02 - logprior: -8.5331e-01
Epoch 7/10
19/19 - 3s - loss: 554.8038 - loglik: -5.5394e+02 - logprior: -8.6271e-01
Epoch 8/10
19/19 - 3s - loss: 554.3079 - loglik: -5.5341e+02 - logprior: -8.9657e-01
Epoch 9/10
19/19 - 3s - loss: 553.8470 - loglik: -5.5292e+02 - logprior: -9.2556e-01
Epoch 10/10
19/19 - 3s - loss: 553.6703 - loglik: -5.5272e+02 - logprior: -9.5110e-01
Fitted a model with MAP estimate = -553.3975
Time for alignment: 96.9105
Computed alignments with likelihoods: ['-554.5207', '-553.9061', '-553.2575', '-555.5670', '-553.3975']
Best model has likelihood: -553.2575  (prior= -0.9730 )
time for generating output: 0.1659
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8822048465416158
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2dea91550>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef1c5467c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff15bd43400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff15bd43f40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2dee13040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe904d0f3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff118bffa30>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff114627640>, <__main__.SimpleDirichletPrior object at 0x7feefefbaca0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 383.8123 - loglik: -3.8037e+02 - logprior: -3.4439e+00
Epoch 2/10
19/19 - 2s - loss: 351.0378 - loglik: -3.4941e+02 - logprior: -1.6234e+00
Epoch 3/10
19/19 - 1s - loss: 338.0246 - loglik: -3.3647e+02 - logprior: -1.5507e+00
Epoch 4/10
19/19 - 1s - loss: 334.1783 - loglik: -3.3259e+02 - logprior: -1.5891e+00
Epoch 5/10
19/19 - 2s - loss: 333.1421 - loglik: -3.3158e+02 - logprior: -1.5631e+00
Epoch 6/10
19/19 - 2s - loss: 332.4498 - loglik: -3.3087e+02 - logprior: -1.5789e+00
Epoch 7/10
19/19 - 2s - loss: 332.4255 - loglik: -3.3083e+02 - logprior: -1.5907e+00
Epoch 8/10
19/19 - 1s - loss: 331.9885 - loglik: -3.3037e+02 - logprior: -1.6190e+00
Epoch 9/10
19/19 - 2s - loss: 332.0430 - loglik: -3.3041e+02 - logprior: -1.6301e+00
Fitted a model with MAP estimate = -331.6480
expansions: [(16, 1), (17, 1), (18, 1), (21, 2), (22, 1), (23, 1), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 332.9067 - loglik: -3.2950e+02 - logprior: -3.4033e+00
Epoch 2/2
19/19 - 2s - loss: 325.5598 - loglik: -3.2426e+02 - logprior: -1.3034e+00
Fitted a model with MAP estimate = -324.2528
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 327.4995 - loglik: -3.2431e+02 - logprior: -3.1895e+00
Epoch 2/10
19/19 - 2s - loss: 324.5912 - loglik: -3.2333e+02 - logprior: -1.2619e+00
Epoch 3/10
19/19 - 2s - loss: 323.7133 - loglik: -3.2260e+02 - logprior: -1.1136e+00
Epoch 4/10
19/19 - 2s - loss: 323.1999 - loglik: -3.2212e+02 - logprior: -1.0802e+00
Epoch 5/10
19/19 - 2s - loss: 322.7195 - loglik: -3.2163e+02 - logprior: -1.0866e+00
Epoch 6/10
19/19 - 2s - loss: 321.7899 - loglik: -3.2069e+02 - logprior: -1.0977e+00
Epoch 7/10
19/19 - 2s - loss: 321.8025 - loglik: -3.2068e+02 - logprior: -1.1193e+00
Fitted a model with MAP estimate = -321.5850
Time for alignment: 46.9586
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 383.6182 - loglik: -3.8017e+02 - logprior: -3.4453e+00
Epoch 2/10
19/19 - 1s - loss: 351.4638 - loglik: -3.4985e+02 - logprior: -1.6166e+00
Epoch 3/10
19/19 - 1s - loss: 340.0325 - loglik: -3.3857e+02 - logprior: -1.4636e+00
Epoch 4/10
19/19 - 2s - loss: 336.7930 - loglik: -3.3529e+02 - logprior: -1.5037e+00
Epoch 5/10
19/19 - 1s - loss: 335.5676 - loglik: -3.3409e+02 - logprior: -1.4748e+00
Epoch 6/10
19/19 - 2s - loss: 335.2196 - loglik: -3.3372e+02 - logprior: -1.5045e+00
Epoch 7/10
19/19 - 1s - loss: 334.6953 - loglik: -3.3317e+02 - logprior: -1.5237e+00
Epoch 8/10
19/19 - 2s - loss: 334.3767 - loglik: -3.3284e+02 - logprior: -1.5342e+00
Epoch 9/10
19/19 - 2s - loss: 334.6656 - loglik: -3.3310e+02 - logprior: -1.5695e+00
Fitted a model with MAP estimate = -334.1372
expansions: [(16, 1), (17, 1), (18, 1), (22, 2), (23, 2), (33, 2), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 333.2206 - loglik: -3.2982e+02 - logprior: -3.4010e+00
Epoch 2/2
19/19 - 2s - loss: 325.4990 - loglik: -3.2418e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -324.2575
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 327.8230 - loglik: -3.2464e+02 - logprior: -3.1782e+00
Epoch 2/2
19/19 - 2s - loss: 324.8800 - loglik: -3.2364e+02 - logprior: -1.2432e+00
Fitted a model with MAP estimate = -323.9808
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 327.3257 - loglik: -3.2418e+02 - logprior: -3.1474e+00
Epoch 2/10
19/19 - 2s - loss: 324.5901 - loglik: -3.2337e+02 - logprior: -1.2241e+00
Epoch 3/10
19/19 - 2s - loss: 324.0455 - loglik: -3.2297e+02 - logprior: -1.0721e+00
Epoch 4/10
19/19 - 2s - loss: 322.9771 - loglik: -3.2194e+02 - logprior: -1.0369e+00
Epoch 5/10
19/19 - 2s - loss: 322.7089 - loglik: -3.2167e+02 - logprior: -1.0379e+00
Epoch 6/10
19/19 - 2s - loss: 321.7427 - loglik: -3.2069e+02 - logprior: -1.0511e+00
Epoch 7/10
19/19 - 2s - loss: 322.1520 - loglik: -3.2109e+02 - logprior: -1.0608e+00
Fitted a model with MAP estimate = -321.5578
Time for alignment: 55.7289
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 383.7922 - loglik: -3.8034e+02 - logprior: -3.4504e+00
Epoch 2/10
19/19 - 2s - loss: 349.7142 - loglik: -3.4807e+02 - logprior: -1.6482e+00
Epoch 3/10
19/19 - 1s - loss: 337.8884 - loglik: -3.3636e+02 - logprior: -1.5316e+00
Epoch 4/10
19/19 - 1s - loss: 334.7668 - loglik: -3.3319e+02 - logprior: -1.5750e+00
Epoch 5/10
19/19 - 1s - loss: 333.7930 - loglik: -3.3225e+02 - logprior: -1.5462e+00
Epoch 6/10
19/19 - 2s - loss: 333.3297 - loglik: -3.3176e+02 - logprior: -1.5665e+00
Epoch 7/10
19/19 - 2s - loss: 332.8387 - loglik: -3.3126e+02 - logprior: -1.5787e+00
Epoch 8/10
19/19 - 2s - loss: 332.7639 - loglik: -3.3116e+02 - logprior: -1.5999e+00
Epoch 9/10
19/19 - 2s - loss: 332.8951 - loglik: -3.3128e+02 - logprior: -1.6164e+00
Fitted a model with MAP estimate = -332.3177
expansions: [(14, 1), (16, 1), (17, 1), (22, 2), (23, 2), (25, 1), (33, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 332.9717 - loglik: -3.2956e+02 - logprior: -3.4149e+00
Epoch 2/2
19/19 - 2s - loss: 325.6325 - loglik: -3.2431e+02 - logprior: -1.3225e+00
Fitted a model with MAP estimate = -324.2730
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 327.7398 - loglik: -3.2456e+02 - logprior: -3.1788e+00
Epoch 2/2
19/19 - 2s - loss: 324.8149 - loglik: -3.2357e+02 - logprior: -1.2414e+00
Fitted a model with MAP estimate = -323.9980
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 327.2881 - loglik: -3.2414e+02 - logprior: -3.1470e+00
Epoch 2/10
19/19 - 2s - loss: 324.6557 - loglik: -3.2343e+02 - logprior: -1.2222e+00
Epoch 3/10
19/19 - 2s - loss: 323.7290 - loglik: -3.2266e+02 - logprior: -1.0700e+00
Epoch 4/10
19/19 - 1s - loss: 323.3136 - loglik: -3.2228e+02 - logprior: -1.0333e+00
Epoch 5/10
19/19 - 2s - loss: 322.4275 - loglik: -3.2139e+02 - logprior: -1.0409e+00
Epoch 6/10
19/19 - 2s - loss: 322.3807 - loglik: -3.2133e+02 - logprior: -1.0458e+00
Epoch 7/10
19/19 - 2s - loss: 321.4964 - loglik: -3.2044e+02 - logprior: -1.0582e+00
Epoch 8/10
19/19 - 2s - loss: 321.5429 - loglik: -3.2047e+02 - logprior: -1.0769e+00
Fitted a model with MAP estimate = -321.4104
Time for alignment: 58.9151
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 383.7556 - loglik: -3.8031e+02 - logprior: -3.4413e+00
Epoch 2/10
19/19 - 2s - loss: 351.8614 - loglik: -3.5026e+02 - logprior: -1.6018e+00
Epoch 3/10
19/19 - 2s - loss: 339.2902 - loglik: -3.3774e+02 - logprior: -1.5537e+00
Epoch 4/10
19/19 - 2s - loss: 336.1575 - loglik: -3.3462e+02 - logprior: -1.5420e+00
Epoch 5/10
19/19 - 2s - loss: 334.5432 - loglik: -3.3303e+02 - logprior: -1.5098e+00
Epoch 6/10
19/19 - 1s - loss: 334.0574 - loglik: -3.3252e+02 - logprior: -1.5332e+00
Epoch 7/10
19/19 - 1s - loss: 332.8399 - loglik: -3.3126e+02 - logprior: -1.5776e+00
Epoch 8/10
19/19 - 2s - loss: 332.5948 - loglik: -3.3098e+02 - logprior: -1.6159e+00
Epoch 9/10
19/19 - 2s - loss: 332.0769 - loglik: -3.3044e+02 - logprior: -1.6325e+00
Epoch 10/10
19/19 - 2s - loss: 332.3262 - loglik: -3.3066e+02 - logprior: -1.6699e+00
Fitted a model with MAP estimate = -331.8012
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (25, 1), (33, 1), (36, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.2913 - loglik: -3.2987e+02 - logprior: -3.4245e+00
Epoch 2/2
19/19 - 2s - loss: 325.4603 - loglik: -3.2415e+02 - logprior: -1.3070e+00
Fitted a model with MAP estimate = -324.3220
expansions: []
discards: [26]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 328.1508 - loglik: -3.2497e+02 - logprior: -3.1778e+00
Epoch 2/2
19/19 - 1s - loss: 325.1634 - loglik: -3.2392e+02 - logprior: -1.2459e+00
Fitted a model with MAP estimate = -324.2897
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 327.6264 - loglik: -3.2447e+02 - logprior: -3.1519e+00
Epoch 2/10
19/19 - 2s - loss: 324.8015 - loglik: -3.2358e+02 - logprior: -1.2256e+00
Epoch 3/10
19/19 - 2s - loss: 324.1373 - loglik: -3.2306e+02 - logprior: -1.0762e+00
Epoch 4/10
19/19 - 2s - loss: 323.2315 - loglik: -3.2220e+02 - logprior: -1.0311e+00
Epoch 5/10
19/19 - 2s - loss: 322.7302 - loglik: -3.2169e+02 - logprior: -1.0421e+00
Epoch 6/10
19/19 - 2s - loss: 322.4693 - loglik: -3.2142e+02 - logprior: -1.0462e+00
Epoch 7/10
19/19 - 2s - loss: 321.8835 - loglik: -3.2081e+02 - logprior: -1.0706e+00
Epoch 8/10
19/19 - 2s - loss: 321.7924 - loglik: -3.2071e+02 - logprior: -1.0791e+00
Epoch 9/10
19/19 - 2s - loss: 321.8868 - loglik: -3.2078e+02 - logprior: -1.1082e+00
Fitted a model with MAP estimate = -321.5387
Time for alignment: 60.0509
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 383.7044 - loglik: -3.8026e+02 - logprior: -3.4440e+00
Epoch 2/10
19/19 - 2s - loss: 352.3538 - loglik: -3.5077e+02 - logprior: -1.5846e+00
Epoch 3/10
19/19 - 1s - loss: 341.2682 - loglik: -3.3984e+02 - logprior: -1.4246e+00
Epoch 4/10
19/19 - 2s - loss: 337.6021 - loglik: -3.3618e+02 - logprior: -1.4249e+00
Epoch 5/10
19/19 - 1s - loss: 335.8183 - loglik: -3.3440e+02 - logprior: -1.4178e+00
Epoch 6/10
19/19 - 1s - loss: 335.7014 - loglik: -3.3427e+02 - logprior: -1.4347e+00
Epoch 7/10
19/19 - 2s - loss: 334.7869 - loglik: -3.3331e+02 - logprior: -1.4773e+00
Epoch 8/10
19/19 - 1s - loss: 333.8648 - loglik: -3.3235e+02 - logprior: -1.5145e+00
Epoch 9/10
19/19 - 2s - loss: 333.8470 - loglik: -3.3230e+02 - logprior: -1.5469e+00
Epoch 10/10
19/19 - 2s - loss: 333.9006 - loglik: -3.3232e+02 - logprior: -1.5781e+00
Fitted a model with MAP estimate = -333.4416
expansions: [(14, 1), (16, 2), (18, 1), (22, 5), (23, 1), (34, 1), (35, 1), (40, 1), (43, 1), (44, 1), (46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 333.9224 - loglik: -3.3047e+02 - logprior: -3.4507e+00
Epoch 2/2
19/19 - 2s - loss: 325.7665 - loglik: -3.2442e+02 - logprior: -1.3451e+00
Fitted a model with MAP estimate = -324.6380
expansions: []
discards: [17 28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 328.2572 - loglik: -3.2507e+02 - logprior: -3.1865e+00
Epoch 2/2
19/19 - 2s - loss: 325.4732 - loglik: -3.2422e+02 - logprior: -1.2573e+00
Fitted a model with MAP estimate = -324.3791
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 327.7726 - loglik: -3.2462e+02 - logprior: -3.1517e+00
Epoch 2/10
19/19 - 2s - loss: 324.8114 - loglik: -3.2358e+02 - logprior: -1.2300e+00
Epoch 3/10
19/19 - 2s - loss: 324.4675 - loglik: -3.2339e+02 - logprior: -1.0797e+00
Epoch 4/10
19/19 - 2s - loss: 323.4799 - loglik: -3.2244e+02 - logprior: -1.0427e+00
Epoch 5/10
19/19 - 2s - loss: 322.8598 - loglik: -3.2182e+02 - logprior: -1.0398e+00
Epoch 6/10
19/19 - 2s - loss: 322.6217 - loglik: -3.2157e+02 - logprior: -1.0489e+00
Epoch 7/10
19/19 - 2s - loss: 322.1112 - loglik: -3.2105e+02 - logprior: -1.0631e+00
Epoch 8/10
19/19 - 1s - loss: 322.1999 - loglik: -3.2112e+02 - logprior: -1.0801e+00
Fitted a model with MAP estimate = -321.8174
Time for alignment: 58.3011
Computed alignments with likelihoods: ['-321.5850', '-321.5578', '-321.4104', '-321.5387', '-321.8174']
Best model has likelihood: -321.4104  (prior= -1.0552 )
time for generating output: 0.1248
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.968432780391186
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff114fe9880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2c59b7280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe919447070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9194479d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11445b400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefce68100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918eb47c0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe94119e970>, <__main__.SimpleDirichletPrior object at 0x7fe92148cb50>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 1552.0228 - loglik: -1.5498e+03 - logprior: -2.2141e+00
Epoch 2/10
39/39 - 25s - loss: 1440.3365 - loglik: -1.4387e+03 - logprior: -1.5900e+00
Epoch 3/10
39/39 - 26s - loss: 1426.9968 - loglik: -1.4254e+03 - logprior: -1.6196e+00
Epoch 4/10
39/39 - 26s - loss: 1423.3993 - loglik: -1.4217e+03 - logprior: -1.6759e+00
Epoch 5/10
39/39 - 26s - loss: 1420.7192 - loglik: -1.4188e+03 - logprior: -1.8761e+00
Epoch 6/10
39/39 - 26s - loss: 1419.0549 - loglik: -1.4169e+03 - logprior: -2.1295e+00
Epoch 7/10
39/39 - 27s - loss: 1419.2819 - loglik: -1.4169e+03 - logprior: -2.3747e+00
Fitted a model with MAP estimate = -1416.4350
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 3), (18, 1), (31, 1), (35, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (69, 1), (70, 1), (71, 1), (77, 1), (78, 1), (102, 2), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 2), (137, 1), (140, 1), (153, 3), (156, 2), (162, 3), (164, 2), (168, 1), (176, 1), (177, 1), (179, 1), (183, 1), (185, 1), (188, 1), (192, 1), (193, 1), (196, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1416.2206 - loglik: -1.4134e+03 - logprior: -2.8470e+00
Epoch 2/2
39/39 - 40s - loss: 1397.9138 - loglik: -1.3965e+03 - logprior: -1.4573e+00
Fitted a model with MAP estimate = -1392.7003
expansions: [(0, 3), (20, 1), (148, 1), (205, 3), (244, 1)]
discards: [  0 122 166 272]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1396.6230 - loglik: -1.3951e+03 - logprior: -1.5075e+00
Epoch 2/2
39/39 - 40s - loss: 1390.5265 - loglik: -1.3901e+03 - logprior: -4.6692e-01
Fitted a model with MAP estimate = -1386.2822
expansions: [(208, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1395.0122 - loglik: -1.3928e+03 - logprior: -2.2072e+00
Epoch 2/10
39/39 - 40s - loss: 1388.2516 - loglik: -1.3879e+03 - logprior: -3.3950e-01
Epoch 3/10
39/39 - 38s - loss: 1384.6620 - loglik: -1.3846e+03 - logprior: -3.7806e-02
Epoch 4/10
39/39 - 40s - loss: 1381.7914 - loglik: -1.3818e+03 - logprior: -2.1412e-02
Epoch 5/10
39/39 - 42s - loss: 1379.9104 - loglik: -1.3798e+03 - logprior: -1.1100e-01
Epoch 6/10
39/39 - 42s - loss: 1379.9603 - loglik: -1.3797e+03 - logprior: -3.0407e-01
Fitted a model with MAP estimate = -1377.7574
Time for alignment: 780.6331
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 1550.5968 - loglik: -1.5484e+03 - logprior: -2.2097e+00
Epoch 2/10
39/39 - 23s - loss: 1437.7565 - loglik: -1.4362e+03 - logprior: -1.5268e+00
Epoch 3/10
39/39 - 23s - loss: 1426.4739 - loglik: -1.4248e+03 - logprior: -1.6374e+00
Epoch 4/10
39/39 - 23s - loss: 1422.3145 - loglik: -1.4206e+03 - logprior: -1.7473e+00
Epoch 5/10
39/39 - 23s - loss: 1420.7574 - loglik: -1.4188e+03 - logprior: -1.9456e+00
Epoch 6/10
39/39 - 23s - loss: 1418.5413 - loglik: -1.4163e+03 - logprior: -2.2055e+00
Epoch 7/10
39/39 - 23s - loss: 1418.1895 - loglik: -1.4157e+03 - logprior: -2.4654e+00
Epoch 8/10
39/39 - 23s - loss: 1417.8424 - loglik: -1.4152e+03 - logprior: -2.6836e+00
Epoch 9/10
39/39 - 23s - loss: 1417.6366 - loglik: -1.4148e+03 - logprior: -2.8782e+00
Epoch 10/10
39/39 - 23s - loss: 1416.4802 - loglik: -1.4135e+03 - logprior: -2.9973e+00
Fitted a model with MAP estimate = -1414.3967
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (30, 1), (34, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (82, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (136, 1), (138, 3), (147, 1), (154, 2), (165, 3), (166, 1), (174, 3), (175, 1), (180, 1), (184, 1), (185, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1414.0305 - loglik: -1.4111e+03 - logprior: -2.9382e+00
Epoch 2/2
39/39 - 34s - loss: 1396.6781 - loglik: -1.3953e+03 - logprior: -1.4162e+00
Fitted a model with MAP estimate = -1391.8835
expansions: [(0, 3), (149, 1), (172, 1), (173, 1), (244, 1)]
discards: [  0 183 219 232 268]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 1396.5924 - loglik: -1.3951e+03 - logprior: -1.5026e+00
Epoch 2/2
39/39 - 34s - loss: 1391.4672 - loglik: -1.3910e+03 - logprior: -4.3021e-01
Fitted a model with MAP estimate = -1387.6700
expansions: [(174, 1)]
discards: [ 0  1 87]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 1396.3390 - loglik: -1.3943e+03 - logprior: -2.0380e+00
Epoch 2/10
39/39 - 34s - loss: 1390.1176 - loglik: -1.3899e+03 - logprior: -2.1561e-01
Epoch 3/10
39/39 - 34s - loss: 1387.0825 - loglik: -1.3871e+03 - logprior: 0.0486
Epoch 4/10
39/39 - 34s - loss: 1384.6873 - loglik: -1.3847e+03 - logprior: 0.0244
Epoch 5/10
39/39 - 35s - loss: 1381.4698 - loglik: -1.3814e+03 - logprior: -5.0904e-02
Epoch 6/10
39/39 - 34s - loss: 1380.5554 - loglik: -1.3803e+03 - logprior: -2.2463e-01
Epoch 7/10
39/39 - 34s - loss: 1380.0624 - loglik: -1.3797e+03 - logprior: -3.3712e-01
Epoch 8/10
39/39 - 34s - loss: 1379.0371 - loglik: -1.3786e+03 - logprior: -3.9369e-01
Epoch 9/10
39/39 - 34s - loss: 1379.0966 - loglik: -1.3786e+03 - logprior: -4.7405e-01
Fitted a model with MAP estimate = -1378.5801
Time for alignment: 843.4000
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1549.7385 - loglik: -1.5475e+03 - logprior: -2.2157e+00
Epoch 2/10
39/39 - 25s - loss: 1436.5228 - loglik: -1.4349e+03 - logprior: -1.6038e+00
Epoch 3/10
39/39 - 24s - loss: 1425.8763 - loglik: -1.4242e+03 - logprior: -1.7088e+00
Epoch 4/10
39/39 - 24s - loss: 1422.3777 - loglik: -1.4206e+03 - logprior: -1.8178e+00
Epoch 5/10
39/39 - 24s - loss: 1418.3812 - loglik: -1.4164e+03 - logprior: -2.0304e+00
Epoch 6/10
39/39 - 24s - loss: 1417.9260 - loglik: -1.4156e+03 - logprior: -2.3156e+00
Epoch 7/10
39/39 - 23s - loss: 1417.2732 - loglik: -1.4147e+03 - logprior: -2.5653e+00
Epoch 8/10
39/39 - 23s - loss: 1417.1934 - loglik: -1.4144e+03 - logprior: -2.7629e+00
Epoch 9/10
39/39 - 23s - loss: 1416.1895 - loglik: -1.4133e+03 - logprior: -2.9173e+00
Epoch 10/10
39/39 - 23s - loss: 1416.5615 - loglik: -1.4135e+03 - logprior: -3.0522e+00
Fitted a model with MAP estimate = -1413.8239
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 3), (18, 2), (30, 1), (31, 1), (33, 1), (35, 1), (37, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 5), (131, 1), (132, 1), (135, 2), (137, 1), (140, 1), (149, 1), (154, 1), (157, 1), (165, 1), (166, 2), (175, 1), (180, 1), (184, 1), (186, 1), (193, 3), (194, 1), (197, 1), (211, 1), (212, 3), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [  0 150]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1412.1641 - loglik: -1.4092e+03 - logprior: -2.9413e+00
Epoch 2/2
39/39 - 31s - loss: 1395.0587 - loglik: -1.3937e+03 - logprior: -1.4029e+00
Fitted a model with MAP estimate = -1389.7038
expansions: [(0, 3), (150, 1), (196, 1), (221, 6)]
discards: [  0 168 207]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1394.0836 - loglik: -1.3926e+03 - logprior: -1.4977e+00
Epoch 2/2
39/39 - 33s - loss: 1388.5807 - loglik: -1.3881e+03 - logprior: -4.3989e-01
Fitted a model with MAP estimate = -1384.3830
expansions: [(225, 1), (227, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1393.3080 - loglik: -1.3913e+03 - logprior: -2.0446e+00
Epoch 2/10
39/39 - 34s - loss: 1386.0104 - loglik: -1.3857e+03 - logprior: -2.8914e-01
Epoch 3/10
39/39 - 34s - loss: 1384.4216 - loglik: -1.3844e+03 - logprior: -3.0428e-02
Epoch 4/10
39/39 - 34s - loss: 1380.2675 - loglik: -1.3803e+03 - logprior: 0.0018
Epoch 5/10
39/39 - 34s - loss: 1379.0464 - loglik: -1.3790e+03 - logprior: -9.4411e-02
Epoch 6/10
39/39 - 34s - loss: 1376.4751 - loglik: -1.3762e+03 - logprior: -2.5772e-01
Epoch 7/10
39/39 - 34s - loss: 1376.1846 - loglik: -1.3758e+03 - logprior: -3.4971e-01
Epoch 8/10
39/39 - 34s - loss: 1375.3627 - loglik: -1.3749e+03 - logprior: -4.9322e-01
Epoch 9/10
39/39 - 34s - loss: 1376.0090 - loglik: -1.3755e+03 - logprior: -5.4264e-01
Fitted a model with MAP estimate = -1374.8961
Time for alignment: 832.9020
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1550.6523 - loglik: -1.5484e+03 - logprior: -2.2244e+00
Epoch 2/10
39/39 - 23s - loss: 1438.4059 - loglik: -1.4367e+03 - logprior: -1.6760e+00
Epoch 3/10
39/39 - 23s - loss: 1424.5857 - loglik: -1.4228e+03 - logprior: -1.8332e+00
Epoch 4/10
39/39 - 23s - loss: 1419.4679 - loglik: -1.4176e+03 - logprior: -1.9053e+00
Epoch 5/10
39/39 - 23s - loss: 1418.3511 - loglik: -1.4163e+03 - logprior: -2.0786e+00
Epoch 6/10
39/39 - 23s - loss: 1416.8005 - loglik: -1.4144e+03 - logprior: -2.3582e+00
Epoch 7/10
39/39 - 23s - loss: 1415.0571 - loglik: -1.4124e+03 - logprior: -2.6275e+00
Epoch 8/10
39/39 - 23s - loss: 1416.4203 - loglik: -1.4136e+03 - logprior: -2.8315e+00
Fitted a model with MAP estimate = -1413.1238
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (32, 2), (34, 1), (36, 1), (42, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (125, 1), (132, 1), (135, 2), (136, 2), (137, 2), (140, 1), (148, 1), (149, 1), (151, 1), (153, 1), (154, 1), (156, 3), (163, 1), (165, 2), (166, 2), (177, 2), (180, 1), (184, 1), (185, 2), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1410.9729 - loglik: -1.4080e+03 - logprior: -2.9429e+00
Epoch 2/2
39/39 - 33s - loss: 1391.6958 - loglik: -1.3902e+03 - logprior: -1.4573e+00
Fitted a model with MAP estimate = -1386.4821
expansions: [(0, 3), (39, 1), (150, 1), (202, 1), (203, 2), (204, 1)]
discards: [  0  40  41  87 174 193 216]
Re-initialized the encoder parameters.
Fitting a model of length 318 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 1393.0419 - loglik: -1.3915e+03 - logprior: -1.5335e+00
Epoch 2/2
39/39 - 36s - loss: 1387.0729 - loglik: -1.3867e+03 - logprior: -4.1571e-01
Fitted a model with MAP estimate = -1383.0230
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1392.0095 - loglik: -1.3900e+03 - logprior: -2.0448e+00
Epoch 2/10
39/39 - 38s - loss: 1386.0913 - loglik: -1.3857e+03 - logprior: -3.4754e-01
Epoch 3/10
39/39 - 39s - loss: 1383.3668 - loglik: -1.3833e+03 - logprior: -2.6585e-02
Epoch 4/10
39/39 - 40s - loss: 1378.8610 - loglik: -1.3789e+03 - logprior: -4.2898e-03
Epoch 5/10
39/39 - 41s - loss: 1377.6497 - loglik: -1.3775e+03 - logprior: -1.0670e-01
Epoch 6/10
39/39 - 40s - loss: 1375.2709 - loglik: -1.3750e+03 - logprior: -2.7691e-01
Epoch 7/10
39/39 - 40s - loss: 1374.7438 - loglik: -1.3743e+03 - logprior: -4.4350e-01
Epoch 8/10
39/39 - 40s - loss: 1373.9196 - loglik: -1.3734e+03 - logprior: -4.9985e-01
Epoch 9/10
39/39 - 40s - loss: 1373.7440 - loglik: -1.3732e+03 - logprior: -5.7203e-01
Epoch 10/10
39/39 - 39s - loss: 1374.3251 - loglik: -1.3737e+03 - logprior: -6.1167e-01
Fitted a model with MAP estimate = -1372.7899
Time for alignment: 884.6849
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 1549.6621 - loglik: -1.5474e+03 - logprior: -2.2150e+00
Epoch 2/10
39/39 - 27s - loss: 1439.6270 - loglik: -1.4381e+03 - logprior: -1.5461e+00
Epoch 3/10
39/39 - 27s - loss: 1426.9663 - loglik: -1.4253e+03 - logprior: -1.6647e+00
Epoch 4/10
39/39 - 27s - loss: 1421.0254 - loglik: -1.4192e+03 - logprior: -1.7840e+00
Epoch 5/10
39/39 - 27s - loss: 1418.4232 - loglik: -1.4164e+03 - logprior: -1.9987e+00
Epoch 6/10
39/39 - 26s - loss: 1416.7659 - loglik: -1.4145e+03 - logprior: -2.2627e+00
Epoch 7/10
39/39 - 27s - loss: 1415.8789 - loglik: -1.4134e+03 - logprior: -2.5158e+00
Epoch 8/10
39/39 - 26s - loss: 1416.3730 - loglik: -1.4137e+03 - logprior: -2.6958e+00
Fitted a model with MAP estimate = -1413.4508
expansions: [(8, 1), (10, 1), (12, 1), (13, 1), (14, 2), (16, 1), (29, 1), (33, 1), (35, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 1), (150, 1), (154, 3), (165, 3), (166, 1), (174, 2), (175, 1), (184, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1413.0117 - loglik: -1.4102e+03 - logprior: -2.8491e+00
Epoch 2/2
39/39 - 39s - loss: 1397.3134 - loglik: -1.3959e+03 - logprior: -1.3835e+00
Fitted a model with MAP estimate = -1392.8788
expansions: [(0, 3), (149, 1), (169, 2)]
discards: [  0  85 189 240 270]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 1397.6777 - loglik: -1.3962e+03 - logprior: -1.4738e+00
Epoch 2/2
39/39 - 38s - loss: 1392.6781 - loglik: -1.3923e+03 - logprior: -4.1783e-01
Fitted a model with MAP estimate = -1388.2798
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 1397.5021 - loglik: -1.3955e+03 - logprior: -2.0206e+00
Epoch 2/10
39/39 - 39s - loss: 1390.9467 - loglik: -1.3907e+03 - logprior: -2.7835e-01
Epoch 3/10
39/39 - 39s - loss: 1387.6666 - loglik: -1.3877e+03 - logprior: 0.0035
Epoch 4/10
39/39 - 39s - loss: 1384.8549 - loglik: -1.3849e+03 - logprior: 0.0092
Epoch 5/10
39/39 - 38s - loss: 1382.8622 - loglik: -1.3828e+03 - logprior: -8.5005e-02
Epoch 6/10
39/39 - 38s - loss: 1381.9690 - loglik: -1.3817e+03 - logprior: -2.4231e-01
Epoch 7/10
39/39 - 38s - loss: 1380.9377 - loglik: -1.3806e+03 - logprior: -3.8672e-01
Epoch 8/10
39/39 - 37s - loss: 1378.3717 - loglik: -1.3779e+03 - logprior: -4.6879e-01
Epoch 9/10
39/39 - 39s - loss: 1378.5494 - loglik: -1.3780e+03 - logprior: -5.5041e-01
Fitted a model with MAP estimate = -1378.5503
Time for alignment: 904.5586
Computed alignments with likelihoods: ['-1377.7574', '-1378.5801', '-1374.8961', '-1372.7899', '-1378.5503']
Best model has likelihood: -1372.7899  (prior= -0.5993 )
time for generating output: 0.3855
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.41483553069649753
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe930ba91c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe94119edf0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe904b1f040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe94052d2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff15c1848e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe90599c190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff114fe9880>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c59b7280>, <__main__.SimpleDirichletPrior object at 0x7fe920536d00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 2084.0542 - loglik: -2.0820e+03 - logprior: -2.0381e+00
Epoch 2/10
39/39 - 48s - loss: 1922.2944 - loglik: -1.9212e+03 - logprior: -1.0617e+00
Epoch 3/10
39/39 - 45s - loss: 1910.6901 - loglik: -1.9094e+03 - logprior: -1.3064e+00
Epoch 4/10
39/39 - 44s - loss: 1907.6284 - loglik: -1.9061e+03 - logprior: -1.5406e+00
Epoch 5/10
39/39 - 44s - loss: 1905.7697 - loglik: -1.9040e+03 - logprior: -1.8178e+00
Epoch 6/10
39/39 - 43s - loss: 1905.2188 - loglik: -1.9031e+03 - logprior: -2.1274e+00
Epoch 7/10
39/39 - 44s - loss: 1905.8153 - loglik: -1.9035e+03 - logprior: -2.3097e+00
Fitted a model with MAP estimate = -1884.2095
expansions: [(0, 3), (5, 1), (44, 1), (52, 1), (53, 1), (57, 1), (70, 1), (71, 1), (74, 2), (100, 1), (121, 2), (122, 2), (123, 2), (132, 1), (146, 3), (147, 1), (148, 1), (167, 1), (170, 1), (176, 1), (177, 1), (186, 1), (193, 2), (196, 1), (197, 3), (219, 1), (220, 1), (221, 1), (223, 2), (224, 3), (248, 1), (249, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (260, 1), (262, 1), (266, 2), (285, 4), (286, 3), (287, 3), (288, 1), (289, 1), (296, 3), (297, 2), (298, 2), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1891.9225 - loglik: -1.8893e+03 - logprior: -2.6646e+00
Epoch 2/2
39/39 - 63s - loss: 1871.5956 - loglik: -1.8708e+03 - logprior: -7.7571e-01
Fitted a model with MAP estimate = -1849.2720
expansions: [(0, 2), (352, 2)]
discards: [  1  84 138 139 170 223 231 322 339 342 343 344 370 371 396]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 1877.7672 - loglik: -1.8756e+03 - logprior: -2.2036e+00
Epoch 2/2
39/39 - 70s - loss: 1872.0175 - loglik: -1.8719e+03 - logprior: -1.5431e-01
Fitted a model with MAP estimate = -1849.8885
expansions: [(0, 2), (342, 1)]
discards: [  1   2   3 338]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 1856.5831 - loglik: -1.8544e+03 - logprior: -2.1653e+00
Epoch 2/10
39/39 - 69s - loss: 1851.9178 - loglik: -1.8520e+03 - logprior: 0.0531
Epoch 3/10
39/39 - 74s - loss: 1850.1410 - loglik: -1.8503e+03 - logprior: 0.1867
Epoch 4/10
39/39 - 79s - loss: 1844.7402 - loglik: -1.8449e+03 - logprior: 0.1660
Epoch 5/10
39/39 - 76s - loss: 1847.5509 - loglik: -1.8476e+03 - logprior: 0.0630
Fitted a model with MAP estimate = -1845.5775
Time for alignment: 1285.6689
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 2082.3032 - loglik: -2.0803e+03 - logprior: -2.0513e+00
Epoch 2/10
39/39 - 54s - loss: 1923.0947 - loglik: -1.9220e+03 - logprior: -1.0870e+00
Epoch 3/10
39/39 - 51s - loss: 1909.5010 - loglik: -1.9081e+03 - logprior: -1.3863e+00
Epoch 4/10
39/39 - 51s - loss: 1906.9740 - loglik: -1.9054e+03 - logprior: -1.5794e+00
Epoch 5/10
39/39 - 52s - loss: 1905.4750 - loglik: -1.9036e+03 - logprior: -1.8624e+00
Epoch 6/10
39/39 - 51s - loss: 1905.1410 - loglik: -1.9031e+03 - logprior: -2.0779e+00
Epoch 7/10
39/39 - 51s - loss: 1903.7404 - loglik: -1.9014e+03 - logprior: -2.3148e+00
Epoch 8/10
39/39 - 51s - loss: 1905.1382 - loglik: -1.9026e+03 - logprior: -2.5148e+00
Fitted a model with MAP estimate = -1883.6877
expansions: [(0, 3), (5, 1), (34, 1), (51, 1), (53, 1), (57, 1), (61, 1), (69, 1), (70, 2), (74, 2), (98, 1), (119, 2), (120, 2), (121, 2), (145, 1), (146, 2), (147, 1), (148, 2), (166, 1), (170, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (263, 1), (286, 6), (287, 3), (288, 1), (289, 1), (296, 3), (297, 1), (299, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1891.7144 - loglik: -1.8890e+03 - logprior: -2.7117e+00
Epoch 2/2
39/39 - 76s - loss: 1873.1382 - loglik: -1.8724e+03 - logprior: -7.1897e-01
Fitted a model with MAP estimate = -1850.6895
expansions: [(0, 2), (346, 1), (347, 2)]
discards: [  1  86 135 139 168 171 265 342 388]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 1876.6760 - loglik: -1.8746e+03 - logprior: -2.0312e+00
Epoch 2/2
39/39 - 70s - loss: 1871.1669 - loglik: -1.8711e+03 - logprior: -1.1155e-01
Fitted a model with MAP estimate = -1849.6929
expansions: [(0, 2), (260, 1)]
discards: [  1   2   3 331 334 335 336]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 1857.6266 - loglik: -1.8555e+03 - logprior: -2.1035e+00
Epoch 2/10
39/39 - 73s - loss: 1852.3285 - loglik: -1.8524e+03 - logprior: 0.0720
Epoch 3/10
39/39 - 70s - loss: 1850.1112 - loglik: -1.8503e+03 - logprior: 0.2092
Epoch 4/10
39/39 - 70s - loss: 1847.0516 - loglik: -1.8472e+03 - logprior: 0.1840
Epoch 5/10
39/39 - 70s - loss: 1850.1022 - loglik: -1.8502e+03 - logprior: 0.0574
Fitted a model with MAP estimate = -1846.6402
Time for alignment: 1450.8448
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 2082.5940 - loglik: -2.0805e+03 - logprior: -2.0765e+00
Epoch 2/10
39/39 - 54s - loss: 1920.5573 - loglik: -1.9193e+03 - logprior: -1.2320e+00
Epoch 3/10
39/39 - 55s - loss: 1909.1995 - loglik: -1.9078e+03 - logprior: -1.4359e+00
Epoch 4/10
39/39 - 52s - loss: 1906.3868 - loglik: -1.9048e+03 - logprior: -1.5797e+00
Epoch 5/10
39/39 - 53s - loss: 1905.2903 - loglik: -1.9035e+03 - logprior: -1.8135e+00
Epoch 6/10
39/39 - 54s - loss: 1905.1426 - loglik: -1.9031e+03 - logprior: -2.0814e+00
Epoch 7/10
39/39 - 52s - loss: 1904.5188 - loglik: -1.9023e+03 - logprior: -2.2583e+00
Epoch 8/10
39/39 - 50s - loss: 1903.7688 - loglik: -1.9012e+03 - logprior: -2.5465e+00
Epoch 9/10
39/39 - 51s - loss: 1904.6981 - loglik: -1.9020e+03 - logprior: -2.6840e+00
Fitted a model with MAP estimate = -1883.3321
expansions: [(0, 2), (20, 1), (48, 1), (52, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (75, 2), (105, 1), (120, 1), (121, 2), (123, 1), (132, 1), (143, 1), (145, 3), (147, 1), (163, 1), (169, 1), (172, 1), (174, 1), (175, 1), (176, 1), (191, 2), (194, 1), (195, 1), (218, 1), (219, 1), (220, 1), (222, 2), (223, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (262, 3), (263, 1), (264, 1), (266, 2), (285, 3), (286, 8), (295, 1), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 1892.0996 - loglik: -1.8898e+03 - logprior: -2.2497e+00
Epoch 2/2
39/39 - 79s - loss: 1874.1444 - loglik: -1.8733e+03 - logprior: -7.9643e-01
Fitted a model with MAP estimate = -1851.7420
expansions: [(0, 2), (7, 1), (343, 2)]
discards: [  1   2  84 135 219 260 310 318 385]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 1876.9729 - loglik: -1.8754e+03 - logprior: -1.6041e+00
Epoch 2/2
39/39 - 62s - loss: 1872.8760 - loglik: -1.8728e+03 - logprior: -2.9870e-02
Fitted a model with MAP estimate = -1850.6872
expansions: [(0, 2), (361, 1)]
discards: [  0   1 333 334]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 1856.9623 - loglik: -1.8558e+03 - logprior: -1.1587e+00
Epoch 2/10
39/39 - 62s - loss: 1852.0238 - loglik: -1.8522e+03 - logprior: 0.2211
Epoch 3/10
39/39 - 61s - loss: 1851.2144 - loglik: -1.8516e+03 - logprior: 0.3501
Epoch 4/10
39/39 - 64s - loss: 1849.1232 - loglik: -1.8494e+03 - logprior: 0.3264
Epoch 5/10
39/39 - 63s - loss: 1846.4257 - loglik: -1.8466e+03 - logprior: 0.2079
Epoch 6/10
39/39 - 60s - loss: 1847.9406 - loglik: -1.8480e+03 - logprior: 0.0598
Fitted a model with MAP estimate = -1845.8766
Time for alignment: 1489.1298
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 2083.8896 - loglik: -2.0818e+03 - logprior: -2.0434e+00
Epoch 2/10
39/39 - 46s - loss: 1922.4590 - loglik: -1.9213e+03 - logprior: -1.1165e+00
Epoch 3/10
39/39 - 47s - loss: 1909.6760 - loglik: -1.9083e+03 - logprior: -1.3915e+00
Epoch 4/10
39/39 - 47s - loss: 1906.0319 - loglik: -1.9045e+03 - logprior: -1.5682e+00
Epoch 5/10
39/39 - 47s - loss: 1904.1056 - loglik: -1.9023e+03 - logprior: -1.8181e+00
Epoch 6/10
39/39 - 47s - loss: 1903.4313 - loglik: -1.9013e+03 - logprior: -2.0994e+00
Epoch 7/10
39/39 - 48s - loss: 1903.2812 - loglik: -1.9010e+03 - logprior: -2.3192e+00
Epoch 8/10
39/39 - 47s - loss: 1903.2982 - loglik: -1.9007e+03 - logprior: -2.5522e+00
Fitted a model with MAP estimate = -1882.3850
expansions: [(0, 3), (5, 1), (42, 1), (47, 1), (51, 1), (52, 1), (56, 1), (61, 1), (69, 1), (70, 1), (72, 2), (98, 1), (119, 1), (120, 2), (122, 1), (143, 1), (146, 3), (147, 2), (166, 1), (170, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (264, 1), (285, 7), (286, 3), (287, 1), (288, 1), (291, 1), (295, 2), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 1889.6260 - loglik: -1.8870e+03 - logprior: -2.6498e+00
Epoch 2/2
39/39 - 62s - loss: 1872.3781 - loglik: -1.8718e+03 - logprior: -5.8096e-01
Fitted a model with MAP estimate = -1850.1069
expansions: [(0, 2), (346, 2)]
discards: [  1 136 169 263 339 340 387]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 1877.1549 - loglik: -1.8750e+03 - logprior: -2.1198e+00
Epoch 2/2
39/39 - 64s - loss: 1871.8228 - loglik: -1.8717e+03 - logprior: -1.6114e-01
Fitted a model with MAP estimate = -1849.7615
expansions: [(0, 2), (261, 1), (341, 1), (342, 1)]
discards: [  1   2   3   5 332 333 334 335 336 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 1858.9669 - loglik: -1.8568e+03 - logprior: -2.1658e+00
Epoch 2/10
39/39 - 61s - loss: 1852.3696 - loglik: -1.8524e+03 - logprior: 0.0718
Epoch 3/10
39/39 - 61s - loss: 1851.3993 - loglik: -1.8516e+03 - logprior: 0.1602
Epoch 4/10
39/39 - 63s - loss: 1850.0275 - loglik: -1.8503e+03 - logprior: 0.2502
Epoch 5/10
39/39 - 63s - loss: 1846.9163 - loglik: -1.8470e+03 - logprior: 0.1182
Epoch 6/10
39/39 - 61s - loss: 1848.5026 - loglik: -1.8485e+03 - logprior: -5.0110e-02
Fitted a model with MAP estimate = -1846.5810
Time for alignment: 1319.5028
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 2082.8284 - loglik: -2.0808e+03 - logprior: -2.0570e+00
Epoch 2/10
39/39 - 46s - loss: 1921.4862 - loglik: -1.9203e+03 - logprior: -1.1393e+00
Epoch 3/10
39/39 - 46s - loss: 1909.2406 - loglik: -1.9079e+03 - logprior: -1.3045e+00
Epoch 4/10
39/39 - 47s - loss: 1906.3350 - loglik: -1.9049e+03 - logprior: -1.4718e+00
Epoch 5/10
39/39 - 47s - loss: 1905.3091 - loglik: -1.9036e+03 - logprior: -1.7406e+00
Epoch 6/10
39/39 - 48s - loss: 1903.5585 - loglik: -1.9016e+03 - logprior: -1.9792e+00
Epoch 7/10
39/39 - 51s - loss: 1903.4573 - loglik: -1.9013e+03 - logprior: -2.2030e+00
Epoch 8/10
39/39 - 52s - loss: 1903.7075 - loglik: -1.9012e+03 - logprior: -2.4638e+00
Fitted a model with MAP estimate = -1882.9321
expansions: [(0, 3), (5, 1), (42, 1), (52, 1), (53, 1), (57, 1), (70, 1), (71, 1), (75, 2), (105, 1), (120, 2), (121, 1), (123, 1), (132, 1), (146, 3), (148, 1), (167, 1), (170, 1), (176, 1), (177, 1), (189, 1), (193, 2), (196, 1), (197, 1), (200, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (262, 3), (263, 1), (264, 1), (266, 2), (285, 4), (286, 3), (287, 3), (288, 1), (289, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: [281]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 1892.6532 - loglik: -1.8899e+03 - logprior: -2.7062e+00
Epoch 2/2
39/39 - 83s - loss: 1875.2147 - loglik: -1.8744e+03 - logprior: -8.3094e-01
Fitted a model with MAP estimate = -1852.5916
expansions: [(0, 2), (333, 1), (364, 2)]
discards: [  1   4 134 220 308 316 335 336 385]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1877.9147 - loglik: -1.8759e+03 - logprior: -2.0482e+00
Epoch 2/2
39/39 - 78s - loss: 1872.4557 - loglik: -1.8723e+03 - logprior: -1.2531e-01
Fitted a model with MAP estimate = -1850.8117
expansions: [(0, 2), (259, 2), (332, 2)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 72s - loss: 1856.6066 - loglik: -1.8545e+03 - logprior: -2.1327e+00
Epoch 2/10
39/39 - 78s - loss: 1850.6466 - loglik: -1.8507e+03 - logprior: 0.0878
Epoch 3/10
39/39 - 68s - loss: 1848.2925 - loglik: -1.8484e+03 - logprior: 0.1359
Epoch 4/10
39/39 - 64s - loss: 1849.0527 - loglik: -1.8492e+03 - logprior: 0.1720
Fitted a model with MAP estimate = -1845.8075
Time for alignment: 1358.1946
Computed alignments with likelihoods: ['-1845.5775', '-1846.6402', '-1845.8766', '-1846.5810', '-1845.8075']
Best model has likelihood: -1845.5775  (prior= -0.0112 )
time for generating output: 1.3681
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.4195977986638874
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe90429df10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff1158a9f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff143020220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9202e6a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe65dee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f8dcd820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe1623a0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe918a86cd0>, <__main__.SimpleDirichletPrior object at 0x7fe9104481f0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 273.3821 - loglik: -2.7000e+02 - logprior: -3.3854e+00
Epoch 2/10
19/19 - 1s - loss: 248.4823 - loglik: -2.4706e+02 - logprior: -1.4178e+00
Epoch 3/10
19/19 - 1s - loss: 241.0115 - loglik: -2.3946e+02 - logprior: -1.5481e+00
Epoch 4/10
19/19 - 1s - loss: 238.6984 - loglik: -2.3726e+02 - logprior: -1.4351e+00
Epoch 5/10
19/19 - 1s - loss: 237.9690 - loglik: -2.3654e+02 - logprior: -1.4286e+00
Epoch 6/10
19/19 - 1s - loss: 237.7806 - loglik: -2.3634e+02 - logprior: -1.4418e+00
Epoch 7/10
19/19 - 1s - loss: 237.6315 - loglik: -2.3618e+02 - logprior: -1.4520e+00
Epoch 8/10
19/19 - 1s - loss: 237.4712 - loglik: -2.3602e+02 - logprior: -1.4499e+00
Epoch 9/10
19/19 - 1s - loss: 237.7182 - loglik: -2.3627e+02 - logprior: -1.4531e+00
Fitted a model with MAP estimate = -237.3223
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 241.8082 - loglik: -2.3755e+02 - logprior: -4.2538e+00
Epoch 2/2
19/19 - 1s - loss: 234.6442 - loglik: -2.3261e+02 - logprior: -2.0329e+00
Fitted a model with MAP estimate = -233.2164
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 235.1398 - loglik: -2.3206e+02 - logprior: -3.0845e+00
Epoch 2/2
19/19 - 1s - loss: 232.3112 - loglik: -2.3104e+02 - logprior: -1.2667e+00
Fitted a model with MAP estimate = -231.7481
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 236.0228 - loglik: -2.3249e+02 - logprior: -3.5332e+00
Epoch 2/10
19/19 - 1s - loss: 232.6600 - loglik: -2.3127e+02 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 1s - loss: 232.0180 - loglik: -2.3072e+02 - logprior: -1.3004e+00
Epoch 4/10
19/19 - 1s - loss: 231.9130 - loglik: -2.3065e+02 - logprior: -1.2612e+00
Epoch 5/10
19/19 - 1s - loss: 231.6345 - loglik: -2.3039e+02 - logprior: -1.2424e+00
Epoch 6/10
19/19 - 1s - loss: 231.6972 - loglik: -2.3046e+02 - logprior: -1.2331e+00
Fitted a model with MAP estimate = -231.4452
Time for alignment: 47.3938
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 273.2812 - loglik: -2.6989e+02 - logprior: -3.3883e+00
Epoch 2/10
19/19 - 1s - loss: 247.9230 - loglik: -2.4649e+02 - logprior: -1.4316e+00
Epoch 3/10
19/19 - 1s - loss: 239.6377 - loglik: -2.3830e+02 - logprior: -1.3376e+00
Epoch 4/10
19/19 - 1s - loss: 238.0909 - loglik: -2.3680e+02 - logprior: -1.2878e+00
Epoch 5/10
19/19 - 1s - loss: 237.7415 - loglik: -2.3647e+02 - logprior: -1.2669e+00
Epoch 6/10
19/19 - 1s - loss: 237.4446 - loglik: -2.3616e+02 - logprior: -1.2821e+00
Epoch 7/10
19/19 - 1s - loss: 237.2834 - loglik: -2.3600e+02 - logprior: -1.2795e+00
Epoch 8/10
19/19 - 1s - loss: 237.2628 - loglik: -2.3597e+02 - logprior: -1.2892e+00
Epoch 9/10
19/19 - 1s - loss: 237.1375 - loglik: -2.3585e+02 - logprior: -1.2899e+00
Epoch 10/10
19/19 - 1s - loss: 237.3661 - loglik: -2.3607e+02 - logprior: -1.2949e+00
Fitted a model with MAP estimate = -236.9801
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 240.4223 - loglik: -2.3593e+02 - logprior: -4.4933e+00
Epoch 2/2
19/19 - 1s - loss: 232.8764 - loglik: -2.3149e+02 - logprior: -1.3839e+00
Fitted a model with MAP estimate = -231.7401
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.3913 - loglik: -2.3329e+02 - logprior: -4.1050e+00
Epoch 2/2
19/19 - 1s - loss: 232.9217 - loglik: -2.3138e+02 - logprior: -1.5388e+00
Fitted a model with MAP estimate = -232.2291
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.0046 - loglik: -2.3173e+02 - logprior: -3.2754e+00
Epoch 2/10
19/19 - 1s - loss: 232.2904 - loglik: -2.3089e+02 - logprior: -1.4047e+00
Epoch 3/10
19/19 - 1s - loss: 232.1333 - loglik: -2.3083e+02 - logprior: -1.3016e+00
Epoch 4/10
19/19 - 1s - loss: 231.7852 - loglik: -2.3053e+02 - logprior: -1.2571e+00
Epoch 5/10
19/19 - 1s - loss: 231.6799 - loglik: -2.3044e+02 - logprior: -1.2382e+00
Epoch 6/10
19/19 - 1s - loss: 231.5482 - loglik: -2.3032e+02 - logprior: -1.2278e+00
Epoch 7/10
19/19 - 1s - loss: 231.5739 - loglik: -2.3035e+02 - logprior: -1.2226e+00
Fitted a model with MAP estimate = -231.3855
Time for alignment: 47.1676
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 273.2187 - loglik: -2.6983e+02 - logprior: -3.3872e+00
Epoch 2/10
19/19 - 1s - loss: 247.9077 - loglik: -2.4648e+02 - logprior: -1.4325e+00
Epoch 3/10
19/19 - 1s - loss: 240.1349 - loglik: -2.3879e+02 - logprior: -1.3490e+00
Epoch 4/10
19/19 - 1s - loss: 238.0254 - loglik: -2.3674e+02 - logprior: -1.2886e+00
Epoch 5/10
19/19 - 1s - loss: 237.6083 - loglik: -2.3634e+02 - logprior: -1.2681e+00
Epoch 6/10
19/19 - 1s - loss: 237.4303 - loglik: -2.3615e+02 - logprior: -1.2802e+00
Epoch 7/10
19/19 - 1s - loss: 237.3983 - loglik: -2.3612e+02 - logprior: -1.2821e+00
Epoch 8/10
19/19 - 1s - loss: 237.2908 - loglik: -2.3600e+02 - logprior: -1.2900e+00
Epoch 9/10
19/19 - 1s - loss: 237.3659 - loglik: -2.3608e+02 - logprior: -1.2902e+00
Fitted a model with MAP estimate = -237.0143
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.3234 - loglik: -2.3588e+02 - logprior: -4.4400e+00
Epoch 2/2
19/19 - 1s - loss: 233.0353 - loglik: -2.3165e+02 - logprior: -1.3838e+00
Fitted a model with MAP estimate = -231.7670
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.2918 - loglik: -2.3318e+02 - logprior: -4.1150e+00
Epoch 2/2
19/19 - 1s - loss: 232.9454 - loglik: -2.3140e+02 - logprior: -1.5459e+00
Fitted a model with MAP estimate = -232.2056
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 234.9507 - loglik: -2.3168e+02 - logprior: -3.2735e+00
Epoch 2/10
19/19 - 1s - loss: 232.4429 - loglik: -2.3104e+02 - logprior: -1.4078e+00
Epoch 3/10
19/19 - 1s - loss: 232.0436 - loglik: -2.3074e+02 - logprior: -1.3028e+00
Epoch 4/10
19/19 - 1s - loss: 231.8426 - loglik: -2.3058e+02 - logprior: -1.2579e+00
Epoch 5/10
19/19 - 1s - loss: 231.5439 - loglik: -2.3030e+02 - logprior: -1.2390e+00
Epoch 6/10
19/19 - 1s - loss: 231.8099 - loglik: -2.3058e+02 - logprior: -1.2282e+00
Fitted a model with MAP estimate = -231.4361
Time for alignment: 45.8735
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 273.2444 - loglik: -2.6985e+02 - logprior: -3.3916e+00
Epoch 2/10
19/19 - 1s - loss: 247.9357 - loglik: -2.4650e+02 - logprior: -1.4345e+00
Epoch 3/10
19/19 - 1s - loss: 240.0791 - loglik: -2.3874e+02 - logprior: -1.3393e+00
Epoch 4/10
19/19 - 1s - loss: 238.2884 - loglik: -2.3701e+02 - logprior: -1.2822e+00
Epoch 5/10
19/19 - 1s - loss: 237.6097 - loglik: -2.3634e+02 - logprior: -1.2671e+00
Epoch 6/10
19/19 - 1s - loss: 237.4514 - loglik: -2.3617e+02 - logprior: -1.2801e+00
Epoch 7/10
19/19 - 1s - loss: 237.4811 - loglik: -2.3619e+02 - logprior: -1.2866e+00
Fitted a model with MAP estimate = -237.0886
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.1045 - loglik: -2.3576e+02 - logprior: -4.3478e+00
Epoch 2/2
19/19 - 1s - loss: 232.9395 - loglik: -2.3155e+02 - logprior: -1.3896e+00
Fitted a model with MAP estimate = -231.7775
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 237.3238 - loglik: -2.3321e+02 - logprior: -4.1116e+00
Epoch 2/2
19/19 - 1s - loss: 232.9447 - loglik: -2.3140e+02 - logprior: -1.5461e+00
Fitted a model with MAP estimate = -232.2071
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.8892 - loglik: -2.3162e+02 - logprior: -3.2736e+00
Epoch 2/10
19/19 - 1s - loss: 232.4782 - loglik: -2.3107e+02 - logprior: -1.4044e+00
Epoch 3/10
19/19 - 1s - loss: 232.0182 - loglik: -2.3072e+02 - logprior: -1.3028e+00
Epoch 4/10
19/19 - 1s - loss: 231.8104 - loglik: -2.3055e+02 - logprior: -1.2584e+00
Epoch 5/10
19/19 - 1s - loss: 231.6788 - loglik: -2.3044e+02 - logprior: -1.2368e+00
Epoch 6/10
19/19 - 1s - loss: 231.6907 - loglik: -2.3046e+02 - logprior: -1.2295e+00
Fitted a model with MAP estimate = -231.4462
Time for alignment: 42.5997
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 273.2773 - loglik: -2.6989e+02 - logprior: -3.3844e+00
Epoch 2/10
19/19 - 1s - loss: 249.6828 - loglik: -2.4828e+02 - logprior: -1.4063e+00
Epoch 3/10
19/19 - 1s - loss: 242.4501 - loglik: -2.4107e+02 - logprior: -1.3804e+00
Epoch 4/10
19/19 - 1s - loss: 240.1368 - loglik: -2.3879e+02 - logprior: -1.3429e+00
Epoch 5/10
19/19 - 1s - loss: 238.5877 - loglik: -2.3711e+02 - logprior: -1.4750e+00
Epoch 6/10
19/19 - 1s - loss: 237.9635 - loglik: -2.3655e+02 - logprior: -1.4148e+00
Epoch 7/10
19/19 - 1s - loss: 237.5063 - loglik: -2.3605e+02 - logprior: -1.4592e+00
Epoch 8/10
19/19 - 1s - loss: 237.4063 - loglik: -2.3595e+02 - logprior: -1.4519e+00
Epoch 9/10
19/19 - 1s - loss: 237.4585 - loglik: -2.3600e+02 - logprior: -1.4572e+00
Fitted a model with MAP estimate = -237.1438
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 241.7808 - loglik: -2.3752e+02 - logprior: -4.2578e+00
Epoch 2/2
19/19 - 1s - loss: 234.7757 - loglik: -2.3270e+02 - logprior: -2.0741e+00
Fitted a model with MAP estimate = -233.2906
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 235.0564 - loglik: -2.3197e+02 - logprior: -3.0818e+00
Epoch 2/2
19/19 - 1s - loss: 232.3504 - loglik: -2.3108e+02 - logprior: -1.2662e+00
Fitted a model with MAP estimate = -231.7349
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 236.1630 - loglik: -2.3261e+02 - logprior: -3.5523e+00
Epoch 2/10
19/19 - 1s - loss: 232.5408 - loglik: -2.3115e+02 - logprior: -1.3924e+00
Epoch 3/10
19/19 - 1s - loss: 232.0442 - loglik: -2.3075e+02 - logprior: -1.2972e+00
Epoch 4/10
19/19 - 1s - loss: 231.8688 - loglik: -2.3061e+02 - logprior: -1.2589e+00
Epoch 5/10
19/19 - 1s - loss: 231.7376 - loglik: -2.3050e+02 - logprior: -1.2412e+00
Epoch 6/10
19/19 - 1s - loss: 231.5334 - loglik: -2.3030e+02 - logprior: -1.2320e+00
Epoch 7/10
19/19 - 1s - loss: 231.5026 - loglik: -2.3027e+02 - logprior: -1.2310e+00
Epoch 8/10
19/19 - 1s - loss: 231.2786 - loglik: -2.3005e+02 - logprior: -1.2327e+00
Epoch 9/10
19/19 - 1s - loss: 231.3299 - loglik: -2.3008e+02 - logprior: -1.2455e+00
Fitted a model with MAP estimate = -231.3009
Time for alignment: 48.1456
Computed alignments with likelihoods: ['-231.4452', '-231.3855', '-231.4361', '-231.4462', '-231.3009']
Best model has likelihood: -231.3009  (prior= -1.2664 )
time for generating output: 0.0990
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8346610551630843
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2c5f2c9d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9041e15e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905022ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f8213be0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe94067cd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9199f9d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f898bc70>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8f8971ac0>, <__main__.SimpleDirichletPrior object at 0x7fe8f0079fa0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1034.8722 - loglik: -1.0327e+03 - logprior: -2.1714e+00
Epoch 2/10
39/39 - 12s - loss: 934.4198 - loglik: -9.3264e+02 - logprior: -1.7766e+00
Epoch 3/10
39/39 - 12s - loss: 924.3129 - loglik: -9.2246e+02 - logprior: -1.8494e+00
Epoch 4/10
39/39 - 12s - loss: 921.5281 - loglik: -9.1972e+02 - logprior: -1.8063e+00
Epoch 5/10
39/39 - 12s - loss: 920.0056 - loglik: -9.1821e+02 - logprior: -1.7940e+00
Epoch 6/10
39/39 - 12s - loss: 919.0267 - loglik: -9.1719e+02 - logprior: -1.8364e+00
Epoch 7/10
39/39 - 12s - loss: 918.9178 - loglik: -9.1701e+02 - logprior: -1.9074e+00
Epoch 8/10
39/39 - 12s - loss: 917.9502 - loglik: -9.1602e+02 - logprior: -1.9275e+00
Epoch 9/10
39/39 - 12s - loss: 916.9692 - loglik: -9.1498e+02 - logprior: -1.9931e+00
Epoch 10/10
39/39 - 12s - loss: 915.5473 - loglik: -9.1346e+02 - logprior: -2.0887e+00
Fitted a model with MAP estimate = -915.5029
expansions: [(2, 1), (3, 3), (4, 2), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 2), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (117, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 904.9351 - loglik: -9.0281e+02 - logprior: -2.1238e+00
Epoch 2/2
39/39 - 15s - loss: 890.6150 - loglik: -8.8970e+02 - logprior: -9.1287e-01
Fitted a model with MAP estimate = -887.9803
expansions: []
discards: [  2   5  53  58  81  85 124 175]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 894.9262 - loglik: -8.9330e+02 - logprior: -1.6279e+00
Epoch 2/2
39/39 - 15s - loss: 890.9170 - loglik: -8.9040e+02 - logprior: -5.1211e-01
Fitted a model with MAP estimate = -888.9099
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 894.0980 - loglik: -8.9266e+02 - logprior: -1.4344e+00
Epoch 2/10
39/39 - 15s - loss: 890.1949 - loglik: -8.8984e+02 - logprior: -3.5466e-01
Epoch 3/10
39/39 - 15s - loss: 888.4045 - loglik: -8.8817e+02 - logprior: -2.3493e-01
Epoch 4/10
39/39 - 15s - loss: 886.7617 - loglik: -8.8661e+02 - logprior: -1.5049e-01
Epoch 5/10
39/39 - 15s - loss: 884.8082 - loglik: -8.8473e+02 - logprior: -7.9547e-02
Epoch 6/10
39/39 - 15s - loss: 883.7964 - loglik: -8.8376e+02 - logprior: -3.4786e-02
Epoch 7/10
39/39 - 15s - loss: 883.3734 - loglik: -8.8336e+02 - logprior: -8.8722e-03
Epoch 8/10
39/39 - 15s - loss: 881.1516 - loglik: -8.8113e+02 - logprior: -2.0979e-02
Epoch 9/10
39/39 - 15s - loss: 880.4012 - loglik: -8.8037e+02 - logprior: -3.4360e-02
Epoch 10/10
39/39 - 15s - loss: 880.3265 - loglik: -8.8025e+02 - logprior: -7.8466e-02
Fitted a model with MAP estimate = -880.1566
Time for alignment: 414.9415
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1033.5760 - loglik: -1.0314e+03 - logprior: -2.1802e+00
Epoch 2/10
39/39 - 12s - loss: 934.9505 - loglik: -9.3317e+02 - logprior: -1.7851e+00
Epoch 3/10
39/39 - 12s - loss: 926.8564 - loglik: -9.2504e+02 - logprior: -1.8141e+00
Epoch 4/10
39/39 - 12s - loss: 924.4828 - loglik: -9.2272e+02 - logprior: -1.7614e+00
Epoch 5/10
39/39 - 12s - loss: 922.7927 - loglik: -9.2102e+02 - logprior: -1.7694e+00
Epoch 6/10
39/39 - 12s - loss: 921.3292 - loglik: -9.1951e+02 - logprior: -1.8185e+00
Epoch 7/10
39/39 - 12s - loss: 921.3739 - loglik: -9.1950e+02 - logprior: -1.8743e+00
Fitted a model with MAP estimate = -920.1805
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 2), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (114, 1), (116, 1), (123, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 904.5552 - loglik: -9.0258e+02 - logprior: -1.9717e+00
Epoch 2/2
39/39 - 15s - loss: 890.7479 - loglik: -8.8995e+02 - logprior: -7.9597e-01
Fitted a model with MAP estimate = -888.4329
expansions: []
discards: [ 80 119 170]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 894.8314 - loglik: -8.9322e+02 - logprior: -1.6148e+00
Epoch 2/2
39/39 - 15s - loss: 890.6490 - loglik: -8.9012e+02 - logprior: -5.2965e-01
Fitted a model with MAP estimate = -888.8951
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 894.2395 - loglik: -8.9279e+02 - logprior: -1.4542e+00
Epoch 2/10
39/39 - 14s - loss: 890.0189 - loglik: -8.8964e+02 - logprior: -3.7462e-01
Epoch 3/10
39/39 - 14s - loss: 888.6186 - loglik: -8.8837e+02 - logprior: -2.5086e-01
Epoch 4/10
39/39 - 14s - loss: 886.3828 - loglik: -8.8622e+02 - logprior: -1.6356e-01
Epoch 5/10
39/39 - 14s - loss: 885.1315 - loglik: -8.8504e+02 - logprior: -9.6435e-02
Epoch 6/10
39/39 - 14s - loss: 883.6757 - loglik: -8.8362e+02 - logprior: -5.9292e-02
Epoch 7/10
39/39 - 14s - loss: 882.9424 - loglik: -8.8292e+02 - logprior: -2.4978e-02
Epoch 8/10
39/39 - 14s - loss: 882.1306 - loglik: -8.8211e+02 - logprior: -2.3723e-02
Epoch 9/10
39/39 - 14s - loss: 880.4311 - loglik: -8.8040e+02 - logprior: -3.4057e-02
Epoch 10/10
39/39 - 13s - loss: 880.3959 - loglik: -8.8033e+02 - logprior: -6.6566e-02
Fitted a model with MAP estimate = -879.8202
Time for alignment: 369.6498
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1033.4130 - loglik: -1.0312e+03 - logprior: -2.1732e+00
Epoch 2/10
39/39 - 11s - loss: 935.2522 - loglik: -9.3348e+02 - logprior: -1.7692e+00
Epoch 3/10
39/39 - 11s - loss: 927.8258 - loglik: -9.2598e+02 - logprior: -1.8409e+00
Epoch 4/10
39/39 - 11s - loss: 924.8863 - loglik: -9.2312e+02 - logprior: -1.7655e+00
Epoch 5/10
39/39 - 11s - loss: 924.0336 - loglik: -9.2227e+02 - logprior: -1.7597e+00
Epoch 6/10
39/39 - 11s - loss: 923.0507 - loglik: -9.2124e+02 - logprior: -1.8082e+00
Epoch 7/10
39/39 - 11s - loss: 921.8051 - loglik: -9.1993e+02 - logprior: -1.8735e+00
Epoch 8/10
39/39 - 12s - loss: 921.7825 - loglik: -9.1986e+02 - logprior: -1.9215e+00
Epoch 9/10
39/39 - 11s - loss: 921.8717 - loglik: -9.1991e+02 - logprior: -1.9656e+00
Fitted a model with MAP estimate = -920.2435
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 3), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (129, 3), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 905.9767 - loglik: -9.0391e+02 - logprior: -2.0632e+00
Epoch 2/2
39/39 - 15s - loss: 891.3130 - loglik: -8.9045e+02 - logprior: -8.6047e-01
Fitted a model with MAP estimate = -889.4197
expansions: []
discards: [ 51 119 163 171]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 895.1892 - loglik: -8.9353e+02 - logprior: -1.6563e+00
Epoch 2/2
39/39 - 14s - loss: 891.3055 - loglik: -8.9073e+02 - logprior: -5.7426e-01
Fitted a model with MAP estimate = -889.4349
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 894.4539 - loglik: -8.9295e+02 - logprior: -1.4999e+00
Epoch 2/10
39/39 - 14s - loss: 890.6509 - loglik: -8.9024e+02 - logprior: -4.1425e-01
Epoch 3/10
39/39 - 14s - loss: 889.3206 - loglik: -8.8904e+02 - logprior: -2.8049e-01
Epoch 4/10
39/39 - 14s - loss: 886.9012 - loglik: -8.8671e+02 - logprior: -1.8681e-01
Epoch 5/10
39/39 - 14s - loss: 886.5483 - loglik: -8.8646e+02 - logprior: -9.0774e-02
Epoch 6/10
39/39 - 14s - loss: 883.9092 - loglik: -8.8386e+02 - logprior: -5.3949e-02
Epoch 7/10
39/39 - 14s - loss: 884.1528 - loglik: -8.8415e+02 - logprior: -5.2157e-03
Fitted a model with MAP estimate = -883.0306
Time for alignment: 339.8010
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1034.2766 - loglik: -1.0321e+03 - logprior: -2.1707e+00
Epoch 2/10
39/39 - 12s - loss: 935.1099 - loglik: -9.3332e+02 - logprior: -1.7898e+00
Epoch 3/10
39/39 - 11s - loss: 926.6239 - loglik: -9.2479e+02 - logprior: -1.8319e+00
Epoch 4/10
39/39 - 11s - loss: 925.0984 - loglik: -9.2333e+02 - logprior: -1.7682e+00
Epoch 5/10
39/39 - 12s - loss: 923.2906 - loglik: -9.2152e+02 - logprior: -1.7677e+00
Epoch 6/10
39/39 - 12s - loss: 922.5523 - loglik: -9.2073e+02 - logprior: -1.8219e+00
Epoch 7/10
39/39 - 12s - loss: 921.5715 - loglik: -9.1968e+02 - logprior: -1.8874e+00
Epoch 8/10
39/39 - 12s - loss: 921.3090 - loglik: -9.1941e+02 - logprior: -1.9021e+00
Epoch 9/10
39/39 - 12s - loss: 920.3882 - loglik: -9.1844e+02 - logprior: -1.9443e+00
Epoch 10/10
39/39 - 12s - loss: 919.9625 - loglik: -9.1791e+02 - logprior: -2.0496e+00
Fitted a model with MAP estimate = -918.7873
expansions: [(4, 2), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 3), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 906.9337 - loglik: -9.0484e+02 - logprior: -2.0968e+00
Epoch 2/2
39/39 - 15s - loss: 892.4295 - loglik: -8.9153e+02 - logprior: -9.0326e-01
Fitted a model with MAP estimate = -890.3608
expansions: []
discards: [  4  52  57 121 165]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 896.2367 - loglik: -8.9457e+02 - logprior: -1.6690e+00
Epoch 2/2
39/39 - 15s - loss: 892.3182 - loglik: -8.9174e+02 - logprior: -5.7505e-01
Fitted a model with MAP estimate = -890.4167
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 895.6882 - loglik: -8.9419e+02 - logprior: -1.4996e+00
Epoch 2/10
39/39 - 15s - loss: 891.5928 - loglik: -8.9119e+02 - logprior: -4.0537e-01
Epoch 3/10
39/39 - 15s - loss: 889.9424 - loglik: -8.8966e+02 - logprior: -2.8261e-01
Epoch 4/10
39/39 - 15s - loss: 888.0030 - loglik: -8.8780e+02 - logprior: -2.0346e-01
Epoch 5/10
39/39 - 15s - loss: 886.8322 - loglik: -8.8673e+02 - logprior: -9.7784e-02
Epoch 6/10
39/39 - 15s - loss: 885.1661 - loglik: -8.8511e+02 - logprior: -5.3953e-02
Epoch 7/10
39/39 - 15s - loss: 884.8050 - loglik: -8.8480e+02 - logprior: -2.1428e-03
Epoch 8/10
39/39 - 15s - loss: 882.7603 - loglik: -8.8274e+02 - logprior: -1.7188e-02
Epoch 9/10
39/39 - 15s - loss: 881.9932 - loglik: -8.8196e+02 - logprior: -3.2694e-02
Epoch 10/10
39/39 - 16s - loss: 883.0013 - loglik: -8.8292e+02 - logprior: -7.8146e-02
Fitted a model with MAP estimate = -881.3433
Time for alignment: 415.3438
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1034.9596 - loglik: -1.0328e+03 - logprior: -2.1783e+00
Epoch 2/10
39/39 - 12s - loss: 934.6287 - loglik: -9.3283e+02 - logprior: -1.7964e+00
Epoch 3/10
39/39 - 12s - loss: 926.6578 - loglik: -9.2484e+02 - logprior: -1.8192e+00
Epoch 4/10
39/39 - 12s - loss: 924.2764 - loglik: -9.2252e+02 - logprior: -1.7556e+00
Epoch 5/10
39/39 - 12s - loss: 922.5433 - loglik: -9.2078e+02 - logprior: -1.7638e+00
Epoch 6/10
39/39 - 12s - loss: 921.8448 - loglik: -9.2002e+02 - logprior: -1.8200e+00
Epoch 7/10
39/39 - 12s - loss: 920.9326 - loglik: -9.1907e+02 - logprior: -1.8612e+00
Epoch 8/10
39/39 - 12s - loss: 919.8917 - loglik: -9.1797e+02 - logprior: -1.9182e+00
Epoch 9/10
39/39 - 12s - loss: 920.1060 - loglik: -9.1814e+02 - logprior: -1.9698e+00
Fitted a model with MAP estimate = -918.4456
expansions: [(4, 2), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 2), (52, 1), (58, 1), (59, 1), (61, 2), (63, 1), (67, 1), (75, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 905.4975 - loglik: -9.0347e+02 - logprior: -2.0245e+00
Epoch 2/2
39/39 - 16s - loss: 891.8295 - loglik: -8.9104e+02 - logprior: -7.9025e-01
Fitted a model with MAP estimate = -889.9628
expansions: []
discards: [  5  54  79 121]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 895.7501 - loglik: -8.9416e+02 - logprior: -1.5873e+00
Epoch 2/2
39/39 - 15s - loss: 892.1175 - loglik: -8.9161e+02 - logprior: -5.0857e-01
Fitted a model with MAP estimate = -890.0085
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 895.3510 - loglik: -8.9391e+02 - logprior: -1.4365e+00
Epoch 2/10
39/39 - 15s - loss: 891.0308 - loglik: -8.9067e+02 - logprior: -3.5683e-01
Epoch 3/10
39/39 - 15s - loss: 890.1163 - loglik: -8.8989e+02 - logprior: -2.2252e-01
Epoch 4/10
39/39 - 15s - loss: 887.9653 - loglik: -8.8782e+02 - logprior: -1.4603e-01
Epoch 5/10
39/39 - 15s - loss: 885.8329 - loglik: -8.8576e+02 - logprior: -7.5489e-02
Epoch 6/10
39/39 - 14s - loss: 885.0858 - loglik: -8.8503e+02 - logprior: -5.8932e-02
Epoch 7/10
39/39 - 14s - loss: 883.9980 - loglik: -8.8397e+02 - logprior: -2.3896e-02
Epoch 8/10
39/39 - 14s - loss: 883.5517 - loglik: -8.8350e+02 - logprior: -5.1344e-02
Epoch 9/10
39/39 - 14s - loss: 881.9066 - loglik: -8.8183e+02 - logprior: -7.2391e-02
Epoch 10/10
39/39 - 14s - loss: 881.1215 - loglik: -8.8103e+02 - logprior: -9.3112e-02
Fitted a model with MAP estimate = -880.9102
Time for alignment: 402.4960
Computed alignments with likelihoods: ['-880.1566', '-879.8202', '-883.0306', '-881.3433', '-880.9102']
Best model has likelihood: -879.8202  (prior= -0.0596 )
time for generating output: 0.2000
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.76095278604849
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8f87977c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2f0762670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f15a7220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f159afd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe92148c430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f0f64730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9193d9e50>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91948f970>, <__main__.SimpleDirichletPrior object at 0x7fe8f0633400>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 522.9117 - loglik: -5.1950e+02 - logprior: -3.4142e+00
Epoch 2/10
19/19 - 2s - loss: 490.0167 - loglik: -4.8872e+02 - logprior: -1.2938e+00
Epoch 3/10
19/19 - 2s - loss: 476.3622 - loglik: -4.7502e+02 - logprior: -1.3422e+00
Epoch 4/10
19/19 - 2s - loss: 473.2717 - loglik: -4.7198e+02 - logprior: -1.2923e+00
Epoch 5/10
19/19 - 2s - loss: 471.7662 - loglik: -4.7040e+02 - logprior: -1.3669e+00
Epoch 6/10
19/19 - 2s - loss: 471.5031 - loglik: -4.7015e+02 - logprior: -1.3556e+00
Epoch 7/10
19/19 - 2s - loss: 471.3252 - loglik: -4.6996e+02 - logprior: -1.3610e+00
Epoch 8/10
19/19 - 2s - loss: 470.9296 - loglik: -4.6956e+02 - logprior: -1.3731e+00
Epoch 9/10
19/19 - 2s - loss: 470.7369 - loglik: -4.6936e+02 - logprior: -1.3802e+00
Epoch 10/10
19/19 - 2s - loss: 470.8957 - loglik: -4.6949e+02 - logprior: -1.4008e+00
Fitted a model with MAP estimate = -469.8418
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (24, 1), (30, 1), (37, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 472.5207 - loglik: -4.6844e+02 - logprior: -4.0805e+00
Epoch 2/2
19/19 - 3s - loss: 461.5479 - loglik: -4.5968e+02 - logprior: -1.8726e+00
Fitted a model with MAP estimate = -458.8975
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 48 82]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 461.9044 - loglik: -4.5906e+02 - logprior: -2.8445e+00
Epoch 2/2
19/19 - 2s - loss: 458.4189 - loglik: -4.5740e+02 - logprior: -1.0203e+00
Fitted a model with MAP estimate = -456.9629
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 462.3078 - loglik: -4.5890e+02 - logprior: -3.4114e+00
Epoch 2/10
19/19 - 2s - loss: 457.8661 - loglik: -4.5676e+02 - logprior: -1.1041e+00
Epoch 3/10
19/19 - 2s - loss: 457.5115 - loglik: -4.5656e+02 - logprior: -9.4812e-01
Epoch 4/10
19/19 - 3s - loss: 456.4031 - loglik: -4.5550e+02 - logprior: -9.0806e-01
Epoch 5/10
19/19 - 2s - loss: 456.1874 - loglik: -4.5531e+02 - logprior: -8.7631e-01
Epoch 6/10
19/19 - 3s - loss: 456.1897 - loglik: -4.5532e+02 - logprior: -8.6681e-01
Fitted a model with MAP estimate = -455.5246
Time for alignment: 79.9354
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 522.9932 - loglik: -5.1958e+02 - logprior: -3.4151e+00
Epoch 2/10
19/19 - 2s - loss: 491.0829 - loglik: -4.8979e+02 - logprior: -1.2903e+00
Epoch 3/10
19/19 - 2s - loss: 476.9911 - loglik: -4.7565e+02 - logprior: -1.3419e+00
Epoch 4/10
19/19 - 2s - loss: 471.8693 - loglik: -4.7064e+02 - logprior: -1.2291e+00
Epoch 5/10
19/19 - 2s - loss: 468.2654 - loglik: -4.6707e+02 - logprior: -1.1907e+00
Epoch 6/10
19/19 - 2s - loss: 468.2342 - loglik: -4.6707e+02 - logprior: -1.1626e+00
Epoch 7/10
19/19 - 2s - loss: 467.2382 - loglik: -4.6608e+02 - logprior: -1.1536e+00
Epoch 8/10
19/19 - 2s - loss: 467.3921 - loglik: -4.6623e+02 - logprior: -1.1584e+00
Fitted a model with MAP estimate = -466.4028
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (32, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 470.7900 - loglik: -4.6674e+02 - logprior: -4.0482e+00
Epoch 2/2
19/19 - 2s - loss: 461.5429 - loglik: -4.5972e+02 - logprior: -1.8181e+00
Fitted a model with MAP estimate = -458.9560
expansions: [(0, 2)]
discards: [ 0  7 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.7198 - loglik: -4.5888e+02 - logprior: -2.8424e+00
Epoch 2/2
19/19 - 3s - loss: 458.3841 - loglik: -4.5736e+02 - logprior: -1.0227e+00
Fitted a model with MAP estimate = -456.9045
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 462.1922 - loglik: -4.5878e+02 - logprior: -3.4077e+00
Epoch 2/10
19/19 - 2s - loss: 458.2177 - loglik: -4.5712e+02 - logprior: -1.0982e+00
Epoch 3/10
19/19 - 3s - loss: 457.0237 - loglik: -4.5607e+02 - logprior: -9.5145e-01
Epoch 4/10
19/19 - 2s - loss: 456.7162 - loglik: -4.5581e+02 - logprior: -9.0626e-01
Epoch 5/10
19/19 - 2s - loss: 455.9528 - loglik: -4.5507e+02 - logprior: -8.7963e-01
Epoch 6/10
19/19 - 3s - loss: 455.8642 - loglik: -4.5500e+02 - logprior: -8.6031e-01
Epoch 7/10
19/19 - 3s - loss: 455.1252 - loglik: -4.5427e+02 - logprior: -8.5570e-01
Epoch 8/10
19/19 - 3s - loss: 455.7240 - loglik: -4.5487e+02 - logprior: -8.5167e-01
Fitted a model with MAP estimate = -455.2869
Time for alignment: 80.7650
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 522.8545 - loglik: -5.1944e+02 - logprior: -3.4139e+00
Epoch 2/10
19/19 - 2s - loss: 490.7569 - loglik: -4.8947e+02 - logprior: -1.2908e+00
Epoch 3/10
19/19 - 2s - loss: 477.0419 - loglik: -4.7573e+02 - logprior: -1.3089e+00
Epoch 4/10
19/19 - 2s - loss: 472.6403 - loglik: -4.7142e+02 - logprior: -1.2161e+00
Epoch 5/10
19/19 - 2s - loss: 470.3279 - loglik: -4.6913e+02 - logprior: -1.1932e+00
Epoch 6/10
19/19 - 2s - loss: 469.0588 - loglik: -4.6794e+02 - logprior: -1.1172e+00
Epoch 7/10
19/19 - 2s - loss: 468.2901 - loglik: -4.6718e+02 - logprior: -1.1077e+00
Epoch 8/10
19/19 - 2s - loss: 468.1444 - loglik: -4.6704e+02 - logprior: -1.1045e+00
Epoch 9/10
19/19 - 2s - loss: 468.3061 - loglik: -4.6719e+02 - logprior: -1.1196e+00
Fitted a model with MAP estimate = -467.1178
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (29, 1), (37, 2), (38, 2), (54, 2), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 471.1420 - loglik: -4.6708e+02 - logprior: -4.0651e+00
Epoch 2/2
19/19 - 3s - loss: 461.5449 - loglik: -4.5968e+02 - logprior: -1.8610e+00
Fitted a model with MAP estimate = -459.0698
expansions: [(0, 2)]
discards: [ 0  7 45 66]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.7342 - loglik: -4.5888e+02 - logprior: -2.8522e+00
Epoch 2/2
19/19 - 2s - loss: 458.0464 - loglik: -4.5703e+02 - logprior: -1.0123e+00
Fitted a model with MAP estimate = -456.9372
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.3937 - loglik: -4.5898e+02 - logprior: -3.4161e+00
Epoch 2/10
19/19 - 2s - loss: 457.7819 - loglik: -4.5667e+02 - logprior: -1.1099e+00
Epoch 3/10
19/19 - 2s - loss: 457.4954 - loglik: -4.5654e+02 - logprior: -9.5170e-01
Epoch 4/10
19/19 - 3s - loss: 456.2856 - loglik: -4.5539e+02 - logprior: -8.9966e-01
Epoch 5/10
19/19 - 3s - loss: 456.2543 - loglik: -4.5537e+02 - logprior: -8.8383e-01
Epoch 6/10
19/19 - 2s - loss: 455.3956 - loglik: -4.5453e+02 - logprior: -8.6242e-01
Epoch 7/10
19/19 - 3s - loss: 455.6234 - loglik: -4.5477e+02 - logprior: -8.5499e-01
Fitted a model with MAP estimate = -455.3338
Time for alignment: 77.4866
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 522.8115 - loglik: -5.1940e+02 - logprior: -3.4119e+00
Epoch 2/10
19/19 - 2s - loss: 489.4687 - loglik: -4.8816e+02 - logprior: -1.3108e+00
Epoch 3/10
19/19 - 2s - loss: 473.9287 - loglik: -4.7254e+02 - logprior: -1.3891e+00
Epoch 4/10
19/19 - 2s - loss: 470.0620 - loglik: -4.6884e+02 - logprior: -1.2187e+00
Epoch 5/10
19/19 - 2s - loss: 468.9781 - loglik: -4.6781e+02 - logprior: -1.1674e+00
Epoch 6/10
19/19 - 2s - loss: 468.1212 - loglik: -4.6699e+02 - logprior: -1.1344e+00
Epoch 7/10
19/19 - 2s - loss: 468.0998 - loglik: -4.6697e+02 - logprior: -1.1266e+00
Epoch 8/10
19/19 - 2s - loss: 468.1997 - loglik: -4.6706e+02 - logprior: -1.1439e+00
Fitted a model with MAP estimate = -466.9055
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (27, 1), (30, 2), (37, 1), (38, 1), (48, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 470.4856 - loglik: -4.6646e+02 - logprior: -4.0235e+00
Epoch 2/2
19/19 - 3s - loss: 461.1434 - loglik: -4.5936e+02 - logprior: -1.7869e+00
Fitted a model with MAP estimate = -458.7868
expansions: [(0, 2)]
discards: [ 0  7 37]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 461.6584 - loglik: -4.5882e+02 - logprior: -2.8346e+00
Epoch 2/2
19/19 - 2s - loss: 458.2099 - loglik: -4.5720e+02 - logprior: -1.0130e+00
Fitted a model with MAP estimate = -456.8878
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.2745 - loglik: -4.5889e+02 - logprior: -3.3877e+00
Epoch 2/10
19/19 - 2s - loss: 457.9495 - loglik: -4.5686e+02 - logprior: -1.0940e+00
Epoch 3/10
19/19 - 2s - loss: 457.2129 - loglik: -4.5627e+02 - logprior: -9.4778e-01
Epoch 4/10
19/19 - 2s - loss: 456.6888 - loglik: -4.5579e+02 - logprior: -8.9830e-01
Epoch 5/10
19/19 - 3s - loss: 456.1834 - loglik: -4.5531e+02 - logprior: -8.7406e-01
Epoch 6/10
19/19 - 2s - loss: 455.3962 - loglik: -4.5454e+02 - logprior: -8.5366e-01
Epoch 7/10
19/19 - 2s - loss: 455.8098 - loglik: -4.5496e+02 - logprior: -8.5210e-01
Fitted a model with MAP estimate = -455.3413
Time for alignment: 76.2096
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 522.5675 - loglik: -5.1916e+02 - logprior: -3.4123e+00
Epoch 2/10
19/19 - 2s - loss: 489.4172 - loglik: -4.8812e+02 - logprior: -1.3005e+00
Epoch 3/10
19/19 - 2s - loss: 475.3834 - loglik: -4.7403e+02 - logprior: -1.3569e+00
Epoch 4/10
19/19 - 2s - loss: 471.4426 - loglik: -4.7016e+02 - logprior: -1.2832e+00
Epoch 5/10
19/19 - 2s - loss: 469.4143 - loglik: -4.6822e+02 - logprior: -1.1986e+00
Epoch 6/10
19/19 - 2s - loss: 468.7950 - loglik: -4.6764e+02 - logprior: -1.1558e+00
Epoch 7/10
19/19 - 2s - loss: 468.5095 - loglik: -4.6737e+02 - logprior: -1.1440e+00
Epoch 8/10
19/19 - 2s - loss: 468.2951 - loglik: -4.6715e+02 - logprior: -1.1461e+00
Epoch 9/10
19/19 - 2s - loss: 468.0978 - loglik: -4.6693e+02 - logprior: -1.1632e+00
Epoch 10/10
19/19 - 2s - loss: 468.6970 - loglik: -4.6752e+02 - logprior: -1.1815e+00
Fitted a model with MAP estimate = -467.1914
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (18, 1), (37, 1), (38, 1), (39, 1), (48, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 471.0844 - loglik: -4.6704e+02 - logprior: -4.0470e+00
Epoch 2/2
19/19 - 3s - loss: 461.4743 - loglik: -4.5967e+02 - logprior: -1.8055e+00
Fitted a model with MAP estimate = -459.0259
expansions: [(0, 2)]
discards: [0 7]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.5820 - loglik: -4.5875e+02 - logprior: -2.8337e+00
Epoch 2/2
19/19 - 3s - loss: 458.4277 - loglik: -4.5741e+02 - logprior: -1.0137e+00
Fitted a model with MAP estimate = -456.9019
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 462.3086 - loglik: -4.5891e+02 - logprior: -3.4006e+00
Epoch 2/10
19/19 - 2s - loss: 457.9927 - loglik: -4.5690e+02 - logprior: -1.0921e+00
Epoch 3/10
19/19 - 2s - loss: 457.0758 - loglik: -4.5613e+02 - logprior: -9.4171e-01
Epoch 4/10
19/19 - 2s - loss: 456.7083 - loglik: -4.5581e+02 - logprior: -9.0035e-01
Epoch 5/10
19/19 - 3s - loss: 455.9190 - loglik: -4.5505e+02 - logprior: -8.6888e-01
Epoch 6/10
19/19 - 3s - loss: 456.0255 - loglik: -4.5517e+02 - logprior: -8.5594e-01
Fitted a model with MAP estimate = -455.4784
Time for alignment: 79.4446
Computed alignments with likelihoods: ['-455.5246', '-455.2869', '-455.3338', '-455.3413', '-455.4784']
Best model has likelihood: -455.2869  (prior= -0.8249 )
time for generating output: 0.1685
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9744492783647538
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff115d7e0a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff14b8a1c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905bcb460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918f4c5b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff11408f7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91045e940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c5ca760>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff12141b220>, <__main__.SimpleDirichletPrior object at 0x7fe8f19359a0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1191.1598 - loglik: -1.1890e+03 - logprior: -2.1265e+00
Epoch 2/10
39/39 - 12s - loss: 1115.3586 - loglik: -1.1141e+03 - logprior: -1.2216e+00
Epoch 3/10
39/39 - 12s - loss: 1105.8060 - loglik: -1.1046e+03 - logprior: -1.2476e+00
Epoch 4/10
39/39 - 12s - loss: 1103.0200 - loglik: -1.1016e+03 - logprior: -1.3743e+00
Epoch 5/10
39/39 - 12s - loss: 1101.6957 - loglik: -1.1002e+03 - logprior: -1.5103e+00
Epoch 6/10
39/39 - 12s - loss: 1100.8706 - loglik: -1.0992e+03 - logprior: -1.6838e+00
Epoch 7/10
39/39 - 12s - loss: 1100.9528 - loglik: -1.0991e+03 - logprior: -1.8211e+00
Fitted a model with MAP estimate = -1094.5994
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (50, 1), (51, 1), (65, 2), (69, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 2), (127, 1), (136, 1), (138, 1), (141, 2), (142, 2), (149, 1), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1096.6876 - loglik: -1.0939e+03 - logprior: -2.8320e+00
Epoch 2/2
39/39 - 15s - loss: 1083.9277 - loglik: -1.0831e+03 - logprior: -8.0631e-01
Fitted a model with MAP estimate = -1076.5701
expansions: []
discards: [ 23  26  56  81  95 114 158 180]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1087.1145 - loglik: -1.0854e+03 - logprior: -1.7566e+00
Epoch 2/2
39/39 - 14s - loss: 1083.4054 - loglik: -1.0829e+03 - logprior: -5.4297e-01
Fitted a model with MAP estimate = -1076.1303
expansions: []
discards: [173]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1080.6091 - loglik: -1.0790e+03 - logprior: -1.6244e+00
Epoch 2/10
39/39 - 15s - loss: 1077.9060 - loglik: -1.0775e+03 - logprior: -4.1818e-01
Epoch 3/10
39/39 - 15s - loss: 1075.8483 - loglik: -1.0755e+03 - logprior: -3.5690e-01
Epoch 4/10
39/39 - 15s - loss: 1074.1880 - loglik: -1.0738e+03 - logprior: -3.6594e-01
Epoch 5/10
39/39 - 15s - loss: 1072.6844 - loglik: -1.0722e+03 - logprior: -4.6798e-01
Epoch 6/10
39/39 - 16s - loss: 1072.3282 - loglik: -1.0718e+03 - logprior: -5.5934e-01
Epoch 7/10
39/39 - 16s - loss: 1071.9961 - loglik: -1.0714e+03 - logprior: -6.0359e-01
Epoch 8/10
39/39 - 15s - loss: 1071.0686 - loglik: -1.0704e+03 - logprior: -6.6130e-01
Epoch 9/10
39/39 - 15s - loss: 1070.8431 - loglik: -1.0701e+03 - logprior: -7.4368e-01
Epoch 10/10
39/39 - 15s - loss: 1070.8379 - loglik: -1.0701e+03 - logprior: -7.7729e-01
Fitted a model with MAP estimate = -1070.1942
Time for alignment: 385.3142
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1191.6041 - loglik: -1.1895e+03 - logprior: -2.1174e+00
Epoch 2/10
39/39 - 12s - loss: 1112.4125 - loglik: -1.1111e+03 - logprior: -1.2964e+00
Epoch 3/10
39/39 - 12s - loss: 1103.4982 - loglik: -1.1022e+03 - logprior: -1.2529e+00
Epoch 4/10
39/39 - 11s - loss: 1101.3700 - loglik: -1.1001e+03 - logprior: -1.3087e+00
Epoch 5/10
39/39 - 11s - loss: 1100.0079 - loglik: -1.0986e+03 - logprior: -1.4364e+00
Epoch 6/10
39/39 - 11s - loss: 1099.4504 - loglik: -1.0978e+03 - logprior: -1.6111e+00
Epoch 7/10
39/39 - 11s - loss: 1098.9301 - loglik: -1.0972e+03 - logprior: -1.7327e+00
Epoch 8/10
39/39 - 11s - loss: 1098.8440 - loglik: -1.0970e+03 - logprior: -1.8725e+00
Epoch 9/10
39/39 - 12s - loss: 1098.5859 - loglik: -1.0966e+03 - logprior: -1.9845e+00
Epoch 10/10
39/39 - 11s - loss: 1098.4481 - loglik: -1.0964e+03 - logprior: -2.0598e+00
Fitted a model with MAP estimate = -1092.4487
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (33, 1), (35, 1), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 3), (132, 1), (142, 4), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1096.1532 - loglik: -1.0933e+03 - logprior: -2.8396e+00
Epoch 2/2
39/39 - 16s - loss: 1082.8073 - loglik: -1.0821e+03 - logprior: -7.0208e-01
Fitted a model with MAP estimate = -1075.6985
expansions: []
discards: [ 25 111 177 178 189]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1085.6357 - loglik: -1.0839e+03 - logprior: -1.7107e+00
Epoch 2/2
39/39 - 16s - loss: 1082.6967 - loglik: -1.0822e+03 - logprior: -4.9141e-01
Fitted a model with MAP estimate = -1075.5352
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1079.8215 - loglik: -1.0782e+03 - logprior: -1.5820e+00
Epoch 2/10
39/39 - 17s - loss: 1076.9412 - loglik: -1.0766e+03 - logprior: -3.7393e-01
Epoch 3/10
39/39 - 17s - loss: 1074.4156 - loglik: -1.0741e+03 - logprior: -2.9695e-01
Epoch 4/10
39/39 - 17s - loss: 1073.5404 - loglik: -1.0732e+03 - logprior: -3.1267e-01
Epoch 5/10
39/39 - 17s - loss: 1072.2877 - loglik: -1.0719e+03 - logprior: -4.0966e-01
Epoch 6/10
39/39 - 17s - loss: 1070.9766 - loglik: -1.0705e+03 - logprior: -4.9461e-01
Epoch 7/10
39/39 - 17s - loss: 1071.0840 - loglik: -1.0705e+03 - logprior: -5.4513e-01
Fitted a model with MAP estimate = -1070.3406
Time for alignment: 391.6186
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1191.8151 - loglik: -1.1897e+03 - logprior: -2.1318e+00
Epoch 2/10
39/39 - 12s - loss: 1112.2980 - loglik: -1.1110e+03 - logprior: -1.2482e+00
Epoch 3/10
39/39 - 12s - loss: 1104.6630 - loglik: -1.1034e+03 - logprior: -1.2458e+00
Epoch 4/10
39/39 - 12s - loss: 1102.0288 - loglik: -1.1007e+03 - logprior: -1.3391e+00
Epoch 5/10
39/39 - 12s - loss: 1100.4050 - loglik: -1.0989e+03 - logprior: -1.4967e+00
Epoch 6/10
39/39 - 12s - loss: 1099.8097 - loglik: -1.0982e+03 - logprior: -1.6355e+00
Epoch 7/10
39/39 - 12s - loss: 1099.7032 - loglik: -1.0979e+03 - logprior: -1.7793e+00
Epoch 8/10
39/39 - 12s - loss: 1099.3235 - loglik: -1.0974e+03 - logprior: -1.9205e+00
Epoch 9/10
39/39 - 12s - loss: 1099.2202 - loglik: -1.0972e+03 - logprior: -2.0216e+00
Epoch 10/10
39/39 - 12s - loss: 1098.9611 - loglik: -1.0969e+03 - logprior: -2.1095e+00
Fitted a model with MAP estimate = -1092.8424
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (33, 1), (35, 1), (45, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (112, 1), (116, 1), (124, 3), (138, 1), (140, 2), (141, 1), (145, 1), (148, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 218 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1095.7329 - loglik: -1.0929e+03 - logprior: -2.8279e+00
Epoch 2/2
39/39 - 16s - loss: 1082.7107 - loglik: -1.0820e+03 - logprior: -7.2582e-01
Fitted a model with MAP estimate = -1075.4742
expansions: []
discards: [ 26 111]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1085.1083 - loglik: -1.0834e+03 - logprior: -1.7157e+00
Epoch 2/2
39/39 - 17s - loss: 1082.0060 - loglik: -1.0815e+03 - logprior: -4.9483e-01
Fitted a model with MAP estimate = -1074.9636
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1079.1052 - loglik: -1.0775e+03 - logprior: -1.5857e+00
Epoch 2/10
39/39 - 17s - loss: 1076.3055 - loglik: -1.0759e+03 - logprior: -3.7240e-01
Epoch 3/10
39/39 - 17s - loss: 1074.7827 - loglik: -1.0745e+03 - logprior: -3.0150e-01
Epoch 4/10
39/39 - 17s - loss: 1072.5627 - loglik: -1.0723e+03 - logprior: -3.1220e-01
Epoch 5/10
39/39 - 17s - loss: 1071.6322 - loglik: -1.0712e+03 - logprior: -4.1708e-01
Epoch 6/10
39/39 - 17s - loss: 1070.6105 - loglik: -1.0701e+03 - logprior: -4.8184e-01
Epoch 7/10
39/39 - 17s - loss: 1070.4713 - loglik: -1.0699e+03 - logprior: -5.2943e-01
Epoch 8/10
39/39 - 17s - loss: 1070.0204 - loglik: -1.0694e+03 - logprior: -6.0894e-01
Epoch 9/10
39/39 - 17s - loss: 1068.8909 - loglik: -1.0682e+03 - logprior: -6.7539e-01
Epoch 10/10
39/39 - 17s - loss: 1069.3820 - loglik: -1.0687e+03 - logprior: -7.3164e-01
Fitted a model with MAP estimate = -1068.4449
Time for alignment: 452.4383
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1190.5006 - loglik: -1.1884e+03 - logprior: -2.1153e+00
Epoch 2/10
39/39 - 12s - loss: 1114.9160 - loglik: -1.1137e+03 - logprior: -1.2245e+00
Epoch 3/10
39/39 - 12s - loss: 1105.8987 - loglik: -1.1047e+03 - logprior: -1.1967e+00
Epoch 4/10
39/39 - 12s - loss: 1103.0007 - loglik: -1.1017e+03 - logprior: -1.3359e+00
Epoch 5/10
39/39 - 12s - loss: 1101.6354 - loglik: -1.1002e+03 - logprior: -1.4681e+00
Epoch 6/10
39/39 - 12s - loss: 1101.2581 - loglik: -1.0996e+03 - logprior: -1.6284e+00
Epoch 7/10
39/39 - 12s - loss: 1100.5602 - loglik: -1.0988e+03 - logprior: -1.7633e+00
Epoch 8/10
39/39 - 12s - loss: 1100.3301 - loglik: -1.0984e+03 - logprior: -1.9113e+00
Epoch 9/10
39/39 - 12s - loss: 1100.5388 - loglik: -1.0985e+03 - logprior: -2.0403e+00
Fitted a model with MAP estimate = -1094.1719
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (69, 1), (70, 1), (76, 1), (85, 1), (86, 1), (87, 3), (88, 2), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 2), (127, 1), (136, 1), (142, 4), (146, 1), (147, 1), (149, 3), (150, 1), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1097.5150 - loglik: -1.0946e+03 - logprior: -2.9338e+00
Epoch 2/2
39/39 - 16s - loss: 1083.7570 - loglik: -1.0829e+03 - logprior: -8.8498e-01
Fitted a model with MAP estimate = -1076.1022
expansions: []
discards: [ 24  55 107 110 113 157 179 180 190 197]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1086.9520 - loglik: -1.0852e+03 - logprior: -1.7287e+00
Epoch 2/2
39/39 - 14s - loss: 1083.4347 - loglik: -1.0829e+03 - logprior: -5.0986e-01
Fitted a model with MAP estimate = -1076.2720
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1080.5190 - loglik: -1.0789e+03 - logprior: -1.5963e+00
Epoch 2/10
39/39 - 15s - loss: 1077.8628 - loglik: -1.0775e+03 - logprior: -3.8091e-01
Epoch 3/10
39/39 - 15s - loss: 1075.3434 - loglik: -1.0750e+03 - logprior: -3.1722e-01
Epoch 4/10
39/39 - 15s - loss: 1074.8762 - loglik: -1.0746e+03 - logprior: -3.2597e-01
Epoch 5/10
39/39 - 15s - loss: 1072.3352 - loglik: -1.0719e+03 - logprior: -4.3152e-01
Epoch 6/10
39/39 - 16s - loss: 1071.4451 - loglik: -1.0709e+03 - logprior: -5.1499e-01
Epoch 7/10
39/39 - 16s - loss: 1071.8921 - loglik: -1.0713e+03 - logprior: -5.7770e-01
Fitted a model with MAP estimate = -1070.9989
Time for alignment: 367.0177
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1192.0697 - loglik: -1.1899e+03 - logprior: -2.1347e+00
Epoch 2/10
39/39 - 12s - loss: 1114.1315 - loglik: -1.1129e+03 - logprior: -1.2331e+00
Epoch 3/10
39/39 - 12s - loss: 1104.9685 - loglik: -1.1037e+03 - logprior: -1.2987e+00
Epoch 4/10
39/39 - 12s - loss: 1102.3914 - loglik: -1.1010e+03 - logprior: -1.3993e+00
Epoch 5/10
39/39 - 12s - loss: 1101.1344 - loglik: -1.0996e+03 - logprior: -1.5332e+00
Epoch 6/10
39/39 - 12s - loss: 1100.5660 - loglik: -1.0989e+03 - logprior: -1.6688e+00
Epoch 7/10
39/39 - 12s - loss: 1100.0823 - loglik: -1.0983e+03 - logprior: -1.7761e+00
Epoch 8/10
39/39 - 12s - loss: 1099.8329 - loglik: -1.0979e+03 - logprior: -1.9078e+00
Epoch 9/10
39/39 - 12s - loss: 1099.7856 - loglik: -1.0977e+03 - logprior: -2.0447e+00
Epoch 10/10
39/39 - 12s - loss: 1099.5574 - loglik: -1.0974e+03 - logprior: -2.1405e+00
Fitted a model with MAP estimate = -1093.4091
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (35, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (126, 1), (127, 2), (129, 1), (131, 1), (134, 1), (139, 2), (140, 1), (148, 2), (149, 2), (150, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1096.4912 - loglik: -1.0936e+03 - logprior: -2.9004e+00
Epoch 2/2
39/39 - 15s - loss: 1083.1351 - loglik: -1.0824e+03 - logprior: -7.2941e-01
Fitted a model with MAP estimate = -1075.8175
expansions: [(186, 1)]
discards: [ 23  25  56  94 113 178 191]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1085.9681 - loglik: -1.0843e+03 - logprior: -1.7083e+00
Epoch 2/2
39/39 - 15s - loss: 1082.4803 - loglik: -1.0820e+03 - logprior: -4.7856e-01
Fitted a model with MAP estimate = -1075.4344
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1079.3065 - loglik: -1.0777e+03 - logprior: -1.5731e+00
Epoch 2/10
39/39 - 16s - loss: 1077.3597 - loglik: -1.0770e+03 - logprior: -3.6487e-01
Epoch 3/10
39/39 - 16s - loss: 1073.9076 - loglik: -1.0736e+03 - logprior: -2.9549e-01
Epoch 4/10
39/39 - 17s - loss: 1073.4362 - loglik: -1.0731e+03 - logprior: -3.1707e-01
Epoch 5/10
39/39 - 17s - loss: 1072.1002 - loglik: -1.0717e+03 - logprior: -4.0774e-01
Epoch 6/10
39/39 - 17s - loss: 1070.9581 - loglik: -1.0705e+03 - logprior: -4.9358e-01
Epoch 7/10
39/39 - 17s - loss: 1071.1792 - loglik: -1.0706e+03 - logprior: -5.4104e-01
Fitted a model with MAP estimate = -1070.0365
Time for alignment: 386.4548
Computed alignments with likelihoods: ['-1070.1942', '-1070.3406', '-1068.4449', '-1070.9989', '-1070.0365']
Best model has likelihood: -1068.4449  (prior= -0.7798 )
time for generating output: 0.3237
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7708023169256537
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe920629be0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fef80129130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c591e1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2dea58b80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe940266190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9302711c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2de8f39d0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef407a6460>, <__main__.SimpleDirichletPrior object at 0x7fe8f1bb0ac0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 337.9573 - loglik: -3.3457e+02 - logprior: -3.3826e+00
Epoch 2/10
19/19 - 1s - loss: 308.4308 - loglik: -3.0701e+02 - logprior: -1.4205e+00
Epoch 3/10
19/19 - 1s - loss: 301.3390 - loglik: -2.9983e+02 - logprior: -1.5083e+00
Epoch 4/10
19/19 - 1s - loss: 299.6347 - loglik: -2.9822e+02 - logprior: -1.4176e+00
Epoch 5/10
19/19 - 1s - loss: 299.0356 - loglik: -2.9758e+02 - logprior: -1.4509e+00
Epoch 6/10
19/19 - 1s - loss: 299.2187 - loglik: -2.9777e+02 - logprior: -1.4488e+00
Fitted a model with MAP estimate = -298.7559
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (28, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 306.8812 - loglik: -3.0264e+02 - logprior: -4.2373e+00
Epoch 2/2
19/19 - 1s - loss: 299.3513 - loglik: -2.9716e+02 - logprior: -2.1937e+00
Fitted a model with MAP estimate = -296.8314
expansions: []
discards: [13 14 38 43 44 53]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.1199 - loglik: -2.9574e+02 - logprior: -3.3811e+00
Epoch 2/2
19/19 - 1s - loss: 295.8845 - loglik: -2.9445e+02 - logprior: -1.4383e+00
Fitted a model with MAP estimate = -295.3529
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 298.2100 - loglik: -2.9495e+02 - logprior: -3.2620e+00
Epoch 2/10
19/19 - 1s - loss: 295.7248 - loglik: -2.9432e+02 - logprior: -1.4000e+00
Epoch 3/10
19/19 - 1s - loss: 295.1892 - loglik: -2.9387e+02 - logprior: -1.3173e+00
Epoch 4/10
19/19 - 1s - loss: 295.0675 - loglik: -2.9379e+02 - logprior: -1.2788e+00
Epoch 5/10
19/19 - 1s - loss: 294.5463 - loglik: -2.9327e+02 - logprior: -1.2724e+00
Epoch 6/10
19/19 - 1s - loss: 294.5806 - loglik: -2.9330e+02 - logprior: -1.2845e+00
Fitted a model with MAP estimate = -294.3209
Time for alignment: 49.0811
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.9165 - loglik: -3.3454e+02 - logprior: -3.3803e+00
Epoch 2/10
19/19 - 1s - loss: 308.4031 - loglik: -3.0698e+02 - logprior: -1.4224e+00
Epoch 3/10
19/19 - 1s - loss: 301.1206 - loglik: -2.9961e+02 - logprior: -1.5145e+00
Epoch 4/10
19/19 - 1s - loss: 299.7910 - loglik: -2.9837e+02 - logprior: -1.4217e+00
Epoch 5/10
19/19 - 1s - loss: 299.1241 - loglik: -2.9767e+02 - logprior: -1.4509e+00
Epoch 6/10
19/19 - 1s - loss: 298.7557 - loglik: -2.9731e+02 - logprior: -1.4483e+00
Epoch 7/10
19/19 - 1s - loss: 298.8698 - loglik: -2.9742e+02 - logprior: -1.4519e+00
Fitted a model with MAP estimate = -298.6576
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (28, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 306.5200 - loglik: -3.0229e+02 - logprior: -4.2313e+00
Epoch 2/2
19/19 - 1s - loss: 299.4191 - loglik: -2.9720e+02 - logprior: -2.2158e+00
Fitted a model with MAP estimate = -296.7587
expansions: []
discards: [12 13 16 36 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.2652 - loglik: -2.9591e+02 - logprior: -3.3570e+00
Epoch 2/2
19/19 - 1s - loss: 296.0664 - loglik: -2.9465e+02 - logprior: -1.4155e+00
Fitted a model with MAP estimate = -295.5628
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.4008 - loglik: -2.9515e+02 - logprior: -3.2517e+00
Epoch 2/10
19/19 - 1s - loss: 295.8163 - loglik: -2.9442e+02 - logprior: -1.3944e+00
Epoch 3/10
19/19 - 1s - loss: 295.3760 - loglik: -2.9407e+02 - logprior: -1.3092e+00
Epoch 4/10
19/19 - 1s - loss: 295.1219 - loglik: -2.9385e+02 - logprior: -1.2690e+00
Epoch 5/10
19/19 - 1s - loss: 294.7584 - loglik: -2.9349e+02 - logprior: -1.2649e+00
Epoch 6/10
19/19 - 1s - loss: 294.7784 - loglik: -2.9350e+02 - logprior: -1.2756e+00
Fitted a model with MAP estimate = -294.5206
Time for alignment: 46.6210
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.9195 - loglik: -3.3453e+02 - logprior: -3.3846e+00
Epoch 2/10
19/19 - 1s - loss: 309.4786 - loglik: -3.0805e+02 - logprior: -1.4273e+00
Epoch 3/10
19/19 - 1s - loss: 301.1144 - loglik: -2.9958e+02 - logprior: -1.5342e+00
Epoch 4/10
19/19 - 1s - loss: 299.7495 - loglik: -2.9831e+02 - logprior: -1.4358e+00
Epoch 5/10
19/19 - 1s - loss: 299.3540 - loglik: -2.9789e+02 - logprior: -1.4613e+00
Epoch 6/10
19/19 - 1s - loss: 299.0648 - loglik: -2.9761e+02 - logprior: -1.4563e+00
Epoch 7/10
19/19 - 1s - loss: 298.7757 - loglik: -2.9731e+02 - logprior: -1.4622e+00
Epoch 8/10
19/19 - 1s - loss: 298.9823 - loglik: -2.9752e+02 - logprior: -1.4649e+00
Fitted a model with MAP estimate = -298.7159
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 306.3807 - loglik: -3.0216e+02 - logprior: -4.2171e+00
Epoch 2/2
19/19 - 1s - loss: 299.6659 - loglik: -2.9748e+02 - logprior: -2.1897e+00
Fitted a model with MAP estimate = -296.9191
expansions: []
discards: [13 14 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.0206 - loglik: -2.9563e+02 - logprior: -3.3880e+00
Epoch 2/2
19/19 - 1s - loss: 295.8132 - loglik: -2.9439e+02 - logprior: -1.4187e+00
Fitted a model with MAP estimate = -295.3381
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.4175 - loglik: -2.9517e+02 - logprior: -3.2500e+00
Epoch 2/10
19/19 - 1s - loss: 295.8026 - loglik: -2.9441e+02 - logprior: -1.3900e+00
Epoch 3/10
19/19 - 1s - loss: 295.6214 - loglik: -2.9431e+02 - logprior: -1.3067e+00
Epoch 4/10
19/19 - 1s - loss: 295.0216 - loglik: -2.9376e+02 - logprior: -1.2661e+00
Epoch 5/10
19/19 - 1s - loss: 294.9117 - loglik: -2.9365e+02 - logprior: -1.2616e+00
Epoch 6/10
19/19 - 1s - loss: 294.9153 - loglik: -2.9364e+02 - logprior: -1.2733e+00
Fitted a model with MAP estimate = -294.5691
Time for alignment: 48.1054
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 337.8567 - loglik: -3.3447e+02 - logprior: -3.3818e+00
Epoch 2/10
19/19 - 1s - loss: 309.7073 - loglik: -3.0829e+02 - logprior: -1.4199e+00
Epoch 3/10
19/19 - 1s - loss: 302.1598 - loglik: -3.0065e+02 - logprior: -1.5147e+00
Epoch 4/10
19/19 - 1s - loss: 299.9737 - loglik: -2.9855e+02 - logprior: -1.4217e+00
Epoch 5/10
19/19 - 1s - loss: 299.1982 - loglik: -2.9774e+02 - logprior: -1.4611e+00
Epoch 6/10
19/19 - 1s - loss: 299.0473 - loglik: -2.9759e+02 - logprior: -1.4563e+00
Epoch 7/10
19/19 - 1s - loss: 299.0291 - loglik: -2.9756e+02 - logprior: -1.4677e+00
Epoch 8/10
19/19 - 1s - loss: 298.7231 - loglik: -2.9726e+02 - logprior: -1.4671e+00
Epoch 9/10
19/19 - 1s - loss: 298.9943 - loglik: -2.9753e+02 - logprior: -1.4679e+00
Fitted a model with MAP estimate = -298.6522
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 306.5254 - loglik: -3.0231e+02 - logprior: -4.2191e+00
Epoch 2/2
19/19 - 1s - loss: 299.4836 - loglik: -2.9729e+02 - logprior: -2.1915e+00
Fitted a model with MAP estimate = -297.0282
expansions: []
discards: [13 14 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.1087 - loglik: -2.9571e+02 - logprior: -3.4024e+00
Epoch 2/2
19/19 - 1s - loss: 295.7874 - loglik: -2.9437e+02 - logprior: -1.4173e+00
Fitted a model with MAP estimate = -295.3395
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 298.5099 - loglik: -2.9526e+02 - logprior: -3.2473e+00
Epoch 2/10
19/19 - 1s - loss: 295.9187 - loglik: -2.9453e+02 - logprior: -1.3882e+00
Epoch 3/10
19/19 - 1s - loss: 295.2914 - loglik: -2.9399e+02 - logprior: -1.3060e+00
Epoch 4/10
19/19 - 1s - loss: 295.2357 - loglik: -2.9397e+02 - logprior: -1.2637e+00
Epoch 5/10
19/19 - 1s - loss: 294.9856 - loglik: -2.9372e+02 - logprior: -1.2633e+00
Epoch 6/10
19/19 - 1s - loss: 294.6777 - loglik: -2.9340e+02 - logprior: -1.2759e+00
Epoch 7/10
19/19 - 1s - loss: 294.4459 - loglik: -2.9317e+02 - logprior: -1.2739e+00
Epoch 8/10
19/19 - 1s - loss: 294.5403 - loglik: -2.9327e+02 - logprior: -1.2718e+00
Fitted a model with MAP estimate = -294.4269
Time for alignment: 54.2491
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 338.0033 - loglik: -3.3462e+02 - logprior: -3.3833e+00
Epoch 2/10
19/19 - 1s - loss: 309.5838 - loglik: -3.0816e+02 - logprior: -1.4228e+00
Epoch 3/10
19/19 - 1s - loss: 301.8566 - loglik: -3.0034e+02 - logprior: -1.5204e+00
Epoch 4/10
19/19 - 1s - loss: 300.1738 - loglik: -2.9875e+02 - logprior: -1.4233e+00
Epoch 5/10
19/19 - 1s - loss: 299.9906 - loglik: -2.9854e+02 - logprior: -1.4501e+00
Epoch 6/10
19/19 - 1s - loss: 299.2220 - loglik: -2.9778e+02 - logprior: -1.4417e+00
Epoch 7/10
19/19 - 1s - loss: 299.4145 - loglik: -2.9796e+02 - logprior: -1.4555e+00
Fitted a model with MAP estimate = -299.2193
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 306.7604 - loglik: -3.0252e+02 - logprior: -4.2354e+00
Epoch 2/2
19/19 - 1s - loss: 299.4915 - loglik: -2.9731e+02 - logprior: -2.1805e+00
Fitted a model with MAP estimate = -296.9323
expansions: []
discards: [11 13 14 17 18 42 48 52]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 299.4012 - loglik: -2.9605e+02 - logprior: -3.3499e+00
Epoch 2/2
19/19 - 1s - loss: 296.0178 - loglik: -2.9461e+02 - logprior: -1.4114e+00
Fitted a model with MAP estimate = -295.5714
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.3999 - loglik: -2.9515e+02 - logprior: -3.2499e+00
Epoch 2/10
19/19 - 1s - loss: 295.9141 - loglik: -2.9452e+02 - logprior: -1.3936e+00
Epoch 3/10
19/19 - 1s - loss: 295.4965 - loglik: -2.9419e+02 - logprior: -1.3084e+00
Epoch 4/10
19/19 - 1s - loss: 295.0883 - loglik: -2.9382e+02 - logprior: -1.2689e+00
Epoch 5/10
19/19 - 1s - loss: 295.1501 - loglik: -2.9388e+02 - logprior: -1.2677e+00
Fitted a model with MAP estimate = -294.7324
Time for alignment: 45.5414
Computed alignments with likelihoods: ['-294.3209', '-294.5206', '-294.5691', '-294.4269', '-294.7324']
Best model has likelihood: -294.3209  (prior= -1.2767 )
time for generating output: 0.1124
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8855421686746988
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9203dbd90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9402766d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe920661190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9195e27f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe92102d340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91040bfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff142644f70>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe920b0f760>, <__main__.SimpleDirichletPrior object at 0x7fe911b4b340>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 1961.1074 - loglik: -1.9588e+03 - logprior: -2.2816e+00
Epoch 2/10
39/39 - 72s - loss: 1779.4958 - loglik: -1.7782e+03 - logprior: -1.2836e+00
Epoch 3/10
39/39 - 74s - loss: 1765.4722 - loglik: -1.7642e+03 - logprior: -1.3000e+00
Epoch 4/10
39/39 - 75s - loss: 1763.4417 - loglik: -1.7622e+03 - logprior: -1.2144e+00
Epoch 5/10
39/39 - 75s - loss: 1762.3318 - loglik: -1.7611e+03 - logprior: -1.2516e+00
Epoch 6/10
39/39 - 66s - loss: 1762.3724 - loglik: -1.7611e+03 - logprior: -1.3122e+00
Fitted a model with MAP estimate = -1759.7910
expansions: [(0, 3), (43, 1), (62, 1), (133, 1), (162, 1), (163, 1), (164, 2), (169, 1), (174, 6), (175, 4), (176, 1), (177, 1), (188, 2), (189, 5), (190, 1), (191, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (207, 1), (209, 1), (214, 1), (217, 2), (221, 2), (222, 7), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (242, 1), (243, 1), (244, 3), (245, 2), (247, 2), (248, 5), (249, 2), (252, 1), (271, 1), (285, 1), (286, 2), (287, 2), (288, 2), (290, 1), (301, 1), (303, 1), (304, 1), (305, 1), (313, 1), (316, 1), (326, 1), (329, 1), (349, 1), (355, 5)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 460 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 97s - loss: 1735.6978 - loglik: -1.7333e+03 - logprior: -2.3603e+00
Epoch 2/2
39/39 - 109s - loss: 1716.9429 - loglik: -1.7162e+03 - logprior: -7.4729e-01
Fitted a model with MAP estimate = -1713.4709
expansions: [(218, 1), (283, 1)]
discards: [  2 187 188 189 190 191 214 215 261 274 289 316 317 318 319 373]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 1724.0677 - loglik: -1.7226e+03 - logprior: -1.4574e+00
Epoch 2/2
39/39 - 114s - loss: 1719.1287 - loglik: -1.7188e+03 - logprior: -3.4009e-01
Fitted a model with MAP estimate = -1716.3265
expansions: [(304, 2)]
discards: [306]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 94s - loss: 1721.5126 - loglik: -1.7202e+03 - logprior: -1.3118e+00
Epoch 2/10
39/39 - 93s - loss: 1717.1426 - loglik: -1.7171e+03 - logprior: -3.9999e-02
Epoch 3/10
39/39 - 91s - loss: 1715.6359 - loglik: -1.7158e+03 - logprior: 0.1604
Epoch 4/10
39/39 - 92s - loss: 1714.5465 - loglik: -1.7147e+03 - logprior: 0.1332
Epoch 5/10
39/39 - 93s - loss: 1714.4343 - loglik: -1.7146e+03 - logprior: 0.1321
Epoch 6/10
39/39 - 92s - loss: 1713.4725 - loglik: -1.7138e+03 - logprior: 0.2889
Epoch 7/10
39/39 - 92s - loss: 1710.7313 - loglik: -1.7111e+03 - logprior: 0.3986
Epoch 8/10
39/39 - 90s - loss: 1713.1263 - loglik: -1.7135e+03 - logprior: 0.4153
Fitted a model with MAP estimate = -1711.8918
Time for alignment: 2043.5273
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 1962.4243 - loglik: -1.9602e+03 - logprior: -2.2263e+00
Epoch 2/10
39/39 - 58s - loss: 1781.5784 - loglik: -1.7804e+03 - logprior: -1.1442e+00
Epoch 3/10
39/39 - 58s - loss: 1766.8459 - loglik: -1.7656e+03 - logprior: -1.2321e+00
Epoch 4/10
39/39 - 58s - loss: 1765.2875 - loglik: -1.7641e+03 - logprior: -1.2374e+00
Epoch 5/10
39/39 - 58s - loss: 1762.7153 - loglik: -1.7614e+03 - logprior: -1.3024e+00
Epoch 6/10
39/39 - 58s - loss: 1759.9807 - loglik: -1.7586e+03 - logprior: -1.3406e+00
Epoch 7/10
39/39 - 58s - loss: 1761.3075 - loglik: -1.7599e+03 - logprior: -1.4279e+00
Fitted a model with MAP estimate = -1759.8040
expansions: [(0, 3), (38, 1), (86, 2), (131, 1), (134, 1), (142, 1), (161, 1), (162, 1), (163, 1), (167, 1), (168, 1), (173, 6), (174, 4), (175, 1), (176, 1), (187, 1), (188, 1), (189, 4), (190, 1), (191, 1), (194, 1), (195, 3), (196, 1), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 2), (221, 7), (224, 1), (225, 2), (226, 3), (227, 2), (228, 1), (239, 1), (243, 1), (244, 3), (245, 2), (247, 2), (248, 5), (250, 1), (253, 1), (269, 1), (270, 1), (271, 1), (286, 1), (288, 1), (289, 2), (291, 1), (302, 1), (304, 4), (326, 1), (330, 1), (339, 1), (349, 1), (355, 6)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 1737.1791 - loglik: -1.7346e+03 - logprior: -2.6124e+00
Epoch 2/2
39/39 - 95s - loss: 1718.3268 - loglik: -1.7173e+03 - logprior: -9.8870e-01
Fitted a model with MAP estimate = -1714.3117
expansions: [(216, 1), (283, 1), (394, 1)]
discards: [  2  90 188 189 190 191 192 217 274 289 316 317 318 319 457]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 1723.9974 - loglik: -1.7225e+03 - logprior: -1.4999e+00
Epoch 2/2
39/39 - 91s - loss: 1718.6877 - loglik: -1.7183e+03 - logprior: -3.5783e-01
Fitted a model with MAP estimate = -1716.1508
expansions: [(211, 1), (305, 2)]
discards: [307]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 94s - loss: 1720.5428 - loglik: -1.7193e+03 - logprior: -1.2794e+00
Epoch 2/10
39/39 - 92s - loss: 1717.7542 - loglik: -1.7178e+03 - logprior: 0.0095
Epoch 3/10
39/39 - 85s - loss: 1714.4435 - loglik: -1.7146e+03 - logprior: 0.1417
Epoch 4/10
39/39 - 83s - loss: 1713.6196 - loglik: -1.7138e+03 - logprior: 0.2240
Epoch 5/10
39/39 - 82s - loss: 1713.6259 - loglik: -1.7138e+03 - logprior: 0.2201
Fitted a model with MAP estimate = -1712.3361
Time for alignment: 1646.6726
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 1960.2843 - loglik: -1.9580e+03 - logprior: -2.2755e+00
Epoch 2/10
39/39 - 55s - loss: 1778.3910 - loglik: -1.7771e+03 - logprior: -1.3294e+00
Epoch 3/10
39/39 - 55s - loss: 1765.3466 - loglik: -1.7640e+03 - logprior: -1.3509e+00
Epoch 4/10
39/39 - 56s - loss: 1762.2875 - loglik: -1.7609e+03 - logprior: -1.3435e+00
Epoch 5/10
39/39 - 57s - loss: 1761.8865 - loglik: -1.7605e+03 - logprior: -1.3931e+00
Epoch 6/10
39/39 - 59s - loss: 1757.3984 - loglik: -1.7559e+03 - logprior: -1.4534e+00
Epoch 7/10
39/39 - 61s - loss: 1760.4847 - loglik: -1.7589e+03 - logprior: -1.5378e+00
Fitted a model with MAP estimate = -1757.4099
expansions: [(0, 3), (38, 1), (42, 1), (87, 1), (133, 1), (134, 1), (135, 1), (144, 1), (161, 1), (162, 1), (166, 1), (172, 8), (173, 1), (174, 1), (175, 1), (183, 1), (185, 1), (186, 1), (187, 4), (188, 1), (189, 1), (192, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 1), (200, 1), (201, 1), (204, 1), (206, 1), (213, 2), (215, 2), (220, 2), (221, 7), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (240, 1), (242, 1), (243, 1), (244, 2), (245, 2), (247, 2), (248, 6), (250, 1), (252, 1), (253, 1), (255, 1), (266, 1), (268, 1), (270, 1), (286, 1), (287, 3), (289, 1), (300, 1), (302, 1), (303, 1), (304, 1), (316, 1), (326, 1), (329, 1), (346, 1), (355, 5)]
discards: [  2 126]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 99s - loss: 1736.2190 - loglik: -1.7340e+03 - logprior: -2.2335e+00
Epoch 2/2
39/39 - 99s - loss: 1717.4847 - loglik: -1.7169e+03 - logprior: -5.7287e-01
Fitted a model with MAP estimate = -1714.4882
expansions: [(213, 1), (281, 1), (312, 1), (374, 1)]
discards: [  2 186 187 214 258 272 288 314 315 316 317 318]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 97s - loss: 1723.4260 - loglik: -1.7220e+03 - logprior: -1.4639e+00
Epoch 2/2
39/39 - 98s - loss: 1719.7496 - loglik: -1.7193e+03 - logprior: -4.4510e-01
Fitted a model with MAP estimate = -1715.4232
expansions: [(212, 1), (304, 1), (305, 2), (306, 1)]
discards: [  2 307 308 309]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 1720.0675 - loglik: -1.7189e+03 - logprior: -1.2008e+00
Epoch 2/10
39/39 - 80s - loss: 1716.7782 - loglik: -1.7169e+03 - logprior: 0.1499
Epoch 3/10
39/39 - 82s - loss: 1714.0950 - loglik: -1.7144e+03 - logprior: 0.3205
Epoch 4/10
39/39 - 83s - loss: 1711.8854 - loglik: -1.7122e+03 - logprior: 0.3027
Epoch 5/10
39/39 - 84s - loss: 1714.9265 - loglik: -1.7154e+03 - logprior: 0.4613
Fitted a model with MAP estimate = -1711.6522
Time for alignment: 1652.2801
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 1959.9143 - loglik: -1.9577e+03 - logprior: -2.2581e+00
Epoch 2/10
39/39 - 57s - loss: 1779.4088 - loglik: -1.7782e+03 - logprior: -1.1879e+00
Epoch 3/10
39/39 - 56s - loss: 1768.7572 - loglik: -1.7676e+03 - logprior: -1.2034e+00
Epoch 4/10
39/39 - 56s - loss: 1765.4825 - loglik: -1.7644e+03 - logprior: -1.1105e+00
Epoch 5/10
39/39 - 56s - loss: 1763.0215 - loglik: -1.7619e+03 - logprior: -1.1587e+00
Epoch 6/10
39/39 - 57s - loss: 1763.2238 - loglik: -1.7619e+03 - logprior: -1.2815e+00
Fitted a model with MAP estimate = -1761.4150
expansions: [(0, 3), (37, 1), (43, 1), (135, 1), (164, 1), (165, 1), (166, 1), (178, 1), (179, 9), (180, 1), (181, 1), (189, 1), (191, 2), (192, 7), (193, 1), (196, 1), (197, 3), (198, 2), (199, 1), (200, 1), (202, 1), (203, 1), (205, 1), (208, 1), (210, 1), (214, 1), (216, 1), (217, 1), (221, 2), (222, 7), (225, 1), (226, 2), (227, 4), (228, 1), (239, 1), (243, 1), (244, 2), (245, 3), (247, 2), (248, 5), (249, 2), (252, 1), (268, 1), (270, 1), (286, 2), (287, 4), (289, 2), (299, 1), (301, 1), (304, 1), (326, 2), (329, 1), (341, 1), (352, 1), (354, 2), (355, 3)]
discards: [  1   2   3 128]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 1737.3340 - loglik: -1.7348e+03 - logprior: -2.5537e+00
Epoch 2/2
39/39 - 86s - loss: 1718.9684 - loglik: -1.7181e+03 - logprior: -8.9690e-01
Fitted a model with MAP estimate = -1715.3216
expansions: [(217, 1), (282, 1), (284, 1), (328, 1)]
discards: [186 187 188 189 190 214 215 271 312 313 314 315 454]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 1723.8057 - loglik: -1.7223e+03 - logprior: -1.5171e+00
Epoch 2/2
39/39 - 84s - loss: 1719.1669 - loglik: -1.7189e+03 - logprior: -3.0632e-01
Fitted a model with MAP estimate = -1716.5596
expansions: [(304, 2)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 1721.2979 - loglik: -1.7203e+03 - logprior: -1.0101e+00
Epoch 2/10
39/39 - 85s - loss: 1719.2448 - loglik: -1.7193e+03 - logprior: 0.0547
Epoch 3/10
39/39 - 91s - loss: 1715.2925 - loglik: -1.7157e+03 - logprior: 0.3914
Epoch 4/10
39/39 - 104s - loss: 1714.6418 - loglik: -1.7151e+03 - logprior: 0.4085
Epoch 5/10
39/39 - 100s - loss: 1715.1794 - loglik: -1.7156e+03 - logprior: 0.4207
Fitted a model with MAP estimate = -1713.1326
Time for alignment: 1532.7975
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1961.5537 - loglik: -1.9593e+03 - logprior: -2.3009e+00
Epoch 2/10
39/39 - 65s - loss: 1778.1981 - loglik: -1.7768e+03 - logprior: -1.4116e+00
Epoch 3/10
39/39 - 68s - loss: 1764.0563 - loglik: -1.7626e+03 - logprior: -1.4358e+00
Epoch 4/10
39/39 - 70s - loss: 1761.8347 - loglik: -1.7605e+03 - logprior: -1.2936e+00
Epoch 5/10
39/39 - 69s - loss: 1758.6185 - loglik: -1.7573e+03 - logprior: -1.3337e+00
Epoch 6/10
39/39 - 71s - loss: 1758.7275 - loglik: -1.7574e+03 - logprior: -1.3770e+00
Fitted a model with MAP estimate = -1757.1453
expansions: [(0, 3), (37, 1), (43, 1), (106, 1), (131, 1), (134, 1), (135, 1), (163, 1), (164, 1), (168, 1), (175, 5), (176, 4), (177, 1), (190, 2), (191, 5), (192, 1), (193, 1), (196, 1), (197, 3), (198, 1), (200, 1), (201, 2), (202, 2), (203, 1), (206, 1), (208, 1), (210, 1), (218, 1), (223, 2), (224, 5), (225, 1), (228, 1), (229, 2), (230, 1), (231, 1), (232, 2), (245, 1), (246, 1), (248, 2), (249, 2), (250, 3), (251, 7), (252, 2), (253, 1), (255, 1), (256, 1), (270, 1), (271, 1), (272, 1), (287, 2), (288, 2), (289, 2), (291, 1), (292, 1), (301, 1), (303, 1), (304, 1), (305, 1), (307, 1), (316, 1), (326, 1), (329, 1), (341, 1), (355, 5)]
discards: [  2   3 127]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 1736.5002 - loglik: -1.7342e+03 - logprior: -2.2535e+00
Epoch 2/2
39/39 - 107s - loss: 1717.9419 - loglik: -1.7173e+03 - logprior: -6.4731e-01
Fitted a model with MAP estimate = -1714.1295
expansions: [(216, 1), (282, 1), (310, 1), (311, 1), (312, 2)]
discards: [  2 187 188 189 190 213 214 238 271 288 313 314 315 316 317 318 372]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 111s - loss: 1724.7341 - loglik: -1.7233e+03 - logprior: -1.4800e+00
Epoch 2/2
39/39 - 101s - loss: 1720.0375 - loglik: -1.7196e+03 - logprior: -4.2008e-01
Fitted a model with MAP estimate = -1717.2035
expansions: [(304, 1)]
discards: [  2 305 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 110s - loss: 1722.6062 - loglik: -1.7215e+03 - logprior: -1.0692e+00
Epoch 2/10
39/39 - 108s - loss: 1719.3149 - loglik: -1.7196e+03 - logprior: 0.2483
Epoch 3/10
39/39 - 105s - loss: 1716.0042 - loglik: -1.7164e+03 - logprior: 0.4125
Epoch 4/10
39/39 - 99s - loss: 1715.1074 - loglik: -1.7156e+03 - logprior: 0.4535
Epoch 5/10
39/39 - 100s - loss: 1716.7828 - loglik: -1.7171e+03 - logprior: 0.2819
Fitted a model with MAP estimate = -1713.9581
Time for alignment: 1867.1436
Computed alignments with likelihoods: ['-1711.8918', '-1712.3361', '-1711.6522', '-1713.1326', '-1713.9581']
Best model has likelihood: -1711.6522  (prior= 0.2606 )
time for generating output: 0.4527
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.859114177609615
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff1149dd640>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe905f0cb80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9207085b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c1e84c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905641cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9056415b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905641190>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe905641460>, <__main__.SimpleDirichletPrior object at 0x7fe8e8dcecd0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 846.2072 - loglik: -8.4279e+02 - logprior: -3.4212e+00
Epoch 2/10
19/19 - 5s - loss: 762.4741 - loglik: -7.6146e+02 - logprior: -1.0180e+00
Epoch 3/10
19/19 - 5s - loss: 734.4952 - loglik: -7.3322e+02 - logprior: -1.2714e+00
Epoch 4/10
19/19 - 5s - loss: 731.1425 - loglik: -7.2985e+02 - logprior: -1.2907e+00
Epoch 5/10
19/19 - 5s - loss: 726.6707 - loglik: -7.2541e+02 - logprior: -1.2656e+00
Epoch 6/10
19/19 - 5s - loss: 728.0773 - loglik: -7.2681e+02 - logprior: -1.2691e+00
Fitted a model with MAP estimate = -725.8724
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (59, 1), (60, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (106, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 727.4138 - loglik: -7.2332e+02 - logprior: -4.0916e+00
Epoch 2/2
19/19 - 6s - loss: 715.5649 - loglik: -7.1436e+02 - logprior: -1.2076e+00
Fitted a model with MAP estimate = -713.2139
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  73 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 720.9722 - loglik: -7.1700e+02 - logprior: -3.9692e+00
Epoch 2/2
19/19 - 6s - loss: 715.8408 - loglik: -7.1465e+02 - logprior: -1.1935e+00
Fitted a model with MAP estimate = -713.7704
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 717.7845 - loglik: -7.1459e+02 - logprior: -3.1980e+00
Epoch 2/10
19/19 - 6s - loss: 714.3997 - loglik: -7.1336e+02 - logprior: -1.0367e+00
Epoch 3/10
19/19 - 6s - loss: 713.1486 - loglik: -7.1227e+02 - logprior: -8.7794e-01
Epoch 4/10
19/19 - 6s - loss: 712.4572 - loglik: -7.1167e+02 - logprior: -7.8305e-01
Epoch 5/10
19/19 - 6s - loss: 711.8301 - loglik: -7.1108e+02 - logprior: -7.4736e-01
Epoch 6/10
19/19 - 6s - loss: 710.7632 - loglik: -7.1006e+02 - logprior: -7.0004e-01
Epoch 7/10
19/19 - 6s - loss: 710.4891 - loglik: -7.0980e+02 - logprior: -6.8415e-01
Epoch 8/10
19/19 - 6s - loss: 710.7320 - loglik: -7.1007e+02 - logprior: -6.6171e-01
Fitted a model with MAP estimate = -709.9972
Time for alignment: 152.1119
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 846.8723 - loglik: -8.4345e+02 - logprior: -3.4240e+00
Epoch 2/10
19/19 - 5s - loss: 763.7523 - loglik: -7.6273e+02 - logprior: -1.0202e+00
Epoch 3/10
19/19 - 5s - loss: 736.3350 - loglik: -7.3510e+02 - logprior: -1.2389e+00
Epoch 4/10
19/19 - 5s - loss: 731.3921 - loglik: -7.3013e+02 - logprior: -1.2577e+00
Epoch 5/10
19/19 - 5s - loss: 729.3712 - loglik: -7.2814e+02 - logprior: -1.2313e+00
Epoch 6/10
19/19 - 5s - loss: 727.5175 - loglik: -7.2628e+02 - logprior: -1.2414e+00
Epoch 7/10
19/19 - 5s - loss: 728.7988 - loglik: -7.2755e+02 - logprior: -1.2470e+00
Fitted a model with MAP estimate = -726.8538
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 1), (58, 2), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (106, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 728.4668 - loglik: -7.2431e+02 - logprior: -4.1614e+00
Epoch 2/2
19/19 - 7s - loss: 716.2026 - loglik: -7.1497e+02 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -713.8137
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  74  79 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 721.5514 - loglik: -7.1754e+02 - logprior: -4.0070e+00
Epoch 2/2
19/19 - 6s - loss: 716.1057 - loglik: -7.1488e+02 - logprior: -1.2254e+00
Fitted a model with MAP estimate = -714.2384
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 718.7941 - loglik: -7.1556e+02 - logprior: -3.2314e+00
Epoch 2/10
19/19 - 6s - loss: 714.3221 - loglik: -7.1327e+02 - logprior: -1.0549e+00
Epoch 3/10
19/19 - 6s - loss: 713.4528 - loglik: -7.1254e+02 - logprior: -9.1118e-01
Epoch 4/10
19/19 - 6s - loss: 713.4213 - loglik: -7.1260e+02 - logprior: -8.2256e-01
Epoch 5/10
19/19 - 6s - loss: 711.3232 - loglik: -7.1057e+02 - logprior: -7.5409e-01
Epoch 6/10
19/19 - 6s - loss: 711.1025 - loglik: -7.1036e+02 - logprior: -7.4153e-01
Epoch 7/10
19/19 - 6s - loss: 711.2198 - loglik: -7.1051e+02 - logprior: -7.0784e-01
Fitted a model with MAP estimate = -710.4704
Time for alignment: 151.5894
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 846.0756 - loglik: -8.4265e+02 - logprior: -3.4297e+00
Epoch 2/10
19/19 - 5s - loss: 762.6993 - loglik: -7.6167e+02 - logprior: -1.0339e+00
Epoch 3/10
19/19 - 5s - loss: 733.7439 - loglik: -7.3251e+02 - logprior: -1.2366e+00
Epoch 4/10
19/19 - 5s - loss: 728.9722 - loglik: -7.2773e+02 - logprior: -1.2407e+00
Epoch 5/10
19/19 - 5s - loss: 726.6340 - loglik: -7.2541e+02 - logprior: -1.2259e+00
Epoch 6/10
19/19 - 5s - loss: 725.9557 - loglik: -7.2472e+02 - logprior: -1.2389e+00
Epoch 7/10
19/19 - 5s - loss: 724.6929 - loglik: -7.2345e+02 - logprior: -1.2446e+00
Epoch 8/10
19/19 - 5s - loss: 724.8658 - loglik: -7.2360e+02 - logprior: -1.2613e+00
Fitted a model with MAP estimate = -723.7912
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 726.9787 - loglik: -7.2276e+02 - logprior: -4.2162e+00
Epoch 2/2
19/19 - 7s - loss: 714.4544 - loglik: -7.1323e+02 - logprior: -1.2272e+00
Fitted a model with MAP estimate = -711.7834
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  80 102 103 149]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 719.5754 - loglik: -7.1559e+02 - logprior: -3.9839e+00
Epoch 2/2
19/19 - 6s - loss: 714.4061 - loglik: -7.1321e+02 - logprior: -1.1972e+00
Fitted a model with MAP estimate = -712.2685
expansions: [(0, 5)]
discards: [ 0  1  2  4 72 74]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 717.0572 - loglik: -7.1389e+02 - logprior: -3.1712e+00
Epoch 2/10
19/19 - 6s - loss: 712.7651 - loglik: -7.1177e+02 - logprior: -9.9631e-01
Epoch 3/10
19/19 - 6s - loss: 711.9393 - loglik: -7.1110e+02 - logprior: -8.3854e-01
Epoch 4/10
19/19 - 6s - loss: 710.8145 - loglik: -7.1007e+02 - logprior: -7.4741e-01
Epoch 5/10
19/19 - 6s - loss: 710.1733 - loglik: -7.0948e+02 - logprior: -6.9762e-01
Epoch 6/10
19/19 - 6s - loss: 708.3745 - loglik: -7.0769e+02 - logprior: -6.8126e-01
Epoch 7/10
19/19 - 6s - loss: 709.6665 - loglik: -7.0900e+02 - logprior: -6.6744e-01
Fitted a model with MAP estimate = -708.6548
Time for alignment: 152.9409
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 845.9490 - loglik: -8.4252e+02 - logprior: -3.4286e+00
Epoch 2/10
19/19 - 5s - loss: 769.2300 - loglik: -7.6821e+02 - logprior: -1.0239e+00
Epoch 3/10
19/19 - 5s - loss: 738.7282 - loglik: -7.3752e+02 - logprior: -1.2072e+00
Epoch 4/10
19/19 - 5s - loss: 733.2698 - loglik: -7.3201e+02 - logprior: -1.2597e+00
Epoch 5/10
19/19 - 5s - loss: 732.3169 - loglik: -7.3107e+02 - logprior: -1.2446e+00
Epoch 6/10
19/19 - 5s - loss: 730.1339 - loglik: -7.2887e+02 - logprior: -1.2616e+00
Epoch 7/10
19/19 - 5s - loss: 730.9260 - loglik: -7.2967e+02 - logprior: -1.2575e+00
Fitted a model with MAP estimate = -729.0360
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (27, 1), (33, 1), (34, 1), (37, 2), (52, 1), (54, 2), (58, 1), (74, 2), (75, 3), (76, 2), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (104, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 730.2286 - loglik: -7.2602e+02 - logprior: -4.2037e+00
Epoch 2/2
19/19 - 7s - loss: 717.0992 - loglik: -7.1582e+02 - logprior: -1.2753e+00
Fitted a model with MAP estimate = -714.0161
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  54  74 100 104]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 721.6904 - loglik: -7.1766e+02 - logprior: -4.0303e+00
Epoch 2/2
19/19 - 6s - loss: 715.9826 - loglik: -7.1473e+02 - logprior: -1.2477e+00
Fitted a model with MAP estimate = -714.1862
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 718.5708 - loglik: -7.1534e+02 - logprior: -3.2354e+00
Epoch 2/10
19/19 - 6s - loss: 714.6740 - loglik: -7.1364e+02 - logprior: -1.0364e+00
Epoch 3/10
19/19 - 6s - loss: 713.4566 - loglik: -7.1260e+02 - logprior: -8.5347e-01
Epoch 4/10
19/19 - 6s - loss: 713.4396 - loglik: -7.1268e+02 - logprior: -7.5562e-01
Epoch 5/10
19/19 - 6s - loss: 711.9045 - loglik: -7.1118e+02 - logprior: -7.2045e-01
Epoch 6/10
19/19 - 6s - loss: 710.8019 - loglik: -7.1010e+02 - logprior: -7.0491e-01
Epoch 7/10
19/19 - 6s - loss: 711.3933 - loglik: -7.1071e+02 - logprior: -6.8682e-01
Fitted a model with MAP estimate = -710.6720
Time for alignment: 149.0264
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 845.9258 - loglik: -8.4250e+02 - logprior: -3.4277e+00
Epoch 2/10
19/19 - 4s - loss: 763.9713 - loglik: -7.6293e+02 - logprior: -1.0374e+00
Epoch 3/10
19/19 - 4s - loss: 736.0226 - loglik: -7.3479e+02 - logprior: -1.2295e+00
Epoch 4/10
19/19 - 4s - loss: 730.0912 - loglik: -7.2887e+02 - logprior: -1.2254e+00
Epoch 5/10
19/19 - 4s - loss: 728.1452 - loglik: -7.2692e+02 - logprior: -1.2215e+00
Epoch 6/10
19/19 - 4s - loss: 727.1087 - loglik: -7.2588e+02 - logprior: -1.2308e+00
Epoch 7/10
19/19 - 4s - loss: 727.1143 - loglik: -7.2586e+02 - logprior: -1.2505e+00
Fitted a model with MAP estimate = -725.6587
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 727.8722 - loglik: -7.2366e+02 - logprior: -4.2082e+00
Epoch 2/2
19/19 - 6s - loss: 714.9236 - loglik: -7.1366e+02 - logprior: -1.2626e+00
Fitted a model with MAP estimate = -712.5984
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  49  75  77  81 103 104 150]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 720.9693 - loglik: -7.1699e+02 - logprior: -3.9809e+00
Epoch 2/2
19/19 - 5s - loss: 715.1762 - loglik: -7.1398e+02 - logprior: -1.1976e+00
Fitted a model with MAP estimate = -713.4628
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 717.9741 - loglik: -7.1479e+02 - logprior: -3.1832e+00
Epoch 2/10
19/19 - 5s - loss: 713.5983 - loglik: -7.1262e+02 - logprior: -9.8024e-01
Epoch 3/10
19/19 - 5s - loss: 712.6545 - loglik: -7.1187e+02 - logprior: -7.8347e-01
Epoch 4/10
19/19 - 5s - loss: 711.8879 - loglik: -7.1119e+02 - logprior: -7.0081e-01
Epoch 5/10
19/19 - 5s - loss: 711.3549 - loglik: -7.1070e+02 - logprior: -6.5128e-01
Epoch 6/10
19/19 - 5s - loss: 710.3090 - loglik: -7.0965e+02 - logprior: -6.5498e-01
Epoch 7/10
19/19 - 5s - loss: 710.4162 - loglik: -7.0976e+02 - logprior: -6.5969e-01
Fitted a model with MAP estimate = -709.6998
Time for alignment: 137.4990
Computed alignments with likelihoods: ['-709.9972', '-710.4704', '-708.6548', '-710.6720', '-709.6998']
Best model has likelihood: -708.6548  (prior= -0.6720 )
time for generating output: 0.1877
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.7039178074992773
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe9109c0100>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe910afe970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff129795670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff139d293d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe950461ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918938d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefd6f0430>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe941ea9c40>, <__main__.SimpleDirichletPrior object at 0x7fe9115dea60>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 141.6277 - loglik: -1.3818e+02 - logprior: -3.4509e+00
Epoch 2/10
19/19 - 1s - loss: 124.2561 - loglik: -1.2289e+02 - logprior: -1.3662e+00
Epoch 3/10
19/19 - 1s - loss: 118.7001 - loglik: -1.1726e+02 - logprior: -1.4376e+00
Epoch 4/10
19/19 - 1s - loss: 117.2414 - loglik: -1.1570e+02 - logprior: -1.5434e+00
Epoch 5/10
19/19 - 1s - loss: 116.8841 - loglik: -1.1535e+02 - logprior: -1.5370e+00
Epoch 6/10
19/19 - 1s - loss: 116.7328 - loglik: -1.1518e+02 - logprior: -1.5510e+00
Epoch 7/10
19/19 - 1s - loss: 116.5792 - loglik: -1.1503e+02 - logprior: -1.5530e+00
Epoch 8/10
19/19 - 1s - loss: 116.4575 - loglik: -1.1491e+02 - logprior: -1.5520e+00
Epoch 9/10
19/19 - 1s - loss: 116.5239 - loglik: -1.1496e+02 - logprior: -1.5592e+00
Fitted a model with MAP estimate = -116.4058
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 119.9054 - loglik: -1.1506e+02 - logprior: -4.8405e+00
Epoch 2/2
19/19 - 1s - loss: 115.1184 - loglik: -1.1366e+02 - logprior: -1.4594e+00
Fitted a model with MAP estimate = -114.3130
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 116.8930 - loglik: -1.1344e+02 - logprior: -3.4485e+00
Epoch 2/2
19/19 - 1s - loss: 114.2556 - loglik: -1.1273e+02 - logprior: -1.5294e+00
Fitted a model with MAP estimate = -113.9723
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 116.5009 - loglik: -1.1314e+02 - logprior: -3.3632e+00
Epoch 2/10
19/19 - 1s - loss: 114.2247 - loglik: -1.1271e+02 - logprior: -1.5108e+00
Epoch 3/10
19/19 - 1s - loss: 113.9010 - loglik: -1.1248e+02 - logprior: -1.4161e+00
Epoch 4/10
19/19 - 1s - loss: 113.5748 - loglik: -1.1219e+02 - logprior: -1.3867e+00
Epoch 5/10
19/19 - 1s - loss: 113.4989 - loglik: -1.1212e+02 - logprior: -1.3818e+00
Epoch 6/10
19/19 - 1s - loss: 113.2945 - loglik: -1.1190e+02 - logprior: -1.3925e+00
Epoch 7/10
19/19 - 1s - loss: 113.1928 - loglik: -1.1180e+02 - logprior: -1.3958e+00
Epoch 8/10
19/19 - 1s - loss: 113.0414 - loglik: -1.1165e+02 - logprior: -1.3958e+00
Epoch 9/10
19/19 - 1s - loss: 113.1435 - loglik: -1.1175e+02 - logprior: -1.3911e+00
Fitted a model with MAP estimate = -113.0253
Time for alignment: 37.2516
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 141.7000 - loglik: -1.3825e+02 - logprior: -3.4541e+00
Epoch 2/10
19/19 - 1s - loss: 124.6841 - loglik: -1.2330e+02 - logprior: -1.3868e+00
Epoch 3/10
19/19 - 1s - loss: 118.6056 - loglik: -1.1711e+02 - logprior: -1.4954e+00
Epoch 4/10
19/19 - 1s - loss: 117.2039 - loglik: -1.1567e+02 - logprior: -1.5376e+00
Epoch 5/10
19/19 - 1s - loss: 116.8702 - loglik: -1.1532e+02 - logprior: -1.5486e+00
Epoch 6/10
19/19 - 1s - loss: 116.6991 - loglik: -1.1515e+02 - logprior: -1.5525e+00
Epoch 7/10
19/19 - 1s - loss: 116.6129 - loglik: -1.1506e+02 - logprior: -1.5487e+00
Epoch 8/10
19/19 - 1s - loss: 116.4500 - loglik: -1.1489e+02 - logprior: -1.5577e+00
Epoch 9/10
19/19 - 1s - loss: 116.4639 - loglik: -1.1491e+02 - logprior: -1.5573e+00
Fitted a model with MAP estimate = -116.4024
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 119.8989 - loglik: -1.1507e+02 - logprior: -4.8253e+00
Epoch 2/2
19/19 - 1s - loss: 115.1049 - loglik: -1.1365e+02 - logprior: -1.4536e+00
Fitted a model with MAP estimate = -114.3054
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 116.8655 - loglik: -1.1341e+02 - logprior: -3.4518e+00
Epoch 2/2
19/19 - 1s - loss: 114.2950 - loglik: -1.1277e+02 - logprior: -1.5295e+00
Fitted a model with MAP estimate = -113.9839
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 116.5203 - loglik: -1.1316e+02 - logprior: -3.3600e+00
Epoch 2/10
19/19 - 1s - loss: 114.2543 - loglik: -1.1274e+02 - logprior: -1.5108e+00
Epoch 3/10
19/19 - 1s - loss: 113.8500 - loglik: -1.1243e+02 - logprior: -1.4165e+00
Epoch 4/10
19/19 - 1s - loss: 113.6759 - loglik: -1.1229e+02 - logprior: -1.3849e+00
Epoch 5/10
19/19 - 1s - loss: 113.3588 - loglik: -1.1197e+02 - logprior: -1.3866e+00
Epoch 6/10
19/19 - 1s - loss: 113.2584 - loglik: -1.1187e+02 - logprior: -1.3922e+00
Epoch 7/10
19/19 - 1s - loss: 113.2700 - loglik: -1.1188e+02 - logprior: -1.3938e+00
Fitted a model with MAP estimate = -113.1013
Time for alignment: 32.6381
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.7693 - loglik: -1.3831e+02 - logprior: -3.4576e+00
Epoch 2/10
19/19 - 1s - loss: 124.5957 - loglik: -1.2321e+02 - logprior: -1.3859e+00
Epoch 3/10
19/19 - 1s - loss: 118.5291 - loglik: -1.1703e+02 - logprior: -1.4964e+00
Epoch 4/10
19/19 - 1s - loss: 117.2365 - loglik: -1.1570e+02 - logprior: -1.5397e+00
Epoch 5/10
19/19 - 1s - loss: 116.9560 - loglik: -1.1542e+02 - logprior: -1.5404e+00
Epoch 6/10
19/19 - 1s - loss: 116.6238 - loglik: -1.1507e+02 - logprior: -1.5549e+00
Epoch 7/10
19/19 - 1s - loss: 116.5751 - loglik: -1.1502e+02 - logprior: -1.5502e+00
Epoch 8/10
19/19 - 1s - loss: 116.5429 - loglik: -1.1498e+02 - logprior: -1.5585e+00
Epoch 9/10
19/19 - 1s - loss: 116.4564 - loglik: -1.1490e+02 - logprior: -1.5593e+00
Epoch 10/10
19/19 - 1s - loss: 116.4805 - loglik: -1.1492e+02 - logprior: -1.5585e+00
Fitted a model with MAP estimate = -116.3779
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 119.9764 - loglik: -1.1508e+02 - logprior: -4.8948e+00
Epoch 2/2
19/19 - 1s - loss: 115.1396 - loglik: -1.1366e+02 - logprior: -1.4806e+00
Fitted a model with MAP estimate = -114.3722
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 116.3143 - loglik: -1.1311e+02 - logprior: -3.2082e+00
Epoch 2/10
19/19 - 1s - loss: 114.2549 - loglik: -1.1283e+02 - logprior: -1.4218e+00
Epoch 3/10
19/19 - 1s - loss: 113.8740 - loglik: -1.1250e+02 - logprior: -1.3779e+00
Epoch 4/10
19/19 - 1s - loss: 113.7798 - loglik: -1.1243e+02 - logprior: -1.3543e+00
Epoch 5/10
19/19 - 1s - loss: 113.5259 - loglik: -1.1218e+02 - logprior: -1.3484e+00
Epoch 6/10
19/19 - 1s - loss: 113.4005 - loglik: -1.1204e+02 - logprior: -1.3589e+00
Epoch 7/10
19/19 - 1s - loss: 113.2427 - loglik: -1.1188e+02 - logprior: -1.3622e+00
Epoch 8/10
19/19 - 1s - loss: 113.2692 - loglik: -1.1191e+02 - logprior: -1.3620e+00
Fitted a model with MAP estimate = -113.1646
Time for alignment: 27.4837
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 141.6945 - loglik: -1.3823e+02 - logprior: -3.4601e+00
Epoch 2/10
19/19 - 1s - loss: 124.5646 - loglik: -1.2319e+02 - logprior: -1.3790e+00
Epoch 3/10
19/19 - 1s - loss: 118.6450 - loglik: -1.1717e+02 - logprior: -1.4748e+00
Epoch 4/10
19/19 - 1s - loss: 117.2060 - loglik: -1.1567e+02 - logprior: -1.5388e+00
Epoch 5/10
19/19 - 1s - loss: 116.9232 - loglik: -1.1538e+02 - logprior: -1.5417e+00
Epoch 6/10
19/19 - 1s - loss: 116.7017 - loglik: -1.1515e+02 - logprior: -1.5508e+00
Epoch 7/10
19/19 - 1s - loss: 116.5108 - loglik: -1.1496e+02 - logprior: -1.5544e+00
Epoch 8/10
19/19 - 1s - loss: 116.5913 - loglik: -1.1503e+02 - logprior: -1.5571e+00
Fitted a model with MAP estimate = -116.4430
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 119.8550 - loglik: -1.1507e+02 - logprior: -4.7883e+00
Epoch 2/2
19/19 - 1s - loss: 115.0693 - loglik: -1.1361e+02 - logprior: -1.4545e+00
Fitted a model with MAP estimate = -114.2971
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 116.8809 - loglik: -1.1343e+02 - logprior: -3.4512e+00
Epoch 2/2
19/19 - 1s - loss: 114.2980 - loglik: -1.1276e+02 - logprior: -1.5349e+00
Fitted a model with MAP estimate = -113.9678
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.5053 - loglik: -1.1314e+02 - logprior: -3.3675e+00
Epoch 2/10
19/19 - 1s - loss: 114.2047 - loglik: -1.1269e+02 - logprior: -1.5110e+00
Epoch 3/10
19/19 - 1s - loss: 113.8750 - loglik: -1.1245e+02 - logprior: -1.4215e+00
Epoch 4/10
19/19 - 1s - loss: 113.5918 - loglik: -1.1220e+02 - logprior: -1.3889e+00
Epoch 5/10
19/19 - 1s - loss: 113.4234 - loglik: -1.1204e+02 - logprior: -1.3848e+00
Epoch 6/10
19/19 - 1s - loss: 113.3178 - loglik: -1.1192e+02 - logprior: -1.3967e+00
Epoch 7/10
19/19 - 1s - loss: 113.1547 - loglik: -1.1175e+02 - logprior: -1.4010e+00
Epoch 8/10
19/19 - 1s - loss: 113.0898 - loglik: -1.1169e+02 - logprior: -1.3961e+00
Epoch 9/10
19/19 - 1s - loss: 113.0351 - loglik: -1.1164e+02 - logprior: -1.3947e+00
Epoch 10/10
19/19 - 1s - loss: 113.0295 - loglik: -1.1163e+02 - logprior: -1.3946e+00
Fitted a model with MAP estimate = -112.9474
Time for alignment: 33.8141
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.7304 - loglik: -1.3828e+02 - logprior: -3.4474e+00
Epoch 2/10
19/19 - 1s - loss: 124.3794 - loglik: -1.2302e+02 - logprior: -1.3637e+00
Epoch 3/10
19/19 - 1s - loss: 118.7672 - loglik: -1.1733e+02 - logprior: -1.4375e+00
Epoch 4/10
19/19 - 1s - loss: 117.2545 - loglik: -1.1571e+02 - logprior: -1.5419e+00
Epoch 5/10
19/19 - 1s - loss: 116.8324 - loglik: -1.1529e+02 - logprior: -1.5422e+00
Epoch 6/10
19/19 - 1s - loss: 116.7244 - loglik: -1.1517e+02 - logprior: -1.5528e+00
Epoch 7/10
19/19 - 1s - loss: 116.6298 - loglik: -1.1507e+02 - logprior: -1.5556e+00
Epoch 8/10
19/19 - 1s - loss: 116.4719 - loglik: -1.1492e+02 - logprior: -1.5555e+00
Epoch 9/10
19/19 - 1s - loss: 116.4261 - loglik: -1.1487e+02 - logprior: -1.5547e+00
Epoch 10/10
19/19 - 1s - loss: 116.5160 - loglik: -1.1495e+02 - logprior: -1.5633e+00
Fitted a model with MAP estimate = -116.3786
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 119.9825 - loglik: -1.1508e+02 - logprior: -4.8987e+00
Epoch 2/2
19/19 - 1s - loss: 115.1329 - loglik: -1.1365e+02 - logprior: -1.4783e+00
Fitted a model with MAP estimate = -114.3661
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.3473 - loglik: -1.1314e+02 - logprior: -3.2114e+00
Epoch 2/10
19/19 - 1s - loss: 114.1784 - loglik: -1.1276e+02 - logprior: -1.4206e+00
Epoch 3/10
19/19 - 1s - loss: 113.9740 - loglik: -1.1260e+02 - logprior: -1.3787e+00
Epoch 4/10
19/19 - 1s - loss: 113.7374 - loglik: -1.1238e+02 - logprior: -1.3540e+00
Epoch 5/10
19/19 - 1s - loss: 113.5474 - loglik: -1.1220e+02 - logprior: -1.3477e+00
Epoch 6/10
19/19 - 1s - loss: 113.3710 - loglik: -1.1201e+02 - logprior: -1.3615e+00
Epoch 7/10
19/19 - 1s - loss: 113.3287 - loglik: -1.1197e+02 - logprior: -1.3616e+00
Epoch 8/10
19/19 - 1s - loss: 113.1398 - loglik: -1.1178e+02 - logprior: -1.3604e+00
Epoch 9/10
19/19 - 1s - loss: 113.2071 - loglik: -1.1185e+02 - logprior: -1.3592e+00
Fitted a model with MAP estimate = -113.1375
Time for alignment: 28.4555
Computed alignments with likelihoods: ['-113.0253', '-113.1013', '-113.1646', '-112.9474', '-113.1375']
Best model has likelihood: -112.9474  (prior= -1.4013 )
time for generating output: 0.0902
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8980738362760835
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941704d60>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8f096d730>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9112695e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918dda490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9310b6730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefc42c7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe930fa75b0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe930fa7910>, <__main__.SimpleDirichletPrior object at 0x7fe8f8f1ad00>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 1644.2582 - loglik: -1.6421e+03 - logprior: -2.1534e+00
Epoch 2/10
39/39 - 28s - loss: 1443.3778 - loglik: -1.4418e+03 - logprior: -1.5861e+00
Epoch 3/10
39/39 - 29s - loss: 1429.9707 - loglik: -1.4283e+03 - logprior: -1.6904e+00
Epoch 4/10
39/39 - 29s - loss: 1427.7629 - loglik: -1.4261e+03 - logprior: -1.6335e+00
Epoch 5/10
39/39 - 30s - loss: 1426.6029 - loglik: -1.4249e+03 - logprior: -1.6847e+00
Epoch 6/10
39/39 - 31s - loss: 1426.2229 - loglik: -1.4245e+03 - logprior: -1.7341e+00
Epoch 7/10
39/39 - 30s - loss: 1425.5094 - loglik: -1.4237e+03 - logprior: -1.8119e+00
Epoch 8/10
39/39 - 31s - loss: 1425.1595 - loglik: -1.4233e+03 - logprior: -1.8491e+00
Epoch 9/10
39/39 - 32s - loss: 1425.3146 - loglik: -1.4234e+03 - logprior: -1.9240e+00
Fitted a model with MAP estimate = -1423.1773
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 1), (112, 2), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (170, 1), (172, 2), (173, 1), (180, 1), (181, 1), (182, 1), (188, 1), (189, 1), (209, 1), (213, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 1395.3466 - loglik: -1.3929e+03 - logprior: -2.4069e+00
Epoch 2/2
39/39 - 52s - loss: 1377.3788 - loglik: -1.3765e+03 - logprior: -9.1126e-01
Fitted a model with MAP estimate = -1373.5492
expansions: []
discards: [  0   1  57  98 141 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 1382.7883 - loglik: -1.3810e+03 - logprior: -1.7416e+00
Epoch 2/2
39/39 - 47s - loss: 1378.3689 - loglik: -1.3783e+03 - logprior: -6.5348e-02
Fitted a model with MAP estimate = -1374.8074
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 1379.6621 - loglik: -1.3780e+03 - logprior: -1.6489e+00
Epoch 2/10
39/39 - 51s - loss: 1375.0557 - loglik: -1.3749e+03 - logprior: -1.3260e-01
Epoch 3/10
39/39 - 51s - loss: 1373.3162 - loglik: -1.3733e+03 - logprior: -4.4978e-03
Epoch 4/10
39/39 - 49s - loss: 1371.5385 - loglik: -1.3717e+03 - logprior: 0.1185
Epoch 5/10
39/39 - 48s - loss: 1371.2163 - loglik: -1.3713e+03 - logprior: 0.0579
Epoch 6/10
39/39 - 49s - loss: 1369.6482 - loglik: -1.3698e+03 - logprior: 0.1844
Epoch 7/10
39/39 - 47s - loss: 1370.1934 - loglik: -1.3704e+03 - logprior: 0.1605
Fitted a model with MAP estimate = -1369.6398
Time for alignment: 1079.5746
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1646.0677 - loglik: -1.6439e+03 - logprior: -2.1518e+00
Epoch 2/10
39/39 - 35s - loss: 1440.2554 - loglik: -1.4386e+03 - logprior: -1.6626e+00
Epoch 3/10
39/39 - 35s - loss: 1427.2449 - loglik: -1.4255e+03 - logprior: -1.7094e+00
Epoch 4/10
39/39 - 35s - loss: 1425.4073 - loglik: -1.4238e+03 - logprior: -1.6126e+00
Epoch 5/10
39/39 - 35s - loss: 1423.7135 - loglik: -1.4221e+03 - logprior: -1.6463e+00
Epoch 6/10
39/39 - 34s - loss: 1423.8950 - loglik: -1.4222e+03 - logprior: -1.7170e+00
Fitted a model with MAP estimate = -1421.4993
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (206, 1), (209, 1), (214, 2), (216, 1), (217, 1), (230, 3), (231, 1), (244, 1), (257, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 1395.3619 - loglik: -1.3930e+03 - logprior: -2.4025e+00
Epoch 2/2
39/39 - 49s - loss: 1377.5734 - loglik: -1.3769e+03 - logprior: -7.0176e-01
Fitted a model with MAP estimate = -1373.9894
expansions: []
discards: [  0   1  57  98 179 214]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 1383.1240 - loglik: -1.3813e+03 - logprior: -1.8236e+00
Epoch 2/2
39/39 - 51s - loss: 1377.9662 - loglik: -1.3778e+03 - logprior: -1.9629e-01
Fitted a model with MAP estimate = -1374.7220
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 1379.7153 - loglik: -1.3781e+03 - logprior: -1.6554e+00
Epoch 2/10
39/39 - 49s - loss: 1374.8596 - loglik: -1.3748e+03 - logprior: -1.0302e-01
Epoch 3/10
39/39 - 49s - loss: 1373.8134 - loglik: -1.3736e+03 - logprior: -1.7047e-01
Epoch 4/10
39/39 - 48s - loss: 1372.2878 - loglik: -1.3724e+03 - logprior: 0.1394
Epoch 5/10
39/39 - 51s - loss: 1370.4358 - loglik: -1.3704e+03 - logprior: -3.0056e-02
Epoch 6/10
39/39 - 49s - loss: 1369.8596 - loglik: -1.3701e+03 - logprior: 0.2228
Epoch 7/10
39/39 - 43s - loss: 1372.0486 - loglik: -1.3720e+03 - logprior: -5.6442e-02
Fitted a model with MAP estimate = -1369.6812
Time for alignment: 1015.5499
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 1645.8472 - loglik: -1.6437e+03 - logprior: -2.1451e+00
Epoch 2/10
39/39 - 28s - loss: 1441.4836 - loglik: -1.4399e+03 - logprior: -1.5797e+00
Epoch 3/10
39/39 - 28s - loss: 1429.0911 - loglik: -1.4275e+03 - logprior: -1.6174e+00
Epoch 4/10
39/39 - 28s - loss: 1426.6250 - loglik: -1.4251e+03 - logprior: -1.5441e+00
Epoch 5/10
39/39 - 28s - loss: 1425.5018 - loglik: -1.4239e+03 - logprior: -1.5951e+00
Epoch 6/10
39/39 - 28s - loss: 1424.7734 - loglik: -1.4231e+03 - logprior: -1.6526e+00
Epoch 7/10
39/39 - 28s - loss: 1424.3340 - loglik: -1.4226e+03 - logprior: -1.6994e+00
Epoch 8/10
39/39 - 28s - loss: 1423.9226 - loglik: -1.4222e+03 - logprior: -1.7413e+00
Epoch 9/10
39/39 - 28s - loss: 1424.1998 - loglik: -1.4224e+03 - logprior: -1.7795e+00
Fitted a model with MAP estimate = -1422.3924
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (40, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (109, 1), (111, 1), (112, 1), (114, 1), (121, 1), (133, 1), (134, 1), (142, 1), (145, 2), (148, 2), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (215, 2), (217, 2), (228, 1), (230, 3), (231, 1), (244, 1), (257, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1394.1709 - loglik: -1.3917e+03 - logprior: -2.4421e+00
Epoch 2/2
39/39 - 42s - loss: 1377.6029 - loglik: -1.3768e+03 - logprior: -8.0052e-01
Fitted a model with MAP estimate = -1374.1447
expansions: [(0, 2)]
discards: [  0   1  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 1381.1224 - loglik: -1.3795e+03 - logprior: -1.6311e+00
Epoch 2/2
39/39 - 42s - loss: 1377.4871 - loglik: -1.3769e+03 - logprior: -5.5287e-01
Fitted a model with MAP estimate = -1374.0577
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 1383.3085 - loglik: -1.3812e+03 - logprior: -2.1255e+00
Epoch 2/10
39/39 - 40s - loss: 1377.0104 - loglik: -1.3769e+03 - logprior: -1.3992e-01
Epoch 3/10
39/39 - 40s - loss: 1374.5752 - loglik: -1.3748e+03 - logprior: 0.2404
Epoch 4/10
39/39 - 39s - loss: 1373.1128 - loglik: -1.3734e+03 - logprior: 0.3242
Epoch 5/10
39/39 - 39s - loss: 1371.8881 - loglik: -1.3722e+03 - logprior: 0.3584
Epoch 6/10
39/39 - 39s - loss: 1372.1459 - loglik: -1.3725e+03 - logprior: 0.3430
Fitted a model with MAP estimate = -1371.2393
Time for alignment: 868.1453
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 1645.9127 - loglik: -1.6438e+03 - logprior: -2.1401e+00
Epoch 2/10
39/39 - 26s - loss: 1449.8622 - loglik: -1.4483e+03 - logprior: -1.5373e+00
Epoch 3/10
39/39 - 26s - loss: 1437.7166 - loglik: -1.4362e+03 - logprior: -1.5567e+00
Epoch 4/10
39/39 - 26s - loss: 1435.0046 - loglik: -1.4335e+03 - logprior: -1.4885e+00
Epoch 5/10
39/39 - 26s - loss: 1432.8397 - loglik: -1.4313e+03 - logprior: -1.5414e+00
Epoch 6/10
39/39 - 26s - loss: 1432.3708 - loglik: -1.4308e+03 - logprior: -1.5891e+00
Epoch 7/10
39/39 - 26s - loss: 1432.5184 - loglik: -1.4309e+03 - logprior: -1.6374e+00
Fitted a model with MAP estimate = -1430.3035
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (43, 1), (44, 1), (45, 2), (47, 1), (64, 1), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 3), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (129, 1), (132, 1), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (182, 3), (188, 1), (189, 1), (209, 1), (212, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: [51]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 1401.8890 - loglik: -1.3994e+03 - logprior: -2.5056e+00
Epoch 2/2
39/39 - 39s - loss: 1383.4915 - loglik: -1.3825e+03 - logprior: -9.5936e-01
Fitted a model with MAP estimate = -1379.7304
expansions: [(0, 2)]
discards: [  0   1  57  97 107 179 214 229]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1386.9216 - loglik: -1.3852e+03 - logprior: -1.7652e+00
Epoch 2/2
39/39 - 38s - loss: 1382.9945 - loglik: -1.3826e+03 - logprior: -4.0201e-01
Fitted a model with MAP estimate = -1379.1486
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1386.9891 - loglik: -1.3848e+03 - logprior: -2.1981e+00
Epoch 2/10
39/39 - 38s - loss: 1382.1425 - loglik: -1.3818e+03 - logprior: -3.2108e-01
Epoch 3/10
39/39 - 39s - loss: 1379.3295 - loglik: -1.3795e+03 - logprior: 0.2017
Epoch 4/10
39/39 - 39s - loss: 1378.4916 - loglik: -1.3785e+03 - logprior: 0.0362
Epoch 5/10
39/39 - 39s - loss: 1376.3021 - loglik: -1.3765e+03 - logprior: 0.2318
Epoch 6/10
39/39 - 39s - loss: 1376.0811 - loglik: -1.3763e+03 - logprior: 0.2507
Epoch 7/10
39/39 - 39s - loss: 1376.5564 - loglik: -1.3767e+03 - logprior: 0.1704
Fitted a model with MAP estimate = -1375.3755
Time for alignment: 794.9128
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 1645.0543 - loglik: -1.6429e+03 - logprior: -2.1644e+00
Epoch 2/10
39/39 - 27s - loss: 1440.3317 - loglik: -1.4386e+03 - logprior: -1.7188e+00
Epoch 3/10
39/39 - 27s - loss: 1428.0659 - loglik: -1.4262e+03 - logprior: -1.8203e+00
Epoch 4/10
39/39 - 27s - loss: 1425.6914 - loglik: -1.4240e+03 - logprior: -1.7355e+00
Epoch 5/10
39/39 - 28s - loss: 1424.3550 - loglik: -1.4226e+03 - logprior: -1.7883e+00
Epoch 6/10
39/39 - 28s - loss: 1423.2793 - loglik: -1.4214e+03 - logprior: -1.8638e+00
Epoch 7/10
39/39 - 28s - loss: 1423.3910 - loglik: -1.4215e+03 - logprior: -1.9227e+00
Fitted a model with MAP estimate = -1421.2869
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (37, 1), (38, 1), (39, 1), (44, 1), (45, 1), (47, 1), (63, 1), (64, 1), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (85, 1), (90, 1), (96, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (141, 1), (144, 2), (147, 1), (148, 1), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (187, 1), (204, 1), (208, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 1394.4021 - loglik: -1.3919e+03 - logprior: -2.4536e+00
Epoch 2/2
39/39 - 42s - loss: 1377.7760 - loglik: -1.3770e+03 - logprior: -7.3773e-01
Fitted a model with MAP estimate = -1374.3726
expansions: [(0, 2)]
discards: [  0   1  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 1381.6219 - loglik: -1.3800e+03 - logprior: -1.6465e+00
Epoch 2/2
39/39 - 43s - loss: 1377.2314 - loglik: -1.3767e+03 - logprior: -5.4729e-01
Fitted a model with MAP estimate = -1373.8538
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1381.4011 - loglik: -1.3791e+03 - logprior: -2.2771e+00
Epoch 2/10
39/39 - 46s - loss: 1376.1052 - loglik: -1.3759e+03 - logprior: -1.8362e-01
Epoch 3/10
39/39 - 49s - loss: 1374.2184 - loglik: -1.3742e+03 - logprior: 0.0074
Epoch 4/10
39/39 - 46s - loss: 1372.2815 - loglik: -1.3725e+03 - logprior: 0.2261
Epoch 5/10
39/39 - 44s - loss: 1370.6572 - loglik: -1.3708e+03 - logprior: 0.1279
Epoch 6/10
39/39 - 49s - loss: 1371.1631 - loglik: -1.3713e+03 - logprior: 0.1478
Fitted a model with MAP estimate = -1370.0799
Time for alignment: 864.1301
Computed alignments with likelihoods: ['-1369.6398', '-1369.6812', '-1371.2393', '-1375.3755', '-1370.0799']
Best model has likelihood: -1369.6398  (prior= 0.1122 )
time for generating output: 0.2908
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9217877094972067
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8e821b130>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8d2a07130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1148d6790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff300a533a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91059d1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905cb5d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe911605eb0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8f823f670>, <__main__.SimpleDirichletPrior object at 0x7fe931a65f70>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 816.8924 - loglik: -8.1338e+02 - logprior: -3.5081e+00
Epoch 2/10
19/19 - 6s - loss: 746.8394 - loglik: -7.4553e+02 - logprior: -1.3067e+00
Epoch 3/10
19/19 - 6s - loss: 725.3604 - loglik: -7.2365e+02 - logprior: -1.7087e+00
Epoch 4/10
19/19 - 6s - loss: 718.1802 - loglik: -7.1663e+02 - logprior: -1.5509e+00
Epoch 5/10
19/19 - 6s - loss: 718.0732 - loglik: -7.1653e+02 - logprior: -1.5413e+00
Epoch 6/10
19/19 - 5s - loss: 716.8139 - loglik: -7.1527e+02 - logprior: -1.5414e+00
Epoch 7/10
19/19 - 5s - loss: 713.9067 - loglik: -7.1231e+02 - logprior: -1.5972e+00
Epoch 8/10
19/19 - 6s - loss: 715.1461 - loglik: -7.1351e+02 - logprior: -1.6352e+00
Fitted a model with MAP estimate = -711.2346
expansions: [(7, 2), (22, 2), (23, 2), (24, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 1), (55, 1), (56, 1), (71, 2), (83, 2), (84, 4), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 721.3444 - loglik: -7.1807e+02 - logprior: -3.2715e+00
Epoch 2/2
39/39 - 9s - loss: 711.5571 - loglik: -7.1007e+02 - logprior: -1.4917e+00
Fitted a model with MAP estimate = -706.5031
expansions: []
discards: [ 23  26  31  38  49  61 107 140]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 713.1519 - loglik: -7.1085e+02 - logprior: -2.3008e+00
Epoch 2/2
39/39 - 9s - loss: 709.9009 - loglik: -7.0870e+02 - logprior: -1.1970e+00
Fitted a model with MAP estimate = -706.2389
expansions: [(107, 1)]
discards: [ 87 102 103 104 105]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 710.8146 - loglik: -7.0863e+02 - logprior: -2.1876e+00
Epoch 2/10
39/39 - 9s - loss: 707.9599 - loglik: -7.0685e+02 - logprior: -1.1138e+00
Epoch 3/10
39/39 - 9s - loss: 706.5713 - loglik: -7.0556e+02 - logprior: -1.0159e+00
Epoch 4/10
39/39 - 9s - loss: 703.8323 - loglik: -7.0287e+02 - logprior: -9.6630e-01
Epoch 5/10
39/39 - 9s - loss: 702.9218 - loglik: -7.0199e+02 - logprior: -9.2771e-01
Epoch 6/10
39/39 - 9s - loss: 701.1098 - loglik: -7.0019e+02 - logprior: -9.1720e-01
Epoch 7/10
39/39 - 9s - loss: 699.4586 - loglik: -6.9849e+02 - logprior: -9.7335e-01
Epoch 8/10
39/39 - 9s - loss: 699.3086 - loglik: -6.9832e+02 - logprior: -9.9321e-01
Epoch 9/10
39/39 - 9s - loss: 698.6483 - loglik: -6.9763e+02 - logprior: -1.0218e+00
Epoch 10/10
39/39 - 9s - loss: 698.6852 - loglik: -6.9765e+02 - logprior: -1.0401e+00
Fitted a model with MAP estimate = -698.4393
Time for alignment: 233.9956
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 816.9456 - loglik: -8.1344e+02 - logprior: -3.5026e+00
Epoch 2/10
19/19 - 6s - loss: 748.5569 - loglik: -7.4726e+02 - logprior: -1.3002e+00
Epoch 3/10
19/19 - 6s - loss: 724.7158 - loglik: -7.2302e+02 - logprior: -1.6954e+00
Epoch 4/10
19/19 - 6s - loss: 720.5040 - loglik: -7.1895e+02 - logprior: -1.5493e+00
Epoch 5/10
19/19 - 6s - loss: 718.5412 - loglik: -7.1701e+02 - logprior: -1.5262e+00
Epoch 6/10
19/19 - 6s - loss: 716.8718 - loglik: -7.1533e+02 - logprior: -1.5437e+00
Epoch 7/10
19/19 - 6s - loss: 715.1461 - loglik: -7.1355e+02 - logprior: -1.5978e+00
Epoch 8/10
19/19 - 6s - loss: 715.3976 - loglik: -7.1377e+02 - logprior: -1.6288e+00
Fitted a model with MAP estimate = -711.7789
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (30, 2), (35, 1), (36, 1), (37, 2), (46, 3), (49, 1), (50, 2), (55, 1), (60, 1), (70, 1), (83, 2), (84, 4), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 721.3613 - loglik: -7.1807e+02 - logprior: -3.2912e+00
Epoch 2/2
39/39 - 10s - loss: 711.1376 - loglik: -7.0962e+02 - logprior: -1.5156e+00
Fitted a model with MAP estimate = -706.4854
expansions: [(113, 1)]
discards: [ 23  30  37  48  59  61  67 106 108 109 110 139]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 713.7573 - loglik: -7.1144e+02 - logprior: -2.3137e+00
Epoch 2/2
39/39 - 9s - loss: 710.2936 - loglik: -7.0906e+02 - logprior: -1.2343e+00
Fitted a model with MAP estimate = -706.7641
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 710.5461 - loglik: -7.0831e+02 - logprior: -2.2339e+00
Epoch 2/10
39/39 - 9s - loss: 708.1388 - loglik: -7.0699e+02 - logprior: -1.1477e+00
Epoch 3/10
39/39 - 9s - loss: 706.5344 - loglik: -7.0548e+02 - logprior: -1.0536e+00
Epoch 4/10
39/39 - 9s - loss: 704.0272 - loglik: -7.0304e+02 - logprior: -9.8957e-01
Epoch 5/10
39/39 - 9s - loss: 702.5331 - loglik: -7.0161e+02 - logprior: -9.2556e-01
Epoch 6/10
39/39 - 9s - loss: 700.3895 - loglik: -6.9946e+02 - logprior: -9.2465e-01
Epoch 7/10
39/39 - 9s - loss: 700.5408 - loglik: -6.9958e+02 - logprior: -9.6089e-01
Fitted a model with MAP estimate = -699.4175
Time for alignment: 208.5877
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 816.4112 - loglik: -8.1291e+02 - logprior: -3.5002e+00
Epoch 2/10
19/19 - 5s - loss: 746.0692 - loglik: -7.4477e+02 - logprior: -1.2977e+00
Epoch 3/10
19/19 - 6s - loss: 722.9818 - loglik: -7.2130e+02 - logprior: -1.6825e+00
Epoch 4/10
19/19 - 6s - loss: 721.4279 - loglik: -7.1990e+02 - logprior: -1.5321e+00
Epoch 5/10
19/19 - 6s - loss: 718.6304 - loglik: -7.1712e+02 - logprior: -1.5097e+00
Epoch 6/10
19/19 - 6s - loss: 716.0559 - loglik: -7.1454e+02 - logprior: -1.5205e+00
Epoch 7/10
19/19 - 6s - loss: 714.6111 - loglik: -7.1305e+02 - logprior: -1.5643e+00
Epoch 8/10
19/19 - 6s - loss: 714.4491 - loglik: -7.1285e+02 - logprior: -1.5955e+00
Epoch 9/10
19/19 - 6s - loss: 714.4269 - loglik: -7.1279e+02 - logprior: -1.6376e+00
Epoch 10/10
19/19 - 6s - loss: 713.7039 - loglik: -7.1200e+02 - logprior: -1.7042e+00
Fitted a model with MAP estimate = -710.7539
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (51, 2), (56, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 721.6407 - loglik: -7.1830e+02 - logprior: -3.3445e+00
Epoch 2/2
39/39 - 10s - loss: 711.2059 - loglik: -7.0961e+02 - logprior: -1.5943e+00
Fitted a model with MAP estimate = -706.5621
expansions: []
discards: [ 23  30  36  48  58  60  67 106 138]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 713.4268 - loglik: -7.1108e+02 - logprior: -2.3452e+00
Epoch 2/2
39/39 - 9s - loss: 710.1614 - loglik: -7.0892e+02 - logprior: -1.2395e+00
Fitted a model with MAP estimate = -706.5314
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 710.4026 - loglik: -7.0814e+02 - logprior: -2.2612e+00
Epoch 2/10
39/39 - 9s - loss: 707.6348 - loglik: -7.0647e+02 - logprior: -1.1605e+00
Epoch 3/10
39/39 - 9s - loss: 706.0399 - loglik: -7.0498e+02 - logprior: -1.0619e+00
Epoch 4/10
39/39 - 9s - loss: 704.5164 - loglik: -7.0353e+02 - logprior: -9.9101e-01
Epoch 5/10
39/39 - 9s - loss: 701.7386 - loglik: -7.0083e+02 - logprior: -9.1304e-01
Epoch 6/10
39/39 - 9s - loss: 701.1985 - loglik: -7.0028e+02 - logprior: -9.2248e-01
Epoch 7/10
39/39 - 9s - loss: 699.7014 - loglik: -6.9875e+02 - logprior: -9.5302e-01
Epoch 8/10
39/39 - 9s - loss: 699.0380 - loglik: -6.9805e+02 - logprior: -9.8839e-01
Epoch 9/10
39/39 - 10s - loss: 698.6232 - loglik: -6.9762e+02 - logprior: -1.0011e+00
Epoch 10/10
39/39 - 9s - loss: 698.0782 - loglik: -6.9707e+02 - logprior: -1.0128e+00
Fitted a model with MAP estimate = -698.3535
Time for alignment: 244.1881
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 816.7981 - loglik: -8.1330e+02 - logprior: -3.5016e+00
Epoch 2/10
19/19 - 5s - loss: 745.6093 - loglik: -7.4431e+02 - logprior: -1.2976e+00
Epoch 3/10
19/19 - 6s - loss: 725.3246 - loglik: -7.2360e+02 - logprior: -1.7197e+00
Epoch 4/10
19/19 - 5s - loss: 719.7404 - loglik: -7.1819e+02 - logprior: -1.5468e+00
Epoch 5/10
19/19 - 5s - loss: 718.7800 - loglik: -7.1726e+02 - logprior: -1.5239e+00
Epoch 6/10
19/19 - 6s - loss: 717.7145 - loglik: -7.1618e+02 - logprior: -1.5319e+00
Epoch 7/10
19/19 - 6s - loss: 714.8964 - loglik: -7.1333e+02 - logprior: -1.5700e+00
Epoch 8/10
19/19 - 5s - loss: 715.5752 - loglik: -7.1397e+02 - logprior: -1.6034e+00
Fitted a model with MAP estimate = -712.0819
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 721.0597 - loglik: -7.1778e+02 - logprior: -3.2795e+00
Epoch 2/2
39/39 - 9s - loss: 711.5802 - loglik: -7.1006e+02 - logprior: -1.5159e+00
Fitted a model with MAP estimate = -706.6158
expansions: []
discards: [ 23  30  37  48  59  61 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 713.1316 - loglik: -7.1079e+02 - logprior: -2.3439e+00
Epoch 2/2
39/39 - 9s - loss: 710.3557 - loglik: -7.0912e+02 - logprior: -1.2400e+00
Fitted a model with MAP estimate = -706.5642
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 710.5019 - loglik: -7.0825e+02 - logprior: -2.2561e+00
Epoch 2/10
39/39 - 9s - loss: 707.9012 - loglik: -7.0674e+02 - logprior: -1.1614e+00
Epoch 3/10
39/39 - 9s - loss: 705.5400 - loglik: -7.0449e+02 - logprior: -1.0538e+00
Epoch 4/10
39/39 - 9s - loss: 704.7217 - loglik: -7.0375e+02 - logprior: -9.7400e-01
Epoch 5/10
39/39 - 9s - loss: 701.9167 - loglik: -7.0099e+02 - logprior: -9.3050e-01
Epoch 6/10
39/39 - 9s - loss: 700.6455 - loglik: -6.9972e+02 - logprior: -9.2556e-01
Epoch 7/10
39/39 - 9s - loss: 700.1771 - loglik: -6.9922e+02 - logprior: -9.5569e-01
Epoch 8/10
39/39 - 9s - loss: 698.4848 - loglik: -6.9750e+02 - logprior: -9.8606e-01
Epoch 9/10
39/39 - 9s - loss: 699.2678 - loglik: -6.9827e+02 - logprior: -9.9906e-01
Fitted a model with MAP estimate = -698.4398
Time for alignment: 224.0258
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 816.9419 - loglik: -8.1345e+02 - logprior: -3.4962e+00
Epoch 2/10
19/19 - 6s - loss: 746.4162 - loglik: -7.4512e+02 - logprior: -1.2938e+00
Epoch 3/10
19/19 - 6s - loss: 723.1392 - loglik: -7.2145e+02 - logprior: -1.6907e+00
Epoch 4/10
19/19 - 6s - loss: 719.6349 - loglik: -7.1811e+02 - logprior: -1.5276e+00
Epoch 5/10
19/19 - 6s - loss: 718.9214 - loglik: -7.1740e+02 - logprior: -1.5164e+00
Epoch 6/10
19/19 - 6s - loss: 715.0541 - loglik: -7.1355e+02 - logprior: -1.5075e+00
Epoch 7/10
19/19 - 6s - loss: 715.7955 - loglik: -7.1424e+02 - logprior: -1.5600e+00
Fitted a model with MAP estimate = -711.9938
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (37, 2), (46, 3), (51, 1), (56, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 720.6647 - loglik: -7.1741e+02 - logprior: -3.2564e+00
Epoch 2/2
39/39 - 10s - loss: 711.2092 - loglik: -7.0973e+02 - logprior: -1.4826e+00
Fitted a model with MAP estimate = -706.5689
expansions: [(113, 1)]
discards: [ 23  30  37  48 136]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 712.8068 - loglik: -7.1047e+02 - logprior: -2.3330e+00
Epoch 2/2
39/39 - 10s - loss: 710.3220 - loglik: -7.0907e+02 - logprior: -1.2542e+00
Fitted a model with MAP estimate = -706.2255
expansions: []
discards: [55]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 710.1680 - loglik: -7.0790e+02 - logprior: -2.2721e+00
Epoch 2/10
39/39 - 10s - loss: 707.7376 - loglik: -7.0656e+02 - logprior: -1.1802e+00
Epoch 3/10
39/39 - 9s - loss: 706.1031 - loglik: -7.0503e+02 - logprior: -1.0722e+00
Epoch 4/10
39/39 - 10s - loss: 704.3472 - loglik: -7.0335e+02 - logprior: -9.9582e-01
Epoch 5/10
39/39 - 10s - loss: 701.5212 - loglik: -7.0058e+02 - logprior: -9.4455e-01
Epoch 6/10
39/39 - 10s - loss: 700.3579 - loglik: -6.9941e+02 - logprior: -9.5159e-01
Epoch 7/10
39/39 - 9s - loss: 699.8388 - loglik: -6.9888e+02 - logprior: -9.5854e-01
Epoch 8/10
39/39 - 9s - loss: 699.1788 - loglik: -6.9818e+02 - logprior: -9.9584e-01
Epoch 9/10
39/39 - 9s - loss: 698.0107 - loglik: -6.9699e+02 - logprior: -1.0159e+00
Epoch 10/10
39/39 - 9s - loss: 698.5277 - loglik: -6.9750e+02 - logprior: -1.0251e+00
Fitted a model with MAP estimate = -698.1770
Time for alignment: 231.4862
Computed alignments with likelihoods: ['-698.4393', '-699.4175', '-698.3535', '-698.4398', '-698.1770']
Best model has likelihood: -698.1770  (prior= -0.9762 )
time for generating output: 0.2767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.4213791958332241
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8f9ef2370>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe90523aa30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe92133bcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe905145700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c7f1ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef1c7f1190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f8dd23d0>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8f9e27c70>, <__main__.SimpleDirichletPrior object at 0x7fe920b14fd0>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 962.0983 - loglik: -9.5986e+02 - logprior: -2.2410e+00
Epoch 2/10
39/39 - 10s - loss: 922.1396 - loglik: -9.2084e+02 - logprior: -1.2979e+00
Epoch 3/10
39/39 - 11s - loss: 917.6345 - loglik: -9.1635e+02 - logprior: -1.2859e+00
Epoch 4/10
39/39 - 11s - loss: 916.2430 - loglik: -9.1486e+02 - logprior: -1.3786e+00
Epoch 5/10
39/39 - 11s - loss: 915.4003 - loglik: -9.1388e+02 - logprior: -1.5246e+00
Epoch 6/10
39/39 - 11s - loss: 912.9590 - loglik: -9.1126e+02 - logprior: -1.6950e+00
Epoch 7/10
39/39 - 11s - loss: 912.9355 - loglik: -9.1106e+02 - logprior: -1.8761e+00
Epoch 8/10
39/39 - 11s - loss: 911.2434 - loglik: -9.0920e+02 - logprior: -2.0440e+00
Epoch 9/10
39/39 - 11s - loss: 911.4832 - loglik: -9.0929e+02 - logprior: -2.1961e+00
Fitted a model with MAP estimate = -910.1855
expansions: [(20, 1), (32, 5), (33, 2), (48, 3), (50, 1), (54, 1), (55, 3), (74, 1), (76, 2), (77, 3), (78, 2), (94, 1), (106, 1), (107, 1), (108, 1), (110, 1), (112, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 917.7164 - loglik: -9.1542e+02 - logprior: -2.2937e+00
Epoch 2/2
39/39 - 12s - loss: 908.3929 - loglik: -9.0746e+02 - logprior: -9.2811e-01
Fitted a model with MAP estimate = -906.2346
expansions: [(38, 1)]
discards: [29 30 56 68 99]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 911.0381 - loglik: -9.0920e+02 - logprior: -1.8332e+00
Epoch 2/2
39/39 - 12s - loss: 907.6620 - loglik: -9.0703e+02 - logprior: -6.3226e-01
Fitted a model with MAP estimate = -906.4181
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 909.5527 - loglik: -9.0782e+02 - logprior: -1.7342e+00
Epoch 2/10
39/39 - 12s - loss: 907.3303 - loglik: -9.0680e+02 - logprior: -5.3364e-01
Epoch 3/10
39/39 - 12s - loss: 906.2800 - loglik: -9.0576e+02 - logprior: -5.2001e-01
Epoch 4/10
39/39 - 12s - loss: 903.9490 - loglik: -9.0340e+02 - logprior: -5.4634e-01
Epoch 5/10
39/39 - 12s - loss: 903.0178 - loglik: -9.0239e+02 - logprior: -6.3138e-01
Epoch 6/10
39/39 - 12s - loss: 900.4462 - loglik: -8.9970e+02 - logprior: -7.4576e-01
Epoch 7/10
39/39 - 12s - loss: 900.7480 - loglik: -8.9985e+02 - logprior: -8.9413e-01
Fitted a model with MAP estimate = -899.2564
Time for alignment: 299.9384
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 961.0828 - loglik: -9.5887e+02 - logprior: -2.2112e+00
Epoch 2/10
39/39 - 11s - loss: 923.3961 - loglik: -9.2222e+02 - logprior: -1.1748e+00
Epoch 3/10
39/39 - 11s - loss: 918.8062 - loglik: -9.1759e+02 - logprior: -1.2178e+00
Epoch 4/10
39/39 - 11s - loss: 916.8118 - loglik: -9.1549e+02 - logprior: -1.3199e+00
Epoch 5/10
39/39 - 11s - loss: 915.5031 - loglik: -9.1406e+02 - logprior: -1.4399e+00
Epoch 6/10
39/39 - 11s - loss: 914.7038 - loglik: -9.1310e+02 - logprior: -1.6089e+00
Epoch 7/10
39/39 - 11s - loss: 914.8306 - loglik: -9.1304e+02 - logprior: -1.7862e+00
Fitted a model with MAP estimate = -912.9381
expansions: [(19, 2), (20, 1), (21, 2), (30, 6), (31, 1), (45, 1), (46, 1), (47, 1), (48, 2), (54, 2), (55, 1), (75, 2), (76, 4), (77, 2), (93, 1), (107, 3), (109, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 914.3620 - loglik: -9.1223e+02 - logprior: -2.1282e+00
Epoch 2/2
39/39 - 13s - loss: 906.3613 - loglik: -9.0555e+02 - logprior: -8.1193e-01
Fitted a model with MAP estimate = -904.8089
expansions: []
discards: [ 24  40  59  99 102]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 910.0002 - loglik: -9.0826e+02 - logprior: -1.7436e+00
Epoch 2/2
39/39 - 12s - loss: 907.2186 - loglik: -9.0665e+02 - logprior: -5.6687e-01
Fitted a model with MAP estimate = -905.4070
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 908.5197 - loglik: -9.0681e+02 - logprior: -1.7049e+00
Epoch 2/10
39/39 - 12s - loss: 906.7465 - loglik: -9.0623e+02 - logprior: -5.1550e-01
Epoch 3/10
39/39 - 12s - loss: 905.2097 - loglik: -9.0471e+02 - logprior: -4.9503e-01
Epoch 4/10
39/39 - 13s - loss: 903.1447 - loglik: -9.0262e+02 - logprior: -5.2814e-01
Epoch 5/10
39/39 - 13s - loss: 902.6432 - loglik: -9.0205e+02 - logprior: -5.9501e-01
Epoch 6/10
39/39 - 13s - loss: 899.3933 - loglik: -8.9867e+02 - logprior: -7.2003e-01
Epoch 7/10
39/39 - 12s - loss: 899.6470 - loglik: -8.9877e+02 - logprior: -8.7845e-01
Fitted a model with MAP estimate = -898.3562
Time for alignment: 285.7141
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 961.2814 - loglik: -9.5905e+02 - logprior: -2.2344e+00
Epoch 2/10
39/39 - 11s - loss: 920.5349 - loglik: -9.1924e+02 - logprior: -1.2906e+00
Epoch 3/10
39/39 - 11s - loss: 915.9619 - loglik: -9.1464e+02 - logprior: -1.3191e+00
Epoch 4/10
39/39 - 10s - loss: 914.9331 - loglik: -9.1350e+02 - logprior: -1.4377e+00
Epoch 5/10
39/39 - 11s - loss: 912.5934 - loglik: -9.1101e+02 - logprior: -1.5851e+00
Epoch 6/10
39/39 - 11s - loss: 911.2712 - loglik: -9.0949e+02 - logprior: -1.7820e+00
Epoch 7/10
39/39 - 11s - loss: 910.8706 - loglik: -9.0888e+02 - logprior: -1.9904e+00
Epoch 8/10
39/39 - 11s - loss: 909.1761 - loglik: -9.0702e+02 - logprior: -2.1580e+00
Epoch 9/10
39/39 - 11s - loss: 908.5483 - loglik: -9.0622e+02 - logprior: -2.3274e+00
Epoch 10/10
39/39 - 11s - loss: 908.2825 - loglik: -9.0584e+02 - logprior: -2.4401e+00
Fitted a model with MAP estimate = -907.4364
expansions: [(19, 2), (20, 1), (30, 1), (31, 1), (32, 2), (33, 3), (48, 1), (49, 1), (50, 2), (54, 1), (55, 3), (75, 1), (77, 1), (79, 2), (80, 6), (99, 4), (107, 1), (108, 1), (117, 1), (118, 1), (119, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 164 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 917.7086 - loglik: -9.1533e+02 - logprior: -2.3806e+00
Epoch 2/2
39/39 - 13s - loss: 905.9764 - loglik: -9.0502e+02 - logprior: -9.5151e-01
Fitted a model with MAP estimate = -903.9626
expansions: []
discards: [ 37  58  69 103 104 105 106 127]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 908.6564 - loglik: -9.0689e+02 - logprior: -1.7629e+00
Epoch 2/2
39/39 - 12s - loss: 905.9003 - loglik: -9.0534e+02 - logprior: -5.5909e-01
Fitted a model with MAP estimate = -904.0626
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 907.3254 - loglik: -9.0566e+02 - logprior: -1.6704e+00
Epoch 2/10
39/39 - 12s - loss: 904.9055 - loglik: -9.0443e+02 - logprior: -4.7269e-01
Epoch 3/10
39/39 - 12s - loss: 903.4622 - loglik: -9.0301e+02 - logprior: -4.5185e-01
Epoch 4/10
39/39 - 12s - loss: 902.2414 - loglik: -9.0176e+02 - logprior: -4.8341e-01
Epoch 5/10
39/39 - 12s - loss: 899.6669 - loglik: -8.9909e+02 - logprior: -5.7815e-01
Epoch 6/10
39/39 - 12s - loss: 899.0050 - loglik: -8.9829e+02 - logprior: -7.1157e-01
Epoch 7/10
39/39 - 12s - loss: 896.6861 - loglik: -8.9581e+02 - logprior: -8.7835e-01
Epoch 8/10
39/39 - 12s - loss: 895.4880 - loglik: -8.9449e+02 - logprior: -1.0002e+00
Epoch 9/10
39/39 - 12s - loss: 894.9086 - loglik: -8.9376e+02 - logprior: -1.1471e+00
Epoch 10/10
39/39 - 12s - loss: 893.6875 - loglik: -8.9241e+02 - logprior: -1.2817e+00
Fitted a model with MAP estimate = -893.3677
Time for alignment: 351.5067
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 962.2239 - loglik: -9.5998e+02 - logprior: -2.2437e+00
Epoch 2/10
39/39 - 11s - loss: 922.3252 - loglik: -9.2103e+02 - logprior: -1.2925e+00
Epoch 3/10
39/39 - 10s - loss: 919.9069 - loglik: -9.1861e+02 - logprior: -1.2935e+00
Epoch 4/10
39/39 - 11s - loss: 917.1364 - loglik: -9.1576e+02 - logprior: -1.3748e+00
Epoch 5/10
39/39 - 11s - loss: 915.3159 - loglik: -9.1381e+02 - logprior: -1.5082e+00
Epoch 6/10
39/39 - 11s - loss: 914.7360 - loglik: -9.1304e+02 - logprior: -1.6989e+00
Epoch 7/10
39/39 - 10s - loss: 914.0869 - loglik: -9.1221e+02 - logprior: -1.8752e+00
Epoch 8/10
39/39 - 11s - loss: 912.5333 - loglik: -9.1046e+02 - logprior: -2.0710e+00
Epoch 9/10
39/39 - 11s - loss: 911.4567 - loglik: -9.0921e+02 - logprior: -2.2468e+00
Epoch 10/10
39/39 - 11s - loss: 910.5255 - loglik: -9.0813e+02 - logprior: -2.4004e+00
Fitted a model with MAP estimate = -909.9388
expansions: [(19, 2), (20, 1), (31, 3), (32, 1), (33, 1), (46, 1), (47, 1), (48, 1), (49, 2), (75, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (94, 1), (95, 1), (98, 1), (102, 1), (105, 1), (106, 1), (107, 1), (109, 2)]
discards: [ 1 58]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 919.7947 - loglik: -9.1752e+02 - logprior: -2.2763e+00
Epoch 2/2
39/39 - 12s - loss: 908.7070 - loglik: -9.0787e+02 - logprior: -8.4152e-01
Fitted a model with MAP estimate = -907.0380
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 910.7975 - loglik: -9.0902e+02 - logprior: -1.7735e+00
Epoch 2/2
39/39 - 12s - loss: 908.9395 - loglik: -9.0836e+02 - logprior: -5.8235e-01
Fitted a model with MAP estimate = -906.7720
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 910.4150 - loglik: -9.0875e+02 - logprior: -1.6628e+00
Epoch 2/10
39/39 - 12s - loss: 907.5475 - loglik: -9.0708e+02 - logprior: -4.7144e-01
Epoch 3/10
39/39 - 12s - loss: 907.2184 - loglik: -9.0677e+02 - logprior: -4.5212e-01
Epoch 4/10
39/39 - 12s - loss: 904.4067 - loglik: -9.0395e+02 - logprior: -4.6077e-01
Epoch 5/10
39/39 - 12s - loss: 903.1500 - loglik: -9.0262e+02 - logprior: -5.3330e-01
Epoch 6/10
39/39 - 12s - loss: 900.2161 - loglik: -8.9955e+02 - logprior: -6.6621e-01
Epoch 7/10
39/39 - 12s - loss: 901.3358 - loglik: -9.0051e+02 - logprior: -8.2086e-01
Fitted a model with MAP estimate = -899.4832
Time for alignment: 317.5245
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 961.0073 - loglik: -9.5877e+02 - logprior: -2.2408e+00
Epoch 2/10
39/39 - 11s - loss: 922.3103 - loglik: -9.2101e+02 - logprior: -1.2953e+00
Epoch 3/10
39/39 - 11s - loss: 917.2333 - loglik: -9.1593e+02 - logprior: -1.3062e+00
Epoch 4/10
39/39 - 11s - loss: 916.0928 - loglik: -9.1470e+02 - logprior: -1.3947e+00
Epoch 5/10
39/39 - 11s - loss: 913.6199 - loglik: -9.1208e+02 - logprior: -1.5389e+00
Epoch 6/10
39/39 - 11s - loss: 913.3705 - loglik: -9.1167e+02 - logprior: -1.7034e+00
Epoch 7/10
39/39 - 11s - loss: 912.2944 - loglik: -9.1043e+02 - logprior: -1.8675e+00
Epoch 8/10
39/39 - 11s - loss: 912.4536 - loglik: -9.1042e+02 - logprior: -2.0353e+00
Fitted a model with MAP estimate = -910.8884
expansions: [(19, 2), (20, 1), (21, 2), (31, 1), (32, 1), (48, 1), (49, 1), (50, 2), (54, 1), (56, 1), (76, 1), (78, 2), (79, 7), (80, 2), (99, 1), (106, 2), (107, 1), (108, 2), (110, 1), (112, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 914.4795 - loglik: -9.1227e+02 - logprior: -2.2098e+00
Epoch 2/2
39/39 - 13s - loss: 906.6475 - loglik: -9.0575e+02 - logprior: -8.9847e-01
Fitted a model with MAP estimate = -904.9173
expansions: [(24, 1)]
discards: [ 55  98  99 100 101 136]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 909.1299 - loglik: -9.0738e+02 - logprior: -1.7508e+00
Epoch 2/2
39/39 - 12s - loss: 906.5564 - loglik: -9.0601e+02 - logprior: -5.4807e-01
Fitted a model with MAP estimate = -904.6971
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 908.1198 - loglik: -9.0644e+02 - logprior: -1.6827e+00
Epoch 2/10
39/39 - 12s - loss: 905.7242 - loglik: -9.0524e+02 - logprior: -4.8103e-01
Epoch 3/10
39/39 - 12s - loss: 904.1341 - loglik: -9.0367e+02 - logprior: -4.6296e-01
Epoch 4/10
39/39 - 12s - loss: 903.2691 - loglik: -9.0278e+02 - logprior: -4.9367e-01
Epoch 5/10
39/39 - 12s - loss: 901.7308 - loglik: -9.0115e+02 - logprior: -5.8437e-01
Epoch 6/10
39/39 - 12s - loss: 899.2198 - loglik: -8.9852e+02 - logprior: -6.9764e-01
Epoch 7/10
39/39 - 12s - loss: 899.2065 - loglik: -8.9838e+02 - logprior: -8.2780e-01
Epoch 8/10
39/39 - 12s - loss: 897.7286 - loglik: -8.9675e+02 - logprior: -9.7641e-01
Epoch 9/10
39/39 - 12s - loss: 897.6901 - loglik: -8.9658e+02 - logprior: -1.1081e+00
Epoch 10/10
39/39 - 12s - loss: 896.5342 - loglik: -8.9530e+02 - logprior: -1.2363e+00
Fitted a model with MAP estimate = -895.6176
Time for alignment: 329.1124
Computed alignments with likelihoods: ['-899.2564', '-898.3562', '-893.3677', '-899.4832', '-895.6176']
Best model has likelihood: -893.3677  (prior= -1.3662 )
time for generating output: 0.2119
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8648864089278597
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feefcf3c430>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff139eacb20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff142d59460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef785fe400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe571520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9500b5370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2dea5d490>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fef807e2f40>, <__main__.SimpleDirichletPrior object at 0x7fe918c0dc70>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1631.5770 - loglik: -1.6296e+03 - logprior: -1.9303e+00
Epoch 2/10
39/39 - 25s - loss: 1562.4880 - loglik: -1.5618e+03 - logprior: -7.0184e-01
Epoch 3/10
39/39 - 26s - loss: 1553.1793 - loglik: -1.5523e+03 - logprior: -9.1344e-01
Epoch 4/10
39/39 - 26s - loss: 1550.2714 - loglik: -1.5493e+03 - logprior: -1.0010e+00
Epoch 5/10
39/39 - 28s - loss: 1548.5109 - loglik: -1.5474e+03 - logprior: -1.1216e+00
Epoch 6/10
39/39 - 29s - loss: 1547.6940 - loglik: -1.5464e+03 - logprior: -1.2954e+00
Epoch 7/10
39/39 - 29s - loss: 1545.9817 - loglik: -1.5445e+03 - logprior: -1.4947e+00
Epoch 8/10
39/39 - 28s - loss: 1545.9355 - loglik: -1.5442e+03 - logprior: -1.7223e+00
Epoch 9/10
39/39 - 26s - loss: 1545.8030 - loglik: -1.5439e+03 - logprior: -1.9408e+00
Epoch 10/10
39/39 - 25s - loss: 1544.8207 - loglik: -1.5427e+03 - logprior: -2.1627e+00
Fitted a model with MAP estimate = -1542.5544
expansions: [(0, 3), (9, 1), (10, 1), (24, 1), (40, 1), (43, 1), (51, 2), (59, 1), (62, 1), (85, 2), (89, 10), (115, 3), (116, 2), (119, 1), (121, 1), (125, 1), (156, 3), (175, 1), (178, 4), (181, 1), (183, 1), (206, 2), (208, 1), (209, 1), (216, 1)]
discards: [166 167]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1555.4196 - loglik: -1.5528e+03 - logprior: -2.5905e+00
Epoch 2/2
39/39 - 30s - loss: 1541.8225 - loglik: -1.5413e+03 - logprior: -5.6666e-01
Fitted a model with MAP estimate = -1538.7094
expansions: [(0, 3), (184, 3)]
discards: [  1   2   3   4  60  97  98 143 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1545.4066 - loglik: -1.5430e+03 - logprior: -2.3908e+00
Epoch 2/2
39/39 - 29s - loss: 1540.7100 - loglik: -1.5403e+03 - logprior: -3.6161e-01
Fitted a model with MAP estimate = -1537.7768
expansions: [(0, 3), (95, 2), (187, 3), (210, 1), (244, 3)]
discards: [  0   1   2 182 183 184]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1542.0222 - loglik: -1.5405e+03 - logprior: -1.5187e+00
Epoch 2/10
39/39 - 30s - loss: 1538.0918 - loglik: -1.5379e+03 - logprior: -1.7393e-01
Epoch 3/10
39/39 - 30s - loss: 1536.6722 - loglik: -1.5365e+03 - logprior: -1.4472e-01
Epoch 4/10
39/39 - 31s - loss: 1534.7328 - loglik: -1.5346e+03 - logprior: -1.5994e-01
Epoch 5/10
39/39 - 31s - loss: 1532.9003 - loglik: -1.5327e+03 - logprior: -1.9609e-01
Epoch 6/10
39/39 - 31s - loss: 1531.7399 - loglik: -1.5315e+03 - logprior: -2.8851e-01
Epoch 7/10
39/39 - 31s - loss: 1530.9700 - loglik: -1.5306e+03 - logprior: -4.1243e-01
Epoch 8/10
39/39 - 30s - loss: 1530.6732 - loglik: -1.5301e+03 - logprior: -5.4323e-01
Epoch 9/10
39/39 - 31s - loss: 1529.5537 - loglik: -1.5288e+03 - logprior: -7.0561e-01
Epoch 10/10
39/39 - 30s - loss: 1528.4305 - loglik: -1.5276e+03 - logprior: -8.5770e-01
Fitted a model with MAP estimate = -1527.5342
Time for alignment: 838.4448
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1631.7760 - loglik: -1.6298e+03 - logprior: -1.9331e+00
Epoch 2/10
39/39 - 23s - loss: 1562.2810 - loglik: -1.5615e+03 - logprior: -7.6490e-01
Epoch 3/10
39/39 - 24s - loss: 1553.0618 - loglik: -1.5521e+03 - logprior: -9.9735e-01
Epoch 4/10
39/39 - 23s - loss: 1550.3496 - loglik: -1.5493e+03 - logprior: -1.0771e+00
Epoch 5/10
39/39 - 23s - loss: 1548.7771 - loglik: -1.5476e+03 - logprior: -1.1860e+00
Epoch 6/10
39/39 - 23s - loss: 1547.4320 - loglik: -1.5461e+03 - logprior: -1.3235e+00
Epoch 7/10
39/39 - 24s - loss: 1545.9989 - loglik: -1.5445e+03 - logprior: -1.5109e+00
Epoch 8/10
39/39 - 23s - loss: 1546.0494 - loglik: -1.5443e+03 - logprior: -1.7194e+00
Fitted a model with MAP estimate = -1543.9120
expansions: [(0, 3), (9, 2), (24, 1), (40, 1), (42, 1), (49, 1), (50, 1), (51, 1), (57, 1), (60, 1), (79, 1), (88, 15), (94, 1), (96, 2), (114, 2), (117, 2), (118, 1), (125, 1), (152, 1), (157, 2), (176, 1), (178, 2), (179, 3), (183, 1), (184, 1), (206, 7), (207, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1551.3868 - loglik: -1.5489e+03 - logprior: -2.5056e+00
Epoch 2/2
39/39 - 32s - loss: 1539.4164 - loglik: -1.5388e+03 - logprior: -6.4078e-01
Fitted a model with MAP estimate = -1536.3436
expansions: [(0, 3), (190, 2)]
discards: [  1   2   3   4  12 105 106 107 108 109 110 127 152 256 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1543.7288 - loglik: -1.5414e+03 - logprior: -2.3736e+00
Epoch 2/2
39/39 - 30s - loss: 1538.9506 - loglik: -1.5386e+03 - logprior: -3.6784e-01
Fitted a model with MAP estimate = -1536.4160
expansions: [(0, 3), (248, 1)]
discards: [  0   1 188]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1540.7426 - loglik: -1.5392e+03 - logprior: -1.5183e+00
Epoch 2/10
39/39 - 31s - loss: 1536.8173 - loglik: -1.5366e+03 - logprior: -2.4921e-01
Epoch 3/10
39/39 - 30s - loss: 1535.5881 - loglik: -1.5354e+03 - logprior: -2.0162e-01
Epoch 4/10
39/39 - 31s - loss: 1534.1930 - loglik: -1.5340e+03 - logprior: -1.9097e-01
Epoch 5/10
39/39 - 31s - loss: 1531.2766 - loglik: -1.5310e+03 - logprior: -2.2688e-01
Epoch 6/10
39/39 - 31s - loss: 1530.6063 - loglik: -1.5303e+03 - logprior: -2.7675e-01
Epoch 7/10
39/39 - 31s - loss: 1530.1654 - loglik: -1.5298e+03 - logprior: -3.8024e-01
Epoch 8/10
39/39 - 31s - loss: 1529.0464 - loglik: -1.5286e+03 - logprior: -4.7349e-01
Epoch 9/10
39/39 - 31s - loss: 1529.6398 - loglik: -1.5290e+03 - logprior: -6.2893e-01
Fitted a model with MAP estimate = -1527.7786
Time for alignment: 736.0319
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1629.0892 - loglik: -1.6272e+03 - logprior: -1.9188e+00
Epoch 2/10
39/39 - 24s - loss: 1561.4685 - loglik: -1.5609e+03 - logprior: -6.1786e-01
Epoch 3/10
39/39 - 24s - loss: 1553.3577 - loglik: -1.5525e+03 - logprior: -8.1709e-01
Epoch 4/10
39/39 - 24s - loss: 1551.5092 - loglik: -1.5506e+03 - logprior: -8.8206e-01
Epoch 5/10
39/39 - 24s - loss: 1548.3685 - loglik: -1.5474e+03 - logprior: -9.9789e-01
Epoch 6/10
39/39 - 24s - loss: 1547.7943 - loglik: -1.5466e+03 - logprior: -1.1792e+00
Epoch 7/10
39/39 - 23s - loss: 1545.8781 - loglik: -1.5445e+03 - logprior: -1.4005e+00
Epoch 8/10
39/39 - 23s - loss: 1545.6709 - loglik: -1.5440e+03 - logprior: -1.6261e+00
Epoch 9/10
39/39 - 23s - loss: 1545.0751 - loglik: -1.5432e+03 - logprior: -1.8566e+00
Epoch 10/10
39/39 - 22s - loss: 1543.0784 - loglik: -1.5409e+03 - logprior: -2.1297e+00
Fitted a model with MAP estimate = -1541.5400
expansions: [(0, 4), (40, 1), (43, 1), (51, 2), (53, 1), (62, 1), (85, 4), (86, 1), (90, 11), (102, 1), (115, 3), (116, 2), (121, 1), (128, 1), (153, 2), (159, 2), (177, 1), (179, 1), (183, 5), (184, 1), (206, 6), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1555.3118 - loglik: -1.5527e+03 - logprior: -2.6438e+00
Epoch 2/2
39/39 - 31s - loss: 1540.1550 - loglik: -1.5396e+03 - logprior: -5.4213e-01
Fitted a model with MAP estimate = -1536.8663
expansions: [(0, 3), (191, 3)]
discards: [  1   2   3   4  58  97  98 109 146 195 258]
Re-initialized the encoder parameters.
Fitting a model of length 278 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1543.6411 - loglik: -1.5413e+03 - logprior: -2.3780e+00
Epoch 2/2
39/39 - 31s - loss: 1539.0336 - loglik: -1.5386e+03 - logprior: -3.8577e-01
Fitted a model with MAP estimate = -1536.0314
expansions: [(0, 3), (186, 1), (221, 1)]
discards: [  0   1   2   3 187 188 189 190 191 202 203 250 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1542.6019 - loglik: -1.5412e+03 - logprior: -1.3662e+00
Epoch 2/10
39/39 - 31s - loss: 1538.7113 - loglik: -1.5386e+03 - logprior: -1.6042e-01
Epoch 3/10
39/39 - 31s - loss: 1537.2095 - loglik: -1.5371e+03 - logprior: -1.4294e-01
Epoch 4/10
39/39 - 31s - loss: 1535.1404 - loglik: -1.5350e+03 - logprior: -1.3958e-01
Epoch 5/10
39/39 - 31s - loss: 1533.8806 - loglik: -1.5337e+03 - logprior: -1.6413e-01
Epoch 6/10
39/39 - 31s - loss: 1532.3602 - loglik: -1.5322e+03 - logprior: -2.0173e-01
Epoch 7/10
39/39 - 31s - loss: 1530.7279 - loglik: -1.5304e+03 - logprior: -3.0688e-01
Epoch 8/10
39/39 - 31s - loss: 1531.9253 - loglik: -1.5315e+03 - logprior: -4.3232e-01
Fitted a model with MAP estimate = -1530.1623
Time for alignment: 756.2363
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1631.7644 - loglik: -1.6298e+03 - logprior: -1.9198e+00
Epoch 2/10
39/39 - 25s - loss: 1561.5863 - loglik: -1.5610e+03 - logprior: -6.0987e-01
Epoch 3/10
39/39 - 25s - loss: 1551.8007 - loglik: -1.5510e+03 - logprior: -7.6803e-01
Epoch 4/10
39/39 - 25s - loss: 1549.4369 - loglik: -1.5486e+03 - logprior: -7.9780e-01
Epoch 5/10
39/39 - 25s - loss: 1546.9374 - loglik: -1.5460e+03 - logprior: -9.0329e-01
Epoch 6/10
39/39 - 25s - loss: 1545.9794 - loglik: -1.5449e+03 - logprior: -1.0536e+00
Epoch 7/10
39/39 - 25s - loss: 1545.2216 - loglik: -1.5440e+03 - logprior: -1.2577e+00
Epoch 8/10
39/39 - 25s - loss: 1544.5555 - loglik: -1.5431e+03 - logprior: -1.4394e+00
Epoch 9/10
39/39 - 25s - loss: 1544.2448 - loglik: -1.5426e+03 - logprior: -1.6533e+00
Epoch 10/10
39/39 - 25s - loss: 1543.0443 - loglik: -1.5411e+03 - logprior: -1.9063e+00
Fitted a model with MAP estimate = -1540.9987
expansions: [(0, 4), (23, 1), (43, 2), (51, 2), (52, 1), (62, 1), (84, 2), (90, 12), (104, 1), (115, 2), (118, 2), (119, 1), (121, 1), (153, 1), (158, 2), (177, 9), (207, 1), (208, 7), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 1554.3158 - loglik: -1.5517e+03 - logprior: -2.5955e+00
Epoch 2/2
39/39 - 34s - loss: 1539.2980 - loglik: -1.5388e+03 - logprior: -5.3499e-01
Fitted a model with MAP estimate = -1536.1711
expansions: [(0, 3), (187, 2)]
discards: [  1   2  91  92  93 107 108 147 214 215 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 1544.0627 - loglik: -1.5417e+03 - logprior: -2.3979e+00
Epoch 2/2
39/39 - 33s - loss: 1539.2266 - loglik: -1.5388e+03 - logprior: -4.5578e-01
Fitted a model with MAP estimate = -1536.4380
expansions: [(0, 3), (97, 2)]
discards: [  1   2   3   4  51  93 184 185 186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 1543.4570 - loglik: -1.5408e+03 - logprior: -2.6192e+00
Epoch 2/10
39/39 - 30s - loss: 1538.7767 - loglik: -1.5384e+03 - logprior: -3.5505e-01
Epoch 3/10
39/39 - 30s - loss: 1536.2357 - loglik: -1.5361e+03 - logprior: -1.2842e-01
Epoch 4/10
39/39 - 30s - loss: 1535.3960 - loglik: -1.5353e+03 - logprior: -1.1766e-01
Epoch 5/10
39/39 - 30s - loss: 1532.7998 - loglik: -1.5326e+03 - logprior: -1.5767e-01
Epoch 6/10
39/39 - 30s - loss: 1532.1980 - loglik: -1.5320e+03 - logprior: -2.0827e-01
Epoch 7/10
39/39 - 30s - loss: 1531.3123 - loglik: -1.5310e+03 - logprior: -3.1419e-01
Epoch 8/10
39/39 - 29s - loss: 1531.6842 - loglik: -1.5313e+03 - logprior: -4.2184e-01
Fitted a model with MAP estimate = -1529.7464
Time for alignment: 775.9374
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1631.3577 - loglik: -1.6294e+03 - logprior: -1.9195e+00
Epoch 2/10
39/39 - 23s - loss: 1566.1888 - loglik: -1.5656e+03 - logprior: -6.2687e-01
Epoch 3/10
39/39 - 23s - loss: 1554.9336 - loglik: -1.5542e+03 - logprior: -7.7986e-01
Epoch 4/10
39/39 - 23s - loss: 1550.8379 - loglik: -1.5500e+03 - logprior: -8.0868e-01
Epoch 5/10
39/39 - 22s - loss: 1548.2838 - loglik: -1.5474e+03 - logprior: -9.2414e-01
Epoch 6/10
39/39 - 23s - loss: 1547.2911 - loglik: -1.5462e+03 - logprior: -1.1069e+00
Epoch 7/10
39/39 - 22s - loss: 1546.0599 - loglik: -1.5447e+03 - logprior: -1.3110e+00
Epoch 8/10
39/39 - 22s - loss: 1545.9240 - loglik: -1.5444e+03 - logprior: -1.5292e+00
Epoch 9/10
39/39 - 23s - loss: 1544.5997 - loglik: -1.5428e+03 - logprior: -1.7645e+00
Epoch 10/10
39/39 - 22s - loss: 1543.7662 - loglik: -1.5418e+03 - logprior: -1.9924e+00
Fitted a model with MAP estimate = -1541.6123
expansions: [(0, 4), (51, 1), (53, 1), (56, 1), (81, 1), (85, 1), (86, 1), (92, 9), (95, 1), (97, 2), (115, 2), (118, 2), (119, 1), (120, 1), (136, 2), (152, 2), (154, 3), (158, 2), (176, 1), (179, 3), (206, 5), (208, 1), (216, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1557.2179 - loglik: -1.5546e+03 - logprior: -2.5686e+00
Epoch 2/2
39/39 - 29s - loss: 1541.7913 - loglik: -1.5413e+03 - logprior: -5.3824e-01
Fitted a model with MAP estimate = -1538.6199
expansions: [(0, 3), (105, 1), (106, 1), (246, 3)]
discards: [  1   2 118 143 165 188 192 193 204 248 249]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 1545.6412 - loglik: -1.5432e+03 - logprior: -2.3964e+00
Epoch 2/2
39/39 - 29s - loss: 1541.0859 - loglik: -1.5406e+03 - logprior: -4.6060e-01
Fitted a model with MAP estimate = -1537.8451
expansions: [(0, 3)]
discards: [  1   2   3   4  90 190 201 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 1544.9923 - loglik: -1.5424e+03 - logprior: -2.6006e+00
Epoch 2/10
39/39 - 28s - loss: 1540.3785 - loglik: -1.5401e+03 - logprior: -3.0453e-01
Epoch 3/10
39/39 - 28s - loss: 1538.3044 - loglik: -1.5382e+03 - logprior: -7.8642e-02
Epoch 4/10
39/39 - 28s - loss: 1536.4403 - loglik: -1.5364e+03 - logprior: -6.4051e-02
Epoch 5/10
39/39 - 28s - loss: 1534.4373 - loglik: -1.5343e+03 - logprior: -9.9402e-02
Epoch 6/10
39/39 - 29s - loss: 1534.2653 - loglik: -1.5341e+03 - logprior: -1.4663e-01
Epoch 7/10
39/39 - 29s - loss: 1532.6646 - loglik: -1.5324e+03 - logprior: -2.5582e-01
Epoch 8/10
39/39 - 29s - loss: 1532.1866 - loglik: -1.5318e+03 - logprior: -3.5692e-01
Epoch 9/10
39/39 - 30s - loss: 1532.1785 - loglik: -1.5317e+03 - logprior: -5.0165e-01
Epoch 10/10
39/39 - 29s - loss: 1529.9731 - loglik: -1.5293e+03 - logprior: -6.3983e-01
Fitted a model with MAP estimate = -1529.5786
Time for alignment: 770.3154
Computed alignments with likelihoods: ['-1527.5342', '-1527.7786', '-1530.1623', '-1529.7464', '-1529.5786']
Best model has likelihood: -1527.5342  (prior= -0.8363 )
time for generating output: 0.4164
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.6102048417132216
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe941383d30>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe930f1dd00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe930864d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fef80653040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff1211837c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe8c1dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feefe8c1820>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe911d9a940>, <__main__.SimpleDirichletPrior object at 0x7fe892ff6c10>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1231.8492 - loglik: -1.2296e+03 - logprior: -2.2503e+00
Epoch 2/10
39/39 - 13s - loss: 1138.9314 - loglik: -1.1373e+03 - logprior: -1.6451e+00
Epoch 3/10
39/39 - 13s - loss: 1131.1511 - loglik: -1.1295e+03 - logprior: -1.6819e+00
Epoch 4/10
39/39 - 13s - loss: 1129.4493 - loglik: -1.1277e+03 - logprior: -1.7371e+00
Epoch 5/10
39/39 - 13s - loss: 1128.5798 - loglik: -1.1267e+03 - logprior: -1.8568e+00
Epoch 6/10
39/39 - 13s - loss: 1128.2576 - loglik: -1.1263e+03 - logprior: -1.9812e+00
Epoch 7/10
39/39 - 13s - loss: 1128.3802 - loglik: -1.1263e+03 - logprior: -2.1178e+00
Fitted a model with MAP estimate = -1121.2875
expansions: [(12, 1), (19, 1), (22, 1), (25, 1), (28, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (109, 1), (110, 2), (112, 2), (122, 1), (129, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 3), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1122.8158 - loglik: -1.1200e+03 - logprior: -2.8587e+00
Epoch 2/2
39/39 - 18s - loss: 1111.8713 - loglik: -1.1106e+03 - logprior: -1.2634e+00
Fitted a model with MAP estimate = -1103.3590
expansions: [(3, 1)]
discards: [  0  84 138 141 188 189 190 191 200 202 204]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1115.9902 - loglik: -1.1136e+03 - logprior: -2.3903e+00
Epoch 2/2
39/39 - 17s - loss: 1111.3323 - loglik: -1.1107e+03 - logprior: -6.5593e-01
Fitted a model with MAP estimate = -1103.4664
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1109.4968 - loglik: -1.1072e+03 - logprior: -2.3111e+00
Epoch 2/10
39/39 - 17s - loss: 1105.1259 - loglik: -1.1041e+03 - logprior: -1.0460e+00
Epoch 3/10
39/39 - 17s - loss: 1102.8413 - loglik: -1.1026e+03 - logprior: -2.1221e-01
Epoch 4/10
39/39 - 17s - loss: 1101.9918 - loglik: -1.1018e+03 - logprior: -2.1816e-01
Epoch 5/10
39/39 - 17s - loss: 1101.0986 - loglik: -1.1008e+03 - logprior: -2.9508e-01
Epoch 6/10
39/39 - 17s - loss: 1101.3606 - loglik: -1.1010e+03 - logprior: -3.8465e-01
Fitted a model with MAP estimate = -1100.2213
Time for alignment: 366.2391
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1233.4835 - loglik: -1.2312e+03 - logprior: -2.2516e+00
Epoch 2/10
39/39 - 13s - loss: 1137.8304 - loglik: -1.1361e+03 - logprior: -1.7154e+00
Epoch 3/10
39/39 - 13s - loss: 1130.4194 - loglik: -1.1286e+03 - logprior: -1.7696e+00
Epoch 4/10
39/39 - 13s - loss: 1128.0970 - loglik: -1.1263e+03 - logprior: -1.7935e+00
Epoch 5/10
39/39 - 13s - loss: 1127.9469 - loglik: -1.1261e+03 - logprior: -1.8848e+00
Epoch 6/10
39/39 - 13s - loss: 1126.9395 - loglik: -1.1249e+03 - logprior: -2.0045e+00
Epoch 7/10
39/39 - 13s - loss: 1127.2146 - loglik: -1.1251e+03 - logprior: -2.1051e+00
Fitted a model with MAP estimate = -1120.2747
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (45, 1), (46, 1), (47, 1), (49, 1), (66, 1), (67, 2), (70, 1), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (169, 1), (176, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1122.8640 - loglik: -1.1201e+03 - logprior: -2.7990e+00
Epoch 2/2
39/39 - 18s - loss: 1111.6298 - loglik: -1.1104e+03 - logprior: -1.1897e+00
Fitted a model with MAP estimate = -1103.1967
expansions: [(3, 1)]
discards: [  0  26  84 139 142 189 190 191 192 200 203]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1116.0774 - loglik: -1.1137e+03 - logprior: -2.3808e+00
Epoch 2/2
39/39 - 17s - loss: 1111.2448 - loglik: -1.1106e+03 - logprior: -6.3096e-01
Fitted a model with MAP estimate = -1103.4402
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1109.2697 - loglik: -1.1070e+03 - logprior: -2.3047e+00
Epoch 2/10
39/39 - 16s - loss: 1105.3451 - loglik: -1.1043e+03 - logprior: -1.0359e+00
Epoch 3/10
39/39 - 16s - loss: 1103.0553 - loglik: -1.1029e+03 - logprior: -2.0074e-01
Epoch 4/10
39/39 - 16s - loss: 1102.2296 - loglik: -1.1020e+03 - logprior: -2.0240e-01
Epoch 5/10
39/39 - 16s - loss: 1101.3138 - loglik: -1.1010e+03 - logprior: -2.8171e-01
Epoch 6/10
39/39 - 16s - loss: 1100.1200 - loglik: -1.0998e+03 - logprior: -3.6308e-01
Epoch 7/10
39/39 - 16s - loss: 1101.6003 - loglik: -1.1012e+03 - logprior: -4.4980e-01
Fitted a model with MAP estimate = -1099.8709
Time for alignment: 380.7985
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1233.0725 - loglik: -1.2308e+03 - logprior: -2.2499e+00
Epoch 2/10
39/39 - 12s - loss: 1139.6543 - loglik: -1.1380e+03 - logprior: -1.6831e+00
Epoch 3/10
39/39 - 12s - loss: 1131.6693 - loglik: -1.1299e+03 - logprior: -1.7602e+00
Epoch 4/10
39/39 - 12s - loss: 1130.0657 - loglik: -1.1283e+03 - logprior: -1.7773e+00
Epoch 5/10
39/39 - 12s - loss: 1129.4794 - loglik: -1.1276e+03 - logprior: -1.8904e+00
Epoch 6/10
39/39 - 12s - loss: 1129.2664 - loglik: -1.1273e+03 - logprior: -2.0115e+00
Epoch 7/10
39/39 - 12s - loss: 1128.9625 - loglik: -1.1268e+03 - logprior: -2.1269e+00
Epoch 8/10
39/39 - 12s - loss: 1128.7327 - loglik: -1.1265e+03 - logprior: -2.2679e+00
Epoch 9/10
39/39 - 12s - loss: 1128.7754 - loglik: -1.1264e+03 - logprior: -2.3856e+00
Fitted a model with MAP estimate = -1121.9091
expansions: [(8, 1), (12, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (50, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (93, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 3), (151, 2), (153, 2), (155, 2), (156, 2), (157, 3), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1123.4308 - loglik: -1.1205e+03 - logprior: -2.8924e+00
Epoch 2/2
39/39 - 16s - loss: 1111.8027 - loglik: -1.1105e+03 - logprior: -1.3419e+00
Fitted a model with MAP estimate = -1103.3821
expansions: [(3, 1)]
discards: [  0  26  85 139 142 190 191 195 201 203 205]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1115.7513 - loglik: -1.1133e+03 - logprior: -2.4018e+00
Epoch 2/2
39/39 - 16s - loss: 1111.3866 - loglik: -1.1108e+03 - logprior: -6.3234e-01
Fitted a model with MAP estimate = -1103.1988
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1108.9679 - loglik: -1.1067e+03 - logprior: -2.2925e+00
Epoch 2/10
39/39 - 16s - loss: 1105.5371 - loglik: -1.1045e+03 - logprior: -1.0024e+00
Epoch 3/10
39/39 - 16s - loss: 1102.2452 - loglik: -1.1021e+03 - logprior: -1.8456e-01
Epoch 4/10
39/39 - 16s - loss: 1101.9487 - loglik: -1.1018e+03 - logprior: -1.9154e-01
Epoch 5/10
39/39 - 16s - loss: 1101.0220 - loglik: -1.1007e+03 - logprior: -2.8848e-01
Epoch 6/10
39/39 - 16s - loss: 1100.7106 - loglik: -1.1004e+03 - logprior: -3.6048e-01
Epoch 7/10
39/39 - 16s - loss: 1100.0530 - loglik: -1.0996e+03 - logprior: -4.6393e-01
Epoch 8/10
39/39 - 16s - loss: 1099.7706 - loglik: -1.0992e+03 - logprior: -5.6860e-01
Epoch 9/10
39/39 - 16s - loss: 1099.4087 - loglik: -1.0988e+03 - logprior: -6.5721e-01
Epoch 10/10
39/39 - 16s - loss: 1099.0884 - loglik: -1.0984e+03 - logprior: -7.3399e-01
Fitted a model with MAP estimate = -1098.3515
Time for alignment: 431.8860
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1232.6761 - loglik: -1.2304e+03 - logprior: -2.2494e+00
Epoch 2/10
39/39 - 12s - loss: 1138.1724 - loglik: -1.1365e+03 - logprior: -1.6916e+00
Epoch 3/10
39/39 - 12s - loss: 1130.2136 - loglik: -1.1284e+03 - logprior: -1.7704e+00
Epoch 4/10
39/39 - 13s - loss: 1129.0059 - loglik: -1.1273e+03 - logprior: -1.7488e+00
Epoch 5/10
39/39 - 13s - loss: 1128.2213 - loglik: -1.1264e+03 - logprior: -1.8660e+00
Epoch 6/10
39/39 - 13s - loss: 1127.5525 - loglik: -1.1256e+03 - logprior: -1.9867e+00
Epoch 7/10
39/39 - 13s - loss: 1127.8014 - loglik: -1.1257e+03 - logprior: -2.1177e+00
Fitted a model with MAP estimate = -1120.8564
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (94, 1), (97, 1), (109, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (169, 1), (178, 1), (183, 1), (184, 1), (188, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1122.9801 - loglik: -1.1202e+03 - logprior: -2.8173e+00
Epoch 2/2
39/39 - 17s - loss: 1111.7036 - loglik: -1.1104e+03 - logprior: -1.2880e+00
Fitted a model with MAP estimate = -1103.1427
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 190 191 192 200 203]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1116.0709 - loglik: -1.1137e+03 - logprior: -2.3783e+00
Epoch 2/2
39/39 - 17s - loss: 1111.4545 - loglik: -1.1108e+03 - logprior: -6.4526e-01
Fitted a model with MAP estimate = -1103.3711
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1109.1940 - loglik: -1.1069e+03 - logprior: -2.2955e+00
Epoch 2/10
39/39 - 17s - loss: 1104.9880 - loglik: -1.1040e+03 - logprior: -9.8947e-01
Epoch 3/10
39/39 - 17s - loss: 1103.5548 - loglik: -1.1034e+03 - logprior: -1.8661e-01
Epoch 4/10
39/39 - 17s - loss: 1101.6648 - loglik: -1.1015e+03 - logprior: -1.9683e-01
Epoch 5/10
39/39 - 17s - loss: 1101.6814 - loglik: -1.1014e+03 - logprior: -2.7023e-01
Fitted a model with MAP estimate = -1100.5179
Time for alignment: 340.2689
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1233.7941 - loglik: -1.2315e+03 - logprior: -2.2473e+00
Epoch 2/10
39/39 - 13s - loss: 1139.0037 - loglik: -1.1373e+03 - logprior: -1.6680e+00
Epoch 3/10
39/39 - 12s - loss: 1130.6519 - loglik: -1.1289e+03 - logprior: -1.7408e+00
Epoch 4/10
39/39 - 13s - loss: 1128.8215 - loglik: -1.1271e+03 - logprior: -1.7521e+00
Epoch 5/10
39/39 - 13s - loss: 1127.8741 - loglik: -1.1260e+03 - logprior: -1.8641e+00
Epoch 6/10
39/39 - 13s - loss: 1127.8937 - loglik: -1.1259e+03 - logprior: -1.9742e+00
Fitted a model with MAP estimate = -1120.9859
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (52, 1), (66, 1), (67, 2), (71, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1122.0349 - loglik: -1.1192e+03 - logprior: -2.8040e+00
Epoch 2/2
39/39 - 17s - loss: 1111.7150 - loglik: -1.1106e+03 - logprior: -1.1365e+00
Fitted a model with MAP estimate = -1103.3085
expansions: [(3, 1)]
discards: [  0  26  85 139 142 190 191 195]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1116.0680 - loglik: -1.1137e+03 - logprior: -2.3969e+00
Epoch 2/2
39/39 - 17s - loss: 1111.0641 - loglik: -1.1104e+03 - logprior: -6.5631e-01
Fitted a model with MAP estimate = -1103.3320
expansions: [(3, 1)]
discards: [  0 181]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1109.2252 - loglik: -1.1069e+03 - logprior: -2.3390e+00
Epoch 2/10
39/39 - 17s - loss: 1105.7975 - loglik: -1.1047e+03 - logprior: -1.0638e+00
Epoch 3/10
39/39 - 17s - loss: 1102.6254 - loglik: -1.1024e+03 - logprior: -2.3996e-01
Epoch 4/10
39/39 - 18s - loss: 1102.1415 - loglik: -1.1019e+03 - logprior: -2.4140e-01
Epoch 5/10
39/39 - 17s - loss: 1100.5809 - loglik: -1.1003e+03 - logprior: -3.2917e-01
Epoch 6/10
39/39 - 17s - loss: 1101.0785 - loglik: -1.1007e+03 - logprior: -4.1210e-01
Fitted a model with MAP estimate = -1100.1807
Time for alignment: 348.7837
Computed alignments with likelihoods: ['-1100.2213', '-1099.8709', '-1098.3515', '-1100.5179', '-1100.1807']
Best model has likelihood: -1098.3515  (prior= -0.7390 )
time for generating output: 0.3684
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.7028591310235228
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff0a8370220>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff0a8370ee0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370d90>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370f10>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370880>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370f70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff0a8370dc0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83703d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8370d60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385250>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385100>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385190>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385520>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83853d0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a83851f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff0a8385370>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff0a8385580> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8f9c24700>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff2c5bae8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9106f75b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff2c59b43a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff0a844c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9503ec8b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918604910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe918604100>] , frozen_insertions : [True, True]
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function reduce_multinomial_emissions at 0x7ff3205f2430>
 , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff1bddaeca0>, <function make_default_emission_matrix at 0x7ff1bddaeca0>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff2c5dc08e0>, <__main__.SimpleDirichletPrior object at 0x7fe89a14e850>] , kernel_dim : [25, 2]
 , num_rate_matrices : 2 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 655.6762 - loglik: -6.5220e+02 - logprior: -3.4793e+00
Epoch 2/10
19/19 - 3s - loss: 610.7225 - loglik: -6.0932e+02 - logprior: -1.3999e+00
Epoch 3/10
19/19 - 3s - loss: 586.4493 - loglik: -5.8473e+02 - logprior: -1.7222e+00
Epoch 4/10
19/19 - 3s - loss: 579.6644 - loglik: -5.7796e+02 - logprior: -1.7004e+00
Epoch 5/10
19/19 - 3s - loss: 578.5995 - loglik: -5.7691e+02 - logprior: -1.6875e+00
Epoch 6/10
19/19 - 3s - loss: 577.2275 - loglik: -5.7556e+02 - logprior: -1.6649e+00
Epoch 7/10
19/19 - 3s - loss: 577.2747 - loglik: -5.7559e+02 - logprior: -1.6849e+00
Fitted a model with MAP estimate = -576.2760
expansions: [(17, 1), (18, 2), (19, 2), (20, 4), (22, 1), (24, 1), (28, 1), (32, 1), (34, 1), (40, 1), (43, 1), (47, 1), (49, 1), (56, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 581.9661 - loglik: -5.7790e+02 - logprior: -4.0671e+00
Epoch 2/2
19/19 - 4s - loss: 572.0944 - loglik: -5.7020e+02 - logprior: -1.8952e+00
Fitted a model with MAP estimate = -569.7812
expansions: [(0, 2)]
discards: [ 0 15 16 19 24 25 89]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 572.7880 - loglik: -5.6989e+02 - logprior: -2.8946e+00
Epoch 2/2
19/19 - 4s - loss: 569.3436 - loglik: -5.6829e+02 - logprior: -1.0549e+00
Fitted a model with MAP estimate = -568.0932
expansions: [(16, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 574.6098 - loglik: -5.7088e+02 - logprior: -3.7336e+00
Epoch 2/10
19/19 - 4s - loss: 569.5093 - loglik: -5.6830e+02 - logprior: -1.2072e+00
Epoch 3/10
19/19 - 4s - loss: 567.4290 - loglik: -5.6644e+02 - logprior: -9.8910e-01
Epoch 4/10
19/19 - 4s - loss: 566.6388 - loglik: -5.6566e+02 - logprior: -9.8095e-01
Epoch 5/10
19/19 - 4s - loss: 564.9844 - loglik: -5.6402e+02 - logprior: -9.6342e-01
Epoch 6/10
19/19 - 4s - loss: 565.5292 - loglik: -5.6455e+02 - logprior: -9.7498e-01
Fitted a model with MAP estimate = -564.7531
Time for alignment: 99.7382
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 655.6946 - loglik: -6.5222e+02 - logprior: -3.4767e+00
Epoch 2/10
19/19 - 3s - loss: 610.5659 - loglik: -6.0918e+02 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 3s - loss: 587.3812 - loglik: -5.8567e+02 - logprior: -1.7148e+00
Epoch 4/10
19/19 - 3s - loss: 580.3218 - loglik: -5.7861e+02 - logprior: -1.7142e+00
Epoch 5/10
19/19 - 3s - loss: 578.4258 - loglik: -5.7671e+02 - logprior: -1.7128e+00
Epoch 6/10
19/19 - 3s - loss: 577.1047 - loglik: -5.7541e+02 - logprior: -1.6933e+00
Epoch 7/10
19/19 - 3s - loss: 576.6453 - loglik: -5.7494e+02 - logprior: -1.7020e+00
Epoch 8/10
19/19 - 3s - loss: 576.5350 - loglik: -5.7483e+02 - logprior: -1.7068e+00
Epoch 9/10
19/19 - 3s - loss: 576.6785 - loglik: -5.7495e+02 - logprior: -1.7237e+00
Fitted a model with MAP estimate = -576.0571
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 1), (23, 1), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (47, 1), (49, 1), (55, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 583.0317 - loglik: -5.7895e+02 - logprior: -4.0786e+00
Epoch 2/2
19/19 - 4s - loss: 572.4003 - loglik: -5.7059e+02 - logprior: -1.8055e+00
Fitted a model with MAP estimate = -570.5463
expansions: [(0, 2)]
discards: [ 0 15 16 19 23 37 43 86]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 573.3723 - loglik: -5.7046e+02 - logprior: -2.9115e+00
Epoch 2/2
19/19 - 4s - loss: 569.7827 - loglik: -5.6876e+02 - logprior: -1.0224e+00
Fitted a model with MAP estimate = -568.3919
expansions: [(21, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 575.0450 - loglik: -5.7138e+02 - logprior: -3.6693e+00
Epoch 2/10
19/19 - 4s - loss: 570.6641 - loglik: -5.6961e+02 - logprior: -1.0500e+00
Epoch 3/10
19/19 - 4s - loss: 567.8624 - loglik: -5.6714e+02 - logprior: -7.2183e-01
Epoch 4/10
19/19 - 4s - loss: 566.7890 - loglik: -5.6603e+02 - logprior: -7.5898e-01
Epoch 5/10
19/19 - 4s - loss: 566.8828 - loglik: -5.6604e+02 - logprior: -8.4252e-01
Fitted a model with MAP estimate = -565.8081
Time for alignment: 99.1021
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 655.7987 - loglik: -6.5232e+02 - logprior: -3.4770e+00
Epoch 2/10
19/19 - 3s - loss: 610.0376 - loglik: -6.0864e+02 - logprior: -1.3984e+00
Epoch 3/10
19/19 - 3s - loss: 586.7128 - loglik: -5.8497e+02 - logprior: -1.7421e+00
Epoch 4/10
19/19 - 3s - loss: 580.7287 - loglik: -5.7901e+02 - logprior: -1.7157e+00
Epoch 5/10
19/19 - 3s - loss: 578.3306 - loglik: -5.7664e+02 - logprior: -1.6890e+00
Epoch 6/10
19/19 - 3s - loss: 577.7791 - loglik: -5.7610e+02 - logprior: -1.6804e+00
Epoch 7/10
19/19 - 3s - loss: 577.9461 - loglik: -5.7626e+02 - logprior: -1.6857e+00
Fitted a model with MAP estimate = -576.9823
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 2), (23, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 583.0519 - loglik: -5.7896e+02 - logprior: -4.0872e+00
Epoch 2/2
19/19 - 4s - loss: 572.3769 - loglik: -5.7043e+02 - logprior: -1.9431e+00
Fitted a model with MAP estimate = -570.0895
expansions: [(0, 2)]
discards: [ 0 15 16 29 38 90]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 572.5627 - loglik: -5.6965e+02 - logprior: -2.9135e+00
Epoch 2/2
19/19 - 4s - loss: 568.7382 - loglik: -5.6768e+02 - logprior: -1.0612e+00
Fitted a model with MAP estimate = -567.6063
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 574.2889 - loglik: -5.7061e+02 - logprior: -3.6770e+00
Epoch 2/10
19/19 - 4s - loss: 569.4092 - loglik: -5.6824e+02 - logprior: -1.1676e+00
Epoch 3/10
19/19 - 4s - loss: 567.0361 - loglik: -5.6604e+02 - logprior: -9.9150e-01
Epoch 4/10
19/19 - 4s - loss: 566.6237 - loglik: -5.6565e+02 - logprior: -9.7449e-01
Epoch 5/10
19/19 - 4s - loss: 565.4956 - loglik: -5.6454e+02 - logprior: -9.5154e-01
Epoch 6/10
19/19 - 4s - loss: 565.3339 - loglik: -5.6437e+02 - logprior: -9.6177e-01
Epoch 7/10
19/19 - 4s - loss: 564.6782 - loglik: -5.6370e+02 - logprior: -9.8077e-01
Epoch 8/10
19/19 - 4s - loss: 564.5063 - loglik: -5.6353e+02 - logprior: -9.7957e-01
Epoch 9/10
19/19 - 4s - loss: 564.2784 - loglik: -5.6329e+02 - logprior: -9.8556e-01
Epoch 10/10
19/19 - 4s - loss: 564.6136 - loglik: -5.6363e+02 - logprior: -9.8655e-01
Fitted a model with MAP estimate = -564.0993
Time for alignment: 110.1989
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 655.8845 - loglik: -6.5240e+02 - logprior: -3.4812e+00
Epoch 2/10
19/19 - 3s - loss: 612.0142 - loglik: -6.1062e+02 - logprior: -1.3962e+00
Epoch 3/10
19/19 - 3s - loss: 589.0672 - loglik: -5.8736e+02 - logprior: -1.7064e+00
Epoch 4/10
19/19 - 3s - loss: 583.1187 - loglik: -5.8145e+02 - logprior: -1.6663e+00
Epoch 5/10
19/19 - 3s - loss: 580.4579 - loglik: -5.7882e+02 - logprior: -1.6365e+00
Epoch 6/10
19/19 - 3s - loss: 580.0008 - loglik: -5.7838e+02 - logprior: -1.6242e+00
Epoch 7/10
19/19 - 3s - loss: 579.5155 - loglik: -5.7788e+02 - logprior: -1.6390e+00
Epoch 8/10
19/19 - 3s - loss: 578.9199 - loglik: -5.7728e+02 - logprior: -1.6430e+00
Epoch 9/10
19/19 - 3s - loss: 579.4172 - loglik: -5.7775e+02 - logprior: -1.6624e+00
Fitted a model with MAP estimate = -578.5498
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 2), (23, 1), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (50, 1), (56, 2), (62, 1), (64, 1), (65, 2), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 583.8815 - loglik: -5.7974e+02 - logprior: -4.1378e+00
Epoch 2/2
19/19 - 4s - loss: 573.0911 - loglik: -5.7108e+02 - logprior: -2.0124e+00
Fitted a model with MAP estimate = -570.7763
expansions: [(0, 2)]
discards: [ 0 15 16 19 29 38 44 87]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 573.4573 - loglik: -5.7053e+02 - logprior: -2.9291e+00
Epoch 2/2
19/19 - 4s - loss: 569.6694 - loglik: -5.6860e+02 - logprior: -1.0739e+00
Fitted a model with MAP estimate = -568.4952
expansions: []
discards: [ 0 21]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 575.2567 - loglik: -5.7158e+02 - logprior: -3.6764e+00
Epoch 2/10
19/19 - 4s - loss: 570.8115 - loglik: -5.6964e+02 - logprior: -1.1737e+00
Epoch 3/10
19/19 - 4s - loss: 568.4153 - loglik: -5.6741e+02 - logprior: -1.0092e+00
Epoch 4/10
19/19 - 4s - loss: 566.8371 - loglik: -5.6586e+02 - logprior: -9.7778e-01
Epoch 5/10
19/19 - 4s - loss: 566.3109 - loglik: -5.6534e+02 - logprior: -9.7095e-01
Epoch 6/10
19/19 - 4s - loss: 565.8823 - loglik: -5.6491e+02 - logprior: -9.7371e-01
Epoch 7/10
19/19 - 4s - loss: 565.4817 - loglik: -5.6449e+02 - logprior: -9.9276e-01
Epoch 8/10
19/19 - 4s - loss: 565.4055 - loglik: -5.6442e+02 - logprior: -9.8844e-01
Epoch 9/10
19/19 - 4s - loss: 564.6906 - loglik: -5.6369e+02 - logprior: -9.9816e-01
Epoch 10/10
19/19 - 4s - loss: 564.6243 - loglik: -5.6362e+02 - logprior: -1.0006e+00
Fitted a model with MAP estimate = -564.6814
Time for alignment: 117.0633
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 655.6929 - loglik: -6.5222e+02 - logprior: -3.4763e+00
Epoch 2/10
19/19 - 3s - loss: 611.8062 - loglik: -6.1042e+02 - logprior: -1.3813e+00
Epoch 3/10
19/19 - 3s - loss: 587.4269 - loglik: -5.8570e+02 - logprior: -1.7305e+00
Epoch 4/10
19/19 - 3s - loss: 581.5902 - loglik: -5.7990e+02 - logprior: -1.6918e+00
Epoch 5/10
19/19 - 3s - loss: 579.8468 - loglik: -5.7817e+02 - logprior: -1.6794e+00
Epoch 6/10
19/19 - 3s - loss: 578.9437 - loglik: -5.7729e+02 - logprior: -1.6504e+00
Epoch 7/10
19/19 - 3s - loss: 578.6256 - loglik: -5.7696e+02 - logprior: -1.6670e+00
Epoch 8/10
19/19 - 3s - loss: 578.0471 - loglik: -5.7637e+02 - logprior: -1.6755e+00
Epoch 9/10
19/19 - 3s - loss: 578.8821 - loglik: -5.7720e+02 - logprior: -1.6827e+00
Fitted a model with MAP estimate = -577.7649
expansions: [(17, 1), (18, 3), (19, 4), (21, 2), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 2), (71, 1), (76, 2), (82, 1), (85, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 583.6511 - loglik: -5.7953e+02 - logprior: -4.1246e+00
Epoch 2/2
19/19 - 4s - loss: 572.2532 - loglik: -5.7030e+02 - logprior: -1.9521e+00
Fitted a model with MAP estimate = -570.3994
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 28 38 44 91]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 573.3348 - loglik: -5.7042e+02 - logprior: -2.9106e+00
Epoch 2/2
19/19 - 4s - loss: 570.0695 - loglik: -5.6902e+02 - logprior: -1.0464e+00
Fitted a model with MAP estimate = -568.6635
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 575.2674 - loglik: -5.7159e+02 - logprior: -3.6806e+00
Epoch 2/10
19/19 - 4s - loss: 570.8076 - loglik: -5.6967e+02 - logprior: -1.1360e+00
Epoch 3/10
19/19 - 4s - loss: 568.7414 - loglik: -5.6784e+02 - logprior: -9.0199e-01
Epoch 4/10
19/19 - 4s - loss: 567.1939 - loglik: -5.6631e+02 - logprior: -8.8337e-01
Epoch 5/10
19/19 - 4s - loss: 566.7451 - loglik: -5.6588e+02 - logprior: -8.6458e-01
Epoch 6/10
19/19 - 4s - loss: 566.7868 - loglik: -5.6591e+02 - logprior: -8.7933e-01
Fitted a model with MAP estimate = -566.0420
Time for alignment: 101.1198
Computed alignments with likelihoods: ['-564.7531', '-565.8081', '-564.0993', '-564.6814', '-566.0420']
Best model has likelihood: -564.0993  (prior= -1.0031 )
time for generating output: 0.1699
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8554767533490938
