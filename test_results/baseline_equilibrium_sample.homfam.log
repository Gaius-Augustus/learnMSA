Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1130.9148 - loglik: -1.1261e+03 - logprior: -4.8648e+00
Epoch 2/10
25/25 - 6s - loss: 1016.8287 - loglik: -1.0151e+03 - logprior: -1.7725e+00
Epoch 3/10
25/25 - 6s - loss: 999.1144 - loglik: -9.9717e+02 - logprior: -1.9436e+00
Epoch 4/10
25/25 - 6s - loss: 997.5325 - loglik: -9.9571e+02 - logprior: -1.8211e+00
Epoch 5/10
25/25 - 6s - loss: 995.8016 - loglik: -9.9399e+02 - logprior: -1.8073e+00
Epoch 6/10
25/25 - 6s - loss: 991.3544 - loglik: -9.8955e+02 - logprior: -1.7999e+00
Epoch 7/10
25/25 - 6s - loss: 995.6584 - loglik: -9.9386e+02 - logprior: -1.7974e+00
Fitted a model with MAP estimate = -993.8101
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (47, 3), (58, 1), (60, 1), (62, 2), (74, 1), (75, 1), (80, 1), (82, 1), (83, 2), (85, 1), (89, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (163, 1), (167, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 992.5930 - loglik: -9.8579e+02 - logprior: -6.7990e+00
Epoch 2/2
25/25 - 8s - loss: 979.5721 - loglik: -9.7712e+02 - logprior: -2.4556e+00
Fitted a model with MAP estimate = -975.9649
expansions: [(0, 3)]
discards: [  0   9  44  61  62  81 107 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 984.4009 - loglik: -9.8014e+02 - logprior: -4.2568e+00
Epoch 2/2
25/25 - 7s - loss: 977.1258 - loglik: -9.7676e+02 - logprior: -3.6390e-01
Fitted a model with MAP estimate = -975.3256
expansions: [(60, 2)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 983.6568 - loglik: -9.7742e+02 - logprior: -6.2353e+00
Epoch 2/10
25/25 - 7s - loss: 980.2813 - loglik: -9.7907e+02 - logprior: -1.2132e+00
Epoch 3/10
25/25 - 7s - loss: 972.5320 - loglik: -9.7302e+02 - logprior: 0.4844
Epoch 4/10
25/25 - 7s - loss: 974.4412 - loglik: -9.7500e+02 - logprior: 0.5591
Fitted a model with MAP estimate = -971.7467
Time for alignment: 138.7640
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 1127.3057 - loglik: -1.1224e+03 - logprior: -4.8675e+00
Epoch 2/10
25/25 - 6s - loss: 1021.0704 - loglik: -1.0195e+03 - logprior: -1.6154e+00
Epoch 3/10
25/25 - 6s - loss: 1001.6861 - loglik: -9.9998e+02 - logprior: -1.7065e+00
Epoch 4/10
25/25 - 6s - loss: 995.9402 - loglik: -9.9434e+02 - logprior: -1.6051e+00
Epoch 5/10
25/25 - 6s - loss: 995.4888 - loglik: -9.9388e+02 - logprior: -1.6058e+00
Epoch 6/10
25/25 - 6s - loss: 996.6978 - loglik: -9.9509e+02 - logprior: -1.6040e+00
Fitted a model with MAP estimate = -995.4570
expansions: [(9, 4), (11, 1), (31, 1), (32, 2), (33, 2), (35, 2), (48, 3), (58, 2), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (91, 2), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 2), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 993.1218 - loglik: -9.8635e+02 - logprior: -6.7697e+00
Epoch 2/2
25/25 - 8s - loss: 978.5266 - loglik: -9.7617e+02 - logprior: -2.3541e+00
Fitted a model with MAP estimate = -976.0175
expansions: [(0, 3)]
discards: [  0  41  45  61  62  81 143 156 158 206]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 982.4142 - loglik: -9.7814e+02 - logprior: -4.2755e+00
Epoch 2/2
25/25 - 7s - loss: 979.2316 - loglik: -9.7884e+02 - logprior: -3.9312e-01
Fitted a model with MAP estimate = -975.2682
expansions: [(60, 2)]
discards: [  0   2 104]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 985.5078 - loglik: -9.7923e+02 - logprior: -6.2739e+00
Epoch 2/10
25/25 - 7s - loss: 978.2033 - loglik: -9.7695e+02 - logprior: -1.2542e+00
Epoch 3/10
25/25 - 7s - loss: 975.8458 - loglik: -9.7628e+02 - logprior: 0.4348
Epoch 4/10
25/25 - 7s - loss: 973.1688 - loglik: -9.7369e+02 - logprior: 0.5234
Epoch 5/10
25/25 - 7s - loss: 968.7518 - loglik: -9.6936e+02 - logprior: 0.6102
Epoch 6/10
25/25 - 7s - loss: 973.1582 - loglik: -9.7389e+02 - logprior: 0.7286
Fitted a model with MAP estimate = -970.9950
Time for alignment: 147.1823
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1129.0066 - loglik: -1.1241e+03 - logprior: -4.8899e+00
Epoch 2/10
25/25 - 6s - loss: 1019.8466 - loglik: -1.0180e+03 - logprior: -1.8025e+00
Epoch 3/10
25/25 - 6s - loss: 996.4561 - loglik: -9.9441e+02 - logprior: -2.0491e+00
Epoch 4/10
25/25 - 6s - loss: 998.0156 - loglik: -9.9609e+02 - logprior: -1.9275e+00
Fitted a model with MAP estimate = -994.4959
expansions: [(9, 3), (10, 1), (11, 1), (13, 1), (31, 1), (32, 2), (33, 2), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 1), (88, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (163, 1), (167, 1), (168, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 994.6353 - loglik: -9.8785e+02 - logprior: -6.7887e+00
Epoch 2/2
25/25 - 8s - loss: 977.6469 - loglik: -9.7541e+02 - logprior: -2.2411e+00
Fitted a model with MAP estimate = -975.6666
expansions: [(0, 2)]
discards: [  0   9  42  62  82 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 983.2645 - loglik: -9.7895e+02 - logprior: -4.3134e+00
Epoch 2/2
25/25 - 7s - loss: 975.7014 - loglik: -9.7529e+02 - logprior: -4.1407e-01
Fitted a model with MAP estimate = -974.6873
expansions: []
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 985.0867 - loglik: -9.7882e+02 - logprior: -6.2687e+00
Epoch 2/10
25/25 - 7s - loss: 978.8906 - loglik: -9.7783e+02 - logprior: -1.0605e+00
Epoch 3/10
25/25 - 7s - loss: 975.6671 - loglik: -9.7608e+02 - logprior: 0.4083
Epoch 4/10
25/25 - 7s - loss: 973.5776 - loglik: -9.7406e+02 - logprior: 0.4842
Epoch 5/10
25/25 - 7s - loss: 972.0542 - loglik: -9.7264e+02 - logprior: 0.5855
Epoch 6/10
25/25 - 7s - loss: 969.7137 - loglik: -9.7041e+02 - logprior: 0.6941
Epoch 7/10
25/25 - 7s - loss: 970.2012 - loglik: -9.7099e+02 - logprior: 0.7842
Fitted a model with MAP estimate = -971.0548
Time for alignment: 143.1431
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1131.1448 - loglik: -1.1263e+03 - logprior: -4.8473e+00
Epoch 2/10
25/25 - 6s - loss: 1017.6259 - loglik: -1.0159e+03 - logprior: -1.7303e+00
Epoch 3/10
25/25 - 6s - loss: 999.0750 - loglik: -9.9718e+02 - logprior: -1.8965e+00
Epoch 4/10
25/25 - 6s - loss: 996.2426 - loglik: -9.9447e+02 - logprior: -1.7678e+00
Epoch 5/10
25/25 - 6s - loss: 992.5240 - loglik: -9.9078e+02 - logprior: -1.7407e+00
Epoch 6/10
25/25 - 6s - loss: 996.2291 - loglik: -9.9450e+02 - logprior: -1.7260e+00
Fitted a model with MAP estimate = -994.0395
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (122, 2), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (163, 1), (167, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 994.8373 - loglik: -9.8802e+02 - logprior: -6.8141e+00
Epoch 2/2
25/25 - 8s - loss: 977.1531 - loglik: -9.7477e+02 - logprior: -2.3858e+00
Fitted a model with MAP estimate = -976.0849
expansions: [(0, 3)]
discards: [  0   9  42  62  63  82 144 157 179]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 983.9958 - loglik: -9.7968e+02 - logprior: -4.3127e+00
Epoch 2/2
25/25 - 7s - loss: 976.9222 - loglik: -9.7649e+02 - logprior: -4.3224e-01
Fitted a model with MAP estimate = -975.1101
expansions: [(61, 2)]
discards: [  0   2  46 105]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 986.4199 - loglik: -9.8013e+02 - logprior: -6.2929e+00
Epoch 2/10
25/25 - 7s - loss: 976.5582 - loglik: -9.7532e+02 - logprior: -1.2406e+00
Epoch 3/10
25/25 - 7s - loss: 975.6289 - loglik: -9.7607e+02 - logprior: 0.4408
Epoch 4/10
25/25 - 7s - loss: 972.2760 - loglik: -9.7281e+02 - logprior: 0.5361
Epoch 5/10
25/25 - 7s - loss: 972.1786 - loglik: -9.7282e+02 - logprior: 0.6410
Epoch 6/10
25/25 - 7s - loss: 968.9543 - loglik: -9.6970e+02 - logprior: 0.7456
Epoch 7/10
25/25 - 7s - loss: 972.9279 - loglik: -9.7378e+02 - logprior: 0.8525
Fitted a model with MAP estimate = -970.8011
Time for alignment: 155.1675
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1124.5820 - loglik: -1.1197e+03 - logprior: -4.8708e+00
Epoch 2/10
25/25 - 6s - loss: 1022.1625 - loglik: -1.0205e+03 - logprior: -1.6607e+00
Epoch 3/10
25/25 - 6s - loss: 1001.8231 - loglik: -1.0001e+03 - logprior: -1.7277e+00
Epoch 4/10
25/25 - 6s - loss: 996.4296 - loglik: -9.9480e+02 - logprior: -1.6288e+00
Epoch 5/10
25/25 - 6s - loss: 997.0623 - loglik: -9.9547e+02 - logprior: -1.5942e+00
Fitted a model with MAP estimate = -996.0342
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (96, 1), (102, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 993.1346 - loglik: -9.8624e+02 - logprior: -6.8980e+00
Epoch 2/2
25/25 - 8s - loss: 980.6945 - loglik: -9.7820e+02 - logprior: -2.4950e+00
Fitted a model with MAP estimate = -976.0671
expansions: [(0, 3), (215, 1)]
discards: [  0   9  42  46  62  82 105 108 178 207]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 982.2006 - loglik: -9.7787e+02 - logprior: -4.3338e+00
Epoch 2/2
25/25 - 7s - loss: 978.5641 - loglik: -9.7815e+02 - logprior: -4.1884e-01
Fitted a model with MAP estimate = -974.8765
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 986.5300 - loglik: -9.8024e+02 - logprior: -6.2901e+00
Epoch 2/10
25/25 - 7s - loss: 978.0938 - loglik: -9.7687e+02 - logprior: -1.2230e+00
Epoch 3/10
25/25 - 7s - loss: 973.7419 - loglik: -9.7417e+02 - logprior: 0.4276
Epoch 4/10
25/25 - 7s - loss: 973.5257 - loglik: -9.7403e+02 - logprior: 0.5041
Epoch 5/10
25/25 - 7s - loss: 972.3867 - loglik: -9.7299e+02 - logprior: 0.5997
Epoch 6/10
25/25 - 7s - loss: 971.4601 - loglik: -9.7216e+02 - logprior: 0.6987
Epoch 7/10
25/25 - 7s - loss: 971.9068 - loglik: -9.7271e+02 - logprior: 0.8057
Fitted a model with MAP estimate = -971.0104
Time for alignment: 149.4960
Computed alignments with likelihoods: ['-971.7467', '-970.9950', '-971.0548', '-970.8011', '-971.0104']
Best model has likelihood: -970.8011  (prior= 0.8937 )
time for generating output: 0.2380
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9320352681851579
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 805.8702 - loglik: -7.8577e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 1s - loss: 741.1541 - loglik: -7.3654e+02 - logprior: -4.6136e+00
Epoch 3/10
10/10 - 1s - loss: 691.0353 - loglik: -6.8854e+02 - logprior: -2.4981e+00
Epoch 4/10
10/10 - 1s - loss: 657.5598 - loglik: -6.5536e+02 - logprior: -2.1977e+00
Epoch 5/10
10/10 - 1s - loss: 643.9592 - loglik: -6.4209e+02 - logprior: -1.8675e+00
Epoch 6/10
10/10 - 1s - loss: 639.6918 - loglik: -6.3810e+02 - logprior: -1.5960e+00
Epoch 7/10
10/10 - 1s - loss: 636.0165 - loglik: -6.3459e+02 - logprior: -1.4309e+00
Epoch 8/10
10/10 - 1s - loss: 635.3812 - loglik: -6.3415e+02 - logprior: -1.2338e+00
Epoch 9/10
10/10 - 1s - loss: 634.2366 - loglik: -6.3310e+02 - logprior: -1.1399e+00
Epoch 10/10
10/10 - 1s - loss: 633.8377 - loglik: -6.3276e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -633.8437
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (18, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (76, 1), (77, 2), (78, 3), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 658.0751 - loglik: -6.3162e+02 - logprior: -2.6453e+01
Epoch 2/2
10/10 - 2s - loss: 619.0226 - loglik: -6.1155e+02 - logprior: -7.4678e+00
Fitted a model with MAP estimate = -612.7257
expansions: []
discards: [ 62  95 122 142]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.7501 - loglik: -6.0825e+02 - logprior: -1.8497e+01
Epoch 2/2
10/10 - 2s - loss: 609.6157 - loglik: -6.0571e+02 - logprior: -3.9051e+00
Fitted a model with MAP estimate = -607.2528
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.1128 - loglik: -6.0576e+02 - logprior: -1.7355e+01
Epoch 2/10
10/10 - 2s - loss: 609.5521 - loglik: -6.0602e+02 - logprior: -3.5292e+00
Epoch 3/10
10/10 - 2s - loss: 605.6289 - loglik: -6.0474e+02 - logprior: -8.8961e-01
Epoch 4/10
10/10 - 2s - loss: 604.3419 - loglik: -6.0435e+02 - logprior: 0.0075
Epoch 5/10
10/10 - 2s - loss: 603.0629 - loglik: -6.0351e+02 - logprior: 0.4427
Epoch 6/10
10/10 - 2s - loss: 603.4266 - loglik: -6.0419e+02 - logprior: 0.7672
Fitted a model with MAP estimate = -602.5734
Time for alignment: 50.5306
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 806.4980 - loglik: -7.8640e+02 - logprior: -2.0103e+01
Epoch 2/10
10/10 - 1s - loss: 740.5477 - loglik: -7.3594e+02 - logprior: -4.6120e+00
Epoch 3/10
10/10 - 1s - loss: 689.5321 - loglik: -6.8708e+02 - logprior: -2.4556e+00
Epoch 4/10
10/10 - 1s - loss: 658.9546 - loglik: -6.5702e+02 - logprior: -1.9345e+00
Epoch 5/10
10/10 - 1s - loss: 646.7127 - loglik: -6.4519e+02 - logprior: -1.5207e+00
Epoch 6/10
10/10 - 1s - loss: 640.3878 - loglik: -6.3906e+02 - logprior: -1.3242e+00
Epoch 7/10
10/10 - 1s - loss: 638.4724 - loglik: -6.3731e+02 - logprior: -1.1631e+00
Epoch 8/10
10/10 - 1s - loss: 636.7002 - loglik: -6.3574e+02 - logprior: -9.5911e-01
Epoch 9/10
10/10 - 1s - loss: 635.9266 - loglik: -6.3507e+02 - logprior: -8.5252e-01
Epoch 10/10
10/10 - 1s - loss: 636.0550 - loglik: -6.3528e+02 - logprior: -7.7093e-01
Fitted a model with MAP estimate = -635.5090
expansions: [(0, 4), (13, 1), (14, 2), (15, 2), (26, 1), (28, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (73, 1), (76, 1), (77, 4), (78, 2), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 657.9703 - loglik: -6.3151e+02 - logprior: -2.6465e+01
Epoch 2/2
10/10 - 2s - loss: 620.8979 - loglik: -6.1337e+02 - logprior: -7.5253e+00
Fitted a model with MAP estimate = -612.8074
expansions: []
discards: [ 19  63 101 123 143]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 627.1436 - loglik: -6.0864e+02 - logprior: -1.8508e+01
Epoch 2/2
10/10 - 2s - loss: 609.9864 - loglik: -6.0604e+02 - logprior: -3.9439e+00
Fitted a model with MAP estimate = -607.3296
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.6767 - loglik: -6.0630e+02 - logprior: -1.7379e+01
Epoch 2/10
10/10 - 2s - loss: 609.2314 - loglik: -6.0567e+02 - logprior: -3.5576e+00
Epoch 3/10
10/10 - 2s - loss: 606.0658 - loglik: -6.0515e+02 - logprior: -9.1945e-01
Epoch 4/10
10/10 - 2s - loss: 604.2466 - loglik: -6.0422e+02 - logprior: -2.3477e-02
Epoch 5/10
10/10 - 2s - loss: 603.4343 - loglik: -6.0385e+02 - logprior: 0.4185
Epoch 6/10
10/10 - 2s - loss: 602.5978 - loglik: -6.0334e+02 - logprior: 0.7439
Epoch 7/10
10/10 - 2s - loss: 602.5228 - loglik: -6.0354e+02 - logprior: 1.0184
Epoch 8/10
10/10 - 2s - loss: 601.9967 - loglik: -6.0319e+02 - logprior: 1.1974
Epoch 9/10
10/10 - 2s - loss: 602.2345 - loglik: -6.0353e+02 - logprior: 1.2945
Fitted a model with MAP estimate = -601.8841
Time for alignment: 55.1707
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 806.0599 - loglik: -7.8596e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 740.7221 - loglik: -7.3612e+02 - logprior: -4.6055e+00
Epoch 3/10
10/10 - 1s - loss: 686.4476 - loglik: -6.8397e+02 - logprior: -2.4728e+00
Epoch 4/10
10/10 - 1s - loss: 656.8060 - loglik: -6.5467e+02 - logprior: -2.1388e+00
Epoch 5/10
10/10 - 1s - loss: 642.4733 - loglik: -6.4067e+02 - logprior: -1.7998e+00
Epoch 6/10
10/10 - 1s - loss: 638.9427 - loglik: -6.3738e+02 - logprior: -1.5620e+00
Epoch 7/10
10/10 - 1s - loss: 635.5375 - loglik: -6.3420e+02 - logprior: -1.3370e+00
Epoch 8/10
10/10 - 1s - loss: 634.8423 - loglik: -6.3374e+02 - logprior: -1.1028e+00
Epoch 9/10
10/10 - 1s - loss: 633.7095 - loglik: -6.3273e+02 - logprior: -9.7936e-01
Epoch 10/10
10/10 - 1s - loss: 633.6537 - loglik: -6.3275e+02 - logprior: -9.0373e-01
Fitted a model with MAP estimate = -633.6686
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 2), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.6198 - loglik: -6.2918e+02 - logprior: -2.6443e+01
Epoch 2/2
10/10 - 2s - loss: 618.4078 - loglik: -6.1102e+02 - logprior: -7.3830e+00
Fitted a model with MAP estimate = -612.1198
expansions: []
discards: [ 61 121 141]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.8854 - loglik: -6.0841e+02 - logprior: -1.8474e+01
Epoch 2/2
10/10 - 2s - loss: 609.4557 - loglik: -6.0553e+02 - logprior: -3.9272e+00
Fitted a model with MAP estimate = -607.2637
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.3558 - loglik: -6.0599e+02 - logprior: -1.7365e+01
Epoch 2/10
10/10 - 2s - loss: 609.5262 - loglik: -6.0599e+02 - logprior: -3.5373e+00
Epoch 3/10
10/10 - 2s - loss: 605.8965 - loglik: -6.0500e+02 - logprior: -9.0086e-01
Epoch 4/10
10/10 - 2s - loss: 603.2785 - loglik: -6.0328e+02 - logprior: -2.8113e-03
Epoch 5/10
10/10 - 2s - loss: 604.2032 - loglik: -6.0464e+02 - logprior: 0.4366
Fitted a model with MAP estimate = -603.0197
Time for alignment: 47.9780
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 806.3657 - loglik: -7.8627e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 739.9136 - loglik: -7.3530e+02 - logprior: -4.6090e+00
Epoch 3/10
10/10 - 1s - loss: 687.4370 - loglik: -6.8498e+02 - logprior: -2.4531e+00
Epoch 4/10
10/10 - 1s - loss: 655.3185 - loglik: -6.5331e+02 - logprior: -2.0097e+00
Epoch 5/10
10/10 - 1s - loss: 643.3363 - loglik: -6.4173e+02 - logprior: -1.6065e+00
Epoch 6/10
10/10 - 1s - loss: 638.6865 - loglik: -6.3734e+02 - logprior: -1.3509e+00
Epoch 7/10
10/10 - 1s - loss: 635.8514 - loglik: -6.3467e+02 - logprior: -1.1776e+00
Epoch 8/10
10/10 - 1s - loss: 634.7912 - loglik: -6.3381e+02 - logprior: -9.8413e-01
Epoch 9/10
10/10 - 1s - loss: 634.1700 - loglik: -6.3329e+02 - logprior: -8.7942e-01
Epoch 10/10
10/10 - 1s - loss: 633.7146 - loglik: -6.3291e+02 - logprior: -8.0076e-01
Fitted a model with MAP estimate = -633.8977
expansions: [(0, 4), (13, 3), (15, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (114, 3), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.2988 - loglik: -6.2888e+02 - logprior: -2.6420e+01
Epoch 2/2
10/10 - 2s - loss: 618.9518 - loglik: -6.1152e+02 - logprior: -7.4278e+00
Fitted a model with MAP estimate = -612.4074
expansions: []
discards: [ 59  62 122 142]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.3309 - loglik: -6.0785e+02 - logprior: -1.8480e+01
Epoch 2/2
10/10 - 2s - loss: 610.1213 - loglik: -6.0620e+02 - logprior: -3.9207e+00
Fitted a model with MAP estimate = -607.2797
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.5186 - loglik: -6.0616e+02 - logprior: -1.7356e+01
Epoch 2/10
10/10 - 2s - loss: 609.1749 - loglik: -6.0565e+02 - logprior: -3.5270e+00
Epoch 3/10
10/10 - 2s - loss: 605.9363 - loglik: -6.0504e+02 - logprior: -8.9692e-01
Epoch 4/10
10/10 - 2s - loss: 604.4125 - loglik: -6.0442e+02 - logprior: 0.0033
Epoch 5/10
10/10 - 2s - loss: 603.5215 - loglik: -6.0396e+02 - logprior: 0.4408
Epoch 6/10
10/10 - 2s - loss: 603.1934 - loglik: -6.0396e+02 - logprior: 0.7681
Epoch 7/10
10/10 - 2s - loss: 601.7714 - loglik: -6.0281e+02 - logprior: 1.0380
Epoch 8/10
10/10 - 2s - loss: 602.6368 - loglik: -6.0386e+02 - logprior: 1.2184
Fitted a model with MAP estimate = -602.0416
Time for alignment: 53.3604
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 805.4330 - loglik: -7.8533e+02 - logprior: -2.0099e+01
Epoch 2/10
10/10 - 1s - loss: 740.7788 - loglik: -7.3617e+02 - logprior: -4.6063e+00
Epoch 3/10
10/10 - 1s - loss: 689.2617 - loglik: -6.8680e+02 - logprior: -2.4580e+00
Epoch 4/10
10/10 - 1s - loss: 657.9500 - loglik: -6.5587e+02 - logprior: -2.0769e+00
Epoch 5/10
10/10 - 1s - loss: 647.1690 - loglik: -6.4558e+02 - logprior: -1.5874e+00
Epoch 6/10
10/10 - 1s - loss: 641.6844 - loglik: -6.4043e+02 - logprior: -1.2529e+00
Epoch 7/10
10/10 - 1s - loss: 639.7666 - loglik: -6.3856e+02 - logprior: -1.2102e+00
Epoch 8/10
10/10 - 1s - loss: 639.1842 - loglik: -6.3801e+02 - logprior: -1.1700e+00
Epoch 9/10
10/10 - 1s - loss: 637.9474 - loglik: -6.3684e+02 - logprior: -1.1058e+00
Epoch 10/10
10/10 - 1s - loss: 637.6969 - loglik: -6.3661e+02 - logprior: -1.0874e+00
Fitted a model with MAP estimate = -637.8614
expansions: [(0, 4), (13, 3), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.8262 - loglik: -6.2939e+02 - logprior: -2.6435e+01
Epoch 2/2
10/10 - 2s - loss: 618.8764 - loglik: -6.1154e+02 - logprior: -7.3377e+00
Fitted a model with MAP estimate = -611.9813
expansions: []
discards: [ 59  62 122]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.3501 - loglik: -6.0790e+02 - logprior: -1.8452e+01
Epoch 2/2
10/10 - 2s - loss: 610.4824 - loglik: -6.0659e+02 - logprior: -3.8968e+00
Fitted a model with MAP estimate = -607.2586
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.1250 - loglik: -6.0577e+02 - logprior: -1.7360e+01
Epoch 2/10
10/10 - 2s - loss: 609.5433 - loglik: -6.0601e+02 - logprior: -3.5311e+00
Epoch 3/10
10/10 - 2s - loss: 605.8365 - loglik: -6.0494e+02 - logprior: -8.9165e-01
Epoch 4/10
10/10 - 2s - loss: 604.4497 - loglik: -6.0446e+02 - logprior: 0.0126
Epoch 5/10
10/10 - 2s - loss: 603.6210 - loglik: -6.0407e+02 - logprior: 0.4506
Epoch 6/10
10/10 - 2s - loss: 602.0090 - loglik: -6.0279e+02 - logprior: 0.7766
Epoch 7/10
10/10 - 2s - loss: 603.1825 - loglik: -6.0423e+02 - logprior: 1.0483
Fitted a model with MAP estimate = -602.2936
Time for alignment: 51.7037
Computed alignments with likelihoods: ['-602.5734', '-601.8841', '-603.0197', '-602.0416', '-602.2936']
Best model has likelihood: -601.8841  (prior= 1.3375 )
time for generating output: 0.1721
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.903754266211604
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 574.3533 - loglik: -5.0589e+02 - logprior: -6.8468e+01
Epoch 2/10
10/10 - 1s - loss: 493.6310 - loglik: -4.7642e+02 - logprior: -1.7206e+01
Epoch 3/10
10/10 - 1s - loss: 461.0607 - loglik: -4.5372e+02 - logprior: -7.3399e+00
Epoch 4/10
10/10 - 1s - loss: 446.1185 - loglik: -4.4233e+02 - logprior: -3.7904e+00
Epoch 5/10
10/10 - 1s - loss: 439.1472 - loglik: -4.3725e+02 - logprior: -1.8951e+00
Epoch 6/10
10/10 - 1s - loss: 435.0676 - loglik: -4.3421e+02 - logprior: -8.5868e-01
Epoch 7/10
10/10 - 1s - loss: 432.8043 - loglik: -4.3258e+02 - logprior: -2.2027e-01
Epoch 8/10
10/10 - 1s - loss: 431.8191 - loglik: -4.3193e+02 - logprior: 0.1090
Epoch 9/10
10/10 - 1s - loss: 431.2781 - loglik: -4.3163e+02 - logprior: 0.3549
Epoch 10/10
10/10 - 1s - loss: 430.9901 - loglik: -4.3153e+02 - logprior: 0.5420
Fitted a model with MAP estimate = -430.8608
expansions: [(9, 3), (14, 1), (15, 1), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.3268 - loglik: -4.3157e+02 - logprior: -7.6755e+01
Epoch 2/2
10/10 - 2s - loss: 455.4085 - loglik: -4.2472e+02 - logprior: -3.0685e+01
Fitted a model with MAP estimate = -446.2754
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 495.2225 - loglik: -4.2246e+02 - logprior: -7.2762e+01
Epoch 2/10
10/10 - 1s - loss: 441.0330 - loglik: -4.2062e+02 - logprior: -2.0415e+01
Epoch 3/10
10/10 - 2s - loss: 426.7550 - loglik: -4.2006e+02 - logprior: -6.6950e+00
Epoch 4/10
10/10 - 2s - loss: 422.0123 - loglik: -4.2009e+02 - logprior: -1.9237e+00
Epoch 5/10
10/10 - 2s - loss: 419.7659 - loglik: -4.2013e+02 - logprior: 0.3654
Epoch 6/10
10/10 - 1s - loss: 418.4522 - loglik: -4.2009e+02 - logprior: 1.6413
Epoch 7/10
10/10 - 2s - loss: 417.6669 - loglik: -4.2008e+02 - logprior: 2.4107
Epoch 8/10
10/10 - 1s - loss: 417.1786 - loglik: -4.2016e+02 - logprior: 2.9839
Epoch 9/10
10/10 - 1s - loss: 416.8153 - loglik: -4.2028e+02 - logprior: 3.4664
Epoch 10/10
10/10 - 2s - loss: 416.5154 - loglik: -4.2038e+02 - logprior: 3.8695
Fitted a model with MAP estimate = -416.3419
Time for alignment: 43.8305
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 574.3192 - loglik: -5.0585e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 493.5319 - loglik: -4.7632e+02 - logprior: -1.7208e+01
Epoch 3/10
10/10 - 1s - loss: 460.8121 - loglik: -4.5346e+02 - logprior: -7.3505e+00
Epoch 4/10
10/10 - 1s - loss: 447.0703 - loglik: -4.4329e+02 - logprior: -3.7846e+00
Epoch 5/10
10/10 - 1s - loss: 441.3971 - loglik: -4.3948e+02 - logprior: -1.9159e+00
Epoch 6/10
10/10 - 1s - loss: 437.6713 - loglik: -4.3683e+02 - logprior: -8.4158e-01
Epoch 7/10
10/10 - 1s - loss: 434.7762 - loglik: -4.3450e+02 - logprior: -2.7372e-01
Epoch 8/10
10/10 - 1s - loss: 433.1107 - loglik: -4.3310e+02 - logprior: -9.5149e-03
Epoch 9/10
10/10 - 1s - loss: 432.1027 - loglik: -4.3237e+02 - logprior: 0.2643
Epoch 10/10
10/10 - 1s - loss: 431.6552 - loglik: -4.3211e+02 - logprior: 0.4577
Fitted a model with MAP estimate = -431.4407
expansions: [(14, 2), (15, 1), (37, 1), (52, 2), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 510.2154 - loglik: -4.3315e+02 - logprior: -7.7063e+01
Epoch 2/2
10/10 - 1s - loss: 457.5664 - loglik: -4.2640e+02 - logprior: -3.1166e+01
Fitted a model with MAP estimate = -448.7131
expansions: []
discards: [ 0 13 55]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 502.1571 - loglik: -4.2667e+02 - logprior: -7.5486e+01
Epoch 2/2
10/10 - 1s - loss: 450.6618 - loglik: -4.2512e+02 - logprior: -2.5538e+01
Fitted a model with MAP estimate = -439.3171
expansions: [(0, 5), (5, 2), (7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 486.6689 - loglik: -4.2546e+02 - logprior: -6.1214e+01
Epoch 2/10
10/10 - 2s - loss: 436.6357 - loglik: -4.2155e+02 - logprior: -1.5089e+01
Epoch 3/10
10/10 - 2s - loss: 425.9670 - loglik: -4.2048e+02 - logprior: -5.4890e+00
Epoch 4/10
10/10 - 2s - loss: 421.9716 - loglik: -4.2053e+02 - logprior: -1.4436e+00
Epoch 5/10
10/10 - 2s - loss: 419.8487 - loglik: -4.2055e+02 - logprior: 0.7062
Epoch 6/10
10/10 - 2s - loss: 418.6218 - loglik: -4.2061e+02 - logprior: 1.9896
Epoch 7/10
10/10 - 2s - loss: 417.9355 - loglik: -4.2076e+02 - logprior: 2.8259
Epoch 8/10
10/10 - 2s - loss: 417.4766 - loglik: -4.2084e+02 - logprior: 3.3676
Epoch 9/10
10/10 - 2s - loss: 417.1219 - loglik: -4.2088e+02 - logprior: 3.7596
Epoch 10/10
10/10 - 2s - loss: 416.8209 - loglik: -4.2092e+02 - logprior: 4.1028
Fitted a model with MAP estimate = -416.6544
Time for alignment: 49.5824
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 574.3198 - loglik: -5.0585e+02 - logprior: -6.8470e+01
Epoch 2/10
10/10 - 1s - loss: 493.4374 - loglik: -4.7623e+02 - logprior: -1.7211e+01
Epoch 3/10
10/10 - 1s - loss: 461.1969 - loglik: -4.5386e+02 - logprior: -7.3354e+00
Epoch 4/10
10/10 - 1s - loss: 448.4297 - loglik: -4.4475e+02 - logprior: -3.6786e+00
Epoch 5/10
10/10 - 1s - loss: 442.0826 - loglik: -4.4038e+02 - logprior: -1.7047e+00
Epoch 6/10
10/10 - 1s - loss: 438.3031 - loglik: -4.3764e+02 - logprior: -6.6632e-01
Epoch 7/10
10/10 - 1s - loss: 435.9010 - loglik: -4.3579e+02 - logprior: -1.0818e-01
Epoch 8/10
10/10 - 1s - loss: 434.3385 - loglik: -4.3450e+02 - logprior: 0.1665
Epoch 9/10
10/10 - 1s - loss: 433.2608 - loglik: -4.3367e+02 - logprior: 0.4074
Epoch 10/10
10/10 - 1s - loss: 432.6104 - loglik: -4.3317e+02 - logprior: 0.5552
Fitted a model with MAP estimate = -432.3747
expansions: [(14, 2), (26, 2), (27, 3), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.9521 - loglik: -4.3201e+02 - logprior: -7.6940e+01
Epoch 2/2
10/10 - 1s - loss: 456.0694 - loglik: -4.2515e+02 - logprior: -3.0921e+01
Fitted a model with MAP estimate = -447.0745
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 499.9456 - loglik: -4.2449e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 448.3948 - loglik: -4.2251e+02 - logprior: -2.5886e+01
Fitted a model with MAP estimate = -436.9188
expansions: [(0, 4), (5, 2), (7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 483.3256 - loglik: -4.2236e+02 - logprior: -6.0970e+01
Epoch 2/10
10/10 - 2s - loss: 433.7825 - loglik: -4.1890e+02 - logprior: -1.4882e+01
Epoch 3/10
10/10 - 2s - loss: 423.2843 - loglik: -4.1801e+02 - logprior: -5.2764e+00
Epoch 4/10
10/10 - 2s - loss: 419.0666 - loglik: -4.1775e+02 - logprior: -1.3121e+00
Epoch 5/10
10/10 - 2s - loss: 416.8166 - loglik: -4.1772e+02 - logprior: 0.9080
Epoch 6/10
10/10 - 2s - loss: 415.6100 - loglik: -4.1788e+02 - logprior: 2.2747
Epoch 7/10
10/10 - 2s - loss: 414.9049 - loglik: -4.1801e+02 - logprior: 3.1063
Epoch 8/10
10/10 - 2s - loss: 414.4120 - loglik: -4.1807e+02 - logprior: 3.6593
Epoch 9/10
10/10 - 2s - loss: 414.0151 - loglik: -4.1809e+02 - logprior: 4.0765
Epoch 10/10
10/10 - 2s - loss: 413.6881 - loglik: -4.1811e+02 - logprior: 4.4240
Fitted a model with MAP estimate = -413.5069
Time for alignment: 50.3825
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 574.3268 - loglik: -5.0586e+02 - logprior: -6.8470e+01
Epoch 2/10
10/10 - 1s - loss: 493.6539 - loglik: -4.7644e+02 - logprior: -1.7211e+01
Epoch 3/10
10/10 - 1s - loss: 461.2000 - loglik: -4.5383e+02 - logprior: -7.3668e+00
Epoch 4/10
10/10 - 1s - loss: 447.3270 - loglik: -4.4350e+02 - logprior: -3.8263e+00
Epoch 5/10
10/10 - 1s - loss: 441.5590 - loglik: -4.3960e+02 - logprior: -1.9622e+00
Epoch 6/10
10/10 - 1s - loss: 438.8946 - loglik: -4.3801e+02 - logprior: -8.8116e-01
Epoch 7/10
10/10 - 1s - loss: 437.5645 - loglik: -4.3729e+02 - logprior: -2.7142e-01
Epoch 8/10
10/10 - 1s - loss: 436.8651 - loglik: -4.3695e+02 - logprior: 0.0829
Epoch 9/10
10/10 - 1s - loss: 436.4498 - loglik: -4.3680e+02 - logprior: 0.3546
Epoch 10/10
10/10 - 1s - loss: 436.1667 - loglik: -4.3672e+02 - logprior: 0.5496
Fitted a model with MAP estimate = -436.0279
expansions: [(14, 2), (15, 1), (24, 1), (25, 1), (26, 2), (37, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 513.8153 - loglik: -4.3685e+02 - logprior: -7.6969e+01
Epoch 2/2
10/10 - 2s - loss: 458.8954 - loglik: -4.2772e+02 - logprior: -3.1171e+01
Fitted a model with MAP estimate = -449.2531
expansions: [(59, 2)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 502.9933 - loglik: -4.2698e+02 - logprior: -7.6009e+01
Epoch 2/2
10/10 - 2s - loss: 451.6044 - loglik: -4.2357e+02 - logprior: -2.8033e+01
Fitted a model with MAP estimate = -440.7854
expansions: [(5, 2), (7, 2)]
discards: [ 0 57]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 501.2873 - loglik: -4.2460e+02 - logprior: -7.6686e+01
Epoch 2/10
10/10 - 2s - loss: 453.5783 - loglik: -4.2295e+02 - logprior: -3.0627e+01
Epoch 3/10
10/10 - 2s - loss: 443.5756 - loglik: -4.2272e+02 - logprior: -2.0853e+01
Epoch 4/10
10/10 - 2s - loss: 438.3827 - loglik: -4.2256e+02 - logprior: -1.5822e+01
Epoch 5/10
10/10 - 2s - loss: 429.6623 - loglik: -4.2234e+02 - logprior: -7.3247e+00
Epoch 6/10
10/10 - 2s - loss: 421.2115 - loglik: -4.2203e+02 - logprior: 0.8137
Epoch 7/10
10/10 - 2s - loss: 419.1935 - loglik: -4.2181e+02 - logprior: 2.6131
Epoch 8/10
10/10 - 2s - loss: 418.5678 - loglik: -4.2195e+02 - logprior: 3.3840
Epoch 9/10
10/10 - 2s - loss: 418.1174 - loglik: -4.2210e+02 - logprior: 3.9793
Epoch 10/10
10/10 - 2s - loss: 417.7821 - loglik: -4.2217e+02 - logprior: 4.3917
Fitted a model with MAP estimate = -417.5899
Time for alignment: 50.2546
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 574.3320 - loglik: -5.0586e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 1s - loss: 493.8732 - loglik: -4.7666e+02 - logprior: -1.7209e+01
Epoch 3/10
10/10 - 1s - loss: 461.3030 - loglik: -4.5395e+02 - logprior: -7.3539e+00
Epoch 4/10
10/10 - 1s - loss: 447.5150 - loglik: -4.4378e+02 - logprior: -3.7399e+00
Epoch 5/10
10/10 - 1s - loss: 442.0463 - loglik: -4.4026e+02 - logprior: -1.7866e+00
Epoch 6/10
10/10 - 1s - loss: 438.2701 - loglik: -4.3747e+02 - logprior: -8.0112e-01
Epoch 7/10
10/10 - 1s - loss: 434.9730 - loglik: -4.3463e+02 - logprior: -3.4295e-01
Epoch 8/10
10/10 - 1s - loss: 433.1248 - loglik: -4.3308e+02 - logprior: -4.9127e-02
Epoch 9/10
10/10 - 1s - loss: 432.1026 - loglik: -4.3233e+02 - logprior: 0.2321
Epoch 10/10
10/10 - 1s - loss: 431.5854 - loglik: -4.3200e+02 - logprior: 0.4118
Fitted a model with MAP estimate = -431.3174
expansions: [(14, 2), (15, 1), (25, 1), (26, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.8864 - loglik: -4.3201e+02 - logprior: -7.6878e+01
Epoch 2/2
10/10 - 1s - loss: 456.0052 - loglik: -4.2509e+02 - logprior: -3.0911e+01
Fitted a model with MAP estimate = -447.0895
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 500.5110 - loglik: -4.2511e+02 - logprior: -7.5404e+01
Epoch 2/2
10/10 - 1s - loss: 449.3810 - loglik: -4.2354e+02 - logprior: -2.5839e+01
Fitted a model with MAP estimate = -438.1016
expansions: [(0, 4), (5, 2), (7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 484.6456 - loglik: -4.2365e+02 - logprior: -6.0994e+01
Epoch 2/10
10/10 - 2s - loss: 435.1376 - loglik: -4.2025e+02 - logprior: -1.4884e+01
Epoch 3/10
10/10 - 2s - loss: 424.6292 - loglik: -4.1935e+02 - logprior: -5.2832e+00
Epoch 4/10
10/10 - 2s - loss: 420.6256 - loglik: -4.1934e+02 - logprior: -1.2843e+00
Epoch 5/10
10/10 - 2s - loss: 418.5556 - loglik: -4.1950e+02 - logprior: 0.9485
Epoch 6/10
10/10 - 2s - loss: 417.4500 - loglik: -4.1974e+02 - logprior: 2.2863
Epoch 7/10
10/10 - 2s - loss: 416.7436 - loglik: -4.1985e+02 - logprior: 3.1025
Epoch 8/10
10/10 - 2s - loss: 416.2018 - loglik: -4.1983e+02 - logprior: 3.6266
Epoch 9/10
10/10 - 2s - loss: 415.8277 - loglik: -4.1985e+02 - logprior: 4.0230
Epoch 10/10
10/10 - 2s - loss: 415.5330 - loglik: -4.1990e+02 - logprior: 4.3642
Fitted a model with MAP estimate = -415.3650
Time for alignment: 50.4315
Computed alignments with likelihoods: ['-416.3419', '-416.6544', '-413.5069', '-417.5899', '-415.3650']
Best model has likelihood: -413.5069  (prior= 4.6006 )
time for generating output: 0.1884
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.38173999338405556
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1531.5756 - loglik: -1.5237e+03 - logprior: -7.8722e+00
Epoch 2/10
20/20 - 9s - loss: 1452.7546 - loglik: -1.4530e+03 - logprior: 0.2210
Epoch 3/10
20/20 - 9s - loss: 1424.1067 - loglik: -1.4242e+03 - logprior: 0.1154
Epoch 4/10
20/20 - 9s - loss: 1420.3171 - loglik: -1.4207e+03 - logprior: 0.3600
Epoch 5/10
20/20 - 9s - loss: 1412.4828 - loglik: -1.4129e+03 - logprior: 0.4432
Epoch 6/10
20/20 - 9s - loss: 1416.3499 - loglik: -1.4168e+03 - logprior: 0.4817
Fitted a model with MAP estimate = -1415.1283
expansions: [(0, 3), (45, 3), (48, 1), (51, 1), (53, 1), (62, 1), (81, 3), (87, 1), (91, 3), (92, 1), (98, 1), (116, 1), (146, 1), (169, 10), (201, 2), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1433.5454 - loglik: -1.4224e+03 - logprior: -1.1110e+01
Epoch 2/2
20/20 - 11s - loss: 1419.3356 - loglik: -1.4189e+03 - logprior: -4.7155e-01
Fitted a model with MAP estimate = -1410.8823
expansions: [(0, 3), (94, 3), (95, 2), (240, 1)]
discards: [  1   2   3   4   5   6 106 190 191 192 193 194]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 1426.6451 - loglik: -1.4148e+03 - logprior: -1.1832e+01
Epoch 2/2
20/20 - 11s - loss: 1408.6255 - loglik: -1.4082e+03 - logprior: -3.9828e-01
Fitted a model with MAP estimate = -1408.2447
expansions: [(0, 3), (20, 5), (95, 3), (171, 1)]
discards: [  0 191]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 1420.0454 - loglik: -1.4118e+03 - logprior: -8.2012e+00
Epoch 2/10
20/20 - 11s - loss: 1406.6526 - loglik: -1.4073e+03 - logprior: 0.6912
Epoch 3/10
20/20 - 11s - loss: 1405.0983 - loglik: -1.4069e+03 - logprior: 1.8051
Epoch 4/10
20/20 - 11s - loss: 1400.1343 - loglik: -1.4023e+03 - logprior: 2.1970
Epoch 5/10
20/20 - 11s - loss: 1400.1594 - loglik: -1.4026e+03 - logprior: 2.4137
Fitted a model with MAP estimate = -1399.1629
Time for alignment: 195.1835
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1527.3430 - loglik: -1.5195e+03 - logprior: -7.8762e+00
Epoch 2/10
20/20 - 9s - loss: 1458.0531 - loglik: -1.4583e+03 - logprior: 0.2969
Epoch 3/10
20/20 - 9s - loss: 1430.2065 - loglik: -1.4305e+03 - logprior: 0.2519
Epoch 4/10
20/20 - 9s - loss: 1414.2166 - loglik: -1.4147e+03 - logprior: 0.5170
Epoch 5/10
20/20 - 9s - loss: 1417.3406 - loglik: -1.4179e+03 - logprior: 0.5405
Fitted a model with MAP estimate = -1414.6055
expansions: [(0, 3), (44, 2), (51, 1), (52, 1), (53, 1), (82, 3), (86, 12), (92, 1), (94, 1), (111, 4), (115, 3), (116, 1), (150, 1), (153, 2), (192, 2), (200, 1), (207, 1), (208, 1)]
discards: [  1 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1431.6217 - loglik: -1.4207e+03 - logprior: -1.0939e+01
Epoch 2/2
20/20 - 11s - loss: 1415.3635 - loglik: -1.4151e+03 - logprior: -2.9407e-01
Fitted a model with MAP estimate = -1408.6055
expansions: [(0, 3)]
discards: [  2   3   5   6  91  92  97  98 135 136 137 138 143]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 1426.7942 - loglik: -1.4149e+03 - logprior: -1.1919e+01
Epoch 2/2
20/20 - 11s - loss: 1411.0881 - loglik: -1.4106e+03 - logprior: -4.4614e-01
Fitted a model with MAP estimate = -1408.4526
expansions: [(0, 4)]
discards: [ 4  5  6 90]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1426.6582 - loglik: -1.4154e+03 - logprior: -1.1209e+01
Epoch 2/10
20/20 - 10s - loss: 1409.1101 - loglik: -1.4083e+03 - logprior: -8.3414e-01
Epoch 3/10
20/20 - 11s - loss: 1406.0846 - loglik: -1.4076e+03 - logprior: 1.5495
Epoch 4/10
20/20 - 11s - loss: 1403.7336 - loglik: -1.4058e+03 - logprior: 2.0571
Epoch 5/10
20/20 - 10s - loss: 1409.1451 - loglik: -1.4114e+03 - logprior: 2.2709
Fitted a model with MAP estimate = -1402.2092
Time for alignment: 182.5731
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1527.9421 - loglik: -1.5201e+03 - logprior: -7.8774e+00
Epoch 2/10
20/20 - 9s - loss: 1460.7955 - loglik: -1.4611e+03 - logprior: 0.2860
Epoch 3/10
20/20 - 9s - loss: 1418.6929 - loglik: -1.4190e+03 - logprior: 0.3235
Epoch 4/10
20/20 - 9s - loss: 1425.7063 - loglik: -1.4263e+03 - logprior: 0.5818
Fitted a model with MAP estimate = -1416.4273
expansions: [(0, 4), (21, 5), (45, 1), (54, 3), (75, 1), (86, 12), (92, 1), (94, 3), (112, 2), (118, 1), (121, 1), (154, 1), (165, 1), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1429.6951 - loglik: -1.4186e+03 - logprior: -1.1099e+01
Epoch 2/2
20/20 - 11s - loss: 1410.0300 - loglik: -1.4096e+03 - logprior: -4.5918e-01
Fitted a model with MAP estimate = -1407.0470
expansions: [(0, 3), (237, 1)]
discards: [  2   3 122]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 1423.0869 - loglik: -1.4115e+03 - logprior: -1.1583e+01
Epoch 2/2
20/20 - 11s - loss: 1408.8396 - loglik: -1.4081e+03 - logprior: -7.7292e-01
Fitted a model with MAP estimate = -1405.5250
expansions: [(0, 3)]
discards: [ 1  2  4  5  6 92]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1423.8738 - loglik: -1.4122e+03 - logprior: -1.1659e+01
Epoch 2/10
20/20 - 11s - loss: 1404.3696 - loglik: -1.4032e+03 - logprior: -1.1264e+00
Epoch 3/10
20/20 - 11s - loss: 1407.4791 - loglik: -1.4090e+03 - logprior: 1.5315
Fitted a model with MAP estimate = -1402.0000
Time for alignment: 157.2503
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1525.3680 - loglik: -1.5175e+03 - logprior: -7.8759e+00
Epoch 2/10
20/20 - 9s - loss: 1456.2009 - loglik: -1.4565e+03 - logprior: 0.2999
Epoch 3/10
20/20 - 9s - loss: 1425.1848 - loglik: -1.4256e+03 - logprior: 0.3758
Epoch 4/10
20/20 - 9s - loss: 1419.7598 - loglik: -1.4204e+03 - logprior: 0.6607
Epoch 5/10
20/20 - 9s - loss: 1414.5524 - loglik: -1.4152e+03 - logprior: 0.6735
Epoch 6/10
20/20 - 9s - loss: 1413.8275 - loglik: -1.4145e+03 - logprior: 0.6945
Epoch 7/10
20/20 - 9s - loss: 1413.9338 - loglik: -1.4147e+03 - logprior: 0.7211
Fitted a model with MAP estimate = -1412.4622
expansions: [(0, 3), (45, 1), (52, 3), (81, 4), (83, 2), (94, 1), (102, 1), (112, 2), (115, 2), (117, 1), (146, 1), (153, 2), (168, 1), (169, 11), (192, 2), (198, 3), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1434.7732 - loglik: -1.4235e+03 - logprior: -1.1225e+01
Epoch 2/2
20/20 - 11s - loss: 1411.0094 - loglik: -1.4106e+03 - logprior: -3.8006e-01
Fitted a model with MAP estimate = -1408.2043
expansions: [(0, 3), (21, 5), (56, 1), (94, 2)]
discards: [  1   4   5   6 132 192 193 194 195 196 197 198 227 236]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1425.1506 - loglik: -1.4136e+03 - logprior: -1.1577e+01
Epoch 2/2
20/20 - 11s - loss: 1407.9993 - loglik: -1.4072e+03 - logprior: -7.9240e-01
Fitted a model with MAP estimate = -1406.0483
expansions: [(0, 4)]
discards: [  1   2 181]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1421.4377 - loglik: -1.4101e+03 - logprior: -1.1295e+01
Epoch 2/10
20/20 - 11s - loss: 1407.1372 - loglik: -1.4060e+03 - logprior: -1.1707e+00
Epoch 3/10
20/20 - 11s - loss: 1406.0240 - loglik: -1.4076e+03 - logprior: 1.5701
Epoch 4/10
20/20 - 11s - loss: 1404.6506 - loglik: -1.4068e+03 - logprior: 2.1701
Epoch 5/10
20/20 - 11s - loss: 1401.2981 - loglik: -1.4037e+03 - logprior: 2.4108
Epoch 6/10
20/20 - 11s - loss: 1396.6410 - loglik: -1.3993e+03 - logprior: 2.6279
Epoch 7/10
20/20 - 11s - loss: 1399.8837 - loglik: -1.4027e+03 - logprior: 2.8215
Fitted a model with MAP estimate = -1398.1253
Time for alignment: 227.1622
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1526.7859 - loglik: -1.5189e+03 - logprior: -7.8808e+00
Epoch 2/10
20/20 - 9s - loss: 1463.5217 - loglik: -1.4638e+03 - logprior: 0.2466
Epoch 3/10
20/20 - 9s - loss: 1429.2189 - loglik: -1.4294e+03 - logprior: 0.1390
Epoch 4/10
20/20 - 9s - loss: 1419.4648 - loglik: -1.4199e+03 - logprior: 0.4130
Epoch 5/10
20/20 - 9s - loss: 1416.3462 - loglik: -1.4168e+03 - logprior: 0.4927
Epoch 6/10
20/20 - 9s - loss: 1418.7664 - loglik: -1.4193e+03 - logprior: 0.5332
Fitted a model with MAP estimate = -1414.9506
expansions: [(0, 4), (21, 5), (24, 1), (52, 1), (63, 1), (64, 1), (81, 4), (86, 1), (93, 1), (110, 5), (118, 1), (121, 1), (150, 1), (153, 2), (154, 1), (201, 1), (207, 1), (208, 1)]
discards: [217 218]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 1436.4919 - loglik: -1.4252e+03 - logprior: -1.1324e+01
Epoch 2/2
20/20 - 11s - loss: 1415.9695 - loglik: -1.4156e+03 - logprior: -3.5503e-01
Fitted a model with MAP estimate = -1411.2914
expansions: [(0, 4), (54, 1), (98, 3), (100, 4)]
discards: [  1   2   3   5  25  26  27 132 133 181]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1424.8276 - loglik: -1.4133e+03 - logprior: -1.1536e+01
Epoch 2/2
20/20 - 11s - loss: 1413.5579 - loglik: -1.4127e+03 - logprior: -8.4911e-01
Fitted a model with MAP estimate = -1407.3076
expansions: [(0, 4), (26, 3)]
discards: [1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1422.6777 - loglik: -1.4106e+03 - logprior: -1.2106e+01
Epoch 2/10
20/20 - 11s - loss: 1406.2864 - loglik: -1.4053e+03 - logprior: -9.4409e-01
Epoch 3/10
20/20 - 11s - loss: 1409.8021 - loglik: -1.4114e+03 - logprior: 1.5651
Fitted a model with MAP estimate = -1402.6750
Time for alignment: 171.2430
Computed alignments with likelihoods: ['-1399.1629', '-1402.2092', '-1402.0000', '-1398.1253', '-1402.6750']
Best model has likelihood: -1398.1253  (prior= 2.9249 )
time for generating output: 0.2946
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6411577140053961
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.2263 - loglik: -2.7099e+02 - logprior: -3.2407e+00
Epoch 2/10
19/19 - 1s - loss: 254.5165 - loglik: -2.5314e+02 - logprior: -1.3793e+00
Epoch 3/10
19/19 - 1s - loss: 247.3765 - loglik: -2.4616e+02 - logprior: -1.2133e+00
Epoch 4/10
19/19 - 1s - loss: 244.8416 - loglik: -2.4370e+02 - logprior: -1.1370e+00
Epoch 5/10
19/19 - 1s - loss: 244.2730 - loglik: -2.4316e+02 - logprior: -1.1176e+00
Epoch 6/10
19/19 - 1s - loss: 243.0228 - loglik: -2.4193e+02 - logprior: -1.0968e+00
Epoch 7/10
19/19 - 1s - loss: 241.4005 - loglik: -2.4041e+02 - logprior: -9.8632e-01
Epoch 8/10
19/19 - 1s - loss: 240.8194 - loglik: -2.3990e+02 - logprior: -9.1584e-01
Epoch 9/10
19/19 - 1s - loss: 240.6660 - loglik: -2.3978e+02 - logprior: -8.8527e-01
Epoch 10/10
19/19 - 1s - loss: 240.6347 - loglik: -2.3976e+02 - logprior: -8.7059e-01
Fitted a model with MAP estimate = -237.2972
expansions: [(9, 2), (11, 2), (12, 1), (13, 1), (22, 3), (35, 1), (36, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.4443 - loglik: -2.3698e+02 - logprior: -3.4652e+00
Epoch 2/2
19/19 - 1s - loss: 233.4801 - loglik: -2.3198e+02 - logprior: -1.5002e+00
Fitted a model with MAP estimate = -229.7645
expansions: []
discards: [ 9 46]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 235.7066 - loglik: -2.3232e+02 - logprior: -3.3913e+00
Epoch 2/2
19/19 - 1s - loss: 232.7588 - loglik: -2.3136e+02 - logprior: -1.4022e+00
Fitted a model with MAP estimate = -229.1435
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.1547 - loglik: -2.2893e+02 - logprior: -3.2200e+00
Epoch 2/10
19/19 - 1s - loss: 229.5878 - loglik: -2.2822e+02 - logprior: -1.3650e+00
Epoch 3/10
19/19 - 1s - loss: 228.8999 - loglik: -2.2766e+02 - logprior: -1.2358e+00
Epoch 4/10
19/19 - 1s - loss: 229.0266 - loglik: -2.2783e+02 - logprior: -1.1981e+00
Fitted a model with MAP estimate = -228.5283
Time for alignment: 40.2642
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.2431 - loglik: -2.7100e+02 - logprior: -3.2407e+00
Epoch 2/10
19/19 - 1s - loss: 253.7762 - loglik: -2.5234e+02 - logprior: -1.4325e+00
Epoch 3/10
19/19 - 1s - loss: 246.1537 - loglik: -2.4490e+02 - logprior: -1.2506e+00
Epoch 4/10
19/19 - 1s - loss: 243.9760 - loglik: -2.4280e+02 - logprior: -1.1749e+00
Epoch 5/10
19/19 - 1s - loss: 242.7549 - loglik: -2.4158e+02 - logprior: -1.1743e+00
Epoch 6/10
19/19 - 1s - loss: 242.4626 - loglik: -2.4129e+02 - logprior: -1.1686e+00
Epoch 7/10
19/19 - 1s - loss: 242.1922 - loglik: -2.4103e+02 - logprior: -1.1581e+00
Epoch 8/10
19/19 - 1s - loss: 241.9249 - loglik: -2.4077e+02 - logprior: -1.1509e+00
Epoch 9/10
19/19 - 1s - loss: 242.0926 - loglik: -2.4095e+02 - logprior: -1.1414e+00
Fitted a model with MAP estimate = -238.7635
expansions: [(10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 242.7643 - loglik: -2.3942e+02 - logprior: -3.3487e+00
Epoch 2/2
19/19 - 1s - loss: 236.6710 - loglik: -2.3537e+02 - logprior: -1.3023e+00
Fitted a model with MAP estimate = -232.7950
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.2766 - loglik: -2.3207e+02 - logprior: -3.2089e+00
Epoch 2/10
19/19 - 1s - loss: 231.4032 - loglik: -2.3002e+02 - logprior: -1.3882e+00
Epoch 3/10
19/19 - 1s - loss: 230.2466 - loglik: -2.2897e+02 - logprior: -1.2727e+00
Epoch 4/10
19/19 - 1s - loss: 229.4321 - loglik: -2.2817e+02 - logprior: -1.2655e+00
Epoch 5/10
19/19 - 1s - loss: 229.6537 - loglik: -2.2842e+02 - logprior: -1.2384e+00
Fitted a model with MAP estimate = -229.2555
Time for alignment: 32.2174
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.2247 - loglik: -2.7098e+02 - logprior: -3.2406e+00
Epoch 2/10
19/19 - 1s - loss: 253.8141 - loglik: -2.5237e+02 - logprior: -1.4450e+00
Epoch 3/10
19/19 - 1s - loss: 246.2843 - loglik: -2.4501e+02 - logprior: -1.2756e+00
Epoch 4/10
19/19 - 1s - loss: 243.6387 - loglik: -2.4244e+02 - logprior: -1.2026e+00
Epoch 5/10
19/19 - 1s - loss: 242.9422 - loglik: -2.4175e+02 - logprior: -1.1957e+00
Epoch 6/10
19/19 - 1s - loss: 242.5163 - loglik: -2.4134e+02 - logprior: -1.1755e+00
Epoch 7/10
19/19 - 1s - loss: 242.2787 - loglik: -2.4112e+02 - logprior: -1.1545e+00
Epoch 8/10
19/19 - 1s - loss: 242.1421 - loglik: -2.4100e+02 - logprior: -1.1457e+00
Epoch 9/10
19/19 - 1s - loss: 242.1326 - loglik: -2.4099e+02 - logprior: -1.1394e+00
Epoch 10/10
19/19 - 1s - loss: 242.2507 - loglik: -2.4111e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -238.8618
expansions: [(10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 244.2464 - loglik: -2.4091e+02 - logprior: -3.3386e+00
Epoch 2/2
19/19 - 1s - loss: 238.8300 - loglik: -2.3756e+02 - logprior: -1.2740e+00
Fitted a model with MAP estimate = -235.2004
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 241.2663 - loglik: -2.3799e+02 - logprior: -3.2809e+00
Epoch 2/2
19/19 - 1s - loss: 237.7989 - loglik: -2.3657e+02 - logprior: -1.2310e+00
Fitted a model with MAP estimate = -232.8610
expansions: [(42, 1), (43, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 234.1433 - loglik: -2.3083e+02 - logprior: -3.3154e+00
Epoch 2/10
19/19 - 1s - loss: 230.0513 - loglik: -2.2870e+02 - logprior: -1.3472e+00
Epoch 3/10
19/19 - 1s - loss: 228.9475 - loglik: -2.2777e+02 - logprior: -1.1740e+00
Epoch 4/10
19/19 - 1s - loss: 228.8283 - loglik: -2.2764e+02 - logprior: -1.1885e+00
Epoch 5/10
19/19 - 1s - loss: 228.4986 - loglik: -2.2731e+02 - logprior: -1.1874e+00
Epoch 6/10
19/19 - 1s - loss: 227.8902 - loglik: -2.2672e+02 - logprior: -1.1690e+00
Epoch 7/10
19/19 - 1s - loss: 228.2802 - loglik: -2.2712e+02 - logprior: -1.1613e+00
Fitted a model with MAP estimate = -227.9879
Time for alignment: 43.8014
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.2183 - loglik: -2.7098e+02 - logprior: -3.2392e+00
Epoch 2/10
19/19 - 1s - loss: 254.0416 - loglik: -2.5267e+02 - logprior: -1.3733e+00
Epoch 3/10
19/19 - 1s - loss: 246.6564 - loglik: -2.4529e+02 - logprior: -1.3646e+00
Epoch 4/10
19/19 - 1s - loss: 244.0872 - loglik: -2.4270e+02 - logprior: -1.3916e+00
Epoch 5/10
19/19 - 1s - loss: 243.3661 - loglik: -2.4200e+02 - logprior: -1.3618e+00
Epoch 6/10
19/19 - 1s - loss: 243.0829 - loglik: -2.4172e+02 - logprior: -1.3609e+00
Epoch 7/10
19/19 - 1s - loss: 242.8249 - loglik: -2.4148e+02 - logprior: -1.3418e+00
Epoch 8/10
19/19 - 1s - loss: 242.8842 - loglik: -2.4155e+02 - logprior: -1.3340e+00
Fitted a model with MAP estimate = -239.5886
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 246.8008 - loglik: -2.4264e+02 - logprior: -4.1657e+00
Epoch 2/2
19/19 - 1s - loss: 239.0861 - loglik: -2.3725e+02 - logprior: -1.8324e+00
Fitted a model with MAP estimate = -234.0138
expansions: [(0, 1), (37, 1)]
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.4944 - loglik: -2.3431e+02 - logprior: -3.1892e+00
Epoch 2/2
19/19 - 1s - loss: 233.1674 - loglik: -2.3178e+02 - logprior: -1.3854e+00
Fitted a model with MAP estimate = -229.6430
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.1218 - loglik: -2.2894e+02 - logprior: -3.1800e+00
Epoch 2/10
19/19 - 1s - loss: 229.3983 - loglik: -2.2803e+02 - logprior: -1.3724e+00
Epoch 3/10
19/19 - 1s - loss: 229.1684 - loglik: -2.2793e+02 - logprior: -1.2408e+00
Epoch 4/10
19/19 - 1s - loss: 228.3953 - loglik: -2.2719e+02 - logprior: -1.2041e+00
Epoch 5/10
19/19 - 1s - loss: 228.3788 - loglik: -2.2718e+02 - logprior: -1.1976e+00
Epoch 6/10
19/19 - 1s - loss: 228.1330 - loglik: -2.2695e+02 - logprior: -1.1798e+00
Epoch 7/10
19/19 - 1s - loss: 228.1108 - loglik: -2.2694e+02 - logprior: -1.1700e+00
Epoch 8/10
19/19 - 1s - loss: 227.7887 - loglik: -2.2663e+02 - logprior: -1.1622e+00
Epoch 9/10
19/19 - 1s - loss: 228.0378 - loglik: -2.2689e+02 - logprior: -1.1503e+00
Fitted a model with MAP estimate = -227.8582
Time for alignment: 42.6572
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 274.1080 - loglik: -2.7087e+02 - logprior: -3.2412e+00
Epoch 2/10
19/19 - 1s - loss: 254.0521 - loglik: -2.5260e+02 - logprior: -1.4512e+00
Epoch 3/10
19/19 - 1s - loss: 246.7893 - loglik: -2.4554e+02 - logprior: -1.2484e+00
Epoch 4/10
19/19 - 1s - loss: 244.8914 - loglik: -2.4374e+02 - logprior: -1.1539e+00
Epoch 5/10
19/19 - 1s - loss: 244.1006 - loglik: -2.4296e+02 - logprior: -1.1417e+00
Epoch 6/10
19/19 - 1s - loss: 243.7954 - loglik: -2.4266e+02 - logprior: -1.1360e+00
Epoch 7/10
19/19 - 1s - loss: 243.8729 - loglik: -2.4275e+02 - logprior: -1.1264e+00
Fitted a model with MAP estimate = -240.1824
expansions: [(9, 2), (11, 2), (12, 1), (13, 1), (23, 2), (28, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 243.5245 - loglik: -2.4015e+02 - logprior: -3.3703e+00
Epoch 2/2
19/19 - 1s - loss: 236.8868 - loglik: -2.3553e+02 - logprior: -1.3547e+00
Fitted a model with MAP estimate = -232.7346
expansions: [(37, 1)]
discards: [ 9 29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.2037 - loglik: -2.3386e+02 - logprior: -3.3415e+00
Epoch 2/2
19/19 - 1s - loss: 233.5349 - loglik: -2.3214e+02 - logprior: -1.3955e+00
Fitted a model with MAP estimate = -230.0750
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 232.6587 - loglik: -2.2944e+02 - logprior: -3.2156e+00
Epoch 2/10
19/19 - 1s - loss: 230.4635 - loglik: -2.2909e+02 - logprior: -1.3746e+00
Epoch 3/10
19/19 - 1s - loss: 228.8180 - loglik: -2.2758e+02 - logprior: -1.2334e+00
Epoch 4/10
19/19 - 1s - loss: 229.2634 - loglik: -2.2806e+02 - logprior: -1.2065e+00
Fitted a model with MAP estimate = -228.6382
Time for alignment: 35.0773
Computed alignments with likelihoods: ['-228.5283', '-229.2555', '-227.9879', '-227.8582', '-228.6382']
Best model has likelihood: -227.8582  (prior= -1.1465 )
time for generating output: 0.0949
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9438877755511023
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 1294.9987 - loglik: -1.2765e+03 - logprior: -1.8508e+01
Epoch 2/10
15/15 - 6s - loss: 1224.3029 - loglik: -1.2239e+03 - logprior: -3.7067e-01
Epoch 3/10
15/15 - 6s - loss: 1150.5414 - loglik: -1.1507e+03 - logprior: 0.1961
Epoch 4/10
15/15 - 6s - loss: 1124.4497 - loglik: -1.1240e+03 - logprior: -4.2733e-01
Epoch 5/10
15/15 - 6s - loss: 1115.7794 - loglik: -1.1150e+03 - logprior: -7.8041e-01
Epoch 6/10
15/15 - 6s - loss: 1119.0973 - loglik: -1.1186e+03 - logprior: -5.3049e-01
Fitted a model with MAP estimate = -1114.5748
expansions: [(23, 1), (24, 2), (25, 2), (39, 2), (40, 2), (41, 1), (44, 2), (47, 1), (48, 1), (49, 2), (50, 3), (53, 1), (57, 1), (64, 1), (65, 1), (69, 1), (72, 1), (74, 1), (75, 1), (76, 1), (92, 1), (98, 1), (99, 1), (101, 3), (107, 1), (129, 2), (130, 1), (131, 1), (132, 2), (133, 2), (136, 1), (156, 1), (157, 6), (173, 8), (183, 3)]
discards: [  1   2 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1120.2466 - loglik: -1.1036e+03 - logprior: -1.6661e+01
Epoch 2/2
15/15 - 9s - loss: 1088.8951 - loglik: -1.0890e+03 - logprior: 0.0980
Fitted a model with MAP estimate = -1081.3373
expansions: [(44, 1), (62, 1), (201, 2), (202, 1), (240, 1)]
discards: [172 227 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1103.2183 - loglik: -1.0875e+03 - logprior: -1.5743e+01
Epoch 2/2
15/15 - 9s - loss: 1075.9347 - loglik: -1.0770e+03 - logprior: 1.1017
Fitted a model with MAP estimate = -1075.3895
expansions: [(0, 3), (62, 1), (243, 1)]
discards: [ 22 164 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1097.1256 - loglik: -1.0735e+03 - logprior: -2.3589e+01
Epoch 2/10
15/15 - 9s - loss: 1084.2062 - loglik: -1.0832e+03 - logprior: -9.5959e-01
Epoch 3/10
15/15 - 9s - loss: 1076.6582 - loglik: -1.0806e+03 - logprior: 3.9322
Epoch 4/10
15/15 - 9s - loss: 1066.8712 - loglik: -1.0723e+03 - logprior: 5.4152
Epoch 5/10
15/15 - 9s - loss: 1070.8468 - loglik: -1.0768e+03 - logprior: 5.9713
Fitted a model with MAP estimate = -1068.3181
Time for alignment: 147.3359
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 1304.3655 - loglik: -1.2859e+03 - logprior: -1.8488e+01
Epoch 2/10
15/15 - 6s - loss: 1212.2715 - loglik: -1.2118e+03 - logprior: -4.3235e-01
Epoch 3/10
15/15 - 6s - loss: 1145.9515 - loglik: -1.1458e+03 - logprior: -1.9159e-01
Epoch 4/10
15/15 - 6s - loss: 1128.8595 - loglik: -1.1279e+03 - logprior: -1.0068e+00
Epoch 5/10
15/15 - 6s - loss: 1107.2424 - loglik: -1.1060e+03 - logprior: -1.2122e+00
Epoch 6/10
15/15 - 6s - loss: 1118.8683 - loglik: -1.1178e+03 - logprior: -1.1059e+00
Fitted a model with MAP estimate = -1112.3350
expansions: [(19, 1), (23, 1), (24, 2), (25, 3), (27, 1), (36, 1), (37, 1), (39, 1), (46, 1), (49, 1), (50, 1), (51, 2), (53, 2), (63, 1), (64, 1), (65, 3), (66, 2), (67, 1), (68, 1), (70, 1), (71, 1), (72, 1), (75, 1), (77, 1), (93, 1), (99, 1), (100, 2), (101, 3), (108, 1), (130, 2), (131, 1), (132, 1), (133, 2), (134, 2), (137, 1), (157, 1), (158, 5), (159, 2), (174, 8)]
discards: [  1   2   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1120.3341 - loglik: -1.1038e+03 - logprior: -1.6534e+01
Epoch 2/2
15/15 - 9s - loss: 1084.7219 - loglik: -1.0850e+03 - logprior: 0.3100
Fitted a model with MAP estimate = -1079.0016
expansions: [(63, 1), (67, 1)]
discards: [ 22  84  85 131 177 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1101.2455 - loglik: -1.0858e+03 - logprior: -1.5420e+01
Epoch 2/2
15/15 - 9s - loss: 1081.8229 - loglik: -1.0831e+03 - logprior: 1.2302
Fitted a model with MAP estimate = -1077.0447
expansions: [(0, 3), (203, 1), (238, 1), (239, 4)]
discards: [165]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1098.4978 - loglik: -1.0753e+03 - logprior: -2.3231e+01
Epoch 2/10
15/15 - 9s - loss: 1083.5095 - loglik: -1.0827e+03 - logprior: -7.6067e-01
Epoch 3/10
15/15 - 9s - loss: 1068.4822 - loglik: -1.0726e+03 - logprior: 4.1158
Epoch 4/10
15/15 - 9s - loss: 1069.2950 - loglik: -1.0749e+03 - logprior: 5.6213
Fitted a model with MAP estimate = -1067.7068
Time for alignment: 138.8246
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 1298.4138 - loglik: -1.2799e+03 - logprior: -1.8561e+01
Epoch 2/10
15/15 - 6s - loss: 1223.2368 - loglik: -1.2229e+03 - logprior: -3.8143e-01
Epoch 3/10
15/15 - 6s - loss: 1157.0481 - loglik: -1.1572e+03 - logprior: 0.1202
Epoch 4/10
15/15 - 6s - loss: 1124.3750 - loglik: -1.1238e+03 - logprior: -6.1150e-01
Epoch 5/10
15/15 - 6s - loss: 1124.2341 - loglik: -1.1235e+03 - logprior: -6.8809e-01
Epoch 6/10
15/15 - 6s - loss: 1117.7704 - loglik: -1.1172e+03 - logprior: -6.1714e-01
Epoch 7/10
15/15 - 6s - loss: 1117.4434 - loglik: -1.1170e+03 - logprior: -4.1311e-01
Epoch 8/10
15/15 - 6s - loss: 1116.2980 - loglik: -1.1160e+03 - logprior: -2.7582e-01
Epoch 9/10
15/15 - 6s - loss: 1116.8524 - loglik: -1.1166e+03 - logprior: -2.3633e-01
Fitted a model with MAP estimate = -1115.8244
expansions: [(40, 1), (43, 3), (44, 1), (46, 1), (50, 1), (52, 2), (53, 1), (55, 2), (56, 3), (66, 1), (69, 4), (70, 2), (71, 1), (72, 1), (73, 1), (74, 1), (77, 2), (78, 1), (85, 1), (100, 4), (101, 3), (103, 2), (107, 1), (129, 2), (130, 1), (131, 1), (132, 2), (133, 2), (136, 1), (156, 1), (157, 5), (158, 2), (173, 7), (183, 6)]
discards: [  1   2   6 198 199 200 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1114.5388 - loglik: -1.0980e+03 - logprior: -1.6549e+01
Epoch 2/2
15/15 - 9s - loss: 1090.5992 - loglik: -1.0908e+03 - logprior: 0.1743
Fitted a model with MAP estimate = -1080.9473
expansions: [(0, 3), (43, 1), (103, 1)]
discards: [ 23  87 129 137 176 231 232 233 234 265]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1112.8334 - loglik: -1.0885e+03 - logprior: -2.4310e+01
Epoch 2/2
15/15 - 9s - loss: 1078.1077 - loglik: -1.0762e+03 - logprior: -1.8624e+00
Fitted a model with MAP estimate = -1076.7816
expansions: [(65, 1), (205, 1), (243, 2), (244, 1)]
discards: [  0   1   2 167 261]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1099.7925 - loglik: -1.0828e+03 - logprior: -1.7009e+01
Epoch 2/10
15/15 - 9s - loss: 1070.6682 - loglik: -1.0727e+03 - logprior: 2.0358
Epoch 3/10
15/15 - 9s - loss: 1072.8878 - loglik: -1.0777e+03 - logprior: 4.7959
Fitted a model with MAP estimate = -1070.2151
Time for alignment: 148.4512
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1300.2849 - loglik: -1.2818e+03 - logprior: -1.8507e+01
Epoch 2/10
15/15 - 6s - loss: 1212.0485 - loglik: -1.2115e+03 - logprior: -5.1394e-01
Epoch 3/10
15/15 - 6s - loss: 1156.2781 - loglik: -1.1561e+03 - logprior: -1.7152e-01
Epoch 4/10
15/15 - 6s - loss: 1121.2848 - loglik: -1.1201e+03 - logprior: -1.1409e+00
Epoch 5/10
15/15 - 6s - loss: 1110.8555 - loglik: -1.1095e+03 - logprior: -1.3588e+00
Epoch 6/10
15/15 - 6s - loss: 1120.6447 - loglik: -1.1194e+03 - logprior: -1.2652e+00
Fitted a model with MAP estimate = -1113.9803
expansions: [(20, 1), (22, 1), (23, 2), (24, 1), (38, 1), (39, 1), (41, 1), (43, 1), (46, 2), (49, 1), (50, 1), (51, 1), (52, 1), (54, 1), (67, 1), (68, 5), (69, 2), (70, 1), (72, 1), (73, 1), (74, 1), (77, 1), (79, 1), (95, 1), (101, 1), (102, 1), (103, 3), (105, 2), (109, 1), (131, 2), (132, 1), (133, 1), (134, 2), (135, 3), (137, 1), (139, 1), (156, 1), (157, 4), (159, 2), (174, 8)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1119.5043 - loglik: -1.1031e+03 - logprior: -1.6357e+01
Epoch 2/2
15/15 - 9s - loss: 1084.5082 - loglik: -1.0851e+03 - logprior: 0.5624
Fitted a model with MAP estimate = -1079.5733
expansions: [(0, 3), (66, 1), (207, 1)]
discards: [ 89 139 178]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1105.9607 - loglik: -1.0816e+03 - logprior: -2.4333e+01
Epoch 2/2
15/15 - 9s - loss: 1079.2736 - loglik: -1.0777e+03 - logprior: -1.6154e+00
Fitted a model with MAP estimate = -1075.7740
expansions: []
discards: [  0   1   2 170]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1098.5341 - loglik: -1.0815e+03 - logprior: -1.6991e+01
Epoch 2/10
15/15 - 9s - loss: 1077.7134 - loglik: -1.0798e+03 - logprior: 2.0598
Epoch 3/10
15/15 - 9s - loss: 1063.9219 - loglik: -1.0688e+03 - logprior: 4.9146
Epoch 4/10
15/15 - 9s - loss: 1068.9215 - loglik: -1.0747e+03 - logprior: 5.8125
Fitted a model with MAP estimate = -1068.9082
Time for alignment: 138.9930
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1298.9218 - loglik: -1.2804e+03 - logprior: -1.8529e+01
Epoch 2/10
15/15 - 6s - loss: 1215.0424 - loglik: -1.2146e+03 - logprior: -4.5655e-01
Epoch 3/10
15/15 - 6s - loss: 1151.0607 - loglik: -1.1512e+03 - logprior: 0.1412
Epoch 4/10
15/15 - 6s - loss: 1134.1802 - loglik: -1.1337e+03 - logprior: -4.7927e-01
Epoch 5/10
15/15 - 6s - loss: 1113.5391 - loglik: -1.1126e+03 - logprior: -9.6104e-01
Epoch 6/10
15/15 - 6s - loss: 1118.8148 - loglik: -1.1180e+03 - logprior: -7.7477e-01
Fitted a model with MAP estimate = -1114.4625
expansions: [(23, 1), (24, 2), (25, 2), (27, 1), (36, 1), (37, 1), (39, 1), (49, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (66, 1), (69, 5), (70, 2), (71, 1), (72, 1), (73, 1), (74, 2), (77, 1), (79, 1), (95, 1), (101, 1), (102, 1), (103, 3), (133, 4), (134, 4), (135, 2), (137, 1), (139, 1), (156, 1), (157, 6), (173, 8), (183, 3)]
discards: [  1   2 200 223]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1120.5242 - loglik: -1.1041e+03 - logprior: -1.6439e+01
Epoch 2/2
15/15 - 9s - loss: 1078.6499 - loglik: -1.0792e+03 - logprior: 0.5140
Fitted a model with MAP estimate = -1077.7933
expansions: [(26, 1), (205, 2), (206, 1), (244, 2)]
discards: [ 23  89 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1099.1097 - loglik: -1.0838e+03 - logprior: -1.5288e+01
Epoch 2/2
15/15 - 9s - loss: 1071.5194 - loglik: -1.0729e+03 - logprior: 1.4183
Fitted a model with MAP estimate = -1073.2370
expansions: [(0, 3)]
discards: [166 204]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1104.8303 - loglik: -1.0812e+03 - logprior: -2.3676e+01
Epoch 2/10
15/15 - 9s - loss: 1073.8683 - loglik: -1.0730e+03 - logprior: -8.7979e-01
Epoch 3/10
15/15 - 9s - loss: 1070.0559 - loglik: -1.0742e+03 - logprior: 4.1343
Epoch 4/10
15/15 - 9s - loss: 1073.5333 - loglik: -1.0792e+03 - logprior: 5.7039
Fitted a model with MAP estimate = -1068.2397
Time for alignment: 139.8359
Computed alignments with likelihoods: ['-1068.3181', '-1067.7068', '-1070.2151', '-1068.9082', '-1068.2397']
Best model has likelihood: -1067.7068  (prior= 5.9661 )
time for generating output: 0.2841
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9191102123356926
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 416.2387 - loglik: -3.7457e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 369.7701 - loglik: -3.5857e+02 - logprior: -1.1197e+01
Epoch 3/10
10/10 - 1s - loss: 350.9159 - loglik: -3.4543e+02 - logprior: -5.4869e+00
Epoch 4/10
10/10 - 1s - loss: 341.8727 - loglik: -3.3833e+02 - logprior: -3.5465e+00
Epoch 5/10
10/10 - 1s - loss: 337.9406 - loglik: -3.3524e+02 - logprior: -2.7026e+00
Epoch 6/10
10/10 - 1s - loss: 336.3947 - loglik: -3.3419e+02 - logprior: -2.2040e+00
Epoch 7/10
10/10 - 1s - loss: 335.9821 - loglik: -3.3423e+02 - logprior: -1.7541e+00
Epoch 8/10
10/10 - 1s - loss: 335.4261 - loglik: -3.3404e+02 - logprior: -1.3901e+00
Epoch 9/10
10/10 - 1s - loss: 335.0462 - loglik: -3.3383e+02 - logprior: -1.2210e+00
Epoch 10/10
10/10 - 1s - loss: 334.7584 - loglik: -3.3359e+02 - logprior: -1.1641e+00
Fitted a model with MAP estimate = -334.7225
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (19, 1), (20, 2), (27, 1), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.9225 - loglik: -3.3104e+02 - logprior: -5.4886e+01
Epoch 2/2
10/10 - 1s - loss: 343.2177 - loglik: -3.2652e+02 - logprior: -1.6698e+01
Fitted a model with MAP estimate = -335.3283
expansions: [(32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 372.9428 - loglik: -3.2566e+02 - logprior: -4.7282e+01
Epoch 2/2
10/10 - 1s - loss: 343.9365 - loglik: -3.2557e+02 - logprior: -1.8364e+01
Fitted a model with MAP estimate = -338.6768
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 367.4352 - loglik: -3.2482e+02 - logprior: -4.2619e+01
Epoch 2/10
10/10 - 1s - loss: 335.7882 - loglik: -3.2420e+02 - logprior: -1.1584e+01
Epoch 3/10
10/10 - 1s - loss: 328.3304 - loglik: -3.2383e+02 - logprior: -4.4977e+00
Epoch 4/10
10/10 - 1s - loss: 325.6363 - loglik: -3.2359e+02 - logprior: -2.0496e+00
Epoch 5/10
10/10 - 1s - loss: 324.5580 - loglik: -3.2355e+02 - logprior: -1.0059e+00
Epoch 6/10
10/10 - 1s - loss: 323.5623 - loglik: -3.2310e+02 - logprior: -4.5970e-01
Epoch 7/10
10/10 - 1s - loss: 323.1785 - loglik: -3.2327e+02 - logprior: 0.0921
Epoch 8/10
10/10 - 1s - loss: 323.2625 - loglik: -3.2377e+02 - logprior: 0.5043
Fitted a model with MAP estimate = -322.9782
Time for alignment: 28.2841
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 416.2838 - loglik: -3.7462e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 369.3377 - loglik: -3.5814e+02 - logprior: -1.1193e+01
Epoch 3/10
10/10 - 1s - loss: 350.2326 - loglik: -3.4472e+02 - logprior: -5.5118e+00
Epoch 4/10
10/10 - 1s - loss: 340.8127 - loglik: -3.3728e+02 - logprior: -3.5367e+00
Epoch 5/10
10/10 - 1s - loss: 336.5245 - loglik: -3.3387e+02 - logprior: -2.6544e+00
Epoch 6/10
10/10 - 1s - loss: 334.4162 - loglik: -3.3226e+02 - logprior: -2.1587e+00
Epoch 7/10
10/10 - 1s - loss: 333.6647 - loglik: -3.3195e+02 - logprior: -1.7141e+00
Epoch 8/10
10/10 - 1s - loss: 333.7275 - loglik: -3.3235e+02 - logprior: -1.3786e+00
Fitted a model with MAP estimate = -333.3320
expansions: [(0, 2), (17, 1), (18, 3), (22, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 383.4864 - loglik: -3.2915e+02 - logprior: -5.4336e+01
Epoch 2/2
10/10 - 1s - loss: 340.7215 - loglik: -3.2423e+02 - logprior: -1.6492e+01
Fitted a model with MAP estimate = -333.0152
expansions: [(3, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 371.0528 - loglik: -3.2381e+02 - logprior: -4.7238e+01
Epoch 2/2
10/10 - 1s - loss: 342.0184 - loglik: -3.2361e+02 - logprior: -1.8407e+01
Fitted a model with MAP estimate = -336.8523
expansions: []
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 365.6745 - loglik: -3.2333e+02 - logprior: -4.2345e+01
Epoch 2/10
10/10 - 1s - loss: 334.1357 - loglik: -3.2274e+02 - logprior: -1.1400e+01
Epoch 3/10
10/10 - 1s - loss: 326.7778 - loglik: -3.2235e+02 - logprior: -4.4313e+00
Epoch 4/10
10/10 - 1s - loss: 324.0406 - loglik: -3.2204e+02 - logprior: -2.0046e+00
Epoch 5/10
10/10 - 1s - loss: 322.5677 - loglik: -3.2161e+02 - logprior: -9.6263e-01
Epoch 6/10
10/10 - 1s - loss: 321.8817 - loglik: -3.2146e+02 - logprior: -4.1869e-01
Epoch 7/10
10/10 - 1s - loss: 321.9068 - loglik: -3.2204e+02 - logprior: 0.1331
Fitted a model with MAP estimate = -321.4629
Time for alignment: 25.8890
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 415.9389 - loglik: -3.7427e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 369.4636 - loglik: -3.5827e+02 - logprior: -1.1193e+01
Epoch 3/10
10/10 - 1s - loss: 350.5170 - loglik: -3.4502e+02 - logprior: -5.4999e+00
Epoch 4/10
10/10 - 1s - loss: 340.9522 - loglik: -3.3744e+02 - logprior: -3.5135e+00
Epoch 5/10
10/10 - 1s - loss: 337.3285 - loglik: -3.3473e+02 - logprior: -2.6001e+00
Epoch 6/10
10/10 - 1s - loss: 335.6690 - loglik: -3.3351e+02 - logprior: -2.1546e+00
Epoch 7/10
10/10 - 1s - loss: 334.7181 - loglik: -3.3299e+02 - logprior: -1.7247e+00
Epoch 8/10
10/10 - 1s - loss: 333.6902 - loglik: -3.3231e+02 - logprior: -1.3800e+00
Epoch 9/10
10/10 - 1s - loss: 333.1979 - loglik: -3.3197e+02 - logprior: -1.2317e+00
Epoch 10/10
10/10 - 1s - loss: 332.8064 - loglik: -3.3165e+02 - logprior: -1.1610e+00
Fitted a model with MAP estimate = -332.8210
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 383.7950 - loglik: -3.2904e+02 - logprior: -5.4753e+01
Epoch 2/2
10/10 - 1s - loss: 341.3225 - loglik: -3.2477e+02 - logprior: -1.6554e+01
Fitted a model with MAP estimate = -333.6469
expansions: [(19, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 371.9344 - loglik: -3.2475e+02 - logprior: -4.7183e+01
Epoch 2/2
10/10 - 1s - loss: 342.0288 - loglik: -3.2374e+02 - logprior: -1.8286e+01
Fitted a model with MAP estimate = -337.0718
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 365.9906 - loglik: -3.2347e+02 - logprior: -4.2522e+01
Epoch 2/10
10/10 - 1s - loss: 334.2154 - loglik: -3.2270e+02 - logprior: -1.1511e+01
Epoch 3/10
10/10 - 1s - loss: 326.9514 - loglik: -3.2252e+02 - logprior: -4.4264e+00
Epoch 4/10
10/10 - 1s - loss: 324.0379 - loglik: -3.2205e+02 - logprior: -1.9847e+00
Epoch 5/10
10/10 - 1s - loss: 322.6752 - loglik: -3.2174e+02 - logprior: -9.3906e-01
Epoch 6/10
10/10 - 1s - loss: 322.0575 - loglik: -3.2167e+02 - logprior: -3.8456e-01
Epoch 7/10
10/10 - 1s - loss: 321.6555 - loglik: -3.2182e+02 - logprior: 0.1693
Epoch 8/10
10/10 - 1s - loss: 321.3210 - loglik: -3.2191e+02 - logprior: 0.5913
Epoch 9/10
10/10 - 1s - loss: 321.0151 - loglik: -3.2182e+02 - logprior: 0.8014
Epoch 10/10
10/10 - 1s - loss: 320.9111 - loglik: -3.2185e+02 - logprior: 0.9393
Fitted a model with MAP estimate = -320.8261
Time for alignment: 27.7851
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 416.0081 - loglik: -3.7435e+02 - logprior: -4.1662e+01
Epoch 2/10
10/10 - 1s - loss: 369.9450 - loglik: -3.5875e+02 - logprior: -1.1193e+01
Epoch 3/10
10/10 - 1s - loss: 351.1975 - loglik: -3.4568e+02 - logprior: -5.5163e+00
Epoch 4/10
10/10 - 1s - loss: 340.5701 - loglik: -3.3701e+02 - logprior: -3.5602e+00
Epoch 5/10
10/10 - 1s - loss: 336.6702 - loglik: -3.3400e+02 - logprior: -2.6657e+00
Epoch 6/10
10/10 - 1s - loss: 335.0767 - loglik: -3.3289e+02 - logprior: -2.1855e+00
Epoch 7/10
10/10 - 1s - loss: 334.6721 - loglik: -3.3293e+02 - logprior: -1.7419e+00
Epoch 8/10
10/10 - 1s - loss: 334.1054 - loglik: -3.3270e+02 - logprior: -1.4056e+00
Epoch 9/10
10/10 - 1s - loss: 333.6867 - loglik: -3.3244e+02 - logprior: -1.2453e+00
Epoch 10/10
10/10 - 1s - loss: 333.7579 - loglik: -3.3259e+02 - logprior: -1.1696e+00
Fitted a model with MAP estimate = -333.5455
expansions: [(0, 2), (1, 1), (17, 1), (19, 2), (20, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 384.6375 - loglik: -3.2968e+02 - logprior: -5.4960e+01
Epoch 2/2
10/10 - 1s - loss: 342.5070 - loglik: -3.2585e+02 - logprior: -1.6655e+01
Fitted a model with MAP estimate = -334.9219
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 373.0174 - loglik: -3.2547e+02 - logprior: -4.7545e+01
Epoch 2/2
10/10 - 1s - loss: 343.9524 - loglik: -3.2545e+02 - logprior: -1.8498e+01
Fitted a model with MAP estimate = -338.6313
expansions: [(18, 1), (31, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 367.2440 - loglik: -3.2459e+02 - logprior: -4.2649e+01
Epoch 2/10
10/10 - 1s - loss: 334.7800 - loglik: -3.2322e+02 - logprior: -1.1562e+01
Epoch 3/10
10/10 - 1s - loss: 327.1696 - loglik: -3.2271e+02 - logprior: -4.4633e+00
Epoch 4/10
10/10 - 1s - loss: 324.5084 - loglik: -3.2251e+02 - logprior: -2.0004e+00
Epoch 5/10
10/10 - 1s - loss: 322.7504 - loglik: -3.2182e+02 - logprior: -9.2686e-01
Epoch 6/10
10/10 - 1s - loss: 322.2362 - loglik: -3.2183e+02 - logprior: -4.0241e-01
Epoch 7/10
10/10 - 1s - loss: 321.8249 - loglik: -3.2195e+02 - logprior: 0.1208
Epoch 8/10
10/10 - 1s - loss: 321.5237 - loglik: -3.2206e+02 - logprior: 0.5325
Epoch 9/10
10/10 - 1s - loss: 321.1322 - loglik: -3.2187e+02 - logprior: 0.7386
Epoch 10/10
10/10 - 1s - loss: 321.0615 - loglik: -3.2193e+02 - logprior: 0.8678
Fitted a model with MAP estimate = -321.0553
Time for alignment: 28.2933
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 416.0487 - loglik: -3.7438e+02 - logprior: -4.1666e+01
Epoch 2/10
10/10 - 1s - loss: 369.1935 - loglik: -3.5800e+02 - logprior: -1.1198e+01
Epoch 3/10
10/10 - 1s - loss: 350.0896 - loglik: -3.4459e+02 - logprior: -5.5027e+00
Epoch 4/10
10/10 - 1s - loss: 340.6805 - loglik: -3.3715e+02 - logprior: -3.5340e+00
Epoch 5/10
10/10 - 1s - loss: 337.0658 - loglik: -3.3444e+02 - logprior: -2.6249e+00
Epoch 6/10
10/10 - 1s - loss: 335.1594 - loglik: -3.3304e+02 - logprior: -2.1230e+00
Epoch 7/10
10/10 - 1s - loss: 334.5228 - loglik: -3.3286e+02 - logprior: -1.6670e+00
Epoch 8/10
10/10 - 1s - loss: 333.6341 - loglik: -3.3230e+02 - logprior: -1.3309e+00
Epoch 9/10
10/10 - 1s - loss: 333.3934 - loglik: -3.3221e+02 - logprior: -1.1834e+00
Epoch 10/10
10/10 - 1s - loss: 332.9020 - loglik: -3.3178e+02 - logprior: -1.1265e+00
Fitted a model with MAP estimate = -332.8747
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 383.2961 - loglik: -3.2854e+02 - logprior: -5.4754e+01
Epoch 2/2
10/10 - 0s - loss: 340.3448 - loglik: -3.2371e+02 - logprior: -1.6631e+01
Fitted a model with MAP estimate = -332.6268
expansions: [(19, 1)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 370.9264 - loglik: -3.2372e+02 - logprior: -4.7206e+01
Epoch 2/2
10/10 - 1s - loss: 341.4775 - loglik: -3.2317e+02 - logprior: -1.8308e+01
Fitted a model with MAP estimate = -336.6252
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 365.3802 - loglik: -3.2283e+02 - logprior: -4.2551e+01
Epoch 2/10
10/10 - 1s - loss: 334.2555 - loglik: -3.2274e+02 - logprior: -1.1516e+01
Epoch 3/10
10/10 - 1s - loss: 326.4803 - loglik: -3.2205e+02 - logprior: -4.4266e+00
Epoch 4/10
10/10 - 1s - loss: 323.7284 - loglik: -3.2175e+02 - logprior: -1.9768e+00
Epoch 5/10
10/10 - 1s - loss: 322.5005 - loglik: -3.2157e+02 - logprior: -9.3161e-01
Epoch 6/10
10/10 - 1s - loss: 322.0719 - loglik: -3.2169e+02 - logprior: -3.8505e-01
Epoch 7/10
10/10 - 1s - loss: 321.1830 - loglik: -3.2135e+02 - logprior: 0.1622
Epoch 8/10
10/10 - 1s - loss: 321.3580 - loglik: -3.2193e+02 - logprior: 0.5687
Fitted a model with MAP estimate = -321.0942
Time for alignment: 27.7683
Computed alignments with likelihoods: ['-322.9782', '-321.4629', '-320.8261', '-321.0553', '-321.0942']
Best model has likelihood: -320.8261  (prior= 1.0238 )
time for generating output: 0.1012
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.8929076010172365
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 490.8520 - loglik: -4.7766e+02 - logprior: -1.3194e+01
Epoch 2/10
11/11 - 1s - loss: 461.9547 - loglik: -4.5853e+02 - logprior: -3.4265e+00
Epoch 3/10
11/11 - 1s - loss: 440.3749 - loglik: -4.3824e+02 - logprior: -2.1382e+00
Epoch 4/10
11/11 - 1s - loss: 429.3659 - loglik: -4.2734e+02 - logprior: -2.0222e+00
Epoch 5/10
11/11 - 1s - loss: 427.7277 - loglik: -4.2578e+02 - logprior: -1.9492e+00
Epoch 6/10
11/11 - 1s - loss: 425.7209 - loglik: -4.2395e+02 - logprior: -1.7699e+00
Epoch 7/10
11/11 - 1s - loss: 424.7161 - loglik: -4.2309e+02 - logprior: -1.6225e+00
Epoch 8/10
11/11 - 1s - loss: 424.3964 - loglik: -4.2281e+02 - logprior: -1.5908e+00
Epoch 9/10
11/11 - 1s - loss: 424.6344 - loglik: -4.2303e+02 - logprior: -1.6088e+00
Fitted a model with MAP estimate = -424.4506
expansions: [(11, 4), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 439.3605 - loglik: -4.2440e+02 - logprior: -1.4965e+01
Epoch 2/2
11/11 - 1s - loss: 423.8097 - loglik: -4.1741e+02 - logprior: -6.3954e+00
Fitted a model with MAP estimate = -421.3920
expansions: [(0, 2)]
discards: [ 0 15 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.6261 - loglik: -4.1585e+02 - logprior: -1.1781e+01
Epoch 2/2
11/11 - 1s - loss: 418.3064 - loglik: -4.1524e+02 - logprior: -3.0694e+00
Fitted a model with MAP estimate = -417.0634
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 431.2815 - loglik: -4.1706e+02 - logprior: -1.4221e+01
Epoch 2/10
11/11 - 1s - loss: 420.1555 - loglik: -4.1586e+02 - logprior: -4.2926e+00
Epoch 3/10
11/11 - 1s - loss: 417.4581 - loglik: -4.1536e+02 - logprior: -2.0958e+00
Epoch 4/10
11/11 - 1s - loss: 416.6647 - loglik: -4.1540e+02 - logprior: -1.2625e+00
Epoch 5/10
11/11 - 1s - loss: 415.0549 - loglik: -4.1403e+02 - logprior: -1.0283e+00
Epoch 6/10
11/11 - 1s - loss: 415.4932 - loglik: -4.1465e+02 - logprior: -8.4135e-01
Fitted a model with MAP estimate = -414.7828
Time for alignment: 42.4764
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 490.7009 - loglik: -4.7751e+02 - logprior: -1.3191e+01
Epoch 2/10
11/11 - 1s - loss: 460.6813 - loglik: -4.5726e+02 - logprior: -3.4237e+00
Epoch 3/10
11/11 - 1s - loss: 441.7531 - loglik: -4.3962e+02 - logprior: -2.1319e+00
Epoch 4/10
11/11 - 1s - loss: 430.0849 - loglik: -4.2808e+02 - logprior: -2.0090e+00
Epoch 5/10
11/11 - 1s - loss: 427.3112 - loglik: -4.2534e+02 - logprior: -1.9711e+00
Epoch 6/10
11/11 - 1s - loss: 426.2030 - loglik: -4.2440e+02 - logprior: -1.7990e+00
Epoch 7/10
11/11 - 1s - loss: 425.8287 - loglik: -4.2418e+02 - logprior: -1.6478e+00
Epoch 8/10
11/11 - 1s - loss: 425.0239 - loglik: -4.2342e+02 - logprior: -1.6079e+00
Epoch 9/10
11/11 - 1s - loss: 424.4478 - loglik: -4.2282e+02 - logprior: -1.6327e+00
Epoch 10/10
11/11 - 1s - loss: 424.7428 - loglik: -4.2310e+02 - logprior: -1.6433e+00
Fitted a model with MAP estimate = -424.3455
expansions: [(11, 5), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.1161 - loglik: -4.2314e+02 - logprior: -1.4973e+01
Epoch 2/2
11/11 - 1s - loss: 424.6907 - loglik: -4.1829e+02 - logprior: -6.3990e+00
Fitted a model with MAP estimate = -421.6770
expansions: [(0, 2)]
discards: [ 0 12 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.5235 - loglik: -4.1574e+02 - logprior: -1.1786e+01
Epoch 2/2
11/11 - 1s - loss: 418.1847 - loglik: -4.1512e+02 - logprior: -3.0674e+00
Fitted a model with MAP estimate = -417.0212
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 430.9989 - loglik: -4.1680e+02 - logprior: -1.4196e+01
Epoch 2/10
11/11 - 1s - loss: 420.7278 - loglik: -4.1645e+02 - logprior: -4.2797e+00
Epoch 3/10
11/11 - 1s - loss: 417.2904 - loglik: -4.1521e+02 - logprior: -2.0840e+00
Epoch 4/10
11/11 - 1s - loss: 416.5561 - loglik: -4.1531e+02 - logprior: -1.2501e+00
Epoch 5/10
11/11 - 1s - loss: 414.6836 - loglik: -4.1367e+02 - logprior: -1.0105e+00
Epoch 6/10
11/11 - 1s - loss: 415.9611 - loglik: -4.1515e+02 - logprior: -8.1216e-01
Fitted a model with MAP estimate = -414.8315
Time for alignment: 42.8242
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 491.0788 - loglik: -4.7789e+02 - logprior: -1.3192e+01
Epoch 2/10
11/11 - 1s - loss: 461.3718 - loglik: -4.5795e+02 - logprior: -3.4226e+00
Epoch 3/10
11/11 - 1s - loss: 441.3589 - loglik: -4.3922e+02 - logprior: -2.1436e+00
Epoch 4/10
11/11 - 1s - loss: 429.9783 - loglik: -4.2791e+02 - logprior: -2.0689e+00
Epoch 5/10
11/11 - 1s - loss: 427.8232 - loglik: -4.2578e+02 - logprior: -2.0467e+00
Epoch 6/10
11/11 - 1s - loss: 426.2623 - loglik: -4.2438e+02 - logprior: -1.8777e+00
Epoch 7/10
11/11 - 1s - loss: 425.4802 - loglik: -4.2376e+02 - logprior: -1.7236e+00
Epoch 8/10
11/11 - 1s - loss: 424.1578 - loglik: -4.2246e+02 - logprior: -1.6959e+00
Epoch 9/10
11/11 - 1s - loss: 424.6528 - loglik: -4.2292e+02 - logprior: -1.7308e+00
Fitted a model with MAP estimate = -424.4572
expansions: [(8, 3), (9, 1), (10, 1), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.7898 - loglik: -4.2380e+02 - logprior: -1.4990e+01
Epoch 2/2
11/11 - 1s - loss: 425.9260 - loglik: -4.1951e+02 - logprior: -6.4141e+00
Fitted a model with MAP estimate = -422.2717
expansions: [(0, 2)]
discards: [ 0  8 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.7281 - loglik: -4.1595e+02 - logprior: -1.1774e+01
Epoch 2/2
11/11 - 1s - loss: 419.1176 - loglik: -4.1604e+02 - logprior: -3.0783e+00
Fitted a model with MAP estimate = -417.1566
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 431.2824 - loglik: -4.1710e+02 - logprior: -1.4187e+01
Epoch 2/10
11/11 - 1s - loss: 420.5242 - loglik: -4.1626e+02 - logprior: -4.2644e+00
Epoch 3/10
11/11 - 1s - loss: 417.8640 - loglik: -4.1577e+02 - logprior: -2.0921e+00
Epoch 4/10
11/11 - 1s - loss: 415.7181 - loglik: -4.1446e+02 - logprior: -1.2598e+00
Epoch 5/10
11/11 - 1s - loss: 416.1782 - loglik: -4.1516e+02 - logprior: -1.0153e+00
Fitted a model with MAP estimate = -415.1922
Time for alignment: 39.1990
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 490.5007 - loglik: -4.7731e+02 - logprior: -1.3192e+01
Epoch 2/10
11/11 - 1s - loss: 462.2134 - loglik: -4.5879e+02 - logprior: -3.4236e+00
Epoch 3/10
11/11 - 1s - loss: 439.4711 - loglik: -4.3733e+02 - logprior: -2.1377e+00
Epoch 4/10
11/11 - 1s - loss: 430.3577 - loglik: -4.2834e+02 - logprior: -2.0156e+00
Epoch 5/10
11/11 - 1s - loss: 427.5337 - loglik: -4.2558e+02 - logprior: -1.9559e+00
Epoch 6/10
11/11 - 1s - loss: 426.4938 - loglik: -4.2470e+02 - logprior: -1.7939e+00
Epoch 7/10
11/11 - 1s - loss: 425.2971 - loglik: -4.2366e+02 - logprior: -1.6389e+00
Epoch 8/10
11/11 - 1s - loss: 425.2180 - loglik: -4.2362e+02 - logprior: -1.5998e+00
Epoch 9/10
11/11 - 1s - loss: 424.4732 - loglik: -4.2286e+02 - logprior: -1.6134e+00
Epoch 10/10
11/11 - 1s - loss: 426.3462 - loglik: -4.2473e+02 - logprior: -1.6154e+00
Fitted a model with MAP estimate = -424.8155
expansions: [(11, 5), (17, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 437.3257 - loglik: -4.2237e+02 - logprior: -1.4955e+01
Epoch 2/2
11/11 - 1s - loss: 426.4566 - loglik: -4.2012e+02 - logprior: -6.3409e+00
Fitted a model with MAP estimate = -421.6237
expansions: [(0, 2)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.8463 - loglik: -4.1608e+02 - logprior: -1.1770e+01
Epoch 2/2
11/11 - 1s - loss: 418.0537 - loglik: -4.1498e+02 - logprior: -3.0781e+00
Fitted a model with MAP estimate = -417.1149
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 431.0655 - loglik: -4.1684e+02 - logprior: -1.4224e+01
Epoch 2/10
11/11 - 1s - loss: 420.5032 - loglik: -4.1621e+02 - logprior: -4.2981e+00
Epoch 3/10
11/11 - 1s - loss: 417.8306 - loglik: -4.1574e+02 - logprior: -2.0952e+00
Epoch 4/10
11/11 - 1s - loss: 416.3795 - loglik: -4.1511e+02 - logprior: -1.2699e+00
Epoch 5/10
11/11 - 1s - loss: 415.8776 - loglik: -4.1487e+02 - logprior: -1.0080e+00
Epoch 6/10
11/11 - 1s - loss: 415.3580 - loglik: -4.1453e+02 - logprior: -8.2739e-01
Epoch 7/10
11/11 - 1s - loss: 414.0378 - loglik: -4.1328e+02 - logprior: -7.5460e-01
Epoch 8/10
11/11 - 1s - loss: 414.8755 - loglik: -4.1415e+02 - logprior: -7.2205e-01
Fitted a model with MAP estimate = -414.5233
Time for alignment: 42.8343
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 491.7231 - loglik: -4.7853e+02 - logprior: -1.3193e+01
Epoch 2/10
11/11 - 1s - loss: 460.4545 - loglik: -4.5704e+02 - logprior: -3.4192e+00
Epoch 3/10
11/11 - 1s - loss: 440.1663 - loglik: -4.3802e+02 - logprior: -2.1426e+00
Epoch 4/10
11/11 - 1s - loss: 430.8173 - loglik: -4.2878e+02 - logprior: -2.0414e+00
Epoch 5/10
11/11 - 1s - loss: 427.6004 - loglik: -4.2559e+02 - logprior: -2.0141e+00
Epoch 6/10
11/11 - 1s - loss: 425.2461 - loglik: -4.2338e+02 - logprior: -1.8658e+00
Epoch 7/10
11/11 - 1s - loss: 426.4411 - loglik: -4.2471e+02 - logprior: -1.7353e+00
Fitted a model with MAP estimate = -425.2654
expansions: [(8, 1), (10, 3), (11, 2), (17, 1), (24, 1), (25, 1), (32, 1), (34, 1), (35, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.6101 - loglik: -4.2364e+02 - logprior: -1.4974e+01
Epoch 2/2
11/11 - 1s - loss: 426.4102 - loglik: -4.2006e+02 - logprior: -6.3453e+00
Fitted a model with MAP estimate = -421.7394
expansions: [(0, 2)]
discards: [ 0 14 15]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 428.3596 - loglik: -4.1657e+02 - logprior: -1.1786e+01
Epoch 2/2
11/11 - 1s - loss: 418.3086 - loglik: -4.1523e+02 - logprior: -3.0831e+00
Fitted a model with MAP estimate = -417.0605
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 430.7052 - loglik: -4.1652e+02 - logprior: -1.4181e+01
Epoch 2/10
11/11 - 1s - loss: 420.9512 - loglik: -4.1669e+02 - logprior: -4.2641e+00
Epoch 3/10
11/11 - 1s - loss: 417.5506 - loglik: -4.1544e+02 - logprior: -2.1056e+00
Epoch 4/10
11/11 - 1s - loss: 416.0136 - loglik: -4.1474e+02 - logprior: -1.2742e+00
Epoch 5/10
11/11 - 1s - loss: 415.5747 - loglik: -4.1455e+02 - logprior: -1.0279e+00
Epoch 6/10
11/11 - 1s - loss: 415.0919 - loglik: -4.1426e+02 - logprior: -8.3026e-01
Epoch 7/10
11/11 - 1s - loss: 414.5844 - loglik: -4.1382e+02 - logprior: -7.6597e-01
Epoch 8/10
11/11 - 1s - loss: 414.8536 - loglik: -4.1413e+02 - logprior: -7.2645e-01
Fitted a model with MAP estimate = -414.4985
Time for alignment: 40.9239
Computed alignments with likelihoods: ['-414.7828', '-414.8315', '-415.1922', '-414.5233', '-414.4985']
Best model has likelihood: -414.4985  (prior= -0.7066 )
time for generating output: 0.1313
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.829308909242298
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 500.8281 - loglik: -4.6018e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 443.9965 - loglik: -4.3322e+02 - logprior: -1.0772e+01
Epoch 3/10
10/10 - 1s - loss: 413.6633 - loglik: -4.0821e+02 - logprior: -5.4521e+00
Epoch 4/10
10/10 - 1s - loss: 391.4622 - loglik: -3.8771e+02 - logprior: -3.7486e+00
Epoch 5/10
10/10 - 1s - loss: 384.0677 - loglik: -3.8106e+02 - logprior: -3.0105e+00
Epoch 6/10
10/10 - 1s - loss: 381.5345 - loglik: -3.7901e+02 - logprior: -2.5293e+00
Epoch 7/10
10/10 - 1s - loss: 380.3313 - loglik: -3.7813e+02 - logprior: -2.2020e+00
Epoch 8/10
10/10 - 1s - loss: 379.4850 - loglik: -3.7745e+02 - logprior: -2.0327e+00
Epoch 9/10
10/10 - 1s - loss: 379.0390 - loglik: -3.7712e+02 - logprior: -1.9160e+00
Epoch 10/10
10/10 - 1s - loss: 378.8342 - loglik: -3.7699e+02 - logprior: -1.8468e+00
Fitted a model with MAP estimate = -378.6707
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.3991 - loglik: -3.6964e+02 - logprior: -3.6763e+01
Epoch 2/2
10/10 - 1s - loss: 373.2509 - loglik: -3.6369e+02 - logprior: -9.5594e+00
Fitted a model with MAP estimate = -368.5038
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 413.0732 - loglik: -3.6788e+02 - logprior: -4.5192e+01
Epoch 2/2
10/10 - 1s - loss: 385.2792 - loglik: -3.6681e+02 - logprior: -1.8468e+01
Fitted a model with MAP estimate = -381.1895
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 400.7357 - loglik: -3.6448e+02 - logprior: -3.6260e+01
Epoch 2/10
10/10 - 1s - loss: 372.5654 - loglik: -3.6356e+02 - logprior: -9.0033e+00
Epoch 3/10
10/10 - 1s - loss: 366.0275 - loglik: -3.6255e+02 - logprior: -3.4741e+00
Epoch 4/10
10/10 - 1s - loss: 363.9251 - loglik: -3.6266e+02 - logprior: -1.2685e+00
Epoch 5/10
10/10 - 1s - loss: 363.6252 - loglik: -3.6350e+02 - logprior: -1.2703e-01
Epoch 6/10
10/10 - 1s - loss: 362.6457 - loglik: -3.6313e+02 - logprior: 0.4856
Epoch 7/10
10/10 - 1s - loss: 362.0539 - loglik: -3.6288e+02 - logprior: 0.8291
Epoch 8/10
10/10 - 1s - loss: 362.5215 - loglik: -3.6359e+02 - logprior: 1.0702
Fitted a model with MAP estimate = -362.0515
Time for alignment: 30.2456
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 500.2206 - loglik: -4.5957e+02 - logprior: -4.0647e+01
Epoch 2/10
10/10 - 1s - loss: 444.9798 - loglik: -4.3421e+02 - logprior: -1.0766e+01
Epoch 3/10
10/10 - 1s - loss: 412.8739 - loglik: -4.0745e+02 - logprior: -5.4277e+00
Epoch 4/10
10/10 - 1s - loss: 391.7663 - loglik: -3.8806e+02 - logprior: -3.7047e+00
Epoch 5/10
10/10 - 1s - loss: 384.5245 - loglik: -3.8158e+02 - logprior: -2.9492e+00
Epoch 6/10
10/10 - 1s - loss: 381.3980 - loglik: -3.7887e+02 - logprior: -2.5240e+00
Epoch 7/10
10/10 - 1s - loss: 380.8997 - loglik: -3.7871e+02 - logprior: -2.1934e+00
Epoch 8/10
10/10 - 1s - loss: 379.7904 - loglik: -3.7779e+02 - logprior: -1.9979e+00
Epoch 9/10
10/10 - 1s - loss: 379.4593 - loglik: -3.7760e+02 - logprior: -1.8570e+00
Epoch 10/10
10/10 - 1s - loss: 379.3514 - loglik: -3.7756e+02 - logprior: -1.7896e+00
Fitted a model with MAP estimate = -379.0578
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 406.7673 - loglik: -3.7000e+02 - logprior: -3.6768e+01
Epoch 2/2
10/10 - 1s - loss: 373.6589 - loglik: -3.6410e+02 - logprior: -9.5605e+00
Fitted a model with MAP estimate = -368.4987
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 412.9362 - loglik: -3.6776e+02 - logprior: -4.5177e+01
Epoch 2/2
10/10 - 1s - loss: 385.1826 - loglik: -3.6671e+02 - logprior: -1.8468e+01
Fitted a model with MAP estimate = -381.1715
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 400.8500 - loglik: -3.6464e+02 - logprior: -3.6214e+01
Epoch 2/10
10/10 - 1s - loss: 372.0284 - loglik: -3.6304e+02 - logprior: -8.9846e+00
Epoch 3/10
10/10 - 1s - loss: 366.3340 - loglik: -3.6287e+02 - logprior: -3.4674e+00
Epoch 4/10
10/10 - 1s - loss: 363.9669 - loglik: -3.6270e+02 - logprior: -1.2651e+00
Epoch 5/10
10/10 - 1s - loss: 363.2903 - loglik: -3.6317e+02 - logprior: -1.2512e-01
Epoch 6/10
10/10 - 1s - loss: 362.7054 - loglik: -3.6319e+02 - logprior: 0.4845
Epoch 7/10
10/10 - 1s - loss: 362.1512 - loglik: -3.6298e+02 - logprior: 0.8298
Epoch 8/10
10/10 - 1s - loss: 362.2090 - loglik: -3.6328e+02 - logprior: 1.0743
Fitted a model with MAP estimate = -362.0459
Time for alignment: 29.3273
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 500.7283 - loglik: -4.6008e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 444.8342 - loglik: -4.3408e+02 - logprior: -1.0759e+01
Epoch 3/10
10/10 - 1s - loss: 413.3983 - loglik: -4.0802e+02 - logprior: -5.3826e+00
Epoch 4/10
10/10 - 1s - loss: 393.3004 - loglik: -3.8970e+02 - logprior: -3.6051e+00
Epoch 5/10
10/10 - 1s - loss: 384.8735 - loglik: -3.8207e+02 - logprior: -2.8084e+00
Epoch 6/10
10/10 - 1s - loss: 382.1554 - loglik: -3.7968e+02 - logprior: -2.4723e+00
Epoch 7/10
10/10 - 1s - loss: 380.8622 - loglik: -3.7868e+02 - logprior: -2.1860e+00
Epoch 8/10
10/10 - 1s - loss: 379.8974 - loglik: -3.7793e+02 - logprior: -1.9703e+00
Epoch 9/10
10/10 - 1s - loss: 379.8967 - loglik: -3.7805e+02 - logprior: -1.8493e+00
Epoch 10/10
10/10 - 1s - loss: 379.3956 - loglik: -3.7765e+02 - logprior: -1.7442e+00
Fitted a model with MAP estimate = -379.3279
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 407.4614 - loglik: -3.7070e+02 - logprior: -3.6758e+01
Epoch 2/2
10/10 - 1s - loss: 373.7553 - loglik: -3.6423e+02 - logprior: -9.5205e+00
Fitted a model with MAP estimate = -368.6564
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 413.2330 - loglik: -3.6803e+02 - logprior: -4.5200e+01
Epoch 2/2
10/10 - 1s - loss: 385.2457 - loglik: -3.6679e+02 - logprior: -1.8458e+01
Fitted a model with MAP estimate = -381.2340
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 400.9664 - loglik: -3.6464e+02 - logprior: -3.6324e+01
Epoch 2/10
10/10 - 1s - loss: 371.9507 - loglik: -3.6293e+02 - logprior: -9.0179e+00
Epoch 3/10
10/10 - 1s - loss: 366.5860 - loglik: -3.6311e+02 - logprior: -3.4780e+00
Epoch 4/10
10/10 - 1s - loss: 364.2128 - loglik: -3.6294e+02 - logprior: -1.2716e+00
Epoch 5/10
10/10 - 1s - loss: 363.0468 - loglik: -3.6292e+02 - logprior: -1.2821e-01
Epoch 6/10
10/10 - 1s - loss: 362.8749 - loglik: -3.6336e+02 - logprior: 0.4833
Epoch 7/10
10/10 - 1s - loss: 362.1929 - loglik: -3.6303e+02 - logprior: 0.8369
Epoch 8/10
10/10 - 1s - loss: 362.3639 - loglik: -3.6344e+02 - logprior: 1.0722
Fitted a model with MAP estimate = -362.0890
Time for alignment: 29.6277
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 500.9490 - loglik: -4.6030e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 444.3466 - loglik: -4.3359e+02 - logprior: -1.0757e+01
Epoch 3/10
10/10 - 1s - loss: 413.4201 - loglik: -4.0803e+02 - logprior: -5.3908e+00
Epoch 4/10
10/10 - 1s - loss: 391.3417 - loglik: -3.8765e+02 - logprior: -3.6966e+00
Epoch 5/10
10/10 - 1s - loss: 384.2836 - loglik: -3.8132e+02 - logprior: -2.9652e+00
Epoch 6/10
10/10 - 1s - loss: 381.3766 - loglik: -3.7889e+02 - logprior: -2.4903e+00
Epoch 7/10
10/10 - 1s - loss: 380.6956 - loglik: -3.7854e+02 - logprior: -2.1606e+00
Epoch 8/10
10/10 - 1s - loss: 379.8669 - loglik: -3.7789e+02 - logprior: -1.9760e+00
Epoch 9/10
10/10 - 1s - loss: 379.6959 - loglik: -3.7785e+02 - logprior: -1.8505e+00
Epoch 10/10
10/10 - 1s - loss: 379.5381 - loglik: -3.7777e+02 - logprior: -1.7720e+00
Fitted a model with MAP estimate = -379.2167
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 407.4673 - loglik: -3.7070e+02 - logprior: -3.6771e+01
Epoch 2/2
10/10 - 1s - loss: 373.1740 - loglik: -3.6361e+02 - logprior: -9.5602e+00
Fitted a model with MAP estimate = -368.6072
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 413.1037 - loglik: -3.6793e+02 - logprior: -4.5170e+01
Epoch 2/2
10/10 - 1s - loss: 385.2336 - loglik: -3.6677e+02 - logprior: -1.8464e+01
Fitted a model with MAP estimate = -381.1751
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 400.6531 - loglik: -3.6445e+02 - logprior: -3.6205e+01
Epoch 2/10
10/10 - 1s - loss: 372.2104 - loglik: -3.6322e+02 - logprior: -8.9862e+00
Epoch 3/10
10/10 - 1s - loss: 366.4801 - loglik: -3.6301e+02 - logprior: -3.4697e+00
Epoch 4/10
10/10 - 1s - loss: 363.8835 - loglik: -3.6262e+02 - logprior: -1.2664e+00
Epoch 5/10
10/10 - 1s - loss: 363.4239 - loglik: -3.6330e+02 - logprior: -1.2424e-01
Epoch 6/10
10/10 - 1s - loss: 362.5934 - loglik: -3.6308e+02 - logprior: 0.4859
Epoch 7/10
10/10 - 1s - loss: 362.3922 - loglik: -3.6323e+02 - logprior: 0.8335
Epoch 8/10
10/10 - 1s - loss: 362.1494 - loglik: -3.6323e+02 - logprior: 1.0758
Epoch 9/10
10/10 - 1s - loss: 361.8076 - loglik: -3.6309e+02 - logprior: 1.2808
Epoch 10/10
10/10 - 1s - loss: 362.0302 - loglik: -3.6349e+02 - logprior: 1.4551
Fitted a model with MAP estimate = -361.7876
Time for alignment: 31.4774
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 500.5607 - loglik: -4.5992e+02 - logprior: -4.0646e+01
Epoch 2/10
10/10 - 1s - loss: 444.7390 - loglik: -4.3397e+02 - logprior: -1.0771e+01
Epoch 3/10
10/10 - 1s - loss: 414.8781 - loglik: -4.0939e+02 - logprior: -5.4918e+00
Epoch 4/10
10/10 - 1s - loss: 393.2742 - loglik: -3.8948e+02 - logprior: -3.7943e+00
Epoch 5/10
10/10 - 1s - loss: 384.6339 - loglik: -3.8157e+02 - logprior: -3.0655e+00
Epoch 6/10
10/10 - 1s - loss: 381.4720 - loglik: -3.7891e+02 - logprior: -2.5652e+00
Epoch 7/10
10/10 - 1s - loss: 380.1784 - loglik: -3.7795e+02 - logprior: -2.2274e+00
Epoch 8/10
10/10 - 1s - loss: 379.7221 - loglik: -3.7765e+02 - logprior: -2.0698e+00
Epoch 9/10
10/10 - 1s - loss: 378.6802 - loglik: -3.7673e+02 - logprior: -1.9491e+00
Epoch 10/10
10/10 - 1s - loss: 379.2463 - loglik: -3.7738e+02 - logprior: -1.8616e+00
Fitted a model with MAP estimate = -378.7064
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 406.5943 - loglik: -3.6982e+02 - logprior: -3.6775e+01
Epoch 2/2
10/10 - 1s - loss: 372.9630 - loglik: -3.6341e+02 - logprior: -9.5579e+00
Fitted a model with MAP estimate = -368.5066
expansions: []
discards: [ 0 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 413.0326 - loglik: -3.6780e+02 - logprior: -4.5228e+01
Epoch 2/2
10/10 - 1s - loss: 385.6085 - loglik: -3.6714e+02 - logprior: -1.8472e+01
Fitted a model with MAP estimate = -381.2067
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 400.9487 - loglik: -3.6458e+02 - logprior: -3.6367e+01
Epoch 2/10
10/10 - 1s - loss: 372.1775 - loglik: -3.6315e+02 - logprior: -9.0232e+00
Epoch 3/10
10/10 - 1s - loss: 366.1951 - loglik: -3.6272e+02 - logprior: -3.4794e+00
Epoch 4/10
10/10 - 1s - loss: 364.4139 - loglik: -3.6315e+02 - logprior: -1.2664e+00
Epoch 5/10
10/10 - 1s - loss: 362.8949 - loglik: -3.6277e+02 - logprior: -1.2277e-01
Epoch 6/10
10/10 - 1s - loss: 362.7657 - loglik: -3.6326e+02 - logprior: 0.4920
Epoch 7/10
10/10 - 1s - loss: 362.2451 - loglik: -3.6309e+02 - logprior: 0.8405
Epoch 8/10
10/10 - 1s - loss: 362.5655 - loglik: -3.6365e+02 - logprior: 1.0824
Fitted a model with MAP estimate = -362.0435
Time for alignment: 30.0374
Computed alignments with likelihoods: ['-362.0515', '-362.0459', '-362.0890', '-361.7876', '-362.0435']
Best model has likelihood: -361.7876  (prior= 1.5328 )
time for generating output: 0.1246
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.8564831261101243
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 349.3793 - loglik: -3.4376e+02 - logprior: -5.6224e+00
Epoch 2/10
15/15 - 1s - loss: 322.5477 - loglik: -3.2078e+02 - logprior: -1.7705e+00
Epoch 3/10
15/15 - 1s - loss: 308.6421 - loglik: -3.0685e+02 - logprior: -1.7966e+00
Epoch 4/10
15/15 - 1s - loss: 304.8180 - loglik: -3.0308e+02 - logprior: -1.7332e+00
Epoch 5/10
15/15 - 1s - loss: 303.5385 - loglik: -3.0191e+02 - logprior: -1.6246e+00
Epoch 6/10
15/15 - 1s - loss: 303.2602 - loglik: -3.0162e+02 - logprior: -1.6444e+00
Epoch 7/10
15/15 - 1s - loss: 303.2339 - loglik: -3.0161e+02 - logprior: -1.6220e+00
Epoch 8/10
15/15 - 1s - loss: 302.9554 - loglik: -3.0136e+02 - logprior: -1.5985e+00
Epoch 9/10
15/15 - 1s - loss: 302.8126 - loglik: -3.0122e+02 - logprior: -1.5923e+00
Epoch 10/10
15/15 - 1s - loss: 303.1270 - loglik: -3.0154e+02 - logprior: -1.5874e+00
Fitted a model with MAP estimate = -302.7682
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (28, 1), (29, 2), (30, 2), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 312.5481 - loglik: -3.0561e+02 - logprior: -6.9331e+00
Epoch 2/2
15/15 - 1s - loss: 303.7521 - loglik: -3.0042e+02 - logprior: -3.3336e+00
Fitted a model with MAP estimate = -301.9838
expansions: [(0, 2)]
discards: [ 0 12 15 48]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.5363 - loglik: -2.9943e+02 - logprior: -5.1112e+00
Epoch 2/2
15/15 - 1s - loss: 298.5728 - loglik: -2.9691e+02 - logprior: -1.6634e+00
Fitted a model with MAP estimate = -297.7814
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 306.0901 - loglik: -2.9952e+02 - logprior: -6.5701e+00
Epoch 2/10
15/15 - 1s - loss: 299.2272 - loglik: -2.9710e+02 - logprior: -2.1295e+00
Epoch 3/10
15/15 - 1s - loss: 297.9266 - loglik: -2.9651e+02 - logprior: -1.4205e+00
Epoch 4/10
15/15 - 1s - loss: 297.6943 - loglik: -2.9645e+02 - logprior: -1.2414e+00
Epoch 5/10
15/15 - 1s - loss: 297.3924 - loglik: -2.9621e+02 - logprior: -1.1833e+00
Epoch 6/10
15/15 - 1s - loss: 297.4567 - loglik: -2.9630e+02 - logprior: -1.1533e+00
Fitted a model with MAP estimate = -297.1917
Time for alignment: 34.1431
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.4012 - loglik: -3.4378e+02 - logprior: -5.6207e+00
Epoch 2/10
15/15 - 1s - loss: 321.9832 - loglik: -3.2022e+02 - logprior: -1.7642e+00
Epoch 3/10
15/15 - 1s - loss: 308.1893 - loglik: -3.0641e+02 - logprior: -1.7776e+00
Epoch 4/10
15/15 - 1s - loss: 304.3519 - loglik: -3.0264e+02 - logprior: -1.7095e+00
Epoch 5/10
15/15 - 1s - loss: 303.5624 - loglik: -3.0197e+02 - logprior: -1.5924e+00
Epoch 6/10
15/15 - 1s - loss: 303.3526 - loglik: -3.0175e+02 - logprior: -1.6018e+00
Epoch 7/10
15/15 - 1s - loss: 303.0770 - loglik: -3.0150e+02 - logprior: -1.5776e+00
Epoch 8/10
15/15 - 1s - loss: 303.0689 - loglik: -3.0151e+02 - logprior: -1.5563e+00
Epoch 9/10
15/15 - 1s - loss: 303.1035 - loglik: -3.0155e+02 - logprior: -1.5496e+00
Fitted a model with MAP estimate = -302.9613
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (24, 1), (29, 2), (30, 2), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 312.0473 - loglik: -3.0512e+02 - logprior: -6.9303e+00
Epoch 2/2
15/15 - 1s - loss: 304.0208 - loglik: -3.0069e+02 - logprior: -3.3327e+00
Fitted a model with MAP estimate = -301.8795
expansions: [(0, 2)]
discards: [ 0 11 15 48]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.9585 - loglik: -2.9983e+02 - logprior: -5.1235e+00
Epoch 2/2
15/15 - 1s - loss: 298.3255 - loglik: -2.9663e+02 - logprior: -1.6985e+00
Fitted a model with MAP estimate = -297.7788
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 306.1257 - loglik: -2.9953e+02 - logprior: -6.5980e+00
Epoch 2/10
15/15 - 1s - loss: 299.1065 - loglik: -2.9693e+02 - logprior: -2.1720e+00
Epoch 3/10
15/15 - 1s - loss: 297.8575 - loglik: -2.9642e+02 - logprior: -1.4329e+00
Epoch 4/10
15/15 - 1s - loss: 297.4232 - loglik: -2.9615e+02 - logprior: -1.2700e+00
Epoch 5/10
15/15 - 1s - loss: 297.4767 - loglik: -2.9627e+02 - logprior: -1.2103e+00
Fitted a model with MAP estimate = -297.2222
Time for alignment: 32.9795
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 349.4154 - loglik: -3.4379e+02 - logprior: -5.6223e+00
Epoch 2/10
15/15 - 1s - loss: 322.1075 - loglik: -3.2033e+02 - logprior: -1.7781e+00
Epoch 3/10
15/15 - 1s - loss: 307.9033 - loglik: -3.0612e+02 - logprior: -1.7840e+00
Epoch 4/10
15/15 - 1s - loss: 304.6942 - loglik: -3.0296e+02 - logprior: -1.7367e+00
Epoch 5/10
15/15 - 1s - loss: 303.4173 - loglik: -3.0178e+02 - logprior: -1.6407e+00
Epoch 6/10
15/15 - 1s - loss: 303.2794 - loglik: -3.0163e+02 - logprior: -1.6518e+00
Epoch 7/10
15/15 - 1s - loss: 303.1692 - loglik: -3.0154e+02 - logprior: -1.6302e+00
Epoch 8/10
15/15 - 1s - loss: 302.9234 - loglik: -3.0131e+02 - logprior: -1.6112e+00
Epoch 9/10
15/15 - 1s - loss: 303.1022 - loglik: -3.0150e+02 - logprior: -1.6068e+00
Fitted a model with MAP estimate = -302.7813
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 313.0603 - loglik: -3.0610e+02 - logprior: -6.9557e+00
Epoch 2/2
15/15 - 1s - loss: 303.9355 - loglik: -3.0057e+02 - logprior: -3.3661e+00
Fitted a model with MAP estimate = -302.0173
expansions: [(0, 2)]
discards: [ 0 12 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 304.9093 - loglik: -2.9979e+02 - logprior: -5.1209e+00
Epoch 2/2
15/15 - 1s - loss: 298.7660 - loglik: -2.9710e+02 - logprior: -1.6694e+00
Fitted a model with MAP estimate = -297.8262
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 306.2607 - loglik: -2.9967e+02 - logprior: -6.5885e+00
Epoch 2/10
15/15 - 1s - loss: 299.2179 - loglik: -2.9705e+02 - logprior: -2.1630e+00
Epoch 3/10
15/15 - 1s - loss: 297.8568 - loglik: -2.9644e+02 - logprior: -1.4160e+00
Epoch 4/10
15/15 - 1s - loss: 297.7830 - loglik: -2.9654e+02 - logprior: -1.2472e+00
Epoch 5/10
15/15 - 1s - loss: 297.3187 - loglik: -2.9614e+02 - logprior: -1.1817e+00
Epoch 6/10
15/15 - 1s - loss: 297.1729 - loglik: -2.9602e+02 - logprior: -1.1565e+00
Epoch 7/10
15/15 - 1s - loss: 297.2464 - loglik: -2.9611e+02 - logprior: -1.1401e+00
Fitted a model with MAP estimate = -297.1823
Time for alignment: 34.0629
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 349.5890 - loglik: -3.4397e+02 - logprior: -5.6202e+00
Epoch 2/10
15/15 - 1s - loss: 322.4312 - loglik: -3.2067e+02 - logprior: -1.7593e+00
Epoch 3/10
15/15 - 1s - loss: 307.8879 - loglik: -3.0611e+02 - logprior: -1.7796e+00
Epoch 4/10
15/15 - 1s - loss: 304.4585 - loglik: -3.0273e+02 - logprior: -1.7275e+00
Epoch 5/10
15/15 - 1s - loss: 303.4174 - loglik: -3.0179e+02 - logprior: -1.6243e+00
Epoch 6/10
15/15 - 1s - loss: 303.0985 - loglik: -3.0146e+02 - logprior: -1.6420e+00
Epoch 7/10
15/15 - 1s - loss: 303.2472 - loglik: -3.0162e+02 - logprior: -1.6291e+00
Fitted a model with MAP estimate = -302.9071
expansions: [(9, 2), (10, 1), (11, 2), (12, 1), (13, 1), (15, 2), (24, 2), (29, 2), (30, 2), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 313.5682 - loglik: -3.0661e+02 - logprior: -6.9567e+00
Epoch 2/2
15/15 - 1s - loss: 304.1309 - loglik: -3.0078e+02 - logprior: -3.3505e+00
Fitted a model with MAP estimate = -301.7187
expansions: []
discards: [ 9 13 21 32 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 305.0299 - loglik: -2.9905e+02 - logprior: -5.9790e+00
Epoch 2/2
15/15 - 1s - loss: 298.7643 - loglik: -2.9682e+02 - logprior: -1.9396e+00
Fitted a model with MAP estimate = -298.1864
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 302.7793 - loglik: -2.9735e+02 - logprior: -5.4253e+00
Epoch 2/10
15/15 - 1s - loss: 298.4014 - loglik: -2.9654e+02 - logprior: -1.8648e+00
Epoch 3/10
15/15 - 1s - loss: 298.1488 - loglik: -2.9674e+02 - logprior: -1.4080e+00
Epoch 4/10
15/15 - 1s - loss: 297.4925 - loglik: -2.9625e+02 - logprior: -1.2424e+00
Epoch 5/10
15/15 - 1s - loss: 297.4997 - loglik: -2.9631e+02 - logprior: -1.1849e+00
Fitted a model with MAP estimate = -297.3338
Time for alignment: 30.1940
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 349.3820 - loglik: -3.4376e+02 - logprior: -5.6227e+00
Epoch 2/10
15/15 - 1s - loss: 322.1544 - loglik: -3.2039e+02 - logprior: -1.7654e+00
Epoch 3/10
15/15 - 1s - loss: 308.0748 - loglik: -3.0630e+02 - logprior: -1.7700e+00
Epoch 4/10
15/15 - 1s - loss: 304.7115 - loglik: -3.0301e+02 - logprior: -1.6997e+00
Epoch 5/10
15/15 - 1s - loss: 303.7044 - loglik: -3.0212e+02 - logprior: -1.5838e+00
Epoch 6/10
15/15 - 1s - loss: 303.5384 - loglik: -3.0194e+02 - logprior: -1.6001e+00
Epoch 7/10
15/15 - 1s - loss: 303.2083 - loglik: -3.0163e+02 - logprior: -1.5808e+00
Epoch 8/10
15/15 - 1s - loss: 303.7616 - loglik: -3.0221e+02 - logprior: -1.5559e+00
Fitted a model with MAP estimate = -303.2458
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 312.9308 - loglik: -3.0597e+02 - logprior: -6.9597e+00
Epoch 2/2
15/15 - 1s - loss: 303.9909 - loglik: -3.0063e+02 - logprior: -3.3623e+00
Fitted a model with MAP estimate = -301.8910
expansions: []
discards: [11 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 304.9651 - loglik: -2.9893e+02 - logprior: -6.0353e+00
Epoch 2/2
15/15 - 1s - loss: 298.9818 - loglik: -2.9700e+02 - logprior: -1.9777e+00
Fitted a model with MAP estimate = -298.0999
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 302.7759 - loglik: -2.9733e+02 - logprior: -5.4429e+00
Epoch 2/10
15/15 - 1s - loss: 298.6162 - loglik: -2.9675e+02 - logprior: -1.8700e+00
Epoch 3/10
15/15 - 1s - loss: 298.0630 - loglik: -2.9665e+02 - logprior: -1.4128e+00
Epoch 4/10
15/15 - 1s - loss: 297.5302 - loglik: -2.9627e+02 - logprior: -1.2632e+00
Epoch 5/10
15/15 - 1s - loss: 297.4838 - loglik: -2.9628e+02 - logprior: -1.2015e+00
Epoch 6/10
15/15 - 1s - loss: 297.4704 - loglik: -2.9630e+02 - logprior: -1.1659e+00
Epoch 7/10
15/15 - 1s - loss: 297.3268 - loglik: -2.9618e+02 - logprior: -1.1508e+00
Epoch 8/10
15/15 - 1s - loss: 297.3185 - loglik: -2.9619e+02 - logprior: -1.1290e+00
Epoch 9/10
15/15 - 1s - loss: 297.1852 - loglik: -2.9607e+02 - logprior: -1.1107e+00
Epoch 10/10
15/15 - 1s - loss: 297.5035 - loglik: -2.9641e+02 - logprior: -1.0931e+00
Fitted a model with MAP estimate = -297.1591
Time for alignment: 35.5551
Computed alignments with likelihoods: ['-297.1917', '-297.2222', '-297.1823', '-297.3338', '-297.1591']
Best model has likelihood: -297.1591  (prior= -1.0495 )
time for generating output: 0.1000
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 383.4172 - loglik: -3.6909e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 2s - loss: 356.8505 - loglik: -3.5275e+02 - logprior: -4.0972e+00
Epoch 3/10
10/10 - 2s - loss: 337.6413 - loglik: -3.3514e+02 - logprior: -2.5038e+00
Epoch 4/10
10/10 - 1s - loss: 325.9085 - loglik: -3.2367e+02 - logprior: -2.2413e+00
Epoch 5/10
10/10 - 1s - loss: 322.0459 - loglik: -3.1979e+02 - logprior: -2.2553e+00
Epoch 6/10
10/10 - 1s - loss: 319.0038 - loglik: -3.1667e+02 - logprior: -2.3374e+00
Epoch 7/10
10/10 - 2s - loss: 319.1500 - loglik: -3.1684e+02 - logprior: -2.3083e+00
Fitted a model with MAP estimate = -317.6215
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 332.9683 - loglik: -3.1694e+02 - logprior: -1.6026e+01
Epoch 2/2
10/10 - 1s - loss: 314.0525 - loglik: -3.0711e+02 - logprior: -6.9460e+00
Fitted a model with MAP estimate = -311.3164
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.0691 - loglik: -3.0542e+02 - logprior: -1.2647e+01
Epoch 2/2
10/10 - 1s - loss: 306.9288 - loglik: -3.0347e+02 - logprior: -3.4624e+00
Fitted a model with MAP estimate = -306.1394
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 321.1891 - loglik: -3.0614e+02 - logprior: -1.5050e+01
Epoch 2/10
10/10 - 1s - loss: 309.7474 - loglik: -3.0487e+02 - logprior: -4.8765e+00
Epoch 3/10
10/10 - 1s - loss: 306.4825 - loglik: -3.0398e+02 - logprior: -2.5068e+00
Epoch 4/10
10/10 - 2s - loss: 305.4404 - loglik: -3.0387e+02 - logprior: -1.5752e+00
Epoch 5/10
10/10 - 1s - loss: 304.4424 - loglik: -3.0314e+02 - logprior: -1.2995e+00
Epoch 6/10
10/10 - 1s - loss: 304.6219 - loglik: -3.0356e+02 - logprior: -1.0574e+00
Fitted a model with MAP estimate = -304.0859
Time for alignment: 43.7637
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 382.6000 - loglik: -3.6827e+02 - logprior: -1.4331e+01
Epoch 2/10
10/10 - 1s - loss: 357.6234 - loglik: -3.5351e+02 - logprior: -4.1093e+00
Epoch 3/10
10/10 - 1s - loss: 338.2451 - loglik: -3.3570e+02 - logprior: -2.5411e+00
Epoch 4/10
10/10 - 1s - loss: 328.1585 - loglik: -3.2584e+02 - logprior: -2.3148e+00
Epoch 5/10
10/10 - 1s - loss: 321.5387 - loglik: -3.1921e+02 - logprior: -2.3257e+00
Epoch 6/10
10/10 - 2s - loss: 320.1300 - loglik: -3.1777e+02 - logprior: -2.3589e+00
Epoch 7/10
10/10 - 1s - loss: 318.4969 - loglik: -3.1621e+02 - logprior: -2.2877e+00
Epoch 8/10
10/10 - 2s - loss: 319.1014 - loglik: -3.1694e+02 - logprior: -2.1580e+00
Fitted a model with MAP estimate = -318.3648
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (38, 2), (39, 2), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.4337 - loglik: -3.1840e+02 - logprior: -1.6037e+01
Epoch 2/2
10/10 - 1s - loss: 315.3853 - loglik: -3.0835e+02 - logprior: -7.0370e+00
Fitted a model with MAP estimate = -311.9632
expansions: [(0, 2)]
discards: [ 0 10 13 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 317.7145 - loglik: -3.0504e+02 - logprior: -1.2674e+01
Epoch 2/2
10/10 - 2s - loss: 308.8781 - loglik: -3.0542e+02 - logprior: -3.4583e+00
Fitted a model with MAP estimate = -306.1972
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 321.1649 - loglik: -3.0612e+02 - logprior: -1.5043e+01
Epoch 2/10
10/10 - 1s - loss: 309.6328 - loglik: -3.0477e+02 - logprior: -4.8663e+00
Epoch 3/10
10/10 - 2s - loss: 306.6931 - loglik: -3.0419e+02 - logprior: -2.5065e+00
Epoch 4/10
10/10 - 1s - loss: 304.6887 - loglik: -3.0311e+02 - logprior: -1.5738e+00
Epoch 5/10
10/10 - 1s - loss: 305.0536 - loglik: -3.0376e+02 - logprior: -1.2979e+00
Fitted a model with MAP estimate = -304.3532
Time for alignment: 43.0013
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 383.5242 - loglik: -3.6919e+02 - logprior: -1.4332e+01
Epoch 2/10
10/10 - 1s - loss: 356.1189 - loglik: -3.5201e+02 - logprior: -4.1127e+00
Epoch 3/10
10/10 - 1s - loss: 339.4856 - loglik: -3.3693e+02 - logprior: -2.5541e+00
Epoch 4/10
10/10 - 1s - loss: 327.7240 - loglik: -3.2539e+02 - logprior: -2.3373e+00
Epoch 5/10
10/10 - 1s - loss: 322.3224 - loglik: -3.1994e+02 - logprior: -2.3838e+00
Epoch 6/10
10/10 - 1s - loss: 318.8414 - loglik: -3.1635e+02 - logprior: -2.4923e+00
Epoch 7/10
10/10 - 1s - loss: 317.7287 - loglik: -3.1525e+02 - logprior: -2.4753e+00
Epoch 8/10
10/10 - 2s - loss: 316.8620 - loglik: -3.1452e+02 - logprior: -2.3462e+00
Epoch 9/10
10/10 - 1s - loss: 316.1006 - loglik: -3.1385e+02 - logprior: -2.2533e+00
Epoch 10/10
10/10 - 1s - loss: 317.2014 - loglik: -3.1498e+02 - logprior: -2.2193e+00
Fitted a model with MAP estimate = -316.4223
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.0627 - loglik: -3.1703e+02 - logprior: -1.6031e+01
Epoch 2/2
10/10 - 1s - loss: 314.2784 - loglik: -3.0731e+02 - logprior: -6.9639e+00
Fitted a model with MAP estimate = -311.4205
expansions: [(0, 2)]
discards: [ 0  9 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 317.2993 - loglik: -3.0465e+02 - logprior: -1.2652e+01
Epoch 2/2
10/10 - 1s - loss: 308.4690 - loglik: -3.0502e+02 - logprior: -3.4534e+00
Fitted a model with MAP estimate = -306.1953
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 320.9359 - loglik: -3.0590e+02 - logprior: -1.5034e+01
Epoch 2/10
10/10 - 1s - loss: 309.7637 - loglik: -3.0491e+02 - logprior: -4.8540e+00
Epoch 3/10
10/10 - 2s - loss: 306.7745 - loglik: -3.0427e+02 - logprior: -2.5028e+00
Epoch 4/10
10/10 - 2s - loss: 304.5974 - loglik: -3.0304e+02 - logprior: -1.5576e+00
Epoch 5/10
10/10 - 1s - loss: 305.1640 - loglik: -3.0387e+02 - logprior: -1.2920e+00
Fitted a model with MAP estimate = -304.3523
Time for alignment: 43.8943
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 383.7043 - loglik: -3.6937e+02 - logprior: -1.4332e+01
Epoch 2/10
10/10 - 2s - loss: 355.6316 - loglik: -3.5153e+02 - logprior: -4.1016e+00
Epoch 3/10
10/10 - 2s - loss: 339.6646 - loglik: -3.3717e+02 - logprior: -2.4915e+00
Epoch 4/10
10/10 - 1s - loss: 327.9636 - loglik: -3.2577e+02 - logprior: -2.1942e+00
Epoch 5/10
10/10 - 1s - loss: 321.8733 - loglik: -3.1965e+02 - logprior: -2.2184e+00
Epoch 6/10
10/10 - 1s - loss: 319.1726 - loglik: -3.1685e+02 - logprior: -2.3246e+00
Epoch 7/10
10/10 - 1s - loss: 318.1784 - loglik: -3.1587e+02 - logprior: -2.3112e+00
Epoch 8/10
10/10 - 1s - loss: 317.5623 - loglik: -3.1535e+02 - logprior: -2.2115e+00
Epoch 9/10
10/10 - 1s - loss: 317.4431 - loglik: -3.1530e+02 - logprior: -2.1407e+00
Epoch 10/10
10/10 - 2s - loss: 317.3569 - loglik: -3.1524e+02 - logprior: -2.1212e+00
Fitted a model with MAP estimate = -317.1154
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.7008 - loglik: -3.1667e+02 - logprior: -1.6032e+01
Epoch 2/2
10/10 - 1s - loss: 314.3269 - loglik: -3.0736e+02 - logprior: -6.9679e+00
Fitted a model with MAP estimate = -311.4010
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 317.8946 - loglik: -3.0524e+02 - logprior: -1.2652e+01
Epoch 2/2
10/10 - 1s - loss: 306.9949 - loglik: -3.0354e+02 - logprior: -3.4548e+00
Fitted a model with MAP estimate = -306.1419
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 320.7747 - loglik: -3.0574e+02 - logprior: -1.5034e+01
Epoch 2/10
10/10 - 2s - loss: 309.8293 - loglik: -3.0497e+02 - logprior: -4.8554e+00
Epoch 3/10
10/10 - 2s - loss: 306.5191 - loglik: -3.0402e+02 - logprior: -2.5031e+00
Epoch 4/10
10/10 - 1s - loss: 305.4244 - loglik: -3.0386e+02 - logprior: -1.5674e+00
Epoch 5/10
10/10 - 2s - loss: 304.4467 - loglik: -3.0316e+02 - logprior: -1.2892e+00
Epoch 6/10
10/10 - 2s - loss: 304.5546 - loglik: -3.0350e+02 - logprior: -1.0506e+00
Fitted a model with MAP estimate = -304.0589
Time for alignment: 46.9675
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 383.6242 - loglik: -3.6929e+02 - logprior: -1.4331e+01
Epoch 2/10
10/10 - 1s - loss: 355.9619 - loglik: -3.5186e+02 - logprior: -4.1027e+00
Epoch 3/10
10/10 - 2s - loss: 339.4963 - loglik: -3.3697e+02 - logprior: -2.5303e+00
Epoch 4/10
10/10 - 2s - loss: 328.0014 - loglik: -3.2571e+02 - logprior: -2.2954e+00
Epoch 5/10
10/10 - 1s - loss: 321.5947 - loglik: -3.1926e+02 - logprior: -2.3355e+00
Epoch 6/10
10/10 - 2s - loss: 319.8350 - loglik: -3.1738e+02 - logprior: -2.4570e+00
Epoch 7/10
10/10 - 1s - loss: 318.3502 - loglik: -3.1590e+02 - logprior: -2.4478e+00
Epoch 8/10
10/10 - 1s - loss: 317.3727 - loglik: -3.1505e+02 - logprior: -2.3244e+00
Epoch 9/10
10/10 - 2s - loss: 317.9128 - loglik: -3.1568e+02 - logprior: -2.2344e+00
Fitted a model with MAP estimate = -317.5515
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 333.1629 - loglik: -3.1712e+02 - logprior: -1.6047e+01
Epoch 2/2
10/10 - 1s - loss: 315.0579 - loglik: -3.0810e+02 - logprior: -6.9576e+00
Fitted a model with MAP estimate = -311.4211
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.3650 - loglik: -3.0571e+02 - logprior: -1.2655e+01
Epoch 2/2
10/10 - 1s - loss: 306.6751 - loglik: -3.0322e+02 - logprior: -3.4559e+00
Fitted a model with MAP estimate = -306.1154
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 321.2943 - loglik: -3.0620e+02 - logprior: -1.5093e+01
Epoch 2/10
10/10 - 1s - loss: 309.4449 - loglik: -3.0454e+02 - logprior: -4.9064e+00
Epoch 3/10
10/10 - 1s - loss: 306.7118 - loglik: -3.0420e+02 - logprior: -2.5111e+00
Epoch 4/10
10/10 - 1s - loss: 305.4936 - loglik: -3.0393e+02 - logprior: -1.5621e+00
Epoch 5/10
10/10 - 2s - loss: 304.4399 - loglik: -3.0315e+02 - logprior: -1.2928e+00
Epoch 6/10
10/10 - 1s - loss: 304.3686 - loglik: -3.0331e+02 - logprior: -1.0547e+00
Epoch 7/10
10/10 - 2s - loss: 304.3248 - loglik: -3.0335e+02 - logprior: -9.7130e-01
Epoch 8/10
10/10 - 1s - loss: 303.6662 - loglik: -3.0275e+02 - logprior: -9.1630e-01
Epoch 9/10
10/10 - 2s - loss: 304.3855 - loglik: -3.0349e+02 - logprior: -8.9344e-01
Fitted a model with MAP estimate = -303.7315
Time for alignment: 48.5688
Computed alignments with likelihoods: ['-304.0859', '-304.3532', '-304.3523', '-304.0589', '-303.7315']
Best model has likelihood: -303.7315  (prior= -0.8774 )
time for generating output: 0.2741
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 1076.6647 - loglik: -1.0705e+03 - logprior: -6.1781e+00
Epoch 2/10
23/23 - 4s - loss: 1003.7001 - loglik: -1.0030e+03 - logprior: -6.9582e-01
Epoch 3/10
23/23 - 4s - loss: 983.4097 - loglik: -9.8293e+02 - logprior: -4.7722e-01
Epoch 4/10
23/23 - 4s - loss: 980.1747 - loglik: -9.7978e+02 - logprior: -3.9011e-01
Epoch 5/10
23/23 - 4s - loss: 977.8021 - loglik: -9.7747e+02 - logprior: -3.3401e-01
Epoch 6/10
23/23 - 4s - loss: 973.7181 - loglik: -9.7335e+02 - logprior: -3.7158e-01
Epoch 7/10
23/23 - 4s - loss: 974.1566 - loglik: -9.7377e+02 - logprior: -3.8179e-01
Fitted a model with MAP estimate = -974.1990
expansions: [(0, 8), (10, 3), (16, 1), (31, 1), (43, 1), (53, 1), (57, 1), (58, 2), (59, 2), (70, 1), (71, 1), (72, 3), (76, 1), (83, 1), (84, 1), (107, 1), (110, 1), (111, 2), (112, 1), (121, 1), (124, 1), (127, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 988.0367 - loglik: -9.7981e+02 - logprior: -8.2278e+00
Epoch 2/2
23/23 - 6s - loss: 971.4626 - loglik: -9.7094e+02 - logprior: -5.2699e-01
Fitted a model with MAP estimate = -966.9018
expansions: [(0, 18), (187, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  77  95 101 142 198 199 200 201
 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 982.7212 - loglik: -9.7433e+02 - logprior: -8.3899e+00
Epoch 2/2
23/23 - 6s - loss: 970.1046 - loglik: -9.6980e+02 - logprior: -3.0200e-01
Fitted a model with MAP estimate = -967.2514
expansions: [(0, 17), (203, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 82]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 980.7348 - loglik: -9.7268e+02 - logprior: -8.0519e+00
Epoch 2/10
23/23 - 6s - loss: 967.9666 - loglik: -9.6738e+02 - logprior: -5.8278e-01
Epoch 3/10
23/23 - 6s - loss: 965.5394 - loglik: -9.6605e+02 - logprior: 0.5097
Epoch 4/10
23/23 - 6s - loss: 959.2751 - loglik: -9.6002e+02 - logprior: 0.7419
Epoch 5/10
23/23 - 6s - loss: 958.6383 - loglik: -9.5980e+02 - logprior: 1.1579
Epoch 6/10
23/23 - 6s - loss: 953.8123 - loglik: -9.5508e+02 - logprior: 1.2654
Epoch 7/10
23/23 - 6s - loss: 956.2508 - loglik: -9.5764e+02 - logprior: 1.3862
Fitted a model with MAP estimate = -954.1194
Time for alignment: 130.6936
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 1076.6681 - loglik: -1.0704e+03 - logprior: -6.2201e+00
Epoch 2/10
23/23 - 4s - loss: 1003.6277 - loglik: -1.0030e+03 - logprior: -6.7309e-01
Epoch 3/10
23/23 - 4s - loss: 987.2257 - loglik: -9.8682e+02 - logprior: -4.0921e-01
Epoch 4/10
23/23 - 4s - loss: 979.1938 - loglik: -9.7890e+02 - logprior: -2.8941e-01
Epoch 5/10
23/23 - 4s - loss: 976.1172 - loglik: -9.7589e+02 - logprior: -2.3141e-01
Epoch 6/10
23/23 - 4s - loss: 976.3588 - loglik: -9.7610e+02 - logprior: -2.6101e-01
Fitted a model with MAP estimate = -974.7945
expansions: [(0, 8), (8, 5), (18, 1), (38, 1), (41, 1), (55, 2), (56, 1), (58, 1), (65, 1), (70, 1), (71, 1), (75, 3), (77, 2), (82, 1), (107, 2), (112, 1), (119, 1), (120, 2), (123, 1), (126, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 987.9948 - loglik: -9.7987e+02 - logprior: -8.1224e+00
Epoch 2/2
23/23 - 6s - loss: 968.9525 - loglik: -9.6854e+02 - logprior: -4.1589e-01
Fitted a model with MAP estimate = -966.9535
expansions: [(0, 17)]
discards: [  1   2   3   4   5   6   7   8   9  72 101 102 137 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 984.0326 - loglik: -9.7559e+02 - logprior: -8.4431e+00
Epoch 2/2
23/23 - 6s - loss: 969.9733 - loglik: -9.6959e+02 - logprior: -3.7885e-01
Fitted a model with MAP estimate = -967.3879
expansions: [(0, 10), (203, 7)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 978.3752 - loglik: -9.7228e+02 - logprior: -6.0977e+00
Epoch 2/10
23/23 - 6s - loss: 967.9042 - loglik: -9.6828e+02 - logprior: 0.3803
Epoch 3/10
23/23 - 6s - loss: 962.5211 - loglik: -9.6356e+02 - logprior: 1.0356
Epoch 4/10
23/23 - 6s - loss: 960.6164 - loglik: -9.6179e+02 - logprior: 1.1773
Epoch 5/10
23/23 - 6s - loss: 955.2134 - loglik: -9.5652e+02 - logprior: 1.3079
Epoch 6/10
23/23 - 6s - loss: 957.1656 - loglik: -9.5859e+02 - logprior: 1.4248
Fitted a model with MAP estimate = -954.3394
Time for alignment: 117.3657
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1075.9423 - loglik: -1.0697e+03 - logprior: -6.2002e+00
Epoch 2/10
23/23 - 4s - loss: 1008.4816 - loglik: -1.0078e+03 - logprior: -6.5775e-01
Epoch 3/10
23/23 - 4s - loss: 989.0910 - loglik: -9.8870e+02 - logprior: -3.8941e-01
Epoch 4/10
23/23 - 4s - loss: 983.2498 - loglik: -9.8304e+02 - logprior: -2.1000e-01
Epoch 5/10
23/23 - 4s - loss: 978.2734 - loglik: -9.7812e+02 - logprior: -1.5635e-01
Epoch 6/10
23/23 - 4s - loss: 975.9477 - loglik: -9.7574e+02 - logprior: -2.0315e-01
Epoch 7/10
23/23 - 4s - loss: 977.3987 - loglik: -9.7717e+02 - logprior: -2.2585e-01
Fitted a model with MAP estimate = -975.6417
expansions: [(0, 8), (9, 3), (19, 1), (34, 1), (42, 1), (52, 1), (56, 1), (57, 2), (58, 2), (69, 1), (70, 1), (71, 1), (73, 3), (91, 1), (112, 1), (115, 1), (116, 2), (121, 1), (124, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 988.1102 - loglik: -9.7989e+02 - logprior: -8.2248e+00
Epoch 2/2
23/23 - 6s - loss: 970.3088 - loglik: -9.6990e+02 - logprior: -4.1074e-01
Fitted a model with MAP estimate = -967.4613
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6   7   8   9  18  76 143 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 983.9338 - loglik: -9.7542e+02 - logprior: -8.5162e+00
Epoch 2/2
23/23 - 6s - loss: 971.8356 - loglik: -9.7146e+02 - logprior: -3.7483e-01
Fitted a model with MAP estimate = -967.8805
expansions: [(0, 17), (203, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 978.5302 - loglik: -9.7241e+02 - logprior: -6.1173e+00
Epoch 2/10
23/23 - 6s - loss: 968.4373 - loglik: -9.6864e+02 - logprior: 0.2045
Epoch 3/10
23/23 - 6s - loss: 963.8787 - loglik: -9.6467e+02 - logprior: 0.7882
Epoch 4/10
23/23 - 6s - loss: 960.2789 - loglik: -9.6126e+02 - logprior: 0.9771
Epoch 5/10
23/23 - 6s - loss: 956.8435 - loglik: -9.5801e+02 - logprior: 1.1650
Epoch 6/10
23/23 - 6s - loss: 955.6378 - loglik: -9.5691e+02 - logprior: 1.2719
Epoch 7/10
23/23 - 6s - loss: 954.0233 - loglik: -9.5541e+02 - logprior: 1.3883
Epoch 8/10
23/23 - 6s - loss: 955.0692 - loglik: -9.5657e+02 - logprior: 1.5047
Fitted a model with MAP estimate = -954.0361
Time for alignment: 135.4218
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1075.6324 - loglik: -1.0694e+03 - logprior: -6.2080e+00
Epoch 2/10
23/23 - 4s - loss: 1008.3429 - loglik: -1.0077e+03 - logprior: -6.3139e-01
Epoch 3/10
23/23 - 4s - loss: 987.3473 - loglik: -9.8711e+02 - logprior: -2.3969e-01
Epoch 4/10
23/23 - 4s - loss: 979.8481 - loglik: -9.7976e+02 - logprior: -8.5615e-02
Epoch 5/10
23/23 - 4s - loss: 978.8329 - loglik: -9.7880e+02 - logprior: -3.1690e-02
Epoch 6/10
23/23 - 4s - loss: 975.3145 - loglik: -9.7527e+02 - logprior: -4.2638e-02
Epoch 7/10
23/23 - 4s - loss: 977.0724 - loglik: -9.7702e+02 - logprior: -4.7893e-02
Fitted a model with MAP estimate = -975.1644
expansions: [(0, 8), (8, 5), (18, 1), (42, 1), (56, 2), (57, 2), (58, 2), (69, 1), (70, 2), (71, 1), (72, 1), (74, 1), (83, 1), (86, 1), (107, 2), (120, 1), (121, 3), (127, 1), (133, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 990.0204 - loglik: -9.8184e+02 - logprior: -8.1779e+00
Epoch 2/2
23/23 - 6s - loss: 972.3892 - loglik: -9.7196e+02 - logprior: -4.2750e-01
Fitted a model with MAP estimate = -968.7015
expansions: [(0, 18)]
discards: [  1   2   3   4   5   6   7   8   9  10  72  77  92  93 137 199 200 201
 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 985.0469 - loglik: -9.7660e+02 - logprior: -8.4426e+00
Epoch 2/2
23/23 - 6s - loss: 971.9101 - loglik: -9.7152e+02 - logprior: -3.9002e-01
Fitted a model with MAP estimate = -969.6484
expansions: [(0, 16), (202, 8)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 981.2974 - loglik: -9.7477e+02 - logprior: -6.5227e+00
Epoch 2/10
23/23 - 6s - loss: 967.8557 - loglik: -9.6804e+02 - logprior: 0.1813
Epoch 3/10
23/23 - 6s - loss: 966.1486 - loglik: -9.6694e+02 - logprior: 0.7883
Epoch 4/10
23/23 - 6s - loss: 961.3989 - loglik: -9.6241e+02 - logprior: 1.0099
Epoch 5/10
23/23 - 6s - loss: 958.1660 - loglik: -9.5931e+02 - logprior: 1.1446
Epoch 6/10
23/23 - 6s - loss: 957.7504 - loglik: -9.5900e+02 - logprior: 1.2474
Epoch 7/10
23/23 - 6s - loss: 955.9518 - loglik: -9.5731e+02 - logprior: 1.3631
Epoch 8/10
23/23 - 6s - loss: 953.1719 - loglik: -9.5463e+02 - logprior: 1.4625
Epoch 9/10
23/23 - 6s - loss: 957.4789 - loglik: -9.5906e+02 - logprior: 1.5813
Fitted a model with MAP estimate = -955.0264
Time for alignment: 140.5292
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 1077.5692 - loglik: -1.0714e+03 - logprior: -6.2062e+00
Epoch 2/10
23/23 - 4s - loss: 1006.7051 - loglik: -1.0060e+03 - logprior: -7.0171e-01
Epoch 3/10
23/23 - 4s - loss: 987.1623 - loglik: -9.8666e+02 - logprior: -5.0259e-01
Epoch 4/10
23/23 - 4s - loss: 979.0553 - loglik: -9.7862e+02 - logprior: -4.3688e-01
Epoch 5/10
23/23 - 4s - loss: 976.2267 - loglik: -9.7585e+02 - logprior: -3.7730e-01
Epoch 6/10
23/23 - 4s - loss: 974.4562 - loglik: -9.7408e+02 - logprior: -3.7769e-01
Epoch 7/10
23/23 - 4s - loss: 973.6276 - loglik: -9.7324e+02 - logprior: -3.9234e-01
Epoch 8/10
23/23 - 4s - loss: 973.3105 - loglik: -9.7293e+02 - logprior: -3.8481e-01
Epoch 9/10
23/23 - 4s - loss: 974.7707 - loglik: -9.7440e+02 - logprior: -3.7170e-01
Fitted a model with MAP estimate = -973.3289
expansions: [(0, 8), (9, 4), (19, 1), (34, 1), (42, 1), (50, 1), (57, 1), (59, 1), (62, 1), (65, 1), (70, 2), (71, 1), (73, 4), (86, 1), (107, 2), (111, 4), (120, 1), (123, 1), (126, 1), (148, 1), (149, 1), (150, 1), (159, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 989.5148 - loglik: -9.8124e+02 - logprior: -8.2760e+00
Epoch 2/2
23/23 - 6s - loss: 970.8602 - loglik: -9.7036e+02 - logprior: -4.9643e-01
Fitted a model with MAP estimate = -967.0436
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  10  91  93  94  97 136 142 199 200
 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 984.6688 - loglik: -9.7629e+02 - logprior: -8.3755e+00
Epoch 2/2
23/23 - 6s - loss: 970.0842 - loglik: -9.6975e+02 - logprior: -3.3479e-01
Fitted a model with MAP estimate = -967.8364
expansions: [(0, 15), (101, 2), (202, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 22]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 982.4987 - loglik: -9.7425e+02 - logprior: -8.2466e+00
Epoch 2/10
23/23 - 6s - loss: 967.3945 - loglik: -9.6671e+02 - logprior: -6.8340e-01
Epoch 3/10
23/23 - 6s - loss: 964.6415 - loglik: -9.6552e+02 - logprior: 0.8754
Epoch 4/10
23/23 - 6s - loss: 959.7891 - loglik: -9.6089e+02 - logprior: 1.0969
Epoch 5/10
23/23 - 6s - loss: 955.3995 - loglik: -9.5667e+02 - logprior: 1.2692
Epoch 6/10
23/23 - 6s - loss: 956.1046 - loglik: -9.5751e+02 - logprior: 1.4031
Fitted a model with MAP estimate = -954.2945
Time for alignment: 132.2561
Computed alignments with likelihoods: ['-954.1194', '-954.3394', '-954.0361', '-955.0264', '-954.2945']
Best model has likelihood: -954.0361  (prior= 1.5620 )
time for generating output: 0.2410
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.665600730927364
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 510.2743 - loglik: -5.0913e+02 - logprior: -1.1471e+00
Epoch 2/10
29/29 - 3s - loss: 485.7805 - loglik: -4.8497e+02 - logprior: -8.1398e-01
Epoch 3/10
29/29 - 3s - loss: 481.1797 - loglik: -4.8039e+02 - logprior: -7.9422e-01
Epoch 4/10
29/29 - 3s - loss: 479.9320 - loglik: -4.7914e+02 - logprior: -7.8977e-01
Epoch 5/10
29/29 - 3s - loss: 479.0905 - loglik: -4.7830e+02 - logprior: -7.9373e-01
Epoch 6/10
29/29 - 3s - loss: 478.3099 - loglik: -4.7752e+02 - logprior: -7.8738e-01
Epoch 7/10
29/29 - 3s - loss: 478.4580 - loglik: -4.7767e+02 - logprior: -7.8443e-01
Fitted a model with MAP estimate = -442.0920
expansions: [(1, 1), (13, 1), (14, 2), (15, 2), (22, 1), (27, 2), (38, 2), (41, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 2), (50, 1), (52, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 480.5396 - loglik: -4.7941e+02 - logprior: -1.1320e+00
Epoch 2/2
29/29 - 3s - loss: 475.6098 - loglik: -4.7486e+02 - logprior: -7.4947e-01
Fitted a model with MAP estimate = -437.5532
expansions: [(3, 1)]
discards: [35 48 58 66]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 477.1259 - loglik: -4.7604e+02 - logprior: -1.0820e+00
Epoch 2/2
29/29 - 3s - loss: 475.8811 - loglik: -4.7519e+02 - logprior: -6.8724e-01
Fitted a model with MAP estimate = -437.2496
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.0742 - loglik: -4.3634e+02 - logprior: -7.3236e-01
Epoch 2/10
42/42 - 4s - loss: 436.9249 - loglik: -4.3636e+02 - logprior: -5.6044e-01
Epoch 3/10
42/42 - 4s - loss: 436.1339 - loglik: -4.3558e+02 - logprior: -5.5101e-01
Epoch 4/10
42/42 - 4s - loss: 435.6562 - loglik: -4.3511e+02 - logprior: -5.4682e-01
Epoch 5/10
42/42 - 4s - loss: 435.4664 - loglik: -4.3492e+02 - logprior: -5.4414e-01
Epoch 6/10
42/42 - 4s - loss: 434.3515 - loglik: -4.3380e+02 - logprior: -5.4751e-01
Epoch 7/10
42/42 - 4s - loss: 433.9729 - loglik: -4.3343e+02 - logprior: -5.3921e-01
Epoch 8/10
42/42 - 4s - loss: 433.3544 - loglik: -4.3281e+02 - logprior: -5.4331e-01
Epoch 9/10
42/42 - 4s - loss: 434.1137 - loglik: -4.3357e+02 - logprior: -5.4266e-01
Fitted a model with MAP estimate = -433.3944
Time for alignment: 112.7833
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 510.6727 - loglik: -5.0952e+02 - logprior: -1.1489e+00
Epoch 2/10
29/29 - 3s - loss: 486.2304 - loglik: -4.8544e+02 - logprior: -7.9481e-01
Epoch 3/10
29/29 - 3s - loss: 481.9652 - loglik: -4.8118e+02 - logprior: -7.9004e-01
Epoch 4/10
29/29 - 3s - loss: 480.4971 - loglik: -4.7968e+02 - logprior: -8.1394e-01
Epoch 5/10
29/29 - 3s - loss: 479.3087 - loglik: -4.7850e+02 - logprior: -8.0599e-01
Epoch 6/10
29/29 - 3s - loss: 478.7281 - loglik: -4.7793e+02 - logprior: -7.9649e-01
Epoch 7/10
29/29 - 3s - loss: 477.7921 - loglik: -4.7700e+02 - logprior: -7.9303e-01
Epoch 8/10
29/29 - 3s - loss: 477.7292 - loglik: -4.7694e+02 - logprior: -7.8857e-01
Epoch 9/10
29/29 - 3s - loss: 477.0798 - loglik: -4.7629e+02 - logprior: -7.8978e-01
Epoch 10/10
29/29 - 3s - loss: 477.7494 - loglik: -4.7696e+02 - logprior: -7.8792e-01
Fitted a model with MAP estimate = -444.2897
expansions: [(1, 1), (2, 1), (13, 4), (14, 3), (35, 1), (38, 1), (41, 1), (44, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (53, 1), (55, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 481.2491 - loglik: -4.8014e+02 - logprior: -1.1122e+00
Epoch 2/2
29/29 - 3s - loss: 475.4891 - loglik: -4.7477e+02 - logprior: -7.2237e-01
Fitted a model with MAP estimate = -437.3094
expansions: []
discards: [ 1 60 66]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 477.7686 - loglik: -4.7676e+02 - logprior: -1.0089e+00
Epoch 2/2
29/29 - 3s - loss: 475.7267 - loglik: -4.7503e+02 - logprior: -6.9750e-01
Fitted a model with MAP estimate = -437.6246
expansions: []
discards: [16]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.3287 - loglik: -4.3666e+02 - logprior: -6.6934e-01
Epoch 2/10
42/42 - 4s - loss: 436.5531 - loglik: -4.3607e+02 - logprior: -4.7979e-01
Epoch 3/10
42/42 - 4s - loss: 435.9061 - loglik: -4.3543e+02 - logprior: -4.7429e-01
Epoch 4/10
42/42 - 4s - loss: 436.0987 - loglik: -4.3563e+02 - logprior: -4.6786e-01
Fitted a model with MAP estimate = -435.1898
Time for alignment: 102.8112
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 510.8174 - loglik: -5.0967e+02 - logprior: -1.1474e+00
Epoch 2/10
29/29 - 3s - loss: 485.8339 - loglik: -4.8503e+02 - logprior: -8.0776e-01
Epoch 3/10
29/29 - 3s - loss: 481.6971 - loglik: -4.8091e+02 - logprior: -7.8366e-01
Epoch 4/10
29/29 - 3s - loss: 480.1580 - loglik: -4.7937e+02 - logprior: -7.8989e-01
Epoch 5/10
29/29 - 3s - loss: 479.6737 - loglik: -4.7888e+02 - logprior: -7.9659e-01
Epoch 6/10
29/29 - 3s - loss: 478.5924 - loglik: -4.7780e+02 - logprior: -7.9114e-01
Epoch 7/10
29/29 - 3s - loss: 478.4109 - loglik: -4.7763e+02 - logprior: -7.8490e-01
Epoch 8/10
29/29 - 3s - loss: 477.2945 - loglik: -4.7651e+02 - logprior: -7.8584e-01
Epoch 9/10
29/29 - 3s - loss: 477.7680 - loglik: -4.7699e+02 - logprior: -7.8212e-01
Fitted a model with MAP estimate = -443.7173
expansions: [(1, 1), (13, 4), (14, 3), (27, 2), (38, 2), (40, 1), (41, 1), (44, 2), (45, 1), (46, 1), (47, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 480.2162 - loglik: -4.7909e+02 - logprior: -1.1222e+00
Epoch 2/2
29/29 - 3s - loss: 476.8040 - loglik: -4.7607e+02 - logprior: -7.3055e-01
Fitted a model with MAP estimate = -437.4874
expansions: [(3, 1)]
discards: [36 49 59]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 5s - loss: 476.7639 - loglik: -4.7569e+02 - logprior: -1.0771e+00
Epoch 2/2
29/29 - 3s - loss: 475.3711 - loglik: -4.7469e+02 - logprior: -6.7723e-01
Fitted a model with MAP estimate = -437.3850
expansions: []
discards: [ 1 17]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.6824 - loglik: -4.3707e+02 - logprior: -6.0985e-01
Epoch 2/10
42/42 - 4s - loss: 436.5694 - loglik: -4.3607e+02 - logprior: -4.9622e-01
Epoch 3/10
42/42 - 4s - loss: 435.2714 - loglik: -4.3479e+02 - logprior: -4.8061e-01
Epoch 4/10
42/42 - 4s - loss: 436.4517 - loglik: -4.3598e+02 - logprior: -4.7220e-01
Fitted a model with MAP estimate = -435.1915
Time for alignment: 99.6862
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 510.0932 - loglik: -5.0895e+02 - logprior: -1.1469e+00
Epoch 2/10
29/29 - 3s - loss: 483.7572 - loglik: -4.8295e+02 - logprior: -8.0394e-01
Epoch 3/10
29/29 - 3s - loss: 480.6504 - loglik: -4.7987e+02 - logprior: -7.7952e-01
Epoch 4/10
29/29 - 2s - loss: 479.2287 - loglik: -4.7844e+02 - logprior: -7.9270e-01
Epoch 5/10
29/29 - 3s - loss: 478.8317 - loglik: -4.7804e+02 - logprior: -7.8909e-01
Epoch 6/10
29/29 - 3s - loss: 477.8976 - loglik: -4.7711e+02 - logprior: -7.8896e-01
Epoch 7/10
29/29 - 3s - loss: 477.0995 - loglik: -4.7631e+02 - logprior: -7.9130e-01
Epoch 8/10
29/29 - 3s - loss: 477.0326 - loglik: -4.7624e+02 - logprior: -7.8920e-01
Epoch 9/10
29/29 - 3s - loss: 477.0718 - loglik: -4.7628e+02 - logprior: -7.8851e-01
Fitted a model with MAP estimate = -442.7529
expansions: [(1, 1), (2, 1), (12, 1), (13, 2), (14, 2), (16, 1), (17, 2), (21, 1), (38, 2), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 480.7672 - loglik: -4.7965e+02 - logprior: -1.1185e+00
Epoch 2/2
29/29 - 3s - loss: 475.8540 - loglik: -4.7512e+02 - logprior: -7.3056e-01
Fitted a model with MAP estimate = -437.2870
expansions: []
discards: [ 1 50 58 61 69]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 477.3612 - loglik: -4.7635e+02 - logprior: -1.0065e+00
Epoch 2/2
29/29 - 3s - loss: 476.3047 - loglik: -4.7560e+02 - logprior: -7.0700e-01
Fitted a model with MAP estimate = -437.7151
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.1711 - loglik: -4.3650e+02 - logprior: -6.6979e-01
Epoch 2/10
42/42 - 4s - loss: 436.0770 - loglik: -4.3559e+02 - logprior: -4.8824e-01
Epoch 3/10
42/42 - 4s - loss: 436.6566 - loglik: -4.3618e+02 - logprior: -4.7963e-01
Fitted a model with MAP estimate = -435.5255
Time for alignment: 94.6865
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 509.4397 - loglik: -5.0829e+02 - logprior: -1.1475e+00
Epoch 2/10
29/29 - 3s - loss: 485.8720 - loglik: -4.8507e+02 - logprior: -8.0600e-01
Epoch 3/10
29/29 - 3s - loss: 480.1611 - loglik: -4.7938e+02 - logprior: -7.8317e-01
Epoch 4/10
29/29 - 3s - loss: 479.2466 - loglik: -4.7847e+02 - logprior: -7.8124e-01
Epoch 5/10
29/29 - 3s - loss: 478.6684 - loglik: -4.7788e+02 - logprior: -7.8650e-01
Epoch 6/10
29/29 - 3s - loss: 478.1567 - loglik: -4.7737e+02 - logprior: -7.8701e-01
Epoch 7/10
29/29 - 3s - loss: 476.7595 - loglik: -4.7598e+02 - logprior: -7.7668e-01
Epoch 8/10
29/29 - 3s - loss: 477.4474 - loglik: -4.7667e+02 - logprior: -7.8159e-01
Fitted a model with MAP estimate = -442.0079
expansions: [(1, 1), (2, 1), (14, 2), (16, 2), (17, 2), (21, 1), (38, 2), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 480.4411 - loglik: -4.7932e+02 - logprior: -1.1253e+00
Epoch 2/2
29/29 - 3s - loss: 475.6379 - loglik: -4.7490e+02 - logprior: -7.3463e-01
Fitted a model with MAP estimate = -437.4205
expansions: []
discards: [ 1 48 56 59 67]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 477.7048 - loglik: -4.7670e+02 - logprior: -1.0080e+00
Epoch 2/2
29/29 - 3s - loss: 475.4928 - loglik: -4.7479e+02 - logprior: -7.0293e-01
Fitted a model with MAP estimate = -437.8487
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.7451 - loglik: -4.3708e+02 - logprior: -6.6788e-01
Epoch 2/10
42/42 - 4s - loss: 436.1667 - loglik: -4.3568e+02 - logprior: -4.8294e-01
Epoch 3/10
42/42 - 4s - loss: 435.8856 - loglik: -4.3541e+02 - logprior: -4.7641e-01
Epoch 4/10
42/42 - 4s - loss: 435.9018 - loglik: -4.3543e+02 - logprior: -4.7295e-01
Fitted a model with MAP estimate = -435.1709
Time for alignment: 96.5018
Computed alignments with likelihoods: ['-433.3944', '-435.1898', '-435.1915', '-435.5255', '-435.1709']
Best model has likelihood: -433.3944  (prior= -0.5207 )
time for generating output: 0.1649
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.4231339594661394
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 727.3303 - loglik: -6.8207e+02 - logprior: -4.5261e+01
Epoch 2/10
10/10 - 1s - loss: 648.7700 - loglik: -6.3795e+02 - logprior: -1.0825e+01
Epoch 3/10
10/10 - 1s - loss: 601.7305 - loglik: -5.9679e+02 - logprior: -4.9395e+00
Epoch 4/10
10/10 - 1s - loss: 572.0248 - loglik: -5.6876e+02 - logprior: -3.2602e+00
Epoch 5/10
10/10 - 1s - loss: 561.3787 - loglik: -5.5890e+02 - logprior: -2.4817e+00
Epoch 6/10
10/10 - 1s - loss: 557.3107 - loglik: -5.5541e+02 - logprior: -1.9019e+00
Epoch 7/10
10/10 - 1s - loss: 556.3862 - loglik: -5.5483e+02 - logprior: -1.5565e+00
Epoch 8/10
10/10 - 1s - loss: 554.9399 - loglik: -5.5360e+02 - logprior: -1.3394e+00
Epoch 9/10
10/10 - 1s - loss: 554.4716 - loglik: -5.5331e+02 - logprior: -1.1598e+00
Epoch 10/10
10/10 - 1s - loss: 553.8649 - loglik: -5.5282e+02 - logprior: -1.0462e+00
Fitted a model with MAP estimate = -553.7702
expansions: [(10, 4), (12, 1), (13, 1), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2), (96, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 601.1343 - loglik: -5.5006e+02 - logprior: -5.1073e+01
Epoch 2/2
10/10 - 1s - loss: 555.6700 - loglik: -5.3555e+02 - logprior: -2.0117e+01
Fitted a model with MAP estimate = -547.5271
expansions: [(0, 3)]
discards: [  0 100 101 117 118 119 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 577.1664 - loglik: -5.3688e+02 - logprior: -4.0283e+01
Epoch 2/2
10/10 - 1s - loss: 543.7078 - loglik: -5.3443e+02 - logprior: -9.2806e+00
Fitted a model with MAP estimate = -538.3985
expansions: [(117, 6)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 586.2711 - loglik: -5.3643e+02 - logprior: -4.9842e+01
Epoch 2/10
10/10 - 1s - loss: 549.1188 - loglik: -5.3192e+02 - logprior: -1.7201e+01
Epoch 3/10
10/10 - 1s - loss: 535.8983 - loglik: -5.3002e+02 - logprior: -5.8786e+00
Epoch 4/10
10/10 - 1s - loss: 529.2675 - loglik: -5.2840e+02 - logprior: -8.6929e-01
Epoch 5/10
10/10 - 1s - loss: 527.2180 - loglik: -5.2803e+02 - logprior: 0.8091
Epoch 6/10
10/10 - 1s - loss: 525.6150 - loglik: -5.2740e+02 - logprior: 1.7813
Epoch 7/10
10/10 - 1s - loss: 526.1701 - loglik: -5.2868e+02 - logprior: 2.5118
Fitted a model with MAP estimate = -525.3751
Time for alignment: 38.9421
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 727.3564 - loglik: -6.8210e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 648.6645 - loglik: -6.3785e+02 - logprior: -1.0818e+01
Epoch 3/10
10/10 - 1s - loss: 600.7346 - loglik: -5.9586e+02 - logprior: -4.8772e+00
Epoch 4/10
10/10 - 1s - loss: 575.5234 - loglik: -5.7255e+02 - logprior: -2.9715e+00
Epoch 5/10
10/10 - 1s - loss: 566.9264 - loglik: -5.6497e+02 - logprior: -1.9584e+00
Epoch 6/10
10/10 - 1s - loss: 560.6313 - loglik: -5.5924e+02 - logprior: -1.3956e+00
Epoch 7/10
10/10 - 1s - loss: 556.2034 - loglik: -5.5505e+02 - logprior: -1.1491e+00
Epoch 8/10
10/10 - 1s - loss: 554.7675 - loglik: -5.5378e+02 - logprior: -9.8416e-01
Epoch 9/10
10/10 - 1s - loss: 553.8555 - loglik: -5.5297e+02 - logprior: -8.8056e-01
Epoch 10/10
10/10 - 1s - loss: 552.7532 - loglik: -5.5195e+02 - logprior: -8.0173e-01
Fitted a model with MAP estimate = -552.9970
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 601.7723 - loglik: -5.5060e+02 - logprior: -5.1171e+01
Epoch 2/2
10/10 - 1s - loss: 559.3395 - loglik: -5.3925e+02 - logprior: -2.0092e+01
Fitted a model with MAP estimate = -550.8937
expansions: [(0, 3)]
discards: [  0   9  13 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.6121 - loglik: -5.3827e+02 - logprior: -4.0343e+01
Epoch 2/2
10/10 - 1s - loss: 543.5655 - loglik: -5.3419e+02 - logprior: -9.3733e+00
Fitted a model with MAP estimate = -538.5478
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 585.8685 - loglik: -5.3641e+02 - logprior: -4.9459e+01
Epoch 2/10
10/10 - 1s - loss: 550.4047 - loglik: -5.3496e+02 - logprior: -1.5443e+01
Epoch 3/10
10/10 - 1s - loss: 537.9898 - loglik: -5.3347e+02 - logprior: -4.5154e+00
Epoch 4/10
10/10 - 1s - loss: 533.0710 - loglik: -5.3245e+02 - logprior: -6.1931e-01
Epoch 5/10
10/10 - 1s - loss: 531.7857 - loglik: -5.3265e+02 - logprior: 0.8610
Epoch 6/10
10/10 - 1s - loss: 530.2959 - loglik: -5.3219e+02 - logprior: 1.8914
Epoch 7/10
10/10 - 1s - loss: 529.3516 - loglik: -5.3200e+02 - logprior: 2.6439
Epoch 8/10
10/10 - 1s - loss: 529.5154 - loglik: -5.3258e+02 - logprior: 3.0691
Fitted a model with MAP estimate = -528.7716
Time for alignment: 40.5124
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 726.8520 - loglik: -6.8159e+02 - logprior: -4.5260e+01
Epoch 2/10
10/10 - 1s - loss: 649.5677 - loglik: -6.3875e+02 - logprior: -1.0821e+01
Epoch 3/10
10/10 - 1s - loss: 602.8580 - loglik: -5.9790e+02 - logprior: -4.9602e+00
Epoch 4/10
10/10 - 1s - loss: 575.9662 - loglik: -5.7280e+02 - logprior: -3.1666e+00
Epoch 5/10
10/10 - 1s - loss: 563.8003 - loglik: -5.6158e+02 - logprior: -2.2161e+00
Epoch 6/10
10/10 - 1s - loss: 558.1602 - loglik: -5.5645e+02 - logprior: -1.7086e+00
Epoch 7/10
10/10 - 1s - loss: 557.0699 - loglik: -5.5561e+02 - logprior: -1.4572e+00
Epoch 8/10
10/10 - 1s - loss: 555.1069 - loglik: -5.5376e+02 - logprior: -1.3439e+00
Epoch 9/10
10/10 - 1s - loss: 554.2184 - loglik: -5.5297e+02 - logprior: -1.2463e+00
Epoch 10/10
10/10 - 1s - loss: 553.9221 - loglik: -5.5278e+02 - logprior: -1.1464e+00
Fitted a model with MAP estimate = -553.6124
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 602.0545 - loglik: -5.5081e+02 - logprior: -5.1245e+01
Epoch 2/2
10/10 - 1s - loss: 558.9346 - loglik: -5.3864e+02 - logprior: -2.0293e+01
Fitted a model with MAP estimate = -551.1147
expansions: [(0, 3)]
discards: [  0   9  20 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 578.2661 - loglik: -5.3782e+02 - logprior: -4.0441e+01
Epoch 2/2
10/10 - 1s - loss: 542.9835 - loglik: -5.3364e+02 - logprior: -9.3414e+00
Fitted a model with MAP estimate = -538.3160
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 585.5062 - loglik: -5.3621e+02 - logprior: -4.9294e+01
Epoch 2/10
10/10 - 1s - loss: 549.8871 - loglik: -5.3493e+02 - logprior: -1.4958e+01
Epoch 3/10
10/10 - 1s - loss: 537.6000 - loglik: -5.3336e+02 - logprior: -4.2373e+00
Epoch 4/10
10/10 - 1s - loss: 533.1539 - loglik: -5.3258e+02 - logprior: -5.7388e-01
Epoch 5/10
10/10 - 1s - loss: 530.7760 - loglik: -5.3165e+02 - logprior: 0.8741
Epoch 6/10
10/10 - 1s - loss: 530.1796 - loglik: -5.3208e+02 - logprior: 1.9023
Epoch 7/10
10/10 - 1s - loss: 529.2982 - loglik: -5.3195e+02 - logprior: 2.6471
Epoch 8/10
10/10 - 1s - loss: 528.9649 - loglik: -5.3204e+02 - logprior: 3.0736
Epoch 9/10
10/10 - 1s - loss: 528.7746 - loglik: -5.3218e+02 - logprior: 3.4054
Epoch 10/10
10/10 - 1s - loss: 527.9617 - loglik: -5.3170e+02 - logprior: 3.7375
Fitted a model with MAP estimate = -528.0335
Time for alignment: 41.7144
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 726.6587 - loglik: -6.8140e+02 - logprior: -4.5257e+01
Epoch 2/10
10/10 - 1s - loss: 649.2468 - loglik: -6.3842e+02 - logprior: -1.0822e+01
Epoch 3/10
10/10 - 1s - loss: 602.2910 - loglik: -5.9733e+02 - logprior: -4.9611e+00
Epoch 4/10
10/10 - 1s - loss: 577.2422 - loglik: -5.7410e+02 - logprior: -3.1437e+00
Epoch 5/10
10/10 - 1s - loss: 565.8087 - loglik: -5.6369e+02 - logprior: -2.1156e+00
Epoch 6/10
10/10 - 1s - loss: 558.6620 - loglik: -5.5700e+02 - logprior: -1.6630e+00
Epoch 7/10
10/10 - 1s - loss: 556.6041 - loglik: -5.5514e+02 - logprior: -1.4624e+00
Epoch 8/10
10/10 - 1s - loss: 554.6664 - loglik: -5.5337e+02 - logprior: -1.2943e+00
Epoch 9/10
10/10 - 1s - loss: 554.7152 - loglik: -5.5354e+02 - logprior: -1.1777e+00
Fitted a model with MAP estimate = -553.6876
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (16, 1), (27, 2), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 603.4819 - loglik: -5.5220e+02 - logprior: -5.1282e+01
Epoch 2/2
10/10 - 1s - loss: 558.8790 - loglik: -5.3847e+02 - logprior: -2.0405e+01
Fitted a model with MAP estimate = -551.1979
expansions: [(0, 3)]
discards: [  0   9  20  36 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 577.7880 - loglik: -5.3730e+02 - logprior: -4.0487e+01
Epoch 2/2
10/10 - 1s - loss: 543.5640 - loglik: -5.3421e+02 - logprior: -9.3554e+00
Fitted a model with MAP estimate = -538.1999
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 585.9272 - loglik: -5.3663e+02 - logprior: -4.9296e+01
Epoch 2/10
10/10 - 1s - loss: 549.2747 - loglik: -5.3435e+02 - logprior: -1.4930e+01
Epoch 3/10
10/10 - 1s - loss: 537.3263 - loglik: -5.3309e+02 - logprior: -4.2384e+00
Epoch 4/10
10/10 - 1s - loss: 533.2400 - loglik: -5.3265e+02 - logprior: -5.9382e-01
Epoch 5/10
10/10 - 1s - loss: 531.1376 - loglik: -5.3199e+02 - logprior: 0.8565
Epoch 6/10
10/10 - 1s - loss: 530.1713 - loglik: -5.3205e+02 - logprior: 1.8804
Epoch 7/10
10/10 - 1s - loss: 529.3678 - loglik: -5.3199e+02 - logprior: 2.6202
Epoch 8/10
10/10 - 1s - loss: 529.4012 - loglik: -5.3244e+02 - logprior: 3.0431
Fitted a model with MAP estimate = -528.6155
Time for alignment: 38.3667
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 726.8658 - loglik: -6.8161e+02 - logprior: -4.5259e+01
Epoch 2/10
10/10 - 1s - loss: 649.6778 - loglik: -6.3885e+02 - logprior: -1.0825e+01
Epoch 3/10
10/10 - 1s - loss: 601.4037 - loglik: -5.9640e+02 - logprior: -4.9997e+00
Epoch 4/10
10/10 - 1s - loss: 571.0988 - loglik: -5.6772e+02 - logprior: -3.3757e+00
Epoch 5/10
10/10 - 1s - loss: 561.5315 - loglik: -5.5895e+02 - logprior: -2.5809e+00
Epoch 6/10
10/10 - 1s - loss: 556.9722 - loglik: -5.5495e+02 - logprior: -2.0216e+00
Epoch 7/10
10/10 - 1s - loss: 556.0591 - loglik: -5.5435e+02 - logprior: -1.7065e+00
Epoch 8/10
10/10 - 1s - loss: 554.8710 - loglik: -5.5341e+02 - logprior: -1.4655e+00
Epoch 9/10
10/10 - 1s - loss: 554.7645 - loglik: -5.5346e+02 - logprior: -1.2998e+00
Epoch 10/10
10/10 - 1s - loss: 553.9843 - loglik: -5.5280e+02 - logprior: -1.1827e+00
Fitted a model with MAP estimate = -553.8849
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (68, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 602.6025 - loglik: -5.5137e+02 - logprior: -5.1230e+01
Epoch 2/2
10/10 - 1s - loss: 559.0818 - loglik: -5.3883e+02 - logprior: -2.0253e+01
Fitted a model with MAP estimate = -551.1341
expansions: [(0, 3)]
discards: [  0  10  18 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 578.2629 - loglik: -5.3786e+02 - logprior: -4.0407e+01
Epoch 2/2
10/10 - 1s - loss: 543.2367 - loglik: -5.3390e+02 - logprior: -9.3321e+00
Fitted a model with MAP estimate = -538.2922
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 585.5334 - loglik: -5.3604e+02 - logprior: -4.9493e+01
Epoch 2/10
10/10 - 1s - loss: 550.5276 - loglik: -5.3493e+02 - logprior: -1.5596e+01
Epoch 3/10
10/10 - 1s - loss: 537.8118 - loglik: -5.3320e+02 - logprior: -4.6118e+00
Epoch 4/10
10/10 - 1s - loss: 533.4652 - loglik: -5.3283e+02 - logprior: -6.3642e-01
Epoch 5/10
10/10 - 1s - loss: 530.9335 - loglik: -5.3179e+02 - logprior: 0.8532
Epoch 6/10
10/10 - 1s - loss: 530.2012 - loglik: -5.3209e+02 - logprior: 1.8850
Epoch 7/10
10/10 - 1s - loss: 529.4057 - loglik: -5.3204e+02 - logprior: 2.6296
Epoch 8/10
10/10 - 1s - loss: 528.7377 - loglik: -5.3180e+02 - logprior: 3.0585
Epoch 9/10
10/10 - 1s - loss: 528.6636 - loglik: -5.3205e+02 - logprior: 3.3849
Epoch 10/10
10/10 - 1s - loss: 528.0054 - loglik: -5.3172e+02 - logprior: 3.7194
Fitted a model with MAP estimate = -528.0337
Time for alignment: 42.3842
Computed alignments with likelihoods: ['-525.3751', '-528.7716', '-528.0335', '-528.6155', '-528.0337']
Best model has likelihood: -525.3751  (prior= 2.8049 )
time for generating output: 0.1400
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9350186535998664
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 1254.5961 - loglik: -1.2322e+03 - logprior: -2.2440e+01
Epoch 2/10
14/14 - 3s - loss: 1169.0260 - loglik: -1.1671e+03 - logprior: -1.9453e+00
Epoch 3/10
14/14 - 3s - loss: 1107.5712 - loglik: -1.1069e+03 - logprior: -6.5045e-01
Epoch 4/10
14/14 - 3s - loss: 1090.8154 - loglik: -1.0904e+03 - logprior: -4.2626e-01
Epoch 5/10
14/14 - 3s - loss: 1081.6219 - loglik: -1.0814e+03 - logprior: -2.3088e-01
Epoch 6/10
14/14 - 3s - loss: 1080.4045 - loglik: -1.0804e+03 - logprior: -2.0969e-02
Epoch 7/10
14/14 - 3s - loss: 1080.1884 - loglik: -1.0803e+03 - logprior: 0.1201
Epoch 8/10
14/14 - 3s - loss: 1081.5059 - loglik: -1.0818e+03 - logprior: 0.2708
Fitted a model with MAP estimate = -1079.7897
expansions: [(15, 1), (17, 1), (29, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (53, 1), (56, 4), (62, 2), (81, 1), (91, 1), (92, 7), (113, 1), (114, 2), (115, 2), (116, 3), (119, 1), (120, 1), (121, 3), (131, 2), (161, 1), (164, 1), (166, 2), (167, 8)]
discards: [ 0 72]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 1103.6702 - loglik: -1.0772e+03 - logprior: -2.6507e+01
Epoch 2/2
14/14 - 5s - loss: 1073.0601 - loglik: -1.0652e+03 - logprior: -7.8711e+00
Fitted a model with MAP estimate = -1066.9047
expansions: [(0, 4), (70, 2), (118, 1), (144, 1)]
discards: [  0  48  50  73  74 114 149 172]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1087.2983 - loglik: -1.0683e+03 - logprior: -1.9013e+01
Epoch 2/2
14/14 - 5s - loss: 1056.3698 - loglik: -1.0561e+03 - logprior: -2.3478e-01
Fitted a model with MAP estimate = -1057.5878
expansions: [(119, 1)]
discards: [  1   2   3  76  80 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1085.3073 - loglik: -1.0671e+03 - logprior: -1.8168e+01
Epoch 2/10
14/14 - 4s - loss: 1060.4226 - loglik: -1.0608e+03 - logprior: 0.3512
Epoch 3/10
14/14 - 4s - loss: 1056.3722 - loglik: -1.0596e+03 - logprior: 3.2064
Epoch 4/10
14/14 - 4s - loss: 1056.0813 - loglik: -1.0606e+03 - logprior: 4.4722
Epoch 5/10
14/14 - 4s - loss: 1055.6910 - loglik: -1.0610e+03 - logprior: 5.3050
Epoch 6/10
14/14 - 4s - loss: 1054.1202 - loglik: -1.0599e+03 - logprior: 5.7394
Epoch 7/10
14/14 - 4s - loss: 1053.8344 - loglik: -1.0600e+03 - logprior: 6.1471
Epoch 8/10
14/14 - 4s - loss: 1053.3602 - loglik: -1.0599e+03 - logprior: 6.4990
Epoch 9/10
14/14 - 4s - loss: 1052.6692 - loglik: -1.0595e+03 - logprior: 6.8297
Epoch 10/10
14/14 - 4s - loss: 1053.9670 - loglik: -1.0611e+03 - logprior: 7.1483
Fitted a model with MAP estimate = -1052.3878
Time for alignment: 113.0277
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1256.2219 - loglik: -1.2338e+03 - logprior: -2.2451e+01
Epoch 2/10
14/14 - 3s - loss: 1160.9652 - loglik: -1.1590e+03 - logprior: -1.9170e+00
Epoch 3/10
14/14 - 3s - loss: 1107.2007 - loglik: -1.1067e+03 - logprior: -4.8893e-01
Epoch 4/10
14/14 - 3s - loss: 1090.5143 - loglik: -1.0903e+03 - logprior: -2.0304e-01
Epoch 5/10
14/14 - 3s - loss: 1083.7284 - loglik: -1.0838e+03 - logprior: 0.0220
Epoch 6/10
14/14 - 3s - loss: 1083.0483 - loglik: -1.0832e+03 - logprior: 0.1916
Epoch 7/10
14/14 - 3s - loss: 1080.5028 - loglik: -1.0809e+03 - logprior: 0.3533
Epoch 8/10
14/14 - 3s - loss: 1081.3915 - loglik: -1.0819e+03 - logprior: 0.4800
Fitted a model with MAP estimate = -1080.9253
expansions: [(14, 3), (16, 1), (28, 1), (29, 1), (30, 2), (32, 1), (36, 1), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (52, 1), (53, 4), (89, 1), (114, 2), (115, 2), (116, 3), (118, 1), (120, 1), (121, 3), (125, 1), (154, 1), (164, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1104.9373 - loglik: -1.0785e+03 - logprior: -2.6473e+01
Epoch 2/2
14/14 - 4s - loss: 1072.8308 - loglik: -1.0651e+03 - logprior: -7.7064e+00
Fitted a model with MAP estimate = -1069.2680
expansions: [(0, 4), (70, 2), (137, 1)]
discards: [  0  14  51  73  74 142]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1087.5338 - loglik: -1.0686e+03 - logprior: -1.8893e+01
Epoch 2/2
14/14 - 4s - loss: 1062.9098 - loglik: -1.0629e+03 - logprior: 0.0386
Fitted a model with MAP estimate = -1061.1962
expansions: []
discards: [ 1  2  3 76]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1082.6964 - loglik: -1.0645e+03 - logprior: -1.8235e+01
Epoch 2/10
14/14 - 4s - loss: 1067.4237 - loglik: -1.0678e+03 - logprior: 0.4131
Epoch 3/10
14/14 - 4s - loss: 1060.3611 - loglik: -1.0636e+03 - logprior: 3.2780
Epoch 4/10
14/14 - 4s - loss: 1058.0323 - loglik: -1.0625e+03 - logprior: 4.5075
Epoch 5/10
14/14 - 4s - loss: 1055.8373 - loglik: -1.0612e+03 - logprior: 5.3534
Epoch 6/10
14/14 - 4s - loss: 1055.7809 - loglik: -1.0615e+03 - logprior: 5.7586
Epoch 7/10
14/14 - 4s - loss: 1056.9240 - loglik: -1.0631e+03 - logprior: 6.1586
Fitted a model with MAP estimate = -1055.4652
Time for alignment: 96.9861
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1257.5996 - loglik: -1.2352e+03 - logprior: -2.2441e+01
Epoch 2/10
14/14 - 3s - loss: 1166.7576 - loglik: -1.1648e+03 - logprior: -1.9754e+00
Epoch 3/10
14/14 - 3s - loss: 1105.0773 - loglik: -1.1043e+03 - logprior: -8.0283e-01
Epoch 4/10
14/14 - 3s - loss: 1089.5394 - loglik: -1.0889e+03 - logprior: -6.7604e-01
Epoch 5/10
14/14 - 3s - loss: 1083.0830 - loglik: -1.0827e+03 - logprior: -3.8385e-01
Epoch 6/10
14/14 - 3s - loss: 1084.3849 - loglik: -1.0842e+03 - logprior: -1.8905e-01
Fitted a model with MAP estimate = -1081.7099
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (29, 1), (30, 2), (32, 1), (36, 1), (40, 1), (41, 2), (42, 2), (50, 1), (51, 1), (52, 1), (53, 3), (80, 1), (89, 1), (112, 1), (113, 3), (114, 2), (115, 1), (116, 1), (118, 1), (119, 1), (120, 1), (125, 1), (130, 2), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1105.9174 - loglik: -1.0795e+03 - logprior: -2.6463e+01
Epoch 2/2
14/14 - 4s - loss: 1071.7994 - loglik: -1.0642e+03 - logprior: -7.6164e+00
Fitted a model with MAP estimate = -1069.7227
expansions: []
discards: [  0  17  49  54 205]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1094.2834 - loglik: -1.0686e+03 - logprior: -2.5684e+01
Epoch 2/2
14/14 - 4s - loss: 1068.6448 - loglik: -1.0629e+03 - logprior: -5.7164e+00
Fitted a model with MAP estimate = -1065.2608
expansions: [(0, 4)]
discards: [  0 161]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1084.3093 - loglik: -1.0659e+03 - logprior: -1.8362e+01
Epoch 2/10
14/14 - 4s - loss: 1060.5071 - loglik: -1.0610e+03 - logprior: 0.4861
Epoch 3/10
14/14 - 4s - loss: 1061.1321 - loglik: -1.0647e+03 - logprior: 3.5241
Fitted a model with MAP estimate = -1057.4193
Time for alignment: 74.0083
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1255.6342 - loglik: -1.2332e+03 - logprior: -2.2443e+01
Epoch 2/10
14/14 - 3s - loss: 1163.3236 - loglik: -1.1613e+03 - logprior: -1.9803e+00
Epoch 3/10
14/14 - 3s - loss: 1109.0182 - loglik: -1.1083e+03 - logprior: -6.9061e-01
Epoch 4/10
14/14 - 3s - loss: 1089.4183 - loglik: -1.0890e+03 - logprior: -4.1440e-01
Epoch 5/10
14/14 - 3s - loss: 1085.2190 - loglik: -1.0851e+03 - logprior: -1.5894e-01
Epoch 6/10
14/14 - 3s - loss: 1084.0428 - loglik: -1.0841e+03 - logprior: 0.0553
Epoch 7/10
14/14 - 3s - loss: 1084.9406 - loglik: -1.0851e+03 - logprior: 0.1667
Fitted a model with MAP estimate = -1082.9316
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (36, 1), (41, 2), (42, 1), (51, 1), (52, 1), (55, 4), (63, 1), (81, 1), (91, 6), (92, 2), (110, 1), (112, 1), (115, 2), (116, 3), (118, 1), (119, 2), (120, 3), (130, 2), (157, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1105.4655 - loglik: -1.0791e+03 - logprior: -2.6337e+01
Epoch 2/2
14/14 - 5s - loss: 1074.1169 - loglik: -1.0664e+03 - logprior: -7.7597e+00
Fitted a model with MAP estimate = -1068.1139
expansions: [(147, 2)]
discards: [  0  18  51 114 155 158 172 212]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1093.7230 - loglik: -1.0679e+03 - logprior: -2.5821e+01
Epoch 2/2
14/14 - 4s - loss: 1067.0980 - loglik: -1.0609e+03 - logprior: -6.2225e+00
Fitted a model with MAP estimate = -1063.0461
expansions: [(0, 4), (110, 1)]
discards: [  0  67 111]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1084.0305 - loglik: -1.0657e+03 - logprior: -1.8335e+01
Epoch 2/10
14/14 - 4s - loss: 1059.7985 - loglik: -1.0601e+03 - logprior: 0.3497
Epoch 3/10
14/14 - 4s - loss: 1056.4187 - loglik: -1.0598e+03 - logprior: 3.3512
Epoch 4/10
14/14 - 4s - loss: 1055.1384 - loglik: -1.0597e+03 - logprior: 4.5314
Epoch 5/10
14/14 - 4s - loss: 1054.3241 - loglik: -1.0594e+03 - logprior: 5.0562
Epoch 6/10
14/14 - 4s - loss: 1052.2889 - loglik: -1.0578e+03 - logprior: 5.4745
Epoch 7/10
14/14 - 4s - loss: 1055.0126 - loglik: -1.0609e+03 - logprior: 5.8844
Fitted a model with MAP estimate = -1052.5377
Time for alignment: 96.5646
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1256.9998 - loglik: -1.2346e+03 - logprior: -2.2447e+01
Epoch 2/10
14/14 - 3s - loss: 1167.6796 - loglik: -1.1657e+03 - logprior: -1.9760e+00
Epoch 3/10
14/14 - 3s - loss: 1107.2319 - loglik: -1.1065e+03 - logprior: -6.9497e-01
Epoch 4/10
14/14 - 3s - loss: 1092.2349 - loglik: -1.0917e+03 - logprior: -5.3603e-01
Epoch 5/10
14/14 - 3s - loss: 1086.1714 - loglik: -1.0859e+03 - logprior: -3.0277e-01
Epoch 6/10
14/14 - 3s - loss: 1083.2166 - loglik: -1.0831e+03 - logprior: -1.0750e-01
Epoch 7/10
14/14 - 3s - loss: 1084.2130 - loglik: -1.0843e+03 - logprior: 0.0503
Fitted a model with MAP estimate = -1082.7637
expansions: [(14, 1), (15, 1), (16, 2), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (55, 4), (63, 1), (79, 1), (81, 1), (90, 1), (110, 1), (114, 2), (115, 2), (116, 3), (118, 2), (119, 2), (120, 3), (130, 2), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1102.4427 - loglik: -1.0760e+03 - logprior: -2.6410e+01
Epoch 2/2
14/14 - 4s - loss: 1075.9736 - loglik: -1.0682e+03 - logprior: -7.7753e+00
Fitted a model with MAP estimate = -1069.0869
expansions: [(0, 3), (141, 1)]
discards: [  0  17  49 146 153 156 170 216]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1084.3107 - loglik: -1.0655e+03 - logprior: -1.8796e+01
Epoch 2/2
14/14 - 4s - loss: 1062.0717 - loglik: -1.0621e+03 - logprior: 0.0597
Fitted a model with MAP estimate = -1059.6731
expansions: [(71, 2)]
discards: [ 0  1 75 76 77 78]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1091.7261 - loglik: -1.0665e+03 - logprior: -2.5263e+01
Epoch 2/10
14/14 - 4s - loss: 1072.6339 - loglik: -1.0669e+03 - logprior: -5.7509e+00
Epoch 3/10
14/14 - 4s - loss: 1063.7611 - loglik: -1.0652e+03 - logprior: 1.4601
Epoch 4/10
14/14 - 4s - loss: 1056.9412 - loglik: -1.0615e+03 - logprior: 4.5666
Epoch 5/10
14/14 - 4s - loss: 1057.6949 - loglik: -1.0632e+03 - logprior: 5.4927
Fitted a model with MAP estimate = -1057.2371
Time for alignment: 86.7893
Computed alignments with likelihoods: ['-1052.3878', '-1055.4652', '-1057.4193', '-1052.5377', '-1057.2371']
Best model has likelihood: -1052.3878  (prior= 7.2926 )
time for generating output: 0.2514
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8981001727115717
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 1443.0135 - loglik: -1.4024e+03 - logprior: -4.0572e+01
Epoch 2/10
11/11 - 9s - loss: 1352.7734 - loglik: -1.3499e+03 - logprior: -2.8945e+00
Epoch 3/10
11/11 - 8s - loss: 1254.9813 - loglik: -1.2561e+03 - logprior: 1.1094
Epoch 4/10
11/11 - 9s - loss: 1204.2721 - loglik: -1.2059e+03 - logprior: 1.6356
Epoch 5/10
11/11 - 8s - loss: 1195.2676 - loglik: -1.1970e+03 - logprior: 1.7673
Epoch 6/10
11/11 - 8s - loss: 1185.2384 - loglik: -1.1874e+03 - logprior: 2.1201
Epoch 7/10
11/11 - 8s - loss: 1179.4937 - loglik: -1.1818e+03 - logprior: 2.3482
Epoch 8/10
11/11 - 8s - loss: 1184.4768 - loglik: -1.1870e+03 - logprior: 2.5271
Fitted a model with MAP estimate = -1180.4034
expansions: [(19, 4), (21, 1), (22, 1), (24, 1), (25, 1), (35, 1), (46, 1), (51, 2), (60, 1), (62, 1), (64, 1), (77, 1), (78, 2), (80, 1), (90, 1), (92, 1), (102, 4), (103, 1), (105, 1), (107, 1), (135, 1), (159, 2), (178, 1), (180, 2), (181, 2), (182, 1), (196, 3), (197, 1), (198, 2), (200, 1), (202, 5), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0 211]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 1227.7827 - loglik: -1.1807e+03 - logprior: -4.7051e+01
Epoch 2/2
11/11 - 11s - loss: 1165.3943 - loglik: -1.1529e+03 - logprior: -1.2466e+01
Fitted a model with MAP estimate = -1162.7931
expansions: [(0, 2)]
discards: [  0  21  61  93 124 292 293]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1195.9532 - loglik: -1.1614e+03 - logprior: -3.4602e+01
Epoch 2/2
11/11 - 11s - loss: 1149.0824 - loglik: -1.1491e+03 - logprior: 0.0540
Fitted a model with MAP estimate = -1148.8179
expansions: [(289, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1201.9421 - loglik: -1.1573e+03 - logprior: -4.4682e+01
Epoch 2/10
11/11 - 11s - loss: 1166.0884 - loglik: -1.1568e+03 - logprior: -9.2557e+00
Epoch 3/10
11/11 - 11s - loss: 1145.2990 - loglik: -1.1486e+03 - logprior: 3.3209
Epoch 4/10
11/11 - 10s - loss: 1139.3829 - loglik: -1.1494e+03 - logprior: 10.0610
Epoch 5/10
11/11 - 10s - loss: 1141.2014 - loglik: -1.1533e+03 - logprior: 12.1459
Fitted a model with MAP estimate = -1136.4682
Time for alignment: 192.2726
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 1445.8082 - loglik: -1.4052e+03 - logprior: -4.0567e+01
Epoch 2/10
11/11 - 8s - loss: 1344.9204 - loglik: -1.3420e+03 - logprior: -2.9067e+00
Epoch 3/10
11/11 - 8s - loss: 1257.8837 - loglik: -1.2592e+03 - logprior: 1.2777
Epoch 4/10
11/11 - 9s - loss: 1203.8717 - loglik: -1.2058e+03 - logprior: 1.8788
Epoch 5/10
11/11 - 8s - loss: 1187.5928 - loglik: -1.1896e+03 - logprior: 2.0416
Epoch 6/10
11/11 - 8s - loss: 1188.1700 - loglik: -1.1905e+03 - logprior: 2.3211
Fitted a model with MAP estimate = -1182.2721
expansions: [(20, 1), (21, 3), (25, 1), (26, 1), (36, 1), (42, 1), (49, 1), (61, 1), (63, 1), (65, 1), (78, 1), (79, 2), (80, 1), (91, 1), (93, 1), (103, 3), (104, 1), (105, 1), (106, 1), (137, 1), (161, 2), (162, 5), (179, 1), (180, 1), (181, 3), (197, 2), (198, 3), (199, 2), (201, 2), (203, 6), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0 211]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1226.4705 - loglik: -1.1791e+03 - logprior: -4.7412e+01
Epoch 2/2
11/11 - 11s - loss: 1171.8589 - loglik: -1.1591e+03 - logprior: -1.2806e+01
Fitted a model with MAP estimate = -1160.5422
expansions: [(0, 2), (195, 1), (214, 1)]
discards: [  0 123 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1188.4135 - loglik: -1.1537e+03 - logprior: -3.4679e+01
Epoch 2/2
11/11 - 11s - loss: 1142.2494 - loglik: -1.1424e+03 - logprior: 0.1634
Fitted a model with MAP estimate = -1140.8602
expansions: []
discards: [  0  93 277 293 295 296]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1205.4768 - loglik: -1.1606e+03 - logprior: -4.4892e+01
Epoch 2/10
11/11 - 11s - loss: 1156.9287 - loglik: -1.1465e+03 - logprior: -1.0433e+01
Epoch 3/10
11/11 - 11s - loss: 1147.0408 - loglik: -1.1476e+03 - logprior: 0.5712
Epoch 4/10
11/11 - 11s - loss: 1141.3829 - loglik: -1.1505e+03 - logprior: 9.0769
Epoch 5/10
11/11 - 11s - loss: 1134.9927 - loglik: -1.1468e+03 - logprior: 11.7613
Epoch 6/10
11/11 - 10s - loss: 1135.3237 - loglik: -1.1484e+03 - logprior: 13.0329
Fitted a model with MAP estimate = -1132.5938
Time for alignment: 184.8256
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1451.2216 - loglik: -1.4106e+03 - logprior: -4.0581e+01
Epoch 2/10
11/11 - 8s - loss: 1345.4276 - loglik: -1.3426e+03 - logprior: -2.8334e+00
Epoch 3/10
11/11 - 8s - loss: 1246.3501 - loglik: -1.2477e+03 - logprior: 1.3631
Epoch 4/10
11/11 - 8s - loss: 1200.0608 - loglik: -1.2019e+03 - logprior: 1.8444
Epoch 5/10
11/11 - 8s - loss: 1193.1345 - loglik: -1.1951e+03 - logprior: 1.9182
Epoch 6/10
11/11 - 7s - loss: 1182.4620 - loglik: -1.1846e+03 - logprior: 2.1298
Epoch 7/10
11/11 - 8s - loss: 1176.3462 - loglik: -1.1788e+03 - logprior: 2.4834
Epoch 8/10
11/11 - 8s - loss: 1183.1411 - loglik: -1.1860e+03 - logprior: 2.8687
Fitted a model with MAP estimate = -1178.7300
expansions: [(19, 4), (21, 1), (22, 1), (24, 1), (35, 1), (42, 1), (49, 1), (52, 2), (63, 1), (64, 3), (65, 1), (78, 1), (79, 1), (80, 1), (91, 2), (92, 1), (102, 4), (103, 1), (107, 4), (128, 1), (160, 2), (181, 1), (182, 2), (183, 1), (197, 1), (198, 5), (201, 2), (202, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1228.0764 - loglik: -1.1810e+03 - logprior: -4.7095e+01
Epoch 2/2
11/11 - 11s - loss: 1162.2225 - loglik: -1.1497e+03 - logprior: -1.2494e+01
Fitted a model with MAP estimate = -1159.0226
expansions: [(0, 2), (196, 4), (243, 2)]
discards: [  0  21  61  78  79  80 126 134 135 136]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1187.7598 - loglik: -1.1532e+03 - logprior: -3.4588e+01
Epoch 2/2
11/11 - 11s - loss: 1146.4559 - loglik: -1.1466e+03 - logprior: 0.1412
Fitted a model with MAP estimate = -1140.7419
expansions: [(194, 1)]
discards: [  0 275]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1195.5618 - loglik: -1.1509e+03 - logprior: -4.4613e+01
Epoch 2/10
11/11 - 11s - loss: 1157.7599 - loglik: -1.1485e+03 - logprior: -9.3005e+00
Epoch 3/10
11/11 - 11s - loss: 1136.4316 - loglik: -1.1396e+03 - logprior: 3.1194
Epoch 4/10
11/11 - 12s - loss: 1137.6865 - loglik: -1.1478e+03 - logprior: 10.0991
Fitted a model with MAP estimate = -1130.8415
Time for alignment: 182.4997
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1457.6017 - loglik: -1.4170e+03 - logprior: -4.0581e+01
Epoch 2/10
11/11 - 8s - loss: 1333.1204 - loglik: -1.3302e+03 - logprior: -2.9086e+00
Epoch 3/10
11/11 - 8s - loss: 1256.4547 - loglik: -1.2576e+03 - logprior: 1.1815
Epoch 4/10
11/11 - 8s - loss: 1207.4044 - loglik: -1.2092e+03 - logprior: 1.7817
Epoch 5/10
11/11 - 9s - loss: 1188.8156 - loglik: -1.1906e+03 - logprior: 1.8117
Epoch 6/10
11/11 - 8s - loss: 1189.5780 - loglik: -1.1916e+03 - logprior: 2.0129
Fitted a model with MAP estimate = -1180.7423
expansions: [(19, 4), (21, 1), (22, 1), (24, 1), (28, 1), (42, 1), (46, 1), (48, 1), (60, 1), (62, 1), (63, 3), (64, 1), (77, 1), (78, 1), (79, 1), (90, 1), (92, 1), (102, 3), (103, 1), (104, 1), (105, 1), (107, 4), (136, 1), (158, 1), (160, 1), (162, 5), (167, 2), (179, 1), (180, 1), (181, 3), (197, 4), (200, 2), (201, 2), (223, 1), (224, 1), (225, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1225.4133 - loglik: -1.1779e+03 - logprior: -4.7552e+01
Epoch 2/2
11/11 - 10s - loss: 1168.8905 - loglik: -1.1560e+03 - logprior: -1.2841e+01
Fitted a model with MAP estimate = -1158.7218
expansions: [(0, 2), (224, 1), (248, 3), (250, 1)]
discards: [  0  21  77  78  79 126 134 135 199 200]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1191.1885 - loglik: -1.1566e+03 - logprior: -3.4616e+01
Epoch 2/2
11/11 - 10s - loss: 1141.1300 - loglik: -1.1411e+03 - logprior: -3.4510e-02
Fitted a model with MAP estimate = -1141.8675
expansions: []
discards: [  0 241 277 295 296]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1198.6067 - loglik: -1.1539e+03 - logprior: -4.4676e+01
Epoch 2/10
11/11 - 10s - loss: 1168.9563 - loglik: -1.1594e+03 - logprior: -9.5611e+00
Epoch 3/10
11/11 - 10s - loss: 1139.4679 - loglik: -1.1426e+03 - logprior: 3.1702
Epoch 4/10
11/11 - 11s - loss: 1139.2816 - loglik: -1.1493e+03 - logprior: 9.9698
Epoch 5/10
11/11 - 11s - loss: 1131.3278 - loglik: -1.1434e+03 - logprior: 12.0296
Epoch 6/10
11/11 - 10s - loss: 1133.2905 - loglik: -1.1464e+03 - logprior: 13.1167
Fitted a model with MAP estimate = -1131.9499
Time for alignment: 185.6876
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 1450.3967 - loglik: -1.4098e+03 - logprior: -4.0561e+01
Epoch 2/10
11/11 - 8s - loss: 1341.3524 - loglik: -1.3385e+03 - logprior: -2.8873e+00
Epoch 3/10
11/11 - 8s - loss: 1255.6313 - loglik: -1.2570e+03 - logprior: 1.3832
Epoch 4/10
11/11 - 8s - loss: 1206.8033 - loglik: -1.2090e+03 - logprior: 2.1657
Epoch 5/10
11/11 - 8s - loss: 1186.6998 - loglik: -1.1892e+03 - logprior: 2.4837
Epoch 6/10
11/11 - 8s - loss: 1181.5518 - loglik: -1.1844e+03 - logprior: 2.8846
Epoch 7/10
11/11 - 8s - loss: 1184.4182 - loglik: -1.1876e+03 - logprior: 3.2284
Fitted a model with MAP estimate = -1180.6979
expansions: [(22, 4), (29, 1), (43, 1), (50, 1), (62, 1), (64, 1), (66, 1), (79, 3), (80, 2), (91, 1), (92, 1), (103, 4), (104, 1), (107, 5), (136, 1), (160, 2), (162, 5), (167, 3), (180, 1), (181, 3), (197, 4), (200, 2), (201, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0 208]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1228.9852 - loglik: -1.1815e+03 - logprior: -4.7495e+01
Epoch 2/2
11/11 - 12s - loss: 1170.9948 - loglik: -1.1581e+03 - logprior: -1.2850e+01
Fitted a model with MAP estimate = -1160.7268
expansions: [(0, 2), (219, 1), (243, 3)]
discards: [  0  92 121 128 129 130 195 202]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1190.6639 - loglik: -1.1558e+03 - logprior: -3.4827e+01
Epoch 2/2
11/11 - 12s - loss: 1148.5101 - loglik: -1.1487e+03 - logprior: 0.2018
Fitted a model with MAP estimate = -1142.1846
expansions: [(22, 3)]
discards: [  0 294 295]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1193.9823 - loglik: -1.1494e+03 - logprior: -4.4569e+01
Epoch 2/10
11/11 - 11s - loss: 1162.4728 - loglik: -1.1530e+03 - logprior: -9.5141e+00
Epoch 3/10
11/11 - 11s - loss: 1144.4692 - loglik: -1.1470e+03 - logprior: 2.5718
Epoch 4/10
11/11 - 11s - loss: 1136.4536 - loglik: -1.1465e+03 - logprior: 10.0069
Epoch 5/10
11/11 - 12s - loss: 1127.0663 - loglik: -1.1392e+03 - logprior: 12.1799
Epoch 6/10
11/11 - 10s - loss: 1131.4783 - loglik: -1.1448e+03 - logprior: 13.3299
Fitted a model with MAP estimate = -1129.4204
Time for alignment: 198.4176
Computed alignments with likelihoods: ['-1136.4682', '-1132.5938', '-1130.8415', '-1131.9499', '-1129.4204']
Best model has likelihood: -1129.4204  (prior= 13.7623 )
time for generating output: 0.5747
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.8961084086170952
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1660.1959 - loglik: -1.6543e+03 - logprior: -5.9448e+00
Epoch 2/10
21/21 - 15s - loss: 1537.4717 - loglik: -1.5368e+03 - logprior: -6.3949e-01
Epoch 3/10
21/21 - 15s - loss: 1483.9636 - loglik: -1.4815e+03 - logprior: -2.4991e+00
Epoch 4/10
21/21 - 15s - loss: 1477.4397 - loglik: -1.4749e+03 - logprior: -2.5478e+00
Epoch 5/10
21/21 - 15s - loss: 1474.8807 - loglik: -1.4725e+03 - logprior: -2.3782e+00
Epoch 6/10
21/21 - 15s - loss: 1473.7577 - loglik: -1.4714e+03 - logprior: -2.3465e+00
Epoch 7/10
21/21 - 15s - loss: 1473.2091 - loglik: -1.4708e+03 - logprior: -2.3758e+00
Epoch 8/10
21/21 - 15s - loss: 1469.3212 - loglik: -1.4669e+03 - logprior: -2.4061e+00
Epoch 9/10
21/21 - 15s - loss: 1477.7643 - loglik: -1.4754e+03 - logprior: -2.3659e+00
Fitted a model with MAP estimate = -1472.8165
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 3), (56, 1), (61, 2), (62, 2), (63, 1), (64, 1), (66, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (101, 1), (103, 1), (110, 1), (117, 1), (130, 1), (136, 1), (137, 1), (138, 1), (154, 1), (155, 1), (156, 1), (158, 2), (159, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (187, 1), (189, 1), (190, 1), (191, 1), (193, 2), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (211, 1), (212, 1), (218, 1), (223, 1), (227, 1), (228, 1), (229, 2), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1474.8693 - loglik: -1.4692e+03 - logprior: -5.6807e+00
Epoch 2/2
21/21 - 22s - loss: 1448.0623 - loglik: -1.4492e+03 - logprior: 1.1140
Fitted a model with MAP estimate = -1445.2724
expansions: [(19, 1), (76, 1), (145, 1), (184, 1)]
discards: [247 332]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1452.4760 - loglik: -1.4479e+03 - logprior: -4.5460e+00
Epoch 2/2
21/21 - 22s - loss: 1446.4108 - loglik: -1.4485e+03 - logprior: 2.0763
Fitted a model with MAP estimate = -1442.3445
expansions: []
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 1452.3064 - loglik: -1.4482e+03 - logprior: -4.0863e+00
Epoch 2/10
21/21 - 22s - loss: 1444.9316 - loglik: -1.4475e+03 - logprior: 2.5735
Epoch 3/10
21/21 - 22s - loss: 1441.8772 - loglik: -1.4452e+03 - logprior: 3.2945
Epoch 4/10
21/21 - 22s - loss: 1441.3884 - loglik: -1.4451e+03 - logprior: 3.7269
Epoch 5/10
21/21 - 22s - loss: 1432.2211 - loglik: -1.4361e+03 - logprior: 3.9080
Epoch 6/10
21/21 - 22s - loss: 1438.2504 - loglik: -1.4424e+03 - logprior: 4.1552
Fitted a model with MAP estimate = -1436.1365
Time for alignment: 430.8777
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 1654.8350 - loglik: -1.6489e+03 - logprior: -5.9228e+00
Epoch 2/10
21/21 - 15s - loss: 1538.7473 - loglik: -1.5381e+03 - logprior: -6.0350e-01
Epoch 3/10
21/21 - 15s - loss: 1489.6470 - loglik: -1.4874e+03 - logprior: -2.2400e+00
Epoch 4/10
21/21 - 15s - loss: 1478.1500 - loglik: -1.4759e+03 - logprior: -2.2338e+00
Epoch 5/10
21/21 - 15s - loss: 1474.6292 - loglik: -1.4725e+03 - logprior: -2.0980e+00
Epoch 6/10
21/21 - 15s - loss: 1478.6433 - loglik: -1.4765e+03 - logprior: -2.1232e+00
Fitted a model with MAP estimate = -1474.6654
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (49, 1), (51, 2), (53, 3), (54, 2), (59, 2), (60, 1), (61, 1), (63, 1), (73, 1), (74, 1), (75, 1), (77, 1), (79, 1), (80, 2), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (112, 1), (114, 1), (116, 1), (136, 1), (139, 1), (142, 2), (154, 1), (155, 1), (156, 1), (158, 2), (159, 1), (160, 1), (161, 1), (170, 1), (179, 1), (180, 1), (182, 1), (186, 1), (188, 1), (189, 1), (190, 1), (192, 1), (193, 1), (194, 1), (195, 1), (208, 1), (211, 1), (212, 1), (218, 1), (229, 5), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1474.5479 - loglik: -1.4662e+03 - logprior: -8.3855e+00
Epoch 2/2
21/21 - 22s - loss: 1447.8497 - loglik: -1.4461e+03 - logprior: -1.7154e+00
Fitted a model with MAP estimate = -1447.3943
expansions: [(0, 3)]
discards: [  0 181 333]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1457.1274 - loglik: -1.4524e+03 - logprior: -4.6935e+00
Epoch 2/2
21/21 - 22s - loss: 1444.8319 - loglik: -1.4468e+03 - logprior: 1.9362
Fitted a model with MAP estimate = -1442.7585
expansions: [(79, 1)]
discards: [ 1 57]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1456.9915 - loglik: -1.4528e+03 - logprior: -4.1910e+00
Epoch 2/10
21/21 - 22s - loss: 1442.7965 - loglik: -1.4451e+03 - logprior: 2.3262
Epoch 3/10
21/21 - 22s - loss: 1441.9146 - loglik: -1.4450e+03 - logprior: 3.0762
Epoch 4/10
21/21 - 22s - loss: 1441.9458 - loglik: -1.4454e+03 - logprior: 3.4092
Fitted a model with MAP estimate = -1438.5307
Time for alignment: 341.2410
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 1653.5846 - loglik: -1.6477e+03 - logprior: -5.9330e+00
Epoch 2/10
21/21 - 15s - loss: 1537.1942 - loglik: -1.5365e+03 - logprior: -6.7886e-01
Epoch 3/10
21/21 - 15s - loss: 1491.4763 - loglik: -1.4892e+03 - logprior: -2.3242e+00
Epoch 4/10
21/21 - 15s - loss: 1474.5651 - loglik: -1.4723e+03 - logprior: -2.2899e+00
Epoch 5/10
21/21 - 15s - loss: 1483.4318 - loglik: -1.4813e+03 - logprior: -2.1684e+00
Fitted a model with MAP estimate = -1475.5651
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 2), (54, 3), (55, 2), (60, 2), (61, 2), (64, 1), (65, 1), (74, 1), (75, 1), (76, 1), (79, 1), (80, 2), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (90, 1), (91, 1), (94, 1), (100, 1), (102, 1), (109, 1), (130, 1), (134, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (186, 1), (189, 3), (190, 2), (191, 1), (192, 2), (193, 1), (194, 1), (208, 1), (211, 1), (213, 1), (223, 1), (227, 1), (228, 2), (229, 2), (231, 1), (238, 1), (255, 1), (256, 1), (257, 1), (258, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1472.7227 - loglik: -1.4645e+03 - logprior: -8.2334e+00
Epoch 2/2
21/21 - 22s - loss: 1451.4390 - loglik: -1.4499e+03 - logprior: -1.5380e+00
Fitted a model with MAP estimate = -1447.8128
expansions: [(0, 3), (144, 1), (336, 1)]
discards: [  0 248 294 333]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1454.9017 - loglik: -1.4501e+03 - logprior: -4.8010e+00
Epoch 2/2
21/21 - 22s - loss: 1445.3213 - loglik: -1.4472e+03 - logprior: 1.9076
Fitted a model with MAP estimate = -1442.7854
expansions: []
discards: [ 1  3 57 58]
Re-initialized the encoder parameters.
Fitting a model of length 361 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1454.5933 - loglik: -1.4504e+03 - logprior: -4.2122e+00
Epoch 2/10
21/21 - 22s - loss: 1443.0590 - loglik: -1.4453e+03 - logprior: 2.2767
Epoch 3/10
21/21 - 22s - loss: 1446.4630 - loglik: -1.4494e+03 - logprior: 2.9649
Fitted a model with MAP estimate = -1440.4006
Time for alignment: 303.1684
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1657.2426 - loglik: -1.6513e+03 - logprior: -5.8996e+00
Epoch 2/10
21/21 - 15s - loss: 1536.7926 - loglik: -1.5361e+03 - logprior: -7.0033e-01
Epoch 3/10
21/21 - 15s - loss: 1487.4568 - loglik: -1.4850e+03 - logprior: -2.4827e+00
Epoch 4/10
21/21 - 15s - loss: 1468.9323 - loglik: -1.4667e+03 - logprior: -2.2493e+00
Epoch 5/10
21/21 - 15s - loss: 1476.5239 - loglik: -1.4744e+03 - logprior: -2.1396e+00
Fitted a model with MAP estimate = -1473.4219
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (52, 4), (53, 2), (55, 1), (60, 2), (61, 2), (64, 1), (75, 1), (76, 1), (77, 1), (78, 1), (80, 1), (81, 1), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (112, 1), (114, 1), (130, 1), (134, 1), (135, 1), (138, 1), (141, 1), (145, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (209, 1), (210, 1), (212, 1), (213, 1), (215, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (256, 1), (257, 2), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1473.9556 - loglik: -1.4656e+03 - logprior: -8.3961e+00
Epoch 2/2
21/21 - 22s - loss: 1445.4978 - loglik: -1.4434e+03 - logprior: -2.1079e+00
Fitted a model with MAP estimate = -1447.6628
expansions: [(0, 3), (76, 1)]
discards: [  0  59 295 335]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1455.7422 - loglik: -1.4512e+03 - logprior: -4.5847e+00
Epoch 2/2
21/21 - 22s - loss: 1442.9885 - loglik: -1.4449e+03 - logprior: 1.8700
Fitted a model with MAP estimate = -1442.9540
expansions: [(328, 1)]
discards: [  1   2 336]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1454.2377 - loglik: -1.4502e+03 - logprior: -4.0530e+00
Epoch 2/10
21/21 - 22s - loss: 1439.1162 - loglik: -1.4415e+03 - logprior: 2.3602
Epoch 3/10
21/21 - 22s - loss: 1449.7985 - loglik: -1.4528e+03 - logprior: 2.9672
Fitted a model with MAP estimate = -1439.9658
Time for alignment: 304.8671
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 1647.6709 - loglik: -1.6418e+03 - logprior: -5.9064e+00
Epoch 2/10
21/21 - 15s - loss: 1542.4772 - loglik: -1.5419e+03 - logprior: -5.9007e-01
Epoch 3/10
21/21 - 15s - loss: 1483.2042 - loglik: -1.4809e+03 - logprior: -2.2993e+00
Epoch 4/10
21/21 - 15s - loss: 1476.0179 - loglik: -1.4738e+03 - logprior: -2.2092e+00
Epoch 5/10
21/21 - 15s - loss: 1476.5223 - loglik: -1.4744e+03 - logprior: -2.0768e+00
Fitted a model with MAP estimate = -1473.7178
expansions: [(13, 1), (14, 2), (15, 1), (53, 4), (54, 3), (55, 2), (57, 1), (59, 2), (60, 2), (61, 1), (62, 2), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (84, 1), (85, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (100, 1), (109, 1), (111, 1), (113, 1), (129, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (181, 1), (184, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (211, 1), (213, 1), (214, 1), (216, 1), (220, 1), (224, 1), (228, 1), (229, 1), (230, 2), (232, 1), (255, 1), (256, 1), (257, 2), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1465.7938 - loglik: -1.4601e+03 - logprior: -5.6726e+00
Epoch 2/2
21/21 - 22s - loss: 1451.7640 - loglik: -1.4528e+03 - logprior: 1.0353
Fitted a model with MAP estimate = -1443.5717
expansions: []
discards: [ 59  61 338 339]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1454.4976 - loglik: -1.4498e+03 - logprior: -4.6605e+00
Epoch 2/2
21/21 - 22s - loss: 1440.0748 - loglik: -1.4420e+03 - logprior: 1.9703
Fitted a model with MAP estimate = -1442.6663
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1451.8456 - loglik: -1.4476e+03 - logprior: -4.2553e+00
Epoch 2/10
21/21 - 22s - loss: 1443.9788 - loglik: -1.4462e+03 - logprior: 2.1822
Epoch 3/10
21/21 - 22s - loss: 1444.5408 - loglik: -1.4476e+03 - logprior: 3.0569
Fitted a model with MAP estimate = -1439.6109
Time for alignment: 304.0307
Computed alignments with likelihoods: ['-1436.1365', '-1438.5307', '-1440.4006', '-1439.9658', '-1439.6109']
Best model has likelihood: -1436.1365  (prior= 4.1684 )
time for generating output: 0.3491
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9654387007328183
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1745.9172 - loglik: -1.7442e+03 - logprior: -1.6768e+00
Epoch 2/10
39/39 - 20s - loss: 1647.4816 - loglik: -1.6463e+03 - logprior: -1.2142e+00
Epoch 3/10
39/39 - 20s - loss: 1636.1863 - loglik: -1.6348e+03 - logprior: -1.3459e+00
Epoch 4/10
39/39 - 20s - loss: 1634.2218 - loglik: -1.6329e+03 - logprior: -1.3636e+00
Epoch 5/10
39/39 - 20s - loss: 1632.7966 - loglik: -1.6314e+03 - logprior: -1.3914e+00
Epoch 6/10
39/39 - 20s - loss: 1631.2549 - loglik: -1.6299e+03 - logprior: -1.3921e+00
Epoch 7/10
39/39 - 20s - loss: 1632.0372 - loglik: -1.6306e+03 - logprior: -1.4011e+00
Fitted a model with MAP estimate = -1405.0700
expansions: [(14, 1), (30, 1), (41, 1), (44, 1), (81, 3), (82, 2), (83, 2), (95, 2), (100, 3), (103, 2), (110, 1), (119, 1), (121, 3), (122, 3), (147, 8), (152, 1), (161, 3), (163, 2), (169, 2), (170, 5), (171, 3), (172, 2), (173, 1), (174, 1), (175, 1), (180, 2), (181, 1), (183, 5), (184, 1), (185, 2), (186, 1), (188, 2), (190, 2), (191, 3), (206, 1), (212, 4), (213, 2), (244, 3)]
discards: [  0 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 323 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 1629.5201 - loglik: -1.6266e+03 - logprior: -2.8715e+00
Epoch 2/2
39/39 - 30s - loss: 1612.9316 - loglik: -1.6112e+03 - logprior: -1.7146e+00
Fitted a model with MAP estimate = -1386.5052
expansions: [(0, 2), (119, 1), (120, 2), (121, 5), (172, 1), (173, 1), (206, 1), (215, 1), (323, 2)]
discards: [  0 113 137 188 189 190 191 192 193 194 217 239 245 252 320 321 322]
Re-initialized the encoder parameters.
Fitting a model of length 322 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1614.2106 - loglik: -1.6122e+03 - logprior: -2.0577e+00
Epoch 2/2
39/39 - 30s - loss: 1607.0399 - loglik: -1.6062e+03 - logprior: -8.8163e-01
Fitted a model with MAP estimate = -1382.3387
expansions: [(114, 1), (256, 2)]
discards: [  0 120 121 122 123 124 125 126 127 128 129 130 185 186 187 188 189 190
 191 192 193 320 321]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 1397.4309 - loglik: -1.3954e+03 - logprior: -1.9904e+00
Epoch 2/10
43/43 - 29s - loss: 1381.4922 - loglik: -1.3809e+03 - logprior: -5.9611e-01
Epoch 3/10
43/43 - 30s - loss: 1386.0769 - loglik: -1.3857e+03 - logprior: -3.2961e-01
Fitted a model with MAP estimate = -1382.0191
Time for alignment: 516.3674
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1749.5024 - loglik: -1.7478e+03 - logprior: -1.6830e+00
Epoch 2/10
39/39 - 20s - loss: 1649.6946 - loglik: -1.6483e+03 - logprior: -1.4167e+00
Epoch 3/10
39/39 - 20s - loss: 1639.6180 - loglik: -1.6381e+03 - logprior: -1.5594e+00
Epoch 4/10
39/39 - 20s - loss: 1636.7559 - loglik: -1.6352e+03 - logprior: -1.5830e+00
Epoch 5/10
39/39 - 20s - loss: 1635.0076 - loglik: -1.6334e+03 - logprior: -1.6016e+00
Epoch 6/10
39/39 - 20s - loss: 1634.1877 - loglik: -1.6326e+03 - logprior: -1.6309e+00
Epoch 7/10
39/39 - 20s - loss: 1633.9967 - loglik: -1.6324e+03 - logprior: -1.6333e+00
Epoch 8/10
39/39 - 20s - loss: 1634.1412 - loglik: -1.6325e+03 - logprior: -1.6240e+00
Fitted a model with MAP estimate = -1407.5542
expansions: [(14, 1), (19, 1), (42, 1), (43, 1), (54, 1), (79, 4), (80, 2), (81, 2), (91, 2), (93, 3), (101, 1), (114, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (144, 1), (145, 2), (146, 10), (166, 1), (167, 6), (170, 2), (171, 3), (172, 1), (179, 1), (184, 2), (185, 1), (186, 2), (187, 3), (188, 1), (192, 1), (194, 1), (208, 2), (209, 1), (213, 4), (224, 1), (244, 3)]
discards: [  0 102 103 104 105 106 107 158 159 160 161 162]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1635.0852 - loglik: -1.6322e+03 - logprior: -2.8989e+00
Epoch 2/2
39/39 - 28s - loss: 1619.1791 - loglik: -1.6175e+03 - logprior: -1.6331e+00
Fitted a model with MAP estimate = -1391.8683
expansions: [(0, 2), (120, 6), (121, 4), (231, 1), (307, 2)]
discards: [  0  91 104 108 116 117 189 190 191 195 226 269 270 304 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1618.9635 - loglik: -1.6171e+03 - logprior: -1.8898e+00
Epoch 2/2
39/39 - 28s - loss: 1610.6802 - loglik: -1.6100e+03 - logprior: -6.9521e-01
Fitted a model with MAP estimate = -1385.9842
expansions: [(117, 1), (119, 2), (125, 1), (126, 2)]
discards: [  0 112 113 114 115 304 305]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 1388.5150 - loglik: -1.3865e+03 - logprior: -2.0001e+00
Epoch 2/10
43/43 - 30s - loss: 1385.8545 - loglik: -1.3854e+03 - logprior: -4.6663e-01
Epoch 3/10
43/43 - 30s - loss: 1380.3223 - loglik: -1.3800e+03 - logprior: -2.7241e-01
Epoch 4/10
43/43 - 30s - loss: 1377.4926 - loglik: -1.3773e+03 - logprior: -1.9069e-01
Epoch 5/10
43/43 - 30s - loss: 1374.3505 - loglik: -1.3742e+03 - logprior: -1.1595e-01
Epoch 6/10
43/43 - 30s - loss: 1376.6649 - loglik: -1.3766e+03 - logprior: -2.5014e-02
Fitted a model with MAP estimate = -1376.8993
Time for alignment: 610.8089
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1749.4031 - loglik: -1.7477e+03 - logprior: -1.6793e+00
Epoch 2/10
39/39 - 20s - loss: 1651.8124 - loglik: -1.6504e+03 - logprior: -1.3914e+00
Epoch 3/10
39/39 - 20s - loss: 1640.2190 - loglik: -1.6387e+03 - logprior: -1.4828e+00
Epoch 4/10
39/39 - 20s - loss: 1636.3737 - loglik: -1.6349e+03 - logprior: -1.4998e+00
Epoch 5/10
39/39 - 20s - loss: 1635.3051 - loglik: -1.6338e+03 - logprior: -1.5191e+00
Epoch 6/10
39/39 - 20s - loss: 1633.8654 - loglik: -1.6323e+03 - logprior: -1.5216e+00
Epoch 7/10
39/39 - 20s - loss: 1634.2543 - loglik: -1.6327e+03 - logprior: -1.5121e+00
Fitted a model with MAP estimate = -1407.3678
expansions: [(14, 1), (27, 1), (40, 1), (53, 1), (55, 1), (56, 1), (79, 3), (80, 2), (81, 1), (83, 1), (88, 1), (91, 3), (93, 3), (99, 1), (103, 2), (117, 1), (118, 1), (119, 1), (120, 3), (121, 2), (150, 5), (151, 1), (167, 5), (171, 2), (172, 2), (173, 1), (174, 1), (177, 2), (178, 2), (179, 3), (185, 1), (186, 1), (187, 1), (190, 2), (191, 2), (206, 1), (207, 1), (212, 4), (218, 1), (244, 3)]
discards: [  0 104 105 106 107 158 159 160 161]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1631.4329 - loglik: -1.6287e+03 - logprior: -2.7633e+00
Epoch 2/2
39/39 - 28s - loss: 1615.3932 - loglik: -1.6139e+03 - logprior: -1.5410e+00
Fitted a model with MAP estimate = -1388.9803
expansions: [(0, 2), (122, 1), (124, 1), (125, 2), (127, 4), (218, 1), (307, 2)]
discards: [  0  34  86  95 106 110 145 195 240 268 269 304 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1616.3820 - loglik: -1.6145e+03 - logprior: -1.8378e+00
Epoch 2/2
39/39 - 28s - loss: 1608.9707 - loglik: -1.6084e+03 - logprior: -6.1752e-01
Fitted a model with MAP estimate = -1383.6933
expansions: [(269, 2)]
discards: [  0  94 112 126 304 305]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 1387.1910 - loglik: -1.3852e+03 - logprior: -1.9829e+00
Epoch 2/10
43/43 - 30s - loss: 1386.2574 - loglik: -1.3858e+03 - logprior: -4.6128e-01
Epoch 3/10
43/43 - 30s - loss: 1384.8988 - loglik: -1.3847e+03 - logprior: -2.3710e-01
Epoch 4/10
43/43 - 30s - loss: 1373.8944 - loglik: -1.3737e+03 - logprior: -1.6381e-01
Epoch 5/10
43/43 - 30s - loss: 1380.3263 - loglik: -1.3802e+03 - logprior: -9.3480e-02
Fitted a model with MAP estimate = -1378.0103
Time for alignment: 558.1157
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1746.8212 - loglik: -1.7451e+03 - logprior: -1.6734e+00
Epoch 2/10
39/39 - 20s - loss: 1651.5035 - loglik: -1.6503e+03 - logprior: -1.2383e+00
Epoch 3/10
39/39 - 20s - loss: 1640.9724 - loglik: -1.6397e+03 - logprior: -1.3151e+00
Epoch 4/10
39/39 - 20s - loss: 1636.2471 - loglik: -1.6349e+03 - logprior: -1.3328e+00
Epoch 5/10
39/39 - 20s - loss: 1634.3461 - loglik: -1.6330e+03 - logprior: -1.3623e+00
Epoch 6/10
39/39 - 20s - loss: 1635.2568 - loglik: -1.6339e+03 - logprior: -1.3697e+00
Fitted a model with MAP estimate = -1407.4281
expansions: [(14, 1), (41, 1), (57, 1), (82, 3), (83, 2), (97, 3), (102, 3), (110, 1), (111, 1), (112, 3), (120, 1), (121, 4), (122, 3), (128, 2), (147, 3), (152, 1), (159, 2), (160, 1), (161, 1), (173, 1), (178, 2), (179, 4), (180, 2), (181, 2), (185, 1), (186, 1), (187, 2), (189, 4), (190, 1), (205, 1), (206, 1), (224, 1), (244, 3)]
discards: [  0  34 106 107 164 165 166 167 168 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1633.8318 - loglik: -1.6310e+03 - logprior: -2.7870e+00
Epoch 2/2
39/39 - 26s - loss: 1618.0747 - loglik: -1.6165e+03 - logprior: -1.5347e+00
Fitted a model with MAP estimate = -1390.7405
expansions: [(0, 2), (172, 2), (175, 3), (187, 1), (296, 2)]
discards: [  0 118 119 120 121 122 124 125 141 151 293 294 295]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1621.1007 - loglik: -1.6192e+03 - logprior: -1.8595e+00
Epoch 2/2
39/39 - 26s - loss: 1614.2579 - loglik: -1.6137e+03 - logprior: -5.8444e-01
Fitted a model with MAP estimate = -1388.4381
expansions: [(119, 6), (172, 1)]
discards: [  0 291 292]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 1388.3778 - loglik: -1.3865e+03 - logprior: -1.9217e+00
Epoch 2/10
43/43 - 29s - loss: 1387.8521 - loglik: -1.3873e+03 - logprior: -5.1843e-01
Epoch 3/10
43/43 - 29s - loss: 1377.8309 - loglik: -1.3776e+03 - logprior: -2.5625e-01
Epoch 4/10
43/43 - 29s - loss: 1385.5840 - loglik: -1.3854e+03 - logprior: -1.7107e-01
Fitted a model with MAP estimate = -1379.5147
Time for alignment: 492.0476
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1748.4363 - loglik: -1.7468e+03 - logprior: -1.6791e+00
Epoch 2/10
39/39 - 20s - loss: 1650.0634 - loglik: -1.6488e+03 - logprior: -1.3077e+00
Epoch 3/10
39/39 - 20s - loss: 1636.8657 - loglik: -1.6354e+03 - logprior: -1.4356e+00
Epoch 4/10
39/39 - 20s - loss: 1632.5033 - loglik: -1.6310e+03 - logprior: -1.4886e+00
Epoch 5/10
39/39 - 20s - loss: 1631.3346 - loglik: -1.6298e+03 - logprior: -1.5011e+00
Epoch 6/10
39/39 - 20s - loss: 1630.6926 - loglik: -1.6292e+03 - logprior: -1.5283e+00
Epoch 7/10
39/39 - 20s - loss: 1631.1962 - loglik: -1.6297e+03 - logprior: -1.5446e+00
Fitted a model with MAP estimate = -1404.3670
expansions: [(14, 1), (31, 1), (57, 1), (76, 1), (78, 1), (80, 1), (81, 4), (83, 1), (95, 1), (100, 3), (103, 1), (110, 1), (119, 1), (120, 1), (121, 1), (122, 4), (123, 2), (146, 1), (151, 1), (154, 1), (157, 1), (158, 1), (160, 5), (168, 1), (169, 4), (170, 5), (171, 4), (173, 1), (174, 1), (179, 3), (185, 5), (186, 1), (187, 1), (188, 2), (190, 3), (191, 1), (212, 4), (218, 1), (244, 3)]
discards: [  0 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 1627.5902 - loglik: -1.6247e+03 - logprior: -2.8801e+00
Epoch 2/2
39/39 - 29s - loss: 1612.6810 - loglik: -1.6110e+03 - logprior: -1.6673e+00
Fitted a model with MAP estimate = -1386.5991
expansions: [(0, 2), (119, 7), (237, 1), (315, 2)]
discards: [  0 112 113 139 142 169 189 202 207 275 276 277 312 313 314]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1613.4779 - loglik: -1.6115e+03 - logprior: -1.9820e+00
Epoch 2/2
39/39 - 29s - loss: 1606.6863 - loglik: -1.6059e+03 - logprior: -7.7469e-01
Fitted a model with MAP estimate = -1381.9212
expansions: [(112, 1), (129, 3), (214, 2), (240, 1)]
discards: [  0 118 120 121 122 123 124 125 126 127 310 311]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 1390.3455 - loglik: -1.3883e+03 - logprior: -2.0650e+00
Epoch 2/10
43/43 - 30s - loss: 1379.5305 - loglik: -1.3789e+03 - logprior: -6.3029e-01
Epoch 3/10
43/43 - 31s - loss: 1377.1838 - loglik: -1.3767e+03 - logprior: -4.9481e-01
Epoch 4/10
43/43 - 30s - loss: 1383.0475 - loglik: -1.3827e+03 - logprior: -3.0435e-01
Fitted a model with MAP estimate = -1377.9163
Time for alignment: 536.7218
Computed alignments with likelihoods: ['-1382.0191', '-1376.8993', '-1378.0103', '-1379.5147', '-1377.9163']
Best model has likelihood: -1376.8993  (prior= 0.0074 )
time for generating output: 0.4767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5252057332824677
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 518.8080 - loglik: -4.8067e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 467.9112 - loglik: -4.5784e+02 - logprior: -1.0069e+01
Epoch 3/10
10/10 - 1s - loss: 441.5571 - loglik: -4.3644e+02 - logprior: -5.1162e+00
Epoch 4/10
10/10 - 1s - loss: 424.4238 - loglik: -4.2094e+02 - logprior: -3.4870e+00
Epoch 5/10
10/10 - 1s - loss: 417.5381 - loglik: -4.1472e+02 - logprior: -2.8181e+00
Epoch 6/10
10/10 - 1s - loss: 414.3776 - loglik: -4.1184e+02 - logprior: -2.5411e+00
Epoch 7/10
10/10 - 1s - loss: 412.8974 - loglik: -4.1086e+02 - logprior: -2.0368e+00
Epoch 8/10
10/10 - 1s - loss: 412.2090 - loglik: -4.1056e+02 - logprior: -1.6465e+00
Epoch 9/10
10/10 - 1s - loss: 411.5996 - loglik: -4.1003e+02 - logprior: -1.5746e+00
Epoch 10/10
10/10 - 1s - loss: 411.1458 - loglik: -4.0960e+02 - logprior: -1.5479e+00
Fitted a model with MAP estimate = -410.9876
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 2), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3884 - loglik: -4.0732e+02 - logprior: -5.0065e+01
Epoch 2/2
10/10 - 1s - loss: 413.1142 - loglik: -3.9811e+02 - logprior: -1.5004e+01
Fitted a model with MAP estimate = -405.4963
expansions: []
discards: [ 0 18 26]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 442.9915 - loglik: -3.9964e+02 - logprior: -4.3349e+01
Epoch 2/2
10/10 - 1s - loss: 415.0353 - loglik: -3.9815e+02 - logprior: -1.6881e+01
Fitted a model with MAP estimate = -410.4860
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 437.3541 - loglik: -3.9776e+02 - logprior: -3.9598e+01
Epoch 2/10
10/10 - 1s - loss: 407.6812 - loglik: -3.9687e+02 - logprior: -1.0809e+01
Epoch 3/10
10/10 - 1s - loss: 400.1331 - loglik: -3.9605e+02 - logprior: -4.0854e+00
Epoch 4/10
10/10 - 1s - loss: 397.2201 - loglik: -3.9523e+02 - logprior: -1.9949e+00
Epoch 5/10
10/10 - 1s - loss: 396.4223 - loglik: -3.9549e+02 - logprior: -9.3423e-01
Epoch 6/10
10/10 - 1s - loss: 395.4394 - loglik: -3.9534e+02 - logprior: -9.9937e-02
Epoch 7/10
10/10 - 1s - loss: 394.9883 - loglik: -3.9536e+02 - logprior: 0.3757
Epoch 8/10
10/10 - 1s - loss: 394.9086 - loglik: -3.9553e+02 - logprior: 0.6238
Epoch 9/10
10/10 - 1s - loss: 394.6270 - loglik: -3.9546e+02 - logprior: 0.8359
Epoch 10/10
10/10 - 1s - loss: 394.4653 - loglik: -3.9549e+02 - logprior: 1.0227
Fitted a model with MAP estimate = -394.4248
Time for alignment: 44.4757
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 518.6110 - loglik: -4.8047e+02 - logprior: -3.8140e+01
Epoch 2/10
10/10 - 1s - loss: 468.1598 - loglik: -4.5810e+02 - logprior: -1.0063e+01
Epoch 3/10
10/10 - 1s - loss: 440.4948 - loglik: -4.3540e+02 - logprior: -5.0911e+00
Epoch 4/10
10/10 - 1s - loss: 426.1518 - loglik: -4.2264e+02 - logprior: -3.5087e+00
Epoch 5/10
10/10 - 1s - loss: 419.7386 - loglik: -4.1715e+02 - logprior: -2.5875e+00
Epoch 6/10
10/10 - 1s - loss: 417.1285 - loglik: -4.1504e+02 - logprior: -2.0891e+00
Epoch 7/10
10/10 - 1s - loss: 414.9896 - loglik: -4.1316e+02 - logprior: -1.8314e+00
Epoch 8/10
10/10 - 1s - loss: 414.4755 - loglik: -4.1274e+02 - logprior: -1.7374e+00
Epoch 9/10
10/10 - 1s - loss: 412.4226 - loglik: -4.1067e+02 - logprior: -1.7492e+00
Epoch 10/10
10/10 - 1s - loss: 413.0851 - loglik: -4.1133e+02 - logprior: -1.7554e+00
Fitted a model with MAP estimate = -412.3055
expansions: [(7, 2), (13, 1), (15, 2), (18, 1), (20, 1), (21, 1), (23, 2), (30, 1), (35, 2), (46, 1), (55, 2), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 452.5926 - loglik: -4.0980e+02 - logprior: -4.2797e+01
Epoch 2/2
10/10 - 1s - loss: 417.3996 - loglik: -3.9998e+02 - logprior: -1.7417e+01
Fitted a model with MAP estimate = -411.8458
expansions: [(0, 2)]
discards: [ 0 18 27 45]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 431.2411 - loglik: -3.9741e+02 - logprior: -3.3831e+01
Epoch 2/2
10/10 - 1s - loss: 404.3134 - loglik: -3.9572e+02 - logprior: -8.5957e+00
Fitted a model with MAP estimate = -400.4167
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 437.7870 - loglik: -3.9841e+02 - logprior: -3.9373e+01
Epoch 2/10
10/10 - 1s - loss: 407.5347 - loglik: -3.9685e+02 - logprior: -1.0686e+01
Epoch 3/10
10/10 - 1s - loss: 400.2225 - loglik: -3.9614e+02 - logprior: -4.0796e+00
Epoch 4/10
10/10 - 1s - loss: 397.8301 - loglik: -3.9580e+02 - logprior: -2.0271e+00
Epoch 5/10
10/10 - 1s - loss: 395.9582 - loglik: -3.9498e+02 - logprior: -9.7721e-01
Epoch 6/10
10/10 - 1s - loss: 395.5373 - loglik: -3.9538e+02 - logprior: -1.5282e-01
Epoch 7/10
10/10 - 1s - loss: 395.1264 - loglik: -3.9547e+02 - logprior: 0.3428
Epoch 8/10
10/10 - 1s - loss: 395.1876 - loglik: -3.9577e+02 - logprior: 0.5872
Fitted a model with MAP estimate = -394.8645
Time for alignment: 41.2063
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 518.3590 - loglik: -4.8022e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 468.4595 - loglik: -4.5839e+02 - logprior: -1.0071e+01
Epoch 3/10
10/10 - 1s - loss: 439.1015 - loglik: -4.3402e+02 - logprior: -5.0842e+00
Epoch 4/10
10/10 - 1s - loss: 423.0288 - loglik: -4.1957e+02 - logprior: -3.4558e+00
Epoch 5/10
10/10 - 1s - loss: 416.5375 - loglik: -4.1373e+02 - logprior: -2.8099e+00
Epoch 6/10
10/10 - 1s - loss: 414.3865 - loglik: -4.1185e+02 - logprior: -2.5330e+00
Epoch 7/10
10/10 - 1s - loss: 412.3942 - loglik: -4.1034e+02 - logprior: -2.0537e+00
Epoch 8/10
10/10 - 1s - loss: 411.6347 - loglik: -4.0995e+02 - logprior: -1.6896e+00
Epoch 9/10
10/10 - 1s - loss: 411.3903 - loglik: -4.0977e+02 - logprior: -1.6165e+00
Epoch 10/10
10/10 - 1s - loss: 410.4225 - loglik: -4.0881e+02 - logprior: -1.6134e+00
Fitted a model with MAP estimate = -410.5488
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 1), (23, 2), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 455.9513 - loglik: -4.0582e+02 - logprior: -5.0136e+01
Epoch 2/2
10/10 - 1s - loss: 412.8253 - loglik: -3.9785e+02 - logprior: -1.4974e+01
Fitted a model with MAP estimate = -405.1103
expansions: []
discards: [ 0 18]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 442.2345 - loglik: -3.9886e+02 - logprior: -4.3377e+01
Epoch 2/2
10/10 - 1s - loss: 415.1887 - loglik: -3.9827e+02 - logprior: -1.6922e+01
Fitted a model with MAP estimate = -410.4584
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 436.9085 - loglik: -3.9727e+02 - logprior: -3.9635e+01
Epoch 2/10
10/10 - 1s - loss: 408.0596 - loglik: -3.9722e+02 - logprior: -1.0836e+01
Epoch 3/10
10/10 - 1s - loss: 400.1880 - loglik: -3.9607e+02 - logprior: -4.1212e+00
Epoch 4/10
10/10 - 1s - loss: 397.6085 - loglik: -3.9557e+02 - logprior: -2.0404e+00
Epoch 5/10
10/10 - 1s - loss: 395.9437 - loglik: -3.9497e+02 - logprior: -9.7051e-01
Epoch 6/10
10/10 - 1s - loss: 395.1412 - loglik: -3.9501e+02 - logprior: -1.2814e-01
Epoch 7/10
10/10 - 1s - loss: 395.6435 - loglik: -3.9598e+02 - logprior: 0.3392
Fitted a model with MAP estimate = -395.0242
Time for alignment: 39.1456
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 518.8334 - loglik: -4.8070e+02 - logprior: -3.8138e+01
Epoch 2/10
10/10 - 1s - loss: 467.7604 - loglik: -4.5770e+02 - logprior: -1.0064e+01
Epoch 3/10
10/10 - 1s - loss: 441.0217 - loglik: -4.3595e+02 - logprior: -5.0729e+00
Epoch 4/10
10/10 - 1s - loss: 427.0736 - loglik: -4.2369e+02 - logprior: -3.3799e+00
Epoch 5/10
10/10 - 1s - loss: 419.6912 - loglik: -4.1718e+02 - logprior: -2.5090e+00
Epoch 6/10
10/10 - 1s - loss: 416.9930 - loglik: -4.1493e+02 - logprior: -2.0639e+00
Epoch 7/10
10/10 - 1s - loss: 415.7088 - loglik: -4.1391e+02 - logprior: -1.7988e+00
Epoch 8/10
10/10 - 1s - loss: 415.0789 - loglik: -4.1350e+02 - logprior: -1.5751e+00
Epoch 9/10
10/10 - 1s - loss: 414.3294 - loglik: -4.1288e+02 - logprior: -1.4476e+00
Epoch 10/10
10/10 - 1s - loss: 414.3246 - loglik: -4.1290e+02 - logprior: -1.4280e+00
Fitted a model with MAP estimate = -413.8496
expansions: [(12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (35, 2), (46, 1), (55, 2), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 443.8224 - loglik: -4.0930e+02 - logprior: -3.4520e+01
Epoch 2/2
10/10 - 1s - loss: 409.9889 - loglik: -4.0086e+02 - logprior: -9.1326e+00
Fitted a model with MAP estimate = -405.1400
expansions: [(7, 2)]
discards: [ 0 16 24 45]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 443.6454 - loglik: -4.0141e+02 - logprior: -4.2236e+01
Epoch 2/2
10/10 - 1s - loss: 414.7489 - loglik: -3.9762e+02 - logprior: -1.7132e+01
Fitted a model with MAP estimate = -410.6702
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 429.1400 - loglik: -3.9565e+02 - logprior: -3.3485e+01
Epoch 2/10
10/10 - 1s - loss: 403.5270 - loglik: -3.9522e+02 - logprior: -8.3093e+00
Epoch 3/10
10/10 - 1s - loss: 397.9344 - loglik: -3.9471e+02 - logprior: -3.2253e+00
Epoch 4/10
10/10 - 1s - loss: 395.8797 - loglik: -3.9464e+02 - logprior: -1.2385e+00
Epoch 5/10
10/10 - 1s - loss: 394.7875 - loglik: -3.9454e+02 - logprior: -2.4278e-01
Epoch 6/10
10/10 - 1s - loss: 393.9175 - loglik: -3.9420e+02 - logprior: 0.2798
Epoch 7/10
10/10 - 1s - loss: 394.3885 - loglik: -3.9498e+02 - logprior: 0.5886
Fitted a model with MAP estimate = -393.8101
Time for alignment: 39.6743
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 518.6453 - loglik: -4.8051e+02 - logprior: -3.8138e+01
Epoch 2/10
10/10 - 1s - loss: 467.7010 - loglik: -4.5764e+02 - logprior: -1.0066e+01
Epoch 3/10
10/10 - 1s - loss: 440.7076 - loglik: -4.3564e+02 - logprior: -5.0669e+00
Epoch 4/10
10/10 - 1s - loss: 425.3611 - loglik: -4.2195e+02 - logprior: -3.4133e+00
Epoch 5/10
10/10 - 1s - loss: 419.1675 - loglik: -4.1660e+02 - logprior: -2.5687e+00
Epoch 6/10
10/10 - 1s - loss: 416.8548 - loglik: -4.1475e+02 - logprior: -2.1073e+00
Epoch 7/10
10/10 - 1s - loss: 415.4907 - loglik: -4.1364e+02 - logprior: -1.8474e+00
Epoch 8/10
10/10 - 1s - loss: 415.3694 - loglik: -4.1364e+02 - logprior: -1.7244e+00
Epoch 9/10
10/10 - 1s - loss: 413.4531 - loglik: -4.1175e+02 - logprior: -1.7042e+00
Epoch 10/10
10/10 - 1s - loss: 413.0039 - loglik: -4.1128e+02 - logprior: -1.7189e+00
Fitted a model with MAP estimate = -412.9650
expansions: [(7, 2), (12, 1), (15, 2), (20, 1), (21, 2), (23, 2), (30, 1), (32, 1), (46, 1), (55, 2), (58, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 452.6339 - loglik: -4.0986e+02 - logprior: -4.2774e+01
Epoch 2/2
10/10 - 1s - loss: 418.4063 - loglik: -4.0111e+02 - logprior: -1.7292e+01
Fitted a model with MAP estimate = -412.2248
expansions: [(0, 2)]
discards: [ 0 18]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 430.9699 - loglik: -3.9719e+02 - logprior: -3.3782e+01
Epoch 2/2
10/10 - 1s - loss: 403.9768 - loglik: -3.9545e+02 - logprior: -8.5307e+00
Fitted a model with MAP estimate = -400.0179
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 436.9129 - loglik: -3.9760e+02 - logprior: -3.9312e+01
Epoch 2/10
10/10 - 1s - loss: 407.5189 - loglik: -3.9689e+02 - logprior: -1.0627e+01
Epoch 3/10
10/10 - 1s - loss: 399.9959 - loglik: -3.9597e+02 - logprior: -4.0303e+00
Epoch 4/10
10/10 - 1s - loss: 397.0093 - loglik: -3.9503e+02 - logprior: -1.9792e+00
Epoch 5/10
10/10 - 1s - loss: 395.7834 - loglik: -3.9485e+02 - logprior: -9.3568e-01
Epoch 6/10
10/10 - 1s - loss: 394.9359 - loglik: -3.9483e+02 - logprior: -1.0344e-01
Epoch 7/10
10/10 - 1s - loss: 394.9112 - loglik: -3.9530e+02 - logprior: 0.3855
Epoch 8/10
10/10 - 1s - loss: 394.7518 - loglik: -3.9538e+02 - logprior: 0.6311
Epoch 9/10
10/10 - 1s - loss: 394.4180 - loglik: -3.9525e+02 - logprior: 0.8284
Epoch 10/10
10/10 - 1s - loss: 394.0461 - loglik: -3.9506e+02 - logprior: 1.0124
Fitted a model with MAP estimate = -394.1484
Time for alignment: 42.8151
Computed alignments with likelihoods: ['-394.4248', '-394.8645', '-395.0242', '-393.8101', '-394.1484']
Best model has likelihood: -393.8101  (prior= 0.7296 )
time for generating output: 0.1934
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9256852218140718
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 615.8779 - loglik: -6.0490e+02 - logprior: -1.0973e+01
Epoch 2/10
19/19 - 4s - loss: 550.6356 - loglik: -5.4842e+02 - logprior: -2.2165e+00
Epoch 3/10
19/19 - 5s - loss: 533.6837 - loglik: -5.3179e+02 - logprior: -1.8890e+00
Epoch 4/10
19/19 - 5s - loss: 528.7308 - loglik: -5.2707e+02 - logprior: -1.6567e+00
Epoch 5/10
19/19 - 5s - loss: 528.6489 - loglik: -5.2721e+02 - logprior: -1.4374e+00
Epoch 6/10
19/19 - 5s - loss: 528.1028 - loglik: -5.2667e+02 - logprior: -1.4338e+00
Epoch 7/10
19/19 - 5s - loss: 529.0969 - loglik: -5.2769e+02 - logprior: -1.4098e+00
Fitted a model with MAP estimate = -527.7637
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (30, 1), (48, 6), (49, 2), (57, 1), (72, 1), (73, 4), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 535.2972 - loglik: -5.2171e+02 - logprior: -1.3590e+01
Epoch 2/2
19/19 - 5s - loss: 517.3239 - loglik: -5.1282e+02 - logprior: -4.5033e+00
Fitted a model with MAP estimate = -513.8565
expansions: [(0, 2)]
discards: [ 0 12 64 92]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 522.3953 - loglik: -5.1301e+02 - logprior: -9.3884e+00
Epoch 2/2
19/19 - 5s - loss: 511.0404 - loglik: -5.0983e+02 - logprior: -1.2127e+00
Fitted a model with MAP estimate = -510.6169
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 524.1057 - loglik: -5.1287e+02 - logprior: -1.1234e+01
Epoch 2/10
19/19 - 4s - loss: 512.7856 - loglik: -5.1121e+02 - logprior: -1.5716e+00
Epoch 3/10
19/19 - 5s - loss: 512.6368 - loglik: -5.1240e+02 - logprior: -2.3335e-01
Epoch 4/10
19/19 - 5s - loss: 511.4331 - loglik: -5.1158e+02 - logprior: 0.1426
Epoch 5/10
19/19 - 4s - loss: 507.4413 - loglik: -5.0775e+02 - logprior: 0.3067
Epoch 6/10
19/19 - 6s - loss: 510.0555 - loglik: -5.1048e+02 - logprior: 0.4197
Fitted a model with MAP estimate = -509.7348
Time for alignment: 108.3331
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 614.7824 - loglik: -6.0382e+02 - logprior: -1.0962e+01
Epoch 2/10
19/19 - 5s - loss: 554.0847 - loglik: -5.5190e+02 - logprior: -2.1856e+00
Epoch 3/10
19/19 - 5s - loss: 538.4387 - loglik: -5.3653e+02 - logprior: -1.9060e+00
Epoch 4/10
19/19 - 4s - loss: 528.1295 - loglik: -5.2651e+02 - logprior: -1.6191e+00
Epoch 5/10
19/19 - 5s - loss: 531.1158 - loglik: -5.2964e+02 - logprior: -1.4733e+00
Fitted a model with MAP estimate = -529.5546
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 2), (34, 1), (42, 1), (47, 2), (48, 4), (49, 2), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 535.5122 - loglik: -5.2190e+02 - logprior: -1.3616e+01
Epoch 2/2
19/19 - 4s - loss: 517.0479 - loglik: -5.1283e+02 - logprior: -4.2176e+00
Fitted a model with MAP estimate = -513.4177
expansions: [(0, 2)]
discards: [ 0 13 62 63]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 521.8988 - loglik: -5.1248e+02 - logprior: -9.4142e+00
Epoch 2/2
19/19 - 5s - loss: 512.4364 - loglik: -5.1119e+02 - logprior: -1.2457e+00
Fitted a model with MAP estimate = -510.6452
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 525.5105 - loglik: -5.1415e+02 - logprior: -1.1360e+01
Epoch 2/10
19/19 - 5s - loss: 511.9814 - loglik: -5.1038e+02 - logprior: -1.6054e+00
Epoch 3/10
19/19 - 5s - loss: 511.3310 - loglik: -5.1104e+02 - logprior: -2.9568e-01
Epoch 4/10
19/19 - 5s - loss: 510.4784 - loglik: -5.1057e+02 - logprior: 0.0934
Epoch 5/10
19/19 - 5s - loss: 509.6793 - loglik: -5.0994e+02 - logprior: 0.2572
Epoch 6/10
19/19 - 5s - loss: 510.0617 - loglik: -5.1043e+02 - logprior: 0.3655
Fitted a model with MAP estimate = -509.6854
Time for alignment: 97.7138
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 614.8849 - loglik: -6.0391e+02 - logprior: -1.0972e+01
Epoch 2/10
19/19 - 4s - loss: 551.1693 - loglik: -5.4897e+02 - logprior: -2.1990e+00
Epoch 3/10
19/19 - 5s - loss: 533.2726 - loglik: -5.3139e+02 - logprior: -1.8828e+00
Epoch 4/10
19/19 - 6s - loss: 531.6534 - loglik: -5.3001e+02 - logprior: -1.6424e+00
Epoch 5/10
19/19 - 5s - loss: 528.8432 - loglik: -5.2738e+02 - logprior: -1.4615e+00
Epoch 6/10
19/19 - 4s - loss: 525.2241 - loglik: -5.2375e+02 - logprior: -1.4741e+00
Epoch 7/10
19/19 - 5s - loss: 527.9647 - loglik: -5.2644e+02 - logprior: -1.5243e+00
Fitted a model with MAP estimate = -527.3143
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (44, 1), (48, 6), (49, 2), (57, 1), (72, 1), (73, 3), (74, 1), (76, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 535.4199 - loglik: -5.2179e+02 - logprior: -1.3633e+01
Epoch 2/2
19/19 - 5s - loss: 517.3725 - loglik: -5.1306e+02 - logprior: -4.3086e+00
Fitted a model with MAP estimate = -513.8082
expansions: [(0, 2)]
discards: [ 0 13 60 64]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 522.3821 - loglik: -5.1298e+02 - logprior: -9.4029e+00
Epoch 2/2
19/19 - 4s - loss: 511.2136 - loglik: -5.0998e+02 - logprior: -1.2349e+00
Fitted a model with MAP estimate = -510.8671
expansions: []
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 524.8159 - loglik: -5.1364e+02 - logprior: -1.1181e+01
Epoch 2/10
19/19 - 5s - loss: 512.4556 - loglik: -5.1089e+02 - logprior: -1.5657e+00
Epoch 3/10
19/19 - 5s - loss: 512.7153 - loglik: -5.1247e+02 - logprior: -2.4451e-01
Fitted a model with MAP estimate = -510.8473
Time for alignment: 94.2228
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 614.9937 - loglik: -6.0402e+02 - logprior: -1.0972e+01
Epoch 2/10
19/19 - 5s - loss: 556.2886 - loglik: -5.5399e+02 - logprior: -2.2986e+00
Epoch 3/10
19/19 - 5s - loss: 534.6224 - loglik: -5.3248e+02 - logprior: -2.1388e+00
Epoch 4/10
19/19 - 5s - loss: 530.2880 - loglik: -5.2827e+02 - logprior: -2.0141e+00
Epoch 5/10
19/19 - 5s - loss: 528.9767 - loglik: -5.2705e+02 - logprior: -1.9227e+00
Epoch 6/10
19/19 - 6s - loss: 528.7081 - loglik: -5.2679e+02 - logprior: -1.9178e+00
Epoch 7/10
19/19 - 5s - loss: 526.8546 - loglik: -5.2497e+02 - logprior: -1.8859e+00
Epoch 8/10
19/19 - 5s - loss: 528.3074 - loglik: -5.2644e+02 - logprior: -1.8697e+00
Fitted a model with MAP estimate = -527.8007
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (30, 1), (31, 1), (38, 1), (41, 1), (42, 1), (46, 1), (47, 3), (49, 1), (50, 2), (60, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 535.9235 - loglik: -5.2226e+02 - logprior: -1.3659e+01
Epoch 2/2
19/19 - 5s - loss: 517.6673 - loglik: -5.1328e+02 - logprior: -4.3828e+00
Fitted a model with MAP estimate = -513.6529
expansions: [(0, 2)]
discards: [ 0 13 60 66]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 521.8559 - loglik: -5.1251e+02 - logprior: -9.3479e+00
Epoch 2/2
19/19 - 4s - loss: 509.0099 - loglik: -5.0782e+02 - logprior: -1.1910e+00
Fitted a model with MAP estimate = -510.5091
expansions: []
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 524.2914 - loglik: -5.1319e+02 - logprior: -1.1104e+01
Epoch 2/10
19/19 - 4s - loss: 512.7531 - loglik: -5.1125e+02 - logprior: -1.5017e+00
Epoch 3/10
19/19 - 4s - loss: 510.2026 - loglik: -5.0999e+02 - logprior: -2.1080e-01
Epoch 4/10
19/19 - 4s - loss: 508.6554 - loglik: -5.0882e+02 - logprior: 0.1681
Epoch 5/10
19/19 - 5s - loss: 512.8568 - loglik: -5.1319e+02 - logprior: 0.3347
Fitted a model with MAP estimate = -510.0166
Time for alignment: 109.2163
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 615.1069 - loglik: -6.0414e+02 - logprior: -1.0972e+01
Epoch 2/10
19/19 - 5s - loss: 553.7382 - loglik: -5.5158e+02 - logprior: -2.1622e+00
Epoch 3/10
19/19 - 6s - loss: 535.2739 - loglik: -5.3356e+02 - logprior: -1.7173e+00
Epoch 4/10
19/19 - 4s - loss: 527.7845 - loglik: -5.2636e+02 - logprior: -1.4277e+00
Epoch 5/10
19/19 - 5s - loss: 530.4800 - loglik: -5.2924e+02 - logprior: -1.2382e+00
Fitted a model with MAP estimate = -529.1029
expansions: [(10, 5), (11, 2), (27, 1), (29, 1), (48, 6), (49, 1), (50, 1), (57, 1), (73, 4), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 535.4724 - loglik: -5.2187e+02 - logprior: -1.3604e+01
Epoch 2/2
19/19 - 4s - loss: 515.2803 - loglik: -5.1107e+02 - logprior: -4.2108e+00
Fitted a model with MAP estimate = -513.3163
expansions: [(0, 2)]
discards: [ 0 15 60]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 520.7283 - loglik: -5.1131e+02 - logprior: -9.4170e+00
Epoch 2/2
19/19 - 6s - loss: 512.9303 - loglik: -5.1167e+02 - logprior: -1.2558e+00
Fitted a model with MAP estimate = -510.5925
expansions: []
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 524.9066 - loglik: -5.1356e+02 - logprior: -1.1344e+01
Epoch 2/10
19/19 - 5s - loss: 513.9197 - loglik: -5.1232e+02 - logprior: -1.6005e+00
Epoch 3/10
19/19 - 5s - loss: 512.1135 - loglik: -5.1180e+02 - logprior: -3.1474e-01
Epoch 4/10
19/19 - 5s - loss: 510.3406 - loglik: -5.1043e+02 - logprior: 0.0864
Epoch 5/10
19/19 - 5s - loss: 512.1369 - loglik: -5.1238e+02 - logprior: 0.2481
Fitted a model with MAP estimate = -510.5082
Time for alignment: 94.9610
Computed alignments with likelihoods: ['-509.7348', '-509.6854', '-510.8473', '-510.0166', '-510.5082']
Best model has likelihood: -509.6854  (prior= 0.4191 )
time for generating output: 0.5634
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21235448742020277
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2350.9656 - loglik: -2.3496e+03 - logprior: -1.3486e+00
Epoch 2/10
49/49 - 41s - loss: 2205.4978 - loglik: -2.2055e+03 - logprior: 0.0503
Epoch 3/10
49/49 - 41s - loss: 2205.0332 - loglik: -2.2050e+03 - logprior: 9.7105e-04
Epoch 4/10
49/49 - 41s - loss: 2193.4482 - loglik: -2.1936e+03 - logprior: 0.1046
Epoch 5/10
49/49 - 41s - loss: 2202.3416 - loglik: -2.2023e+03 - logprior: -5.9010e-02
Fitted a model with MAP estimate = -2197.8749
expansions: [(0, 5), (83, 2), (84, 4), (128, 1), (138, 1), (179, 1), (182, 1), (207, 1), (209, 1), (210, 1), (224, 1), (227, 2), (228, 2), (229, 3), (230, 3), (232, 2), (236, 2), (238, 1), (239, 1), (248, 1), (250, 2), (251, 5), (252, 3), (253, 2), (272, 1), (273, 1), (274, 1), (275, 1), (284, 1), (292, 2), (293, 3), (294, 3), (295, 4), (296, 2), (297, 8), (307, 1), (309, 1), (312, 1), (326, 1), (327, 1), (328, 1), (329, 1), (331, 1), (345, 5), (346, 1), (366, 2), (373, 1), (398, 1), (400, 8), (402, 3)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 2188.0518 - loglik: -2.1860e+03 - logprior: -2.0881e+00
Epoch 2/2
49/49 - 56s - loss: 2172.7485 - loglik: -2.1750e+03 - logprior: 2.2896
Fitted a model with MAP estimate = -2172.3914
expansions: [(0, 6), (334, 2), (368, 1), (481, 1), (486, 3)]
discards: [  1   2   3   4   5   6   7   8   9  74  75  76  77 231 232 278 345 487
 488 489 490 491]
Re-initialized the encoder parameters.
Fitting a model of length 483 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 58s - loss: 2182.1218 - loglik: -2.1808e+03 - logprior: -1.3297e+00
Epoch 2/2
49/49 - 54s - loss: 2183.2371 - loglik: -2.1869e+03 - logprior: 3.6703
Fitted a model with MAP estimate = -2172.8379
expansions: [(0, 6), (9, 1), (321, 3), (483, 7)]
discards: [  1   2   3   4   5   6 325 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 2181.1306 - loglik: -2.1802e+03 - logprior: -8.9318e-01
Epoch 2/10
49/49 - 56s - loss: 2172.8806 - loglik: -2.1771e+03 - logprior: 4.1738
Epoch 3/10
49/49 - 56s - loss: 2171.4961 - loglik: -2.1762e+03 - logprior: 4.6991
Epoch 4/10
49/49 - 56s - loss: 2163.6799 - loglik: -2.1687e+03 - logprior: 4.9936
Epoch 5/10
49/49 - 56s - loss: 2171.0918 - loglik: -2.1769e+03 - logprior: 5.8265
Fitted a model with MAP estimate = -2165.4109
Time for alignment: 881.0552
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2351.5657 - loglik: -2.3502e+03 - logprior: -1.3794e+00
Epoch 2/10
49/49 - 41s - loss: 2213.9741 - loglik: -2.2142e+03 - logprior: 0.1973
Epoch 3/10
49/49 - 41s - loss: 2194.0762 - loglik: -2.1943e+03 - logprior: 0.2188
Epoch 4/10
49/49 - 41s - loss: 2210.8704 - loglik: -2.2110e+03 - logprior: 0.1459
Fitted a model with MAP estimate = -2201.9545
expansions: [(0, 5), (131, 1), (141, 1), (181, 1), (182, 1), (192, 1), (212, 3), (226, 1), (228, 3), (229, 2), (230, 3), (231, 1), (234, 2), (238, 2), (240, 1), (241, 1), (250, 1), (252, 2), (253, 8), (254, 2), (270, 1), (271, 1), (272, 2), (273, 1), (274, 1), (287, 1), (292, 2), (293, 2), (294, 5), (295, 1), (296, 2), (297, 3), (299, 6), (308, 1), (310, 1), (311, 2), (313, 1), (314, 2), (325, 1), (326, 1), (327, 1), (346, 4), (347, 1), (348, 1), (394, 1), (395, 4), (399, 1), (400, 13), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 2198.7253 - loglik: -2.1966e+03 - logprior: -2.1305e+00
Epoch 2/2
49/49 - 57s - loss: 2167.0476 - loglik: -2.1691e+03 - logprior: 2.0540
Fitted a model with MAP estimate = -2171.2378
expansions: [(0, 6), (341, 1), (347, 1), (483, 1), (484, 3)]
discards: [  1   2   3   4   5   6 225 321 460 461 489 490 491 492 493]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 2179.6189 - loglik: -2.1780e+03 - logprior: -1.6478e+00
Epoch 2/2
49/49 - 56s - loss: 2179.9114 - loglik: -2.1830e+03 - logprior: 3.0731
Fitted a model with MAP estimate = -2170.8244
expansions: [(0, 7), (486, 1)]
discards: [  0   1   2   3   4   5   6   7 321 322 489 490]
Re-initialized the encoder parameters.
Fitting a model of length 487 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 58s - loss: 2186.2073 - loglik: -2.1867e+03 - logprior: 0.4939
Epoch 2/10
49/49 - 55s - loss: 2171.5999 - loglik: -2.1762e+03 - logprior: 4.5527
Epoch 3/10
49/49 - 56s - loss: 2174.8154 - loglik: -2.1799e+03 - logprior: 5.1176
Fitted a model with MAP estimate = -2168.1136
Time for alignment: 733.4061
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 2350.1248 - loglik: -2.3489e+03 - logprior: -1.1874e+00
Epoch 2/10
49/49 - 41s - loss: 2213.4285 - loglik: -2.2138e+03 - logprior: 0.3306
Epoch 3/10
49/49 - 41s - loss: 2192.5503 - loglik: -2.1929e+03 - logprior: 0.3382
Epoch 4/10
49/49 - 41s - loss: 2212.5259 - loglik: -2.2128e+03 - logprior: 0.2721
Fitted a model with MAP estimate = -2201.2413
expansions: [(0, 5), (142, 1), (213, 1), (216, 1), (234, 2), (235, 3), (236, 1), (237, 2), (238, 1), (247, 1), (250, 1), (251, 1), (260, 1), (262, 2), (263, 3), (264, 3), (265, 2), (266, 1), (280, 1), (281, 2), (282, 7), (290, 1), (298, 5), (299, 7), (300, 2), (301, 7), (302, 2), (304, 1), (309, 2), (310, 1), (311, 5), (312, 1), (313, 2), (314, 1), (322, 1), (323, 1), (342, 1), (343, 1), (344, 4), (345, 1), (377, 2), (378, 2), (391, 2), (392, 3), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 83 84]
Re-initialized the encoder parameters.
Fitting a model of length 486 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 58s - loss: 2195.5540 - loglik: -2.1932e+03 - logprior: -2.3936e+00
Epoch 2/2
49/49 - 55s - loss: 2177.4729 - loglik: -2.1797e+03 - logprior: 2.1878
Fitted a model with MAP estimate = -2175.1989
expansions: [(0, 7), (23, 3), (299, 1), (329, 3), (335, 1), (474, 1), (477, 2), (483, 2)]
discards: [  1   2   3   4   5   6 225 226 359 406 462 484 485]
Re-initialized the encoder parameters.
Fitting a model of length 493 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 2190.9407 - loglik: -2.1896e+03 - logprior: -1.3558e+00
Epoch 2/2
49/49 - 57s - loss: 2165.1687 - loglik: -2.1685e+03 - logprior: 3.2998
Fitted a model with MAP estimate = -2171.0257
expansions: [(0, 6), (81, 1), (336, 1), (488, 1), (489, 5), (490, 2)]
discards: [  1   2   3   4   5   6   7   8  24  25 331 332]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 60s - loss: 2173.0715 - loglik: -2.1721e+03 - logprior: -1.0186e+00
Epoch 2/10
49/49 - 57s - loss: 2176.2205 - loglik: -2.1799e+03 - logprior: 3.7168
Fitted a model with MAP estimate = -2167.9128
Time for alignment: 679.0647
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 2353.7874 - loglik: -2.3524e+03 - logprior: -1.3798e+00
Epoch 2/10
49/49 - 41s - loss: 2201.7224 - loglik: -2.2018e+03 - logprior: 0.0406
Epoch 3/10
49/49 - 41s - loss: 2202.5879 - loglik: -2.2027e+03 - logprior: 0.1263
Fitted a model with MAP estimate = -2201.0134
expansions: [(0, 5), (36, 3), (130, 1), (140, 1), (182, 1), (192, 1), (212, 1), (213, 1), (226, 2), (229, 3), (230, 2), (231, 4), (233, 2), (237, 2), (238, 2), (239, 2), (248, 1), (250, 2), (251, 5), (252, 3), (253, 2), (272, 1), (273, 1), (274, 1), (275, 1), (282, 1), (292, 1), (293, 2), (294, 2), (295, 4), (298, 4), (300, 5), (310, 1), (314, 3), (315, 1), (316, 1), (322, 1), (324, 1), (325, 2), (326, 2), (327, 1), (346, 5), (347, 1), (348, 1), (350, 1), (378, 1), (399, 13), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 495 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 2190.1616 - loglik: -2.1878e+03 - logprior: -2.3236e+00
Epoch 2/2
49/49 - 57s - loss: 2178.1724 - loglik: -2.1800e+03 - logprior: 1.8086
Fitted a model with MAP estimate = -2171.8356
expansions: [(0, 6), (339, 3), (350, 1), (479, 1), (484, 1), (485, 3)]
discards: [  1   2   3   4   5   6  23  24  82 252 274 325 387 412 490 491 492 493
 494]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 2180.3179 - loglik: -2.1788e+03 - logprior: -1.5616e+00
Epoch 2/2
49/49 - 56s - loss: 2174.0286 - loglik: -2.1769e+03 - logprior: 2.8271
Fitted a model with MAP estimate = -2171.0187
expansions: [(0, 6), (9, 1), (486, 3)]
discards: [  0   1   2   3   4   5   6  23 333 487 488 489 490]
Re-initialized the encoder parameters.
Fitting a model of length 488 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 2178.7871 - loglik: -2.1793e+03 - logprior: 0.4851
Epoch 2/10
49/49 - 56s - loss: 2178.2129 - loglik: -2.1826e+03 - logprior: 4.4057
Epoch 3/10
49/49 - 55s - loss: 2162.8826 - loglik: -2.1680e+03 - logprior: 5.0866
Epoch 4/10
49/49 - 56s - loss: 2176.1267 - loglik: -2.1818e+03 - logprior: 5.6248
Fitted a model with MAP estimate = -2167.8883
Time for alignment: 749.1873
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2346.3293 - loglik: -2.3451e+03 - logprior: -1.2479e+00
Epoch 2/10
49/49 - 41s - loss: 2210.6680 - loglik: -2.2112e+03 - logprior: 0.5077
Epoch 3/10
49/49 - 41s - loss: 2205.2305 - loglik: -2.2054e+03 - logprior: 0.1347
Epoch 4/10
49/49 - 41s - loss: 2193.1309 - loglik: -2.1934e+03 - logprior: 0.2649
Epoch 5/10
49/49 - 41s - loss: 2204.0498 - loglik: -2.2044e+03 - logprior: 0.3308
Fitted a model with MAP estimate = -2198.4496
expansions: [(0, 5), (35, 3), (129, 1), (130, 1), (138, 1), (179, 1), (182, 1), (210, 1), (211, 1), (223, 1), (224, 1), (226, 3), (227, 2), (228, 3), (229, 1), (232, 2), (236, 2), (238, 1), (249, 1), (251, 2), (252, 3), (253, 4), (254, 2), (255, 1), (274, 1), (275, 1), (276, 1), (277, 2), (294, 2), (295, 2), (296, 5), (297, 2), (298, 4), (300, 6), (301, 2), (309, 2), (310, 1), (311, 5), (314, 1), (330, 1), (331, 1), (333, 1), (346, 5), (347, 1), (348, 1), (366, 2), (397, 1), (399, 12), (400, 2), (404, 5)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 18]
Re-initialized the encoder parameters.
Fitting a model of length 502 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 62s - loss: 2184.5640 - loglik: -2.1826e+03 - logprior: -2.0033e+00
Epoch 2/2
49/49 - 58s - loss: 2178.1523 - loglik: -2.1804e+03 - logprior: 2.2048
Fitted a model with MAP estimate = -2169.9934
expansions: [(0, 6), (491, 1), (492, 3)]
discards: [  1   2   3   4   5   6   7   8  25  26  27 102 231 280 370 371 497 498
 499 500 501]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 2182.2913 - loglik: -2.1809e+03 - logprior: -1.4358e+00
Epoch 2/2
49/49 - 56s - loss: 2168.4182 - loglik: -2.1717e+03 - logprior: 3.2803
Fitted a model with MAP estimate = -2171.0305
expansions: [(0, 7), (330, 2), (486, 1)]
discards: [  1   2   3   4   5   6 320 487 488 489 490]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 2184.3071 - loglik: -2.1836e+03 - logprior: -6.7448e-01
Epoch 2/10
49/49 - 56s - loss: 2166.4951 - loglik: -2.1710e+03 - logprior: 4.5215
Epoch 3/10
49/49 - 56s - loss: 2177.0425 - loglik: -2.1821e+03 - logprior: 5.0456
Fitted a model with MAP estimate = -2167.7469
Time for alignment: 780.0675
Computed alignments with likelihoods: ['-2165.4109', '-2168.1136', '-2167.9128', '-2167.8883', '-2167.7469']
Best model has likelihood: -2165.4109  (prior= 6.1306 )
time for generating output: 0.4578
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8167511336356361
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 1009.2095 - loglik: -1.0073e+03 - logprior: -1.9573e+00
Epoch 2/10
39/39 - 8s - loss: 942.9636 - loglik: -9.4135e+02 - logprior: -1.6101e+00
Epoch 3/10
39/39 - 8s - loss: 935.7960 - loglik: -9.3413e+02 - logprior: -1.6614e+00
Epoch 4/10
39/39 - 8s - loss: 933.9589 - loglik: -9.3228e+02 - logprior: -1.6779e+00
Epoch 5/10
39/39 - 8s - loss: 933.2501 - loglik: -9.3159e+02 - logprior: -1.6619e+00
Epoch 6/10
39/39 - 8s - loss: 932.5469 - loglik: -9.3088e+02 - logprior: -1.6708e+00
Epoch 7/10
39/39 - 8s - loss: 932.4453 - loglik: -9.3077e+02 - logprior: -1.6763e+00
Epoch 8/10
39/39 - 8s - loss: 932.3436 - loglik: -9.3067e+02 - logprior: -1.6738e+00
Epoch 9/10
39/39 - 8s - loss: 932.4982 - loglik: -9.3083e+02 - logprior: -1.6708e+00
Fitted a model with MAP estimate = -921.0434
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (32, 1), (40, 2), (45, 1), (46, 1), (55, 1), (56, 1), (57, 2), (58, 2), (59, 2), (69, 2), (71, 2), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (133, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 929.1969 - loglik: -9.2630e+02 - logprior: -2.8933e+00
Epoch 2/2
39/39 - 10s - loss: 916.3372 - loglik: -9.1530e+02 - logprior: -1.0360e+00
Fitted a model with MAP estimate = -902.9018
expansions: []
discards: [ 13  71  74  77  90  93 113 120 166]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 918.0988 - loglik: -9.1616e+02 - logprior: -1.9379e+00
Epoch 2/2
39/39 - 10s - loss: 915.4608 - loglik: -9.1481e+02 - logprior: -6.5123e-01
Fitted a model with MAP estimate = -903.1509
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 905.3901 - loglik: -9.0381e+02 - logprior: -1.5841e+00
Epoch 2/10
41/41 - 10s - loss: 899.2032 - loglik: -8.9873e+02 - logprior: -4.6869e-01
Epoch 3/10
41/41 - 10s - loss: 902.3737 - loglik: -9.0198e+02 - logprior: -3.9497e-01
Fitted a model with MAP estimate = -899.9100
Time for alignment: 202.9967
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1008.5786 - loglik: -1.0066e+03 - logprior: -1.9447e+00
Epoch 2/10
39/39 - 8s - loss: 940.7083 - loglik: -9.3911e+02 - logprior: -1.5971e+00
Epoch 3/10
39/39 - 8s - loss: 933.1171 - loglik: -9.3151e+02 - logprior: -1.6105e+00
Epoch 4/10
39/39 - 8s - loss: 930.4907 - loglik: -9.2887e+02 - logprior: -1.6210e+00
Epoch 5/10
39/39 - 8s - loss: 930.1628 - loglik: -9.2854e+02 - logprior: -1.6234e+00
Epoch 6/10
39/39 - 8s - loss: 929.4274 - loglik: -9.2781e+02 - logprior: -1.6181e+00
Epoch 7/10
39/39 - 8s - loss: 929.2883 - loglik: -9.2767e+02 - logprior: -1.6185e+00
Epoch 8/10
39/39 - 8s - loss: 928.8724 - loglik: -9.2726e+02 - logprior: -1.6104e+00
Epoch 9/10
39/39 - 8s - loss: 929.4548 - loglik: -9.2783e+02 - logprior: -1.6240e+00
Fitted a model with MAP estimate = -917.6879
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (27, 1), (40, 3), (41, 2), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 925.5897 - loglik: -9.2273e+02 - logprior: -2.8620e+00
Epoch 2/2
39/39 - 11s - loss: 912.3212 - loglik: -9.1133e+02 - logprior: -9.8758e-01
Fitted a model with MAP estimate = -899.1739
expansions: []
discards: [ 13  50  53  71  74  79  91  95 116 123 169]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 914.9410 - loglik: -9.1306e+02 - logprior: -1.8823e+00
Epoch 2/2
39/39 - 10s - loss: 911.6667 - loglik: -9.1108e+02 - logprior: -5.8337e-01
Fitted a model with MAP estimate = -899.5870
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 14s - loss: 901.7659 - loglik: -9.0024e+02 - logprior: -1.5239e+00
Epoch 2/10
41/41 - 10s - loss: 898.6320 - loglik: -8.9821e+02 - logprior: -4.1948e-01
Epoch 3/10
41/41 - 10s - loss: 895.2356 - loglik: -8.9488e+02 - logprior: -3.5533e-01
Epoch 4/10
41/41 - 10s - loss: 895.5364 - loglik: -8.9526e+02 - logprior: -2.8030e-01
Fitted a model with MAP estimate = -894.8661
Time for alignment: 215.5547
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 1007.8587 - loglik: -1.0059e+03 - logprior: -1.9343e+00
Epoch 2/10
39/39 - 8s - loss: 940.4893 - loglik: -9.3885e+02 - logprior: -1.6380e+00
Epoch 3/10
39/39 - 8s - loss: 933.7435 - loglik: -9.3205e+02 - logprior: -1.6926e+00
Epoch 4/10
39/39 - 8s - loss: 932.2053 - loglik: -9.3051e+02 - logprior: -1.6922e+00
Epoch 5/10
39/39 - 8s - loss: 931.4586 - loglik: -9.2976e+02 - logprior: -1.6957e+00
Epoch 6/10
39/39 - 8s - loss: 930.6663 - loglik: -9.2898e+02 - logprior: -1.6842e+00
Epoch 7/10
39/39 - 8s - loss: 930.5793 - loglik: -9.2889e+02 - logprior: -1.6902e+00
Epoch 8/10
39/39 - 8s - loss: 930.3706 - loglik: -9.2868e+02 - logprior: -1.6930e+00
Epoch 9/10
39/39 - 8s - loss: 929.8122 - loglik: -9.2811e+02 - logprior: -1.6993e+00
Epoch 10/10
39/39 - 8s - loss: 930.1963 - loglik: -9.2851e+02 - logprior: -1.6907e+00
Fitted a model with MAP estimate = -919.1486
expansions: [(9, 2), (10, 2), (11, 2), (12, 2), (22, 2), (23, 1), (26, 1), (39, 1), (40, 3), (42, 1), (44, 1), (45, 1), (55, 1), (56, 1), (57, 2), (58, 2), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 1), (128, 1), (133, 1)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 927.5848 - loglik: -9.2465e+02 - logprior: -2.9305e+00
Epoch 2/2
39/39 - 11s - loss: 914.0501 - loglik: -9.1288e+02 - logprior: -1.1658e+00
Fitted a model with MAP estimate = -901.3814
expansions: [(64, 1)]
discards: [ 12  15  30  53  54  76  80  82  94  98 119 126]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 915.0721 - loglik: -9.1311e+02 - logprior: -1.9645e+00
Epoch 2/2
39/39 - 10s - loss: 911.0489 - loglik: -9.1034e+02 - logprior: -7.0865e-01
Fitted a model with MAP estimate = -899.9209
expansions: []
discards: [8]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 902.1352 - loglik: -9.0053e+02 - logprior: -1.6091e+00
Epoch 2/10
41/41 - 10s - loss: 898.9531 - loglik: -8.9842e+02 - logprior: -5.3608e-01
Epoch 3/10
41/41 - 10s - loss: 896.7509 - loglik: -8.9631e+02 - logprior: -4.4524e-01
Epoch 4/10
41/41 - 10s - loss: 895.7515 - loglik: -8.9538e+02 - logprior: -3.6916e-01
Epoch 5/10
41/41 - 10s - loss: 896.3152 - loglik: -8.9602e+02 - logprior: -2.9706e-01
Fitted a model with MAP estimate = -894.7155
Time for alignment: 233.1693
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1009.6258 - loglik: -1.0077e+03 - logprior: -1.9468e+00
Epoch 2/10
39/39 - 8s - loss: 941.3297 - loglik: -9.3974e+02 - logprior: -1.5902e+00
Epoch 3/10
39/39 - 8s - loss: 934.9175 - loglik: -9.3338e+02 - logprior: -1.5378e+00
Epoch 4/10
39/39 - 8s - loss: 932.7128 - loglik: -9.3121e+02 - logprior: -1.5035e+00
Epoch 5/10
39/39 - 8s - loss: 931.5067 - loglik: -9.2999e+02 - logprior: -1.5158e+00
Epoch 6/10
39/39 - 8s - loss: 931.7870 - loglik: -9.3029e+02 - logprior: -1.4926e+00
Fitted a model with MAP estimate = -919.8913
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (33, 1), (40, 4), (42, 1), (44, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 2), (104, 3), (106, 2), (116, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 926.8016 - loglik: -9.2398e+02 - logprior: -2.8240e+00
Epoch 2/2
39/39 - 11s - loss: 913.0374 - loglik: -9.1208e+02 - logprior: -9.5314e-01
Fitted a model with MAP estimate = -900.1135
expansions: []
discards: [ 13  50  51  71  74  79  91  95 116 137 144 170]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 915.8439 - loglik: -9.1395e+02 - logprior: -1.8895e+00
Epoch 2/2
39/39 - 10s - loss: 913.1159 - loglik: -9.1251e+02 - logprior: -6.0258e-01
Fitted a model with MAP estimate = -900.4903
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 902.2460 - loglik: -9.0069e+02 - logprior: -1.5546e+00
Epoch 2/10
41/41 - 10s - loss: 899.1625 - loglik: -8.9872e+02 - logprior: -4.3755e-01
Epoch 3/10
41/41 - 10s - loss: 896.6339 - loglik: -8.9628e+02 - logprior: -3.5779e-01
Epoch 4/10
41/41 - 10s - loss: 896.4841 - loglik: -8.9617e+02 - logprior: -3.1058e-01
Epoch 5/10
41/41 - 10s - loss: 897.2410 - loglik: -8.9699e+02 - logprior: -2.5105e-01
Fitted a model with MAP estimate = -895.4297
Time for alignment: 201.9698
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1008.4873 - loglik: -1.0066e+03 - logprior: -1.9330e+00
Epoch 2/10
39/39 - 8s - loss: 941.2462 - loglik: -9.3966e+02 - logprior: -1.5912e+00
Epoch 3/10
39/39 - 8s - loss: 932.5522 - loglik: -9.3086e+02 - logprior: -1.6888e+00
Epoch 4/10
39/39 - 8s - loss: 930.7552 - loglik: -9.2909e+02 - logprior: -1.6629e+00
Epoch 5/10
39/39 - 8s - loss: 930.0530 - loglik: -9.2840e+02 - logprior: -1.6505e+00
Epoch 6/10
39/39 - 8s - loss: 929.1700 - loglik: -9.2753e+02 - logprior: -1.6439e+00
Epoch 7/10
39/39 - 8s - loss: 929.1461 - loglik: -9.2750e+02 - logprior: -1.6487e+00
Epoch 8/10
39/39 - 8s - loss: 929.4569 - loglik: -9.2781e+02 - logprior: -1.6449e+00
Fitted a model with MAP estimate = -917.8500
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (22, 2), (23, 2), (25, 1), (39, 2), (40, 2), (43, 1), (44, 1), (54, 2), (56, 2), (57, 1), (58, 2), (68, 2), (80, 1), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 926.4580 - loglik: -9.2355e+02 - logprior: -2.9116e+00
Epoch 2/2
39/39 - 11s - loss: 912.0062 - loglik: -9.1099e+02 - logprior: -1.0191e+00
Fitted a model with MAP estimate = -898.4171
expansions: []
discards: [ 13  19  31  32  54  73  76  81  93 117 124 170]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 913.7360 - loglik: -9.1185e+02 - logprior: -1.8909e+00
Epoch 2/2
39/39 - 10s - loss: 910.2990 - loglik: -9.0971e+02 - logprior: -5.8858e-01
Fitted a model with MAP estimate = -898.7503
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 899.5090 - loglik: -8.9798e+02 - logprior: -1.5264e+00
Epoch 2/10
41/41 - 10s - loss: 897.8698 - loglik: -8.9745e+02 - logprior: -4.2400e-01
Epoch 3/10
41/41 - 10s - loss: 895.3333 - loglik: -8.9498e+02 - logprior: -3.5434e-01
Epoch 4/10
41/41 - 10s - loss: 895.5707 - loglik: -8.9528e+02 - logprior: -2.9315e-01
Fitted a model with MAP estimate = -894.3840
Time for alignment: 205.2516
Computed alignments with likelihoods: ['-899.9100', '-894.8661', '-894.7155', '-895.4297', '-894.3840']
Best model has likelihood: -894.3840  (prior= -0.2268 )
time for generating output: 0.2627
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.2936117936117936
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 294.4138 - loglik: -2.3551e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 229.4347 - loglik: -2.1286e+02 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 204.3643 - loglik: -1.9610e+02 - logprior: -8.2654e+00
Epoch 4/10
10/10 - 0s - loss: 193.4528 - loglik: -1.8822e+02 - logprior: -5.2347e+00
Epoch 5/10
10/10 - 0s - loss: 189.2296 - loglik: -1.8552e+02 - logprior: -3.7129e+00
Epoch 6/10
10/10 - 0s - loss: 187.6686 - loglik: -1.8473e+02 - logprior: -2.9399e+00
Epoch 7/10
10/10 - 0s - loss: 186.9045 - loglik: -1.8444e+02 - logprior: -2.4680e+00
Epoch 8/10
10/10 - 0s - loss: 186.8129 - loglik: -1.8472e+02 - logprior: -2.0942e+00
Epoch 9/10
10/10 - 0s - loss: 186.4769 - loglik: -1.8465e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 0s - loss: 186.0679 - loglik: -1.8437e+02 - logprior: -1.7015e+00
Fitted a model with MAP estimate = -186.0694
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 261.7451 - loglik: -1.8273e+02 - logprior: -7.9014e+01
Epoch 2/2
10/10 - 0s - loss: 203.4123 - loglik: -1.7783e+02 - logprior: -2.5587e+01
Fitted a model with MAP estimate = -192.4550
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 241.1704 - loglik: -1.7548e+02 - logprior: -6.5688e+01
Epoch 2/2
10/10 - 0s - loss: 197.8694 - loglik: -1.7482e+02 - logprior: -2.3051e+01
Fitted a model with MAP estimate = -188.9198
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 241.2373 - loglik: -1.7552e+02 - logprior: -6.5718e+01
Epoch 2/10
10/10 - 0s - loss: 203.1278 - loglik: -1.7586e+02 - logprior: -2.7264e+01
Epoch 3/10
10/10 - 0s - loss: 191.9963 - loglik: -1.7601e+02 - logprior: -1.5988e+01
Epoch 4/10
10/10 - 0s - loss: 182.4954 - loglik: -1.7605e+02 - logprior: -6.4410e+00
Epoch 5/10
10/10 - 0s - loss: 179.0089 - loglik: -1.7620e+02 - logprior: -2.8070e+00
Epoch 6/10
10/10 - 0s - loss: 177.7400 - loglik: -1.7605e+02 - logprior: -1.6865e+00
Epoch 7/10
10/10 - 0s - loss: 177.2083 - loglik: -1.7610e+02 - logprior: -1.1090e+00
Epoch 8/10
10/10 - 0s - loss: 177.1086 - loglik: -1.7636e+02 - logprior: -7.5345e-01
Epoch 9/10
10/10 - 0s - loss: 176.8533 - loglik: -1.7635e+02 - logprior: -5.0271e-01
Epoch 10/10
10/10 - 0s - loss: 176.5189 - loglik: -1.7622e+02 - logprior: -2.9941e-01
Fitted a model with MAP estimate = -176.5000
Time for alignment: 26.1344
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 294.4036 - loglik: -2.3550e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 229.4221 - loglik: -2.1285e+02 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 204.3543 - loglik: -1.9609e+02 - logprior: -8.2649e+00
Epoch 4/10
10/10 - 0s - loss: 193.5354 - loglik: -1.8830e+02 - logprior: -5.2340e+00
Epoch 5/10
10/10 - 0s - loss: 189.1901 - loglik: -1.8548e+02 - logprior: -3.7119e+00
Epoch 6/10
10/10 - 0s - loss: 187.5686 - loglik: -1.8463e+02 - logprior: -2.9415e+00
Epoch 7/10
10/10 - 0s - loss: 187.0802 - loglik: -1.8461e+02 - logprior: -2.4711e+00
Epoch 8/10
10/10 - 0s - loss: 186.7070 - loglik: -1.8461e+02 - logprior: -2.0993e+00
Epoch 9/10
10/10 - 0s - loss: 186.4910 - loglik: -1.8466e+02 - logprior: -1.8337e+00
Epoch 10/10
10/10 - 0s - loss: 186.2679 - loglik: -1.8457e+02 - logprior: -1.7027e+00
Fitted a model with MAP estimate = -186.0726
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 261.5754 - loglik: -1.8256e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 0s - loss: 203.5165 - loglik: -1.7793e+02 - logprior: -2.5587e+01
Fitted a model with MAP estimate = -192.4580
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.1244 - loglik: -1.7544e+02 - logprior: -6.5688e+01
Epoch 2/2
10/10 - 0s - loss: 197.8425 - loglik: -1.7479e+02 - logprior: -2.3052e+01
Fitted a model with MAP estimate = -188.9156
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 241.2963 - loglik: -1.7558e+02 - logprior: -6.5721e+01
Epoch 2/10
10/10 - 0s - loss: 203.0719 - loglik: -1.7580e+02 - logprior: -2.7268e+01
Epoch 3/10
10/10 - 0s - loss: 191.9386 - loglik: -1.7595e+02 - logprior: -1.5991e+01
Epoch 4/10
10/10 - 0s - loss: 182.5007 - loglik: -1.7606e+02 - logprior: -6.4415e+00
Epoch 5/10
10/10 - 0s - loss: 179.0857 - loglik: -1.7627e+02 - logprior: -2.8185e+00
Epoch 6/10
10/10 - 0s - loss: 177.7537 - loglik: -1.7607e+02 - logprior: -1.6831e+00
Epoch 7/10
10/10 - 0s - loss: 177.3996 - loglik: -1.7629e+02 - logprior: -1.1088e+00
Epoch 8/10
10/10 - 0s - loss: 176.9366 - loglik: -1.7618e+02 - logprior: -7.5737e-01
Epoch 9/10
10/10 - 0s - loss: 176.6625 - loglik: -1.7616e+02 - logprior: -5.0694e-01
Epoch 10/10
10/10 - 0s - loss: 176.6205 - loglik: -1.7632e+02 - logprior: -3.0364e-01
Fitted a model with MAP estimate = -176.5044
Time for alignment: 24.2425
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 294.3686 - loglik: -2.3547e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 229.4176 - loglik: -2.1284e+02 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 204.3685 - loglik: -1.9610e+02 - logprior: -8.2661e+00
Epoch 4/10
10/10 - 0s - loss: 193.4305 - loglik: -1.8819e+02 - logprior: -5.2382e+00
Epoch 5/10
10/10 - 0s - loss: 189.2388 - loglik: -1.8553e+02 - logprior: -3.7135e+00
Epoch 6/10
10/10 - 0s - loss: 187.6847 - loglik: -1.8474e+02 - logprior: -2.9422e+00
Epoch 7/10
10/10 - 0s - loss: 187.2081 - loglik: -1.8474e+02 - logprior: -2.4676e+00
Epoch 8/10
10/10 - 0s - loss: 186.5156 - loglik: -1.8442e+02 - logprior: -2.0957e+00
Epoch 9/10
10/10 - 0s - loss: 186.3698 - loglik: -1.8454e+02 - logprior: -1.8310e+00
Epoch 10/10
10/10 - 0s - loss: 186.1336 - loglik: -1.8443e+02 - logprior: -1.7028e+00
Fitted a model with MAP estimate = -186.0694
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 261.7509 - loglik: -1.8274e+02 - logprior: -7.9015e+01
Epoch 2/2
10/10 - 0s - loss: 203.4181 - loglik: -1.7783e+02 - logprior: -2.5586e+01
Fitted a model with MAP estimate = -192.4283
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.0741 - loglik: -1.7538e+02 - logprior: -6.5690e+01
Epoch 2/2
10/10 - 0s - loss: 197.9617 - loglik: -1.7490e+02 - logprior: -2.3059e+01
Fitted a model with MAP estimate = -188.8936
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 241.2168 - loglik: -1.7549e+02 - logprior: -6.5723e+01
Epoch 2/10
10/10 - 0s - loss: 203.0460 - loglik: -1.7577e+02 - logprior: -2.7275e+01
Epoch 3/10
10/10 - 0s - loss: 192.0628 - loglik: -1.7605e+02 - logprior: -1.6008e+01
Epoch 4/10
10/10 - 0s - loss: 182.5554 - loglik: -1.7611e+02 - logprior: -6.4499e+00
Epoch 5/10
10/10 - 0s - loss: 179.0226 - loglik: -1.7619e+02 - logprior: -2.8309e+00
Epoch 6/10
10/10 - 0s - loss: 177.7536 - loglik: -1.7607e+02 - logprior: -1.6878e+00
Epoch 7/10
10/10 - 0s - loss: 177.5092 - loglik: -1.7639e+02 - logprior: -1.1143e+00
Epoch 8/10
10/10 - 0s - loss: 176.8598 - loglik: -1.7610e+02 - logprior: -7.6431e-01
Epoch 9/10
10/10 - 0s - loss: 176.6926 - loglik: -1.7618e+02 - logprior: -5.1173e-01
Epoch 10/10
10/10 - 0s - loss: 176.6948 - loglik: -1.7638e+02 - logprior: -3.1216e-01
Fitted a model with MAP estimate = -176.5145
Time for alignment: 24.6507
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 294.4566 - loglik: -2.3556e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 229.2221 - loglik: -2.1265e+02 - logprior: -1.6575e+01
Epoch 3/10
10/10 - 0s - loss: 204.5492 - loglik: -1.9629e+02 - logprior: -8.2588e+00
Epoch 4/10
10/10 - 0s - loss: 194.0703 - loglik: -1.8887e+02 - logprior: -5.1958e+00
Epoch 5/10
10/10 - 0s - loss: 190.0173 - loglik: -1.8635e+02 - logprior: -3.6643e+00
Epoch 6/10
10/10 - 0s - loss: 187.2876 - loglik: -1.8435e+02 - logprior: -2.9330e+00
Epoch 7/10
10/10 - 0s - loss: 186.2820 - loglik: -1.8381e+02 - logprior: -2.4700e+00
Epoch 8/10
10/10 - 0s - loss: 185.6288 - loglik: -1.8350e+02 - logprior: -2.1296e+00
Epoch 9/10
10/10 - 0s - loss: 185.3698 - loglik: -1.8353e+02 - logprior: -1.8410e+00
Epoch 10/10
10/10 - 0s - loss: 184.9442 - loglik: -1.8331e+02 - logprior: -1.6364e+00
Fitted a model with MAP estimate = -185.0171
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 260.6589 - loglik: -1.8139e+02 - logprior: -7.9266e+01
Epoch 2/2
10/10 - 0s - loss: 203.0544 - loglik: -1.7759e+02 - logprior: -2.5460e+01
Fitted a model with MAP estimate = -191.9529
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 241.0160 - loglik: -1.7521e+02 - logprior: -6.5804e+01
Epoch 2/2
10/10 - 0s - loss: 197.6406 - loglik: -1.7457e+02 - logprior: -2.3074e+01
Fitted a model with MAP estimate = -188.6884
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 241.0493 - loglik: -1.7521e+02 - logprior: -6.5843e+01
Epoch 2/10
10/10 - 0s - loss: 202.8646 - loglik: -1.7559e+02 - logprior: -2.7275e+01
Epoch 3/10
10/10 - 0s - loss: 191.4693 - loglik: -1.7564e+02 - logprior: -1.5828e+01
Epoch 4/10
10/10 - 0s - loss: 182.4034 - loglik: -1.7602e+02 - logprior: -6.3867e+00
Epoch 5/10
10/10 - 0s - loss: 178.9589 - loglik: -1.7606e+02 - logprior: -2.9012e+00
Epoch 6/10
10/10 - 0s - loss: 177.4552 - loglik: -1.7567e+02 - logprior: -1.7868e+00
Epoch 7/10
10/10 - 0s - loss: 177.3828 - loglik: -1.7618e+02 - logprior: -1.2061e+00
Epoch 8/10
10/10 - 0s - loss: 176.7948 - loglik: -1.7595e+02 - logprior: -8.4106e-01
Epoch 9/10
10/10 - 0s - loss: 176.6554 - loglik: -1.7607e+02 - logprior: -5.8411e-01
Epoch 10/10
10/10 - 0s - loss: 176.4208 - loglik: -1.7604e+02 - logprior: -3.8189e-01
Fitted a model with MAP estimate = -176.3901
Time for alignment: 24.2455
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 294.4731 - loglik: -2.3557e+02 - logprior: -5.8901e+01
Epoch 2/10
10/10 - 0s - loss: 229.3374 - loglik: -2.1276e+02 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 204.3933 - loglik: -1.9613e+02 - logprior: -8.2595e+00
Epoch 4/10
10/10 - 0s - loss: 193.9327 - loglik: -1.8873e+02 - logprior: -5.2030e+00
Epoch 5/10
10/10 - 0s - loss: 189.7125 - loglik: -1.8603e+02 - logprior: -3.6835e+00
Epoch 6/10
10/10 - 0s - loss: 187.8179 - loglik: -1.8488e+02 - logprior: -2.9331e+00
Epoch 7/10
10/10 - 0s - loss: 187.1143 - loglik: -1.8464e+02 - logprior: -2.4767e+00
Epoch 8/10
10/10 - 0s - loss: 186.6223 - loglik: -1.8451e+02 - logprior: -2.1130e+00
Epoch 9/10
10/10 - 0s - loss: 186.2747 - loglik: -1.8444e+02 - logprior: -1.8381e+00
Epoch 10/10
10/10 - 0s - loss: 186.4332 - loglik: -1.8474e+02 - logprior: -1.6981e+00
Fitted a model with MAP estimate = -186.0863
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 261.7305 - loglik: -1.8271e+02 - logprior: -7.9016e+01
Epoch 2/2
10/10 - 0s - loss: 203.2824 - loglik: -1.7770e+02 - logprior: -2.5584e+01
Fitted a model with MAP estimate = -192.4479
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.2438 - loglik: -1.7556e+02 - logprior: -6.5687e+01
Epoch 2/2
10/10 - 0s - loss: 197.8332 - loglik: -1.7478e+02 - logprior: -2.3056e+01
Fitted a model with MAP estimate = -188.8919
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 241.2962 - loglik: -1.7558e+02 - logprior: -6.5719e+01
Epoch 2/10
10/10 - 0s - loss: 203.1505 - loglik: -1.7588e+02 - logprior: -2.7269e+01
Epoch 3/10
10/10 - 0s - loss: 191.9354 - loglik: -1.7594e+02 - logprior: -1.5993e+01
Epoch 4/10
10/10 - 0s - loss: 182.5317 - loglik: -1.7609e+02 - logprior: -6.4466e+00
Epoch 5/10
10/10 - 0s - loss: 179.0066 - loglik: -1.7619e+02 - logprior: -2.8118e+00
Epoch 6/10
10/10 - 0s - loss: 177.6945 - loglik: -1.7601e+02 - logprior: -1.6810e+00
Epoch 7/10
10/10 - 0s - loss: 177.3358 - loglik: -1.7623e+02 - logprior: -1.1036e+00
Epoch 8/10
10/10 - 0s - loss: 177.0187 - loglik: -1.7627e+02 - logprior: -7.4731e-01
Epoch 9/10
10/10 - 0s - loss: 176.5983 - loglik: -1.7610e+02 - logprior: -4.9694e-01
Epoch 10/10
10/10 - 0s - loss: 176.7524 - loglik: -1.7646e+02 - logprior: -2.9397e-01
Fitted a model with MAP estimate = -176.4974
Time for alignment: 24.7348
Computed alignments with likelihoods: ['-176.5000', '-176.5044', '-176.5145', '-176.3901', '-176.4974']
Best model has likelihood: -176.3901  (prior= -0.2869 )
time for generating output: 0.0775
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9589371980676329
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 758.5802 - loglik: -7.5579e+02 - logprior: -2.7897e+00
Epoch 2/10
20/20 - 3s - loss: 714.7537 - loglik: -7.1353e+02 - logprior: -1.2258e+00
Epoch 3/10
20/20 - 3s - loss: 695.1086 - loglik: -6.9365e+02 - logprior: -1.4545e+00
Epoch 4/10
20/20 - 3s - loss: 689.8632 - loglik: -6.8851e+02 - logprior: -1.3574e+00
Epoch 5/10
20/20 - 3s - loss: 689.2352 - loglik: -6.8787e+02 - logprior: -1.3623e+00
Epoch 6/10
20/20 - 3s - loss: 687.6621 - loglik: -6.8633e+02 - logprior: -1.3324e+00
Epoch 7/10
20/20 - 4s - loss: 687.7858 - loglik: -6.8646e+02 - logprior: -1.3211e+00
Fitted a model with MAP estimate = -643.3305
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (39, 1), (46, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 2), (75, 1), (76, 3), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 687.1799 - loglik: -6.8454e+02 - logprior: -2.6383e+00
Epoch 2/2
40/40 - 5s - loss: 676.1519 - loglik: -6.7526e+02 - logprior: -8.9103e-01
Fitted a model with MAP estimate = -630.5614
expansions: [(141, 2)]
discards: [  8  30  46  74  83  85 109 126 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 678.6005 - loglik: -6.7675e+02 - logprior: -1.8486e+00
Epoch 2/2
40/40 - 5s - loss: 675.2112 - loglik: -6.7455e+02 - logprior: -6.6444e-01
Fitted a model with MAP estimate = -630.9668
expansions: [(133, 2)]
discards: [131 132]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 10s - loss: 627.7701 - loglik: -6.2677e+02 - logprior: -9.9734e-01
Epoch 2/10
57/57 - 6s - loss: 626.8109 - loglik: -6.2622e+02 - logprior: -5.9347e-01
Epoch 3/10
57/57 - 6s - loss: 624.5897 - loglik: -6.2399e+02 - logprior: -5.9469e-01
Epoch 4/10
57/57 - 6s - loss: 624.0807 - loglik: -6.2349e+02 - logprior: -5.8780e-01
Epoch 5/10
57/57 - 6s - loss: 622.0547 - loglik: -6.2149e+02 - logprior: -5.6626e-01
Epoch 6/10
57/57 - 6s - loss: 622.4481 - loglik: -6.2190e+02 - logprior: -5.5271e-01
Fitted a model with MAP estimate = -622.4170
Time for alignment: 131.3449
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 758.5900 - loglik: -7.5580e+02 - logprior: -2.7904e+00
Epoch 2/10
20/20 - 3s - loss: 714.4727 - loglik: -7.1325e+02 - logprior: -1.2233e+00
Epoch 3/10
20/20 - 3s - loss: 696.0833 - loglik: -6.9466e+02 - logprior: -1.4208e+00
Epoch 4/10
20/20 - 3s - loss: 690.2205 - loglik: -6.8888e+02 - logprior: -1.3438e+00
Epoch 5/10
20/20 - 3s - loss: 689.2300 - loglik: -6.8786e+02 - logprior: -1.3663e+00
Epoch 6/10
20/20 - 3s - loss: 687.9968 - loglik: -6.8666e+02 - logprior: -1.3353e+00
Epoch 7/10
20/20 - 3s - loss: 688.0386 - loglik: -6.8672e+02 - logprior: -1.3172e+00
Fitted a model with MAP estimate = -643.5177
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 687.6284 - loglik: -6.8498e+02 - logprior: -2.6523e+00
Epoch 2/2
40/40 - 5s - loss: 675.9415 - loglik: -6.7502e+02 - logprior: -9.1654e-01
Fitted a model with MAP estimate = -630.4083
expansions: [(142, 2)]
discards: [  8  30  46  74  82  84 104 107 110 127 140 141]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 678.8380 - loglik: -6.7698e+02 - logprior: -1.8532e+00
Epoch 2/2
40/40 - 5s - loss: 675.5770 - loglik: -6.7491e+02 - logprior: -6.6326e-01
Fitted a model with MAP estimate = -630.8796
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 10s - loss: 628.1545 - loglik: -6.2716e+02 - logprior: -9.9744e-01
Epoch 2/10
57/57 - 6s - loss: 625.4686 - loglik: -6.2486e+02 - logprior: -6.0741e-01
Epoch 3/10
57/57 - 7s - loss: 625.4206 - loglik: -6.2483e+02 - logprior: -5.8898e-01
Epoch 4/10
57/57 - 6s - loss: 624.3387 - loglik: -6.2376e+02 - logprior: -5.7834e-01
Epoch 5/10
57/57 - 6s - loss: 623.7696 - loglik: -6.2321e+02 - logprior: -5.6044e-01
Epoch 6/10
57/57 - 6s - loss: 621.7339 - loglik: -6.2119e+02 - logprior: -5.4100e-01
Epoch 7/10
57/57 - 6s - loss: 622.3683 - loglik: -6.2185e+02 - logprior: -5.2002e-01
Fitted a model with MAP estimate = -622.4764
Time for alignment: 137.7095
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 758.1381 - loglik: -7.5534e+02 - logprior: -2.7951e+00
Epoch 2/10
20/20 - 3s - loss: 715.1500 - loglik: -7.1391e+02 - logprior: -1.2450e+00
Epoch 3/10
20/20 - 3s - loss: 696.6465 - loglik: -6.9518e+02 - logprior: -1.4632e+00
Epoch 4/10
20/20 - 3s - loss: 692.1611 - loglik: -6.9079e+02 - logprior: -1.3722e+00
Epoch 5/10
20/20 - 3s - loss: 689.7560 - loglik: -6.8837e+02 - logprior: -1.3892e+00
Epoch 6/10
20/20 - 3s - loss: 688.7429 - loglik: -6.8738e+02 - logprior: -1.3586e+00
Epoch 7/10
20/20 - 3s - loss: 687.5801 - loglik: -6.8625e+02 - logprior: -1.3308e+00
Epoch 8/10
20/20 - 3s - loss: 688.4120 - loglik: -6.8709e+02 - logprior: -1.3250e+00
Fitted a model with MAP estimate = -644.3667
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (51, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (64, 1), (76, 1), (78, 2), (81, 2), (87, 1), (88, 1), (94, 2), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 686.9554 - loglik: -6.8437e+02 - logprior: -2.5840e+00
Epoch 2/2
40/40 - 5s - loss: 676.8992 - loglik: -6.7608e+02 - logprior: -8.1521e-01
Fitted a model with MAP estimate = -630.9331
expansions: [(137, 2)]
discards: [  8  30  46 100 122 135 136]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 678.7617 - loglik: -6.7691e+02 - logprior: -1.8514e+00
Epoch 2/2
40/40 - 5s - loss: 675.7211 - loglik: -6.7504e+02 - logprior: -6.8368e-01
Fitted a model with MAP estimate = -631.1585
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 627.7595 - loglik: -6.2675e+02 - logprior: -1.0077e+00
Epoch 2/10
57/57 - 7s - loss: 627.0416 - loglik: -6.2642e+02 - logprior: -6.2285e-01
Epoch 3/10
57/57 - 7s - loss: 624.8676 - loglik: -6.2427e+02 - logprior: -6.0217e-01
Epoch 4/10
57/57 - 6s - loss: 624.1138 - loglik: -6.2352e+02 - logprior: -5.9156e-01
Epoch 5/10
57/57 - 7s - loss: 622.1027 - loglik: -6.2153e+02 - logprior: -5.7230e-01
Epoch 6/10
57/57 - 6s - loss: 622.4710 - loglik: -6.2193e+02 - logprior: -5.4422e-01
Fitted a model with MAP estimate = -622.3708
Time for alignment: 133.5617
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 758.3904 - loglik: -7.5560e+02 - logprior: -2.7899e+00
Epoch 2/10
20/20 - 3s - loss: 712.8506 - loglik: -7.1163e+02 - logprior: -1.2199e+00
Epoch 3/10
20/20 - 3s - loss: 694.7689 - loglik: -6.9334e+02 - logprior: -1.4288e+00
Epoch 4/10
20/20 - 3s - loss: 690.8176 - loglik: -6.8945e+02 - logprior: -1.3679e+00
Epoch 5/10
20/20 - 3s - loss: 689.2614 - loglik: -6.8786e+02 - logprior: -1.3966e+00
Epoch 6/10
20/20 - 3s - loss: 688.5912 - loglik: -6.8722e+02 - logprior: -1.3704e+00
Epoch 7/10
20/20 - 3s - loss: 687.6876 - loglik: -6.8633e+02 - logprior: -1.3615e+00
Epoch 8/10
20/20 - 3s - loss: 688.1247 - loglik: -6.8677e+02 - logprior: -1.3552e+00
Fitted a model with MAP estimate = -643.9382
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (37, 2), (38, 2), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (81, 1), (84, 2), (85, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 687.8719 - loglik: -6.8527e+02 - logprior: -2.6035e+00
Epoch 2/2
40/40 - 5s - loss: 676.8794 - loglik: -6.7607e+02 - logprior: -8.0725e-01
Fitted a model with MAP estimate = -631.3226
expansions: [(139, 2)]
discards: [  8  46  48  74  82  87 103 113 126]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 679.3183 - loglik: -6.7740e+02 - logprior: -1.9200e+00
Epoch 2/2
40/40 - 5s - loss: 675.8564 - loglik: -6.7516e+02 - logprior: -6.9201e-01
Fitted a model with MAP estimate = -631.2494
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 629.3613 - loglik: -6.2836e+02 - logprior: -1.0042e+00
Epoch 2/10
57/57 - 6s - loss: 624.0953 - loglik: -6.2349e+02 - logprior: -6.0210e-01
Epoch 3/10
57/57 - 7s - loss: 626.2272 - loglik: -6.2563e+02 - logprior: -6.0175e-01
Fitted a model with MAP estimate = -624.0136
Time for alignment: 115.3299
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 758.2582 - loglik: -7.5546e+02 - logprior: -2.7940e+00
Epoch 2/10
20/20 - 3s - loss: 713.0566 - loglik: -7.1181e+02 - logprior: -1.2508e+00
Epoch 3/10
20/20 - 3s - loss: 694.0944 - loglik: -6.9262e+02 - logprior: -1.4773e+00
Epoch 4/10
20/20 - 3s - loss: 689.8729 - loglik: -6.8847e+02 - logprior: -1.3987e+00
Epoch 5/10
20/20 - 3s - loss: 688.7464 - loglik: -6.8734e+02 - logprior: -1.4075e+00
Epoch 6/10
20/20 - 3s - loss: 687.8563 - loglik: -6.8648e+02 - logprior: -1.3805e+00
Epoch 7/10
20/20 - 3s - loss: 687.6736 - loglik: -6.8631e+02 - logprior: -1.3668e+00
Epoch 8/10
20/20 - 3s - loss: 686.8177 - loglik: -6.8545e+02 - logprior: -1.3647e+00
Epoch 9/10
20/20 - 3s - loss: 687.0884 - loglik: -6.8572e+02 - logprior: -1.3728e+00
Fitted a model with MAP estimate = -643.7215
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (42, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 687.4371 - loglik: -6.8485e+02 - logprior: -2.5896e+00
Epoch 2/2
40/40 - 5s - loss: 676.9133 - loglik: -6.7613e+02 - logprior: -7.8757e-01
Fitted a model with MAP estimate = -631.2494
expansions: [(138, 2)]
discards: [  8  45  80  85 102 104 108 125]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 7s - loss: 678.9748 - loglik: -6.7709e+02 - logprior: -1.8867e+00
Epoch 2/2
40/40 - 5s - loss: 675.9263 - loglik: -6.7526e+02 - logprior: -6.6997e-01
Fitted a model with MAP estimate = -631.5765
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 628.1440 - loglik: -6.2714e+02 - logprior: -9.9908e-01
Epoch 2/10
57/57 - 6s - loss: 626.2159 - loglik: -6.2560e+02 - logprior: -6.1630e-01
Epoch 3/10
57/57 - 7s - loss: 625.2482 - loglik: -6.2464e+02 - logprior: -6.0501e-01
Epoch 4/10
57/57 - 6s - loss: 623.1321 - loglik: -6.2254e+02 - logprior: -5.9543e-01
Epoch 5/10
57/57 - 6s - loss: 624.1473 - loglik: -6.2357e+02 - logprior: -5.7687e-01
Fitted a model with MAP estimate = -622.3618
Time for alignment: 130.3062
Computed alignments with likelihoods: ['-622.4170', '-622.4764', '-622.3708', '-624.0136', '-622.3618']
Best model has likelihood: -622.3618  (prior= -0.5589 )
time for generating output: 0.2852
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6711409395973155
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 1378.6494 - loglik: -1.3691e+03 - logprior: -9.5373e+00
Epoch 2/10
19/19 - 6s - loss: 1313.6515 - loglik: -1.3131e+03 - logprior: -5.7378e-01
Epoch 3/10
19/19 - 6s - loss: 1284.3018 - loglik: -1.2838e+03 - logprior: -4.9044e-01
Epoch 4/10
19/19 - 6s - loss: 1276.7328 - loglik: -1.2766e+03 - logprior: -1.0454e-01
Epoch 5/10
19/19 - 6s - loss: 1271.7178 - loglik: -1.2719e+03 - logprior: 0.1857
Epoch 6/10
19/19 - 6s - loss: 1272.8066 - loglik: -1.2731e+03 - logprior: 0.2589
Fitted a model with MAP estimate = -1271.5089
expansions: [(25, 1), (28, 2), (30, 2), (40, 6), (63, 2), (64, 2), (87, 1), (89, 3), (101, 4), (104, 1), (120, 1), (137, 1), (148, 2), (150, 4), (166, 5), (182, 1), (183, 1), (184, 2), (198, 3)]
discards: [170]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1289.0789 - loglik: -1.2800e+03 - logprior: -9.1144e+00
Epoch 2/2
19/19 - 8s - loss: 1270.4963 - loglik: -1.2699e+03 - logprior: -5.9164e-01
Fitted a model with MAP estimate = -1266.7388
expansions: [(206, 1), (224, 1), (225, 1)]
discards: [ 33  34 201 202 207 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 1279.5763 - loglik: -1.2711e+03 - logprior: -8.4715e+00
Epoch 2/2
19/19 - 8s - loss: 1265.2178 - loglik: -1.2655e+03 - logprior: 0.2912
Fitted a model with MAP estimate = -1264.3509
expansions: []
discards: [185 186]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1279.0903 - loglik: -1.2710e+03 - logprior: -8.1380e+00
Epoch 2/10
19/19 - 8s - loss: 1265.9879 - loglik: -1.2667e+03 - logprior: 0.6772
Epoch 3/10
19/19 - 8s - loss: 1266.5918 - loglik: -1.2683e+03 - logprior: 1.7350
Fitted a model with MAP estimate = -1262.8483
Time for alignment: 126.8551
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1379.3479 - loglik: -1.3698e+03 - logprior: -9.5347e+00
Epoch 2/10
19/19 - 6s - loss: 1311.1542 - loglik: -1.3106e+03 - logprior: -5.9192e-01
Epoch 3/10
19/19 - 6s - loss: 1281.6279 - loglik: -1.2811e+03 - logprior: -5.7316e-01
Epoch 4/10
19/19 - 6s - loss: 1274.0939 - loglik: -1.2738e+03 - logprior: -2.9033e-01
Epoch 5/10
19/19 - 6s - loss: 1274.1425 - loglik: -1.2741e+03 - logprior: -3.5735e-02
Fitted a model with MAP estimate = -1271.1297
expansions: [(25, 1), (27, 1), (29, 3), (39, 7), (62, 2), (89, 3), (91, 2), (102, 1), (117, 1), (119, 1), (131, 1), (134, 1), (150, 3), (152, 1), (162, 2), (165, 6), (181, 1), (183, 1), (184, 2), (185, 1), (198, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1287.7526 - loglik: -1.2788e+03 - logprior: -8.9813e+00
Epoch 2/2
19/19 - 8s - loss: 1268.7377 - loglik: -1.2682e+03 - logprior: -4.9039e-01
Fitted a model with MAP estimate = -1265.9140
expansions: [(77, 2)]
discards: [ 44  45 177 181 183 184 191 206 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1283.3590 - loglik: -1.2747e+03 - logprior: -8.6407e+00
Epoch 2/2
19/19 - 8s - loss: 1268.9958 - loglik: -1.2689e+03 - logprior: -1.1049e-01
Fitted a model with MAP estimate = -1266.9595
expansions: [(105, 2), (177, 1), (178, 1), (180, 1), (181, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1278.0930 - loglik: -1.2699e+03 - logprior: -8.2342e+00
Epoch 2/10
19/19 - 8s - loss: 1265.0178 - loglik: -1.2655e+03 - logprior: 0.4840
Epoch 3/10
19/19 - 8s - loss: 1263.4261 - loglik: -1.2650e+03 - logprior: 1.5512
Epoch 4/10
19/19 - 8s - loss: 1257.7640 - loglik: -1.2598e+03 - logprior: 2.0522
Epoch 5/10
19/19 - 8s - loss: 1255.9204 - loglik: -1.2582e+03 - logprior: 2.2945
Epoch 6/10
19/19 - 8s - loss: 1258.6680 - loglik: -1.2612e+03 - logprior: 2.4832
Fitted a model with MAP estimate = -1255.9949
Time for alignment: 145.4795
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 1379.6025 - loglik: -1.3701e+03 - logprior: -9.5291e+00
Epoch 2/10
19/19 - 6s - loss: 1314.1243 - loglik: -1.3135e+03 - logprior: -6.3456e-01
Epoch 3/10
19/19 - 6s - loss: 1287.1069 - loglik: -1.2864e+03 - logprior: -6.9312e-01
Epoch 4/10
19/19 - 6s - loss: 1279.2505 - loglik: -1.2788e+03 - logprior: -4.3196e-01
Epoch 5/10
19/19 - 6s - loss: 1277.0659 - loglik: -1.2769e+03 - logprior: -2.1341e-01
Epoch 6/10
19/19 - 6s - loss: 1271.9291 - loglik: -1.2718e+03 - logprior: -1.6621e-01
Epoch 7/10
19/19 - 6s - loss: 1269.4680 - loglik: -1.2693e+03 - logprior: -1.9353e-01
Epoch 8/10
19/19 - 6s - loss: 1277.8799 - loglik: -1.2777e+03 - logprior: -1.9863e-01
Fitted a model with MAP estimate = -1272.7137
expansions: [(0, 2), (25, 1), (27, 1), (33, 2), (43, 5), (44, 1), (64, 2), (65, 2), (87, 1), (89, 1), (90, 2), (101, 3), (103, 1), (118, 3), (134, 3), (135, 1), (151, 1), (152, 1), (153, 1), (162, 1), (166, 5), (182, 1), (183, 1), (184, 2), (185, 1), (198, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1296.0184 - loglik: -1.2831e+03 - logprior: -1.2937e+01
Epoch 2/2
19/19 - 8s - loss: 1270.6332 - loglik: -1.2700e+03 - logprior: -6.2708e-01
Fitted a model with MAP estimate = -1266.0566
expansions: [(31, 1)]
discards: [  0 186 187 202 203 204 209 210 242 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1285.2495 - loglik: -1.2727e+03 - logprior: -1.2508e+01
Epoch 2/2
19/19 - 8s - loss: 1272.9868 - loglik: -1.2705e+03 - logprior: -2.5123e+00
Fitted a model with MAP estimate = -1267.0253
expansions: [(0, 2), (186, 1), (220, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1278.0609 - loglik: -1.2699e+03 - logprior: -8.1300e+00
Epoch 2/10
19/19 - 8s - loss: 1266.7306 - loglik: -1.2675e+03 - logprior: 0.7594
Epoch 3/10
19/19 - 8s - loss: 1262.1952 - loglik: -1.2639e+03 - logprior: 1.7104
Epoch 4/10
19/19 - 8s - loss: 1260.1656 - loglik: -1.2624e+03 - logprior: 2.2533
Epoch 5/10
19/19 - 8s - loss: 1260.5393 - loglik: -1.2630e+03 - logprior: 2.4863
Fitted a model with MAP estimate = -1257.4955
Time for alignment: 155.4024
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 1379.6056 - loglik: -1.3701e+03 - logprior: -9.5227e+00
Epoch 2/10
19/19 - 6s - loss: 1319.2661 - loglik: -1.3188e+03 - logprior: -5.0739e-01
Epoch 3/10
19/19 - 6s - loss: 1287.1940 - loglik: -1.2864e+03 - logprior: -8.0724e-01
Epoch 4/10
19/19 - 6s - loss: 1276.8727 - loglik: -1.2763e+03 - logprior: -5.2419e-01
Epoch 5/10
19/19 - 6s - loss: 1276.1239 - loglik: -1.2757e+03 - logprior: -3.7968e-01
Epoch 6/10
19/19 - 6s - loss: 1272.3344 - loglik: -1.2720e+03 - logprior: -3.7283e-01
Epoch 7/10
19/19 - 6s - loss: 1272.3514 - loglik: -1.2721e+03 - logprior: -3.0029e-01
Fitted a model with MAP estimate = -1271.5841
expansions: [(25, 1), (29, 2), (34, 2), (44, 4), (45, 1), (65, 4), (91, 2), (100, 4), (104, 1), (119, 1), (121, 1), (133, 1), (135, 1), (136, 1), (147, 3), (149, 4), (153, 1), (162, 1), (163, 1), (166, 5), (181, 1), (183, 1), (184, 2), (185, 1), (198, 3)]
discards: [169 170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 1296.2700 - loglik: -1.2871e+03 - logprior: -9.1874e+00
Epoch 2/2
19/19 - 8s - loss: 1271.0817 - loglik: -1.2705e+03 - logprior: -6.0172e-01
Fitted a model with MAP estimate = -1267.2556
expansions: [(54, 3), (106, 1)]
discards: [118 178 179 196 202 203 204 205 206 235 236 237]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1282.0033 - loglik: -1.2734e+03 - logprior: -8.5974e+00
Epoch 2/2
19/19 - 7s - loss: 1270.9907 - loglik: -1.2710e+03 - logprior: 0.0226
Fitted a model with MAP estimate = -1267.5138
expansions: [(197, 1), (200, 10)]
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1279.0220 - loglik: -1.2708e+03 - logprior: -8.2100e+00
Epoch 2/10
19/19 - 8s - loss: 1264.7876 - loglik: -1.2653e+03 - logprior: 0.4951
Epoch 3/10
19/19 - 8s - loss: 1265.7585 - loglik: -1.2674e+03 - logprior: 1.6536
Fitted a model with MAP estimate = -1261.1282
Time for alignment: 131.7916
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1380.2156 - loglik: -1.3707e+03 - logprior: -9.5425e+00
Epoch 2/10
19/19 - 6s - loss: 1310.6045 - loglik: -1.3101e+03 - logprior: -5.1738e-01
Epoch 3/10
19/19 - 6s - loss: 1282.5708 - loglik: -1.2821e+03 - logprior: -4.9201e-01
Epoch 4/10
19/19 - 6s - loss: 1276.7007 - loglik: -1.2766e+03 - logprior: -6.2737e-02
Epoch 5/10
19/19 - 6s - loss: 1272.9801 - loglik: -1.2731e+03 - logprior: 0.1515
Epoch 6/10
19/19 - 6s - loss: 1271.2516 - loglik: -1.2714e+03 - logprior: 0.1938
Epoch 7/10
19/19 - 6s - loss: 1270.4117 - loglik: -1.2706e+03 - logprior: 0.2140
Epoch 8/10
19/19 - 6s - loss: 1272.5295 - loglik: -1.2728e+03 - logprior: 0.2301
Fitted a model with MAP estimate = -1270.3398
expansions: [(0, 2), (25, 1), (26, 1), (28, 2), (45, 1), (48, 17), (89, 4), (102, 1), (117, 1), (119, 1), (135, 1), (151, 3), (152, 1), (162, 1), (165, 6), (181, 1), (183, 1), (184, 2), (185, 1), (198, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 248 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1297.6412 - loglik: -1.2846e+03 - logprior: -1.3011e+01
Epoch 2/2
19/19 - 8s - loss: 1273.5039 - loglik: -1.2728e+03 - logprior: -7.0373e-01
Fitted a model with MAP estimate = -1268.9893
expansions: [(182, 1), (186, 1)]
discards: [  0  28  32  55  56  57  58  59  60  61  62  63  64  65 189 202 203 204
 205 211 212 245 246 247]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1288.2885 - loglik: -1.2757e+03 - logprior: -1.2625e+01
Epoch 2/2
19/19 - 7s - loss: 1275.1776 - loglik: -1.2724e+03 - logprior: -2.7398e+00
Fitted a model with MAP estimate = -1269.9394
expansions: [(0, 2), (27, 1), (177, 1)]
discards: [  0  52  53  54  55  56 100]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1281.7083 - loglik: -1.2734e+03 - logprior: -8.2919e+00
Epoch 2/10
19/19 - 7s - loss: 1272.3970 - loglik: -1.2730e+03 - logprior: 0.6437
Epoch 3/10
19/19 - 7s - loss: 1267.0492 - loglik: -1.2686e+03 - logprior: 1.5766
Epoch 4/10
19/19 - 7s - loss: 1264.2485 - loglik: -1.2664e+03 - logprior: 2.1140
Epoch 5/10
19/19 - 7s - loss: 1263.8196 - loglik: -1.2662e+03 - logprior: 2.3365
Epoch 6/10
19/19 - 7s - loss: 1261.9092 - loglik: -1.2644e+03 - logprior: 2.5060
Epoch 7/10
19/19 - 7s - loss: 1263.5254 - loglik: -1.2662e+03 - logprior: 2.6522
Fitted a model with MAP estimate = -1261.1655
Time for alignment: 165.9228
Computed alignments with likelihoods: ['-1262.8483', '-1255.9949', '-1257.4955', '-1261.1282', '-1261.1655']
Best model has likelihood: -1255.9949  (prior= 2.5362 )
time for generating output: 0.2984
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7903966597077244
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.4668 - loglik: -3.1723e+02 - logprior: -3.2333e+00
Epoch 2/10
19/19 - 1s - loss: 288.1130 - loglik: -2.8657e+02 - logprior: -1.5461e+00
Epoch 3/10
19/19 - 1s - loss: 274.6734 - loglik: -2.7305e+02 - logprior: -1.6269e+00
Epoch 4/10
19/19 - 1s - loss: 271.6185 - loglik: -2.7002e+02 - logprior: -1.5949e+00
Epoch 5/10
19/19 - 1s - loss: 270.9501 - loglik: -2.6939e+02 - logprior: -1.5563e+00
Epoch 6/10
19/19 - 1s - loss: 270.2257 - loglik: -2.6870e+02 - logprior: -1.5278e+00
Epoch 7/10
19/19 - 1s - loss: 270.2196 - loglik: -2.6869e+02 - logprior: -1.5288e+00
Epoch 8/10
19/19 - 1s - loss: 269.4291 - loglik: -2.6789e+02 - logprior: -1.5361e+00
Epoch 9/10
19/19 - 1s - loss: 269.7071 - loglik: -2.6817e+02 - logprior: -1.5406e+00
Fitted a model with MAP estimate = -254.7956
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.8312 - loglik: -2.6851e+02 - logprior: -3.3261e+00
Epoch 2/2
19/19 - 1s - loss: 262.1123 - loglik: -2.6065e+02 - logprior: -1.4635e+00
Fitted a model with MAP estimate = -246.5671
expansions: []
discards: [12 38 42 44 47]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 264.0714 - loglik: -2.6090e+02 - logprior: -3.1683e+00
Epoch 2/2
19/19 - 1s - loss: 260.7177 - loglik: -2.5944e+02 - logprior: -1.2791e+00
Fitted a model with MAP estimate = -246.1995
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 247.9397 - loglik: -2.4535e+02 - logprior: -2.5886e+00
Epoch 2/10
21/21 - 1s - loss: 245.3654 - loglik: -2.4410e+02 - logprior: -1.2636e+00
Epoch 3/10
21/21 - 1s - loss: 244.4471 - loglik: -2.4327e+02 - logprior: -1.1781e+00
Epoch 4/10
21/21 - 1s - loss: 244.3701 - loglik: -2.4324e+02 - logprior: -1.1254e+00
Epoch 5/10
21/21 - 1s - loss: 244.1570 - loglik: -2.4304e+02 - logprior: -1.1160e+00
Epoch 6/10
21/21 - 1s - loss: 243.8564 - loglik: -2.4274e+02 - logprior: -1.1162e+00
Epoch 7/10
21/21 - 1s - loss: 243.3378 - loglik: -2.4223e+02 - logprior: -1.1071e+00
Epoch 8/10
21/21 - 1s - loss: 243.5876 - loglik: -2.4249e+02 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -243.4132
Time for alignment: 44.2627
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.2924 - loglik: -3.1706e+02 - logprior: -3.2336e+00
Epoch 2/10
19/19 - 1s - loss: 286.8372 - loglik: -2.8528e+02 - logprior: -1.5591e+00
Epoch 3/10
19/19 - 1s - loss: 273.5808 - loglik: -2.7198e+02 - logprior: -1.6036e+00
Epoch 4/10
19/19 - 1s - loss: 270.5484 - loglik: -2.6899e+02 - logprior: -1.5563e+00
Epoch 5/10
19/19 - 1s - loss: 269.5976 - loglik: -2.6808e+02 - logprior: -1.5185e+00
Epoch 6/10
19/19 - 1s - loss: 269.2693 - loglik: -2.6777e+02 - logprior: -1.4951e+00
Epoch 7/10
19/19 - 1s - loss: 268.9495 - loglik: -2.6748e+02 - logprior: -1.4725e+00
Epoch 8/10
19/19 - 1s - loss: 268.8046 - loglik: -2.6735e+02 - logprior: -1.4595e+00
Epoch 9/10
19/19 - 1s - loss: 268.7776 - loglik: -2.6732e+02 - logprior: -1.4581e+00
Epoch 10/10
19/19 - 1s - loss: 268.9999 - loglik: -2.6755e+02 - logprior: -1.4497e+00
Fitted a model with MAP estimate = -254.4491
expansions: [(5, 1), (7, 2), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 269.2426 - loglik: -2.6595e+02 - logprior: -3.2943e+00
Epoch 2/2
19/19 - 1s - loss: 261.2742 - loglik: -2.5984e+02 - logprior: -1.4311e+00
Fitted a model with MAP estimate = -246.1979
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 263.7003 - loglik: -2.6053e+02 - logprior: -3.1748e+00
Epoch 2/2
19/19 - 1s - loss: 260.8811 - loglik: -2.5959e+02 - logprior: -1.2946e+00
Fitted a model with MAP estimate = -246.1479
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 248.1847 - loglik: -2.4559e+02 - logprior: -2.5953e+00
Epoch 2/10
21/21 - 1s - loss: 245.4224 - loglik: -2.4415e+02 - logprior: -1.2752e+00
Epoch 3/10
21/21 - 1s - loss: 244.9473 - loglik: -2.4375e+02 - logprior: -1.1929e+00
Epoch 4/10
21/21 - 1s - loss: 244.2876 - loglik: -2.4313e+02 - logprior: -1.1566e+00
Epoch 5/10
21/21 - 1s - loss: 243.7163 - loglik: -2.4257e+02 - logprior: -1.1453e+00
Epoch 6/10
21/21 - 1s - loss: 244.0243 - loglik: -2.4288e+02 - logprior: -1.1401e+00
Fitted a model with MAP estimate = -243.6067
Time for alignment: 43.4559
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.4024 - loglik: -3.1717e+02 - logprior: -3.2343e+00
Epoch 2/10
19/19 - 1s - loss: 287.0361 - loglik: -2.8548e+02 - logprior: -1.5521e+00
Epoch 3/10
19/19 - 1s - loss: 272.6659 - loglik: -2.7104e+02 - logprior: -1.6275e+00
Epoch 4/10
19/19 - 1s - loss: 269.5017 - loglik: -2.6790e+02 - logprior: -1.5975e+00
Epoch 5/10
19/19 - 1s - loss: 268.7884 - loglik: -2.6725e+02 - logprior: -1.5382e+00
Epoch 6/10
19/19 - 1s - loss: 268.3800 - loglik: -2.6686e+02 - logprior: -1.5231e+00
Epoch 7/10
19/19 - 1s - loss: 268.2162 - loglik: -2.6671e+02 - logprior: -1.5085e+00
Epoch 8/10
19/19 - 1s - loss: 267.8530 - loglik: -2.6636e+02 - logprior: -1.4968e+00
Epoch 9/10
19/19 - 1s - loss: 268.0330 - loglik: -2.6654e+02 - logprior: -1.4894e+00
Fitted a model with MAP estimate = -253.4800
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 269.5498 - loglik: -2.6626e+02 - logprior: -3.2939e+00
Epoch 2/2
19/19 - 1s - loss: 261.2887 - loglik: -2.5989e+02 - logprior: -1.4005e+00
Fitted a model with MAP estimate = -246.3874
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.7137 - loglik: -2.6055e+02 - logprior: -3.1670e+00
Epoch 2/2
19/19 - 1s - loss: 260.8237 - loglik: -2.5955e+02 - logprior: -1.2735e+00
Fitted a model with MAP estimate = -246.1824
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 247.9448 - loglik: -2.4537e+02 - logprior: -2.5705e+00
Epoch 2/10
21/21 - 1s - loss: 245.6098 - loglik: -2.4435e+02 - logprior: -1.2607e+00
Epoch 3/10
21/21 - 1s - loss: 244.5341 - loglik: -2.4338e+02 - logprior: -1.1589e+00
Epoch 4/10
21/21 - 1s - loss: 244.2985 - loglik: -2.4317e+02 - logprior: -1.1300e+00
Epoch 5/10
21/21 - 1s - loss: 244.0270 - loglik: -2.4292e+02 - logprior: -1.1056e+00
Epoch 6/10
21/21 - 1s - loss: 243.8158 - loglik: -2.4270e+02 - logprior: -1.1175e+00
Epoch 7/10
21/21 - 1s - loss: 243.8340 - loglik: -2.4273e+02 - logprior: -1.1022e+00
Fitted a model with MAP estimate = -243.4741
Time for alignment: 43.1822
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.3812 - loglik: -3.1715e+02 - logprior: -3.2338e+00
Epoch 2/10
19/19 - 1s - loss: 287.5245 - loglik: -2.8595e+02 - logprior: -1.5747e+00
Epoch 3/10
19/19 - 1s - loss: 272.2961 - loglik: -2.7066e+02 - logprior: -1.6350e+00
Epoch 4/10
19/19 - 1s - loss: 268.9857 - loglik: -2.6737e+02 - logprior: -1.6142e+00
Epoch 5/10
19/19 - 1s - loss: 268.4697 - loglik: -2.6690e+02 - logprior: -1.5664e+00
Epoch 6/10
19/19 - 1s - loss: 267.9788 - loglik: -2.6644e+02 - logprior: -1.5377e+00
Epoch 7/10
19/19 - 1s - loss: 267.6476 - loglik: -2.6613e+02 - logprior: -1.5188e+00
Epoch 8/10
19/19 - 1s - loss: 267.9565 - loglik: -2.6645e+02 - logprior: -1.5071e+00
Fitted a model with MAP estimate = -252.9438
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 269.7604 - loglik: -2.6644e+02 - logprior: -3.3164e+00
Epoch 2/2
19/19 - 1s - loss: 261.4942 - loglik: -2.6005e+02 - logprior: -1.4403e+00
Fitted a model with MAP estimate = -246.3442
expansions: []
discards: [20 38 42 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 263.7743 - loglik: -2.6059e+02 - logprior: -3.1836e+00
Epoch 2/2
19/19 - 1s - loss: 260.9462 - loglik: -2.5965e+02 - logprior: -1.2966e+00
Fitted a model with MAP estimate = -246.1725
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 247.8552 - loglik: -2.4526e+02 - logprior: -2.5988e+00
Epoch 2/10
21/21 - 1s - loss: 245.5248 - loglik: -2.4425e+02 - logprior: -1.2762e+00
Epoch 3/10
21/21 - 1s - loss: 244.9002 - loglik: -2.4370e+02 - logprior: -1.1955e+00
Epoch 4/10
21/21 - 1s - loss: 244.3875 - loglik: -2.4323e+02 - logprior: -1.1582e+00
Epoch 5/10
21/21 - 1s - loss: 243.8538 - loglik: -2.4270e+02 - logprior: -1.1494e+00
Epoch 6/10
21/21 - 1s - loss: 243.9456 - loglik: -2.4281e+02 - logprior: -1.1391e+00
Fitted a model with MAP estimate = -243.5510
Time for alignment: 39.8683
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 320.4046 - loglik: -3.1717e+02 - logprior: -3.2314e+00
Epoch 2/10
19/19 - 1s - loss: 287.0189 - loglik: -2.8546e+02 - logprior: -1.5555e+00
Epoch 3/10
19/19 - 1s - loss: 272.4976 - loglik: -2.7088e+02 - logprior: -1.6184e+00
Epoch 4/10
19/19 - 1s - loss: 269.7848 - loglik: -2.6821e+02 - logprior: -1.5709e+00
Epoch 5/10
19/19 - 1s - loss: 268.8039 - loglik: -2.6728e+02 - logprior: -1.5242e+00
Epoch 6/10
19/19 - 1s - loss: 268.6371 - loglik: -2.6714e+02 - logprior: -1.4932e+00
Epoch 7/10
19/19 - 1s - loss: 268.1720 - loglik: -2.6669e+02 - logprior: -1.4816e+00
Epoch 8/10
19/19 - 1s - loss: 268.1654 - loglik: -2.6670e+02 - logprior: -1.4674e+00
Epoch 9/10
19/19 - 1s - loss: 268.1272 - loglik: -2.6667e+02 - logprior: -1.4604e+00
Epoch 10/10
19/19 - 1s - loss: 267.9905 - loglik: -2.6654e+02 - logprior: -1.4541e+00
Fitted a model with MAP estimate = -253.6330
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 269.0712 - loglik: -2.6578e+02 - logprior: -3.2943e+00
Epoch 2/2
19/19 - 1s - loss: 261.3683 - loglik: -2.5995e+02 - logprior: -1.4204e+00
Fitted a model with MAP estimate = -246.2441
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 263.6490 - loglik: -2.6047e+02 - logprior: -3.1760e+00
Epoch 2/2
19/19 - 1s - loss: 260.8268 - loglik: -2.5953e+02 - logprior: -1.2946e+00
Fitted a model with MAP estimate = -246.0701
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 248.2232 - loglik: -2.4564e+02 - logprior: -2.5833e+00
Epoch 2/10
21/21 - 1s - loss: 245.2416 - loglik: -2.4396e+02 - logprior: -1.2792e+00
Epoch 3/10
21/21 - 1s - loss: 244.9571 - loglik: -2.4377e+02 - logprior: -1.1890e+00
Epoch 4/10
21/21 - 1s - loss: 244.6053 - loglik: -2.4346e+02 - logprior: -1.1488e+00
Epoch 5/10
21/21 - 1s - loss: 243.7515 - loglik: -2.4262e+02 - logprior: -1.1322e+00
Epoch 6/10
21/21 - 1s - loss: 243.7447 - loglik: -2.4260e+02 - logprior: -1.1429e+00
Epoch 7/10
21/21 - 1s - loss: 243.3940 - loglik: -2.4228e+02 - logprior: -1.1149e+00
Epoch 8/10
21/21 - 1s - loss: 243.8910 - loglik: -2.4279e+02 - logprior: -1.1008e+00
Fitted a model with MAP estimate = -243.4052
Time for alignment: 45.0229
Computed alignments with likelihoods: ['-243.4132', '-243.6067', '-243.4741', '-243.5510', '-243.4052']
Best model has likelihood: -243.4052  (prior= -1.0998 )
time for generating output: 0.1461
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9517063747585319
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 873.1904 - loglik: -8.6851e+02 - logprior: -4.6852e+00
Epoch 2/10
27/27 - 4s - loss: 788.1879 - loglik: -7.8629e+02 - logprior: -1.8979e+00
Epoch 3/10
27/27 - 4s - loss: 773.6439 - loglik: -7.7178e+02 - logprior: -1.8650e+00
Epoch 4/10
27/27 - 3s - loss: 770.4662 - loglik: -7.6866e+02 - logprior: -1.8079e+00
Epoch 5/10
27/27 - 4s - loss: 771.4421 - loglik: -7.6961e+02 - logprior: -1.8318e+00
Fitted a model with MAP estimate = -769.7030
expansions: [(0, 2), (10, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (101, 3), (102, 1), (103, 1), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 763.5449 - loglik: -7.5729e+02 - logprior: -6.2572e+00
Epoch 2/2
27/27 - 4s - loss: 746.0092 - loglik: -7.4470e+02 - logprior: -1.3069e+00
Fitted a model with MAP estimate = -743.9013
expansions: [(42, 1)]
discards: [  0  29  45  46  47  87 110 138]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 754.6011 - loglik: -7.4831e+02 - logprior: -6.2947e+00
Epoch 2/2
27/27 - 4s - loss: 749.1392 - loglik: -7.4790e+02 - logprior: -1.2434e+00
Fitted a model with MAP estimate = -746.2308
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 751.5616 - loglik: -7.4740e+02 - logprior: -4.1632e+00
Epoch 2/10
27/27 - 4s - loss: 746.4484 - loglik: -7.4570e+02 - logprior: -7.5017e-01
Epoch 3/10
27/27 - 4s - loss: 745.4048 - loglik: -7.4496e+02 - logprior: -4.4533e-01
Epoch 4/10
27/27 - 4s - loss: 744.2410 - loglik: -7.4391e+02 - logprior: -3.2807e-01
Epoch 5/10
27/27 - 4s - loss: 745.4933 - loglik: -7.4528e+02 - logprior: -2.1495e-01
Fitted a model with MAP estimate = -744.1422
Time for alignment: 88.7611
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 873.6257 - loglik: -8.6893e+02 - logprior: -4.6934e+00
Epoch 2/10
27/27 - 3s - loss: 789.5738 - loglik: -7.8759e+02 - logprior: -1.9794e+00
Epoch 3/10
27/27 - 4s - loss: 774.9308 - loglik: -7.7293e+02 - logprior: -2.0049e+00
Epoch 4/10
27/27 - 4s - loss: 770.0712 - loglik: -7.6807e+02 - logprior: -2.0035e+00
Epoch 5/10
27/27 - 4s - loss: 770.8466 - loglik: -7.6890e+02 - logprior: -1.9503e+00
Fitted a model with MAP estimate = -769.8849
expansions: [(0, 2), (19, 1), (22, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (68, 1), (69, 1), (72, 1), (73, 1), (74, 1), (77, 1), (78, 1), (81, 1), (82, 2), (100, 1), (102, 1), (103, 1), (104, 2), (105, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 765.1016 - loglik: -7.5880e+02 - logprior: -6.3003e+00
Epoch 2/2
27/27 - 4s - loss: 746.4742 - loglik: -7.4509e+02 - logprior: -1.3860e+00
Fitted a model with MAP estimate = -744.0233
expansions: [(129, 1)]
discards: [  0  29  47  48 108 135]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 753.6282 - loglik: -7.4733e+02 - logprior: -6.2973e+00
Epoch 2/2
27/27 - 4s - loss: 746.9774 - loglik: -7.4581e+02 - logprior: -1.1701e+00
Fitted a model with MAP estimate = -744.6607
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 750.2476 - loglik: -7.4611e+02 - logprior: -4.1355e+00
Epoch 2/10
27/27 - 4s - loss: 745.1663 - loglik: -7.4444e+02 - logprior: -7.2857e-01
Epoch 3/10
27/27 - 4s - loss: 742.8895 - loglik: -7.4243e+02 - logprior: -4.5917e-01
Epoch 4/10
27/27 - 4s - loss: 743.1812 - loglik: -7.4281e+02 - logprior: -3.6708e-01
Fitted a model with MAP estimate = -742.8446
Time for alignment: 84.7249
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 871.8929 - loglik: -8.6719e+02 - logprior: -4.7031e+00
Epoch 2/10
27/27 - 4s - loss: 791.2058 - loglik: -7.8922e+02 - logprior: -1.9819e+00
Epoch 3/10
27/27 - 4s - loss: 773.4277 - loglik: -7.7146e+02 - logprior: -1.9644e+00
Epoch 4/10
27/27 - 4s - loss: 769.9597 - loglik: -7.6797e+02 - logprior: -1.9935e+00
Epoch 5/10
27/27 - 4s - loss: 767.7976 - loglik: -7.6577e+02 - logprior: -2.0306e+00
Epoch 6/10
27/27 - 4s - loss: 766.7324 - loglik: -7.6473e+02 - logprior: -2.0054e+00
Epoch 7/10
27/27 - 4s - loss: 767.0061 - loglik: -7.6502e+02 - logprior: -1.9881e+00
Fitted a model with MAP estimate = -766.8903
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (35, 1), (36, 1), (38, 4), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (69, 1), (72, 1), (73, 1), (74, 1), (77, 1), (78, 1), (81, 1), (83, 2), (102, 1), (103, 1), (104, 2), (105, 2), (106, 1), (108, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 760.9802 - loglik: -7.5459e+02 - logprior: -6.3933e+00
Epoch 2/2
27/27 - 4s - loss: 744.1879 - loglik: -7.4285e+02 - logprior: -1.3405e+00
Fitted a model with MAP estimate = -742.7322
expansions: []
discards: [  0  12  30  49  50 109 135 137]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 753.7304 - loglik: -7.4742e+02 - logprior: -6.3140e+00
Epoch 2/2
27/27 - 4s - loss: 746.3682 - loglik: -7.4511e+02 - logprior: -1.2584e+00
Fitted a model with MAP estimate = -744.7491
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 749.8799 - loglik: -7.4570e+02 - logprior: -4.1784e+00
Epoch 2/10
27/27 - 4s - loss: 745.3192 - loglik: -7.4457e+02 - logprior: -7.5126e-01
Epoch 3/10
27/27 - 4s - loss: 743.6500 - loglik: -7.4319e+02 - logprior: -4.5861e-01
Epoch 4/10
27/27 - 4s - loss: 743.8970 - loglik: -7.4354e+02 - logprior: -3.5236e-01
Fitted a model with MAP estimate = -743.0341
Time for alignment: 92.5142
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 872.2000 - loglik: -8.6751e+02 - logprior: -4.6912e+00
Epoch 2/10
27/27 - 4s - loss: 790.3687 - loglik: -7.8840e+02 - logprior: -1.9675e+00
Epoch 3/10
27/27 - 4s - loss: 774.7059 - loglik: -7.7278e+02 - logprior: -1.9247e+00
Epoch 4/10
27/27 - 4s - loss: 771.0176 - loglik: -7.6908e+02 - logprior: -1.9407e+00
Epoch 5/10
27/27 - 4s - loss: 770.0816 - loglik: -7.6814e+02 - logprior: -1.9409e+00
Epoch 6/10
27/27 - 4s - loss: 768.2664 - loglik: -7.6627e+02 - logprior: -1.9928e+00
Epoch 7/10
27/27 - 4s - loss: 767.8668 - loglik: -7.6583e+02 - logprior: -2.0349e+00
Epoch 8/10
27/27 - 4s - loss: 767.3589 - loglik: -7.6534e+02 - logprior: -2.0204e+00
Epoch 9/10
27/27 - 4s - loss: 769.0925 - loglik: -7.6708e+02 - logprior: -2.0102e+00
Fitted a model with MAP estimate = -767.6409
expansions: [(0, 2), (9, 1), (10, 1), (21, 1), (25, 2), (26, 1), (35, 2), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (69, 1), (72, 1), (73, 1), (74, 1), (77, 1), (78, 1), (82, 2), (83, 2), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 762.7249 - loglik: -7.5619e+02 - logprior: -6.5317e+00
Epoch 2/2
27/27 - 5s - loss: 745.7833 - loglik: -7.4438e+02 - logprior: -1.4038e+00
Fitted a model with MAP estimate = -743.0095
expansions: []
discards: [  0  13  30  48  49 108 110 136 138]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 755.6329 - loglik: -7.4937e+02 - logprior: -6.2670e+00
Epoch 2/2
27/27 - 4s - loss: 746.1390 - loglik: -7.4495e+02 - logprior: -1.1925e+00
Fitted a model with MAP estimate = -745.4127
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 750.2439 - loglik: -7.4608e+02 - logprior: -4.1629e+00
Epoch 2/10
27/27 - 4s - loss: 745.9771 - loglik: -7.4524e+02 - logprior: -7.3513e-01
Epoch 3/10
27/27 - 4s - loss: 745.3928 - loglik: -7.4496e+02 - logprior: -4.3191e-01
Epoch 4/10
27/27 - 4s - loss: 744.6266 - loglik: -7.4430e+02 - logprior: -3.2426e-01
Epoch 5/10
27/27 - 4s - loss: 743.6505 - loglik: -7.4345e+02 - logprior: -2.0475e-01
Epoch 6/10
27/27 - 4s - loss: 743.9171 - loglik: -7.4382e+02 - logprior: -1.0003e-01
Fitted a model with MAP estimate = -743.3903
Time for alignment: 106.2693
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 872.9273 - loglik: -8.6823e+02 - logprior: -4.6970e+00
Epoch 2/10
27/27 - 3s - loss: 786.3807 - loglik: -7.8444e+02 - logprior: -1.9378e+00
Epoch 3/10
27/27 - 4s - loss: 774.2954 - loglik: -7.7238e+02 - logprior: -1.9123e+00
Epoch 4/10
27/27 - 3s - loss: 771.0854 - loglik: -7.6924e+02 - logprior: -1.8498e+00
Epoch 5/10
27/27 - 4s - loss: 771.2265 - loglik: -7.6940e+02 - logprior: -1.8310e+00
Fitted a model with MAP estimate = -769.8130
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (35, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 2), (100, 1), (101, 1), (102, 1), (105, 1), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 763.5386 - loglik: -7.5736e+02 - logprior: -6.1757e+00
Epoch 2/2
27/27 - 4s - loss: 747.3715 - loglik: -7.4625e+02 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -745.3633
expansions: []
discards: [  0  12  29  47  48  49 110]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 756.7664 - loglik: -7.5045e+02 - logprior: -6.3187e+00
Epoch 2/2
27/27 - 4s - loss: 748.6762 - loglik: -7.4739e+02 - logprior: -1.2839e+00
Fitted a model with MAP estimate = -746.9083
expansions: []
discards: [146]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 753.2245 - loglik: -7.4903e+02 - logprior: -4.1964e+00
Epoch 2/10
27/27 - 4s - loss: 747.8221 - loglik: -7.4705e+02 - logprior: -7.7480e-01
Epoch 3/10
27/27 - 4s - loss: 748.5621 - loglik: -7.4808e+02 - logprior: -4.8157e-01
Fitted a model with MAP estimate = -746.8129
Time for alignment: 78.4441
Computed alignments with likelihoods: ['-743.9013', '-742.8446', '-742.7322', '-743.0095', '-745.3633']
Best model has likelihood: -742.7322  (prior= -1.1335 )
time for generating output: 0.2837
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5598006644518272
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1611.8811 - loglik: -1.6098e+03 - logprior: -2.1040e+00
Epoch 2/10
34/34 - 14s - loss: 1486.1166 - loglik: -1.4846e+03 - logprior: -1.4949e+00
Epoch 3/10
34/34 - 14s - loss: 1472.4155 - loglik: -1.4708e+03 - logprior: -1.6015e+00
Epoch 4/10
34/34 - 14s - loss: 1465.5044 - loglik: -1.4640e+03 - logprior: -1.5407e+00
Epoch 5/10
34/34 - 14s - loss: 1465.4133 - loglik: -1.4639e+03 - logprior: -1.5450e+00
Epoch 6/10
34/34 - 14s - loss: 1466.3134 - loglik: -1.4647e+03 - logprior: -1.5662e+00
Fitted a model with MAP estimate = -1464.6967
expansions: [(9, 1), (12, 1), (16, 3), (18, 6), (27, 1), (29, 1), (30, 2), (52, 2), (56, 6), (65, 1), (66, 1), (67, 1), (88, 2), (92, 1), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 3), (172, 2), (173, 2), (175, 1), (178, 1), (188, 1), (190, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1449.2035 - loglik: -1.4458e+03 - logprior: -3.3713e+00
Epoch 2/2
34/34 - 20s - loss: 1428.3994 - loglik: -1.4270e+03 - logprior: -1.4441e+00
Fitted a model with MAP estimate = -1422.3196
expansions: [(24, 2), (73, 1), (187, 1), (216, 1), (217, 1)]
discards: [ 17  18  41  74  75 224 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1432.4200 - loglik: -1.4303e+03 - logprior: -2.0982e+00
Epoch 2/2
34/34 - 20s - loss: 1427.6310 - loglik: -1.4272e+03 - logprior: -3.9182e-01
Fitted a model with MAP estimate = -1424.4082
expansions: [(259, 2)]
discards: [ 23 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 1433.0867 - loglik: -1.4312e+03 - logprior: -1.8644e+00
Epoch 2/10
34/34 - 19s - loss: 1426.0675 - loglik: -1.4260e+03 - logprior: -4.4318e-02
Epoch 3/10
34/34 - 20s - loss: 1422.9989 - loglik: -1.4232e+03 - logprior: 0.1661
Epoch 4/10
34/34 - 19s - loss: 1425.0099 - loglik: -1.4253e+03 - logprior: 0.3305
Fitted a model with MAP estimate = -1422.6905
Time for alignment: 352.7306
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 1615.9130 - loglik: -1.6138e+03 - logprior: -2.1105e+00
Epoch 2/10
34/34 - 14s - loss: 1486.5149 - loglik: -1.4849e+03 - logprior: -1.6320e+00
Epoch 3/10
34/34 - 14s - loss: 1469.9404 - loglik: -1.4682e+03 - logprior: -1.7291e+00
Epoch 4/10
34/34 - 14s - loss: 1467.8187 - loglik: -1.4661e+03 - logprior: -1.6928e+00
Epoch 5/10
34/34 - 14s - loss: 1467.9486 - loglik: -1.4662e+03 - logprior: -1.7022e+00
Fitted a model with MAP estimate = -1464.8387
expansions: [(7, 1), (8, 1), (14, 1), (15, 3), (16, 2), (17, 5), (29, 1), (30, 2), (51, 2), (55, 4), (56, 2), (65, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (134, 1), (136, 1), (139, 1), (141, 1), (143, 2), (144, 1), (147, 1), (164, 3), (167, 1), (171, 1), (172, 1), (173, 1), (175, 1), (178, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1449.8719 - loglik: -1.4465e+03 - logprior: -3.3559e+00
Epoch 2/2
34/34 - 19s - loss: 1428.3945 - loglik: -1.4270e+03 - logprior: -1.4213e+00
Fitted a model with MAP estimate = -1423.9106
expansions: [(67, 2), (73, 2), (186, 1)]
discards: [ 18  22  42  74  75  76  77 210 223 259]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 1438.5499 - loglik: -1.4365e+03 - logprior: -2.0186e+00
Epoch 2/2
34/34 - 19s - loss: 1427.5098 - loglik: -1.4273e+03 - logprior: -2.5592e-01
Fitted a model with MAP estimate = -1428.5274
expansions: []
discards: [65]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1435.4254 - loglik: -1.4336e+03 - logprior: -1.8420e+00
Epoch 2/10
34/34 - 19s - loss: 1427.5516 - loglik: -1.4276e+03 - logprior: 0.0779
Epoch 3/10
34/34 - 19s - loss: 1428.5818 - loglik: -1.4288e+03 - logprior: 0.2439
Fitted a model with MAP estimate = -1426.2169
Time for alignment: 314.5937
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1612.2871 - loglik: -1.6102e+03 - logprior: -2.0996e+00
Epoch 2/10
34/34 - 14s - loss: 1485.4283 - loglik: -1.4838e+03 - logprior: -1.6188e+00
Epoch 3/10
34/34 - 14s - loss: 1472.1444 - loglik: -1.4703e+03 - logprior: -1.7954e+00
Epoch 4/10
34/34 - 14s - loss: 1466.3746 - loglik: -1.4646e+03 - logprior: -1.7462e+00
Epoch 5/10
34/34 - 14s - loss: 1465.8721 - loglik: -1.4641e+03 - logprior: -1.7786e+00
Epoch 6/10
34/34 - 14s - loss: 1462.4680 - loglik: -1.4607e+03 - logprior: -1.7931e+00
Epoch 7/10
34/34 - 14s - loss: 1464.0862 - loglik: -1.4623e+03 - logprior: -1.8148e+00
Fitted a model with MAP estimate = -1463.6477
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 1), (30, 2), (34, 1), (48, 1), (50, 2), (54, 4), (64, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 3), (168, 1), (172, 1), (173, 2), (176, 1), (184, 2), (185, 4), (186, 2), (188, 1), (190, 1), (193, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1448.5532 - loglik: -1.4452e+03 - logprior: -3.3462e+00
Epoch 2/2
34/34 - 20s - loss: 1424.1771 - loglik: -1.4228e+03 - logprior: -1.3757e+00
Fitted a model with MAP estimate = -1417.3045
expansions: [(209, 1)]
discards: [ 19  41  73  74  75  76 260 261 262]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1432.4812 - loglik: -1.4304e+03 - logprior: -2.0459e+00
Epoch 2/2
34/34 - 19s - loss: 1426.3871 - loglik: -1.4262e+03 - logprior: -1.5000e-01
Fitted a model with MAP estimate = -1423.7791
expansions: [(71, 5), (82, 1), (178, 1)]
discards: [201 202]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1427.6215 - loglik: -1.4259e+03 - logprior: -1.7204e+00
Epoch 2/10
34/34 - 20s - loss: 1419.6365 - loglik: -1.4197e+03 - logprior: 0.0825
Epoch 3/10
34/34 - 19s - loss: 1422.5121 - loglik: -1.4228e+03 - logprior: 0.2493
Fitted a model with MAP estimate = -1417.3392
Time for alignment: 345.7803
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1613.6907 - loglik: -1.6116e+03 - logprior: -2.1134e+00
Epoch 2/10
34/34 - 14s - loss: 1494.5557 - loglik: -1.4928e+03 - logprior: -1.7060e+00
Epoch 3/10
34/34 - 14s - loss: 1475.1498 - loglik: -1.4732e+03 - logprior: -1.9105e+00
Epoch 4/10
34/34 - 14s - loss: 1467.5077 - loglik: -1.4657e+03 - logprior: -1.8551e+00
Epoch 5/10
34/34 - 14s - loss: 1467.7502 - loglik: -1.4659e+03 - logprior: -1.8465e+00
Fitted a model with MAP estimate = -1466.9949
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 4), (17, 2), (24, 1), (37, 1), (48, 1), (50, 2), (51, 1), (56, 1), (60, 1), (64, 1), (65, 1), (69, 1), (87, 2), (92, 1), (93, 1), (95, 2), (98, 1), (100, 1), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (130, 1), (136, 1), (138, 1), (141, 1), (142, 3), (143, 1), (164, 2), (167, 1), (171, 1), (172, 1), (175, 1), (184, 2), (186, 1), (187, 3), (189, 1), (190, 1), (193, 1), (203, 1), (208, 1), (225, 1), (226, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1449.4059 - loglik: -1.4460e+03 - logprior: -3.3592e+00
Epoch 2/2
34/34 - 20s - loss: 1429.4386 - loglik: -1.4281e+03 - logprior: -1.3480e+00
Fitted a model with MAP estimate = -1421.7054
expansions: []
discards: [ 19 109 110 207 257 258 259]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 1432.4697 - loglik: -1.4304e+03 - logprior: -2.0739e+00
Epoch 2/2
34/34 - 19s - loss: 1429.7877 - loglik: -1.4296e+03 - logprior: -2.3422e-01
Fitted a model with MAP estimate = -1426.3485
expansions: []
discards: [181 182]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1434.0409 - loglik: -1.4324e+03 - logprior: -1.6863e+00
Epoch 2/10
34/34 - 19s - loss: 1426.7295 - loglik: -1.4268e+03 - logprior: 0.0878
Epoch 3/10
34/34 - 19s - loss: 1427.3782 - loglik: -1.4276e+03 - logprior: 0.2066
Fitted a model with MAP estimate = -1425.2033
Time for alignment: 313.9050
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 1616.7587 - loglik: -1.6147e+03 - logprior: -2.1011e+00
Epoch 2/10
34/34 - 14s - loss: 1489.9481 - loglik: -1.4884e+03 - logprior: -1.5079e+00
Epoch 3/10
34/34 - 14s - loss: 1477.3889 - loglik: -1.4758e+03 - logprior: -1.6202e+00
Epoch 4/10
34/34 - 14s - loss: 1470.2469 - loglik: -1.4687e+03 - logprior: -1.5621e+00
Epoch 5/10
34/34 - 14s - loss: 1470.7285 - loglik: -1.4692e+03 - logprior: -1.5472e+00
Fitted a model with MAP estimate = -1469.8735
expansions: [(9, 1), (12, 1), (14, 1), (16, 2), (18, 5), (23, 1), (29, 2), (30, 2), (52, 2), (56, 5), (57, 1), (65, 1), (66, 1), (70, 1), (73, 2), (92, 1), (94, 2), (96, 2), (99, 1), (100, 2), (102, 1), (105, 1), (110, 1), (111, 1), (131, 1), (137, 1), (140, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 3), (172, 1), (173, 1), (175, 1), (178, 1), (184, 1), (185, 3), (186, 2), (188, 1), (190, 1), (199, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 1453.2202 - loglik: -1.4498e+03 - logprior: -3.3722e+00
Epoch 2/2
34/34 - 20s - loss: 1425.2622 - loglik: -1.4238e+03 - logprior: -1.4392e+00
Fitted a model with MAP estimate = -1421.4265
expansions: [(42, 1), (189, 1)]
discards: [ 22  23  66 122 217 218 263 264 265]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 1432.1167 - loglik: -1.4300e+03 - logprior: -2.0740e+00
Epoch 2/2
34/34 - 20s - loss: 1425.8081 - loglik: -1.4256e+03 - logprior: -2.5590e-01
Fitted a model with MAP estimate = -1423.3304
expansions: [(62, 1)]
discards: [31]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1429.8821 - loglik: -1.4281e+03 - logprior: -1.8301e+00
Epoch 2/10
34/34 - 19s - loss: 1424.0643 - loglik: -1.4241e+03 - logprior: 0.0296
Epoch 3/10
34/34 - 20s - loss: 1421.0625 - loglik: -1.4213e+03 - logprior: 0.2224
Epoch 4/10
34/34 - 20s - loss: 1422.4534 - loglik: -1.4228e+03 - logprior: 0.3771
Fitted a model with MAP estimate = -1420.2781
Time for alignment: 339.1722
Computed alignments with likelihoods: ['-1422.3196', '-1423.9106', '-1417.3045', '-1421.7054', '-1420.2781']
Best model has likelihood: -1417.3045  (prior= -0.6141 )
time for generating output: 0.3729
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.6911111111111111
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 465.7868 - loglik: -3.7755e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 378.1159 - loglik: -3.5439e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 347.4234 - loglik: -3.3623e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 333.2303 - loglik: -3.2665e+02 - logprior: -6.5816e+00
Epoch 5/10
10/10 - 1s - loss: 326.7693 - loglik: -3.2261e+02 - logprior: -4.1603e+00
Epoch 6/10
10/10 - 1s - loss: 323.4724 - loglik: -3.2066e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 321.8212 - loglik: -3.1990e+02 - logprior: -1.9169e+00
Epoch 8/10
10/10 - 1s - loss: 320.8367 - loglik: -3.1949e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 320.1655 - loglik: -3.1917e+02 - logprior: -9.9284e-01
Epoch 10/10
10/10 - 1s - loss: 319.6224 - loglik: -3.1887e+02 - logprior: -7.5082e-01
Fitted a model with MAP estimate = -319.3566
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 416.0153 - loglik: -3.1740e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 351.3855 - loglik: -3.1114e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -339.8121
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.8879 - loglik: -3.0713e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 325.5236 - loglik: -3.0479e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -316.5833
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 381.5039 - loglik: -3.0420e+02 - logprior: -7.7304e+01
Epoch 2/10
10/10 - 1s - loss: 324.1342 - loglik: -3.0406e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 312.4241 - loglik: -3.0421e+02 - logprior: -8.2154e+00
Epoch 4/10
10/10 - 1s - loss: 307.8812 - loglik: -3.0471e+02 - logprior: -3.1684e+00
Epoch 5/10
10/10 - 1s - loss: 305.5676 - loglik: -3.0522e+02 - logprior: -3.5196e-01
Epoch 6/10
10/10 - 1s - loss: 304.2679 - loglik: -3.0557e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 303.4695 - loglik: -3.0578e+02 - logprior: 2.3110
Epoch 8/10
10/10 - 1s - loss: 302.9131 - loglik: -3.0587e+02 - logprior: 2.9561
Epoch 9/10
10/10 - 1s - loss: 302.4617 - loglik: -3.0591e+02 - logprior: 3.4496
Epoch 10/10
10/10 - 1s - loss: 302.0268 - loglik: -3.0583e+02 - logprior: 3.7996
Fitted a model with MAP estimate = -301.7635
Time for alignment: 29.8602
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 465.7868 - loglik: -3.7755e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 378.1159 - loglik: -3.5439e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 347.4234 - loglik: -3.3623e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 333.2303 - loglik: -3.2665e+02 - logprior: -6.5816e+00
Epoch 5/10
10/10 - 1s - loss: 326.7693 - loglik: -3.2261e+02 - logprior: -4.1603e+00
Epoch 6/10
10/10 - 1s - loss: 323.4723 - loglik: -3.2066e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 321.8212 - loglik: -3.1990e+02 - logprior: -1.9169e+00
Epoch 8/10
10/10 - 1s - loss: 320.8367 - loglik: -3.1949e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 320.1655 - loglik: -3.1917e+02 - logprior: -9.9284e-01
Epoch 10/10
10/10 - 1s - loss: 319.6224 - loglik: -3.1887e+02 - logprior: -7.5082e-01
Fitted a model with MAP estimate = -319.3566
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 416.0153 - loglik: -3.1740e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 351.3855 - loglik: -3.1114e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -339.8121
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 385.8879 - loglik: -3.0713e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 325.5236 - loglik: -3.0479e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -316.5833
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 381.5038 - loglik: -3.0420e+02 - logprior: -7.7304e+01
Epoch 2/10
10/10 - 1s - loss: 324.1342 - loglik: -3.0406e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 312.4241 - loglik: -3.0421e+02 - logprior: -8.2154e+00
Epoch 4/10
10/10 - 1s - loss: 307.8812 - loglik: -3.0471e+02 - logprior: -3.1684e+00
Epoch 5/10
10/10 - 1s - loss: 305.5675 - loglik: -3.0522e+02 - logprior: -3.5194e-01
Epoch 6/10
10/10 - 1s - loss: 304.2679 - loglik: -3.0557e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 0s - loss: 303.4694 - loglik: -3.0578e+02 - logprior: 2.3107
Epoch 8/10
10/10 - 0s - loss: 302.9131 - loglik: -3.0587e+02 - logprior: 2.9562
Epoch 9/10
10/10 - 0s - loss: 302.4617 - loglik: -3.0591e+02 - logprior: 3.4495
Epoch 10/10
10/10 - 1s - loss: 302.0269 - loglik: -3.0583e+02 - logprior: 3.7996
Fitted a model with MAP estimate = -301.7635
Time for alignment: 29.1445
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 465.7868 - loglik: -3.7755e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 378.1159 - loglik: -3.5439e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 347.4234 - loglik: -3.3623e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 333.2303 - loglik: -3.2665e+02 - logprior: -6.5816e+00
Epoch 5/10
10/10 - 1s - loss: 326.7694 - loglik: -3.2261e+02 - logprior: -4.1603e+00
Epoch 6/10
10/10 - 1s - loss: 323.4724 - loglik: -3.2066e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 321.8212 - loglik: -3.1990e+02 - logprior: -1.9169e+00
Epoch 8/10
10/10 - 1s - loss: 320.8367 - loglik: -3.1949e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 320.1655 - loglik: -3.1917e+02 - logprior: -9.9284e-01
Epoch 10/10
10/10 - 1s - loss: 319.6224 - loglik: -3.1887e+02 - logprior: -7.5082e-01
Fitted a model with MAP estimate = -319.3566
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 416.0153 - loglik: -3.1740e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 351.3855 - loglik: -3.1114e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -339.8122
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 385.8879 - loglik: -3.0713e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 325.5236 - loglik: -3.0479e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -316.5833
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 381.5039 - loglik: -3.0420e+02 - logprior: -7.7304e+01
Epoch 2/10
10/10 - 1s - loss: 324.1342 - loglik: -3.0406e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 312.4241 - loglik: -3.0421e+02 - logprior: -8.2154e+00
Epoch 4/10
10/10 - 1s - loss: 307.8812 - loglik: -3.0471e+02 - logprior: -3.1684e+00
Epoch 5/10
10/10 - 1s - loss: 305.5675 - loglik: -3.0522e+02 - logprior: -3.5195e-01
Epoch 6/10
10/10 - 1s - loss: 304.2679 - loglik: -3.0557e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 303.4695 - loglik: -3.0578e+02 - logprior: 2.3110
Epoch 8/10
10/10 - 1s - loss: 302.9131 - loglik: -3.0587e+02 - logprior: 2.9561
Epoch 9/10
10/10 - 1s - loss: 302.4617 - loglik: -3.0591e+02 - logprior: 3.4496
Epoch 10/10
10/10 - 1s - loss: 302.0268 - loglik: -3.0583e+02 - logprior: 3.7996
Fitted a model with MAP estimate = -301.7635
Time for alignment: 27.5428
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 465.7868 - loglik: -3.7755e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 378.1159 - loglik: -3.5439e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 347.4234 - loglik: -3.3623e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 333.2303 - loglik: -3.2665e+02 - logprior: -6.5816e+00
Epoch 5/10
10/10 - 1s - loss: 326.7693 - loglik: -3.2261e+02 - logprior: -4.1603e+00
Epoch 6/10
10/10 - 1s - loss: 323.4723 - loglik: -3.2066e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 321.8212 - loglik: -3.1990e+02 - logprior: -1.9169e+00
Epoch 8/10
10/10 - 1s - loss: 320.8367 - loglik: -3.1949e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 320.1655 - loglik: -3.1917e+02 - logprior: -9.9284e-01
Epoch 10/10
10/10 - 1s - loss: 319.6225 - loglik: -3.1887e+02 - logprior: -7.5082e-01
Fitted a model with MAP estimate = -319.3565
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 416.0153 - loglik: -3.1740e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 351.3855 - loglik: -3.1114e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -339.8121
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 385.8879 - loglik: -3.0713e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 325.5236 - loglik: -3.0479e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -316.5833
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 381.5040 - loglik: -3.0420e+02 - logprior: -7.7304e+01
Epoch 2/10
10/10 - 1s - loss: 324.1342 - loglik: -3.0406e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 1s - loss: 312.4241 - loglik: -3.0421e+02 - logprior: -8.2153e+00
Epoch 4/10
10/10 - 1s - loss: 307.8812 - loglik: -3.0471e+02 - logprior: -3.1683e+00
Epoch 5/10
10/10 - 1s - loss: 305.5676 - loglik: -3.0522e+02 - logprior: -3.5202e-01
Epoch 6/10
10/10 - 1s - loss: 304.2681 - loglik: -3.0557e+02 - logprior: 1.3063
Epoch 7/10
10/10 - 1s - loss: 303.4699 - loglik: -3.0578e+02 - logprior: 2.3117
Epoch 8/10
10/10 - 1s - loss: 302.9131 - loglik: -3.0587e+02 - logprior: 2.9561
Epoch 9/10
10/10 - 1s - loss: 302.4617 - loglik: -3.0591e+02 - logprior: 3.4496
Epoch 10/10
10/10 - 1s - loss: 302.0268 - loglik: -3.0583e+02 - logprior: 3.7997
Fitted a model with MAP estimate = -301.7634
Time for alignment: 29.3928
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 465.7868 - loglik: -3.7755e+02 - logprior: -8.8234e+01
Epoch 2/10
10/10 - 1s - loss: 378.1159 - loglik: -3.5439e+02 - logprior: -2.3722e+01
Epoch 3/10
10/10 - 1s - loss: 347.4234 - loglik: -3.3623e+02 - logprior: -1.1193e+01
Epoch 4/10
10/10 - 1s - loss: 333.2303 - loglik: -3.2665e+02 - logprior: -6.5816e+00
Epoch 5/10
10/10 - 1s - loss: 326.7693 - loglik: -3.2261e+02 - logprior: -4.1603e+00
Epoch 6/10
10/10 - 1s - loss: 323.4723 - loglik: -3.2066e+02 - logprior: -2.8148e+00
Epoch 7/10
10/10 - 1s - loss: 321.8212 - loglik: -3.1990e+02 - logprior: -1.9169e+00
Epoch 8/10
10/10 - 1s - loss: 320.8367 - loglik: -3.1949e+02 - logprior: -1.3452e+00
Epoch 9/10
10/10 - 1s - loss: 320.1655 - loglik: -3.1917e+02 - logprior: -9.9284e-01
Epoch 10/10
10/10 - 1s - loss: 319.6224 - loglik: -3.1887e+02 - logprior: -7.5082e-01
Fitted a model with MAP estimate = -319.3566
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 416.0153 - loglik: -3.1740e+02 - logprior: -9.8620e+01
Epoch 2/2
10/10 - 1s - loss: 351.3855 - loglik: -3.1114e+02 - logprior: -4.0245e+01
Fitted a model with MAP estimate = -339.8121
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.8879 - loglik: -3.0713e+02 - logprior: -7.8761e+01
Epoch 2/2
10/10 - 1s - loss: 325.5236 - loglik: -3.0479e+02 - logprior: -2.0730e+01
Fitted a model with MAP estimate = -316.5833
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 381.5039 - loglik: -3.0420e+02 - logprior: -7.7304e+01
Epoch 2/10
10/10 - 0s - loss: 324.1342 - loglik: -3.0406e+02 - logprior: -2.0075e+01
Epoch 3/10
10/10 - 0s - loss: 312.4242 - loglik: -3.0421e+02 - logprior: -8.2154e+00
Epoch 4/10
10/10 - 0s - loss: 307.8812 - loglik: -3.0471e+02 - logprior: -3.1684e+00
Epoch 5/10
10/10 - 0s - loss: 305.5676 - loglik: -3.0522e+02 - logprior: -3.5195e-01
Epoch 6/10
10/10 - 0s - loss: 304.2679 - loglik: -3.0557e+02 - logprior: 1.3064
Epoch 7/10
10/10 - 1s - loss: 303.4695 - loglik: -3.0578e+02 - logprior: 2.3110
Epoch 8/10
10/10 - 1s - loss: 302.9131 - loglik: -3.0587e+02 - logprior: 2.9561
Epoch 9/10
10/10 - 1s - loss: 302.4617 - loglik: -3.0591e+02 - logprior: 3.4496
Epoch 10/10
10/10 - 0s - loss: 302.0268 - loglik: -3.0583e+02 - logprior: 3.7996
Fitted a model with MAP estimate = -301.7635
Time for alignment: 26.0697
Computed alignments with likelihoods: ['-301.7635', '-301.7635', '-301.7635', '-301.7634', '-301.7635']
Best model has likelihood: -301.7634  (prior= 3.9382 )
time for generating output: 0.1121
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8518282699654357
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 635.4693 - loglik: -6.2778e+02 - logprior: -7.6939e+00
Epoch 2/10
13/13 - 1s - loss: 604.6283 - loglik: -6.0275e+02 - logprior: -1.8783e+00
Epoch 3/10
13/13 - 1s - loss: 580.5081 - loglik: -5.7885e+02 - logprior: -1.6570e+00
Epoch 4/10
13/13 - 1s - loss: 570.6067 - loglik: -5.6874e+02 - logprior: -1.8619e+00
Epoch 5/10
13/13 - 1s - loss: 569.2272 - loglik: -5.6747e+02 - logprior: -1.7576e+00
Epoch 6/10
13/13 - 1s - loss: 568.5456 - loglik: -5.6687e+02 - logprior: -1.6739e+00
Epoch 7/10
13/13 - 1s - loss: 567.3779 - loglik: -5.6569e+02 - logprior: -1.6877e+00
Epoch 8/10
13/13 - 1s - loss: 567.4656 - loglik: -5.6578e+02 - logprior: -1.6857e+00
Fitted a model with MAP estimate = -567.4908
expansions: [(9, 1), (10, 1), (13, 1), (20, 1), (28, 1), (29, 1), (30, 3), (31, 2), (32, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 576.6086 - loglik: -5.6758e+02 - logprior: -9.0333e+00
Epoch 2/2
13/13 - 2s - loss: 566.6986 - loglik: -5.6273e+02 - logprior: -3.9710e+00
Fitted a model with MAP estimate = -565.2559
expansions: [(0, 2)]
discards: [ 0 66 92 95]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 569.0030 - loglik: -5.6220e+02 - logprior: -6.8070e+00
Epoch 2/2
13/13 - 1s - loss: 563.1103 - loglik: -5.6129e+02 - logprior: -1.8230e+00
Fitted a model with MAP estimate = -562.2985
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 571.9612 - loglik: -5.6334e+02 - logprior: -8.6241e+00
Epoch 2/10
13/13 - 1s - loss: 564.8405 - loglik: -5.6228e+02 - logprior: -2.5638e+00
Epoch 3/10
13/13 - 1s - loss: 562.0704 - loglik: -5.6085e+02 - logprior: -1.2244e+00
Epoch 4/10
13/13 - 1s - loss: 562.6017 - loglik: -5.6170e+02 - logprior: -9.0216e-01
Fitted a model with MAP estimate = -561.6613
Time for alignment: 45.2801
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 635.9661 - loglik: -6.2827e+02 - logprior: -7.6918e+00
Epoch 2/10
13/13 - 1s - loss: 604.6683 - loglik: -6.0281e+02 - logprior: -1.8627e+00
Epoch 3/10
13/13 - 1s - loss: 581.7124 - loglik: -5.8008e+02 - logprior: -1.6284e+00
Epoch 4/10
13/13 - 1s - loss: 571.3853 - loglik: -5.6954e+02 - logprior: -1.8460e+00
Epoch 5/10
13/13 - 1s - loss: 568.8618 - loglik: -5.6711e+02 - logprior: -1.7526e+00
Epoch 6/10
13/13 - 1s - loss: 569.1926 - loglik: -5.6753e+02 - logprior: -1.6638e+00
Fitted a model with MAP estimate = -567.7950
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 3), (31, 2), (32, 2), (40, 1), (43, 1), (50, 1), (53, 1), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 577.3448 - loglik: -5.6831e+02 - logprior: -9.0331e+00
Epoch 2/2
13/13 - 2s - loss: 567.0352 - loglik: -5.6306e+02 - logprior: -3.9761e+00
Fitted a model with MAP estimate = -565.3950
expansions: [(0, 2)]
discards: [ 0 14 92 95]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 569.1214 - loglik: -5.6229e+02 - logprior: -6.8266e+00
Epoch 2/2
13/13 - 2s - loss: 563.5603 - loglik: -5.6173e+02 - logprior: -1.8316e+00
Fitted a model with MAP estimate = -562.3086
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 572.3806 - loglik: -5.6374e+02 - logprior: -8.6451e+00
Epoch 2/10
13/13 - 1s - loss: 563.7754 - loglik: -5.6117e+02 - logprior: -2.6056e+00
Epoch 3/10
13/13 - 1s - loss: 562.8992 - loglik: -5.6166e+02 - logprior: -1.2370e+00
Epoch 4/10
13/13 - 1s - loss: 562.2429 - loglik: -5.6133e+02 - logprior: -9.1468e-01
Epoch 5/10
13/13 - 1s - loss: 561.1949 - loglik: -5.6038e+02 - logprior: -8.1960e-01
Epoch 6/10
13/13 - 2s - loss: 561.8226 - loglik: -5.6105e+02 - logprior: -7.7463e-01
Fitted a model with MAP estimate = -561.1420
Time for alignment: 43.6241
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 636.0258 - loglik: -6.2834e+02 - logprior: -7.6892e+00
Epoch 2/10
13/13 - 1s - loss: 604.5485 - loglik: -6.0269e+02 - logprior: -1.8603e+00
Epoch 3/10
13/13 - 1s - loss: 580.1713 - loglik: -5.7854e+02 - logprior: -1.6348e+00
Epoch 4/10
13/13 - 1s - loss: 571.5608 - loglik: -5.6969e+02 - logprior: -1.8703e+00
Epoch 5/10
13/13 - 1s - loss: 568.9970 - loglik: -5.6719e+02 - logprior: -1.8118e+00
Epoch 6/10
13/13 - 1s - loss: 567.9502 - loglik: -5.6623e+02 - logprior: -1.7174e+00
Epoch 7/10
13/13 - 1s - loss: 568.7521 - loglik: -5.6704e+02 - logprior: -1.7141e+00
Fitted a model with MAP estimate = -567.7291
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (21, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 2), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 577.4385 - loglik: -5.6841e+02 - logprior: -9.0276e+00
Epoch 2/2
13/13 - 2s - loss: 567.5513 - loglik: -5.6354e+02 - logprior: -4.0100e+00
Fitted a model with MAP estimate = -565.4472
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 568.6868 - loglik: -5.6189e+02 - logprior: -6.7926e+00
Epoch 2/2
13/13 - 2s - loss: 563.6938 - loglik: -5.6190e+02 - logprior: -1.7915e+00
Fitted a model with MAP estimate = -562.3092
expansions: []
discards: [ 0 90]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 571.4617 - loglik: -5.6286e+02 - logprior: -8.6046e+00
Epoch 2/10
13/13 - 1s - loss: 565.4981 - loglik: -5.6295e+02 - logprior: -2.5507e+00
Epoch 3/10
13/13 - 1s - loss: 562.1220 - loglik: -5.6091e+02 - logprior: -1.2170e+00
Epoch 4/10
13/13 - 1s - loss: 562.1735 - loglik: -5.6128e+02 - logprior: -8.9012e-01
Fitted a model with MAP estimate = -561.7016
Time for alignment: 43.4068
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 636.6969 - loglik: -6.2901e+02 - logprior: -7.6898e+00
Epoch 2/10
13/13 - 1s - loss: 603.8630 - loglik: -6.0200e+02 - logprior: -1.8636e+00
Epoch 3/10
13/13 - 1s - loss: 579.9390 - loglik: -5.7831e+02 - logprior: -1.6314e+00
Epoch 4/10
13/13 - 1s - loss: 571.4041 - loglik: -5.6955e+02 - logprior: -1.8534e+00
Epoch 5/10
13/13 - 1s - loss: 568.5621 - loglik: -5.6676e+02 - logprior: -1.7989e+00
Epoch 6/10
13/13 - 1s - loss: 568.8114 - loglik: -5.6711e+02 - logprior: -1.7052e+00
Fitted a model with MAP estimate = -568.1152
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 2), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 578.1019 - loglik: -5.6907e+02 - logprior: -9.0280e+00
Epoch 2/2
13/13 - 2s - loss: 566.5524 - loglik: -5.6257e+02 - logprior: -3.9848e+00
Fitted a model with MAP estimate = -565.4394
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 569.1136 - loglik: -5.6232e+02 - logprior: -6.7938e+00
Epoch 2/2
13/13 - 2s - loss: 563.3737 - loglik: -5.6158e+02 - logprior: -1.7925e+00
Fitted a model with MAP estimate = -562.3458
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 571.5496 - loglik: -5.6296e+02 - logprior: -8.5856e+00
Epoch 2/10
13/13 - 2s - loss: 564.8593 - loglik: -5.6232e+02 - logprior: -2.5441e+00
Epoch 3/10
13/13 - 2s - loss: 562.3736 - loglik: -5.6117e+02 - logprior: -1.2043e+00
Epoch 4/10
13/13 - 2s - loss: 561.8455 - loglik: -5.6096e+02 - logprior: -8.8981e-01
Epoch 5/10
13/13 - 1s - loss: 561.8176 - loglik: -5.6104e+02 - logprior: -7.8090e-01
Epoch 6/10
13/13 - 1s - loss: 561.3535 - loglik: -5.6060e+02 - logprior: -7.5396e-01
Epoch 7/10
13/13 - 1s - loss: 560.8189 - loglik: -5.6009e+02 - logprior: -7.2435e-01
Epoch 8/10
13/13 - 2s - loss: 561.1687 - loglik: -5.6046e+02 - logprior: -7.0854e-01
Fitted a model with MAP estimate = -560.9411
Time for alignment: 47.4335
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 635.6304 - loglik: -6.2794e+02 - logprior: -7.6896e+00
Epoch 2/10
13/13 - 1s - loss: 604.9044 - loglik: -6.0303e+02 - logprior: -1.8754e+00
Epoch 3/10
13/13 - 1s - loss: 582.9485 - loglik: -5.8129e+02 - logprior: -1.6633e+00
Epoch 4/10
13/13 - 1s - loss: 573.8793 - loglik: -5.7204e+02 - logprior: -1.8368e+00
Epoch 5/10
13/13 - 1s - loss: 569.0317 - loglik: -5.6728e+02 - logprior: -1.7534e+00
Epoch 6/10
13/13 - 1s - loss: 568.9901 - loglik: -5.6732e+02 - logprior: -1.6659e+00
Epoch 7/10
13/13 - 1s - loss: 568.0548 - loglik: -5.6638e+02 - logprior: -1.6742e+00
Epoch 8/10
13/13 - 1s - loss: 568.8745 - loglik: -5.6721e+02 - logprior: -1.6620e+00
Fitted a model with MAP estimate = -568.1650
expansions: [(9, 1), (10, 1), (13, 2), (18, 1), (28, 4), (29, 3), (30, 1), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 578.6401 - loglik: -5.6961e+02 - logprior: -9.0305e+00
Epoch 2/2
13/13 - 2s - loss: 567.1353 - loglik: -5.6312e+02 - logprior: -4.0190e+00
Fitted a model with MAP estimate = -565.4636
expansions: [(0, 2)]
discards: [ 0 14 36 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 569.6553 - loglik: -5.6283e+02 - logprior: -6.8238e+00
Epoch 2/2
13/13 - 1s - loss: 562.6361 - loglik: -5.6082e+02 - logprior: -1.8165e+00
Fitted a model with MAP estimate = -562.3970
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 571.6832 - loglik: -5.6306e+02 - logprior: -8.6281e+00
Epoch 2/10
13/13 - 1s - loss: 564.8115 - loglik: -5.6222e+02 - logprior: -2.5873e+00
Epoch 3/10
13/13 - 1s - loss: 562.3744 - loglik: -5.6116e+02 - logprior: -1.2186e+00
Epoch 4/10
13/13 - 1s - loss: 562.1622 - loglik: -5.6126e+02 - logprior: -9.0534e-01
Epoch 5/10
13/13 - 1s - loss: 561.5625 - loglik: -5.6075e+02 - logprior: -8.1194e-01
Epoch 6/10
13/13 - 1s - loss: 561.7223 - loglik: -5.6096e+02 - logprior: -7.6450e-01
Fitted a model with MAP estimate = -561.2059
Time for alignment: 47.1255
Computed alignments with likelihoods: ['-561.6613', '-561.1420', '-561.7016', '-560.9411', '-561.2059']
Best model has likelihood: -560.9411  (prior= -0.6573 )
time for generating output: 0.1784
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9373409669211196
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 798.9009 - loglik: -7.1333e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 694.4040 - loglik: -6.7458e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 643.3477 - loglik: -6.3536e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 611.8727 - loglik: -6.0736e+02 - logprior: -4.5112e+00
Epoch 5/10
10/10 - 1s - loss: 598.1114 - loglik: -5.9569e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 592.6218 - loglik: -5.9147e+02 - logprior: -1.1499e+00
Epoch 7/10
10/10 - 1s - loss: 589.7726 - loglik: -5.8954e+02 - logprior: -2.3715e-01
Epoch 8/10
10/10 - 1s - loss: 588.1628 - loglik: -5.8853e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 587.1884 - loglik: -5.8801e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 586.5338 - loglik: -5.8778e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -586.2926
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 678.8517 - loglik: -5.8330e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 603.3854 - loglik: -5.6725e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -590.3447
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 639.2821 - loglik: -5.6423e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 576.2350 - loglik: -5.6021e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -566.5795
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 652.6873 - loglik: -5.6288e+02 - logprior: -8.9809e+01
Epoch 2/10
10/10 - 1s - loss: 584.0193 - loglik: -5.6144e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 564.8714 - loglik: -5.6058e+02 - logprior: -4.2915e+00
Epoch 4/10
10/10 - 1s - loss: 557.9018 - loglik: -5.6009e+02 - logprior: 2.1895
Epoch 5/10
10/10 - 1s - loss: 554.5847 - loglik: -5.5988e+02 - logprior: 5.2983
Epoch 6/10
10/10 - 1s - loss: 552.7491 - loglik: -5.5982e+02 - logprior: 7.0708
Epoch 7/10
10/10 - 1s - loss: 551.6107 - loglik: -5.5987e+02 - logprior: 8.2594
Epoch 8/10
10/10 - 1s - loss: 550.8001 - loglik: -5.6001e+02 - logprior: 9.2060
Epoch 9/10
10/10 - 1s - loss: 550.1486 - loglik: -5.6017e+02 - logprior: 10.0210
Epoch 10/10
10/10 - 1s - loss: 549.5818 - loglik: -5.6029e+02 - logprior: 10.7096
Fitted a model with MAP estimate = -549.2905
Time for alignment: 44.0915
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 798.9009 - loglik: -7.1333e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 694.4040 - loglik: -6.7458e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 643.3477 - loglik: -6.3536e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 611.8727 - loglik: -6.0736e+02 - logprior: -4.5112e+00
Epoch 5/10
10/10 - 1s - loss: 598.1114 - loglik: -5.9569e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 592.6218 - loglik: -5.9147e+02 - logprior: -1.1499e+00
Epoch 7/10
10/10 - 1s - loss: 589.7725 - loglik: -5.8954e+02 - logprior: -2.3715e-01
Epoch 8/10
10/10 - 1s - loss: 588.1628 - loglik: -5.8853e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 587.1885 - loglik: -5.8801e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 586.5338 - loglik: -5.8778e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -586.2929
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 678.8517 - loglik: -5.8330e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 603.3854 - loglik: -5.6725e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -590.3446
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 639.2821 - loglik: -5.6423e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 576.2350 - loglik: -5.6021e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -566.5795
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 652.6873 - loglik: -5.6288e+02 - logprior: -8.9809e+01
Epoch 2/10
10/10 - 1s - loss: 584.0193 - loglik: -5.6144e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 564.8713 - loglik: -5.6058e+02 - logprior: -4.2915e+00
Epoch 4/10
10/10 - 1s - loss: 557.9018 - loglik: -5.6009e+02 - logprior: 2.1895
Epoch 5/10
10/10 - 1s - loss: 554.5846 - loglik: -5.5988e+02 - logprior: 5.2983
Epoch 6/10
10/10 - 1s - loss: 552.7491 - loglik: -5.5982e+02 - logprior: 7.0708
Epoch 7/10
10/10 - 1s - loss: 551.6107 - loglik: -5.5987e+02 - logprior: 8.2594
Epoch 8/10
10/10 - 1s - loss: 550.8001 - loglik: -5.6001e+02 - logprior: 9.2060
Epoch 9/10
10/10 - 1s - loss: 550.1487 - loglik: -5.6017e+02 - logprior: 10.0210
Epoch 10/10
10/10 - 1s - loss: 549.5818 - loglik: -5.6029e+02 - logprior: 10.7096
Fitted a model with MAP estimate = -549.2908
Time for alignment: 44.0271
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 798.9009 - loglik: -7.1333e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 694.4040 - loglik: -6.7458e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 643.3477 - loglik: -6.3536e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 611.8727 - loglik: -6.0736e+02 - logprior: -4.5112e+00
Epoch 5/10
10/10 - 1s - loss: 598.1114 - loglik: -5.9569e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 592.6218 - loglik: -5.9147e+02 - logprior: -1.1499e+00
Epoch 7/10
10/10 - 1s - loss: 589.7726 - loglik: -5.8954e+02 - logprior: -2.3714e-01
Epoch 8/10
10/10 - 1s - loss: 588.1628 - loglik: -5.8853e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 587.1884 - loglik: -5.8801e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 586.5338 - loglik: -5.8778e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -586.2925
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 678.8517 - loglik: -5.8330e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 603.3854 - loglik: -5.6725e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -590.3447
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 639.2821 - loglik: -5.6423e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 576.2350 - loglik: -5.6021e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -566.5795
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 652.6873 - loglik: -5.6288e+02 - logprior: -8.9809e+01
Epoch 2/10
10/10 - 1s - loss: 584.0193 - loglik: -5.6144e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 564.8714 - loglik: -5.6058e+02 - logprior: -4.2916e+00
Epoch 4/10
10/10 - 1s - loss: 557.9018 - loglik: -5.6009e+02 - logprior: 2.1895
Epoch 5/10
10/10 - 1s - loss: 554.5846 - loglik: -5.5988e+02 - logprior: 5.2983
Epoch 6/10
10/10 - 1s - loss: 552.7490 - loglik: -5.5982e+02 - logprior: 7.0708
Epoch 7/10
10/10 - 1s - loss: 551.6107 - loglik: -5.5987e+02 - logprior: 8.2594
Epoch 8/10
10/10 - 1s - loss: 550.8002 - loglik: -5.6001e+02 - logprior: 9.2060
Epoch 9/10
10/10 - 1s - loss: 550.1486 - loglik: -5.6017e+02 - logprior: 10.0210
Epoch 10/10
10/10 - 1s - loss: 549.5818 - loglik: -5.6029e+02 - logprior: 10.7096
Fitted a model with MAP estimate = -549.2907
Time for alignment: 44.9072
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 798.9009 - loglik: -7.1333e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 694.4040 - loglik: -6.7458e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 643.3477 - loglik: -6.3536e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 611.8727 - loglik: -6.0736e+02 - logprior: -4.5112e+00
Epoch 5/10
10/10 - 1s - loss: 598.1113 - loglik: -5.9569e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 592.6217 - loglik: -5.9147e+02 - logprior: -1.1499e+00
Epoch 7/10
10/10 - 1s - loss: 589.7726 - loglik: -5.8954e+02 - logprior: -2.3715e-01
Epoch 8/10
10/10 - 1s - loss: 588.1628 - loglik: -5.8853e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 587.1885 - loglik: -5.8801e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 586.5338 - loglik: -5.8778e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -586.2927
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 678.8517 - loglik: -5.8330e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 603.3854 - loglik: -5.6725e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -590.3447
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 639.2821 - loglik: -5.6423e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 576.2350 - loglik: -5.6021e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -566.5795
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 652.6873 - loglik: -5.6288e+02 - logprior: -8.9809e+01
Epoch 2/10
10/10 - 1s - loss: 584.0193 - loglik: -5.6144e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 564.8714 - loglik: -5.6058e+02 - logprior: -4.2916e+00
Epoch 4/10
10/10 - 1s - loss: 557.9018 - loglik: -5.6009e+02 - logprior: 2.1895
Epoch 5/10
10/10 - 1s - loss: 554.5846 - loglik: -5.5988e+02 - logprior: 5.2983
Epoch 6/10
10/10 - 1s - loss: 552.7491 - loglik: -5.5982e+02 - logprior: 7.0708
Epoch 7/10
10/10 - 1s - loss: 551.6107 - loglik: -5.5987e+02 - logprior: 8.2594
Epoch 8/10
10/10 - 1s - loss: 550.8002 - loglik: -5.6001e+02 - logprior: 9.2060
Epoch 9/10
10/10 - 1s - loss: 550.1486 - loglik: -5.6017e+02 - logprior: 10.0210
Epoch 10/10
10/10 - 1s - loss: 549.5818 - loglik: -5.6029e+02 - logprior: 10.7096
Fitted a model with MAP estimate = -549.2907
Time for alignment: 42.7065
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 798.9009 - loglik: -7.1333e+02 - logprior: -8.5567e+01
Epoch 2/10
10/10 - 1s - loss: 694.4040 - loglik: -6.7458e+02 - logprior: -1.9826e+01
Epoch 3/10
10/10 - 1s - loss: 643.3477 - loglik: -6.3536e+02 - logprior: -7.9897e+00
Epoch 4/10
10/10 - 1s - loss: 611.8727 - loglik: -6.0736e+02 - logprior: -4.5112e+00
Epoch 5/10
10/10 - 1s - loss: 598.1113 - loglik: -5.9569e+02 - logprior: -2.4253e+00
Epoch 6/10
10/10 - 1s - loss: 592.6218 - loglik: -5.9147e+02 - logprior: -1.1499e+00
Epoch 7/10
10/10 - 1s - loss: 589.7726 - loglik: -5.8954e+02 - logprior: -2.3715e-01
Epoch 8/10
10/10 - 1s - loss: 588.1627 - loglik: -5.8853e+02 - logprior: 0.3647
Epoch 9/10
10/10 - 1s - loss: 587.1885 - loglik: -5.8801e+02 - logprior: 0.8230
Epoch 10/10
10/10 - 1s - loss: 586.5339 - loglik: -5.8778e+02 - logprior: 1.2455
Fitted a model with MAP estimate = -586.2927
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 678.8517 - loglik: -5.8330e+02 - logprior: -9.5547e+01
Epoch 2/2
10/10 - 1s - loss: 603.3854 - loglik: -5.6725e+02 - logprior: -3.6138e+01
Fitted a model with MAP estimate = -590.3446
expansions: [(0, 3)]
discards: [  0  35 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 639.2821 - loglik: -5.6423e+02 - logprior: -7.5056e+01
Epoch 2/2
10/10 - 1s - loss: 576.2350 - loglik: -5.6021e+02 - logprior: -1.6026e+01
Fitted a model with MAP estimate = -566.5795
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 652.6873 - loglik: -5.6288e+02 - logprior: -8.9809e+01
Epoch 2/10
10/10 - 1s - loss: 584.0193 - loglik: -5.6144e+02 - logprior: -2.2580e+01
Epoch 3/10
10/10 - 1s - loss: 564.8714 - loglik: -5.6058e+02 - logprior: -4.2915e+00
Epoch 4/10
10/10 - 1s - loss: 557.9018 - loglik: -5.6009e+02 - logprior: 2.1895
Epoch 5/10
10/10 - 1s - loss: 554.5846 - loglik: -5.5988e+02 - logprior: 5.2983
Epoch 6/10
10/10 - 1s - loss: 552.7490 - loglik: -5.5982e+02 - logprior: 7.0708
Epoch 7/10
10/10 - 1s - loss: 551.6107 - loglik: -5.5987e+02 - logprior: 8.2594
Epoch 8/10
10/10 - 1s - loss: 550.8001 - loglik: -5.6001e+02 - logprior: 9.2060
Epoch 9/10
10/10 - 1s - loss: 550.1487 - loglik: -5.6017e+02 - logprior: 10.0210
Epoch 10/10
10/10 - 1s - loss: 549.5818 - loglik: -5.6029e+02 - logprior: 10.7096
Fitted a model with MAP estimate = -549.2907
Time for alignment: 43.1031
Computed alignments with likelihoods: ['-549.2905', '-549.2908', '-549.2907', '-549.2907', '-549.2907']
Best model has likelihood: -549.2905  (prior= 11.0450 )
time for generating output: 0.1394
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.9116417910447762
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 309.3250 - loglik: -2.7765e+02 - logprior: -3.1675e+01
Epoch 2/10
10/10 - 0s - loss: 265.9386 - loglik: -2.5697e+02 - logprior: -8.9665e+00
Epoch 3/10
10/10 - 0s - loss: 240.1046 - loglik: -2.3518e+02 - logprior: -4.9268e+00
Epoch 4/10
10/10 - 0s - loss: 225.0448 - loglik: -2.2110e+02 - logprior: -3.9454e+00
Epoch 5/10
10/10 - 0s - loss: 220.3572 - loglik: -2.1679e+02 - logprior: -3.5639e+00
Epoch 6/10
10/10 - 0s - loss: 218.7262 - loglik: -2.1573e+02 - logprior: -2.9965e+00
Epoch 7/10
10/10 - 0s - loss: 218.0694 - loglik: -2.1549e+02 - logprior: -2.5818e+00
Epoch 8/10
10/10 - 0s - loss: 217.6401 - loglik: -2.1521e+02 - logprior: -2.4255e+00
Epoch 9/10
10/10 - 0s - loss: 217.3722 - loglik: -2.1504e+02 - logprior: -2.3275e+00
Epoch 10/10
10/10 - 0s - loss: 217.1739 - loglik: -2.1496e+02 - logprior: -2.2145e+00
Fitted a model with MAP estimate = -217.1597
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 255.5583 - loglik: -2.1330e+02 - logprior: -4.2258e+01
Epoch 2/2
10/10 - 0s - loss: 219.9338 - loglik: -2.0631e+02 - logprior: -1.3624e+01
Fitted a model with MAP estimate = -212.8651
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.0754 - loglik: -2.0326e+02 - logprior: -2.9812e+01
Epoch 2/2
10/10 - 0s - loss: 211.3114 - loglik: -2.0289e+02 - logprior: -8.4253e+00
Fitted a model with MAP estimate = -208.4309
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8348 - loglik: -2.0262e+02 - logprior: -2.8210e+01
Epoch 2/10
10/10 - 0s - loss: 210.8667 - loglik: -2.0289e+02 - logprior: -7.9783e+00
Epoch 3/10
10/10 - 0s - loss: 207.1262 - loglik: -2.0315e+02 - logprior: -3.9797e+00
Epoch 4/10
10/10 - 0s - loss: 205.9497 - loglik: -2.0349e+02 - logprior: -2.4566e+00
Epoch 5/10
10/10 - 0s - loss: 205.1871 - loglik: -2.0345e+02 - logprior: -1.7329e+00
Epoch 6/10
10/10 - 0s - loss: 204.8474 - loglik: -2.0347e+02 - logprior: -1.3731e+00
Epoch 7/10
10/10 - 0s - loss: 204.6081 - loglik: -2.0348e+02 - logprior: -1.1314e+00
Epoch 8/10
10/10 - 0s - loss: 204.5984 - loglik: -2.0367e+02 - logprior: -9.3098e-01
Epoch 9/10
10/10 - 0s - loss: 204.5107 - loglik: -2.0374e+02 - logprior: -7.6961e-01
Epoch 10/10
10/10 - 0s - loss: 204.4312 - loglik: -2.0376e+02 - logprior: -6.7167e-01
Fitted a model with MAP estimate = -204.3502
Time for alignment: 27.2025
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 309.3785 - loglik: -2.7770e+02 - logprior: -3.1676e+01
Epoch 2/10
10/10 - 0s - loss: 266.0616 - loglik: -2.5709e+02 - logprior: -8.9737e+00
Epoch 3/10
10/10 - 0s - loss: 240.3915 - loglik: -2.3546e+02 - logprior: -4.9346e+00
Epoch 4/10
10/10 - 0s - loss: 225.3575 - loglik: -2.2141e+02 - logprior: -3.9524e+00
Epoch 5/10
10/10 - 0s - loss: 220.6257 - loglik: -2.1705e+02 - logprior: -3.5764e+00
Epoch 6/10
10/10 - 0s - loss: 218.7856 - loglik: -2.1579e+02 - logprior: -2.9998e+00
Epoch 7/10
10/10 - 0s - loss: 218.0391 - loglik: -2.1545e+02 - logprior: -2.5851e+00
Epoch 8/10
10/10 - 0s - loss: 217.8277 - loglik: -2.1540e+02 - logprior: -2.4308e+00
Epoch 9/10
10/10 - 0s - loss: 217.3217 - loglik: -2.1499e+02 - logprior: -2.3292e+00
Epoch 10/10
10/10 - 0s - loss: 217.2229 - loglik: -2.1501e+02 - logprior: -2.2163e+00
Fitted a model with MAP estimate = -217.1679
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.6861 - loglik: -2.1345e+02 - logprior: -4.2238e+01
Epoch 2/2
10/10 - 0s - loss: 219.8125 - loglik: -2.0619e+02 - logprior: -1.3620e+01
Fitted a model with MAP estimate = -212.8751
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 233.0622 - loglik: -2.0325e+02 - logprior: -2.9810e+01
Epoch 2/2
10/10 - 0s - loss: 211.4578 - loglik: -2.0303e+02 - logprior: -8.4250e+00
Fitted a model with MAP estimate = -208.4225
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.9814 - loglik: -2.0277e+02 - logprior: -2.8209e+01
Epoch 2/10
10/10 - 0s - loss: 210.7225 - loglik: -2.0274e+02 - logprior: -7.9829e+00
Epoch 3/10
10/10 - 0s - loss: 207.0219 - loglik: -2.0305e+02 - logprior: -3.9764e+00
Epoch 4/10
10/10 - 0s - loss: 205.9529 - loglik: -2.0350e+02 - logprior: -2.4563e+00
Epoch 5/10
10/10 - 0s - loss: 205.2812 - loglik: -2.0355e+02 - logprior: -1.7332e+00
Epoch 6/10
10/10 - 0s - loss: 204.7893 - loglik: -2.0342e+02 - logprior: -1.3717e+00
Epoch 7/10
10/10 - 0s - loss: 204.7299 - loglik: -2.0359e+02 - logprior: -1.1355e+00
Epoch 8/10
10/10 - 0s - loss: 204.5692 - loglik: -2.0364e+02 - logprior: -9.2751e-01
Epoch 9/10
10/10 - 0s - loss: 204.5246 - loglik: -2.0375e+02 - logprior: -7.7651e-01
Epoch 10/10
10/10 - 0s - loss: 204.3480 - loglik: -2.0367e+02 - logprior: -6.7680e-01
Fitted a model with MAP estimate = -204.3522
Time for alignment: 26.8282
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 309.2655 - loglik: -2.7759e+02 - logprior: -3.1676e+01
Epoch 2/10
10/10 - 0s - loss: 266.2082 - loglik: -2.5724e+02 - logprior: -8.9709e+00
Epoch 3/10
10/10 - 0s - loss: 240.3712 - loglik: -2.3545e+02 - logprior: -4.9206e+00
Epoch 4/10
10/10 - 0s - loss: 225.5983 - loglik: -2.2167e+02 - logprior: -3.9303e+00
Epoch 5/10
10/10 - 0s - loss: 220.5497 - loglik: -2.1698e+02 - logprior: -3.5663e+00
Epoch 6/10
10/10 - 0s - loss: 218.8264 - loglik: -2.1583e+02 - logprior: -2.9975e+00
Epoch 7/10
10/10 - 0s - loss: 218.1425 - loglik: -2.1555e+02 - logprior: -2.5883e+00
Epoch 8/10
10/10 - 0s - loss: 217.8044 - loglik: -2.1537e+02 - logprior: -2.4335e+00
Epoch 9/10
10/10 - 0s - loss: 217.2894 - loglik: -2.1496e+02 - logprior: -2.3247e+00
Epoch 10/10
10/10 - 0s - loss: 217.2107 - loglik: -2.1499e+02 - logprior: -2.2207e+00
Fitted a model with MAP estimate = -217.1701
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 255.6127 - loglik: -2.1336e+02 - logprior: -4.2248e+01
Epoch 2/2
10/10 - 0s - loss: 219.8792 - loglik: -2.0626e+02 - logprior: -1.3621e+01
Fitted a model with MAP estimate = -212.8656
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 233.0385 - loglik: -2.0323e+02 - logprior: -2.9811e+01
Epoch 2/2
10/10 - 0s - loss: 211.3950 - loglik: -2.0297e+02 - logprior: -8.4255e+00
Fitted a model with MAP estimate = -208.4272
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8057 - loglik: -2.0260e+02 - logprior: -2.8204e+01
Epoch 2/10
10/10 - 0s - loss: 210.8804 - loglik: -2.0290e+02 - logprior: -7.9847e+00
Epoch 3/10
10/10 - 0s - loss: 207.1216 - loglik: -2.0315e+02 - logprior: -3.9695e+00
Epoch 4/10
10/10 - 0s - loss: 205.8808 - loglik: -2.0342e+02 - logprior: -2.4576e+00
Epoch 5/10
10/10 - 0s - loss: 205.2290 - loglik: -2.0350e+02 - logprior: -1.7281e+00
Epoch 6/10
10/10 - 0s - loss: 204.8682 - loglik: -2.0350e+02 - logprior: -1.3685e+00
Epoch 7/10
10/10 - 0s - loss: 204.6939 - loglik: -2.0356e+02 - logprior: -1.1325e+00
Epoch 8/10
10/10 - 0s - loss: 204.6216 - loglik: -2.0370e+02 - logprior: -9.2425e-01
Epoch 9/10
10/10 - 0s - loss: 204.4020 - loglik: -2.0363e+02 - logprior: -7.6815e-01
Epoch 10/10
10/10 - 0s - loss: 204.4569 - loglik: -2.0379e+02 - logprior: -6.6835e-01
Fitted a model with MAP estimate = -204.3478
Time for alignment: 24.7227
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 309.4070 - loglik: -2.7773e+02 - logprior: -3.1677e+01
Epoch 2/10
10/10 - 0s - loss: 265.9005 - loglik: -2.5693e+02 - logprior: -8.9734e+00
Epoch 3/10
10/10 - 0s - loss: 240.3803 - loglik: -2.3545e+02 - logprior: -4.9295e+00
Epoch 4/10
10/10 - 0s - loss: 225.2736 - loglik: -2.2134e+02 - logprior: -3.9335e+00
Epoch 5/10
10/10 - 0s - loss: 220.4187 - loglik: -2.1685e+02 - logprior: -3.5656e+00
Epoch 6/10
10/10 - 0s - loss: 218.8619 - loglik: -2.1587e+02 - logprior: -2.9948e+00
Epoch 7/10
10/10 - 0s - loss: 218.1023 - loglik: -2.1552e+02 - logprior: -2.5872e+00
Epoch 8/10
10/10 - 0s - loss: 217.6530 - loglik: -2.1522e+02 - logprior: -2.4308e+00
Epoch 9/10
10/10 - 0s - loss: 217.3820 - loglik: -2.1506e+02 - logprior: -2.3238e+00
Epoch 10/10
10/10 - 0s - loss: 217.2336 - loglik: -2.1502e+02 - logprior: -2.2174e+00
Fitted a model with MAP estimate = -217.1659
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 255.6980 - loglik: -2.1345e+02 - logprior: -4.2248e+01
Epoch 2/2
10/10 - 0s - loss: 219.7681 - loglik: -2.0615e+02 - logprior: -1.3622e+01
Fitted a model with MAP estimate = -212.8629
expansions: []
discards: [32]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 233.1927 - loglik: -2.0338e+02 - logprior: -2.9808e+01
Epoch 2/2
10/10 - 0s - loss: 211.1848 - loglik: -2.0275e+02 - logprior: -8.4314e+00
Fitted a model with MAP estimate = -208.4254
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.7863 - loglik: -2.0258e+02 - logprior: -2.8207e+01
Epoch 2/10
10/10 - 0s - loss: 210.8560 - loglik: -2.0287e+02 - logprior: -7.9883e+00
Epoch 3/10
10/10 - 0s - loss: 207.2124 - loglik: -2.0324e+02 - logprior: -3.9688e+00
Epoch 4/10
10/10 - 0s - loss: 205.9185 - loglik: -2.0346e+02 - logprior: -2.4615e+00
Epoch 5/10
10/10 - 0s - loss: 205.1598 - loglik: -2.0343e+02 - logprior: -1.7293e+00
Epoch 6/10
10/10 - 0s - loss: 204.9034 - loglik: -2.0353e+02 - logprior: -1.3729e+00
Epoch 7/10
10/10 - 0s - loss: 204.6305 - loglik: -2.0350e+02 - logprior: -1.1335e+00
Epoch 8/10
10/10 - 0s - loss: 204.6599 - loglik: -2.0373e+02 - logprior: -9.2767e-01
Fitted a model with MAP estimate = -204.5089
Time for alignment: 24.9470
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 309.4069 - loglik: -2.7773e+02 - logprior: -3.1677e+01
Epoch 2/10
10/10 - 0s - loss: 266.0933 - loglik: -2.5712e+02 - logprior: -8.9711e+00
Epoch 3/10
10/10 - 0s - loss: 240.6153 - loglik: -2.3570e+02 - logprior: -4.9162e+00
Epoch 4/10
10/10 - 0s - loss: 225.7183 - loglik: -2.2180e+02 - logprior: -3.9207e+00
Epoch 5/10
10/10 - 0s - loss: 220.8321 - loglik: -2.1725e+02 - logprior: -3.5790e+00
Epoch 6/10
10/10 - 0s - loss: 218.8679 - loglik: -2.1585e+02 - logprior: -3.0142e+00
Epoch 7/10
10/10 - 0s - loss: 218.2804 - loglik: -2.1569e+02 - logprior: -2.5896e+00
Epoch 8/10
10/10 - 0s - loss: 217.7074 - loglik: -2.1528e+02 - logprior: -2.4314e+00
Epoch 9/10
10/10 - 0s - loss: 217.4563 - loglik: -2.1512e+02 - logprior: -2.3325e+00
Epoch 10/10
10/10 - 0s - loss: 217.3207 - loglik: -2.1510e+02 - logprior: -2.2207e+00
Fitted a model with MAP estimate = -217.1752
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 256.6788 - loglik: -2.1445e+02 - logprior: -4.2232e+01
Epoch 2/2
10/10 - 0s - loss: 220.0983 - loglik: -2.0633e+02 - logprior: -1.3765e+01
Fitted a model with MAP estimate = -213.1574
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 233.2280 - loglik: -2.0340e+02 - logprior: -2.9826e+01
Epoch 2/2
10/10 - 0s - loss: 211.3641 - loglik: -2.0293e+02 - logprior: -8.4307e+00
Fitted a model with MAP estimate = -208.4461
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8201 - loglik: -2.0261e+02 - logprior: -2.8208e+01
Epoch 2/10
10/10 - 0s - loss: 210.8636 - loglik: -2.0288e+02 - logprior: -7.9868e+00
Epoch 3/10
10/10 - 0s - loss: 207.2329 - loglik: -2.0326e+02 - logprior: -3.9724e+00
Epoch 4/10
10/10 - 0s - loss: 205.8271 - loglik: -2.0337e+02 - logprior: -2.4603e+00
Epoch 5/10
10/10 - 0s - loss: 205.2663 - loglik: -2.0353e+02 - logprior: -1.7349e+00
Epoch 6/10
10/10 - 0s - loss: 204.8785 - loglik: -2.0350e+02 - logprior: -1.3738e+00
Epoch 7/10
10/10 - 0s - loss: 204.6534 - loglik: -2.0351e+02 - logprior: -1.1406e+00
Epoch 8/10
10/10 - 0s - loss: 204.5680 - loglik: -2.0364e+02 - logprior: -9.3061e-01
Epoch 9/10
10/10 - 0s - loss: 204.4933 - loglik: -2.0372e+02 - logprior: -7.7704e-01
Epoch 10/10
10/10 - 0s - loss: 204.4727 - loglik: -2.0379e+02 - logprior: -6.7853e-01
Fitted a model with MAP estimate = -204.3578
Time for alignment: 24.5095
Computed alignments with likelihoods: ['-204.3502', '-204.3522', '-204.3478', '-204.5089', '-204.3578']
Best model has likelihood: -204.3478  (prior= -0.6371 )
time for generating output: 0.0955
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 908.6230 - loglik: -9.0330e+02 - logprior: -5.3220e+00
Epoch 2/10
15/15 - 3s - loss: 830.8721 - loglik: -8.2959e+02 - logprior: -1.2784e+00
Epoch 3/10
15/15 - 3s - loss: 782.3555 - loglik: -7.8080e+02 - logprior: -1.5537e+00
Epoch 4/10
15/15 - 3s - loss: 770.9872 - loglik: -7.6934e+02 - logprior: -1.6509e+00
Epoch 5/10
15/15 - 3s - loss: 766.8821 - loglik: -7.6541e+02 - logprior: -1.4731e+00
Epoch 6/10
15/15 - 3s - loss: 767.3537 - loglik: -7.6592e+02 - logprior: -1.4364e+00
Fitted a model with MAP estimate = -765.4357
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (60, 2), (66, 2), (69, 1), (91, 1), (92, 1), (112, 1), (114, 4), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 767.0203 - loglik: -7.6044e+02 - logprior: -6.5804e+00
Epoch 2/2
15/15 - 4s - loss: 752.2262 - loglik: -7.4925e+02 - logprior: -2.9803e+00
Fitted a model with MAP estimate = -750.7515
expansions: [(0, 2)]
discards: [0 7]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 754.7298 - loglik: -7.4977e+02 - logprior: -4.9596e+00
Epoch 2/2
15/15 - 4s - loss: 747.2697 - loglik: -7.4582e+02 - logprior: -1.4462e+00
Fitted a model with MAP estimate = -747.1339
expansions: []
discards: [ 0 81]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 756.3428 - loglik: -7.4989e+02 - logprior: -6.4491e+00
Epoch 2/10
15/15 - 4s - loss: 748.0866 - loglik: -7.4609e+02 - logprior: -1.9946e+00
Epoch 3/10
15/15 - 4s - loss: 747.0068 - loglik: -7.4605e+02 - logprior: -9.5441e-01
Epoch 4/10
15/15 - 4s - loss: 744.4940 - loglik: -7.4370e+02 - logprior: -7.9426e-01
Epoch 5/10
15/15 - 4s - loss: 744.3973 - loglik: -7.4365e+02 - logprior: -7.5219e-01
Epoch 6/10
15/15 - 4s - loss: 743.9114 - loglik: -7.4320e+02 - logprior: -7.0795e-01
Epoch 7/10
15/15 - 4s - loss: 743.3851 - loglik: -7.4272e+02 - logprior: -6.6728e-01
Epoch 8/10
15/15 - 4s - loss: 742.7353 - loglik: -7.4211e+02 - logprior: -6.2782e-01
Epoch 9/10
15/15 - 4s - loss: 743.7073 - loglik: -7.4314e+02 - logprior: -5.6312e-01
Fitted a model with MAP estimate = -742.7869
Time for alignment: 109.3138
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 909.8093 - loglik: -9.0449e+02 - logprior: -5.3229e+00
Epoch 2/10
15/15 - 3s - loss: 831.0078 - loglik: -8.2974e+02 - logprior: -1.2650e+00
Epoch 3/10
15/15 - 3s - loss: 784.7389 - loglik: -7.8324e+02 - logprior: -1.5012e+00
Epoch 4/10
15/15 - 3s - loss: 771.0397 - loglik: -7.6944e+02 - logprior: -1.6031e+00
Epoch 5/10
15/15 - 3s - loss: 766.3290 - loglik: -7.6484e+02 - logprior: -1.4885e+00
Epoch 6/10
15/15 - 3s - loss: 765.5034 - loglik: -7.6398e+02 - logprior: -1.5248e+00
Epoch 7/10
15/15 - 3s - loss: 762.7443 - loglik: -7.6121e+02 - logprior: -1.5380e+00
Epoch 8/10
15/15 - 3s - loss: 763.3513 - loglik: -7.6185e+02 - logprior: -1.5039e+00
Fitted a model with MAP estimate = -762.4688
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (24, 2), (26, 1), (53, 1), (55, 1), (59, 1), (65, 2), (69, 1), (91, 1), (92, 2), (110, 1), (112, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 767.2538 - loglik: -7.6068e+02 - logprior: -6.5691e+00
Epoch 2/2
15/15 - 4s - loss: 750.7459 - loglik: -7.4770e+02 - logprior: -3.0431e+00
Fitted a model with MAP estimate = -749.8312
expansions: [(0, 2)]
discards: [  0   7  81 106]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 753.4950 - loglik: -7.4856e+02 - logprior: -4.9335e+00
Epoch 2/2
15/15 - 4s - loss: 749.3547 - loglik: -7.4796e+02 - logprior: -1.3967e+00
Fitted a model with MAP estimate = -747.4673
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 756.3284 - loglik: -7.4992e+02 - logprior: -6.4092e+00
Epoch 2/10
15/15 - 4s - loss: 749.4176 - loglik: -7.4742e+02 - logprior: -1.9998e+00
Epoch 3/10
15/15 - 4s - loss: 748.6119 - loglik: -7.4768e+02 - logprior: -9.3595e-01
Epoch 4/10
15/15 - 4s - loss: 746.2410 - loglik: -7.4546e+02 - logprior: -7.7720e-01
Epoch 5/10
15/15 - 4s - loss: 745.4464 - loglik: -7.4472e+02 - logprior: -7.2512e-01
Epoch 6/10
15/15 - 4s - loss: 746.3054 - loglik: -7.4562e+02 - logprior: -6.8750e-01
Fitted a model with MAP estimate = -744.8491
Time for alignment: 101.7987
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 910.4780 - loglik: -9.0516e+02 - logprior: -5.3196e+00
Epoch 2/10
15/15 - 3s - loss: 830.3005 - loglik: -8.2904e+02 - logprior: -1.2627e+00
Epoch 3/10
15/15 - 3s - loss: 787.5217 - loglik: -7.8605e+02 - logprior: -1.4705e+00
Epoch 4/10
15/15 - 3s - loss: 773.9702 - loglik: -7.7235e+02 - logprior: -1.6239e+00
Epoch 5/10
15/15 - 3s - loss: 769.8867 - loglik: -7.6842e+02 - logprior: -1.4642e+00
Epoch 6/10
15/15 - 3s - loss: 767.1266 - loglik: -7.6566e+02 - logprior: -1.4690e+00
Epoch 7/10
15/15 - 3s - loss: 764.5297 - loglik: -7.6306e+02 - logprior: -1.4725e+00
Epoch 8/10
15/15 - 3s - loss: 765.0323 - loglik: -7.6357e+02 - logprior: -1.4637e+00
Fitted a model with MAP estimate = -763.7496
expansions: [(6, 1), (7, 2), (10, 1), (14, 1), (24, 2), (26, 1), (49, 2), (60, 1), (65, 2), (66, 2), (69, 1), (91, 2), (92, 2), (106, 2), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 6288 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 8s - loss: 760.3304 - loglik: -7.5550e+02 - logprior: -4.8325e+00
Epoch 2/2
30/30 - 5s - loss: 747.2684 - loglik: -7.4558e+02 - logprior: -1.6873e+00
Fitted a model with MAP estimate = -745.3768
expansions: [(0, 1)]
discards: [  0   5  57  77 109 125 137 142]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 754.0139 - loglik: -7.4913e+02 - logprior: -4.8884e+00
Epoch 2/2
15/15 - 4s - loss: 748.7111 - loglik: -7.4751e+02 - logprior: -1.1986e+00
Fitted a model with MAP estimate = -746.5286
expansions: []
discards: [81]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 752.0754 - loglik: -7.4732e+02 - logprior: -4.7538e+00
Epoch 2/10
15/15 - 4s - loss: 749.1465 - loglik: -7.4794e+02 - logprior: -1.2042e+00
Epoch 3/10
15/15 - 4s - loss: 746.0521 - loglik: -7.4513e+02 - logprior: -9.2084e-01
Epoch 4/10
15/15 - 4s - loss: 745.2365 - loglik: -7.4452e+02 - logprior: -7.2023e-01
Epoch 5/10
15/15 - 4s - loss: 744.7255 - loglik: -7.4404e+02 - logprior: -6.8904e-01
Epoch 6/10
15/15 - 4s - loss: 744.1034 - loglik: -7.4345e+02 - logprior: -6.5795e-01
Epoch 7/10
15/15 - 4s - loss: 744.7179 - loglik: -7.4411e+02 - logprior: -6.0778e-01
Fitted a model with MAP estimate = -743.6530
Time for alignment: 110.5732
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 910.0599 - loglik: -9.0475e+02 - logprior: -5.3111e+00
Epoch 2/10
15/15 - 3s - loss: 832.2185 - loglik: -8.3096e+02 - logprior: -1.2625e+00
Epoch 3/10
15/15 - 3s - loss: 781.5930 - loglik: -7.8006e+02 - logprior: -1.5305e+00
Epoch 4/10
15/15 - 3s - loss: 770.9185 - loglik: -7.6920e+02 - logprior: -1.7207e+00
Epoch 5/10
15/15 - 3s - loss: 766.6864 - loglik: -7.6510e+02 - logprior: -1.5842e+00
Epoch 6/10
15/15 - 3s - loss: 764.6589 - loglik: -7.6306e+02 - logprior: -1.5995e+00
Epoch 7/10
15/15 - 3s - loss: 764.8504 - loglik: -7.6324e+02 - logprior: -1.6116e+00
Fitted a model with MAP estimate = -763.4413
expansions: [(5, 1), (7, 2), (10, 1), (16, 1), (24, 3), (25, 1), (49, 2), (60, 1), (66, 3), (69, 2), (91, 2), (92, 1), (108, 1), (112, 1), (114, 1), (115, 1), (117, 3), (118, 1), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 768.1537 - loglik: -7.6158e+02 - logprior: -6.5750e+00
Epoch 2/2
15/15 - 4s - loss: 751.5964 - loglik: -7.4852e+02 - logprior: -3.0730e+00
Fitted a model with MAP estimate = -749.7160
expansions: [(0, 2)]
discards: [  0   7  29  58  79 108]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 753.5511 - loglik: -7.4859e+02 - logprior: -4.9625e+00
Epoch 2/2
15/15 - 4s - loss: 749.4037 - loglik: -7.4796e+02 - logprior: -1.4430e+00
Fitted a model with MAP estimate = -747.0837
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 755.9056 - loglik: -7.4946e+02 - logprior: -6.4420e+00
Epoch 2/10
15/15 - 4s - loss: 749.5136 - loglik: -7.4749e+02 - logprior: -2.0237e+00
Epoch 3/10
15/15 - 4s - loss: 746.7258 - loglik: -7.4576e+02 - logprior: -9.7017e-01
Epoch 4/10
15/15 - 4s - loss: 746.3907 - loglik: -7.4561e+02 - logprior: -7.8501e-01
Epoch 5/10
15/15 - 4s - loss: 745.2125 - loglik: -7.4448e+02 - logprior: -7.3688e-01
Epoch 6/10
15/15 - 4s - loss: 745.2228 - loglik: -7.4450e+02 - logprior: -7.1946e-01
Fitted a model with MAP estimate = -744.2570
Time for alignment: 100.5972
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 909.6497 - loglik: -9.0433e+02 - logprior: -5.3204e+00
Epoch 2/10
15/15 - 3s - loss: 833.1088 - loglik: -8.3183e+02 - logprior: -1.2773e+00
Epoch 3/10
15/15 - 3s - loss: 784.1371 - loglik: -7.8261e+02 - logprior: -1.5232e+00
Epoch 4/10
15/15 - 3s - loss: 770.2562 - loglik: -7.6855e+02 - logprior: -1.7057e+00
Epoch 5/10
15/15 - 3s - loss: 766.3701 - loglik: -7.6482e+02 - logprior: -1.5459e+00
Epoch 6/10
15/15 - 3s - loss: 765.1996 - loglik: -7.6368e+02 - logprior: -1.5232e+00
Epoch 7/10
15/15 - 3s - loss: 764.6589 - loglik: -7.6316e+02 - logprior: -1.4948e+00
Epoch 8/10
15/15 - 3s - loss: 763.6276 - loglik: -7.6216e+02 - logprior: -1.4721e+00
Epoch 9/10
15/15 - 3s - loss: 762.9867 - loglik: -7.6149e+02 - logprior: -1.4983e+00
Epoch 10/10
15/15 - 3s - loss: 762.5484 - loglik: -7.6103e+02 - logprior: -1.5186e+00
Fitted a model with MAP estimate = -762.0287
expansions: [(6, 1), (7, 2), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (55, 1), (59, 1), (64, 2), (65, 2), (91, 1), (92, 2), (106, 2), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 767.5689 - loglik: -7.6096e+02 - logprior: -6.6125e+00
Epoch 2/2
15/15 - 4s - loss: 751.3292 - loglik: -7.4821e+02 - logprior: -3.1240e+00
Fitted a model with MAP estimate = -749.4570
expansions: [(0, 2)]
discards: [  0   5  65  76 107 123 135]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 755.6508 - loglik: -7.5077e+02 - logprior: -4.8759e+00
Epoch 2/2
15/15 - 4s - loss: 748.0603 - loglik: -7.4662e+02 - logprior: -1.4385e+00
Fitted a model with MAP estimate = -747.4150
expansions: [(65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 757.0487 - loglik: -7.5065e+02 - logprior: -6.4016e+00
Epoch 2/10
15/15 - 4s - loss: 748.7333 - loglik: -7.4672e+02 - logprior: -2.0157e+00
Epoch 3/10
15/15 - 4s - loss: 746.2950 - loglik: -7.4536e+02 - logprior: -9.3681e-01
Epoch 4/10
15/15 - 4s - loss: 744.5718 - loglik: -7.4379e+02 - logprior: -7.8086e-01
Epoch 5/10
15/15 - 4s - loss: 743.7888 - loglik: -7.4305e+02 - logprior: -7.4209e-01
Epoch 6/10
15/15 - 4s - loss: 744.9599 - loglik: -7.4425e+02 - logprior: -7.0840e-01
Fitted a model with MAP estimate = -743.3642
Time for alignment: 109.5350
Computed alignments with likelihoods: ['-742.7869', '-744.8491', '-743.6530', '-744.2570', '-743.3642']
Best model has likelihood: -742.7869  (prior= -0.5148 )
time for generating output: 0.1684
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9247491638795987
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 672.9739 - loglik: -6.7010e+02 - logprior: -2.8762e+00
Epoch 2/10
34/34 - 5s - loss: 583.0240 - loglik: -5.8111e+02 - logprior: -1.9105e+00
Epoch 3/10
34/34 - 4s - loss: 574.2751 - loglik: -5.7243e+02 - logprior: -1.8481e+00
Epoch 4/10
34/34 - 4s - loss: 573.8375 - loglik: -5.7204e+02 - logprior: -1.8024e+00
Epoch 5/10
34/34 - 4s - loss: 573.9393 - loglik: -5.7213e+02 - logprior: -1.8070e+00
Fitted a model with MAP estimate = -573.0181
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (41, 1), (42, 1), (45, 1), (46, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (70, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 560.4832 - loglik: -5.5749e+02 - logprior: -2.9966e+00
Epoch 2/2
34/34 - 5s - loss: 550.7574 - loglik: -5.4946e+02 - logprior: -1.2953e+00
Fitted a model with MAP estimate = -549.3348
expansions: []
discards: [ 20 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 551.9755 - loglik: -5.4933e+02 - logprior: -2.6461e+00
Epoch 2/2
34/34 - 5s - loss: 549.4500 - loglik: -5.4836e+02 - logprior: -1.0919e+00
Fitted a model with MAP estimate = -549.0051
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 552.6871 - loglik: -5.5018e+02 - logprior: -2.5099e+00
Epoch 2/10
34/34 - 5s - loss: 548.3513 - loglik: -5.4740e+02 - logprior: -9.4947e-01
Epoch 3/10
34/34 - 5s - loss: 547.6271 - loglik: -5.4679e+02 - logprior: -8.3827e-01
Epoch 4/10
34/34 - 5s - loss: 549.9422 - loglik: -5.4921e+02 - logprior: -7.3604e-01
Fitted a model with MAP estimate = -547.8705
Time for alignment: 100.9749
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 672.1005 - loglik: -6.6923e+02 - logprior: -2.8749e+00
Epoch 2/10
34/34 - 4s - loss: 585.0549 - loglik: -5.8316e+02 - logprior: -1.8934e+00
Epoch 3/10
34/34 - 4s - loss: 576.4998 - loglik: -5.7463e+02 - logprior: -1.8695e+00
Epoch 4/10
34/34 - 4s - loss: 573.2105 - loglik: -5.7138e+02 - logprior: -1.8335e+00
Epoch 5/10
34/34 - 5s - loss: 574.2933 - loglik: -5.7247e+02 - logprior: -1.8187e+00
Fitted a model with MAP estimate = -573.2836
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (42, 1), (43, 1), (45, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 560.9491 - loglik: -5.5794e+02 - logprior: -3.0117e+00
Epoch 2/2
34/34 - 5s - loss: 550.7223 - loglik: -5.4946e+02 - logprior: -1.2590e+00
Fitted a model with MAP estimate = -549.2904
expansions: []
discards: [ 20 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 552.2564 - loglik: -5.4961e+02 - logprior: -2.6503e+00
Epoch 2/2
34/34 - 5s - loss: 549.4009 - loglik: -5.4830e+02 - logprior: -1.1017e+00
Fitted a model with MAP estimate = -549.0060
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 552.7256 - loglik: -5.5020e+02 - logprior: -2.5256e+00
Epoch 2/10
34/34 - 5s - loss: 549.5359 - loglik: -5.4859e+02 - logprior: -9.4876e-01
Epoch 3/10
34/34 - 5s - loss: 546.8716 - loglik: -5.4603e+02 - logprior: -8.3828e-01
Epoch 4/10
34/34 - 5s - loss: 549.5955 - loglik: -5.4884e+02 - logprior: -7.5192e-01
Fitted a model with MAP estimate = -547.8295
Time for alignment: 99.6753
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 672.4684 - loglik: -6.6961e+02 - logprior: -2.8578e+00
Epoch 2/10
34/34 - 5s - loss: 583.9057 - loglik: -5.8204e+02 - logprior: -1.8621e+00
Epoch 3/10
34/34 - 5s - loss: 577.8521 - loglik: -5.7603e+02 - logprior: -1.8242e+00
Epoch 4/10
34/34 - 5s - loss: 573.4030 - loglik: -5.7161e+02 - logprior: -1.7944e+00
Epoch 5/10
34/34 - 4s - loss: 572.2817 - loglik: -5.7048e+02 - logprior: -1.7986e+00
Epoch 6/10
34/34 - 4s - loss: 572.8650 - loglik: -5.7108e+02 - logprior: -1.7854e+00
Fitted a model with MAP estimate = -572.6583
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 561.0224 - loglik: -5.5801e+02 - logprior: -3.0116e+00
Epoch 2/2
34/34 - 5s - loss: 551.5893 - loglik: -5.5031e+02 - logprior: -1.2794e+00
Fitted a model with MAP estimate = -550.1458
expansions: []
discards: [ 20  35 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 554.5212 - loglik: -5.5189e+02 - logprior: -2.6310e+00
Epoch 2/2
34/34 - 5s - loss: 550.4257 - loglik: -5.4934e+02 - logprior: -1.0886e+00
Fitted a model with MAP estimate = -550.3571
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 553.9055 - loglik: -5.5139e+02 - logprior: -2.5119e+00
Epoch 2/10
34/34 - 5s - loss: 551.9478 - loglik: -5.5100e+02 - logprior: -9.4325e-01
Epoch 3/10
34/34 - 5s - loss: 547.9380 - loglik: -5.4711e+02 - logprior: -8.2540e-01
Epoch 4/10
34/34 - 5s - loss: 551.8966 - loglik: -5.5116e+02 - logprior: -7.3924e-01
Fitted a model with MAP estimate = -549.2038
Time for alignment: 105.4902
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 672.8691 - loglik: -6.7001e+02 - logprior: -2.8553e+00
Epoch 2/10
34/34 - 5s - loss: 583.4390 - loglik: -5.8159e+02 - logprior: -1.8455e+00
Epoch 3/10
34/34 - 5s - loss: 573.7490 - loglik: -5.7195e+02 - logprior: -1.7996e+00
Epoch 4/10
34/34 - 4s - loss: 572.2474 - loglik: -5.7049e+02 - logprior: -1.7557e+00
Epoch 5/10
34/34 - 5s - loss: 573.1944 - loglik: -5.7145e+02 - logprior: -1.7433e+00
Fitted a model with MAP estimate = -572.4309
expansions: [(0, 2), (15, 1), (16, 4), (26, 1), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (77, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 561.5905 - loglik: -5.5863e+02 - logprior: -2.9614e+00
Epoch 2/2
34/34 - 5s - loss: 549.9066 - loglik: -5.4869e+02 - logprior: -1.2212e+00
Fitted a model with MAP estimate = -549.5913
expansions: []
discards: [138]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 552.6101 - loglik: -5.4997e+02 - logprior: -2.6367e+00
Epoch 2/2
34/34 - 5s - loss: 550.6412 - loglik: -5.4955e+02 - logprior: -1.0933e+00
Fitted a model with MAP estimate = -549.3202
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 552.2495 - loglik: -5.4972e+02 - logprior: -2.5259e+00
Epoch 2/10
34/34 - 5s - loss: 550.3918 - loglik: -5.4945e+02 - logprior: -9.4661e-01
Epoch 3/10
34/34 - 5s - loss: 548.0121 - loglik: -5.4718e+02 - logprior: -8.3256e-01
Epoch 4/10
34/34 - 5s - loss: 550.7105 - loglik: -5.4997e+02 - logprior: -7.4511e-01
Fitted a model with MAP estimate = -548.2273
Time for alignment: 99.9247
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 671.8050 - loglik: -6.6895e+02 - logprior: -2.8525e+00
Epoch 2/10
34/34 - 4s - loss: 585.9303 - loglik: -5.8409e+02 - logprior: -1.8383e+00
Epoch 3/10
34/34 - 4s - loss: 574.7869 - loglik: -5.7297e+02 - logprior: -1.8200e+00
Epoch 4/10
34/34 - 4s - loss: 572.8497 - loglik: -5.7106e+02 - logprior: -1.7875e+00
Epoch 5/10
34/34 - 5s - loss: 574.5539 - loglik: -5.7279e+02 - logprior: -1.7678e+00
Fitted a model with MAP estimate = -573.2001
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (26, 2), (27, 1), (28, 1), (40, 1), (41, 1), (42, 1), (45, 1), (47, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (81, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 560.5481 - loglik: -5.5758e+02 - logprior: -2.9714e+00
Epoch 2/2
34/34 - 5s - loss: 548.7181 - loglik: -5.4750e+02 - logprior: -1.2205e+00
Fitted a model with MAP estimate = -548.8941
expansions: [(124, 1)]
discards: [ 33 139]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 552.9689 - loglik: -5.5034e+02 - logprior: -2.6293e+00
Epoch 2/2
34/34 - 5s - loss: 550.0565 - loglik: -5.4896e+02 - logprior: -1.0918e+00
Fitted a model with MAP estimate = -548.8364
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 553.3224 - loglik: -5.5080e+02 - logprior: -2.5220e+00
Epoch 2/10
34/34 - 5s - loss: 548.4651 - loglik: -5.4751e+02 - logprior: -9.5059e-01
Epoch 3/10
34/34 - 5s - loss: 548.1520 - loglik: -5.4732e+02 - logprior: -8.3383e-01
Epoch 4/10
34/34 - 5s - loss: 548.3209 - loglik: -5.4758e+02 - logprior: -7.4171e-01
Fitted a model with MAP estimate = -547.6697
Time for alignment: 100.0032
Computed alignments with likelihoods: ['-547.8705', '-547.8295', '-549.2038', '-548.2273', '-547.6697']
Best model has likelihood: -547.6697  (prior= -0.7125 )
time for generating output: 0.3035
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6297376093294461
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 830.9517 - loglik: -8.1025e+02 - logprior: -2.0703e+01
Epoch 2/10
10/10 - 2s - loss: 776.4844 - loglik: -7.7174e+02 - logprior: -4.7421e+00
Epoch 3/10
10/10 - 2s - loss: 734.3726 - loglik: -7.3186e+02 - logprior: -2.5079e+00
Epoch 4/10
10/10 - 2s - loss: 704.4856 - loglik: -7.0238e+02 - logprior: -2.1097e+00
Epoch 5/10
10/10 - 2s - loss: 691.7369 - loglik: -6.8961e+02 - logprior: -2.1289e+00
Epoch 6/10
10/10 - 2s - loss: 687.1894 - loglik: -6.8506e+02 - logprior: -2.1267e+00
Epoch 7/10
10/10 - 2s - loss: 685.1071 - loglik: -6.8309e+02 - logprior: -2.0161e+00
Epoch 8/10
10/10 - 2s - loss: 682.7134 - loglik: -6.8087e+02 - logprior: -1.8444e+00
Epoch 9/10
10/10 - 2s - loss: 682.6506 - loglik: -6.8092e+02 - logprior: -1.7337e+00
Epoch 10/10
10/10 - 2s - loss: 683.4138 - loglik: -6.8166e+02 - logprior: -1.7519e+00
Fitted a model with MAP estimate = -682.2582
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (29, 2), (53, 1), (55, 2), (62, 1), (79, 2), (80, 2), (81, 1), (82, 2), (85, 1), (98, 1), (99, 1), (101, 4), (102, 1), (103, 1), (111, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 700.2073 - loglik: -6.8137e+02 - logprior: -1.8842e+01
Epoch 2/2
10/10 - 2s - loss: 672.9174 - loglik: -6.6830e+02 - logprior: -4.6167e+00
Fitted a model with MAP estimate = -668.2817
expansions: []
discards: [  0   7   8  39  68  96  98 129 130]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 695.8937 - loglik: -6.7257e+02 - logprior: -2.3321e+01
Epoch 2/2
10/10 - 2s - loss: 677.4347 - loglik: -6.6829e+02 - logprior: -9.1437e+00
Fitted a model with MAP estimate = -675.0089
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 688.0921 - loglik: -6.6966e+02 - logprior: -1.8432e+01
Epoch 2/10
10/10 - 2s - loss: 669.7284 - loglik: -6.6574e+02 - logprior: -3.9929e+00
Epoch 3/10
10/10 - 2s - loss: 667.5483 - loglik: -6.6638e+02 - logprior: -1.1701e+00
Epoch 4/10
10/10 - 2s - loss: 664.6418 - loglik: -6.6449e+02 - logprior: -1.4917e-01
Epoch 5/10
10/10 - 2s - loss: 664.1955 - loglik: -6.6449e+02 - logprior: 0.2937
Epoch 6/10
10/10 - 2s - loss: 663.4030 - loglik: -6.6397e+02 - logprior: 0.5692
Epoch 7/10
10/10 - 2s - loss: 662.2977 - loglik: -6.6311e+02 - logprior: 0.8081
Epoch 8/10
10/10 - 2s - loss: 662.7034 - loglik: -6.6368e+02 - logprior: 0.9788
Fitted a model with MAP estimate = -662.5053
Time for alignment: 59.4192
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 831.3708 - loglik: -8.1067e+02 - logprior: -2.0704e+01
Epoch 2/10
10/10 - 2s - loss: 776.0571 - loglik: -7.7131e+02 - logprior: -4.7472e+00
Epoch 3/10
10/10 - 2s - loss: 733.5841 - loglik: -7.3100e+02 - logprior: -2.5814e+00
Epoch 4/10
10/10 - 2s - loss: 702.8099 - loglik: -7.0060e+02 - logprior: -2.2123e+00
Epoch 5/10
10/10 - 2s - loss: 690.9861 - loglik: -6.8880e+02 - logprior: -2.1842e+00
Epoch 6/10
10/10 - 2s - loss: 685.4907 - loglik: -6.8344e+02 - logprior: -2.0478e+00
Epoch 7/10
10/10 - 2s - loss: 684.0153 - loglik: -6.8206e+02 - logprior: -1.9589e+00
Epoch 8/10
10/10 - 2s - loss: 684.0106 - loglik: -6.8213e+02 - logprior: -1.8820e+00
Epoch 9/10
10/10 - 2s - loss: 682.5292 - loglik: -6.8070e+02 - logprior: -1.8313e+00
Epoch 10/10
10/10 - 2s - loss: 681.5627 - loglik: -6.7978e+02 - logprior: -1.7845e+00
Fitted a model with MAP estimate = -682.0958
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (45, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (86, 2), (98, 1), (101, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 696.2963 - loglik: -6.7746e+02 - logprior: -1.8834e+01
Epoch 2/2
10/10 - 2s - loss: 676.1912 - loglik: -6.7174e+02 - logprior: -4.4496e+00
Fitted a model with MAP estimate = -670.3641
expansions: []
discards: [  0 103]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 695.0475 - loglik: -6.7171e+02 - logprior: -2.3339e+01
Epoch 2/2
10/10 - 2s - loss: 677.3348 - loglik: -6.6821e+02 - logprior: -9.1236e+00
Fitted a model with MAP estimate = -674.8388
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 687.1719 - loglik: -6.6879e+02 - logprior: -1.8378e+01
Epoch 2/10
10/10 - 2s - loss: 671.0654 - loglik: -6.6705e+02 - logprior: -4.0137e+00
Epoch 3/10
10/10 - 2s - loss: 666.2836 - loglik: -6.6511e+02 - logprior: -1.1706e+00
Epoch 4/10
10/10 - 2s - loss: 666.0677 - loglik: -6.6589e+02 - logprior: -1.7644e-01
Epoch 5/10
10/10 - 2s - loss: 663.8516 - loglik: -6.6413e+02 - logprior: 0.2770
Epoch 6/10
10/10 - 2s - loss: 663.4281 - loglik: -6.6398e+02 - logprior: 0.5469
Epoch 7/10
10/10 - 2s - loss: 662.4469 - loglik: -6.6324e+02 - logprior: 0.7917
Epoch 8/10
10/10 - 2s - loss: 663.7477 - loglik: -6.6470e+02 - logprior: 0.9541
Fitted a model with MAP estimate = -662.6987
Time for alignment: 58.2619
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 831.3101 - loglik: -8.1061e+02 - logprior: -2.0702e+01
Epoch 2/10
10/10 - 2s - loss: 774.6741 - loglik: -7.6995e+02 - logprior: -4.7251e+00
Epoch 3/10
10/10 - 2s - loss: 729.9493 - loglik: -7.2749e+02 - logprior: -2.4544e+00
Epoch 4/10
10/10 - 2s - loss: 703.9282 - loglik: -7.0189e+02 - logprior: -2.0369e+00
Epoch 5/10
10/10 - 2s - loss: 695.1693 - loglik: -6.9323e+02 - logprior: -1.9357e+00
Epoch 6/10
10/10 - 2s - loss: 690.7941 - loglik: -6.8906e+02 - logprior: -1.7315e+00
Epoch 7/10
10/10 - 2s - loss: 688.6373 - loglik: -6.8707e+02 - logprior: -1.5634e+00
Epoch 8/10
10/10 - 2s - loss: 688.7175 - loglik: -6.8723e+02 - logprior: -1.4839e+00
Fitted a model with MAP estimate = -687.9733
expansions: [(6, 2), (7, 1), (8, 1), (9, 1), (16, 1), (23, 2), (24, 3), (26, 1), (53, 1), (55, 2), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (98, 1), (101, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 703.3031 - loglik: -6.8451e+02 - logprior: -1.8795e+01
Epoch 2/2
10/10 - 2s - loss: 675.6825 - loglik: -6.7108e+02 - logprior: -4.5983e+00
Fitted a model with MAP estimate = -670.1111
expansions: []
discards: [  0   6  11  30  68 127 128]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 696.0668 - loglik: -6.7268e+02 - logprior: -2.3384e+01
Epoch 2/2
10/10 - 2s - loss: 679.5178 - loglik: -6.7027e+02 - logprior: -9.2458e+00
Fitted a model with MAP estimate = -675.8964
expansions: [(0, 5)]
discards: [ 0 18]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 688.5149 - loglik: -6.7000e+02 - logprior: -1.8510e+01
Epoch 2/10
10/10 - 2s - loss: 672.0002 - loglik: -6.6783e+02 - logprior: -4.1729e+00
Epoch 3/10
10/10 - 2s - loss: 668.1619 - loglik: -6.6685e+02 - logprior: -1.3114e+00
Epoch 4/10
10/10 - 2s - loss: 666.0414 - loglik: -6.6573e+02 - logprior: -3.1240e-01
Epoch 5/10
10/10 - 2s - loss: 665.1940 - loglik: -6.6531e+02 - logprior: 0.1150
Epoch 6/10
10/10 - 2s - loss: 663.2954 - loglik: -6.6373e+02 - logprior: 0.4378
Epoch 7/10
10/10 - 2s - loss: 663.7576 - loglik: -6.6446e+02 - logprior: 0.6996
Fitted a model with MAP estimate = -663.5329
Time for alignment: 54.5742
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 831.8275 - loglik: -8.1112e+02 - logprior: -2.0703e+01
Epoch 2/10
10/10 - 2s - loss: 775.8054 - loglik: -7.7107e+02 - logprior: -4.7373e+00
Epoch 3/10
10/10 - 2s - loss: 732.3896 - loglik: -7.2990e+02 - logprior: -2.4856e+00
Epoch 4/10
10/10 - 2s - loss: 705.0106 - loglik: -7.0303e+02 - logprior: -1.9831e+00
Epoch 5/10
10/10 - 2s - loss: 692.9612 - loglik: -6.9112e+02 - logprior: -1.8382e+00
Epoch 6/10
10/10 - 2s - loss: 691.3262 - loglik: -6.8965e+02 - logprior: -1.6776e+00
Epoch 7/10
10/10 - 2s - loss: 689.5931 - loglik: -6.8801e+02 - logprior: -1.5787e+00
Epoch 8/10
10/10 - 2s - loss: 688.0923 - loglik: -6.8655e+02 - logprior: -1.5447e+00
Epoch 9/10
10/10 - 2s - loss: 687.7817 - loglik: -6.8631e+02 - logprior: -1.4690e+00
Epoch 10/10
10/10 - 2s - loss: 687.1450 - loglik: -6.8571e+02 - logprior: -1.4389e+00
Fitted a model with MAP estimate = -687.4063
expansions: [(8, 1), (9, 1), (14, 1), (17, 1), (24, 4), (45, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 701.0872 - loglik: -6.8221e+02 - logprior: -1.8874e+01
Epoch 2/2
10/10 - 2s - loss: 676.1144 - loglik: -6.7159e+02 - logprior: -4.5273e+00
Fitted a model with MAP estimate = -671.0485
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.7084 - loglik: -6.7125e+02 - logprior: -2.3456e+01
Epoch 2/2
10/10 - 2s - loss: 676.8752 - loglik: -6.6765e+02 - logprior: -9.2258e+00
Fitted a model with MAP estimate = -673.9719
expansions: [(0, 5), (116, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 686.1556 - loglik: -6.6762e+02 - logprior: -1.8533e+01
Epoch 2/10
10/10 - 2s - loss: 666.7941 - loglik: -6.6262e+02 - logprior: -4.1738e+00
Epoch 3/10
10/10 - 2s - loss: 663.0417 - loglik: -6.6169e+02 - logprior: -1.3524e+00
Epoch 4/10
10/10 - 2s - loss: 662.9120 - loglik: -6.6257e+02 - logprior: -3.4177e-01
Epoch 5/10
10/10 - 2s - loss: 660.7670 - loglik: -6.6089e+02 - logprior: 0.1183
Epoch 6/10
10/10 - 2s - loss: 659.1055 - loglik: -6.5950e+02 - logprior: 0.3916
Epoch 7/10
10/10 - 2s - loss: 659.0522 - loglik: -6.5969e+02 - logprior: 0.6377
Epoch 8/10
10/10 - 2s - loss: 659.6680 - loglik: -6.6048e+02 - logprior: 0.8116
Fitted a model with MAP estimate = -659.1012
Time for alignment: 59.8329
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 831.2484 - loglik: -8.1055e+02 - logprior: -2.0703e+01
Epoch 2/10
10/10 - 2s - loss: 775.4042 - loglik: -7.7066e+02 - logprior: -4.7398e+00
Epoch 3/10
10/10 - 2s - loss: 732.7039 - loglik: -7.3018e+02 - logprior: -2.5274e+00
Epoch 4/10
10/10 - 2s - loss: 704.3593 - loglik: -7.0230e+02 - logprior: -2.0624e+00
Epoch 5/10
10/10 - 2s - loss: 694.7244 - loglik: -6.9275e+02 - logprior: -1.9734e+00
Epoch 6/10
10/10 - 2s - loss: 689.7551 - loglik: -6.8794e+02 - logprior: -1.8181e+00
Epoch 7/10
10/10 - 2s - loss: 688.6281 - loglik: -6.8701e+02 - logprior: -1.6217e+00
Epoch 8/10
10/10 - 2s - loss: 687.4175 - loglik: -6.8592e+02 - logprior: -1.4981e+00
Epoch 9/10
10/10 - 2s - loss: 687.8686 - loglik: -6.8645e+02 - logprior: -1.4190e+00
Fitted a model with MAP estimate = -686.8729
expansions: [(6, 3), (7, 1), (8, 1), (9, 1), (14, 1), (23, 1), (24, 3), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (85, 2), (86, 3), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 702.9769 - loglik: -6.8413e+02 - logprior: -1.8846e+01
Epoch 2/2
10/10 - 2s - loss: 676.8541 - loglik: -6.7222e+02 - logprior: -4.6338e+00
Fitted a model with MAP estimate = -673.2769
expansions: []
discards: [  0   7   8 105 109]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 697.3289 - loglik: -6.7390e+02 - logprior: -2.3426e+01
Epoch 2/2
10/10 - 2s - loss: 681.2229 - loglik: -6.7203e+02 - logprior: -9.1920e+00
Fitted a model with MAP estimate = -677.8938
expansions: [(0, 5), (117, 3)]
discards: [  0 118]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 691.3156 - loglik: -6.7278e+02 - logprior: -1.8535e+01
Epoch 2/10
10/10 - 2s - loss: 672.3658 - loglik: -6.6817e+02 - logprior: -4.1959e+00
Epoch 3/10
10/10 - 2s - loss: 667.8038 - loglik: -6.6638e+02 - logprior: -1.4189e+00
Epoch 4/10
10/10 - 2s - loss: 666.5878 - loglik: -6.6617e+02 - logprior: -4.1989e-01
Epoch 5/10
10/10 - 2s - loss: 664.0505 - loglik: -6.6408e+02 - logprior: 0.0291
Epoch 6/10
10/10 - 2s - loss: 663.8618 - loglik: -6.6418e+02 - logprior: 0.3147
Epoch 7/10
10/10 - 2s - loss: 662.8549 - loglik: -6.6340e+02 - logprior: 0.5448
Epoch 8/10
10/10 - 2s - loss: 663.5161 - loglik: -6.6424e+02 - logprior: 0.7195
Fitted a model with MAP estimate = -662.7366
Time for alignment: 56.8262
Computed alignments with likelihoods: ['-662.5053', '-662.6987', '-663.5329', '-659.1012', '-662.7366']
Best model has likelihood: -659.1012  (prior= 0.8806 )
time for generating output: 0.1695
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8679183135704874
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.1414 - loglik: -4.6684e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 464.8229 - loglik: -4.5528e+02 - logprior: -9.5463e+00
Epoch 3/10
10/10 - 1s - loss: 447.0153 - loglik: -4.4269e+02 - logprior: -4.3272e+00
Epoch 4/10
10/10 - 1s - loss: 434.5698 - loglik: -4.3189e+02 - logprior: -2.6755e+00
Epoch 5/10
10/10 - 1s - loss: 429.3147 - loglik: -4.2711e+02 - logprior: -2.2032e+00
Epoch 6/10
10/10 - 1s - loss: 426.9686 - loglik: -4.2553e+02 - logprior: -1.4379e+00
Epoch 7/10
10/10 - 1s - loss: 426.0038 - loglik: -4.2520e+02 - logprior: -8.0142e-01
Epoch 8/10
10/10 - 1s - loss: 425.4258 - loglik: -4.2482e+02 - logprior: -6.0544e-01
Epoch 9/10
10/10 - 1s - loss: 424.9464 - loglik: -4.2452e+02 - logprior: -4.2649e-01
Epoch 10/10
10/10 - 1s - loss: 424.7420 - loglik: -4.2450e+02 - logprior: -2.4287e-01
Fitted a model with MAP estimate = -424.6198
expansions: [(0, 4), (6, 1), (22, 1), (25, 2), (27, 2), (31, 2), (40, 1), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 477.8061 - loglik: -4.2903e+02 - logprior: -4.8778e+01
Epoch 2/2
10/10 - 1s - loss: 436.2332 - loglik: -4.2183e+02 - logprior: -1.4401e+01
Fitted a model with MAP estimate = -428.8621
expansions: [(53, 1)]
discards: [ 0 31]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 462.9797 - loglik: -4.2108e+02 - logprior: -4.1903e+01
Epoch 2/2
10/10 - 1s - loss: 435.8962 - loglik: -4.1998e+02 - logprior: -1.5912e+01
Fitted a model with MAP estimate = -431.3346
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 459.4698 - loglik: -4.2012e+02 - logprior: -3.9349e+01
Epoch 2/10
10/10 - 1s - loss: 430.2984 - loglik: -4.1902e+02 - logprior: -1.1276e+01
Epoch 3/10
10/10 - 1s - loss: 422.8005 - loglik: -4.1919e+02 - logprior: -3.6078e+00
Epoch 4/10
10/10 - 1s - loss: 419.4227 - loglik: -4.1828e+02 - logprior: -1.1461e+00
Epoch 5/10
10/10 - 1s - loss: 417.9003 - loglik: -4.1772e+02 - logprior: -1.7949e-01
Epoch 6/10
10/10 - 1s - loss: 417.2665 - loglik: -4.1769e+02 - logprior: 0.4216
Epoch 7/10
10/10 - 1s - loss: 416.6366 - loglik: -4.1757e+02 - logprior: 0.9350
Epoch 8/10
10/10 - 1s - loss: 416.4133 - loglik: -4.1766e+02 - logprior: 1.2508
Epoch 9/10
10/10 - 1s - loss: 416.1723 - loglik: -4.1759e+02 - logprior: 1.4215
Epoch 10/10
10/10 - 1s - loss: 415.9674 - loglik: -4.1754e+02 - logprior: 1.5757
Fitted a model with MAP estimate = -415.8822
Time for alignment: 37.4897
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.1689 - loglik: -4.6686e+02 - logprior: -3.7306e+01
Epoch 2/10
10/10 - 1s - loss: 465.1170 - loglik: -4.5557e+02 - logprior: -9.5502e+00
Epoch 3/10
10/10 - 1s - loss: 447.2979 - loglik: -4.4295e+02 - logprior: -4.3459e+00
Epoch 4/10
10/10 - 1s - loss: 435.5746 - loglik: -4.3291e+02 - logprior: -2.6631e+00
Epoch 5/10
10/10 - 1s - loss: 430.3431 - loglik: -4.2815e+02 - logprior: -2.1979e+00
Epoch 6/10
10/10 - 1s - loss: 427.4871 - loglik: -4.2597e+02 - logprior: -1.5205e+00
Epoch 7/10
10/10 - 1s - loss: 426.2484 - loglik: -4.2535e+02 - logprior: -9.0189e-01
Epoch 8/10
10/10 - 1s - loss: 425.4341 - loglik: -4.2473e+02 - logprior: -7.0157e-01
Epoch 9/10
10/10 - 1s - loss: 425.4442 - loglik: -4.2490e+02 - logprior: -5.4052e-01
Fitted a model with MAP estimate = -425.0322
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 2), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 476.7300 - loglik: -4.2827e+02 - logprior: -4.8459e+01
Epoch 2/2
10/10 - 1s - loss: 435.9242 - loglik: -4.2179e+02 - logprior: -1.4137e+01
Fitted a model with MAP estimate = -428.5555
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 463.1152 - loglik: -4.2123e+02 - logprior: -4.1890e+01
Epoch 2/2
10/10 - 1s - loss: 436.5760 - loglik: -4.2060e+02 - logprior: -1.5980e+01
Fitted a model with MAP estimate = -431.7759
expansions: [(0, 2), (38, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 457.3706 - loglik: -4.2012e+02 - logprior: -3.7248e+01
Epoch 2/10
10/10 - 1s - loss: 428.3619 - loglik: -4.1904e+02 - logprior: -9.3199e+00
Epoch 3/10
10/10 - 1s - loss: 421.7339 - loglik: -4.1856e+02 - logprior: -3.1692e+00
Epoch 4/10
10/10 - 1s - loss: 419.2326 - loglik: -4.1829e+02 - logprior: -9.3801e-01
Epoch 5/10
10/10 - 1s - loss: 417.9415 - loglik: -4.1806e+02 - logprior: 0.1150
Epoch 6/10
10/10 - 1s - loss: 417.1215 - loglik: -4.1774e+02 - logprior: 0.6156
Epoch 7/10
10/10 - 1s - loss: 416.7794 - loglik: -4.1760e+02 - logprior: 0.8223
Epoch 8/10
10/10 - 1s - loss: 416.0339 - loglik: -4.1708e+02 - logprior: 1.0459
Epoch 9/10
10/10 - 1s - loss: 416.4058 - loglik: -4.1775e+02 - logprior: 1.3429
Fitted a model with MAP estimate = -415.9868
Time for alignment: 36.2493
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 504.2845 - loglik: -4.6698e+02 - logprior: -3.7306e+01
Epoch 2/10
10/10 - 1s - loss: 465.2105 - loglik: -4.5566e+02 - logprior: -9.5456e+00
Epoch 3/10
10/10 - 1s - loss: 447.2151 - loglik: -4.4289e+02 - logprior: -4.3212e+00
Epoch 4/10
10/10 - 1s - loss: 435.3870 - loglik: -4.3276e+02 - logprior: -2.6251e+00
Epoch 5/10
10/10 - 1s - loss: 429.9875 - loglik: -4.2779e+02 - logprior: -2.1944e+00
Epoch 6/10
10/10 - 1s - loss: 427.1015 - loglik: -4.2561e+02 - logprior: -1.4913e+00
Epoch 7/10
10/10 - 1s - loss: 425.8775 - loglik: -4.2503e+02 - logprior: -8.4944e-01
Epoch 8/10
10/10 - 1s - loss: 425.4582 - loglik: -4.2481e+02 - logprior: -6.4878e-01
Epoch 9/10
10/10 - 1s - loss: 425.3627 - loglik: -4.2488e+02 - logprior: -4.8472e-01
Epoch 10/10
10/10 - 1s - loss: 425.1925 - loglik: -4.2489e+02 - logprior: -2.9878e-01
Fitted a model with MAP estimate = -424.7880
expansions: [(0, 3), (6, 1), (22, 1), (25, 2), (27, 2), (31, 2), (40, 1), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 477.7812 - loglik: -4.2897e+02 - logprior: -4.8811e+01
Epoch 2/2
10/10 - 1s - loss: 436.9203 - loglik: -4.2252e+02 - logprior: -1.4396e+01
Fitted a model with MAP estimate = -429.2114
expansions: [(52, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 464.0063 - loglik: -4.2197e+02 - logprior: -4.2037e+01
Epoch 2/2
10/10 - 1s - loss: 435.9774 - loglik: -4.1997e+02 - logprior: -1.6006e+01
Fitted a model with MAP estimate = -431.8321
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 457.3191 - loglik: -4.1999e+02 - logprior: -3.7326e+01
Epoch 2/10
10/10 - 1s - loss: 428.7807 - loglik: -4.1943e+02 - logprior: -9.3532e+00
Epoch 3/10
10/10 - 1s - loss: 422.3076 - loglik: -4.1912e+02 - logprior: -3.1865e+00
Epoch 4/10
10/10 - 1s - loss: 419.7165 - loglik: -4.1875e+02 - logprior: -9.6765e-01
Epoch 5/10
10/10 - 1s - loss: 418.3847 - loglik: -4.1848e+02 - logprior: 0.0941
Epoch 6/10
10/10 - 1s - loss: 417.7198 - loglik: -4.1830e+02 - logprior: 0.5787
Epoch 7/10
10/10 - 1s - loss: 417.1366 - loglik: -4.1791e+02 - logprior: 0.7768
Epoch 8/10
10/10 - 1s - loss: 417.1166 - loglik: -4.1813e+02 - logprior: 1.0146
Epoch 9/10
10/10 - 1s - loss: 416.5510 - loglik: -4.1787e+02 - logprior: 1.3195
Epoch 10/10
10/10 - 1s - loss: 416.4242 - loglik: -4.1794e+02 - logprior: 1.5202
Fitted a model with MAP estimate = -416.3527
Time for alignment: 36.2910
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 504.2668 - loglik: -4.6696e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 464.6122 - loglik: -4.5507e+02 - logprior: -9.5391e+00
Epoch 3/10
10/10 - 1s - loss: 447.3363 - loglik: -4.4305e+02 - logprior: -4.2912e+00
Epoch 4/10
10/10 - 1s - loss: 435.6335 - loglik: -4.3309e+02 - logprior: -2.5397e+00
Epoch 5/10
10/10 - 1s - loss: 431.1619 - loglik: -4.2917e+02 - logprior: -1.9877e+00
Epoch 6/10
10/10 - 1s - loss: 428.6157 - loglik: -4.2739e+02 - logprior: -1.2240e+00
Epoch 7/10
10/10 - 1s - loss: 427.1849 - loglik: -4.2661e+02 - logprior: -5.7412e-01
Epoch 8/10
10/10 - 1s - loss: 426.9010 - loglik: -4.2651e+02 - logprior: -3.9130e-01
Epoch 9/10
10/10 - 1s - loss: 426.0784 - loglik: -4.2586e+02 - logprior: -2.1528e-01
Epoch 10/10
10/10 - 1s - loss: 425.9543 - loglik: -4.2594e+02 - logprior: -1.4194e-02
Fitted a model with MAP estimate = -425.7504
expansions: [(0, 4), (6, 1), (26, 3), (31, 2), (40, 2), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 478.2927 - loglik: -4.2951e+02 - logprior: -4.8780e+01
Epoch 2/2
10/10 - 1s - loss: 437.6411 - loglik: -4.2336e+02 - logprior: -1.4278e+01
Fitted a model with MAP estimate = -429.8309
expansions: [(39, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.6818 - loglik: -4.2268e+02 - logprior: -4.2004e+01
Epoch 2/2
10/10 - 1s - loss: 436.9991 - loglik: -4.2105e+02 - logprior: -1.5952e+01
Fitted a model with MAP estimate = -432.4814
expansions: [(0, 3), (32, 1)]
discards: [ 0  1 37]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 457.8887 - loglik: -4.2067e+02 - logprior: -3.7216e+01
Epoch 2/10
10/10 - 1s - loss: 428.7657 - loglik: -4.1951e+02 - logprior: -9.2536e+00
Epoch 3/10
10/10 - 1s - loss: 422.0229 - loglik: -4.1892e+02 - logprior: -3.0992e+00
Epoch 4/10
10/10 - 1s - loss: 419.4587 - loglik: -4.1859e+02 - logprior: -8.7174e-01
Epoch 5/10
10/10 - 1s - loss: 418.0920 - loglik: -4.1828e+02 - logprior: 0.1857
Epoch 6/10
10/10 - 1s - loss: 417.0100 - loglik: -4.1767e+02 - logprior: 0.6627
Epoch 7/10
10/10 - 1s - loss: 417.0574 - loglik: -4.1792e+02 - logprior: 0.8647
Fitted a model with MAP estimate = -416.5881
Time for alignment: 33.4278
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 504.0973 - loglik: -4.6679e+02 - logprior: -3.7307e+01
Epoch 2/10
10/10 - 1s - loss: 465.2073 - loglik: -4.5566e+02 - logprior: -9.5501e+00
Epoch 3/10
10/10 - 1s - loss: 448.4352 - loglik: -4.4403e+02 - logprior: -4.4087e+00
Epoch 4/10
10/10 - 1s - loss: 436.5197 - loglik: -4.3372e+02 - logprior: -2.8011e+00
Epoch 5/10
10/10 - 1s - loss: 430.6997 - loglik: -4.2830e+02 - logprior: -2.3949e+00
Epoch 6/10
10/10 - 1s - loss: 428.0779 - loglik: -4.2623e+02 - logprior: -1.8514e+00
Epoch 7/10
10/10 - 1s - loss: 426.4189 - loglik: -4.2511e+02 - logprior: -1.3087e+00
Epoch 8/10
10/10 - 1s - loss: 425.7232 - loglik: -4.2456e+02 - logprior: -1.1654e+00
Epoch 9/10
10/10 - 1s - loss: 425.1680 - loglik: -4.2419e+02 - logprior: -9.8295e-01
Epoch 10/10
10/10 - 1s - loss: 424.8783 - loglik: -4.2413e+02 - logprior: -7.5278e-01
Fitted a model with MAP estimate = -424.8559
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (34, 1), (40, 2), (49, 1), (54, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 477.2502 - loglik: -4.2863e+02 - logprior: -4.8620e+01
Epoch 2/2
10/10 - 1s - loss: 435.3094 - loglik: -4.2122e+02 - logprior: -1.4091e+01
Fitted a model with MAP estimate = -427.9972
expansions: []
discards: [ 0 39]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 463.0224 - loglik: -4.2112e+02 - logprior: -4.1901e+01
Epoch 2/2
10/10 - 1s - loss: 436.4615 - loglik: -4.2048e+02 - logprior: -1.5980e+01
Fitted a model with MAP estimate = -431.7958
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 457.3691 - loglik: -4.2003e+02 - logprior: -3.7335e+01
Epoch 2/10
10/10 - 1s - loss: 428.4046 - loglik: -4.1905e+02 - logprior: -9.3549e+00
Epoch 3/10
10/10 - 1s - loss: 422.2361 - loglik: -4.1905e+02 - logprior: -3.1835e+00
Epoch 4/10
10/10 - 1s - loss: 419.6982 - loglik: -4.1873e+02 - logprior: -9.6356e-01
Epoch 5/10
10/10 - 1s - loss: 418.4626 - loglik: -4.1856e+02 - logprior: 0.0953
Epoch 6/10
10/10 - 1s - loss: 417.5400 - loglik: -4.1813e+02 - logprior: 0.5927
Epoch 7/10
10/10 - 1s - loss: 417.0574 - loglik: -4.1786e+02 - logprior: 0.8014
Epoch 8/10
10/10 - 1s - loss: 416.7591 - loglik: -4.1778e+02 - logprior: 1.0257
Epoch 9/10
10/10 - 1s - loss: 416.5670 - loglik: -4.1789e+02 - logprior: 1.3273
Epoch 10/10
10/10 - 1s - loss: 416.3404 - loglik: -4.1788e+02 - logprior: 1.5360
Fitted a model with MAP estimate = -416.2198
Time for alignment: 35.3974
Computed alignments with likelihoods: ['-415.8822', '-415.9868', '-416.3527', '-416.5881', '-416.2198']
Best model has likelihood: -415.8822  (prior= 1.6772 )
time for generating output: 0.1556
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7483028720626632
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 2000.0276 - loglik: -1.9988e+03 - logprior: -1.2311e+00
Epoch 2/10
43/43 - 27s - loss: 1889.2224 - loglik: -1.8877e+03 - logprior: -1.5615e+00
Epoch 3/10
43/43 - 27s - loss: 1870.8452 - loglik: -1.8692e+03 - logprior: -1.6465e+00
Epoch 4/10
43/43 - 27s - loss: 1868.3192 - loglik: -1.8667e+03 - logprior: -1.6064e+00
Epoch 5/10
43/43 - 27s - loss: 1866.2102 - loglik: -1.8646e+03 - logprior: -1.6382e+00
Epoch 6/10
43/43 - 27s - loss: 1865.7843 - loglik: -1.8641e+03 - logprior: -1.6402e+00
Epoch 7/10
43/43 - 27s - loss: 1863.1384 - loglik: -1.8615e+03 - logprior: -1.6349e+00
Epoch 8/10
43/43 - 27s - loss: 1864.5033 - loglik: -1.8628e+03 - logprior: -1.6662e+00
Fitted a model with MAP estimate = -1804.8427
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (41, 1), (43, 1), (45, 1), (46, 2), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (98, 1), (104, 2), (119, 1), (122, 1), (123, 1), (125, 1), (130, 1), (132, 2), (144, 1), (147, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (196, 1), (203, 1), (204, 1), (206, 2), (207, 1), (208, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (240, 1), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 1860.3683 - loglik: -1.8585e+03 - logprior: -1.8527e+00
Epoch 2/2
43/43 - 41s - loss: 1838.6101 - loglik: -1.8378e+03 - logprior: -7.6733e-01
Fitted a model with MAP estimate = -1767.8790
expansions: [(312, 2)]
discards: [  0  60 110 171 206 291 296 351 353 356]
Re-initialized the encoder parameters.
Fitting a model of length 358 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1845.0732 - loglik: -1.8431e+03 - logprior: -1.9586e+00
Epoch 2/2
43/43 - 40s - loss: 1841.1313 - loglik: -1.8407e+03 - logprior: -3.8533e-01
Fitted a model with MAP estimate = -1768.0606
expansions: [(0, 1), (262, 1), (286, 1)]
discards: [  0 133 194 195 260 305]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 1753.7610 - loglik: -1.7531e+03 - logprior: -6.6565e-01
Epoch 2/10
61/61 - 55s - loss: 1745.0688 - loglik: -1.7447e+03 - logprior: -4.0032e-01
Epoch 3/10
61/61 - 55s - loss: 1745.8645 - loglik: -1.7455e+03 - logprior: -3.5074e-01
Fitted a model with MAP estimate = -1741.8302
Time for alignment: 852.1200
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1999.9032 - loglik: -1.9986e+03 - logprior: -1.2585e+00
Epoch 2/10
43/43 - 27s - loss: 1883.7371 - loglik: -1.8820e+03 - logprior: -1.7379e+00
Epoch 3/10
43/43 - 28s - loss: 1872.7964 - loglik: -1.8710e+03 - logprior: -1.7771e+00
Epoch 4/10
43/43 - 27s - loss: 1869.0283 - loglik: -1.8673e+03 - logprior: -1.7305e+00
Epoch 5/10
43/43 - 27s - loss: 1863.5062 - loglik: -1.8618e+03 - logprior: -1.7384e+00
Epoch 6/10
43/43 - 27s - loss: 1864.9747 - loglik: -1.8632e+03 - logprior: -1.7833e+00
Fitted a model with MAP estimate = -1800.4253
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 2), (81, 1), (82, 2), (91, 2), (92, 2), (93, 1), (96, 1), (98, 1), (99, 1), (104, 2), (120, 1), (123, 1), (124, 1), (128, 1), (131, 1), (133, 2), (143, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (183, 3), (184, 1), (185, 1), (187, 1), (188, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (240, 1), (242, 1), (245, 1), (248, 1), (250, 2), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 1859.8392 - loglik: -1.8577e+03 - logprior: -2.1291e+00
Epoch 2/2
43/43 - 43s - loss: 1840.3879 - loglik: -1.8394e+03 - logprior: -9.4396e-01
Fitted a model with MAP estimate = -1768.5010
expansions: [(0, 2), (316, 2)]
discards: [  0  32  33 102 105 117 122 137 173 208 238 267 268 269 295 300 328 356
 358 361]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1844.5938 - loglik: -1.8435e+03 - logprior: -1.0727e+00
Epoch 2/2
43/43 - 40s - loss: 1839.3656 - loglik: -1.8390e+03 - logprior: -3.4623e-01
Fitted a model with MAP estimate = -1769.5576
expansions: []
discards: [  0   1 193 194 302]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 56s - loss: 1755.2654 - loglik: -1.7544e+03 - logprior: -8.7977e-01
Epoch 2/10
61/61 - 54s - loss: 1748.7684 - loglik: -1.7485e+03 - logprior: -2.8824e-01
Epoch 3/10
61/61 - 53s - loss: 1746.1744 - loglik: -1.7459e+03 - logprior: -2.3717e-01
Epoch 4/10
61/61 - 53s - loss: 1743.0201 - loglik: -1.7428e+03 - logprior: -1.8720e-01
Epoch 5/10
61/61 - 54s - loss: 1741.0487 - loglik: -1.7409e+03 - logprior: -1.2999e-01
Epoch 6/10
61/61 - 54s - loss: 1737.2535 - loglik: -1.7371e+03 - logprior: -1.2178e-01
Epoch 7/10
61/61 - 54s - loss: 1741.8962 - loglik: -1.7419e+03 - logprior: -2.7318e-02
Fitted a model with MAP estimate = -1738.7915
Time for alignment: 1006.0518
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 1997.2726 - loglik: -1.9960e+03 - logprior: -1.2267e+00
Epoch 2/10
43/43 - 27s - loss: 1883.2363 - loglik: -1.8817e+03 - logprior: -1.5229e+00
Epoch 3/10
43/43 - 27s - loss: 1872.1017 - loglik: -1.8705e+03 - logprior: -1.5550e+00
Epoch 4/10
43/43 - 27s - loss: 1865.6492 - loglik: -1.8640e+03 - logprior: -1.6011e+00
Epoch 5/10
43/43 - 27s - loss: 1864.1383 - loglik: -1.8625e+03 - logprior: -1.6384e+00
Epoch 6/10
43/43 - 27s - loss: 1863.9104 - loglik: -1.8623e+03 - logprior: -1.6420e+00
Epoch 7/10
43/43 - 27s - loss: 1861.3843 - loglik: -1.8598e+03 - logprior: -1.6299e+00
Epoch 8/10
43/43 - 27s - loss: 1863.7769 - loglik: -1.8622e+03 - logprior: -1.5942e+00
Fitted a model with MAP estimate = -1802.9610
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 1), (24, 3), (25, 1), (29, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 2), (61, 1), (62, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (101, 1), (105, 2), (121, 1), (122, 1), (125, 1), (131, 1), (133, 2), (143, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (166, 1), (181, 1), (183, 1), (185, 2), (186, 2), (187, 1), (188, 1), (197, 1), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (239, 4), (240, 1), (242, 1), (245, 1), (250, 1), (257, 1), (260, 3), (269, 2), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 373 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 1861.2729 - loglik: -1.8593e+03 - logprior: -1.9294e+00
Epoch 2/2
43/43 - 43s - loss: 1840.0879 - loglik: -1.8392e+03 - logprior: -8.9045e-01
Fitted a model with MAP estimate = -1768.3261
expansions: []
discards: [  0   1  32  33  60  78 109 139 173 199 206 240 242 267 268 269 295 300
 317 318 346 361 363]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1846.2991 - loglik: -1.8452e+03 - logprior: -1.0894e+00
Epoch 2/2
43/43 - 39s - loss: 1841.4137 - loglik: -1.8412e+03 - logprior: -1.8867e-01
Fitted a model with MAP estimate = -1769.7637
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 1754.8807 - loglik: -1.7542e+03 - logprior: -7.2171e-01
Epoch 2/10
61/61 - 54s - loss: 1746.7224 - loglik: -1.7463e+03 - logprior: -3.7819e-01
Epoch 3/10
61/61 - 54s - loss: 1746.1813 - loglik: -1.7459e+03 - logprior: -3.2481e-01
Epoch 4/10
61/61 - 54s - loss: 1742.4233 - loglik: -1.7421e+03 - logprior: -2.9672e-01
Epoch 5/10
61/61 - 55s - loss: 1742.0409 - loglik: -1.7418e+03 - logprior: -2.6949e-01
Epoch 6/10
61/61 - 54s - loss: 1737.3788 - loglik: -1.7372e+03 - logprior: -2.1318e-01
Epoch 7/10
61/61 - 54s - loss: 1741.3274 - loglik: -1.7412e+03 - logprior: -1.6585e-01
Fitted a model with MAP estimate = -1738.7984
Time for alignment: 1064.4985
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 1998.6949 - loglik: -1.9974e+03 - logprior: -1.2528e+00
Epoch 2/10
43/43 - 27s - loss: 1884.9755 - loglik: -1.8833e+03 - logprior: -1.6581e+00
Epoch 3/10
43/43 - 27s - loss: 1873.3204 - loglik: -1.8716e+03 - logprior: -1.7319e+00
Epoch 4/10
43/43 - 27s - loss: 1868.3539 - loglik: -1.8666e+03 - logprior: -1.7413e+00
Epoch 5/10
43/43 - 27s - loss: 1865.2068 - loglik: -1.8635e+03 - logprior: -1.7532e+00
Epoch 6/10
43/43 - 27s - loss: 1865.3801 - loglik: -1.8636e+03 - logprior: -1.7863e+00
Fitted a model with MAP estimate = -1800.0715
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (29, 1), (40, 1), (41, 2), (42, 1), (43, 1), (45, 2), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 2), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (96, 1), (98, 1), (99, 1), (104, 2), (120, 1), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 1), (188, 1), (197, 1), (200, 2), (205, 3), (206, 2), (207, 1), (208, 2), (209, 1), (219, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 3), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 374 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 1862.7195 - loglik: -1.8606e+03 - logprior: -2.1493e+00
Epoch 2/2
43/43 - 43s - loss: 1840.9446 - loglik: -1.8400e+03 - logprior: -9.8945e-01
Fitted a model with MAP estimate = -1767.9402
expansions: [(0, 2)]
discards: [  0  27  53  60 103 107 113 138 173 200 207 241 261 269 270 271 277 297
 302 319 359 361 364]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1844.1451 - loglik: -1.8430e+03 - logprior: -1.0979e+00
Epoch 2/2
43/43 - 39s - loss: 1841.4635 - loglik: -1.8411e+03 - logprior: -3.5077e-01
Fitted a model with MAP estimate = -1770.3418
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 1752.7902 - loglik: -1.7520e+03 - logprior: -8.0079e-01
Epoch 2/10
61/61 - 54s - loss: 1749.6772 - loglik: -1.7493e+03 - logprior: -3.8313e-01
Epoch 3/10
61/61 - 54s - loss: 1746.4624 - loglik: -1.7461e+03 - logprior: -3.4344e-01
Epoch 4/10
61/61 - 54s - loss: 1739.3256 - loglik: -1.7390e+03 - logprior: -2.8983e-01
Epoch 5/10
61/61 - 54s - loss: 1742.9952 - loglik: -1.7427e+03 - logprior: -2.6021e-01
Fitted a model with MAP estimate = -1738.4944
Time for alignment: 902.9246
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1998.1362 - loglik: -1.9969e+03 - logprior: -1.2762e+00
Epoch 2/10
43/43 - 27s - loss: 1882.7600 - loglik: -1.8811e+03 - logprior: -1.7048e+00
Epoch 3/10
43/43 - 27s - loss: 1873.4048 - loglik: -1.8717e+03 - logprior: -1.7121e+00
Epoch 4/10
43/43 - 27s - loss: 1870.2847 - loglik: -1.8686e+03 - logprior: -1.6933e+00
Epoch 5/10
43/43 - 27s - loss: 1867.2040 - loglik: -1.8655e+03 - logprior: -1.7290e+00
Epoch 6/10
43/43 - 27s - loss: 1864.3340 - loglik: -1.8626e+03 - logprior: -1.6962e+00
Epoch 7/10
43/43 - 27s - loss: 1865.4473 - loglik: -1.8637e+03 - logprior: -1.7122e+00
Fitted a model with MAP estimate = -1803.1796
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (57, 1), (60, 1), (61, 1), (62, 1), (65, 1), (80, 2), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (101, 1), (104, 2), (119, 1), (122, 1), (125, 1), (128, 1), (130, 1), (142, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (204, 1), (205, 3), (206, 2), (207, 1), (208, 2), (209, 1), (219, 2), (220, 1), (221, 2), (224, 2), (225, 2), (238, 2), (242, 1), (244, 2), (245, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 372 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 1862.0383 - loglik: -1.8598e+03 - logprior: -2.1920e+00
Epoch 2/2
43/43 - 43s - loss: 1840.4843 - loglik: -1.8395e+03 - logprior: -9.6432e-01
Fitted a model with MAP estimate = -1767.6281
expansions: [(0, 2)]
discards: [  0  27 101 105 111 198 206 264 265 266 273 293 298 300 316 324 357 359
 362]
Re-initialized the encoder parameters.
Fitting a model of length 355 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1843.7747 - loglik: -1.8427e+03 - logprior: -1.1141e+00
Epoch 2/2
43/43 - 40s - loss: 1839.7821 - loglik: -1.8394e+03 - logprior: -3.7581e-01
Fitted a model with MAP estimate = -1768.8603
expansions: []
discards: [  0 134 284]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 1754.2572 - loglik: -1.7535e+03 - logprior: -8.0689e-01
Epoch 2/10
61/61 - 54s - loss: 1748.4541 - loglik: -1.7481e+03 - logprior: -3.8440e-01
Epoch 3/10
61/61 - 54s - loss: 1745.1652 - loglik: -1.7448e+03 - logprior: -3.3401e-01
Epoch 4/10
61/61 - 54s - loss: 1740.3717 - loglik: -1.7401e+03 - logprior: -3.1829e-01
Epoch 5/10
61/61 - 54s - loss: 1742.7343 - loglik: -1.7425e+03 - logprior: -2.4878e-01
Fitted a model with MAP estimate = -1737.5789
Time for alignment: 931.3094
Computed alignments with likelihoods: ['-1741.8302', '-1738.7915', '-1738.7984', '-1738.4944', '-1737.5789']
Best model has likelihood: -1737.5789  (prior= -0.2277 )
time for generating output: 0.3437
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8268123604746799
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 445.0907 - loglik: -3.8843e+02 - logprior: -5.6662e+01
Epoch 2/10
10/10 - 1s - loss: 381.9023 - loglik: -3.6633e+02 - logprior: -1.5577e+01
Epoch 3/10
10/10 - 1s - loss: 356.6418 - loglik: -3.4903e+02 - logprior: -7.6131e+00
Epoch 4/10
10/10 - 1s - loss: 348.8471 - loglik: -3.4435e+02 - logprior: -4.4983e+00
Epoch 5/10
10/10 - 1s - loss: 345.6030 - loglik: -3.4264e+02 - logprior: -2.9634e+00
Epoch 6/10
10/10 - 1s - loss: 343.0535 - loglik: -3.4100e+02 - logprior: -2.0500e+00
Epoch 7/10
10/10 - 1s - loss: 342.7649 - loglik: -3.4123e+02 - logprior: -1.5338e+00
Epoch 8/10
10/10 - 1s - loss: 341.5872 - loglik: -3.4031e+02 - logprior: -1.2749e+00
Epoch 9/10
10/10 - 1s - loss: 342.1104 - loglik: -3.4100e+02 - logprior: -1.1130e+00
Fitted a model with MAP estimate = -341.5546
expansions: [(13, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.8233 - loglik: -3.3976e+02 - logprior: -6.4065e+01
Epoch 2/2
10/10 - 1s - loss: 365.6178 - loglik: -3.3862e+02 - logprior: -2.6998e+01
Fitted a model with MAP estimate = -359.4325
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 389.3564 - loglik: -3.3694e+02 - logprior: -5.2414e+01
Epoch 2/2
10/10 - 1s - loss: 351.0685 - loglik: -3.3663e+02 - logprior: -1.4435e+01
Fitted a model with MAP estimate = -345.6607
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 400.2978 - loglik: -3.3847e+02 - logprior: -6.1831e+01
Epoch 2/10
10/10 - 1s - loss: 356.4991 - loglik: -3.3604e+02 - logprior: -2.0460e+01
Epoch 3/10
10/10 - 1s - loss: 344.0764 - loglik: -3.3604e+02 - logprior: -8.0334e+00
Epoch 4/10
10/10 - 1s - loss: 341.9956 - loglik: -3.3816e+02 - logprior: -3.8337e+00
Epoch 5/10
10/10 - 1s - loss: 339.9405 - loglik: -3.3779e+02 - logprior: -2.1528e+00
Epoch 6/10
10/10 - 1s - loss: 338.8309 - loglik: -3.3752e+02 - logprior: -1.3117e+00
Epoch 7/10
10/10 - 1s - loss: 338.8681 - loglik: -3.3815e+02 - logprior: -7.1851e-01
Fitted a model with MAP estimate = -337.9890
Time for alignment: 36.9182
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 445.2023 - loglik: -3.8854e+02 - logprior: -5.6664e+01
Epoch 2/10
10/10 - 1s - loss: 382.1423 - loglik: -3.6656e+02 - logprior: -1.5579e+01
Epoch 3/10
10/10 - 1s - loss: 357.3365 - loglik: -3.4975e+02 - logprior: -7.5891e+00
Epoch 4/10
10/10 - 1s - loss: 348.9854 - loglik: -3.4445e+02 - logprior: -4.5379e+00
Epoch 5/10
10/10 - 1s - loss: 343.5096 - loglik: -3.4041e+02 - logprior: -3.1045e+00
Epoch 6/10
10/10 - 1s - loss: 343.9407 - loglik: -3.4172e+02 - logprior: -2.2231e+00
Fitted a model with MAP estimate = -342.9579
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 402.5933 - loglik: -3.3859e+02 - logprior: -6.4004e+01
Epoch 2/2
10/10 - 1s - loss: 366.8390 - loglik: -3.3992e+02 - logprior: -2.6920e+01
Fitted a model with MAP estimate = -359.1102
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.9094 - loglik: -3.3563e+02 - logprior: -5.2284e+01
Epoch 2/2
10/10 - 1s - loss: 352.1512 - loglik: -3.3772e+02 - logprior: -1.4435e+01
Fitted a model with MAP estimate = -345.1592
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 398.6668 - loglik: -3.3651e+02 - logprior: -6.2159e+01
Epoch 2/10
10/10 - 1s - loss: 358.0375 - loglik: -3.3704e+02 - logprior: -2.1001e+01
Epoch 3/10
10/10 - 1s - loss: 344.9400 - loglik: -3.3666e+02 - logprior: -8.2840e+00
Epoch 4/10
10/10 - 1s - loss: 341.1230 - loglik: -3.3722e+02 - logprior: -3.9028e+00
Epoch 5/10
10/10 - 1s - loss: 338.8062 - loglik: -3.3661e+02 - logprior: -2.1950e+00
Epoch 6/10
10/10 - 1s - loss: 338.0182 - loglik: -3.3667e+02 - logprior: -1.3472e+00
Epoch 7/10
10/10 - 1s - loss: 338.8089 - loglik: -3.3805e+02 - logprior: -7.5929e-01
Fitted a model with MAP estimate = -337.6706
Time for alignment: 33.6743
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 445.2248 - loglik: -3.8856e+02 - logprior: -5.6664e+01
Epoch 2/10
10/10 - 1s - loss: 381.8627 - loglik: -3.6627e+02 - logprior: -1.5594e+01
Epoch 3/10
10/10 - 1s - loss: 356.5706 - loglik: -3.4897e+02 - logprior: -7.5990e+00
Epoch 4/10
10/10 - 1s - loss: 349.3816 - loglik: -3.4483e+02 - logprior: -4.5558e+00
Epoch 5/10
10/10 - 1s - loss: 344.3912 - loglik: -3.4125e+02 - logprior: -3.1395e+00
Epoch 6/10
10/10 - 1s - loss: 344.2751 - loglik: -3.4203e+02 - logprior: -2.2489e+00
Epoch 7/10
10/10 - 1s - loss: 342.8236 - loglik: -3.4116e+02 - logprior: -1.6684e+00
Epoch 8/10
10/10 - 1s - loss: 341.8571 - loglik: -3.4046e+02 - logprior: -1.3965e+00
Epoch 9/10
10/10 - 1s - loss: 342.1017 - loglik: -3.4086e+02 - logprior: -1.2405e+00
Fitted a model with MAP estimate = -342.0068
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.2079 - loglik: -3.3924e+02 - logprior: -6.3965e+01
Epoch 2/2
10/10 - 1s - loss: 365.2033 - loglik: -3.3831e+02 - logprior: -2.6890e+01
Fitted a model with MAP estimate = -358.7242
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 388.0545 - loglik: -3.3593e+02 - logprior: -5.2125e+01
Epoch 2/2
10/10 - 1s - loss: 350.5847 - loglik: -3.3622e+02 - logprior: -1.4362e+01
Fitted a model with MAP estimate = -345.0421
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 398.4834 - loglik: -3.3634e+02 - logprior: -6.2141e+01
Epoch 2/10
10/10 - 1s - loss: 358.3589 - loglik: -3.3729e+02 - logprior: -2.1072e+01
Epoch 3/10
10/10 - 1s - loss: 345.6034 - loglik: -3.3732e+02 - logprior: -8.2809e+00
Epoch 4/10
10/10 - 1s - loss: 340.4755 - loglik: -3.3664e+02 - logprior: -3.8402e+00
Epoch 5/10
10/10 - 1s - loss: 339.3611 - loglik: -3.3723e+02 - logprior: -2.1293e+00
Epoch 6/10
10/10 - 1s - loss: 337.9307 - loglik: -3.3665e+02 - logprior: -1.2795e+00
Epoch 7/10
10/10 - 1s - loss: 338.1146 - loglik: -3.3742e+02 - logprior: -6.9548e-01
Fitted a model with MAP estimate = -337.6193
Time for alignment: 38.4022
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 446.8577 - loglik: -3.9019e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 380.6164 - loglik: -3.6502e+02 - logprior: -1.5598e+01
Epoch 3/10
10/10 - 1s - loss: 357.4012 - loglik: -3.4982e+02 - logprior: -7.5766e+00
Epoch 4/10
10/10 - 1s - loss: 348.7962 - loglik: -3.4424e+02 - logprior: -4.5516e+00
Epoch 5/10
10/10 - 1s - loss: 343.4227 - loglik: -3.4026e+02 - logprior: -3.1668e+00
Epoch 6/10
10/10 - 1s - loss: 344.3569 - loglik: -3.4200e+02 - logprior: -2.3532e+00
Fitted a model with MAP estimate = -342.8810
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.9044 - loglik: -3.3987e+02 - logprior: -6.4039e+01
Epoch 2/2
10/10 - 1s - loss: 364.1699 - loglik: -3.3729e+02 - logprior: -2.6880e+01
Fitted a model with MAP estimate = -358.6609
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 388.9167 - loglik: -3.3680e+02 - logprior: -5.2115e+01
Epoch 2/2
10/10 - 1s - loss: 349.9557 - loglik: -3.3551e+02 - logprior: -1.4446e+01
Fitted a model with MAP estimate = -345.0485
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 398.2018 - loglik: -3.3589e+02 - logprior: -6.2316e+01
Epoch 2/10
10/10 - 1s - loss: 358.7923 - loglik: -3.3745e+02 - logprior: -2.1344e+01
Epoch 3/10
10/10 - 1s - loss: 344.2224 - loglik: -3.3576e+02 - logprior: -8.4666e+00
Epoch 4/10
10/10 - 1s - loss: 341.1574 - loglik: -3.3721e+02 - logprior: -3.9436e+00
Epoch 5/10
10/10 - 1s - loss: 339.9411 - loglik: -3.3773e+02 - logprior: -2.2072e+00
Epoch 6/10
10/10 - 1s - loss: 337.3315 - loglik: -3.3598e+02 - logprior: -1.3478e+00
Epoch 7/10
10/10 - 1s - loss: 338.0738 - loglik: -3.3734e+02 - logprior: -7.3585e-01
Fitted a model with MAP estimate = -337.5557
Time for alignment: 33.6168
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 444.2853 - loglik: -3.8762e+02 - logprior: -5.6667e+01
Epoch 2/10
10/10 - 1s - loss: 383.2991 - loglik: -3.6770e+02 - logprior: -1.5602e+01
Epoch 3/10
10/10 - 1s - loss: 357.3909 - loglik: -3.4981e+02 - logprior: -7.5789e+00
Epoch 4/10
10/10 - 1s - loss: 347.5565 - loglik: -3.4298e+02 - logprior: -4.5759e+00
Epoch 5/10
10/10 - 1s - loss: 345.2415 - loglik: -3.4201e+02 - logprior: -3.2268e+00
Epoch 6/10
10/10 - 1s - loss: 343.0881 - loglik: -3.4074e+02 - logprior: -2.3515e+00
Epoch 7/10
10/10 - 1s - loss: 342.4196 - loglik: -3.4064e+02 - logprior: -1.7767e+00
Epoch 8/10
10/10 - 1s - loss: 341.6809 - loglik: -3.4021e+02 - logprior: -1.4686e+00
Epoch 9/10
10/10 - 1s - loss: 342.2552 - loglik: -3.4095e+02 - logprior: -1.3010e+00
Fitted a model with MAP estimate = -341.8656
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.0474 - loglik: -3.3901e+02 - logprior: -6.4033e+01
Epoch 2/2
10/10 - 1s - loss: 364.5751 - loglik: -3.3768e+02 - logprior: -2.6898e+01
Fitted a model with MAP estimate = -358.6088
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 388.1052 - loglik: -3.3597e+02 - logprior: -5.2133e+01
Epoch 2/2
10/10 - 1s - loss: 350.7301 - loglik: -3.3632e+02 - logprior: -1.4406e+01
Fitted a model with MAP estimate = -344.9577
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.2387 - loglik: -3.3693e+02 - logprior: -6.2313e+01
Epoch 2/10
10/10 - 1s - loss: 357.8580 - loglik: -3.3637e+02 - logprior: -2.1488e+01
Epoch 3/10
10/10 - 1s - loss: 345.1020 - loglik: -3.3660e+02 - logprior: -8.4999e+00
Epoch 4/10
10/10 - 1s - loss: 341.1214 - loglik: -3.3721e+02 - logprior: -3.9071e+00
Epoch 5/10
10/10 - 1s - loss: 339.2375 - loglik: -3.3706e+02 - logprior: -2.1738e+00
Epoch 6/10
10/10 - 1s - loss: 337.5794 - loglik: -3.3627e+02 - logprior: -1.3117e+00
Epoch 7/10
10/10 - 1s - loss: 337.9894 - loglik: -3.3730e+02 - logprior: -6.9282e-01
Fitted a model with MAP estimate = -337.5286
Time for alignment: 36.9035
Computed alignments with likelihoods: ['-337.9890', '-337.6706', '-337.6193', '-337.5557', '-337.5286']
Best model has likelihood: -337.5286  (prior= -0.4156 )
time for generating output: 0.1027
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9070735090152566
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1022.7796 - loglik: -9.5603e+02 - logprior: -6.6745e+01
Epoch 2/10
10/10 - 2s - loss: 922.8096 - loglik: -9.0955e+02 - logprior: -1.3264e+01
Epoch 3/10
10/10 - 2s - loss: 871.1776 - loglik: -8.6689e+02 - logprior: -4.2915e+00
Epoch 4/10
10/10 - 2s - loss: 841.7311 - loglik: -8.4034e+02 - logprior: -1.3894e+00
Epoch 5/10
10/10 - 2s - loss: 829.0087 - loglik: -8.2904e+02 - logprior: 0.0281
Epoch 6/10
10/10 - 2s - loss: 822.2853 - loglik: -8.2317e+02 - logprior: 0.8812
Epoch 7/10
10/10 - 2s - loss: 818.8803 - loglik: -8.2042e+02 - logprior: 1.5377
Epoch 8/10
10/10 - 2s - loss: 816.8911 - loglik: -8.1884e+02 - logprior: 1.9445
Epoch 9/10
10/10 - 2s - loss: 815.1510 - loglik: -8.1743e+02 - logprior: 2.2746
Epoch 10/10
10/10 - 2s - loss: 815.1462 - loglik: -8.1771e+02 - logprior: 2.5648
Fitted a model with MAP estimate = -814.3288
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (61, 1), (65, 1), (68, 1), (78, 2), (79, 5), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 889.7712 - loglik: -8.1480e+02 - logprior: -7.4972e+01
Epoch 2/2
10/10 - 3s - loss: 824.9457 - loglik: -7.9775e+02 - logprior: -2.7195e+01
Fitted a model with MAP estimate = -813.4055
expansions: [(0, 2), (60, 1), (128, 1)]
discards: [  0  11  23 114]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 851.0842 - loglik: -7.9299e+02 - logprior: -5.8098e+01
Epoch 2/2
10/10 - 3s - loss: 795.7251 - loglik: -7.8570e+02 - logprior: -1.0030e+01
Fitted a model with MAP estimate = -786.9515
expansions: []
discards: [  0 141 162]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 859.9225 - loglik: -7.8902e+02 - logprior: -7.0906e+01
Epoch 2/10
10/10 - 3s - loss: 803.5499 - loglik: -7.8662e+02 - logprior: -1.6929e+01
Epoch 3/10
10/10 - 3s - loss: 784.6399 - loglik: -7.8336e+02 - logprior: -1.2827e+00
Epoch 4/10
10/10 - 3s - loss: 778.6144 - loglik: -7.8280e+02 - logprior: 4.1853
Epoch 5/10
10/10 - 3s - loss: 775.3313 - loglik: -7.8203e+02 - logprior: 6.6991
Epoch 6/10
10/10 - 3s - loss: 774.1750 - loglik: -7.8239e+02 - logprior: 8.2121
Epoch 7/10
10/10 - 3s - loss: 772.6285 - loglik: -7.8201e+02 - logprior: 9.3845
Epoch 8/10
10/10 - 3s - loss: 772.1885 - loglik: -7.8252e+02 - logprior: 10.3334
Epoch 9/10
10/10 - 3s - loss: 771.6520 - loglik: -7.8271e+02 - logprior: 11.0552
Epoch 10/10
10/10 - 3s - loss: 770.4302 - loglik: -7.8207e+02 - logprior: 11.6357
Fitted a model with MAP estimate = -770.7122
Time for alignment: 74.8401
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1022.8063 - loglik: -9.5606e+02 - logprior: -6.6743e+01
Epoch 2/10
10/10 - 2s - loss: 922.8097 - loglik: -9.0954e+02 - logprior: -1.3268e+01
Epoch 3/10
10/10 - 2s - loss: 869.8231 - loglik: -8.6555e+02 - logprior: -4.2740e+00
Epoch 4/10
10/10 - 2s - loss: 838.9386 - loglik: -8.3731e+02 - logprior: -1.6325e+00
Epoch 5/10
10/10 - 2s - loss: 826.3195 - loglik: -8.2602e+02 - logprior: -2.9507e-01
Epoch 6/10
10/10 - 2s - loss: 819.5909 - loglik: -8.2024e+02 - logprior: 0.6505
Epoch 7/10
10/10 - 2s - loss: 815.4402 - loglik: -8.1676e+02 - logprior: 1.3216
Epoch 8/10
10/10 - 2s - loss: 813.4081 - loglik: -8.1509e+02 - logprior: 1.6791
Epoch 9/10
10/10 - 2s - loss: 811.4477 - loglik: -8.1332e+02 - logprior: 1.8756
Epoch 10/10
10/10 - 2s - loss: 810.5329 - loglik: -8.1266e+02 - logprior: 2.1288
Fitted a model with MAP estimate = -810.2821
expansions: [(11, 3), (12, 2), (16, 3), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (55, 1), (66, 1), (68, 1), (71, 2), (77, 2), (79, 2), (80, 2), (101, 2), (103, 2), (110, 3), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 886.8770 - loglik: -8.1195e+02 - logprior: -7.4930e+01
Epoch 2/2
10/10 - 3s - loss: 820.0499 - loglik: -7.9270e+02 - logprior: -2.7355e+01
Fitted a model with MAP estimate = -808.0977
expansions: [(0, 2), (26, 1)]
discards: [  0  11  46  51  53  90 128 132 142]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 846.8896 - loglik: -7.8873e+02 - logprior: -5.8155e+01
Epoch 2/2
10/10 - 3s - loss: 793.4068 - loglik: -7.8338e+02 - logprior: -1.0026e+01
Fitted a model with MAP estimate = -784.5682
expansions: []
discards: [  0 162]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 857.7409 - loglik: -7.8660e+02 - logprior: -7.1140e+01
Epoch 2/10
10/10 - 3s - loss: 801.6367 - loglik: -7.8427e+02 - logprior: -1.7370e+01
Epoch 3/10
10/10 - 3s - loss: 783.4603 - loglik: -7.8214e+02 - logprior: -1.3243e+00
Epoch 4/10
10/10 - 3s - loss: 777.2077 - loglik: -7.8148e+02 - logprior: 4.2681
Epoch 5/10
10/10 - 3s - loss: 773.2852 - loglik: -7.8004e+02 - logprior: 6.7523
Epoch 6/10
10/10 - 3s - loss: 772.1112 - loglik: -7.8037e+02 - logprior: 8.2573
Epoch 7/10
10/10 - 3s - loss: 771.2596 - loglik: -7.8069e+02 - logprior: 9.4276
Epoch 8/10
10/10 - 3s - loss: 770.4355 - loglik: -7.8082e+02 - logprior: 10.3821
Epoch 9/10
10/10 - 3s - loss: 770.3461 - loglik: -7.8145e+02 - logprior: 11.1054
Epoch 10/10
10/10 - 3s - loss: 769.1936 - loglik: -7.8087e+02 - logprior: 11.6769
Fitted a model with MAP estimate = -769.0346
Time for alignment: 72.7541
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1022.5974 - loglik: -9.5585e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 2s - loss: 923.6896 - loglik: -9.1042e+02 - logprior: -1.3269e+01
Epoch 3/10
10/10 - 2s - loss: 871.1227 - loglik: -8.6682e+02 - logprior: -4.3052e+00
Epoch 4/10
10/10 - 2s - loss: 842.3107 - loglik: -8.4096e+02 - logprior: -1.3519e+00
Epoch 5/10
10/10 - 2s - loss: 829.2156 - loglik: -8.2926e+02 - logprior: 0.0482
Epoch 6/10
10/10 - 2s - loss: 821.3494 - loglik: -8.2223e+02 - logprior: 0.8811
Epoch 7/10
10/10 - 2s - loss: 817.5809 - loglik: -8.1907e+02 - logprior: 1.4879
Epoch 8/10
10/10 - 2s - loss: 815.6550 - loglik: -8.1748e+02 - logprior: 1.8205
Epoch 9/10
10/10 - 2s - loss: 813.8455 - loglik: -8.1602e+02 - logprior: 2.1712
Epoch 10/10
10/10 - 2s - loss: 813.0266 - loglik: -8.1543e+02 - logprior: 2.4083
Fitted a model with MAP estimate = -812.7650
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (49, 1), (51, 1), (61, 1), (65, 1), (68, 1), (71, 2), (77, 2), (79, 1), (80, 2), (90, 3), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 889.6947 - loglik: -8.1446e+02 - logprior: -7.5230e+01
Epoch 2/2
10/10 - 3s - loss: 824.2614 - loglik: -7.9648e+02 - logprior: -2.7785e+01
Fitted a model with MAP estimate = -812.2465
expansions: [(0, 2), (49, 1), (129, 1)]
discards: [  0  11  23  46  87 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 851.0817 - loglik: -7.9270e+02 - logprior: -5.8387e+01
Epoch 2/2
10/10 - 3s - loss: 796.3293 - loglik: -7.8603e+02 - logprior: -1.0298e+01
Fitted a model with MAP estimate = -787.3549
expansions: [(124, 1)]
discards: [  0 139]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 861.4129 - loglik: -7.8958e+02 - logprior: -7.1835e+01
Epoch 2/10
10/10 - 3s - loss: 804.5002 - loglik: -7.8567e+02 - logprior: -1.8827e+01
Epoch 3/10
10/10 - 3s - loss: 786.0279 - loglik: -7.8394e+02 - logprior: -2.0889e+00
Epoch 4/10
10/10 - 3s - loss: 778.5114 - loglik: -7.8249e+02 - logprior: 3.9798
Epoch 5/10
10/10 - 3s - loss: 775.5859 - loglik: -7.8213e+02 - logprior: 6.5487
Epoch 6/10
10/10 - 3s - loss: 773.9099 - loglik: -7.8197e+02 - logprior: 8.0561
Epoch 7/10
10/10 - 3s - loss: 772.9235 - loglik: -7.8214e+02 - logprior: 9.2119
Epoch 8/10
10/10 - 3s - loss: 771.8756 - loglik: -7.8202e+02 - logprior: 10.1434
Epoch 9/10
10/10 - 3s - loss: 771.6761 - loglik: -7.8255e+02 - logprior: 10.8719
Epoch 10/10
10/10 - 3s - loss: 771.2302 - loglik: -7.8268e+02 - logprior: 11.4468
Fitted a model with MAP estimate = -770.7499
Time for alignment: 73.6733
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1022.3492 - loglik: -9.5561e+02 - logprior: -6.6740e+01
Epoch 2/10
10/10 - 2s - loss: 923.3691 - loglik: -9.1010e+02 - logprior: -1.3270e+01
Epoch 3/10
10/10 - 2s - loss: 869.4460 - loglik: -8.6508e+02 - logprior: -4.3679e+00
Epoch 4/10
10/10 - 2s - loss: 841.0363 - loglik: -8.3946e+02 - logprior: -1.5808e+00
Epoch 5/10
10/10 - 2s - loss: 827.4235 - loglik: -8.2721e+02 - logprior: -2.1284e-01
Epoch 6/10
10/10 - 2s - loss: 821.4987 - loglik: -8.2211e+02 - logprior: 0.6117
Epoch 7/10
10/10 - 2s - loss: 817.5704 - loglik: -8.1888e+02 - logprior: 1.3075
Epoch 8/10
10/10 - 2s - loss: 815.6535 - loglik: -8.1743e+02 - logprior: 1.7735
Epoch 9/10
10/10 - 2s - loss: 813.3110 - loglik: -8.1539e+02 - logprior: 2.0792
Epoch 10/10
10/10 - 2s - loss: 813.8431 - loglik: -8.1617e+02 - logprior: 2.3246
Fitted a model with MAP estimate = -812.8159
expansions: [(11, 3), (12, 2), (16, 2), (24, 1), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (49, 1), (51, 1), (65, 1), (68, 2), (77, 3), (79, 2), (80, 1), (89, 1), (90, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 889.1407 - loglik: -8.1408e+02 - logprior: -7.5060e+01
Epoch 2/2
10/10 - 3s - loss: 822.6265 - loglik: -7.9540e+02 - logprior: -2.7231e+01
Fitted a model with MAP estimate = -810.6880
expansions: [(0, 1), (21, 1)]
discards: [ 0 11 84]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 848.8068 - loglik: -7.9070e+02 - logprior: -5.8104e+01
Epoch 2/2
10/10 - 3s - loss: 794.1144 - loglik: -7.8388e+02 - logprior: -1.0233e+01
Fitted a model with MAP estimate = -785.9206
expansions: [(81, 1)]
discards: [142 163]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 842.1590 - loglik: -7.8537e+02 - logprior: -5.6787e+01
Epoch 2/10
10/10 - 3s - loss: 791.8280 - loglik: -7.8265e+02 - logprior: -9.1754e+00
Epoch 3/10
10/10 - 3s - loss: 780.1292 - loglik: -7.8078e+02 - logprior: 0.6484
Epoch 4/10
10/10 - 3s - loss: 775.5305 - loglik: -7.8041e+02 - logprior: 4.8788
Epoch 5/10
10/10 - 3s - loss: 773.4392 - loglik: -7.8079e+02 - logprior: 7.3491
Epoch 6/10
10/10 - 3s - loss: 771.5980 - loglik: -7.8050e+02 - logprior: 8.9058
Epoch 7/10
10/10 - 3s - loss: 770.0424 - loglik: -7.7992e+02 - logprior: 9.8794
Epoch 8/10
10/10 - 3s - loss: 770.1844 - loglik: -7.8067e+02 - logprior: 10.4818
Fitted a model with MAP estimate = -769.2154
Time for alignment: 67.0114
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1022.1078 - loglik: -9.5537e+02 - logprior: -6.6741e+01
Epoch 2/10
10/10 - 2s - loss: 923.7614 - loglik: -9.1048e+02 - logprior: -1.3280e+01
Epoch 3/10
10/10 - 2s - loss: 869.8537 - loglik: -8.6549e+02 - logprior: -4.3643e+00
Epoch 4/10
10/10 - 2s - loss: 839.7590 - loglik: -8.3825e+02 - logprior: -1.5135e+00
Epoch 5/10
10/10 - 2s - loss: 825.8188 - loglik: -8.2573e+02 - logprior: -8.7354e-02
Epoch 6/10
10/10 - 2s - loss: 818.3902 - loglik: -8.1910e+02 - logprior: 0.7049
Epoch 7/10
10/10 - 2s - loss: 814.0845 - loglik: -8.1535e+02 - logprior: 1.2634
Epoch 8/10
10/10 - 2s - loss: 811.4031 - loglik: -8.1303e+02 - logprior: 1.6225
Epoch 9/10
10/10 - 2s - loss: 809.8495 - loglik: -8.1173e+02 - logprior: 1.8839
Epoch 10/10
10/10 - 2s - loss: 808.9000 - loglik: -8.1093e+02 - logprior: 2.0345
Fitted a model with MAP estimate = -808.7827
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 2), (39, 3), (55, 1), (66, 1), (68, 1), (71, 1), (77, 2), (79, 2), (80, 2), (89, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 884.8309 - loglik: -8.0967e+02 - logprior: -7.5165e+01
Epoch 2/2
10/10 - 3s - loss: 819.8027 - loglik: -7.9251e+02 - logprior: -2.7291e+01
Fitted a model with MAP estimate = -809.0630
expansions: [(0, 2)]
discards: [  0  11  23 101 162]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 848.0490 - loglik: -7.8969e+02 - logprior: -5.8358e+01
Epoch 2/2
10/10 - 3s - loss: 796.7250 - loglik: -7.8636e+02 - logprior: -1.0368e+01
Fitted a model with MAP estimate = -787.7362
expansions: []
discards: [  0  45 139]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 861.3120 - loglik: -7.9032e+02 - logprior: -7.0996e+01
Epoch 2/10
10/10 - 2s - loss: 805.2611 - loglik: -7.8838e+02 - logprior: -1.6878e+01
Epoch 3/10
10/10 - 2s - loss: 786.5151 - loglik: -7.8499e+02 - logprior: -1.5231e+00
Epoch 4/10
10/10 - 2s - loss: 781.0325 - loglik: -7.8483e+02 - logprior: 3.7956
Epoch 5/10
10/10 - 2s - loss: 777.9495 - loglik: -7.8422e+02 - logprior: 6.2668
Epoch 6/10
10/10 - 2s - loss: 776.5538 - loglik: -7.8432e+02 - logprior: 7.7685
Epoch 7/10
10/10 - 2s - loss: 775.8903 - loglik: -7.8482e+02 - logprior: 8.9255
Epoch 8/10
10/10 - 2s - loss: 774.3041 - loglik: -7.8417e+02 - logprior: 9.8697
Epoch 9/10
10/10 - 3s - loss: 774.0720 - loglik: -7.8467e+02 - logprior: 10.5941
Epoch 10/10
10/10 - 2s - loss: 773.6863 - loglik: -7.8485e+02 - logprior: 11.1611
Fitted a model with MAP estimate = -773.3246
Time for alignment: 70.4542
Computed alignments with likelihoods: ['-770.7122', '-769.0346', '-770.7499', '-769.2154', '-773.3246']
Best model has likelihood: -769.0346  (prior= 11.9534 )
time for generating output: 0.1850
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.8140111593304402
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 582.1584 - loglik: -5.7224e+02 - logprior: -9.9146e+00
Epoch 2/10
12/12 - 1s - loss: 545.6343 - loglik: -5.4322e+02 - logprior: -2.4100e+00
Epoch 3/10
12/12 - 1s - loss: 517.4626 - loglik: -5.1573e+02 - logprior: -1.7295e+00
Epoch 4/10
12/12 - 1s - loss: 506.9954 - loglik: -5.0519e+02 - logprior: -1.8074e+00
Epoch 5/10
12/12 - 1s - loss: 501.3434 - loglik: -4.9944e+02 - logprior: -1.9023e+00
Epoch 6/10
12/12 - 1s - loss: 498.4209 - loglik: -4.9656e+02 - logprior: -1.8611e+00
Epoch 7/10
12/12 - 1s - loss: 496.0587 - loglik: -4.9425e+02 - logprior: -1.8109e+00
Epoch 8/10
12/12 - 1s - loss: 496.6134 - loglik: -4.9481e+02 - logprior: -1.8052e+00
Fitted a model with MAP estimate = -495.1895
expansions: [(8, 1), (10, 3), (11, 1), (12, 2), (13, 1), (21, 1), (36, 3), (49, 1), (50, 3), (52, 1), (57, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 510.0161 - loglik: -4.9854e+02 - logprior: -1.1478e+01
Epoch 2/2
12/12 - 1s - loss: 489.2729 - loglik: -4.8442e+02 - logprior: -4.8573e+00
Fitted a model with MAP estimate = -486.2983
expansions: [(0, 3)]
discards: [ 0 16 46 75 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 492.7204 - loglik: -4.8374e+02 - logprior: -8.9811e+00
Epoch 2/2
12/12 - 1s - loss: 482.2233 - loglik: -4.7989e+02 - logprior: -2.3319e+00
Fitted a model with MAP estimate = -480.8379
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 493.7644 - loglik: -4.8253e+02 - logprior: -1.1235e+01
Epoch 2/10
12/12 - 1s - loss: 485.2344 - loglik: -4.8131e+02 - logprior: -3.9262e+00
Epoch 3/10
12/12 - 1s - loss: 479.7422 - loglik: -4.7802e+02 - logprior: -1.7261e+00
Epoch 4/10
12/12 - 1s - loss: 477.8320 - loglik: -4.7684e+02 - logprior: -9.9510e-01
Epoch 5/10
12/12 - 1s - loss: 475.7858 - loglik: -4.7491e+02 - logprior: -8.7123e-01
Epoch 6/10
12/12 - 1s - loss: 475.8914 - loglik: -4.7511e+02 - logprior: -7.8521e-01
Fitted a model with MAP estimate = -474.1212
Time for alignment: 45.6796
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 582.2059 - loglik: -5.7229e+02 - logprior: -9.9159e+00
Epoch 2/10
12/12 - 1s - loss: 545.0086 - loglik: -5.4261e+02 - logprior: -2.4024e+00
Epoch 3/10
12/12 - 1s - loss: 517.1081 - loglik: -5.1544e+02 - logprior: -1.6722e+00
Epoch 4/10
12/12 - 1s - loss: 506.9337 - loglik: -5.0528e+02 - logprior: -1.6505e+00
Epoch 5/10
12/12 - 1s - loss: 501.9424 - loglik: -5.0028e+02 - logprior: -1.6643e+00
Epoch 6/10
12/12 - 1s - loss: 501.3645 - loglik: -4.9976e+02 - logprior: -1.6023e+00
Epoch 7/10
12/12 - 1s - loss: 498.6256 - loglik: -4.9706e+02 - logprior: -1.5610e+00
Epoch 8/10
12/12 - 1s - loss: 498.8844 - loglik: -4.9731e+02 - logprior: -1.5701e+00
Fitted a model with MAP estimate = -498.3538
expansions: [(6, 3), (10, 4), (11, 1), (13, 1), (21, 1), (36, 4), (49, 2), (50, 3), (52, 1), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 509.4993 - loglik: -4.9808e+02 - logprior: -1.1415e+01
Epoch 2/2
12/12 - 1s - loss: 492.0636 - loglik: -4.8732e+02 - logprior: -4.7402e+00
Fitted a model with MAP estimate = -487.9186
expansions: [(0, 5)]
discards: [ 0 46 63 80]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 494.7493 - loglik: -4.8564e+02 - logprior: -9.1064e+00
Epoch 2/2
12/12 - 1s - loss: 483.9716 - loglik: -4.8167e+02 - logprior: -2.3057e+00
Fitted a model with MAP estimate = -481.9982
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 490.8182 - loglik: -4.8199e+02 - logprior: -8.8325e+00
Epoch 2/10
12/12 - 1s - loss: 482.5813 - loglik: -4.8021e+02 - logprior: -2.3696e+00
Epoch 3/10
12/12 - 1s - loss: 480.4028 - loglik: -4.7895e+02 - logprior: -1.4497e+00
Epoch 4/10
12/12 - 1s - loss: 478.0581 - loglik: -4.7699e+02 - logprior: -1.0729e+00
Epoch 5/10
12/12 - 1s - loss: 476.3546 - loglik: -4.7546e+02 - logprior: -8.9036e-01
Epoch 6/10
12/12 - 1s - loss: 475.2049 - loglik: -4.7439e+02 - logprior: -8.1825e-01
Epoch 7/10
12/12 - 1s - loss: 472.6591 - loglik: -4.7188e+02 - logprior: -7.7930e-01
Epoch 8/10
12/12 - 1s - loss: 472.3492 - loglik: -4.7158e+02 - logprior: -7.6739e-01
Epoch 9/10
12/12 - 1s - loss: 472.9079 - loglik: -4.7217e+02 - logprior: -7.3958e-01
Fitted a model with MAP estimate = -472.2796
Time for alignment: 49.3800
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 583.5105 - loglik: -5.7360e+02 - logprior: -9.9149e+00
Epoch 2/10
12/12 - 1s - loss: 543.6053 - loglik: -5.4120e+02 - logprior: -2.4047e+00
Epoch 3/10
12/12 - 1s - loss: 518.0493 - loglik: -5.1636e+02 - logprior: -1.6910e+00
Epoch 4/10
12/12 - 1s - loss: 509.0576 - loglik: -5.0739e+02 - logprior: -1.6639e+00
Epoch 5/10
12/12 - 1s - loss: 502.5168 - loglik: -5.0082e+02 - logprior: -1.6929e+00
Epoch 6/10
12/12 - 1s - loss: 499.2876 - loglik: -4.9761e+02 - logprior: -1.6783e+00
Epoch 7/10
12/12 - 1s - loss: 497.1366 - loglik: -4.9550e+02 - logprior: -1.6355e+00
Epoch 8/10
12/12 - 1s - loss: 495.8903 - loglik: -4.9423e+02 - logprior: -1.6596e+00
Epoch 9/10
12/12 - 1s - loss: 497.2260 - loglik: -4.9557e+02 - logprior: -1.6577e+00
Fitted a model with MAP estimate = -495.7659
expansions: [(6, 3), (10, 4), (13, 1), (20, 1), (36, 3), (49, 1), (50, 2), (52, 1), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 509.5273 - loglik: -4.9805e+02 - logprior: -1.1482e+01
Epoch 2/2
12/12 - 1s - loss: 489.9244 - loglik: -4.8514e+02 - logprior: -4.7833e+00
Fitted a model with MAP estimate = -486.7488
expansions: [(0, 4), (63, 1)]
discards: [ 0 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 493.4879 - loglik: -4.8433e+02 - logprior: -9.1623e+00
Epoch 2/2
12/12 - 1s - loss: 480.8929 - loglik: -4.7854e+02 - logprior: -2.3573e+00
Fitted a model with MAP estimate = -479.7284
expansions: []
discards: [ 1  2  3 49]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 490.5216 - loglik: -4.8166e+02 - logprior: -8.8626e+00
Epoch 2/10
12/12 - 1s - loss: 481.9060 - loglik: -4.7942e+02 - logprior: -2.4864e+00
Epoch 3/10
12/12 - 1s - loss: 479.7249 - loglik: -4.7822e+02 - logprior: -1.5059e+00
Epoch 4/10
12/12 - 1s - loss: 478.1299 - loglik: -4.7702e+02 - logprior: -1.1054e+00
Epoch 5/10
12/12 - 1s - loss: 477.4093 - loglik: -4.7649e+02 - logprior: -9.2347e-01
Epoch 6/10
12/12 - 1s - loss: 475.0425 - loglik: -4.7418e+02 - logprior: -8.6534e-01
Epoch 7/10
12/12 - 1s - loss: 475.8189 - loglik: -4.7498e+02 - logprior: -8.3919e-01
Fitted a model with MAP estimate = -474.1724
Time for alignment: 45.9661
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 582.9857 - loglik: -5.7307e+02 - logprior: -9.9165e+00
Epoch 2/10
12/12 - 1s - loss: 543.3392 - loglik: -5.4093e+02 - logprior: -2.4065e+00
Epoch 3/10
12/12 - 1s - loss: 516.4219 - loglik: -5.1471e+02 - logprior: -1.7079e+00
Epoch 4/10
12/12 - 1s - loss: 505.6038 - loglik: -5.0390e+02 - logprior: -1.7073e+00
Epoch 5/10
12/12 - 1s - loss: 501.0271 - loglik: -4.9930e+02 - logprior: -1.7257e+00
Epoch 6/10
12/12 - 1s - loss: 500.2671 - loglik: -4.9859e+02 - logprior: -1.6737e+00
Epoch 7/10
12/12 - 1s - loss: 498.1772 - loglik: -4.9655e+02 - logprior: -1.6305e+00
Epoch 8/10
12/12 - 1s - loss: 497.8003 - loglik: -4.9614e+02 - logprior: -1.6618e+00
Epoch 9/10
12/12 - 1s - loss: 497.2994 - loglik: -4.9562e+02 - logprior: -1.6748e+00
Epoch 10/10
12/12 - 1s - loss: 496.9676 - loglik: -4.9531e+02 - logprior: -1.6578e+00
Fitted a model with MAP estimate = -497.0445
expansions: [(6, 3), (9, 2), (10, 2), (12, 2), (13, 1), (21, 1), (36, 3), (49, 4), (57, 1), (58, 2), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 511.8569 - loglik: -5.0041e+02 - logprior: -1.1446e+01
Epoch 2/2
12/12 - 1s - loss: 492.2786 - loglik: -4.8743e+02 - logprior: -4.8462e+00
Fitted a model with MAP estimate = -488.8925
expansions: [(0, 6), (65, 1)]
discards: [ 0 12 14 18 48 76 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 496.6115 - loglik: -4.8743e+02 - logprior: -9.1852e+00
Epoch 2/2
12/12 - 1s - loss: 485.4577 - loglik: -4.8310e+02 - logprior: -2.3594e+00
Fitted a model with MAP estimate = -483.5282
expansions: []
discards: [1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 492.1921 - loglik: -4.8335e+02 - logprior: -8.8383e+00
Epoch 2/10
12/12 - 1s - loss: 484.5760 - loglik: -4.8220e+02 - logprior: -2.3795e+00
Epoch 3/10
12/12 - 1s - loss: 482.7179 - loglik: -4.8125e+02 - logprior: -1.4691e+00
Epoch 4/10
12/12 - 1s - loss: 480.2804 - loglik: -4.7924e+02 - logprior: -1.0454e+00
Epoch 5/10
12/12 - 1s - loss: 479.4517 - loglik: -4.7855e+02 - logprior: -8.9786e-01
Epoch 6/10
12/12 - 1s - loss: 478.9595 - loglik: -4.7815e+02 - logprior: -8.1282e-01
Epoch 7/10
12/12 - 1s - loss: 476.5289 - loglik: -4.7573e+02 - logprior: -7.9446e-01
Epoch 8/10
12/12 - 1s - loss: 475.9949 - loglik: -4.7521e+02 - logprior: -7.8114e-01
Epoch 9/10
12/12 - 1s - loss: 476.4622 - loglik: -4.7571e+02 - logprior: -7.4801e-01
Fitted a model with MAP estimate = -475.9262
Time for alignment: 49.1928
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 582.9763 - loglik: -5.7306e+02 - logprior: -9.9168e+00
Epoch 2/10
12/12 - 1s - loss: 543.4509 - loglik: -5.4105e+02 - logprior: -2.3976e+00
Epoch 3/10
12/12 - 1s - loss: 517.3640 - loglik: -5.1571e+02 - logprior: -1.6538e+00
Epoch 4/10
12/12 - 1s - loss: 507.2084 - loglik: -5.0558e+02 - logprior: -1.6273e+00
Epoch 5/10
12/12 - 1s - loss: 502.3206 - loglik: -5.0067e+02 - logprior: -1.6544e+00
Epoch 6/10
12/12 - 1s - loss: 499.7596 - loglik: -4.9818e+02 - logprior: -1.5819e+00
Epoch 7/10
12/12 - 1s - loss: 499.8906 - loglik: -4.9835e+02 - logprior: -1.5362e+00
Fitted a model with MAP estimate = -498.5352
expansions: [(7, 2), (8, 1), (10, 4), (11, 2), (21, 1), (36, 4), (49, 1), (50, 3), (58, 3), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 511.5056 - loglik: -5.0014e+02 - logprior: -1.1362e+01
Epoch 2/2
12/12 - 1s - loss: 490.7994 - loglik: -4.8609e+02 - logprior: -4.7096e+00
Fitted a model with MAP estimate = -487.4300
expansions: [(0, 5), (41, 1), (47, 1)]
discards: [ 0 14 15 76 77 79]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 494.3087 - loglik: -4.8530e+02 - logprior: -9.0114e+00
Epoch 2/2
12/12 - 1s - loss: 484.1875 - loglik: -4.8199e+02 - logprior: -2.1941e+00
Fitted a model with MAP estimate = -481.2174
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 490.6219 - loglik: -4.8188e+02 - logprior: -8.7444e+00
Epoch 2/10
12/12 - 1s - loss: 482.5744 - loglik: -4.8032e+02 - logprior: -2.2544e+00
Epoch 3/10
12/12 - 1s - loss: 480.6342 - loglik: -4.7926e+02 - logprior: -1.3730e+00
Epoch 4/10
12/12 - 1s - loss: 477.9594 - loglik: -4.7700e+02 - logprior: -9.5556e-01
Epoch 5/10
12/12 - 1s - loss: 476.8456 - loglik: -4.7602e+02 - logprior: -8.2911e-01
Epoch 6/10
12/12 - 1s - loss: 475.3541 - loglik: -4.7461e+02 - logprior: -7.4294e-01
Epoch 7/10
12/12 - 1s - loss: 474.2402 - loglik: -4.7351e+02 - logprior: -7.3444e-01
Epoch 8/10
12/12 - 1s - loss: 474.1699 - loglik: -4.7346e+02 - logprior: -7.1368e-01
Epoch 9/10
12/12 - 1s - loss: 474.2203 - loglik: -4.7354e+02 - logprior: -6.8435e-01
Fitted a model with MAP estimate = -473.5116
Time for alignment: 46.6481
Computed alignments with likelihoods: ['-474.1212', '-472.2796', '-474.1724', '-475.9262', '-473.5116']
Best model has likelihood: -472.2796  (prior= -0.7371 )
time for generating output: 0.1757
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8986556630521072
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.1791 - loglik: -5.8295e+02 - logprior: -4.0228e+01
Epoch 2/10
10/10 - 1s - loss: 571.5816 - loglik: -5.6152e+02 - logprior: -1.0062e+01
Epoch 3/10
10/10 - 1s - loss: 543.1682 - loglik: -5.3898e+02 - logprior: -4.1841e+00
Epoch 4/10
10/10 - 1s - loss: 531.1423 - loglik: -5.2906e+02 - logprior: -2.0793e+00
Epoch 5/10
10/10 - 1s - loss: 525.5634 - loglik: -5.2444e+02 - logprior: -1.1263e+00
Epoch 6/10
10/10 - 1s - loss: 522.8803 - loglik: -5.2217e+02 - logprior: -7.0808e-01
Epoch 7/10
10/10 - 1s - loss: 520.8580 - loglik: -5.2039e+02 - logprior: -4.7076e-01
Epoch 8/10
10/10 - 1s - loss: 519.6429 - loglik: -5.1939e+02 - logprior: -2.5565e-01
Epoch 9/10
10/10 - 1s - loss: 519.6398 - loglik: -5.1957e+02 - logprior: -6.9546e-02
Epoch 10/10
10/10 - 1s - loss: 519.4506 - loglik: -5.1951e+02 - logprior: 0.0571
Fitted a model with MAP estimate = -518.9789
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (37, 2), (43, 6), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 569.3823 - loglik: -5.1710e+02 - logprior: -5.2279e+01
Epoch 2/2
10/10 - 1s - loss: 526.8940 - loglik: -5.1158e+02 - logprior: -1.5316e+01
Fitted a model with MAP estimate = -520.4032
expansions: [(7, 1)]
discards: [ 0  1 68]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 558.2529 - loglik: -5.1236e+02 - logprior: -4.5890e+01
Epoch 2/2
10/10 - 1s - loss: 529.6980 - loglik: -5.1206e+02 - logprior: -1.7643e+01
Fitted a model with MAP estimate = -524.4444
expansions: [(0, 3)]
discards: [ 0 40 53 54 55]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 552.9956 - loglik: -5.1264e+02 - logprior: -4.0353e+01
Epoch 2/10
10/10 - 1s - loss: 521.4093 - loglik: -5.1145e+02 - logprior: -9.9600e+00
Epoch 3/10
10/10 - 1s - loss: 514.2460 - loglik: -5.1095e+02 - logprior: -3.3002e+00
Epoch 4/10
10/10 - 1s - loss: 512.3696 - loglik: -5.1156e+02 - logprior: -8.1173e-01
Epoch 5/10
10/10 - 1s - loss: 510.7874 - loglik: -5.1121e+02 - logprior: 0.4209
Epoch 6/10
10/10 - 1s - loss: 510.8388 - loglik: -5.1194e+02 - logprior: 1.1014
Fitted a model with MAP estimate = -510.3101
Time for alignment: 37.2649
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.7452 - loglik: -5.8351e+02 - logprior: -4.0233e+01
Epoch 2/10
10/10 - 1s - loss: 570.4024 - loglik: -5.6033e+02 - logprior: -1.0072e+01
Epoch 3/10
10/10 - 1s - loss: 544.6885 - loglik: -5.4051e+02 - logprior: -4.1779e+00
Epoch 4/10
10/10 - 1s - loss: 532.0175 - loglik: -5.2998e+02 - logprior: -2.0403e+00
Epoch 5/10
10/10 - 1s - loss: 526.1183 - loglik: -5.2502e+02 - logprior: -1.0949e+00
Epoch 6/10
10/10 - 1s - loss: 523.0073 - loglik: -5.2228e+02 - logprior: -7.3129e-01
Epoch 7/10
10/10 - 1s - loss: 521.7343 - loglik: -5.2122e+02 - logprior: -5.1817e-01
Epoch 8/10
10/10 - 1s - loss: 521.3657 - loglik: -5.2108e+02 - logprior: -2.8583e-01
Epoch 9/10
10/10 - 1s - loss: 519.1017 - loglik: -5.1899e+02 - logprior: -1.1276e-01
Epoch 10/10
10/10 - 1s - loss: 519.8182 - loglik: -5.1981e+02 - logprior: -1.2319e-02
Fitted a model with MAP estimate = -519.2271
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 568.8004 - loglik: -5.1667e+02 - logprior: -5.2132e+01
Epoch 2/2
10/10 - 1s - loss: 527.1849 - loglik: -5.1195e+02 - logprior: -1.5232e+01
Fitted a model with MAP estimate = -519.6999
expansions: []
discards: [ 0  1 11 69]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 557.9145 - loglik: -5.1215e+02 - logprior: -4.5762e+01
Epoch 2/2
10/10 - 1s - loss: 529.4966 - loglik: -5.1191e+02 - logprior: -1.7586e+01
Fitted a model with MAP estimate = -524.7092
expansions: [(0, 3)]
discards: [ 0 40 53 54]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 553.2971 - loglik: -5.1283e+02 - logprior: -4.0466e+01
Epoch 2/10
10/10 - 1s - loss: 520.9199 - loglik: -5.1091e+02 - logprior: -1.0006e+01
Epoch 3/10
10/10 - 1s - loss: 514.8362 - loglik: -5.1154e+02 - logprior: -3.2993e+00
Epoch 4/10
10/10 - 1s - loss: 511.9225 - loglik: -5.1111e+02 - logprior: -8.0967e-01
Epoch 5/10
10/10 - 1s - loss: 511.5792 - loglik: -5.1201e+02 - logprior: 0.4284
Epoch 6/10
10/10 - 1s - loss: 510.1219 - loglik: -5.1122e+02 - logprior: 1.1028
Epoch 7/10
10/10 - 1s - loss: 510.4427 - loglik: -5.1192e+02 - logprior: 1.4761
Fitted a model with MAP estimate = -510.0669
Time for alignment: 38.4823
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 624.0251 - loglik: -5.8379e+02 - logprior: -4.0233e+01
Epoch 2/10
10/10 - 1s - loss: 570.2858 - loglik: -5.6022e+02 - logprior: -1.0071e+01
Epoch 3/10
10/10 - 1s - loss: 545.9714 - loglik: -5.4178e+02 - logprior: -4.1916e+00
Epoch 4/10
10/10 - 1s - loss: 532.6360 - loglik: -5.3053e+02 - logprior: -2.1025e+00
Epoch 5/10
10/10 - 1s - loss: 526.3949 - loglik: -5.2523e+02 - logprior: -1.1694e+00
Epoch 6/10
10/10 - 1s - loss: 524.0188 - loglik: -5.2327e+02 - logprior: -7.4411e-01
Epoch 7/10
10/10 - 1s - loss: 522.3414 - loglik: -5.2182e+02 - logprior: -5.2357e-01
Epoch 8/10
10/10 - 1s - loss: 521.3279 - loglik: -5.2098e+02 - logprior: -3.4343e-01
Epoch 9/10
10/10 - 1s - loss: 520.9244 - loglik: -5.2071e+02 - logprior: -2.1720e-01
Epoch 10/10
10/10 - 1s - loss: 520.1789 - loglik: -5.1999e+02 - logprior: -1.9037e-01
Fitted a model with MAP estimate = -519.6299
expansions: [(0, 3), (6, 1), (7, 1), (8, 1), (37, 2), (43, 6), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 569.5917 - loglik: -5.1726e+02 - logprior: -5.2334e+01
Epoch 2/2
10/10 - 1s - loss: 528.3254 - loglik: -5.1299e+02 - logprior: -1.5337e+01
Fitted a model with MAP estimate = -520.9332
expansions: []
discards: [ 0  1 68]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 559.3712 - loglik: -5.1341e+02 - logprior: -4.5958e+01
Epoch 2/2
10/10 - 1s - loss: 529.4576 - loglik: -5.1170e+02 - logprior: -1.7753e+01
Fitted a model with MAP estimate = -525.3638
expansions: [(0, 3)]
discards: [ 0 52 53]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 553.2578 - loglik: -5.1257e+02 - logprior: -4.0688e+01
Epoch 2/10
10/10 - 1s - loss: 521.7874 - loglik: -5.1157e+02 - logprior: -1.0220e+01
Epoch 3/10
10/10 - 1s - loss: 515.2759 - loglik: -5.1178e+02 - logprior: -3.4923e+00
Epoch 4/10
10/10 - 1s - loss: 512.1602 - loglik: -5.1118e+02 - logprior: -9.7667e-01
Epoch 5/10
10/10 - 1s - loss: 512.1719 - loglik: -5.1246e+02 - logprior: 0.2858
Fitted a model with MAP estimate = -511.0816
Time for alignment: 34.4266
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.6776 - loglik: -5.8345e+02 - logprior: -4.0230e+01
Epoch 2/10
10/10 - 1s - loss: 570.2426 - loglik: -5.6018e+02 - logprior: -1.0064e+01
Epoch 3/10
10/10 - 1s - loss: 544.5433 - loglik: -5.4039e+02 - logprior: -4.1550e+00
Epoch 4/10
10/10 - 1s - loss: 530.8945 - loglik: -5.2886e+02 - logprior: -2.0378e+00
Epoch 5/10
10/10 - 1s - loss: 525.4104 - loglik: -5.2429e+02 - logprior: -1.1237e+00
Epoch 6/10
10/10 - 1s - loss: 523.1444 - loglik: -5.2242e+02 - logprior: -7.2539e-01
Epoch 7/10
10/10 - 1s - loss: 521.1283 - loglik: -5.2062e+02 - logprior: -5.1288e-01
Epoch 8/10
10/10 - 1s - loss: 520.2443 - loglik: -5.1992e+02 - logprior: -3.2604e-01
Epoch 9/10
10/10 - 1s - loss: 519.4448 - loglik: -5.1927e+02 - logprior: -1.7217e-01
Epoch 10/10
10/10 - 1s - loss: 519.7809 - loglik: -5.1973e+02 - logprior: -4.9455e-02
Fitted a model with MAP estimate = -519.0693
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 568.6024 - loglik: -5.1643e+02 - logprior: -5.2171e+01
Epoch 2/2
10/10 - 1s - loss: 526.9095 - loglik: -5.1166e+02 - logprior: -1.5252e+01
Fitted a model with MAP estimate = -519.8404
expansions: []
discards: [ 0  1 11 56 57 69]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 559.2836 - loglik: -5.1334e+02 - logprior: -4.5943e+01
Epoch 2/2
10/10 - 1s - loss: 530.0333 - loglik: -5.1245e+02 - logprior: -1.7585e+01
Fitted a model with MAP estimate = -525.2543
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 552.1700 - loglik: -5.1149e+02 - logprior: -4.0681e+01
Epoch 2/10
10/10 - 1s - loss: 521.2956 - loglik: -5.1117e+02 - logprior: -1.0124e+01
Epoch 3/10
10/10 - 1s - loss: 513.5232 - loglik: -5.1016e+02 - logprior: -3.3681e+00
Epoch 4/10
10/10 - 1s - loss: 512.1625 - loglik: -5.1129e+02 - logprior: -8.7005e-01
Epoch 5/10
10/10 - 1s - loss: 510.5889 - loglik: -5.1096e+02 - logprior: 0.3739
Epoch 6/10
10/10 - 1s - loss: 510.4538 - loglik: -5.1151e+02 - logprior: 1.0539
Epoch 7/10
10/10 - 1s - loss: 509.2381 - loglik: -5.1066e+02 - logprior: 1.4263
Epoch 8/10
10/10 - 1s - loss: 509.4106 - loglik: -5.1109e+02 - logprior: 1.6744
Fitted a model with MAP estimate = -509.2624
Time for alignment: 37.8115
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.7432 - loglik: -5.8351e+02 - logprior: -4.0233e+01
Epoch 2/10
10/10 - 1s - loss: 570.2062 - loglik: -5.6013e+02 - logprior: -1.0079e+01
Epoch 3/10
10/10 - 1s - loss: 545.7170 - loglik: -5.4149e+02 - logprior: -4.2258e+00
Epoch 4/10
10/10 - 1s - loss: 531.2318 - loglik: -5.2909e+02 - logprior: -2.1424e+00
Epoch 5/10
10/10 - 1s - loss: 526.8791 - loglik: -5.2565e+02 - logprior: -1.2244e+00
Epoch 6/10
10/10 - 1s - loss: 524.1736 - loglik: -5.2337e+02 - logprior: -8.0550e-01
Epoch 7/10
10/10 - 1s - loss: 522.3112 - loglik: -5.2178e+02 - logprior: -5.3429e-01
Epoch 8/10
10/10 - 1s - loss: 521.8777 - loglik: -5.2156e+02 - logprior: -3.1663e-01
Epoch 9/10
10/10 - 1s - loss: 520.6165 - loglik: -5.2042e+02 - logprior: -1.9532e-01
Epoch 10/10
10/10 - 1s - loss: 519.7038 - loglik: -5.1955e+02 - logprior: -1.5333e-01
Fitted a model with MAP estimate = -519.9362
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (37, 2), (44, 9), (51, 1), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 570.1785 - loglik: -5.1813e+02 - logprior: -5.2052e+01
Epoch 2/2
10/10 - 1s - loss: 526.3485 - loglik: -5.1094e+02 - logprior: -1.5413e+01
Fitted a model with MAP estimate = -519.0595
expansions: [(43, 1)]
discards: [ 0  1  9 73]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 557.2252 - loglik: -5.1164e+02 - logprior: -4.5590e+01
Epoch 2/2
10/10 - 1s - loss: 527.0970 - loglik: -5.0955e+02 - logprior: -1.7546e+01
Fitted a model with MAP estimate = -522.9788
expansions: [(0, 3)]
discards: [ 0 53]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.2365 - loglik: -5.1053e+02 - logprior: -4.0705e+01
Epoch 2/10
10/10 - 1s - loss: 518.7522 - loglik: -5.0857e+02 - logprior: -1.0180e+01
Epoch 3/10
10/10 - 1s - loss: 512.4415 - loglik: -5.0909e+02 - logprior: -3.3490e+00
Epoch 4/10
10/10 - 1s - loss: 509.9887 - loglik: -5.0917e+02 - logprior: -8.2143e-01
Epoch 5/10
10/10 - 1s - loss: 508.3633 - loglik: -5.0881e+02 - logprior: 0.4429
Epoch 6/10
10/10 - 1s - loss: 508.5356 - loglik: -5.0967e+02 - logprior: 1.1349
Fitted a model with MAP estimate = -507.8251
Time for alignment: 37.2120
Computed alignments with likelihoods: ['-510.3101', '-510.0669', '-511.0816', '-509.2624', '-507.8251']
Best model has likelihood: -507.8251  (prior= 1.3666 )
time for generating output: 0.1831
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7305359606424239
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1053.7268 - loglik: -9.7595e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 922.3318 - loglik: -9.0711e+02 - logprior: -1.5221e+01
Epoch 3/10
10/10 - 2s - loss: 843.7523 - loglik: -8.3840e+02 - logprior: -5.3510e+00
Epoch 4/10
10/10 - 2s - loss: 796.9348 - loglik: -7.9355e+02 - logprior: -3.3834e+00
Epoch 5/10
10/10 - 2s - loss: 778.2877 - loglik: -7.7542e+02 - logprior: -2.8639e+00
Epoch 6/10
10/10 - 2s - loss: 771.3474 - loglik: -7.6954e+02 - logprior: -1.8115e+00
Epoch 7/10
10/10 - 2s - loss: 768.1623 - loglik: -7.6738e+02 - logprior: -7.8129e-01
Epoch 8/10
10/10 - 2s - loss: 766.6393 - loglik: -7.6626e+02 - logprior: -3.8355e-01
Epoch 9/10
10/10 - 2s - loss: 765.5536 - loglik: -7.6554e+02 - logprior: -1.0386e-02
Epoch 10/10
10/10 - 2s - loss: 763.9681 - loglik: -7.6441e+02 - logprior: 0.4458
Fitted a model with MAP estimate = -764.6676
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 847.2962 - loglik: -7.5881e+02 - logprior: -8.8481e+01
Epoch 2/2
10/10 - 2s - loss: 771.6913 - loglik: -7.3967e+02 - logprior: -3.2024e+01
Fitted a model with MAP estimate = -759.3958
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 802.9246 - loglik: -7.3351e+02 - logprior: -6.9418e+01
Epoch 2/2
10/10 - 2s - loss: 737.0010 - loglik: -7.2518e+02 - logprior: -1.1826e+01
Fitted a model with MAP estimate = -727.2806
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 812.0600 - loglik: -7.2728e+02 - logprior: -8.4778e+01
Epoch 2/10
10/10 - 2s - loss: 749.2262 - loglik: -7.2386e+02 - logprior: -2.5362e+01
Epoch 3/10
10/10 - 2s - loss: 728.1406 - loglik: -7.2379e+02 - logprior: -4.3463e+00
Epoch 4/10
10/10 - 2s - loss: 717.0718 - loglik: -7.2302e+02 - logprior: 5.9498
Epoch 5/10
10/10 - 2s - loss: 714.9481 - loglik: -7.2457e+02 - logprior: 9.6261
Epoch 6/10
10/10 - 2s - loss: 709.4398 - loglik: -7.2097e+02 - logprior: 11.5334
Epoch 7/10
10/10 - 2s - loss: 710.1608 - loglik: -7.2295e+02 - logprior: 12.7928
Fitted a model with MAP estimate = -709.5306
Time for alignment: 62.4299
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1053.7196 - loglik: -9.7595e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 921.7872 - loglik: -9.0657e+02 - logprior: -1.5218e+01
Epoch 3/10
10/10 - 2s - loss: 844.5287 - loglik: -8.3919e+02 - logprior: -5.3406e+00
Epoch 4/10
10/10 - 2s - loss: 795.7748 - loglik: -7.9243e+02 - logprior: -3.3456e+00
Epoch 5/10
10/10 - 2s - loss: 777.9374 - loglik: -7.7528e+02 - logprior: -2.6621e+00
Epoch 6/10
10/10 - 2s - loss: 770.8843 - loglik: -7.6903e+02 - logprior: -1.8529e+00
Epoch 7/10
10/10 - 2s - loss: 767.5152 - loglik: -7.6649e+02 - logprior: -1.0279e+00
Epoch 8/10
10/10 - 2s - loss: 766.1089 - loglik: -7.6558e+02 - logprior: -5.2571e-01
Epoch 9/10
10/10 - 2s - loss: 764.6021 - loglik: -7.6437e+02 - logprior: -2.3403e-01
Epoch 10/10
10/10 - 2s - loss: 763.9642 - loglik: -7.6417e+02 - logprior: 0.2100
Fitted a model with MAP estimate = -763.8830
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 848.5397 - loglik: -7.6003e+02 - logprior: -8.8506e+01
Epoch 2/2
10/10 - 2s - loss: 770.7294 - loglik: -7.3905e+02 - logprior: -3.1680e+01
Fitted a model with MAP estimate = -760.1276
expansions: [(0, 3), (15, 3), (16, 1), (85, 1)]
discards: [  0  43 105 118]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 804.0724 - loglik: -7.3484e+02 - logprior: -6.9229e+01
Epoch 2/2
10/10 - 2s - loss: 739.0782 - loglik: -7.2745e+02 - logprior: -1.1633e+01
Fitted a model with MAP estimate = -728.6441
expansions: [(16, 1), (17, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 812.3669 - loglik: -7.2754e+02 - logprior: -8.4822e+01
Epoch 2/10
10/10 - 2s - loss: 750.9277 - loglik: -7.2528e+02 - logprior: -2.5651e+01
Epoch 3/10
10/10 - 2s - loss: 727.8459 - loglik: -7.2312e+02 - logprior: -4.7302e+00
Epoch 4/10
10/10 - 2s - loss: 719.0330 - loglik: -7.2489e+02 - logprior: 5.8612
Epoch 5/10
10/10 - 2s - loss: 712.6022 - loglik: -7.2221e+02 - logprior: 9.6043
Epoch 6/10
10/10 - 2s - loss: 710.0465 - loglik: -7.2157e+02 - logprior: 11.5189
Epoch 7/10
10/10 - 2s - loss: 711.5309 - loglik: -7.2430e+02 - logprior: 12.7700
Fitted a model with MAP estimate = -709.5823
Time for alignment: 61.5099
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1053.7881 - loglik: -9.7602e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 2s - loss: 922.2936 - loglik: -9.0707e+02 - logprior: -1.5220e+01
Epoch 3/10
10/10 - 2s - loss: 843.3469 - loglik: -8.3800e+02 - logprior: -5.3457e+00
Epoch 4/10
10/10 - 2s - loss: 796.7272 - loglik: -7.9342e+02 - logprior: -3.3095e+00
Epoch 5/10
10/10 - 2s - loss: 779.3308 - loglik: -7.7691e+02 - logprior: -2.4197e+00
Epoch 6/10
10/10 - 2s - loss: 773.1151 - loglik: -7.7180e+02 - logprior: -1.3176e+00
Epoch 7/10
10/10 - 2s - loss: 768.8053 - loglik: -7.6818e+02 - logprior: -6.2380e-01
Epoch 8/10
10/10 - 2s - loss: 767.4229 - loglik: -7.6708e+02 - logprior: -3.4462e-01
Epoch 9/10
10/10 - 2s - loss: 765.1491 - loglik: -7.6517e+02 - logprior: 0.0223
Epoch 10/10
10/10 - 2s - loss: 764.9362 - loglik: -7.6544e+02 - logprior: 0.5067
Fitted a model with MAP estimate = -764.8499
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 848.0063 - loglik: -7.5954e+02 - logprior: -8.8466e+01
Epoch 2/2
10/10 - 2s - loss: 773.5172 - loglik: -7.4166e+02 - logprior: -3.1856e+01
Fitted a model with MAP estimate = -759.6503
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 802.9137 - loglik: -7.3349e+02 - logprior: -6.9424e+01
Epoch 2/2
10/10 - 2s - loss: 738.0304 - loglik: -7.2624e+02 - logprior: -1.1793e+01
Fitted a model with MAP estimate = -727.2148
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 811.1335 - loglik: -7.2636e+02 - logprior: -8.4776e+01
Epoch 2/10
10/10 - 2s - loss: 750.0210 - loglik: -7.2463e+02 - logprior: -2.5396e+01
Epoch 3/10
10/10 - 2s - loss: 727.1131 - loglik: -7.2273e+02 - logprior: -4.3877e+00
Epoch 4/10
10/10 - 2s - loss: 718.3034 - loglik: -7.2425e+02 - logprior: 5.9443
Epoch 5/10
10/10 - 2s - loss: 712.7253 - loglik: -7.2235e+02 - logprior: 9.6216
Epoch 6/10
10/10 - 2s - loss: 710.1498 - loglik: -7.2167e+02 - logprior: 11.5247
Epoch 7/10
10/10 - 2s - loss: 713.0085 - loglik: -7.2581e+02 - logprior: 12.8014
Fitted a model with MAP estimate = -709.4158
Time for alignment: 62.7887
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1053.8376 - loglik: -9.7607e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 921.6668 - loglik: -9.0645e+02 - logprior: -1.5218e+01
Epoch 3/10
10/10 - 2s - loss: 844.5988 - loglik: -8.3927e+02 - logprior: -5.3283e+00
Epoch 4/10
10/10 - 2s - loss: 796.7850 - loglik: -7.9345e+02 - logprior: -3.3400e+00
Epoch 5/10
10/10 - 2s - loss: 778.8643 - loglik: -7.7624e+02 - logprior: -2.6230e+00
Epoch 6/10
10/10 - 2s - loss: 771.8049 - loglik: -7.7015e+02 - logprior: -1.6510e+00
Epoch 7/10
10/10 - 2s - loss: 768.7958 - loglik: -7.6811e+02 - logprior: -6.8491e-01
Epoch 8/10
10/10 - 2s - loss: 766.6696 - loglik: -7.6647e+02 - logprior: -1.9663e-01
Epoch 9/10
10/10 - 2s - loss: 765.4928 - loglik: -7.6562e+02 - logprior: 0.1228
Epoch 10/10
10/10 - 2s - loss: 765.7755 - loglik: -7.6631e+02 - logprior: 0.5357
Fitted a model with MAP estimate = -765.0123
expansions: [(21, 1), (22, 1), (23, 1), (26, 1), (39, 2), (41, 3), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 849.4020 - loglik: -7.6095e+02 - logprior: -8.8454e+01
Epoch 2/2
10/10 - 2s - loss: 772.2077 - loglik: -7.4024e+02 - logprior: -3.1963e+01
Fitted a model with MAP estimate = -760.7413
expansions: [(0, 3), (15, 4), (87, 1)]
discards: [  0  43  47  54 107 120]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 803.8176 - loglik: -7.3451e+02 - logprior: -6.9312e+01
Epoch 2/2
10/10 - 2s - loss: 738.6651 - loglik: -7.2689e+02 - logprior: -1.1775e+01
Fitted a model with MAP estimate = -727.8028
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 811.5414 - loglik: -7.2670e+02 - logprior: -8.4845e+01
Epoch 2/10
10/10 - 2s - loss: 751.0695 - loglik: -7.2542e+02 - logprior: -2.5650e+01
Epoch 3/10
10/10 - 2s - loss: 728.0397 - loglik: -7.2327e+02 - logprior: -4.7741e+00
Epoch 4/10
10/10 - 2s - loss: 718.4519 - loglik: -7.2431e+02 - logprior: 5.8620
Epoch 5/10
10/10 - 2s - loss: 711.2192 - loglik: -7.2083e+02 - logprior: 9.6117
Epoch 6/10
10/10 - 2s - loss: 713.0956 - loglik: -7.2462e+02 - logprior: 11.5214
Fitted a model with MAP estimate = -710.5285
Time for alignment: 59.7302
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1053.8824 - loglik: -9.7611e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 2s - loss: 922.6173 - loglik: -9.0739e+02 - logprior: -1.5223e+01
Epoch 3/10
10/10 - 2s - loss: 845.4116 - loglik: -8.4006e+02 - logprior: -5.3552e+00
Epoch 4/10
10/10 - 2s - loss: 795.5760 - loglik: -7.9219e+02 - logprior: -3.3854e+00
Epoch 5/10
10/10 - 2s - loss: 778.7198 - loglik: -7.7586e+02 - logprior: -2.8593e+00
Epoch 6/10
10/10 - 2s - loss: 771.9733 - loglik: -7.7024e+02 - logprior: -1.7344e+00
Epoch 7/10
10/10 - 2s - loss: 769.3726 - loglik: -7.6858e+02 - logprior: -7.9524e-01
Epoch 8/10
10/10 - 2s - loss: 766.9518 - loglik: -7.6660e+02 - logprior: -3.5577e-01
Epoch 9/10
10/10 - 2s - loss: 764.2010 - loglik: -7.6416e+02 - logprior: -4.3348e-02
Epoch 10/10
10/10 - 2s - loss: 765.0336 - loglik: -7.6543e+02 - logprior: 0.3996
Fitted a model with MAP estimate = -764.7164
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (122, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 847.3386 - loglik: -7.5886e+02 - logprior: -8.8480e+01
Epoch 2/2
10/10 - 2s - loss: 772.9088 - loglik: -7.4092e+02 - logprior: -3.1988e+01
Fitted a model with MAP estimate = -759.4560
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [  0  43  53 106 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 802.0584 - loglik: -7.3265e+02 - logprior: -6.9405e+01
Epoch 2/2
10/10 - 2s - loss: 739.2828 - loglik: -7.2746e+02 - logprior: -1.1819e+01
Fitted a model with MAP estimate = -727.3242
expansions: [(18, 1)]
discards: [ 0  1 22]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 811.7537 - loglik: -7.2696e+02 - logprior: -8.4792e+01
Epoch 2/10
10/10 - 2s - loss: 750.7708 - loglik: -7.2542e+02 - logprior: -2.5355e+01
Epoch 3/10
10/10 - 2s - loss: 726.6146 - loglik: -7.2228e+02 - logprior: -4.3392e+00
Epoch 4/10
10/10 - 2s - loss: 716.7109 - loglik: -7.2266e+02 - logprior: 5.9470
Epoch 5/10
10/10 - 2s - loss: 712.8943 - loglik: -7.2253e+02 - logprior: 9.6312
Epoch 6/10
10/10 - 2s - loss: 711.2883 - loglik: -7.2284e+02 - logprior: 11.5480
Epoch 7/10
10/10 - 2s - loss: 711.0080 - loglik: -7.2380e+02 - logprior: 12.7916
Epoch 8/10
10/10 - 2s - loss: 709.1683 - loglik: -7.2296e+02 - logprior: 13.7905
Epoch 9/10
10/10 - 2s - loss: 708.2426 - loglik: -7.2286e+02 - logprior: 14.6154
Epoch 10/10
10/10 - 2s - loss: 707.5922 - loglik: -7.2295e+02 - logprior: 15.3613
Fitted a model with MAP estimate = -707.4649
Time for alignment: 68.9669
Computed alignments with likelihoods: ['-709.5306', '-709.5823', '-709.4158', '-710.5285', '-707.4649']
Best model has likelihood: -707.4649  (prior= 15.7254 )
time for generating output: 0.2289
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9114349775784754
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 1684.2565 - loglik: -1.6823e+03 - logprior: -2.0029e+00
Epoch 2/10
33/33 - 21s - loss: 1594.6405 - loglik: -1.5942e+03 - logprior: -4.8057e-01
Epoch 3/10
33/33 - 20s - loss: 1583.9128 - loglik: -1.5835e+03 - logprior: -4.2254e-01
Epoch 4/10
33/33 - 21s - loss: 1589.1796 - loglik: -1.5887e+03 - logprior: -4.3357e-01
Fitted a model with MAP estimate = -1580.9891
expansions: [(0, 4), (7, 1), (9, 1), (34, 4), (61, 1), (62, 2), (71, 1), (72, 2), (77, 1), (110, 1), (112, 1), (115, 1), (117, 1), (133, 1), (154, 3), (162, 5), (180, 2), (204, 1), (222, 3), (223, 2), (230, 3)]
discards: [  4 224 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 1585.5736 - loglik: -1.5827e+03 - logprior: -2.8615e+00
Epoch 2/2
33/33 - 25s - loss: 1568.3485 - loglik: -1.5680e+03 - logprior: -3.4178e-01
Fitted a model with MAP estimate = -1572.1414
expansions: [(0, 4), (9, 1), (15, 1), (265, 4)]
discards: [  1   2   3   7  40  70 176 187 188 189 190 191 210 261 262 264]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 1581.7957 - loglik: -1.5788e+03 - logprior: -3.0420e+00
Epoch 2/2
33/33 - 24s - loss: 1573.9065 - loglik: -1.5738e+03 - logprior: -1.3796e-01
Fitted a model with MAP estimate = -1572.6894
expansions: [(249, 1), (259, 4)]
discards: [  0   6   7   8  16 253 254 255 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 1582.2408 - loglik: -1.5791e+03 - logprior: -3.1010e+00
Epoch 2/10
33/33 - 24s - loss: 1573.9424 - loglik: -1.5736e+03 - logprior: -3.8188e-01
Epoch 3/10
33/33 - 24s - loss: 1575.2584 - loglik: -1.5756e+03 - logprior: 0.3699
Fitted a model with MAP estimate = -1571.6787
Time for alignment: 360.5140
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 1686.8533 - loglik: -1.6849e+03 - logprior: -1.9845e+00
Epoch 2/10
33/33 - 20s - loss: 1592.7523 - loglik: -1.5922e+03 - logprior: -5.1131e-01
Epoch 3/10
33/33 - 21s - loss: 1584.3838 - loglik: -1.5839e+03 - logprior: -4.6387e-01
Epoch 4/10
33/33 - 21s - loss: 1584.9082 - loglik: -1.5845e+03 - logprior: -4.4441e-01
Fitted a model with MAP estimate = -1580.7887
expansions: [(0, 5), (5, 1), (8, 1), (33, 2), (34, 2), (42, 1), (44, 2), (61, 1), (62, 2), (71, 1), (72, 2), (77, 1), (108, 1), (112, 1), (118, 1), (132, 2), (155, 3), (163, 4), (180, 2), (203, 1), (216, 1), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 269 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1581.2782 - loglik: -1.5785e+03 - logprior: -2.7925e+00
Epoch 2/2
33/33 - 26s - loss: 1575.4658 - loglik: -1.5751e+03 - logprior: -3.6576e-01
Fitted a model with MAP estimate = -1570.9276
expansions: [(269, 4)]
discards: [  1   6  15  41  56  75 182 193 194 195 214 263 264 265 266 267 268]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 1580.0297 - loglik: -1.5780e+03 - logprior: -2.0112e+00
Epoch 2/2
33/33 - 24s - loss: 1571.0488 - loglik: -1.5710e+03 - logprior: -1.6430e-02
Fitted a model with MAP estimate = -1573.1629
expansions: [(0, 4), (249, 1), (251, 1), (256, 4)]
discards: [  6 252 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 1578.9209 - loglik: -1.5760e+03 - logprior: -2.9454e+00
Epoch 2/10
33/33 - 25s - loss: 1575.2751 - loglik: -1.5753e+03 - logprior: -7.2151e-03
Epoch 3/10
33/33 - 25s - loss: 1576.6578 - loglik: -1.5769e+03 - logprior: 0.2151
Fitted a model with MAP estimate = -1570.8380
Time for alignment: 364.4117
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 1687.8258 - loglik: -1.6858e+03 - logprior: -2.0104e+00
Epoch 2/10
33/33 - 20s - loss: 1589.4066 - loglik: -1.5888e+03 - logprior: -5.7239e-01
Epoch 3/10
33/33 - 21s - loss: 1587.9701 - loglik: -1.5875e+03 - logprior: -4.3311e-01
Epoch 4/10
33/33 - 21s - loss: 1579.1652 - loglik: -1.5788e+03 - logprior: -3.9114e-01
Epoch 5/10
33/33 - 21s - loss: 1577.5045 - loglik: -1.5771e+03 - logprior: -3.5604e-01
Epoch 6/10
33/33 - 21s - loss: 1579.9507 - loglik: -1.5796e+03 - logprior: -3.5697e-01
Fitted a model with MAP estimate = -1578.6888
expansions: [(0, 6), (10, 1), (34, 5), (42, 1), (44, 2), (63, 1), (65, 1), (71, 1), (73, 1), (78, 2), (94, 1), (113, 1), (116, 1), (118, 2), (135, 2), (154, 3), (162, 4), (180, 2), (210, 1), (220, 2), (221, 4), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1579.9342 - loglik: -1.5770e+03 - logprior: -2.8863e+00
Epoch 2/2
33/33 - 26s - loss: 1572.9204 - loglik: -1.5725e+03 - logprior: -4.3469e-01
Fitted a model with MAP estimate = -1570.1786
expansions: [(273, 4)]
discards: [  1  42  43  57 143 183 194 195 196 197 216 259 267 268 269 270 271 272]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1579.3184 - loglik: -1.5774e+03 - logprior: -1.9646e+00
Epoch 2/2
33/33 - 24s - loss: 1573.0690 - loglik: -1.5730e+03 - logprior: -2.2801e-02
Fitted a model with MAP estimate = -1571.8197
expansions: [(0, 4), (75, 1), (259, 4)]
discards: [  4   5   6  73 255 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 1580.2346 - loglik: -1.5774e+03 - logprior: -2.8816e+00
Epoch 2/10
33/33 - 24s - loss: 1570.7111 - loglik: -1.5708e+03 - logprior: 0.1229
Epoch 3/10
33/33 - 25s - loss: 1575.3317 - loglik: -1.5756e+03 - logprior: 0.3145
Fitted a model with MAP estimate = -1570.2486
Time for alignment: 408.2327
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 1694.1818 - loglik: -1.6922e+03 - logprior: -2.0025e+00
Epoch 2/10
33/33 - 20s - loss: 1594.3230 - loglik: -1.5937e+03 - logprior: -6.2688e-01
Epoch 3/10
33/33 - 21s - loss: 1577.8502 - loglik: -1.5773e+03 - logprior: -5.4163e-01
Epoch 4/10
33/33 - 21s - loss: 1577.4290 - loglik: -1.5769e+03 - logprior: -5.0126e-01
Epoch 5/10
33/33 - 21s - loss: 1583.9277 - loglik: -1.5834e+03 - logprior: -5.1888e-01
Fitted a model with MAP estimate = -1579.2431
expansions: [(0, 5), (7, 1), (9, 2), (10, 1), (33, 9), (63, 2), (65, 1), (71, 1), (73, 1), (78, 1), (80, 1), (108, 2), (109, 4), (113, 1), (118, 1), (132, 2), (155, 3), (163, 4), (174, 1), (204, 1), (210, 1), (222, 3), (223, 2), (230, 3)]
discards: [  2   3 224 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 30s - loss: 1580.2867 - loglik: -1.5774e+03 - logprior: -2.8404e+00
Epoch 2/2
33/33 - 27s - loss: 1576.0902 - loglik: -1.5757e+03 - logprior: -3.7686e-01
Fitted a model with MAP estimate = -1570.2694
expansions: [(276, 4)]
discards: [  1   6   7  42  43  44  45  46 132 133 134 135 189 200 201 202 203 271
 272 273 274 275]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 1579.6746 - loglik: -1.5777e+03 - logprior: -1.9688e+00
Epoch 2/2
33/33 - 24s - loss: 1572.3104 - loglik: -1.5724e+03 - logprior: 0.0418
Fitted a model with MAP estimate = -1572.4869
expansions: [(0, 5), (258, 4)]
discards: [  1   4 254 255 256]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 1582.5956 - loglik: -1.5797e+03 - logprior: -2.9127e+00
Epoch 2/10
33/33 - 25s - loss: 1569.6390 - loglik: -1.5697e+03 - logprior: 0.0326
Epoch 3/10
33/33 - 25s - loss: 1576.9054 - loglik: -1.5771e+03 - logprior: 0.2085
Fitted a model with MAP estimate = -1570.2234
Time for alignment: 389.8384
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 1686.0917 - loglik: -1.6841e+03 - logprior: -1.9881e+00
Epoch 2/10
33/33 - 21s - loss: 1594.8682 - loglik: -1.5943e+03 - logprior: -5.9665e-01
Epoch 3/10
33/33 - 21s - loss: 1590.4209 - loglik: -1.5899e+03 - logprior: -5.5712e-01
Epoch 4/10
33/33 - 20s - loss: 1587.1876 - loglik: -1.5867e+03 - logprior: -5.2300e-01
Epoch 5/10
33/33 - 21s - loss: 1579.0004 - loglik: -1.5785e+03 - logprior: -5.1372e-01
Epoch 6/10
33/33 - 20s - loss: 1579.5791 - loglik: -1.5791e+03 - logprior: -5.1780e-01
Fitted a model with MAP estimate = -1581.8113
expansions: [(0, 4), (10, 2), (11, 1), (34, 6), (72, 1), (73, 1), (74, 3), (78, 1), (83, 1), (110, 1), (113, 1), (118, 1), (132, 2), (155, 3), (163, 4), (183, 1), (203, 1), (213, 1), (221, 3), (230, 3)]
discards: [  3   4 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 1580.5612 - loglik: -1.5777e+03 - logprior: -2.8965e+00
Epoch 2/2
33/33 - 25s - loss: 1576.2959 - loglik: -1.5760e+03 - logprior: -2.8557e-01
Fitted a model with MAP estimate = -1571.5879
expansions: [(0, 4), (265, 4)]
discards: [  4   5   6  41  42 179 189 190 191 260 261 262 263]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1580.7869 - loglik: -1.5778e+03 - logprior: -2.9887e+00
Epoch 2/2
33/33 - 24s - loss: 1574.4398 - loglik: -1.5743e+03 - logprior: -9.7176e-02
Fitted a model with MAP estimate = -1572.6829
expansions: [(187, 5), (251, 1), (253, 1), (260, 4)]
discards: [  1   5   6   7 255 256 258 259]
Re-initialized the encoder parameters.
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 1579.0377 - loglik: -1.5772e+03 - logprior: -1.8268e+00
Epoch 2/10
33/33 - 25s - loss: 1577.0129 - loglik: -1.5771e+03 - logprior: 0.1351
Epoch 3/10
33/33 - 25s - loss: 1563.0608 - loglik: -1.5634e+03 - logprior: 0.3328
Epoch 4/10
33/33 - 25s - loss: 1569.6174 - loglik: -1.5700e+03 - logprior: 0.4281
Fitted a model with MAP estimate = -1568.0970
Time for alignment: 430.6287
Computed alignments with likelihoods: ['-1571.6787', '-1570.8380', '-1570.1786', '-1570.2234', '-1568.0970']
Best model has likelihood: -1568.0970  (prior= 0.4609 )
time for generating output: 0.3378
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7673390439347886
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 771.4165 - loglik: -7.0950e+02 - logprior: -6.1918e+01
Epoch 2/10
10/10 - 1s - loss: 681.1648 - loglik: -6.6686e+02 - logprior: -1.4303e+01
Epoch 3/10
10/10 - 1s - loss: 633.5874 - loglik: -6.2741e+02 - logprior: -6.1817e+00
Epoch 4/10
10/10 - 1s - loss: 609.5092 - loglik: -6.0579e+02 - logprior: -3.7229e+00
Epoch 5/10
10/10 - 1s - loss: 598.4095 - loglik: -5.9591e+02 - logprior: -2.5005e+00
Epoch 6/10
10/10 - 1s - loss: 595.1548 - loglik: -5.9350e+02 - logprior: -1.6500e+00
Epoch 7/10
10/10 - 1s - loss: 593.8662 - loglik: -5.9294e+02 - logprior: -9.2719e-01
Epoch 8/10
10/10 - 1s - loss: 591.8928 - loglik: -5.9133e+02 - logprior: -5.6662e-01
Epoch 9/10
10/10 - 1s - loss: 592.3652 - loglik: -5.9201e+02 - logprior: -3.5244e-01
Fitted a model with MAP estimate = -591.4635
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 3), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.0441 - loglik: -5.9274e+02 - logprior: -6.9303e+01
Epoch 2/2
10/10 - 1s - loss: 600.7935 - loglik: -5.7455e+02 - logprior: -2.6246e+01
Fitted a model with MAP estimate = -589.7755
expansions: [(0, 5)]
discards: [  0  79 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 626.4929 - loglik: -5.7203e+02 - logprior: -5.4467e+01
Epoch 2/2
10/10 - 2s - loss: 578.5349 - loglik: -5.6680e+02 - logprior: -1.1730e+01
Fitted a model with MAP estimate = -570.4116
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 622.8333 - loglik: -5.6948e+02 - logprior: -5.3348e+01
Epoch 2/10
10/10 - 1s - loss: 577.6956 - loglik: -5.6661e+02 - logprior: -1.1087e+01
Epoch 3/10
10/10 - 1s - loss: 568.0672 - loglik: -5.6571e+02 - logprior: -2.3586e+00
Epoch 4/10
10/10 - 1s - loss: 562.6052 - loglik: -5.6388e+02 - logprior: 1.2734
Epoch 5/10
10/10 - 1s - loss: 559.3633 - loglik: -5.6256e+02 - logprior: 3.1985
Epoch 6/10
10/10 - 1s - loss: 558.3693 - loglik: -5.6260e+02 - logprior: 4.2263
Epoch 7/10
10/10 - 1s - loss: 557.6201 - loglik: -5.6258e+02 - logprior: 4.9635
Epoch 8/10
10/10 - 1s - loss: 556.9618 - loglik: -5.6264e+02 - logprior: 5.6780
Epoch 9/10
10/10 - 1s - loss: 556.6754 - loglik: -5.6293e+02 - logprior: 6.2537
Epoch 10/10
10/10 - 1s - loss: 556.2992 - loglik: -5.6298e+02 - logprior: 6.6760
Fitted a model with MAP estimate = -555.9890
Time for alignment: 48.4409
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 770.7567 - loglik: -7.0884e+02 - logprior: -6.1917e+01
Epoch 2/10
10/10 - 1s - loss: 681.4392 - loglik: -6.6714e+02 - logprior: -1.4299e+01
Epoch 3/10
10/10 - 1s - loss: 634.0221 - loglik: -6.2784e+02 - logprior: -6.1796e+00
Epoch 4/10
10/10 - 1s - loss: 608.2001 - loglik: -6.0436e+02 - logprior: -3.8393e+00
Epoch 5/10
10/10 - 1s - loss: 598.5802 - loglik: -5.9600e+02 - logprior: -2.5767e+00
Epoch 6/10
10/10 - 1s - loss: 595.3029 - loglik: -5.9362e+02 - logprior: -1.6845e+00
Epoch 7/10
10/10 - 1s - loss: 593.7291 - loglik: -5.9266e+02 - logprior: -1.0671e+00
Epoch 8/10
10/10 - 1s - loss: 592.3328 - loglik: -5.9169e+02 - logprior: -6.4057e-01
Epoch 9/10
10/10 - 1s - loss: 592.0115 - loglik: -5.9159e+02 - logprior: -4.1835e-01
Epoch 10/10
10/10 - 1s - loss: 591.4200 - loglik: -5.9121e+02 - logprior: -2.0696e-01
Fitted a model with MAP estimate = -591.3459
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (85, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.1561 - loglik: -5.9286e+02 - logprior: -6.9298e+01
Epoch 2/2
10/10 - 1s - loss: 601.1054 - loglik: -5.7488e+02 - logprior: -2.6221e+01
Fitted a model with MAP estimate = -589.9150
expansions: [(0, 5)]
discards: [  0  87 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.0062 - loglik: -5.7159e+02 - logprior: -5.4420e+01
Epoch 2/2
10/10 - 1s - loss: 578.3528 - loglik: -5.6669e+02 - logprior: -1.1666e+01
Fitted a model with MAP estimate = -570.4353
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 622.5466 - loglik: -5.6923e+02 - logprior: -5.3317e+01
Epoch 2/10
10/10 - 1s - loss: 578.5585 - loglik: -5.6752e+02 - logprior: -1.1042e+01
Epoch 3/10
10/10 - 1s - loss: 568.8119 - loglik: -5.6651e+02 - logprior: -2.3016e+00
Epoch 4/10
10/10 - 1s - loss: 562.0173 - loglik: -5.6335e+02 - logprior: 1.3329
Epoch 5/10
10/10 - 1s - loss: 560.6315 - loglik: -5.6390e+02 - logprior: 3.2654
Epoch 6/10
10/10 - 1s - loss: 557.8380 - loglik: -5.6213e+02 - logprior: 4.2949
Epoch 7/10
10/10 - 1s - loss: 557.6605 - loglik: -5.6267e+02 - logprior: 5.0079
Epoch 8/10
10/10 - 1s - loss: 556.8170 - loglik: -5.6253e+02 - logprior: 5.7089
Epoch 9/10
10/10 - 1s - loss: 556.2761 - loglik: -5.6257e+02 - logprior: 6.2903
Epoch 10/10
10/10 - 1s - loss: 555.9005 - loglik: -5.6262e+02 - logprior: 6.7151
Fitted a model with MAP estimate = -555.7774
Time for alignment: 49.7234
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 771.1304 - loglik: -7.0921e+02 - logprior: -6.1918e+01
Epoch 2/10
10/10 - 1s - loss: 681.9108 - loglik: -6.6761e+02 - logprior: -1.4303e+01
Epoch 3/10
10/10 - 1s - loss: 634.7881 - loglik: -6.2863e+02 - logprior: -6.1605e+00
Epoch 4/10
10/10 - 1s - loss: 609.3092 - loglik: -6.0564e+02 - logprior: -3.6659e+00
Epoch 5/10
10/10 - 1s - loss: 599.8826 - loglik: -5.9760e+02 - logprior: -2.2809e+00
Epoch 6/10
10/10 - 1s - loss: 596.2172 - loglik: -5.9472e+02 - logprior: -1.4979e+00
Epoch 7/10
10/10 - 1s - loss: 593.6562 - loglik: -5.9271e+02 - logprior: -9.4204e-01
Epoch 8/10
10/10 - 1s - loss: 592.4139 - loglik: -5.9188e+02 - logprior: -5.3386e-01
Epoch 9/10
10/10 - 1s - loss: 592.5275 - loglik: -5.9220e+02 - logprior: -3.2830e-01
Fitted a model with MAP estimate = -591.7983
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (55, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 662.5190 - loglik: -5.9317e+02 - logprior: -6.9345e+01
Epoch 2/2
10/10 - 1s - loss: 600.9298 - loglik: -5.7471e+02 - logprior: -2.6215e+01
Fitted a model with MAP estimate = -590.1057
expansions: [(0, 5)]
discards: [  0  87 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.7411 - loglik: -5.7225e+02 - logprior: -5.4487e+01
Epoch 2/2
10/10 - 2s - loss: 578.9454 - loglik: -5.6727e+02 - logprior: -1.1678e+01
Fitted a model with MAP estimate = -570.7316
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 623.8763 - loglik: -5.7051e+02 - logprior: -5.3362e+01
Epoch 2/10
10/10 - 1s - loss: 578.1638 - loglik: -5.6705e+02 - logprior: -1.1111e+01
Epoch 3/10
10/10 - 1s - loss: 568.9946 - loglik: -5.6666e+02 - logprior: -2.3313e+00
Epoch 4/10
10/10 - 1s - loss: 562.8146 - loglik: -5.6419e+02 - logprior: 1.3753
Epoch 5/10
10/10 - 1s - loss: 559.2072 - loglik: -5.6257e+02 - logprior: 3.3588
Epoch 6/10
10/10 - 1s - loss: 558.6635 - loglik: -5.6306e+02 - logprior: 4.3984
Epoch 7/10
10/10 - 1s - loss: 556.4457 - loglik: -5.6148e+02 - logprior: 5.0361
Epoch 8/10
10/10 - 1s - loss: 557.3695 - loglik: -5.6304e+02 - logprior: 5.6747
Fitted a model with MAP estimate = -556.2693
Time for alignment: 43.5725
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 771.0882 - loglik: -7.0917e+02 - logprior: -6.1917e+01
Epoch 2/10
10/10 - 1s - loss: 681.9576 - loglik: -6.6765e+02 - logprior: -1.4305e+01
Epoch 3/10
10/10 - 1s - loss: 634.2753 - loglik: -6.2806e+02 - logprior: -6.2123e+00
Epoch 4/10
10/10 - 1s - loss: 609.3981 - loglik: -6.0559e+02 - logprior: -3.8120e+00
Epoch 5/10
10/10 - 1s - loss: 599.5311 - loglik: -5.9707e+02 - logprior: -2.4562e+00
Epoch 6/10
10/10 - 1s - loss: 595.8236 - loglik: -5.9436e+02 - logprior: -1.4677e+00
Epoch 7/10
10/10 - 1s - loss: 593.9606 - loglik: -5.9307e+02 - logprior: -8.8555e-01
Epoch 8/10
10/10 - 1s - loss: 592.6610 - loglik: -5.9212e+02 - logprior: -5.3968e-01
Epoch 9/10
10/10 - 1s - loss: 592.4132 - loglik: -5.9212e+02 - logprior: -2.9732e-01
Epoch 10/10
10/10 - 1s - loss: 591.6808 - loglik: -5.9161e+02 - logprior: -6.8602e-02
Fitted a model with MAP estimate = -591.6101
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 2), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.1774 - loglik: -5.9293e+02 - logprior: -6.9246e+01
Epoch 2/2
10/10 - 1s - loss: 600.2965 - loglik: -5.7424e+02 - logprior: -2.6055e+01
Fitted a model with MAP estimate = -589.7675
expansions: [(0, 5)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.2450 - loglik: -5.7188e+02 - logprior: -5.4364e+01
Epoch 2/2
10/10 - 2s - loss: 578.1492 - loglik: -5.6651e+02 - logprior: -1.1638e+01
Fitted a model with MAP estimate = -570.2577
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.0071 - loglik: -5.6971e+02 - logprior: -5.3295e+01
Epoch 2/10
10/10 - 1s - loss: 577.8176 - loglik: -5.6679e+02 - logprior: -1.1030e+01
Epoch 3/10
10/10 - 1s - loss: 567.2891 - loglik: -5.6500e+02 - logprior: -2.2891e+00
Epoch 4/10
10/10 - 1s - loss: 562.7944 - loglik: -5.6413e+02 - logprior: 1.3381
Epoch 5/10
10/10 - 1s - loss: 559.9506 - loglik: -5.6321e+02 - logprior: 3.2632
Epoch 6/10
10/10 - 1s - loss: 558.0765 - loglik: -5.6236e+02 - logprior: 4.2817
Epoch 7/10
10/10 - 1s - loss: 557.6849 - loglik: -5.6271e+02 - logprior: 5.0273
Epoch 8/10
10/10 - 1s - loss: 556.7953 - loglik: -5.6254e+02 - logprior: 5.7433
Epoch 9/10
10/10 - 1s - loss: 556.0800 - loglik: -5.6241e+02 - logprior: 6.3290
Epoch 10/10
10/10 - 1s - loss: 556.5256 - loglik: -5.6327e+02 - logprior: 6.7448
Fitted a model with MAP estimate = -555.7485
Time for alignment: 49.1744
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 770.7817 - loglik: -7.0886e+02 - logprior: -6.1917e+01
Epoch 2/10
10/10 - 1s - loss: 681.7499 - loglik: -6.6745e+02 - logprior: -1.4299e+01
Epoch 3/10
10/10 - 1s - loss: 633.8242 - loglik: -6.2764e+02 - logprior: -6.1853e+00
Epoch 4/10
10/10 - 1s - loss: 608.5276 - loglik: -6.0469e+02 - logprior: -3.8349e+00
Epoch 5/10
10/10 - 1s - loss: 598.5947 - loglik: -5.9607e+02 - logprior: -2.5230e+00
Epoch 6/10
10/10 - 1s - loss: 595.1298 - loglik: -5.9345e+02 - logprior: -1.6830e+00
Epoch 7/10
10/10 - 1s - loss: 593.8636 - loglik: -5.9278e+02 - logprior: -1.0811e+00
Epoch 8/10
10/10 - 1s - loss: 592.8282 - loglik: -5.9215e+02 - logprior: -6.7594e-01
Epoch 9/10
10/10 - 1s - loss: 591.7675 - loglik: -5.9132e+02 - logprior: -4.4732e-01
Epoch 10/10
10/10 - 1s - loss: 591.6973 - loglik: -5.9145e+02 - logprior: -2.4925e-01
Fitted a model with MAP estimate = -591.4680
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.0064 - loglik: -5.9271e+02 - logprior: -6.9293e+01
Epoch 2/2
10/10 - 1s - loss: 601.2441 - loglik: -5.7503e+02 - logprior: -2.6216e+01
Fitted a model with MAP estimate = -589.9403
expansions: [(0, 5)]
discards: [  0  87 109]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 625.8116 - loglik: -5.7141e+02 - logprior: -5.4406e+01
Epoch 2/2
10/10 - 2s - loss: 578.8885 - loglik: -5.6723e+02 - logprior: -1.1663e+01
Fitted a model with MAP estimate = -570.4720
expansions: []
discards: [ 1  2  3  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 623.2826 - loglik: -5.6997e+02 - logprior: -5.3309e+01
Epoch 2/10
10/10 - 1s - loss: 578.3805 - loglik: -5.6734e+02 - logprior: -1.1041e+01
Epoch 3/10
10/10 - 1s - loss: 567.6152 - loglik: -5.6531e+02 - logprior: -2.3037e+00
Epoch 4/10
10/10 - 1s - loss: 562.2544 - loglik: -5.6360e+02 - logprior: 1.3429
Epoch 5/10
10/10 - 1s - loss: 560.6167 - loglik: -5.6389e+02 - logprior: 3.2770
Epoch 6/10
10/10 - 1s - loss: 557.9716 - loglik: -5.6228e+02 - logprior: 4.3075
Epoch 7/10
10/10 - 1s - loss: 558.1388 - loglik: -5.6315e+02 - logprior: 5.0155
Fitted a model with MAP estimate = -557.0676
Time for alignment: 43.5137
Computed alignments with likelihoods: ['-555.9890', '-555.7774', '-556.2693', '-555.7485', '-557.0676']
Best model has likelihood: -555.7485  (prior= 6.9463 )
time for generating output: 0.1706
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9410339256865913
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 402.3048 - loglik: -4.0012e+02 - logprior: -2.1875e+00
Epoch 2/10
22/22 - 1s - loss: 371.3761 - loglik: -3.7005e+02 - logprior: -1.3238e+00
Epoch 3/10
22/22 - 1s - loss: 362.9933 - loglik: -3.6159e+02 - logprior: -1.4054e+00
Epoch 4/10
22/22 - 1s - loss: 362.0109 - loglik: -3.6072e+02 - logprior: -1.2950e+00
Epoch 5/10
22/22 - 1s - loss: 361.0059 - loglik: -3.5971e+02 - logprior: -1.2944e+00
Epoch 6/10
22/22 - 1s - loss: 360.5015 - loglik: -3.5923e+02 - logprior: -1.2705e+00
Epoch 7/10
22/22 - 1s - loss: 360.3014 - loglik: -3.5904e+02 - logprior: -1.2601e+00
Epoch 8/10
22/22 - 1s - loss: 360.3931 - loglik: -3.5914e+02 - logprior: -1.2526e+00
Fitted a model with MAP estimate = -348.7808
expansions: [(8, 1), (9, 2), (11, 2), (14, 2), (20, 2), (21, 3), (22, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 363.4768 - loglik: -3.6063e+02 - logprior: -2.8506e+00
Epoch 2/2
22/22 - 1s - loss: 354.6544 - loglik: -3.5320e+02 - logprior: -1.4518e+00
Fitted a model with MAP estimate = -340.8426
expansions: [(0, 2)]
discards: [ 0  9 14 18 26 30 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 354.0899 - loglik: -3.5198e+02 - logprior: -2.1092e+00
Epoch 2/2
22/22 - 1s - loss: 351.6069 - loglik: -3.5060e+02 - logprior: -1.0040e+00
Fitted a model with MAP estimate = -339.9182
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.6943 - loglik: -3.3855e+02 - logprior: -1.1418e+00
Epoch 2/10
32/32 - 2s - loss: 338.2830 - loglik: -3.3748e+02 - logprior: -8.0150e-01
Epoch 3/10
32/32 - 2s - loss: 337.5924 - loglik: -3.3681e+02 - logprior: -7.7853e-01
Epoch 4/10
32/32 - 2s - loss: 337.2543 - loglik: -3.3648e+02 - logprior: -7.7315e-01
Epoch 5/10
32/32 - 2s - loss: 336.6646 - loglik: -3.3590e+02 - logprior: -7.6679e-01
Epoch 6/10
32/32 - 2s - loss: 336.2275 - loglik: -3.3546e+02 - logprior: -7.7075e-01
Epoch 7/10
32/32 - 2s - loss: 336.2065 - loglik: -3.3544e+02 - logprior: -7.6471e-01
Epoch 8/10
32/32 - 2s - loss: 336.1364 - loglik: -3.3537e+02 - logprior: -7.6585e-01
Epoch 9/10
32/32 - 2s - loss: 336.1457 - loglik: -3.3539e+02 - logprior: -7.5761e-01
Fitted a model with MAP estimate = -336.0671
Time for alignment: 64.2950
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 401.8405 - loglik: -3.9965e+02 - logprior: -2.1894e+00
Epoch 2/10
22/22 - 1s - loss: 369.5072 - loglik: -3.6819e+02 - logprior: -1.3178e+00
Epoch 3/10
22/22 - 1s - loss: 362.1945 - loglik: -3.6078e+02 - logprior: -1.4170e+00
Epoch 4/10
22/22 - 1s - loss: 360.9589 - loglik: -3.5965e+02 - logprior: -1.3132e+00
Epoch 5/10
22/22 - 1s - loss: 360.4072 - loglik: -3.5909e+02 - logprior: -1.3147e+00
Epoch 6/10
22/22 - 1s - loss: 359.9909 - loglik: -3.5870e+02 - logprior: -1.2870e+00
Epoch 7/10
22/22 - 1s - loss: 359.9044 - loglik: -3.5863e+02 - logprior: -1.2775e+00
Epoch 8/10
22/22 - 1s - loss: 359.6787 - loglik: -3.5841e+02 - logprior: -1.2720e+00
Epoch 9/10
22/22 - 1s - loss: 359.5745 - loglik: -3.5831e+02 - logprior: -1.2663e+00
Epoch 10/10
22/22 - 1s - loss: 359.8400 - loglik: -3.5858e+02 - logprior: -1.2647e+00
Fitted a model with MAP estimate = -348.4414
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 2), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.3181 - loglik: -3.6047e+02 - logprior: -2.8434e+00
Epoch 2/2
22/22 - 1s - loss: 354.4281 - loglik: -3.5300e+02 - logprior: -1.4312e+00
Fitted a model with MAP estimate = -340.9689
expansions: [(0, 2)]
discards: [ 0  9 17 26 29 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 354.4371 - loglik: -3.5233e+02 - logprior: -2.1026e+00
Epoch 2/2
22/22 - 1s - loss: 351.5179 - loglik: -3.5051e+02 - logprior: -1.0033e+00
Fitted a model with MAP estimate = -339.8181
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 340.1713 - loglik: -3.3904e+02 - logprior: -1.1341e+00
Epoch 2/10
32/32 - 2s - loss: 337.7520 - loglik: -3.3695e+02 - logprior: -8.0280e-01
Epoch 3/10
32/32 - 2s - loss: 337.4068 - loglik: -3.3663e+02 - logprior: -7.7343e-01
Epoch 4/10
32/32 - 2s - loss: 337.0379 - loglik: -3.3627e+02 - logprior: -7.7192e-01
Epoch 5/10
32/32 - 2s - loss: 336.8932 - loglik: -3.3613e+02 - logprior: -7.6460e-01
Epoch 6/10
32/32 - 2s - loss: 336.4328 - loglik: -3.3567e+02 - logprior: -7.6711e-01
Epoch 7/10
32/32 - 2s - loss: 336.1927 - loglik: -3.3543e+02 - logprior: -7.6615e-01
Epoch 8/10
32/32 - 2s - loss: 336.5345 - loglik: -3.3577e+02 - logprior: -7.5963e-01
Fitted a model with MAP estimate = -336.0584
Time for alignment: 63.4859
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 402.3800 - loglik: -4.0019e+02 - logprior: -2.1868e+00
Epoch 2/10
22/22 - 1s - loss: 369.5493 - loglik: -3.6823e+02 - logprior: -1.3207e+00
Epoch 3/10
22/22 - 1s - loss: 361.9477 - loglik: -3.6053e+02 - logprior: -1.4142e+00
Epoch 4/10
22/22 - 1s - loss: 360.4456 - loglik: -3.5915e+02 - logprior: -1.2965e+00
Epoch 5/10
22/22 - 1s - loss: 359.7292 - loglik: -3.5843e+02 - logprior: -1.2971e+00
Epoch 6/10
22/22 - 1s - loss: 359.6434 - loglik: -3.5837e+02 - logprior: -1.2743e+00
Epoch 7/10
22/22 - 1s - loss: 359.2751 - loglik: -3.5801e+02 - logprior: -1.2658e+00
Epoch 8/10
22/22 - 1s - loss: 359.4189 - loglik: -3.5816e+02 - logprior: -1.2581e+00
Fitted a model with MAP estimate = -347.9288
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 1), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 362.9718 - loglik: -3.6013e+02 - logprior: -2.8426e+00
Epoch 2/2
22/22 - 1s - loss: 354.6069 - loglik: -3.5317e+02 - logprior: -1.4336e+00
Fitted a model with MAP estimate = -340.7276
expansions: [(0, 2)]
discards: [ 0  9 17 25 29 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 354.1570 - loglik: -3.5205e+02 - logprior: -2.1057e+00
Epoch 2/2
22/22 - 1s - loss: 351.6695 - loglik: -3.5066e+02 - logprior: -1.0081e+00
Fitted a model with MAP estimate = -339.8348
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.9102 - loglik: -3.3877e+02 - logprior: -1.1396e+00
Epoch 2/10
32/32 - 2s - loss: 338.0581 - loglik: -3.3726e+02 - logprior: -8.0017e-01
Epoch 3/10
32/32 - 2s - loss: 337.5244 - loglik: -3.3675e+02 - logprior: -7.7521e-01
Epoch 4/10
32/32 - 2s - loss: 336.9894 - loglik: -3.3622e+02 - logprior: -7.7030e-01
Epoch 5/10
32/32 - 2s - loss: 336.8158 - loglik: -3.3605e+02 - logprior: -7.6736e-01
Epoch 6/10
32/32 - 2s - loss: 336.3524 - loglik: -3.3559e+02 - logprior: -7.6016e-01
Epoch 7/10
32/32 - 2s - loss: 336.6712 - loglik: -3.3591e+02 - logprior: -7.6580e-01
Fitted a model with MAP estimate = -336.1221
Time for alignment: 60.1024
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 402.1090 - loglik: -3.9992e+02 - logprior: -2.1883e+00
Epoch 2/10
22/22 - 1s - loss: 370.3436 - loglik: -3.6903e+02 - logprior: -1.3174e+00
Epoch 3/10
22/22 - 1s - loss: 362.7810 - loglik: -3.6138e+02 - logprior: -1.3966e+00
Epoch 4/10
22/22 - 1s - loss: 360.9864 - loglik: -3.5968e+02 - logprior: -1.3065e+00
Epoch 5/10
22/22 - 1s - loss: 360.1609 - loglik: -3.5884e+02 - logprior: -1.3163e+00
Epoch 6/10
22/22 - 1s - loss: 359.9453 - loglik: -3.5866e+02 - logprior: -1.2851e+00
Epoch 7/10
22/22 - 1s - loss: 359.9543 - loglik: -3.5868e+02 - logprior: -1.2760e+00
Fitted a model with MAP estimate = -348.1387
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (14, 2), (21, 1), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 363.1115 - loglik: -3.6027e+02 - logprior: -2.8440e+00
Epoch 2/2
22/22 - 1s - loss: 354.7631 - loglik: -3.5334e+02 - logprior: -1.4216e+00
Fitted a model with MAP estimate = -340.6718
expansions: [(0, 2)]
discards: [ 0  9 17 19 30 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 354.0661 - loglik: -3.5196e+02 - logprior: -2.1056e+00
Epoch 2/2
22/22 - 1s - loss: 351.5950 - loglik: -3.5059e+02 - logprior: -1.0042e+00
Fitted a model with MAP estimate = -339.7751
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.7214 - loglik: -3.3859e+02 - logprior: -1.1361e+00
Epoch 2/10
32/32 - 2s - loss: 338.1462 - loglik: -3.3735e+02 - logprior: -7.9477e-01
Epoch 3/10
32/32 - 2s - loss: 337.3640 - loglik: -3.3659e+02 - logprior: -7.7131e-01
Epoch 4/10
32/32 - 2s - loss: 337.1159 - loglik: -3.3634e+02 - logprior: -7.7518e-01
Epoch 5/10
32/32 - 2s - loss: 336.9473 - loglik: -3.3618e+02 - logprior: -7.6383e-01
Epoch 6/10
32/32 - 2s - loss: 335.8849 - loglik: -3.3512e+02 - logprior: -7.6728e-01
Epoch 7/10
32/32 - 2s - loss: 336.7080 - loglik: -3.3594e+02 - logprior: -7.6516e-01
Fitted a model with MAP estimate = -336.0981
Time for alignment: 57.4324
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 402.1971 - loglik: -4.0001e+02 - logprior: -2.1859e+00
Epoch 2/10
22/22 - 1s - loss: 370.2830 - loglik: -3.6896e+02 - logprior: -1.3190e+00
Epoch 3/10
22/22 - 1s - loss: 361.8607 - loglik: -3.6045e+02 - logprior: -1.4141e+00
Epoch 4/10
22/22 - 1s - loss: 360.9167 - loglik: -3.5961e+02 - logprior: -1.3043e+00
Epoch 5/10
22/22 - 1s - loss: 360.2359 - loglik: -3.5893e+02 - logprior: -1.3064e+00
Epoch 6/10
22/22 - 1s - loss: 359.7896 - loglik: -3.5851e+02 - logprior: -1.2793e+00
Epoch 7/10
22/22 - 1s - loss: 359.4609 - loglik: -3.5819e+02 - logprior: -1.2677e+00
Epoch 8/10
22/22 - 1s - loss: 359.6488 - loglik: -3.5839e+02 - logprior: -1.2632e+00
Fitted a model with MAP estimate = -348.2322
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (17, 2), (21, 2), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 363.4596 - loglik: -3.6062e+02 - logprior: -2.8433e+00
Epoch 2/2
22/22 - 1s - loss: 354.6682 - loglik: -3.5322e+02 - logprior: -1.4525e+00
Fitted a model with MAP estimate = -340.7925
expansions: [(0, 2)]
discards: [ 0  9 17 22 29 31 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 354.3461 - loglik: -3.5224e+02 - logprior: -2.1027e+00
Epoch 2/2
22/22 - 1s - loss: 351.7577 - loglik: -3.5075e+02 - logprior: -1.0071e+00
Fitted a model with MAP estimate = -339.7278
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.9910 - loglik: -3.3886e+02 - logprior: -1.1341e+00
Epoch 2/10
32/32 - 2s - loss: 338.0281 - loglik: -3.3723e+02 - logprior: -7.9738e-01
Epoch 3/10
32/32 - 2s - loss: 337.3672 - loglik: -3.3659e+02 - logprior: -7.7728e-01
Epoch 4/10
32/32 - 2s - loss: 337.2957 - loglik: -3.3653e+02 - logprior: -7.7051e-01
Epoch 5/10
32/32 - 2s - loss: 336.5792 - loglik: -3.3581e+02 - logprior: -7.6529e-01
Epoch 6/10
32/32 - 2s - loss: 336.2434 - loglik: -3.3548e+02 - logprior: -7.6770e-01
Epoch 7/10
32/32 - 2s - loss: 336.3930 - loglik: -3.3563e+02 - logprior: -7.6703e-01
Fitted a model with MAP estimate = -336.1072
Time for alignment: 57.6169
Computed alignments with likelihoods: ['-336.0671', '-336.0584', '-336.1221', '-336.0981', '-336.1072']
Best model has likelihood: -336.0584  (prior= -0.7541 )
time for generating output: 0.1558
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8385537103216313
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 964.3304 - loglik: -9.5142e+02 - logprior: -1.2911e+01
Epoch 2/10
17/17 - 4s - loss: 829.6808 - loglik: -8.2846e+02 - logprior: -1.2181e+00
Epoch 3/10
17/17 - 4s - loss: 779.3841 - loglik: -7.7860e+02 - logprior: -7.8479e-01
Epoch 4/10
17/17 - 4s - loss: 761.0831 - loglik: -7.6059e+02 - logprior: -4.9004e-01
Epoch 5/10
17/17 - 4s - loss: 757.2133 - loglik: -7.5678e+02 - logprior: -4.3588e-01
Epoch 6/10
17/17 - 4s - loss: 757.9988 - loglik: -7.5758e+02 - logprior: -4.1491e-01
Fitted a model with MAP estimate = -757.7709
expansions: [(0, 37), (1, 1), (7, 1), (24, 1), (26, 1), (27, 1), (50, 1), (59, 1), (80, 1), (89, 1), (104, 1), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 210 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 767.0819 - loglik: -7.5117e+02 - logprior: -1.5914e+01
Epoch 2/2
17/17 - 5s - loss: 721.2707 - loglik: -7.1969e+02 - logprior: -1.5769e+00
Fitted a model with MAP estimate = -713.7095
expansions: [(0, 17)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 757.8683 - loglik: -7.4463e+02 - logprior: -1.3234e+01
Epoch 2/2
17/17 - 5s - loss: 733.0308 - loglik: -7.3271e+02 - logprior: -3.1885e-01
Fitted a model with MAP estimate = -727.5868
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 755.2727 - loglik: -7.4359e+02 - logprior: -1.1682e+01
Epoch 2/10
17/17 - 5s - loss: 722.1497 - loglik: -7.2221e+02 - logprior: 0.0612
Epoch 3/10
17/17 - 5s - loss: 713.9118 - loglik: -7.1527e+02 - logprior: 1.3626
Epoch 4/10
17/17 - 5s - loss: 713.0609 - loglik: -7.1501e+02 - logprior: 1.9533
Epoch 5/10
17/17 - 5s - loss: 713.7725 - loglik: -7.1598e+02 - logprior: 2.2091
Fitted a model with MAP estimate = -709.7202
Time for alignment: 94.6763
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 965.5750 - loglik: -9.5262e+02 - logprior: -1.2954e+01
Epoch 2/10
17/17 - 4s - loss: 816.1569 - loglik: -8.1440e+02 - logprior: -1.7596e+00
Epoch 3/10
17/17 - 4s - loss: 766.4088 - loglik: -7.6495e+02 - logprior: -1.4559e+00
Epoch 4/10
17/17 - 4s - loss: 754.2181 - loglik: -7.5321e+02 - logprior: -1.0058e+00
Epoch 5/10
17/17 - 4s - loss: 754.4536 - loglik: -7.5342e+02 - logprior: -1.0293e+00
Fitted a model with MAP estimate = -751.6678
expansions: [(49, 1), (80, 1), (89, 1), (92, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 777.8022 - loglik: -7.6027e+02 - logprior: -1.7533e+01
Epoch 2/2
17/17 - 4s - loss: 755.8032 - loglik: -7.4971e+02 - logprior: -6.0972e+00
Fitted a model with MAP estimate = -754.3425
expansions: [(0, 16)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 763.7769 - loglik: -7.5055e+02 - logprior: -1.3228e+01
Epoch 2/2
17/17 - 4s - loss: 743.8207 - loglik: -7.4178e+02 - logprior: -2.0384e+00
Fitted a model with MAP estimate = -740.7718
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 769.2000 - loglik: -7.5269e+02 - logprior: -1.6508e+01
Epoch 2/10
17/17 - 4s - loss: 751.9214 - loglik: -7.4936e+02 - logprior: -2.5602e+00
Epoch 3/10
17/17 - 4s - loss: 745.6541 - loglik: -7.4611e+02 - logprior: 0.4518
Epoch 4/10
17/17 - 4s - loss: 745.9178 - loglik: -7.4683e+02 - logprior: 0.9120
Fitted a model with MAP estimate = -744.6819
Time for alignment: 75.3783
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 966.0626 - loglik: -9.5311e+02 - logprior: -1.2949e+01
Epoch 2/10
17/17 - 3s - loss: 820.0509 - loglik: -8.1836e+02 - logprior: -1.6912e+00
Epoch 3/10
17/17 - 4s - loss: 768.8553 - loglik: -7.6747e+02 - logprior: -1.3891e+00
Epoch 4/10
17/17 - 3s - loss: 759.6287 - loglik: -7.5860e+02 - logprior: -1.0250e+00
Epoch 5/10
17/17 - 4s - loss: 757.6119 - loglik: -7.5661e+02 - logprior: -1.0040e+00
Epoch 6/10
17/17 - 4s - loss: 755.2315 - loglik: -7.5424e+02 - logprior: -9.8705e-01
Epoch 7/10
17/17 - 4s - loss: 753.1031 - loglik: -7.5207e+02 - logprior: -1.0290e+00
Epoch 8/10
17/17 - 4s - loss: 756.0018 - loglik: -7.5494e+02 - logprior: -1.0615e+00
Fitted a model with MAP estimate = -754.3897
expansions: [(50, 1), (53, 1), (80, 1), (89, 1), (117, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 779.2846 - loglik: -7.6182e+02 - logprior: -1.7467e+01
Epoch 2/2
17/17 - 4s - loss: 759.5097 - loglik: -7.5359e+02 - logprior: -5.9178e+00
Fitted a model with MAP estimate = -757.5669
expansions: [(0, 16)]
discards: [ 0 43 44]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 769.2902 - loglik: -7.5599e+02 - logprior: -1.3305e+01
Epoch 2/2
17/17 - 4s - loss: 748.3723 - loglik: -7.4642e+02 - logprior: -1.9510e+00
Fitted a model with MAP estimate = -744.3897
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 779.1479 - loglik: -7.6344e+02 - logprior: -1.5712e+01
Epoch 2/10
17/17 - 4s - loss: 761.3139 - loglik: -7.5990e+02 - logprior: -1.4167e+00
Epoch 3/10
17/17 - 4s - loss: 758.6666 - loglik: -7.5912e+02 - logprior: 0.4574
Epoch 4/10
17/17 - 4s - loss: 753.7192 - loglik: -7.5453e+02 - logprior: 0.8097
Epoch 5/10
17/17 - 4s - loss: 754.1971 - loglik: -7.5523e+02 - logprior: 1.0368
Fitted a model with MAP estimate = -753.7742
Time for alignment: 90.4176
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 968.4614 - loglik: -9.5556e+02 - logprior: -1.2903e+01
Epoch 2/10
17/17 - 4s - loss: 824.2077 - loglik: -8.2310e+02 - logprior: -1.1050e+00
Epoch 3/10
17/17 - 4s - loss: 771.8643 - loglik: -7.7138e+02 - logprior: -4.8492e-01
Epoch 4/10
17/17 - 3s - loss: 765.5240 - loglik: -7.6538e+02 - logprior: -1.4876e-01
Epoch 5/10
17/17 - 4s - loss: 758.5637 - loglik: -7.5852e+02 - logprior: -4.0466e-02
Epoch 6/10
17/17 - 4s - loss: 755.0548 - loglik: -7.5507e+02 - logprior: 0.0117
Epoch 7/10
17/17 - 4s - loss: 760.2814 - loglik: -7.6028e+02 - logprior: -3.4361e-03
Fitted a model with MAP estimate = -758.1072
expansions: [(0, 37), (24, 2), (25, 1), (50, 1), (60, 1), (80, 1), (89, 1), (108, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 208 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 768.2127 - loglik: -7.5215e+02 - logprior: -1.6063e+01
Epoch 2/2
17/17 - 5s - loss: 723.9536 - loglik: -7.2222e+02 - logprior: -1.7317e+00
Fitted a model with MAP estimate = -714.0423
expansions: [(0, 18)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 757.5785 - loglik: -7.4430e+02 - logprior: -1.3277e+01
Epoch 2/2
17/17 - 4s - loss: 730.1199 - loglik: -7.2978e+02 - logprior: -3.3884e-01
Fitted a model with MAP estimate = -726.4063
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 752.4991 - loglik: -7.4082e+02 - logprior: -1.1680e+01
Epoch 2/10
17/17 - 5s - loss: 725.0190 - loglik: -7.2510e+02 - logprior: 0.0852
Epoch 3/10
17/17 - 5s - loss: 714.2034 - loglik: -7.1560e+02 - logprior: 1.3955
Epoch 4/10
17/17 - 5s - loss: 713.8572 - loglik: -7.1582e+02 - logprior: 1.9591
Epoch 5/10
17/17 - 5s - loss: 712.4462 - loglik: -7.1469e+02 - logprior: 2.2423
Epoch 6/10
17/17 - 5s - loss: 705.7145 - loglik: -7.0819e+02 - logprior: 2.4715
Epoch 7/10
17/17 - 5s - loss: 710.4844 - loglik: -7.1314e+02 - logprior: 2.6580
Fitted a model with MAP estimate = -708.6650
Time for alignment: 104.8467
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 965.3784 - loglik: -9.5245e+02 - logprior: -1.2930e+01
Epoch 2/10
17/17 - 4s - loss: 816.6715 - loglik: -8.1516e+02 - logprior: -1.5120e+00
Epoch 3/10
17/17 - 4s - loss: 765.9915 - loglik: -7.6463e+02 - logprior: -1.3589e+00
Epoch 4/10
17/17 - 4s - loss: 754.5077 - loglik: -7.5354e+02 - logprior: -9.6898e-01
Epoch 5/10
17/17 - 3s - loss: 750.0623 - loglik: -7.4907e+02 - logprior: -9.9139e-01
Epoch 6/10
17/17 - 4s - loss: 752.4573 - loglik: -7.5146e+02 - logprior: -9.9848e-01
Fitted a model with MAP estimate = -750.4301
expansions: [(29, 1), (49, 1), (59, 1), (79, 1), (92, 1), (129, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 775.6891 - loglik: -7.5824e+02 - logprior: -1.7447e+01
Epoch 2/2
17/17 - 4s - loss: 754.4692 - loglik: -7.4850e+02 - logprior: -5.9731e+00
Fitted a model with MAP estimate = -752.9201
expansions: [(0, 16)]
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 763.5257 - loglik: -7.5047e+02 - logprior: -1.3060e+01
Epoch 2/2
17/17 - 4s - loss: 742.6126 - loglik: -7.4064e+02 - logprior: -1.9764e+00
Fitted a model with MAP estimate = -739.8008
expansions: [(61, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 764.0009 - loglik: -7.5166e+02 - logprior: -1.2337e+01
Epoch 2/10
17/17 - 4s - loss: 748.5447 - loglik: -7.4747e+02 - logprior: -1.0752e+00
Epoch 3/10
17/17 - 4s - loss: 748.3143 - loglik: -7.4871e+02 - logprior: 0.3924
Epoch 4/10
17/17 - 4s - loss: 743.7324 - loglik: -7.4459e+02 - logprior: 0.8529
Epoch 5/10
17/17 - 4s - loss: 739.2057 - loglik: -7.4025e+02 - logprior: 1.0473
Epoch 6/10
17/17 - 4s - loss: 744.8285 - loglik: -7.4603e+02 - logprior: 1.2061
Fitted a model with MAP estimate = -741.8495
Time for alignment: 84.7453
Computed alignments with likelihoods: ['-709.7202', '-740.7718', '-744.3897', '-708.6650', '-739.8008']
Best model has likelihood: -708.6650  (prior= 2.7613 )
time for generating output: 0.2795
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.4418360930622511
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 473.4713 - loglik: -4.5321e+02 - logprior: -2.0260e+01
Epoch 2/10
10/10 - 1s - loss: 443.3124 - loglik: -4.3788e+02 - logprior: -5.4297e+00
Epoch 3/10
10/10 - 1s - loss: 425.5667 - loglik: -4.2263e+02 - logprior: -2.9378e+00
Epoch 4/10
10/10 - 1s - loss: 417.0526 - loglik: -4.1470e+02 - logprior: -2.3572e+00
Epoch 5/10
10/10 - 1s - loss: 413.8670 - loglik: -4.1184e+02 - logprior: -2.0276e+00
Epoch 6/10
10/10 - 1s - loss: 411.5722 - loglik: -4.0998e+02 - logprior: -1.5874e+00
Epoch 7/10
10/10 - 1s - loss: 411.0741 - loglik: -4.0969e+02 - logprior: -1.3836e+00
Epoch 8/10
10/10 - 1s - loss: 410.3488 - loglik: -4.0900e+02 - logprior: -1.3461e+00
Epoch 9/10
10/10 - 1s - loss: 410.6311 - loglik: -4.0932e+02 - logprior: -1.3110e+00
Fitted a model with MAP estimate = -410.2203
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 3), (42, 3), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 436.0652 - loglik: -4.0958e+02 - logprior: -2.6481e+01
Epoch 2/2
10/10 - 1s - loss: 414.0412 - loglik: -4.0597e+02 - logprior: -8.0707e+00
Fitted a model with MAP estimate = -409.4973
expansions: []
discards: [ 0 48 50]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 428.7631 - loglik: -4.0564e+02 - logprior: -2.3118e+01
Epoch 2/2
10/10 - 1s - loss: 414.5529 - loglik: -4.0551e+02 - logprior: -9.0417e+00
Fitted a model with MAP estimate = -411.2490
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 424.4235 - loglik: -4.0428e+02 - logprior: -2.0146e+01
Epoch 2/10
10/10 - 1s - loss: 409.0775 - loglik: -4.0379e+02 - logprior: -5.2894e+00
Epoch 3/10
10/10 - 1s - loss: 406.0832 - loglik: -4.0384e+02 - logprior: -2.2447e+00
Epoch 4/10
10/10 - 1s - loss: 404.4162 - loglik: -4.0307e+02 - logprior: -1.3435e+00
Epoch 5/10
10/10 - 1s - loss: 403.4615 - loglik: -4.0246e+02 - logprior: -1.0060e+00
Epoch 6/10
10/10 - 1s - loss: 402.9015 - loglik: -4.0226e+02 - logprior: -6.3838e-01
Epoch 7/10
10/10 - 1s - loss: 402.5637 - loglik: -4.0217e+02 - logprior: -3.9329e-01
Epoch 8/10
10/10 - 1s - loss: 402.8248 - loglik: -4.0252e+02 - logprior: -3.0585e-01
Fitted a model with MAP estimate = -402.4786
Time for alignment: 35.5236
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.6723 - loglik: -4.5241e+02 - logprior: -2.0261e+01
Epoch 2/10
10/10 - 1s - loss: 443.3752 - loglik: -4.3795e+02 - logprior: -5.4300e+00
Epoch 3/10
10/10 - 1s - loss: 425.6351 - loglik: -4.2272e+02 - logprior: -2.9162e+00
Epoch 4/10
10/10 - 1s - loss: 417.5802 - loglik: -4.1523e+02 - logprior: -2.3488e+00
Epoch 5/10
10/10 - 1s - loss: 413.4822 - loglik: -4.1143e+02 - logprior: -2.0538e+00
Epoch 6/10
10/10 - 1s - loss: 412.4873 - loglik: -4.1081e+02 - logprior: -1.6735e+00
Epoch 7/10
10/10 - 1s - loss: 411.2839 - loglik: -4.0980e+02 - logprior: -1.4791e+00
Epoch 8/10
10/10 - 1s - loss: 411.3287 - loglik: -4.0990e+02 - logprior: -1.4298e+00
Fitted a model with MAP estimate = -410.9739
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 3), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 437.1747 - loglik: -4.1087e+02 - logprior: -2.6305e+01
Epoch 2/2
10/10 - 1s - loss: 414.0084 - loglik: -4.0607e+02 - logprior: -7.9403e+00
Fitted a model with MAP estimate = -410.7368
expansions: []
discards: [ 0 49 54]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 430.3867 - loglik: -4.0725e+02 - logprior: -2.3140e+01
Epoch 2/2
10/10 - 1s - loss: 416.1576 - loglik: -4.0710e+02 - logprior: -9.0608e+00
Fitted a model with MAP estimate = -412.9230
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 426.2008 - loglik: -4.0609e+02 - logprior: -2.0107e+01
Epoch 2/10
10/10 - 1s - loss: 410.5904 - loglik: -4.0531e+02 - logprior: -5.2832e+00
Epoch 3/10
10/10 - 1s - loss: 407.9966 - loglik: -4.0575e+02 - logprior: -2.2463e+00
Epoch 4/10
10/10 - 1s - loss: 406.2852 - loglik: -4.0494e+02 - logprior: -1.3435e+00
Epoch 5/10
10/10 - 1s - loss: 405.2738 - loglik: -4.0426e+02 - logprior: -1.0187e+00
Epoch 6/10
10/10 - 1s - loss: 404.3972 - loglik: -4.0373e+02 - logprior: -6.7030e-01
Epoch 7/10
10/10 - 1s - loss: 404.2346 - loglik: -4.0383e+02 - logprior: -4.0126e-01
Epoch 8/10
10/10 - 1s - loss: 404.3910 - loglik: -4.0408e+02 - logprior: -3.1391e-01
Fitted a model with MAP estimate = -404.1012
Time for alignment: 32.5912
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 473.0331 - loglik: -4.5277e+02 - logprior: -2.0262e+01
Epoch 2/10
10/10 - 1s - loss: 443.8859 - loglik: -4.3846e+02 - logprior: -5.4306e+00
Epoch 3/10
10/10 - 1s - loss: 425.8281 - loglik: -4.2288e+02 - logprior: -2.9449e+00
Epoch 4/10
10/10 - 1s - loss: 417.2214 - loglik: -4.1476e+02 - logprior: -2.4583e+00
Epoch 5/10
10/10 - 1s - loss: 412.9200 - loglik: -4.1070e+02 - logprior: -2.2243e+00
Epoch 6/10
10/10 - 1s - loss: 412.0567 - loglik: -4.1023e+02 - logprior: -1.8264e+00
Epoch 7/10
10/10 - 1s - loss: 410.6237 - loglik: -4.0903e+02 - logprior: -1.5912e+00
Epoch 8/10
10/10 - 1s - loss: 411.2011 - loglik: -4.0967e+02 - logprior: -1.5323e+00
Fitted a model with MAP estimate = -410.5131
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (34, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 436.4328 - loglik: -4.1017e+02 - logprior: -2.6261e+01
Epoch 2/2
10/10 - 1s - loss: 413.7791 - loglik: -4.0574e+02 - logprior: -8.0364e+00
Fitted a model with MAP estimate = -410.0850
expansions: []
discards: [ 0 12 54 62]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 430.2632 - loglik: -4.0707e+02 - logprior: -2.3189e+01
Epoch 2/2
10/10 - 1s - loss: 415.1392 - loglik: -4.0599e+02 - logprior: -9.1492e+00
Fitted a model with MAP estimate = -412.2532
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.5956 - loglik: -4.0539e+02 - logprior: -2.0209e+01
Epoch 2/10
10/10 - 1s - loss: 409.5788 - loglik: -4.0422e+02 - logprior: -5.3605e+00
Epoch 3/10
10/10 - 1s - loss: 407.2708 - loglik: -4.0495e+02 - logprior: -2.3238e+00
Epoch 4/10
10/10 - 1s - loss: 405.7029 - loglik: -4.0429e+02 - logprior: -1.4094e+00
Epoch 5/10
10/10 - 1s - loss: 404.1819 - loglik: -4.0310e+02 - logprior: -1.0818e+00
Epoch 6/10
10/10 - 1s - loss: 403.9977 - loglik: -4.0327e+02 - logprior: -7.2679e-01
Epoch 7/10
10/10 - 1s - loss: 403.5605 - loglik: -4.0312e+02 - logprior: -4.4260e-01
Epoch 8/10
10/10 - 1s - loss: 403.7390 - loglik: -4.0339e+02 - logprior: -3.4739e-01
Fitted a model with MAP estimate = -403.4901
Time for alignment: 33.3725
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 473.8326 - loglik: -4.5357e+02 - logprior: -2.0262e+01
Epoch 2/10
10/10 - 1s - loss: 442.4968 - loglik: -4.3706e+02 - logprior: -5.4346e+00
Epoch 3/10
10/10 - 1s - loss: 426.0917 - loglik: -4.2314e+02 - logprior: -2.9468e+00
Epoch 4/10
10/10 - 1s - loss: 417.4850 - loglik: -4.1514e+02 - logprior: -2.3456e+00
Epoch 5/10
10/10 - 1s - loss: 414.2316 - loglik: -4.1218e+02 - logprior: -2.0482e+00
Epoch 6/10
10/10 - 1s - loss: 411.6108 - loglik: -4.0991e+02 - logprior: -1.7029e+00
Epoch 7/10
10/10 - 1s - loss: 411.1791 - loglik: -4.0965e+02 - logprior: -1.5293e+00
Epoch 8/10
10/10 - 1s - loss: 410.7397 - loglik: -4.0923e+02 - logprior: -1.5048e+00
Epoch 9/10
10/10 - 1s - loss: 410.4833 - loglik: -4.0903e+02 - logprior: -1.4569e+00
Epoch 10/10
10/10 - 1s - loss: 410.2368 - loglik: -4.0882e+02 - logprior: -1.4170e+00
Fitted a model with MAP estimate = -410.2241
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 436.5455 - loglik: -4.0993e+02 - logprior: -2.6614e+01
Epoch 2/2
10/10 - 1s - loss: 414.2357 - loglik: -4.0609e+02 - logprior: -8.1452e+00
Fitted a model with MAP estimate = -409.7369
expansions: []
discards: [ 0 51 53 61]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 430.1707 - loglik: -4.0689e+02 - logprior: -2.3278e+01
Epoch 2/2
10/10 - 1s - loss: 415.0068 - loglik: -4.0584e+02 - logprior: -9.1705e+00
Fitted a model with MAP estimate = -412.2987
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.5926 - loglik: -4.0531e+02 - logprior: -2.0285e+01
Epoch 2/10
10/10 - 1s - loss: 409.9045 - loglik: -4.0450e+02 - logprior: -5.4072e+00
Epoch 3/10
10/10 - 1s - loss: 407.2707 - loglik: -4.0493e+02 - logprior: -2.3435e+00
Epoch 4/10
10/10 - 1s - loss: 405.7585 - loglik: -4.0433e+02 - logprior: -1.4304e+00
Epoch 5/10
10/10 - 1s - loss: 404.3943 - loglik: -4.0329e+02 - logprior: -1.1057e+00
Epoch 6/10
10/10 - 1s - loss: 404.0067 - loglik: -4.0324e+02 - logprior: -7.6860e-01
Epoch 7/10
10/10 - 1s - loss: 404.0678 - loglik: -4.0359e+02 - logprior: -4.8215e-01
Fitted a model with MAP estimate = -403.7408
Time for alignment: 32.3132
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 473.6091 - loglik: -4.5335e+02 - logprior: -2.0259e+01
Epoch 2/10
10/10 - 1s - loss: 442.8621 - loglik: -4.3743e+02 - logprior: -5.4277e+00
Epoch 3/10
10/10 - 1s - loss: 426.4935 - loglik: -4.2356e+02 - logprior: -2.9293e+00
Epoch 4/10
10/10 - 1s - loss: 417.4599 - loglik: -4.1508e+02 - logprior: -2.3806e+00
Epoch 5/10
10/10 - 1s - loss: 414.1729 - loglik: -4.1212e+02 - logprior: -2.0507e+00
Epoch 6/10
10/10 - 1s - loss: 412.8290 - loglik: -4.1120e+02 - logprior: -1.6271e+00
Epoch 7/10
10/10 - 1s - loss: 411.9032 - loglik: -4.1046e+02 - logprior: -1.4445e+00
Epoch 8/10
10/10 - 1s - loss: 410.8227 - loglik: -4.0942e+02 - logprior: -1.4005e+00
Epoch 9/10
10/10 - 1s - loss: 411.2428 - loglik: -4.0992e+02 - logprior: -1.3257e+00
Fitted a model with MAP estimate = -410.9569
expansions: [(0, 2), (8, 1), (9, 1), (23, 2), (41, 2), (42, 3), (43, 1), (44, 2), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 437.3265 - loglik: -4.1090e+02 - logprior: -2.6426e+01
Epoch 2/2
10/10 - 1s - loss: 415.1363 - loglik: -4.0705e+02 - logprior: -8.0849e+00
Fitted a model with MAP estimate = -410.7361
expansions: []
discards: [ 0 27 54]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 430.4505 - loglik: -4.0727e+02 - logprior: -2.3184e+01
Epoch 2/2
10/10 - 1s - loss: 415.7553 - loglik: -4.0664e+02 - logprior: -9.1190e+00
Fitted a model with MAP estimate = -412.8364
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 426.0522 - loglik: -4.0580e+02 - logprior: -2.0249e+01
Epoch 2/10
10/10 - 1s - loss: 410.8801 - loglik: -4.0551e+02 - logprior: -5.3721e+00
Epoch 3/10
10/10 - 1s - loss: 407.6417 - loglik: -4.0532e+02 - logprior: -2.3209e+00
Epoch 4/10
10/10 - 1s - loss: 406.0651 - loglik: -4.0469e+02 - logprior: -1.3783e+00
Epoch 5/10
10/10 - 1s - loss: 404.8602 - loglik: -4.0382e+02 - logprior: -1.0353e+00
Epoch 6/10
10/10 - 1s - loss: 404.4623 - loglik: -4.0378e+02 - logprior: -6.8386e-01
Epoch 7/10
10/10 - 1s - loss: 404.0727 - loglik: -4.0368e+02 - logprior: -3.9172e-01
Epoch 8/10
10/10 - 1s - loss: 404.2691 - loglik: -4.0398e+02 - logprior: -2.8489e-01
Fitted a model with MAP estimate = -403.8506
Time for alignment: 32.5500
Computed alignments with likelihoods: ['-402.4786', '-404.1012', '-403.4901', '-403.7408', '-403.8506']
Best model has likelihood: -402.4786  (prior= -0.2634 )
time for generating output: 0.1317
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6805970149253732
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 401.2004 - loglik: -3.8971e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 359.9984 - loglik: -3.5674e+02 - logprior: -3.2564e+00
Epoch 3/10
11/11 - 1s - loss: 325.6163 - loglik: -3.2319e+02 - logprior: -2.4305e+00
Epoch 4/10
11/11 - 1s - loss: 311.5475 - loglik: -3.0948e+02 - logprior: -2.0708e+00
Epoch 5/10
11/11 - 1s - loss: 307.0553 - loglik: -3.0519e+02 - logprior: -1.8609e+00
Epoch 6/10
11/11 - 1s - loss: 306.8665 - loglik: -3.0500e+02 - logprior: -1.8675e+00
Epoch 7/10
11/11 - 1s - loss: 306.0465 - loglik: -3.0427e+02 - logprior: -1.7810e+00
Epoch 8/10
11/11 - 1s - loss: 305.4840 - loglik: -3.0372e+02 - logprior: -1.7636e+00
Epoch 9/10
11/11 - 1s - loss: 304.9840 - loglik: -3.0324e+02 - logprior: -1.7443e+00
Epoch 10/10
11/11 - 1s - loss: 305.0257 - loglik: -3.0329e+02 - logprior: -1.7387e+00
Fitted a model with MAP estimate = -304.9491
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 315.1494 - loglik: -3.0145e+02 - logprior: -1.3695e+01
Epoch 2/2
11/11 - 1s - loss: 298.0518 - loglik: -2.9386e+02 - logprior: -4.1942e+00
Fitted a model with MAP estimate = -295.6963
expansions: []
discards: [ 0 35 43]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 308.0074 - loglik: -2.9490e+02 - logprior: -1.3111e+01
Epoch 2/2
11/11 - 1s - loss: 299.0629 - loglik: -2.9356e+02 - logprior: -5.5001e+00
Fitted a model with MAP estimate = -296.7181
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.6041 - loglik: -2.9382e+02 - logprior: -1.0780e+01
Epoch 2/10
11/11 - 1s - loss: 295.6081 - loglik: -2.9266e+02 - logprior: -2.9444e+00
Epoch 3/10
11/11 - 1s - loss: 294.5467 - loglik: -2.9280e+02 - logprior: -1.7441e+00
Epoch 4/10
11/11 - 1s - loss: 293.0093 - loglik: -2.9148e+02 - logprior: -1.5301e+00
Epoch 5/10
11/11 - 1s - loss: 293.2413 - loglik: -2.9177e+02 - logprior: -1.4681e+00
Fitted a model with MAP estimate = -292.9300
Time for alignment: 30.1735
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 402.1747 - loglik: -3.9069e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 360.0341 - loglik: -3.5677e+02 - logprior: -3.2633e+00
Epoch 3/10
11/11 - 1s - loss: 330.0448 - loglik: -3.2760e+02 - logprior: -2.4462e+00
Epoch 4/10
11/11 - 1s - loss: 313.8987 - loglik: -3.1186e+02 - logprior: -2.0433e+00
Epoch 5/10
11/11 - 1s - loss: 307.9129 - loglik: -3.0606e+02 - logprior: -1.8531e+00
Epoch 6/10
11/11 - 1s - loss: 307.0169 - loglik: -3.0512e+02 - logprior: -1.8988e+00
Epoch 7/10
11/11 - 1s - loss: 304.8803 - loglik: -3.0304e+02 - logprior: -1.8431e+00
Epoch 8/10
11/11 - 1s - loss: 305.4386 - loglik: -3.0358e+02 - logprior: -1.8591e+00
Fitted a model with MAP estimate = -304.8674
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 314.4204 - loglik: -3.0082e+02 - logprior: -1.3602e+01
Epoch 2/2
11/11 - 1s - loss: 298.4759 - loglik: -2.9434e+02 - logprior: -4.1380e+00
Fitted a model with MAP estimate = -295.5374
expansions: []
discards: [ 0 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 308.7540 - loglik: -2.9563e+02 - logprior: -1.3119e+01
Epoch 2/2
11/11 - 1s - loss: 298.2073 - loglik: -2.9268e+02 - logprior: -5.5241e+00
Fitted a model with MAP estimate = -296.7316
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 304.3346 - loglik: -2.9357e+02 - logprior: -1.0763e+01
Epoch 2/10
11/11 - 1s - loss: 295.9111 - loglik: -2.9296e+02 - logprior: -2.9483e+00
Epoch 3/10
11/11 - 1s - loss: 293.8172 - loglik: -2.9207e+02 - logprior: -1.7497e+00
Epoch 4/10
11/11 - 1s - loss: 293.5024 - loglik: -2.9196e+02 - logprior: -1.5441e+00
Epoch 5/10
11/11 - 1s - loss: 293.8573 - loglik: -2.9238e+02 - logprior: -1.4766e+00
Fitted a model with MAP estimate = -292.9200
Time for alignment: 28.9538
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 401.2682 - loglik: -3.8978e+02 - logprior: -1.1488e+01
Epoch 2/10
11/11 - 1s - loss: 359.9859 - loglik: -3.5672e+02 - logprior: -3.2650e+00
Epoch 3/10
11/11 - 1s - loss: 326.3607 - loglik: -3.2392e+02 - logprior: -2.4447e+00
Epoch 4/10
11/11 - 1s - loss: 311.3261 - loglik: -3.0925e+02 - logprior: -2.0797e+00
Epoch 5/10
11/11 - 1s - loss: 307.9177 - loglik: -3.0602e+02 - logprior: -1.8959e+00
Epoch 6/10
11/11 - 1s - loss: 305.5892 - loglik: -3.0369e+02 - logprior: -1.9003e+00
Epoch 7/10
11/11 - 1s - loss: 305.8048 - loglik: -3.0398e+02 - logprior: -1.8248e+00
Fitted a model with MAP estimate = -305.1353
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 315.7957 - loglik: -3.0225e+02 - logprior: -1.3545e+01
Epoch 2/2
11/11 - 1s - loss: 298.4716 - loglik: -2.9432e+02 - logprior: -4.1529e+00
Fitted a model with MAP estimate = -295.7723
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 308.4265 - loglik: -2.9531e+02 - logprior: -1.3117e+01
Epoch 2/2
11/11 - 1s - loss: 298.7730 - loglik: -2.9325e+02 - logprior: -5.5215e+00
Fitted a model with MAP estimate = -296.8003
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 304.5835 - loglik: -2.9383e+02 - logprior: -1.0750e+01
Epoch 2/10
11/11 - 1s - loss: 295.5940 - loglik: -2.9266e+02 - logprior: -2.9370e+00
Epoch 3/10
11/11 - 1s - loss: 294.3808 - loglik: -2.9263e+02 - logprior: -1.7506e+00
Epoch 4/10
11/11 - 1s - loss: 293.3556 - loglik: -2.9181e+02 - logprior: -1.5428e+00
Epoch 5/10
11/11 - 1s - loss: 293.6538 - loglik: -2.9218e+02 - logprior: -1.4757e+00
Fitted a model with MAP estimate = -292.9275
Time for alignment: 27.4963
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 401.1130 - loglik: -3.8963e+02 - logprior: -1.1488e+01
Epoch 2/10
11/11 - 1s - loss: 361.5569 - loglik: -3.5831e+02 - logprior: -3.2482e+00
Epoch 3/10
11/11 - 1s - loss: 329.0999 - loglik: -3.2668e+02 - logprior: -2.4236e+00
Epoch 4/10
11/11 - 1s - loss: 313.5893 - loglik: -3.1153e+02 - logprior: -2.0627e+00
Epoch 5/10
11/11 - 1s - loss: 308.6858 - loglik: -3.0684e+02 - logprior: -1.8501e+00
Epoch 6/10
11/11 - 1s - loss: 307.4868 - loglik: -3.0560e+02 - logprior: -1.8835e+00
Epoch 7/10
11/11 - 1s - loss: 305.8711 - loglik: -3.0406e+02 - logprior: -1.8072e+00
Epoch 8/10
11/11 - 1s - loss: 306.8683 - loglik: -3.0508e+02 - logprior: -1.7871e+00
Fitted a model with MAP estimate = -305.9188
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 315.7088 - loglik: -3.0208e+02 - logprior: -1.3633e+01
Epoch 2/2
11/11 - 1s - loss: 298.5135 - loglik: -2.9432e+02 - logprior: -4.1949e+00
Fitted a model with MAP estimate = -295.6295
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 308.2517 - loglik: -2.9513e+02 - logprior: -1.3120e+01
Epoch 2/2
11/11 - 1s - loss: 298.8671 - loglik: -2.9335e+02 - logprior: -5.5217e+00
Fitted a model with MAP estimate = -296.7715
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 304.4568 - loglik: -2.9368e+02 - logprior: -1.0777e+01
Epoch 2/10
11/11 - 1s - loss: 295.7759 - loglik: -2.9283e+02 - logprior: -2.9446e+00
Epoch 3/10
11/11 - 1s - loss: 294.5375 - loglik: -2.9279e+02 - logprior: -1.7465e+00
Epoch 4/10
11/11 - 1s - loss: 293.4053 - loglik: -2.9187e+02 - logprior: -1.5382e+00
Epoch 5/10
11/11 - 1s - loss: 292.8589 - loglik: -2.9139e+02 - logprior: -1.4722e+00
Epoch 6/10
11/11 - 1s - loss: 292.8622 - loglik: -2.9154e+02 - logprior: -1.3182e+00
Fitted a model with MAP estimate = -292.7140
Time for alignment: 28.3199
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 402.3336 - loglik: -3.9085e+02 - logprior: -1.1487e+01
Epoch 2/10
11/11 - 1s - loss: 360.4158 - loglik: -3.5718e+02 - logprior: -3.2396e+00
Epoch 3/10
11/11 - 1s - loss: 329.3639 - loglik: -3.2693e+02 - logprior: -2.4304e+00
Epoch 4/10
11/11 - 1s - loss: 312.2155 - loglik: -3.1012e+02 - logprior: -2.0909e+00
Epoch 5/10
11/11 - 1s - loss: 308.5318 - loglik: -3.0668e+02 - logprior: -1.8526e+00
Epoch 6/10
11/11 - 1s - loss: 306.8432 - loglik: -3.0497e+02 - logprior: -1.8771e+00
Epoch 7/10
11/11 - 1s - loss: 306.1471 - loglik: -3.0436e+02 - logprior: -1.7874e+00
Epoch 8/10
11/11 - 1s - loss: 305.9769 - loglik: -3.0420e+02 - logprior: -1.7743e+00
Epoch 9/10
11/11 - 1s - loss: 306.2403 - loglik: -3.0449e+02 - logprior: -1.7471e+00
Fitted a model with MAP estimate = -305.6715
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 313.9467 - loglik: -3.0031e+02 - logprior: -1.3639e+01
Epoch 2/2
11/11 - 1s - loss: 298.2316 - loglik: -2.9411e+02 - logprior: -4.1233e+00
Fitted a model with MAP estimate = -295.4918
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 308.0751 - loglik: -2.9497e+02 - logprior: -1.3108e+01
Epoch 2/2
11/11 - 1s - loss: 298.5118 - loglik: -2.9301e+02 - logprior: -5.5019e+00
Fitted a model with MAP estimate = -296.7195
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 304.0526 - loglik: -2.9327e+02 - logprior: -1.0778e+01
Epoch 2/10
11/11 - 1s - loss: 296.2508 - loglik: -2.9332e+02 - logprior: -2.9327e+00
Epoch 3/10
11/11 - 1s - loss: 294.4439 - loglik: -2.9270e+02 - logprior: -1.7390e+00
Epoch 4/10
11/11 - 1s - loss: 293.0134 - loglik: -2.9148e+02 - logprior: -1.5340e+00
Epoch 5/10
11/11 - 1s - loss: 293.1429 - loglik: -2.9168e+02 - logprior: -1.4638e+00
Fitted a model with MAP estimate = -292.9231
Time for alignment: 28.7604
Computed alignments with likelihoods: ['-292.9300', '-292.9200', '-292.9275', '-292.7140', '-292.9231']
Best model has likelihood: -292.7140  (prior= -1.2400 )
time for generating output: 0.1063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 445.3629 - loglik: -4.4464e+02 - logprior: -7.1828e-01
Epoch 2/10
42/42 - 3s - loss: 355.0748 - loglik: -3.5411e+02 - logprior: -9.6417e-01
Epoch 3/10
42/42 - 3s - loss: 352.6057 - loglik: -3.5167e+02 - logprior: -9.3915e-01
Epoch 4/10
42/42 - 3s - loss: 351.7923 - loglik: -3.5086e+02 - logprior: -9.3032e-01
Epoch 5/10
42/42 - 3s - loss: 350.8904 - loglik: -3.4997e+02 - logprior: -9.2248e-01
Epoch 6/10
42/42 - 3s - loss: 350.7586 - loglik: -3.4984e+02 - logprior: -9.1577e-01
Epoch 7/10
42/42 - 3s - loss: 350.6813 - loglik: -3.4977e+02 - logprior: -9.1159e-01
Epoch 8/10
42/42 - 3s - loss: 349.8990 - loglik: -3.4899e+02 - logprior: -9.0946e-01
Epoch 9/10
42/42 - 3s - loss: 350.2908 - loglik: -3.4938e+02 - logprior: -9.0596e-01
Fitted a model with MAP estimate = -347.3688
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 321.4672 - loglik: -3.2050e+02 - logprior: -9.7045e-01
Epoch 2/2
42/42 - 4s - loss: 309.8156 - loglik: -3.0908e+02 - logprior: -7.3906e-01
Fitted a model with MAP estimate = -305.5071
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 305.9251 - loglik: -3.0532e+02 - logprior: -6.0102e-01
Epoch 2/10
59/59 - 5s - loss: 304.7451 - loglik: -3.0424e+02 - logprior: -5.0217e-01
Epoch 3/10
59/59 - 5s - loss: 305.2505 - loglik: -3.0476e+02 - logprior: -4.9353e-01
Fitted a model with MAP estimate = -303.4156
Time for alignment: 110.7691
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 446.4410 - loglik: -4.4572e+02 - logprior: -7.1651e-01
Epoch 2/10
42/42 - 3s - loss: 355.7647 - loglik: -3.5480e+02 - logprior: -9.6007e-01
Epoch 3/10
42/42 - 3s - loss: 352.1527 - loglik: -3.5121e+02 - logprior: -9.3991e-01
Epoch 4/10
42/42 - 3s - loss: 351.6965 - loglik: -3.5076e+02 - logprior: -9.3229e-01
Epoch 5/10
42/42 - 3s - loss: 350.6775 - loglik: -3.4975e+02 - logprior: -9.2560e-01
Epoch 6/10
42/42 - 3s - loss: 350.5443 - loglik: -3.4963e+02 - logprior: -9.1702e-01
Epoch 7/10
42/42 - 3s - loss: 350.2539 - loglik: -3.4934e+02 - logprior: -9.1281e-01
Epoch 8/10
42/42 - 3s - loss: 350.2162 - loglik: -3.4931e+02 - logprior: -9.0996e-01
Epoch 9/10
42/42 - 3s - loss: 349.9672 - loglik: -3.4906e+02 - logprior: -9.0813e-01
Epoch 10/10
42/42 - 3s - loss: 349.5207 - loglik: -3.4861e+02 - logprior: -9.0675e-01
Fitted a model with MAP estimate = -347.0846
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 321.6550 - loglik: -3.2068e+02 - logprior: -9.7319e-01
Epoch 2/2
42/42 - 3s - loss: 309.5125 - loglik: -3.0877e+02 - logprior: -7.4064e-01
Fitted a model with MAP estimate = -305.4752
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 305.7597 - loglik: -3.0516e+02 - logprior: -5.9950e-01
Epoch 2/10
59/59 - 5s - loss: 305.2492 - loglik: -3.0475e+02 - logprior: -5.0260e-01
Epoch 3/10
59/59 - 5s - loss: 304.8265 - loglik: -3.0434e+02 - logprior: -4.9103e-01
Epoch 4/10
59/59 - 5s - loss: 303.5659 - loglik: -3.0309e+02 - logprior: -4.8043e-01
Epoch 5/10
59/59 - 5s - loss: 303.1643 - loglik: -3.0269e+02 - logprior: -4.6993e-01
Epoch 6/10
59/59 - 5s - loss: 303.1602 - loglik: -3.0269e+02 - logprior: -4.6771e-01
Epoch 7/10
59/59 - 5s - loss: 302.6525 - loglik: -3.0219e+02 - logprior: -4.6204e-01
Epoch 8/10
59/59 - 5s - loss: 302.0436 - loglik: -3.0159e+02 - logprior: -4.5727e-01
Epoch 9/10
59/59 - 5s - loss: 302.6653 - loglik: -3.0221e+02 - logprior: -4.5264e-01
Fitted a model with MAP estimate = -302.0939
Time for alignment: 140.4787
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 445.5348 - loglik: -4.4482e+02 - logprior: -7.1712e-01
Epoch 2/10
42/42 - 3s - loss: 355.0083 - loglik: -3.5405e+02 - logprior: -9.6294e-01
Epoch 3/10
42/42 - 3s - loss: 352.2496 - loglik: -3.5131e+02 - logprior: -9.3800e-01
Epoch 4/10
42/42 - 3s - loss: 351.9378 - loglik: -3.5101e+02 - logprior: -9.2924e-01
Epoch 5/10
42/42 - 3s - loss: 350.9718 - loglik: -3.5005e+02 - logprior: -9.2316e-01
Epoch 6/10
42/42 - 3s - loss: 350.6505 - loglik: -3.4973e+02 - logprior: -9.1561e-01
Epoch 7/10
42/42 - 3s - loss: 350.4720 - loglik: -3.4956e+02 - logprior: -9.1142e-01
Epoch 8/10
42/42 - 3s - loss: 350.2188 - loglik: -3.4931e+02 - logprior: -9.0894e-01
Epoch 9/10
42/42 - 3s - loss: 349.9202 - loglik: -3.4901e+02 - logprior: -9.0608e-01
Epoch 10/10
42/42 - 3s - loss: 350.2611 - loglik: -3.4936e+02 - logprior: -9.0568e-01
Fitted a model with MAP estimate = -347.2854
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 321.5346 - loglik: -3.2056e+02 - logprior: -9.7067e-01
Epoch 2/2
42/42 - 4s - loss: 309.8728 - loglik: -3.0914e+02 - logprior: -7.3618e-01
Fitted a model with MAP estimate = -305.4789
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 305.5821 - loglik: -3.0498e+02 - logprior: -6.0071e-01
Epoch 2/10
59/59 - 5s - loss: 305.0583 - loglik: -3.0456e+02 - logprior: -5.0086e-01
Epoch 3/10
59/59 - 5s - loss: 305.0768 - loglik: -3.0458e+02 - logprior: -4.9283e-01
Fitted a model with MAP estimate = -303.4209
Time for alignment: 110.5773
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 445.8759 - loglik: -4.4516e+02 - logprior: -7.1767e-01
Epoch 2/10
42/42 - 3s - loss: 355.1473 - loglik: -3.5419e+02 - logprior: -9.6192e-01
Epoch 3/10
42/42 - 3s - loss: 352.8428 - loglik: -3.5190e+02 - logprior: -9.3791e-01
Epoch 4/10
42/42 - 3s - loss: 351.5382 - loglik: -3.5061e+02 - logprior: -9.3225e-01
Epoch 5/10
42/42 - 3s - loss: 351.0248 - loglik: -3.5010e+02 - logprior: -9.2325e-01
Epoch 6/10
42/42 - 3s - loss: 350.6048 - loglik: -3.4969e+02 - logprior: -9.1580e-01
Epoch 7/10
42/42 - 3s - loss: 350.7284 - loglik: -3.4982e+02 - logprior: -9.1295e-01
Fitted a model with MAP estimate = -347.5567
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 321.9340 - loglik: -3.2097e+02 - logprior: -9.5992e-01
Epoch 2/2
42/42 - 3s - loss: 309.4471 - loglik: -3.0870e+02 - logprior: -7.4295e-01
Fitted a model with MAP estimate = -305.6353
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 306.0109 - loglik: -3.0541e+02 - logprior: -6.0507e-01
Epoch 2/10
59/59 - 5s - loss: 305.2040 - loglik: -3.0470e+02 - logprior: -5.0309e-01
Epoch 3/10
59/59 - 5s - loss: 304.6234 - loglik: -3.0413e+02 - logprior: -4.9075e-01
Epoch 4/10
59/59 - 5s - loss: 303.6351 - loglik: -3.0316e+02 - logprior: -4.7908e-01
Epoch 5/10
59/59 - 5s - loss: 303.1641 - loglik: -3.0269e+02 - logprior: -4.7207e-01
Epoch 6/10
59/59 - 5s - loss: 302.9932 - loglik: -3.0253e+02 - logprior: -4.6673e-01
Epoch 7/10
59/59 - 5s - loss: 302.4605 - loglik: -3.0200e+02 - logprior: -4.6329e-01
Epoch 8/10
59/59 - 5s - loss: 302.1823 - loglik: -3.0173e+02 - logprior: -4.5659e-01
Epoch 9/10
59/59 - 5s - loss: 302.7906 - loglik: -3.0234e+02 - logprior: -4.5033e-01
Fitted a model with MAP estimate = -302.1152
Time for alignment: 130.4295
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 445.1403 - loglik: -4.4442e+02 - logprior: -7.2042e-01
Epoch 2/10
42/42 - 3s - loss: 355.1253 - loglik: -3.5416e+02 - logprior: -9.6124e-01
Epoch 3/10
42/42 - 3s - loss: 352.4236 - loglik: -3.5149e+02 - logprior: -9.3723e-01
Epoch 4/10
42/42 - 3s - loss: 351.5280 - loglik: -3.5060e+02 - logprior: -9.3000e-01
Epoch 5/10
42/42 - 3s - loss: 351.0094 - loglik: -3.5009e+02 - logprior: -9.2184e-01
Epoch 6/10
42/42 - 3s - loss: 350.4517 - loglik: -3.4954e+02 - logprior: -9.1458e-01
Epoch 7/10
42/42 - 3s - loss: 350.7063 - loglik: -3.4979e+02 - logprior: -9.1156e-01
Fitted a model with MAP estimate = -347.5696
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 321.4039 - loglik: -3.2043e+02 - logprior: -9.7140e-01
Epoch 2/2
42/42 - 4s - loss: 309.8509 - loglik: -3.0911e+02 - logprior: -7.3893e-01
Fitted a model with MAP estimate = -305.4975
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 305.8061 - loglik: -3.0520e+02 - logprior: -6.0124e-01
Epoch 2/10
59/59 - 5s - loss: 304.9357 - loglik: -3.0443e+02 - logprior: -5.0105e-01
Epoch 3/10
59/59 - 5s - loss: 304.9794 - loglik: -3.0449e+02 - logprior: -4.9074e-01
Fitted a model with MAP estimate = -303.4029
Time for alignment: 100.7170
Computed alignments with likelihoods: ['-303.4156', '-302.0939', '-303.4209', '-302.1152', '-303.4029']
Best model has likelihood: -302.0939  (prior= -0.4539 )
time for generating output: 0.1376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.5886332882273342
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 939.7298 - loglik: -9.1641e+02 - logprior: -2.3319e+01
Epoch 2/10
14/14 - 3s - loss: 873.1724 - loglik: -8.6974e+02 - logprior: -3.4349e+00
Epoch 3/10
14/14 - 3s - loss: 836.8593 - loglik: -8.3543e+02 - logprior: -1.4331e+00
Epoch 4/10
14/14 - 3s - loss: 826.0303 - loglik: -8.2500e+02 - logprior: -1.0255e+00
Epoch 5/10
14/14 - 3s - loss: 823.6330 - loglik: -8.2277e+02 - logprior: -8.6059e-01
Epoch 6/10
14/14 - 3s - loss: 820.5831 - loglik: -8.1990e+02 - logprior: -6.8722e-01
Epoch 7/10
14/14 - 3s - loss: 821.1749 - loglik: -8.2054e+02 - logprior: -6.3281e-01
Fitted a model with MAP estimate = -819.7370
expansions: [(10, 1), (11, 1), (16, 4), (18, 1), (35, 1), (36, 3), (38, 2), (43, 1), (44, 2), (66, 2), (76, 1), (78, 1), (80, 2), (101, 1), (102, 3), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 847.4286 - loglik: -8.1957e+02 - logprior: -2.7861e+01
Epoch 2/2
14/14 - 3s - loss: 822.0827 - loglik: -8.1235e+02 - logprior: -9.7340e+00
Fitted a model with MAP estimate = -816.9156
expansions: [(0, 21), (58, 2), (81, 1)]
discards: [  0  12  13  44  48 123]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 834.4858 - loglik: -8.1359e+02 - logprior: -2.0894e+01
Epoch 2/2
14/14 - 4s - loss: 810.8178 - loglik: -8.0803e+02 - logprior: -2.7831e+00
Fitted a model with MAP estimate = -808.7059
expansions: [(32, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 834.1339 - loglik: -8.0918e+02 - logprior: -2.4958e+01
Epoch 2/10
14/14 - 3s - loss: 813.0741 - loglik: -8.0994e+02 - logprior: -3.1337e+00
Epoch 3/10
14/14 - 3s - loss: 807.6728 - loglik: -8.0831e+02 - logprior: 0.6365
Epoch 4/10
14/14 - 3s - loss: 805.5236 - loglik: -8.0727e+02 - logprior: 1.7514
Epoch 5/10
14/14 - 3s - loss: 803.0473 - loglik: -8.0538e+02 - logprior: 2.3359
Epoch 6/10
14/14 - 3s - loss: 804.9754 - loglik: -8.0766e+02 - logprior: 2.6797
Fitted a model with MAP estimate = -803.2983
Time for alignment: 73.5207
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 937.4561 - loglik: -9.1414e+02 - logprior: -2.3319e+01
Epoch 2/10
14/14 - 3s - loss: 876.7992 - loglik: -8.7332e+02 - logprior: -3.4815e+00
Epoch 3/10
14/14 - 3s - loss: 836.1238 - loglik: -8.3462e+02 - logprior: -1.5077e+00
Epoch 4/10
14/14 - 3s - loss: 824.4188 - loglik: -8.2335e+02 - logprior: -1.0669e+00
Epoch 5/10
14/14 - 3s - loss: 822.0894 - loglik: -8.2124e+02 - logprior: -8.4658e-01
Epoch 6/10
14/14 - 3s - loss: 821.3707 - loglik: -8.2070e+02 - logprior: -6.7546e-01
Epoch 7/10
14/14 - 3s - loss: 817.4893 - loglik: -8.1693e+02 - logprior: -5.6394e-01
Epoch 8/10
14/14 - 3s - loss: 820.8235 - loglik: -8.2033e+02 - logprior: -4.9578e-01
Fitted a model with MAP estimate = -819.3278
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (34, 1), (36, 2), (41, 1), (44, 1), (66, 2), (79, 5), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 850.1236 - loglik: -8.2222e+02 - logprior: -2.7903e+01
Epoch 2/2
14/14 - 3s - loss: 821.6865 - loglik: -8.1195e+02 - logprior: -9.7334e+00
Fitted a model with MAP estimate = -818.5387
expansions: [(0, 24), (16, 1), (79, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 833.8636 - loglik: -8.1292e+02 - logprior: -2.0939e+01
Epoch 2/2
14/14 - 4s - loss: 813.5704 - loglik: -8.1077e+02 - logprior: -2.8039e+00
Fitted a model with MAP estimate = -809.9868
expansions: [(80, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 35]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 836.6713 - loglik: -8.1159e+02 - logprior: -2.5083e+01
Epoch 2/10
14/14 - 3s - loss: 809.2046 - loglik: -8.0591e+02 - logprior: -3.2927e+00
Epoch 3/10
14/14 - 3s - loss: 807.1385 - loglik: -8.0767e+02 - logprior: 0.5354
Epoch 4/10
14/14 - 3s - loss: 803.3353 - loglik: -8.0496e+02 - logprior: 1.6281
Epoch 5/10
14/14 - 3s - loss: 804.0099 - loglik: -8.0609e+02 - logprior: 2.0758
Fitted a model with MAP estimate = -802.7397
Time for alignment: 71.3903
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 940.9263 - loglik: -9.1761e+02 - logprior: -2.3320e+01
Epoch 2/10
14/14 - 3s - loss: 872.6308 - loglik: -8.6918e+02 - logprior: -3.4459e+00
Epoch 3/10
14/14 - 3s - loss: 836.9053 - loglik: -8.3545e+02 - logprior: -1.4585e+00
Epoch 4/10
14/14 - 3s - loss: 828.6436 - loglik: -8.2750e+02 - logprior: -1.1396e+00
Epoch 5/10
14/14 - 3s - loss: 821.6270 - loglik: -8.2060e+02 - logprior: -1.0254e+00
Epoch 6/10
14/14 - 3s - loss: 820.5413 - loglik: -8.1965e+02 - logprior: -8.8799e-01
Epoch 7/10
14/14 - 3s - loss: 821.1758 - loglik: -8.2040e+02 - logprior: -7.7515e-01
Fitted a model with MAP estimate = -820.2753
expansions: [(10, 1), (11, 1), (16, 4), (18, 1), (35, 1), (36, 3), (38, 1), (41, 1), (44, 1), (66, 1), (67, 1), (74, 1), (79, 5), (100, 1), (102, 1), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 850.0278 - loglik: -8.2220e+02 - logprior: -2.7831e+01
Epoch 2/2
14/14 - 3s - loss: 820.0590 - loglik: -8.1029e+02 - logprior: -9.7661e+00
Fitted a model with MAP estimate = -817.4448
expansions: [(0, 23), (19, 1), (58, 2)]
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 828.5216 - loglik: -8.0762e+02 - logprior: -2.0903e+01
Epoch 2/2
14/14 - 4s - loss: 812.7592 - loglik: -8.1009e+02 - logprior: -2.6645e+00
Fitted a model with MAP estimate = -806.3556
expansions: [(79, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 829.8859 - loglik: -8.0537e+02 - logprior: -2.4514e+01
Epoch 2/10
14/14 - 3s - loss: 810.2475 - loglik: -8.0740e+02 - logprior: -2.8477e+00
Epoch 3/10
14/14 - 3s - loss: 805.5744 - loglik: -8.0635e+02 - logprior: 0.7740
Epoch 4/10
14/14 - 3s - loss: 800.3382 - loglik: -8.0225e+02 - logprior: 1.9164
Epoch 5/10
14/14 - 3s - loss: 799.8569 - loglik: -8.0235e+02 - logprior: 2.4933
Epoch 6/10
14/14 - 3s - loss: 801.7094 - loglik: -8.0454e+02 - logprior: 2.8349
Fitted a model with MAP estimate = -800.2550
Time for alignment: 74.5301
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 939.9207 - loglik: -9.1661e+02 - logprior: -2.3313e+01
Epoch 2/10
14/14 - 3s - loss: 873.3898 - loglik: -8.6991e+02 - logprior: -3.4778e+00
Epoch 3/10
14/14 - 3s - loss: 834.1550 - loglik: -8.3236e+02 - logprior: -1.7914e+00
Epoch 4/10
14/14 - 3s - loss: 826.5641 - loglik: -8.2499e+02 - logprior: -1.5770e+00
Epoch 5/10
14/14 - 3s - loss: 821.5197 - loglik: -8.2014e+02 - logprior: -1.3783e+00
Epoch 6/10
14/14 - 3s - loss: 818.1508 - loglik: -8.1698e+02 - logprior: -1.1722e+00
Epoch 7/10
14/14 - 3s - loss: 819.9767 - loglik: -8.1892e+02 - logprior: -1.0588e+00
Fitted a model with MAP estimate = -818.7417
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (36, 2), (38, 1), (41, 1), (42, 1), (44, 1), (57, 1), (65, 1), (66, 1), (76, 1), (78, 2), (79, 3), (100, 1), (102, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 842.8056 - loglik: -8.1510e+02 - logprior: -2.7705e+01
Epoch 2/2
14/14 - 3s - loss: 825.0178 - loglik: -8.1546e+02 - logprior: -9.5563e+00
Fitted a model with MAP estimate = -817.2490
expansions: [(0, 21), (56, 2)]
discards: [ 0 12 13 94]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 834.1216 - loglik: -8.1321e+02 - logprior: -2.0913e+01
Epoch 2/2
14/14 - 4s - loss: 812.5791 - loglik: -8.0982e+02 - logprior: -2.7604e+00
Fitted a model with MAP estimate = -809.6133
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 76]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 835.0189 - loglik: -8.1017e+02 - logprior: -2.4848e+01
Epoch 2/10
14/14 - 3s - loss: 813.8505 - loglik: -8.1067e+02 - logprior: -3.1763e+00
Epoch 3/10
14/14 - 3s - loss: 808.8300 - loglik: -8.0934e+02 - logprior: 0.5114
Epoch 4/10
14/14 - 3s - loss: 808.0142 - loglik: -8.0963e+02 - logprior: 1.6163
Epoch 5/10
14/14 - 3s - loss: 802.7083 - loglik: -8.0493e+02 - logprior: 2.2177
Epoch 6/10
14/14 - 3s - loss: 805.3983 - loglik: -8.0795e+02 - logprior: 2.5548
Fitted a model with MAP estimate = -804.7015
Time for alignment: 71.7028
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 936.5790 - loglik: -9.1326e+02 - logprior: -2.3323e+01
Epoch 2/10
14/14 - 3s - loss: 877.6118 - loglik: -8.7414e+02 - logprior: -3.4670e+00
Epoch 3/10
14/14 - 3s - loss: 836.0814 - loglik: -8.3457e+02 - logprior: -1.5093e+00
Epoch 4/10
14/14 - 3s - loss: 826.5140 - loglik: -8.2526e+02 - logprior: -1.2515e+00
Epoch 5/10
14/14 - 3s - loss: 821.7031 - loglik: -8.2062e+02 - logprior: -1.0860e+00
Epoch 6/10
14/14 - 3s - loss: 822.3301 - loglik: -8.2147e+02 - logprior: -8.6051e-01
Fitted a model with MAP estimate = -820.4871
expansions: [(10, 1), (11, 1), (16, 5), (17, 1), (36, 2), (38, 2), (43, 1), (66, 1), (67, 1), (77, 1), (79, 5), (100, 1), (102, 1), (109, 1), (110, 1), (111, 2), (112, 1), (113, 2)]
discards: [ 0 45 46 47]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 851.4999 - loglik: -8.2360e+02 - logprior: -2.7901e+01
Epoch 2/2
14/14 - 3s - loss: 822.4304 - loglik: -8.1255e+02 - logprior: -9.8815e+00
Fitted a model with MAP estimate = -819.3435
expansions: [(0, 23), (16, 1), (56, 3), (57, 2)]
discards: [ 0 47]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 835.2504 - loglik: -8.1429e+02 - logprior: -2.0956e+01
Epoch 2/2
14/14 - 4s - loss: 810.1670 - loglik: -8.0751e+02 - logprior: -2.6559e+00
Fitted a model with MAP estimate = -807.3898
expansions: [(80, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 82 86
 87 88]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 836.9759 - loglik: -8.1231e+02 - logprior: -2.4667e+01
Epoch 2/10
14/14 - 3s - loss: 811.3718 - loglik: -8.0830e+02 - logprior: -3.0712e+00
Epoch 3/10
14/14 - 3s - loss: 806.9603 - loglik: -8.0752e+02 - logprior: 0.5620
Epoch 4/10
14/14 - 3s - loss: 805.3228 - loglik: -8.0698e+02 - logprior: 1.6586
Epoch 5/10
14/14 - 3s - loss: 803.7840 - loglik: -8.0598e+02 - logprior: 2.2000
Epoch 6/10
14/14 - 3s - loss: 803.2106 - loglik: -8.0579e+02 - logprior: 2.5810
Epoch 7/10
14/14 - 3s - loss: 805.6725 - loglik: -8.0850e+02 - logprior: 2.8296
Fitted a model with MAP estimate = -802.6452
Time for alignment: 72.5693
Computed alignments with likelihoods: ['-803.2983', '-802.7397', '-800.2550', '-804.7015', '-802.6452']
Best model has likelihood: -800.2550  (prior= 2.9904 )
time for generating output: 0.1620
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9705758722152165
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 24s - loss: 1801.5283 - loglik: -1.7988e+03 - logprior: -2.7101e+00
Epoch 2/10
29/29 - 19s - loss: 1621.7812 - loglik: -1.6201e+03 - logprior: -1.6736e+00
Epoch 3/10
29/29 - 19s - loss: 1591.0334 - loglik: -1.5891e+03 - logprior: -1.9484e+00
Epoch 4/10
29/29 - 19s - loss: 1590.1066 - loglik: -1.5882e+03 - logprior: -1.9464e+00
Epoch 5/10
29/29 - 19s - loss: 1580.3376 - loglik: -1.5784e+03 - logprior: -1.9432e+00
Epoch 6/10
29/29 - 19s - loss: 1586.7571 - loglik: -1.5848e+03 - logprior: -1.9654e+00
Fitted a model with MAP estimate = -1582.3145
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (46, 1), (48, 2), (52, 1), (65, 1), (76, 1), (87, 1), (88, 1), (89, 3), (120, 2), (123, 2), (124, 2), (127, 1), (141, 1), (147, 1), (150, 1), (152, 1), (154, 1), (162, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1563.4183 - loglik: -1.5585e+03 - logprior: -4.8770e+00
Epoch 2/2
29/29 - 25s - loss: 1541.6564 - loglik: -1.5398e+03 - logprior: -1.8184e+00
Fitted a model with MAP estimate = -1538.3929
expansions: [(0, 2), (139, 1)]
discards: [  0 259 323 324]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1544.0601 - loglik: -1.5415e+03 - logprior: -2.5854e+00
Epoch 2/2
29/29 - 25s - loss: 1536.1832 - loglik: -1.5362e+03 - logprior: 0.0156
Fitted a model with MAP estimate = -1535.5619
expansions: [(324, 2)]
discards: [  0 106]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 1545.2947 - loglik: -1.5411e+03 - logprior: -4.1882e+00
Epoch 2/10
29/29 - 25s - loss: 1538.8451 - loglik: -1.5384e+03 - logprior: -4.7894e-01
Epoch 3/10
29/29 - 25s - loss: 1535.6305 - loglik: -1.5360e+03 - logprior: 0.3370
Epoch 4/10
29/29 - 25s - loss: 1535.4634 - loglik: -1.5362e+03 - logprior: 0.7169
Epoch 5/10
29/29 - 25s - loss: 1531.8921 - loglik: -1.5326e+03 - logprior: 0.7306
Epoch 6/10
29/29 - 25s - loss: 1532.1930 - loglik: -1.5332e+03 - logprior: 1.0530
Fitted a model with MAP estimate = -1532.9367
Time for alignment: 475.7052
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 1801.5784 - loglik: -1.7989e+03 - logprior: -2.7031e+00
Epoch 2/10
29/29 - 19s - loss: 1622.3591 - loglik: -1.6207e+03 - logprior: -1.6949e+00
Epoch 3/10
29/29 - 19s - loss: 1591.1667 - loglik: -1.5892e+03 - logprior: -1.9978e+00
Epoch 4/10
29/29 - 19s - loss: 1586.1025 - loglik: -1.5841e+03 - logprior: -2.0050e+00
Epoch 5/10
29/29 - 19s - loss: 1582.9166 - loglik: -1.5809e+03 - logprior: -1.9951e+00
Epoch 6/10
29/29 - 19s - loss: 1577.7057 - loglik: -1.5757e+03 - logprior: -1.9784e+00
Epoch 7/10
29/29 - 19s - loss: 1583.5868 - loglik: -1.5816e+03 - logprior: -1.9800e+00
Fitted a model with MAP estimate = -1580.2420
expansions: [(16, 1), (17, 1), (21, 1), (23, 1), (28, 1), (29, 1), (30, 1), (31, 1), (38, 1), (48, 2), (49, 1), (50, 2), (64, 2), (69, 1), (87, 1), (88, 2), (89, 1), (94, 1), (117, 1), (120, 2), (123, 1), (125, 1), (142, 1), (144, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (174, 1), (181, 1), (184, 1), (185, 1), (190, 1), (191, 1), (204, 1), (217, 3), (218, 2), (243, 1), (249, 1), (251, 2), (255, 1), (257, 1), (259, 1), (260, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 1567.1140 - loglik: -1.5622e+03 - logprior: -4.8787e+00
Epoch 2/2
29/29 - 25s - loss: 1541.9268 - loglik: -1.5399e+03 - logprior: -1.9769e+00
Fitted a model with MAP estimate = -1537.9470
expansions: [(0, 2)]
discards: [  0  61  62  78 106 262 299 325 326]
Re-initialized the encoder parameters.
Fitting a model of length 332 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1545.2476 - loglik: -1.5427e+03 - logprior: -2.5054e+00
Epoch 2/2
29/29 - 25s - loss: 1539.7382 - loglik: -1.5399e+03 - logprior: 0.1489
Fitted a model with MAP estimate = -1537.1824
expansions: [(320, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 333 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 1546.1304 - loglik: -1.5420e+03 - logprior: -4.1570e+00
Epoch 2/10
29/29 - 25s - loss: 1540.6693 - loglik: -1.5404e+03 - logprior: -2.4810e-01
Epoch 3/10
29/29 - 25s - loss: 1534.1395 - loglik: -1.5348e+03 - logprior: 0.7104
Epoch 4/10
29/29 - 24s - loss: 1537.4236 - loglik: -1.5380e+03 - logprior: 0.5611
Fitted a model with MAP estimate = -1534.4685
Time for alignment: 442.0684
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 1805.3286 - loglik: -1.8026e+03 - logprior: -2.7106e+00
Epoch 2/10
29/29 - 19s - loss: 1614.5353 - loglik: -1.6127e+03 - logprior: -1.8333e+00
Epoch 3/10
29/29 - 19s - loss: 1590.2781 - loglik: -1.5882e+03 - logprior: -2.1162e+00
Epoch 4/10
29/29 - 19s - loss: 1579.0015 - loglik: -1.5770e+03 - logprior: -2.0290e+00
Epoch 5/10
29/29 - 19s - loss: 1584.0950 - loglik: -1.5821e+03 - logprior: -1.9918e+00
Fitted a model with MAP estimate = -1582.1944
expansions: [(16, 1), (17, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (36, 2), (41, 1), (48, 2), (49, 1), (50, 2), (76, 1), (87, 1), (88, 2), (89, 3), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (128, 1), (141, 2), (144, 1), (151, 1), (154, 1), (155, 1), (163, 1), (172, 1), (181, 1), (184, 1), (185, 1), (186, 2), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (263, 1), (264, 2), (270, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 345 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 1573.6449 - loglik: -1.5687e+03 - logprior: -4.9680e+00
Epoch 2/2
29/29 - 26s - loss: 1535.7603 - loglik: -1.5336e+03 - logprior: -2.1164e+00
Fitted a model with MAP estimate = -1536.0175
expansions: [(0, 2)]
discards: [  0  37  63  64 106 107 171 265 304 323 331 332]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 1545.1141 - loglik: -1.5424e+03 - logprior: -2.6850e+00
Epoch 2/2
29/29 - 25s - loss: 1532.1104 - loglik: -1.5321e+03 - logprior: -2.2786e-02
Fitted a model with MAP estimate = -1532.4770
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 27s - loss: 1539.5708 - loglik: -1.5356e+03 - logprior: -3.9880e+00
Epoch 2/10
29/29 - 25s - loss: 1535.0067 - loglik: -1.5348e+03 - logprior: -2.3502e-01
Epoch 3/10
29/29 - 25s - loss: 1534.8125 - loglik: -1.5355e+03 - logprior: 0.7184
Epoch 4/10
29/29 - 25s - loss: 1529.6058 - loglik: -1.5305e+03 - logprior: 0.9036
Epoch 5/10
29/29 - 25s - loss: 1533.3334 - loglik: -1.5345e+03 - logprior: 1.1320
Fitted a model with MAP estimate = -1529.5299
Time for alignment: 434.5460
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 1804.4648 - loglik: -1.8018e+03 - logprior: -2.6841e+00
Epoch 2/10
29/29 - 19s - loss: 1614.7112 - loglik: -1.6131e+03 - logprior: -1.5698e+00
Epoch 3/10
29/29 - 19s - loss: 1588.5195 - loglik: -1.5867e+03 - logprior: -1.8330e+00
Epoch 4/10
29/29 - 19s - loss: 1579.1564 - loglik: -1.5773e+03 - logprior: -1.8081e+00
Epoch 5/10
29/29 - 19s - loss: 1585.8492 - loglik: -1.5841e+03 - logprior: -1.7703e+00
Fitted a model with MAP estimate = -1580.4267
expansions: [(16, 1), (22, 2), (24, 1), (25, 1), (28, 2), (29, 1), (30, 1), (31, 2), (35, 1), (38, 1), (48, 2), (63, 1), (76, 1), (89, 1), (90, 3), (121, 2), (123, 1), (124, 2), (125, 1), (142, 1), (151, 1), (153, 1), (154, 1), (155, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (191, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (263, 1), (264, 2), (270, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1567.6460 - loglik: -1.5626e+03 - logprior: -5.0101e+00
Epoch 2/2
29/29 - 25s - loss: 1546.7711 - loglik: -1.5446e+03 - logprior: -2.2100e+00
Fitted a model with MAP estimate = -1539.7417
expansions: [(0, 2), (109, 1)]
discards: [  0  23  32 147 260 318 326 327]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1544.9956 - loglik: -1.5423e+03 - logprior: -2.7032e+00
Epoch 2/2
29/29 - 25s - loss: 1536.9966 - loglik: -1.5369e+03 - logprior: -9.5846e-02
Fitted a model with MAP estimate = -1536.3957
expansions: [(114, 2)]
discards: [  0 297]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 1548.7289 - loglik: -1.5446e+03 - logprior: -4.1350e+00
Epoch 2/10
29/29 - 25s - loss: 1533.7205 - loglik: -1.5334e+03 - logprior: -3.3682e-01
Epoch 3/10
29/29 - 25s - loss: 1535.5172 - loglik: -1.5361e+03 - logprior: 0.5635
Fitted a model with MAP estimate = -1534.6081
Time for alignment: 381.6070
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 1806.9312 - loglik: -1.8042e+03 - logprior: -2.6998e+00
Epoch 2/10
29/29 - 19s - loss: 1618.6765 - loglik: -1.6169e+03 - logprior: -1.7723e+00
Epoch 3/10
29/29 - 19s - loss: 1594.4792 - loglik: -1.5925e+03 - logprior: -2.0163e+00
Epoch 4/10
29/29 - 19s - loss: 1588.3932 - loglik: -1.5864e+03 - logprior: -1.9686e+00
Epoch 5/10
29/29 - 19s - loss: 1589.0383 - loglik: -1.5871e+03 - logprior: -1.9571e+00
Fitted a model with MAP estimate = -1585.4510
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (37, 1), (39, 1), (42, 1), (48, 2), (49, 1), (65, 1), (76, 1), (87, 1), (88, 1), (89, 1), (119, 2), (120, 2), (121, 2), (123, 1), (124, 1), (128, 1), (142, 2), (144, 1), (151, 1), (155, 1), (156, 1), (163, 1), (172, 1), (183, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (216, 1), (218, 2), (219, 2), (240, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 1571.4623 - loglik: -1.5664e+03 - logprior: -5.0216e+00
Epoch 2/2
29/29 - 25s - loss: 1546.1263 - loglik: -1.5440e+03 - logprior: -2.1744e+00
Fitted a model with MAP estimate = -1541.6922
expansions: [(0, 2), (26, 1), (82, 1)]
discards: [  0  57 135 141 168 325 326]
Re-initialized the encoder parameters.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1547.4592 - loglik: -1.5447e+03 - logprior: -2.7524e+00
Epoch 2/2
29/29 - 25s - loss: 1541.0419 - loglik: -1.5410e+03 - logprior: -4.6588e-02
Fitted a model with MAP estimate = -1539.0554
expansions: [(324, 2)]
discards: [  0 296]
Re-initialized the encoder parameters.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 1551.5125 - loglik: -1.5473e+03 - logprior: -4.2260e+00
Epoch 2/10
29/29 - 25s - loss: 1537.4385 - loglik: -1.5370e+03 - logprior: -4.0875e-01
Epoch 3/10
29/29 - 25s - loss: 1539.0210 - loglik: -1.5397e+03 - logprior: 0.6756
Fitted a model with MAP estimate = -1537.2043
Time for alignment: 383.4221
Computed alignments with likelihoods: ['-1532.9367', '-1534.4685', '-1529.5299', '-1534.6081', '-1537.2043']
Best model has likelihood: -1529.5299  (prior= 1.2073 )
time for generating output: 0.3388
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8665701881331404
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 754.8254 - loglik: -2.6607e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 379.4039 - loglik: -2.4443e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 286.6880 - loglik: -2.2441e+02 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 245.9637 - loglik: -2.1120e+02 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 227.4719 - loglik: -2.0739e+02 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 217.5661 - loglik: -2.0717e+02 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 211.7416 - loglik: -2.0766e+02 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 208.1900 - loglik: -2.0792e+02 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 205.8284 - loglik: -2.0806e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 204.1405 - loglik: -2.0823e+02 - logprior: 4.0846
Fitted a model with MAP estimate = -203.3814
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 852.0217 - loglik: -2.0001e+02 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 395.8028 - loglik: -1.9249e+02 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -309.2649
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 643.1503 - loglik: -1.8778e+02 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 308.5782 - loglik: -1.8747e+02 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -258.6486
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 614.5032 - loglik: -1.8679e+02 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 301.0665 - loglik: -1.8741e+02 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 235.1161 - loglik: -1.8811e+02 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 207.1017 - loglik: -1.8865e+02 - logprior: -1.8449e+01
Epoch 5/10
10/10 - 0s - loss: 191.6830 - loglik: -1.8915e+02 - logprior: -2.5314e+00
Epoch 6/10
10/10 - 0s - loss: 182.6540 - loglik: -1.8956e+02 - logprior: 6.9047
Epoch 7/10
10/10 - 0s - loss: 177.0106 - loglik: -1.8986e+02 - logprior: 12.8446
Epoch 8/10
10/10 - 0s - loss: 173.1588 - loglik: -1.9008e+02 - logprior: 16.9235
Epoch 9/10
10/10 - 0s - loss: 170.2864 - loglik: -1.9026e+02 - logprior: 19.9769
Epoch 10/10
10/10 - 0s - loss: 167.9802 - loglik: -1.9041e+02 - logprior: 22.4305
Fitted a model with MAP estimate = -166.8586
Time for alignment: 26.3220
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 754.8254 - loglik: -2.6607e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 379.4039 - loglik: -2.4443e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 286.6880 - loglik: -2.2441e+02 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 245.9637 - loglik: -2.1120e+02 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 227.4719 - loglik: -2.0739e+02 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 217.5661 - loglik: -2.0717e+02 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 211.7416 - loglik: -2.0766e+02 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 208.1900 - loglik: -2.0792e+02 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 205.8284 - loglik: -2.0806e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 204.1405 - loglik: -2.0823e+02 - logprior: 4.0846
Fitted a model with MAP estimate = -203.3814
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 852.0217 - loglik: -2.0001e+02 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 395.8028 - loglik: -1.9249e+02 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -309.2649
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 643.1503 - loglik: -1.8778e+02 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 308.5782 - loglik: -1.8747e+02 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -258.6485
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.5032 - loglik: -1.8679e+02 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 301.0666 - loglik: -1.8741e+02 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 235.1162 - loglik: -1.8811e+02 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 207.1018 - loglik: -1.8865e+02 - logprior: -1.8449e+01
Epoch 5/10
10/10 - 0s - loss: 191.6832 - loglik: -1.8915e+02 - logprior: -2.5317e+00
Epoch 6/10
10/10 - 0s - loss: 182.6542 - loglik: -1.8956e+02 - logprior: 6.9043
Epoch 7/10
10/10 - 0s - loss: 177.0108 - loglik: -1.8986e+02 - logprior: 12.8443
Epoch 8/10
10/10 - 0s - loss: 173.1590 - loglik: -1.9008e+02 - logprior: 16.9231
Epoch 9/10
10/10 - 0s - loss: 170.2866 - loglik: -1.9026e+02 - logprior: 19.9765
Epoch 10/10
10/10 - 0s - loss: 167.9804 - loglik: -1.9041e+02 - logprior: 22.4301
Fitted a model with MAP estimate = -166.8588
Time for alignment: 23.6074
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 754.8254 - loglik: -2.6607e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 379.4039 - loglik: -2.4443e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 286.6880 - loglik: -2.2441e+02 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 245.9637 - loglik: -2.1120e+02 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 227.4718 - loglik: -2.0739e+02 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 217.5661 - loglik: -2.0717e+02 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 211.7416 - loglik: -2.0766e+02 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 208.1900 - loglik: -2.0792e+02 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 205.8284 - loglik: -2.0806e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 204.1405 - loglik: -2.0823e+02 - logprior: 4.0846
Fitted a model with MAP estimate = -203.3813
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 852.0217 - loglik: -2.0001e+02 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 395.8028 - loglik: -1.9249e+02 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -309.2648
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 643.1503 - loglik: -1.8778e+02 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 308.5782 - loglik: -1.8747e+02 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -258.6486
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.5031 - loglik: -1.8679e+02 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 301.0665 - loglik: -1.8741e+02 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 235.1160 - loglik: -1.8811e+02 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 207.1017 - loglik: -1.8865e+02 - logprior: -1.8449e+01
Epoch 5/10
10/10 - 0s - loss: 191.6830 - loglik: -1.8915e+02 - logprior: -2.5314e+00
Epoch 6/10
10/10 - 0s - loss: 182.6540 - loglik: -1.8956e+02 - logprior: 6.9047
Epoch 7/10
10/10 - 0s - loss: 177.0106 - loglik: -1.8986e+02 - logprior: 12.8447
Epoch 8/10
10/10 - 0s - loss: 173.1587 - loglik: -1.9008e+02 - logprior: 16.9235
Epoch 9/10
10/10 - 0s - loss: 170.2863 - loglik: -1.9026e+02 - logprior: 19.9770
Epoch 10/10
10/10 - 0s - loss: 167.9801 - loglik: -1.9041e+02 - logprior: 22.4306
Fitted a model with MAP estimate = -166.8585
Time for alignment: 23.3954
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 754.8254 - loglik: -2.6607e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 379.4039 - loglik: -2.4443e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 286.6880 - loglik: -2.2441e+02 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 245.9637 - loglik: -2.1120e+02 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 227.4718 - loglik: -2.0739e+02 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 217.5661 - loglik: -2.0717e+02 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 211.7416 - loglik: -2.0766e+02 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 208.1900 - loglik: -2.0792e+02 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 205.8284 - loglik: -2.0806e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 204.1405 - loglik: -2.0823e+02 - logprior: 4.0846
Fitted a model with MAP estimate = -203.3814
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 852.0217 - loglik: -2.0001e+02 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 395.8028 - loglik: -1.9249e+02 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -309.2648
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 643.1503 - loglik: -1.8778e+02 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 308.5782 - loglik: -1.8747e+02 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -258.6486
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 614.5032 - loglik: -1.8679e+02 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 301.0667 - loglik: -1.8741e+02 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 235.1164 - loglik: -1.8811e+02 - logprior: -4.7004e+01
Epoch 4/10
10/10 - 0s - loss: 207.1021 - loglik: -1.8865e+02 - logprior: -1.8450e+01
Epoch 5/10
10/10 - 0s - loss: 191.6835 - loglik: -1.8915e+02 - logprior: -2.5324e+00
Epoch 6/10
10/10 - 0s - loss: 182.6546 - loglik: -1.8956e+02 - logprior: 6.9035
Epoch 7/10
10/10 - 0s - loss: 177.0113 - loglik: -1.8985e+02 - logprior: 12.8434
Epoch 8/10
10/10 - 0s - loss: 173.1595 - loglik: -1.9008e+02 - logprior: 16.9222
Epoch 9/10
10/10 - 0s - loss: 170.2872 - loglik: -1.9026e+02 - logprior: 19.9756
Epoch 10/10
10/10 - 0s - loss: 167.9810 - loglik: -1.9041e+02 - logprior: 22.4292
Fitted a model with MAP estimate = -166.8595
Time for alignment: 23.4125
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 754.8254 - loglik: -2.6607e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 379.4039 - loglik: -2.4443e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 286.6880 - loglik: -2.2441e+02 - logprior: -6.2274e+01
Epoch 4/10
10/10 - 0s - loss: 245.9637 - loglik: -2.1120e+02 - logprior: -3.4759e+01
Epoch 5/10
10/10 - 0s - loss: 227.4718 - loglik: -2.0739e+02 - logprior: -2.0080e+01
Epoch 6/10
10/10 - 0s - loss: 217.5661 - loglik: -2.0717e+02 - logprior: -1.0399e+01
Epoch 7/10
10/10 - 0s - loss: 211.7416 - loglik: -2.0766e+02 - logprior: -4.0831e+00
Epoch 8/10
10/10 - 0s - loss: 208.1900 - loglik: -2.0792e+02 - logprior: -2.7165e-01
Epoch 9/10
10/10 - 0s - loss: 205.8284 - loglik: -2.0806e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 0s - loss: 204.1405 - loglik: -2.0823e+02 - logprior: 4.0846
Fitted a model with MAP estimate = -203.3813
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 852.0217 - loglik: -2.0001e+02 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 395.8028 - loglik: -1.9249e+02 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -309.2648
expansions: []
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 643.1503 - loglik: -1.8778e+02 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 308.5782 - loglik: -1.8747e+02 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -258.6485
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 614.5031 - loglik: -1.8679e+02 - logprior: -4.2771e+02
Epoch 2/10
10/10 - 0s - loss: 301.0665 - loglik: -1.8741e+02 - logprior: -1.1366e+02
Epoch 3/10
10/10 - 0s - loss: 235.1161 - loglik: -1.8811e+02 - logprior: -4.7003e+01
Epoch 4/10
10/10 - 0s - loss: 207.1017 - loglik: -1.8865e+02 - logprior: -1.8449e+01
Epoch 5/10
10/10 - 0s - loss: 191.6830 - loglik: -1.8915e+02 - logprior: -2.5316e+00
Epoch 6/10
10/10 - 0s - loss: 182.6541 - loglik: -1.8956e+02 - logprior: 6.9045
Epoch 7/10
10/10 - 0s - loss: 177.0107 - loglik: -1.8986e+02 - logprior: 12.8444
Epoch 8/10
10/10 - 0s - loss: 173.1588 - loglik: -1.9008e+02 - logprior: 16.9233
Epoch 9/10
10/10 - 0s - loss: 170.2864 - loglik: -1.9026e+02 - logprior: 19.9767
Epoch 10/10
10/10 - 0s - loss: 167.9802 - loglik: -1.9041e+02 - logprior: 22.4304
Fitted a model with MAP estimate = -166.8586
Time for alignment: 24.4758
Computed alignments with likelihoods: ['-166.8586', '-166.8588', '-166.8585', '-166.8595', '-166.8586']
Best model has likelihood: -166.8585  (prior= 23.6246 )
time for generating output: 0.0847
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 958.0944 - loglik: -9.5390e+02 - logprior: -4.1934e+00
Epoch 2/10
16/16 - 4s - loss: 921.0317 - loglik: -9.2003e+02 - logprior: -1.0037e+00
Epoch 3/10
16/16 - 4s - loss: 898.2958 - loglik: -8.9705e+02 - logprior: -1.2434e+00
Epoch 4/10
16/16 - 4s - loss: 890.3793 - loglik: -8.8912e+02 - logprior: -1.2599e+00
Epoch 5/10
16/16 - 4s - loss: 883.5471 - loglik: -8.8232e+02 - logprior: -1.2226e+00
Epoch 6/10
16/16 - 4s - loss: 884.2694 - loglik: -8.8300e+02 - logprior: -1.2677e+00
Fitted a model with MAP estimate = -882.2272
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 3), (42, 1), (43, 1), (48, 2), (50, 1), (54, 1), (56, 2), (57, 1), (69, 1), (73, 2), (74, 3), (94, 2), (95, 4), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 886.8441 - loglik: -8.8309e+02 - logprior: -3.7515e+00
Epoch 2/2
33/33 - 6s - loss: 877.4752 - loglik: -8.7650e+02 - logprior: -9.7611e-01
Fitted a model with MAP estimate = -874.6765
expansions: [(35, 1), (93, 1), (177, 2)]
discards: [  0  32  33  69  72 117 118 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 884.9799 - loglik: -8.8139e+02 - logprior: -3.5925e+00
Epoch 2/2
33/33 - 6s - loss: 878.3707 - loglik: -8.7760e+02 - logprior: -7.6812e-01
Fitted a model with MAP estimate = -876.9456
expansions: [(0, 1), (31, 1), (171, 2)]
discards: [169 170]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 880.3613 - loglik: -8.7793e+02 - logprior: -2.4310e+00
Epoch 2/10
33/33 - 6s - loss: 876.6697 - loglik: -8.7619e+02 - logprior: -4.8084e-01
Epoch 3/10
33/33 - 6s - loss: 872.3150 - loglik: -8.7200e+02 - logprior: -3.1327e-01
Epoch 4/10
33/33 - 6s - loss: 874.1670 - loglik: -8.7391e+02 - logprior: -2.5209e-01
Fitted a model with MAP estimate = -870.7159
Time for alignment: 117.7309
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 957.5142 - loglik: -9.5333e+02 - logprior: -4.1818e+00
Epoch 2/10
16/16 - 4s - loss: 921.4615 - loglik: -9.2045e+02 - logprior: -1.0124e+00
Epoch 3/10
16/16 - 4s - loss: 897.3276 - loglik: -8.9603e+02 - logprior: -1.2936e+00
Epoch 4/10
16/16 - 4s - loss: 892.0809 - loglik: -8.9080e+02 - logprior: -1.2776e+00
Epoch 5/10
16/16 - 4s - loss: 884.3734 - loglik: -8.8313e+02 - logprior: -1.2465e+00
Epoch 6/10
16/16 - 4s - loss: 884.5867 - loglik: -8.8329e+02 - logprior: -1.2919e+00
Fitted a model with MAP estimate = -882.9910
expansions: [(15, 4), (17, 1), (19, 1), (28, 2), (30, 1), (41, 1), (45, 1), (49, 2), (50, 2), (51, 1), (57, 1), (58, 1), (70, 1), (73, 3), (74, 2), (91, 1), (95, 4), (99, 1), (114, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 887.2240 - loglik: -8.8346e+02 - logprior: -3.7612e+00
Epoch 2/2
33/33 - 6s - loss: 876.3051 - loglik: -8.7533e+02 - logprior: -9.7823e-01
Fitted a model with MAP estimate = -874.0890
expansions: [(178, 2)]
discards: [  0  16  17  34  35  63  75  98 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 886.0605 - loglik: -8.8248e+02 - logprior: -3.5756e+00
Epoch 2/2
33/33 - 6s - loss: 879.5975 - loglik: -8.7885e+02 - logprior: -7.4721e-01
Fitted a model with MAP estimate = -877.2595
expansions: [(0, 1), (169, 2)]
discards: [167 168]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 880.8661 - loglik: -8.7842e+02 - logprior: -2.4488e+00
Epoch 2/10
33/33 - 6s - loss: 876.9928 - loglik: -8.7651e+02 - logprior: -4.8337e-01
Epoch 3/10
33/33 - 6s - loss: 875.1865 - loglik: -8.7487e+02 - logprior: -3.1746e-01
Epoch 4/10
33/33 - 6s - loss: 873.4323 - loglik: -8.7317e+02 - logprior: -2.6490e-01
Epoch 5/10
33/33 - 6s - loss: 872.3083 - loglik: -8.7209e+02 - logprior: -2.1693e-01
Epoch 6/10
33/33 - 6s - loss: 866.0027 - loglik: -8.6582e+02 - logprior: -1.8164e-01
Epoch 7/10
33/33 - 6s - loss: 872.6220 - loglik: -8.7249e+02 - logprior: -1.3078e-01
Fitted a model with MAP estimate = -869.4082
Time for alignment: 135.4898
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 956.5932 - loglik: -9.5240e+02 - logprior: -4.1900e+00
Epoch 2/10
16/16 - 4s - loss: 921.3898 - loglik: -9.2037e+02 - logprior: -1.0217e+00
Epoch 3/10
16/16 - 4s - loss: 896.4161 - loglik: -8.9513e+02 - logprior: -1.2906e+00
Epoch 4/10
16/16 - 4s - loss: 886.4237 - loglik: -8.8512e+02 - logprior: -1.3074e+00
Epoch 5/10
16/16 - 4s - loss: 885.6569 - loglik: -8.8440e+02 - logprior: -1.2526e+00
Epoch 6/10
16/16 - 4s - loss: 883.0025 - loglik: -8.8171e+02 - logprior: -1.2888e+00
Epoch 7/10
16/16 - 4s - loss: 881.7177 - loglik: -8.8044e+02 - logprior: -1.2811e+00
Epoch 8/10
16/16 - 4s - loss: 883.0237 - loglik: -8.8173e+02 - logprior: -1.2937e+00
Fitted a model with MAP estimate = -881.1242
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 3), (29, 1), (42, 1), (44, 1), (47, 1), (48, 1), (49, 2), (51, 1), (52, 1), (56, 1), (57, 2), (71, 1), (72, 2), (73, 2), (74, 2), (94, 3), (95, 1), (104, 1), (115, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 888.5151 - loglik: -8.8473e+02 - logprior: -3.7812e+00
Epoch 2/2
33/33 - 6s - loss: 877.1645 - loglik: -8.7611e+02 - logprior: -1.0542e+00
Fitted a model with MAP estimate = -875.0833
expansions: [(177, 2)]
discards: [  0  32  33  61  64  74 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 884.7269 - loglik: -8.8118e+02 - logprior: -3.5497e+00
Epoch 2/2
33/33 - 6s - loss: 880.9673 - loglik: -8.8024e+02 - logprior: -7.2762e-01
Fitted a model with MAP estimate = -876.4986
expansions: [(0, 1), (33, 1), (113, 2), (170, 2)]
discards: [ 93 168 169]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 880.1890 - loglik: -8.7779e+02 - logprior: -2.3944e+00
Epoch 2/10
33/33 - 6s - loss: 876.2272 - loglik: -8.7579e+02 - logprior: -4.4072e-01
Epoch 3/10
33/33 - 6s - loss: 874.1042 - loglik: -8.7384e+02 - logprior: -2.6775e-01
Epoch 4/10
33/33 - 6s - loss: 871.3639 - loglik: -8.7114e+02 - logprior: -2.2169e-01
Epoch 5/10
33/33 - 6s - loss: 871.5649 - loglik: -8.7140e+02 - logprior: -1.6942e-01
Fitted a model with MAP estimate = -869.0870
Time for alignment: 131.7013
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 957.4033 - loglik: -9.5324e+02 - logprior: -4.1666e+00
Epoch 2/10
16/16 - 4s - loss: 919.7127 - loglik: -9.1872e+02 - logprior: -9.9014e-01
Epoch 3/10
16/16 - 4s - loss: 897.9087 - loglik: -8.9666e+02 - logprior: -1.2463e+00
Epoch 4/10
16/16 - 4s - loss: 887.8110 - loglik: -8.8655e+02 - logprior: -1.2574e+00
Epoch 5/10
16/16 - 4s - loss: 886.3126 - loglik: -8.8511e+02 - logprior: -1.2036e+00
Epoch 6/10
16/16 - 4s - loss: 884.8737 - loglik: -8.8362e+02 - logprior: -1.2528e+00
Epoch 7/10
16/16 - 4s - loss: 880.5942 - loglik: -8.7932e+02 - logprior: -1.2791e+00
Epoch 8/10
16/16 - 4s - loss: 881.3077 - loglik: -8.8001e+02 - logprior: -1.2941e+00
Fitted a model with MAP estimate = -881.1544
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 2), (30, 1), (41, 1), (45, 1), (48, 3), (50, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (74, 3), (93, 1), (94, 5), (95, 1), (102, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 887.7186 - loglik: -8.8393e+02 - logprior: -3.7914e+00
Epoch 2/2
33/33 - 7s - loss: 878.1616 - loglik: -8.7714e+02 - logprior: -1.0226e+00
Fitted a model with MAP estimate = -875.0093
expansions: [(94, 1), (178, 2)]
discards: [  0  31  32  33  72 147 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 883.9541 - loglik: -8.8038e+02 - logprior: -3.5754e+00
Epoch 2/2
33/33 - 6s - loss: 881.4867 - loglik: -8.8071e+02 - logprior: -7.7693e-01
Fitted a model with MAP estimate = -877.3122
expansions: [(0, 1), (30, 2), (172, 2)]
discards: [ 86 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 881.7629 - loglik: -8.7937e+02 - logprior: -2.3895e+00
Epoch 2/10
33/33 - 6s - loss: 877.0311 - loglik: -8.7657e+02 - logprior: -4.6469e-01
Epoch 3/10
33/33 - 6s - loss: 874.3408 - loglik: -8.7404e+02 - logprior: -3.0323e-01
Epoch 4/10
33/33 - 6s - loss: 872.4135 - loglik: -8.7217e+02 - logprior: -2.4380e-01
Epoch 5/10
33/33 - 6s - loss: 871.1877 - loglik: -8.7099e+02 - logprior: -2.0141e-01
Epoch 6/10
33/33 - 6s - loss: 870.1709 - loglik: -8.7002e+02 - logprior: -1.5523e-01
Epoch 7/10
33/33 - 6s - loss: 869.7834 - loglik: -8.6967e+02 - logprior: -1.0901e-01
Epoch 8/10
33/33 - 6s - loss: 868.9489 - loglik: -8.6888e+02 - logprior: -6.4950e-02
Epoch 9/10
33/33 - 6s - loss: 869.3870 - loglik: -8.6938e+02 - logprior: -4.5453e-03
Fitted a model with MAP estimate = -868.6385
Time for alignment: 159.1054
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 957.3210 - loglik: -9.5314e+02 - logprior: -4.1771e+00
Epoch 2/10
16/16 - 4s - loss: 922.0072 - loglik: -9.2103e+02 - logprior: -9.7397e-01
Epoch 3/10
16/16 - 4s - loss: 897.1631 - loglik: -8.9595e+02 - logprior: -1.2144e+00
Epoch 4/10
16/16 - 4s - loss: 887.0103 - loglik: -8.8570e+02 - logprior: -1.3131e+00
Epoch 5/10
16/16 - 4s - loss: 885.5623 - loglik: -8.8436e+02 - logprior: -1.2026e+00
Epoch 6/10
16/16 - 4s - loss: 882.0103 - loglik: -8.8075e+02 - logprior: -1.2608e+00
Epoch 7/10
16/16 - 4s - loss: 882.7948 - loglik: -8.8152e+02 - logprior: -1.2719e+00
Fitted a model with MAP estimate = -881.4406
expansions: [(13, 1), (14, 1), (17, 1), (23, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (51, 1), (53, 1), (56, 1), (57, 1), (69, 1), (73, 2), (74, 3), (94, 4), (95, 1), (96, 2), (99, 1), (114, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 886.4301 - loglik: -8.8270e+02 - logprior: -3.7293e+00
Epoch 2/2
33/33 - 6s - loss: 876.7823 - loglik: -8.7583e+02 - logprior: -9.4844e-01
Fitted a model with MAP estimate = -874.3504
expansions: [(94, 1), (178, 2)]
discards: [  0  32  33  72 120 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 884.1002 - loglik: -8.8051e+02 - logprior: -3.5896e+00
Epoch 2/2
33/33 - 6s - loss: 879.1326 - loglik: -8.7831e+02 - logprior: -8.1791e-01
Fitted a model with MAP estimate = -876.1938
expansions: [(0, 1), (33, 1), (173, 2)]
discards: [ 87 115 116 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 881.4593 - loglik: -8.7905e+02 - logprior: -2.4126e+00
Epoch 2/10
33/33 - 6s - loss: 876.6263 - loglik: -8.7616e+02 - logprior: -4.6148e-01
Epoch 3/10
33/33 - 6s - loss: 873.9239 - loglik: -8.7364e+02 - logprior: -2.8840e-01
Epoch 4/10
33/33 - 6s - loss: 871.7811 - loglik: -8.7154e+02 - logprior: -2.3684e-01
Epoch 5/10
33/33 - 6s - loss: 871.5770 - loglik: -8.7139e+02 - logprior: -1.8849e-01
Epoch 6/10
33/33 - 6s - loss: 870.0354 - loglik: -8.6988e+02 - logprior: -1.5610e-01
Epoch 7/10
33/33 - 6s - loss: 869.2330 - loglik: -8.6913e+02 - logprior: -1.0080e-01
Epoch 8/10
33/33 - 6s - loss: 869.1043 - loglik: -8.6906e+02 - logprior: -4.7474e-02
Epoch 9/10
33/33 - 6s - loss: 867.7808 - loglik: -8.6779e+02 - logprior: 0.0084
Epoch 10/10
33/33 - 6s - loss: 868.4913 - loglik: -8.6855e+02 - logprior: 0.0575
Fitted a model with MAP estimate = -868.3201
Time for alignment: 158.3511
Computed alignments with likelihoods: ['-870.7159', '-869.4082', '-869.0870', '-868.6385', '-868.3201']
Best model has likelihood: -868.3201  (prior= 0.0532 )
time for generating output: 0.1970
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7811804008908686
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 571.8698 - loglik: -5.5694e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 532.9275 - loglik: -5.2904e+02 - logprior: -3.8905e+00
Epoch 3/10
10/10 - 1s - loss: 508.8630 - loglik: -5.0666e+02 - logprior: -2.2003e+00
Epoch 4/10
10/10 - 1s - loss: 492.0863 - loglik: -4.9023e+02 - logprior: -1.8531e+00
Epoch 5/10
10/10 - 1s - loss: 486.4875 - loglik: -4.8492e+02 - logprior: -1.5660e+00
Epoch 6/10
10/10 - 1s - loss: 486.3258 - loglik: -4.8486e+02 - logprior: -1.4702e+00
Epoch 7/10
10/10 - 1s - loss: 483.3956 - loglik: -4.8205e+02 - logprior: -1.3448e+00
Epoch 8/10
10/10 - 1s - loss: 483.7505 - loglik: -4.8244e+02 - logprior: -1.3116e+00
Fitted a model with MAP estimate = -482.7015
expansions: [(10, 2), (20, 1), (21, 1), (26, 1), (33, 1), (39, 1), (44, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 506.2579 - loglik: -4.8903e+02 - logprior: -1.7227e+01
Epoch 2/2
10/10 - 1s - loss: 486.9278 - loglik: -4.7961e+02 - logprior: -7.3150e+00
Fitted a model with MAP estimate = -483.4155
expansions: []
discards: [ 0 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 500.8889 - loglik: -4.8393e+02 - logprior: -1.6959e+01
Epoch 2/2
10/10 - 1s - loss: 481.6011 - loglik: -4.7565e+02 - logprior: -5.9473e+00
Fitted a model with MAP estimate = -480.3873
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 492.4125 - loglik: -4.7794e+02 - logprior: -1.4468e+01
Epoch 2/10
10/10 - 1s - loss: 473.0414 - loglik: -4.6859e+02 - logprior: -4.4536e+00
Epoch 3/10
10/10 - 1s - loss: 465.6324 - loglik: -4.6308e+02 - logprior: -2.5552e+00
Epoch 4/10
10/10 - 1s - loss: 462.6772 - loglik: -4.6082e+02 - logprior: -1.8609e+00
Epoch 5/10
10/10 - 1s - loss: 465.0551 - loglik: -4.6345e+02 - logprior: -1.6090e+00
Fitted a model with MAP estimate = -462.2673
Time for alignment: 39.5801
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 573.3376 - loglik: -5.5841e+02 - logprior: -1.4929e+01
Epoch 2/10
10/10 - 1s - loss: 531.1099 - loglik: -5.2724e+02 - logprior: -3.8651e+00
Epoch 3/10
10/10 - 1s - loss: 508.2528 - loglik: -5.0618e+02 - logprior: -2.0745e+00
Epoch 4/10
10/10 - 1s - loss: 492.0913 - loglik: -4.9055e+02 - logprior: -1.5429e+00
Epoch 5/10
10/10 - 1s - loss: 487.3724 - loglik: -4.8608e+02 - logprior: -1.2928e+00
Epoch 6/10
10/10 - 1s - loss: 485.5948 - loglik: -4.8436e+02 - logprior: -1.2334e+00
Epoch 7/10
10/10 - 1s - loss: 482.2967 - loglik: -4.8116e+02 - logprior: -1.1326e+00
Epoch 8/10
10/10 - 1s - loss: 482.1259 - loglik: -4.8107e+02 - logprior: -1.0584e+00
Epoch 9/10
10/10 - 1s - loss: 481.9646 - loglik: -4.8093e+02 - logprior: -1.0394e+00
Epoch 10/10
10/10 - 1s - loss: 480.6608 - loglik: -4.7961e+02 - logprior: -1.0501e+00
Fitted a model with MAP estimate = -481.1003
expansions: [(36, 3), (47, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 509.3400 - loglik: -4.9209e+02 - logprior: -1.7251e+01
Epoch 2/2
10/10 - 1s - loss: 486.3818 - loglik: -4.7900e+02 - logprior: -7.3771e+00
Fitted a model with MAP estimate = -487.3225
expansions: [(0, 25)]
discards: [ 0 36 37 60]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 499.6235 - loglik: -4.8524e+02 - logprior: -1.4380e+01
Epoch 2/2
10/10 - 1s - loss: 476.7227 - loglik: -4.7226e+02 - logprior: -4.4640e+00
Fitted a model with MAP estimate = -471.4775
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 500.9943 - loglik: -4.8619e+02 - logprior: -1.4806e+01
Epoch 2/10
10/10 - 1s - loss: 484.7978 - loglik: -4.8064e+02 - logprior: -4.1554e+00
Epoch 3/10
10/10 - 1s - loss: 484.4980 - loglik: -4.8249e+02 - logprior: -2.0057e+00
Epoch 4/10
10/10 - 1s - loss: 480.1821 - loglik: -4.7900e+02 - logprior: -1.1858e+00
Epoch 5/10
10/10 - 1s - loss: 480.2128 - loglik: -4.7941e+02 - logprior: -7.9828e-01
Fitted a model with MAP estimate = -480.0926
Time for alignment: 39.6472
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 572.6433 - loglik: -5.5771e+02 - logprior: -1.4933e+01
Epoch 2/10
10/10 - 1s - loss: 534.2922 - loglik: -5.3038e+02 - logprior: -3.9133e+00
Epoch 3/10
10/10 - 1s - loss: 505.9172 - loglik: -5.0365e+02 - logprior: -2.2667e+00
Epoch 4/10
10/10 - 1s - loss: 490.9634 - loglik: -4.8896e+02 - logprior: -1.9993e+00
Epoch 5/10
10/10 - 1s - loss: 485.3940 - loglik: -4.8349e+02 - logprior: -1.9046e+00
Epoch 6/10
10/10 - 1s - loss: 485.4651 - loglik: -4.8350e+02 - logprior: -1.9686e+00
Fitted a model with MAP estimate = -482.9204
expansions: [(23, 1), (26, 1), (27, 1), (34, 1), (36, 2), (37, 1), (46, 2), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 502.9435 - loglik: -4.8571e+02 - logprior: -1.7230e+01
Epoch 2/2
10/10 - 1s - loss: 482.0817 - loglik: -4.7470e+02 - logprior: -7.3845e+00
Fitted a model with MAP estimate = -479.2791
expansions: [(52, 1)]
discards: [ 0 39 40]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 494.5908 - loglik: -4.7776e+02 - logprior: -1.6828e+01
Epoch 2/2
10/10 - 1s - loss: 481.0887 - loglik: -4.7514e+02 - logprior: -5.9492e+00
Fitted a model with MAP estimate = -477.1742
expansions: [(0, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 488.2886 - loglik: -4.7410e+02 - logprior: -1.4184e+01
Epoch 2/10
10/10 - 1s - loss: 473.9551 - loglik: -4.6966e+02 - logprior: -4.2922e+00
Epoch 3/10
10/10 - 1s - loss: 466.4553 - loglik: -4.6412e+02 - logprior: -2.3317e+00
Epoch 4/10
10/10 - 1s - loss: 466.9208 - loglik: -4.6518e+02 - logprior: -1.7418e+00
Fitted a model with MAP estimate = -464.8762
Time for alignment: 33.3751
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 571.7692 - loglik: -5.5684e+02 - logprior: -1.4930e+01
Epoch 2/10
10/10 - 1s - loss: 530.9055 - loglik: -5.2702e+02 - logprior: -3.8839e+00
Epoch 3/10
10/10 - 1s - loss: 510.5657 - loglik: -5.0837e+02 - logprior: -2.2001e+00
Epoch 4/10
10/10 - 1s - loss: 492.0415 - loglik: -4.9024e+02 - logprior: -1.7988e+00
Epoch 5/10
10/10 - 1s - loss: 488.3296 - loglik: -4.8673e+02 - logprior: -1.5974e+00
Epoch 6/10
10/10 - 1s - loss: 487.7366 - loglik: -4.8613e+02 - logprior: -1.6068e+00
Epoch 7/10
10/10 - 1s - loss: 481.2294 - loglik: -4.7965e+02 - logprior: -1.5833e+00
Epoch 8/10
10/10 - 1s - loss: 482.8483 - loglik: -4.8133e+02 - logprior: -1.5157e+00
Fitted a model with MAP estimate = -483.1306
expansions: [(19, 1), (29, 3), (46, 2), (47, 1), (48, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 507.8739 - loglik: -4.9062e+02 - logprior: -1.7252e+01
Epoch 2/2
10/10 - 1s - loss: 489.8002 - loglik: -4.8239e+02 - logprior: -7.4092e+00
Fitted a model with MAP estimate = -485.5469
expansions: [(49, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 499.7386 - loglik: -4.8277e+02 - logprior: -1.6964e+01
Epoch 2/2
10/10 - 1s - loss: 483.8015 - loglik: -4.7770e+02 - logprior: -6.1053e+00
Fitted a model with MAP estimate = -481.2660
expansions: [(0, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 490.1080 - loglik: -4.7593e+02 - logprior: -1.4177e+01
Epoch 2/10
10/10 - 1s - loss: 475.5777 - loglik: -4.7135e+02 - logprior: -4.2258e+00
Epoch 3/10
10/10 - 1s - loss: 473.6083 - loglik: -4.7129e+02 - logprior: -2.3209e+00
Epoch 4/10
10/10 - 1s - loss: 470.5694 - loglik: -4.6879e+02 - logprior: -1.7820e+00
Epoch 5/10
10/10 - 1s - loss: 469.3046 - loglik: -4.6772e+02 - logprior: -1.5839e+00
Epoch 6/10
10/10 - 1s - loss: 468.7821 - loglik: -4.6735e+02 - logprior: -1.4365e+00
Epoch 7/10
10/10 - 1s - loss: 467.1721 - loglik: -4.6588e+02 - logprior: -1.2924e+00
Epoch 8/10
10/10 - 1s - loss: 469.7653 - loglik: -4.6855e+02 - logprior: -1.2175e+00
Fitted a model with MAP estimate = -467.5547
Time for alignment: 41.7299
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 570.5604 - loglik: -5.5563e+02 - logprior: -1.4929e+01
Epoch 2/10
10/10 - 1s - loss: 535.5198 - loglik: -5.3164e+02 - logprior: -3.8756e+00
Epoch 3/10
10/10 - 1s - loss: 505.9819 - loglik: -5.0385e+02 - logprior: -2.1295e+00
Epoch 4/10
10/10 - 1s - loss: 491.7140 - loglik: -4.8992e+02 - logprior: -1.7910e+00
Epoch 5/10
10/10 - 1s - loss: 488.6346 - loglik: -4.8694e+02 - logprior: -1.6988e+00
Epoch 6/10
10/10 - 1s - loss: 484.3465 - loglik: -4.8268e+02 - logprior: -1.6688e+00
Epoch 7/10
10/10 - 1s - loss: 488.2955 - loglik: -4.8666e+02 - logprior: -1.6390e+00
Fitted a model with MAP estimate = -484.9692
expansions: [(27, 3), (37, 1), (44, 1), (45, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 506.8935 - loglik: -4.8966e+02 - logprior: -1.7231e+01
Epoch 2/2
10/10 - 1s - loss: 489.7461 - loglik: -4.8235e+02 - logprior: -7.4008e+00
Fitted a model with MAP estimate = -486.0724
expansions: [(28, 1)]
discards: [ 0 49]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 499.1766 - loglik: -4.8220e+02 - logprior: -1.6977e+01
Epoch 2/2
10/10 - 1s - loss: 484.1403 - loglik: -4.7804e+02 - logprior: -6.1014e+00
Fitted a model with MAP estimate = -481.6212
expansions: [(0, 8), (26, 1), (49, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 492.5359 - loglik: -4.7830e+02 - logprior: -1.4235e+01
Epoch 2/10
10/10 - 1s - loss: 471.3675 - loglik: -4.6713e+02 - logprior: -4.2338e+00
Epoch 3/10
10/10 - 1s - loss: 466.8094 - loglik: -4.6450e+02 - logprior: -2.3129e+00
Epoch 4/10
10/10 - 1s - loss: 464.0706 - loglik: -4.6234e+02 - logprior: -1.7290e+00
Epoch 5/10
10/10 - 1s - loss: 461.6048 - loglik: -4.6005e+02 - logprior: -1.5587e+00
Epoch 6/10
10/10 - 1s - loss: 461.5578 - loglik: -4.6017e+02 - logprior: -1.3864e+00
Epoch 7/10
10/10 - 1s - loss: 462.0032 - loglik: -4.6078e+02 - logprior: -1.2262e+00
Fitted a model with MAP estimate = -460.6375
Time for alignment: 37.8392
Computed alignments with likelihoods: ['-462.2673', '-471.4775', '-464.8762', '-467.5547', '-460.6375']
Best model has likelihood: -460.6375  (prior= -1.1678 )
time for generating output: 0.1774
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.6699340245051838
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1187.5833 - loglik: -1.1348e+03 - logprior: -5.2806e+01
Epoch 2/10
10/10 - 2s - loss: 1090.2007 - loglik: -1.0814e+03 - logprior: -8.7742e+00
Epoch 3/10
10/10 - 2s - loss: 1028.2507 - loglik: -1.0268e+03 - logprior: -1.4652e+00
Epoch 4/10
10/10 - 2s - loss: 993.7678 - loglik: -9.9440e+02 - logprior: 0.6325
Epoch 5/10
10/10 - 2s - loss: 980.8478 - loglik: -9.8239e+02 - logprior: 1.5394
Epoch 6/10
10/10 - 2s - loss: 974.1188 - loglik: -9.7622e+02 - logprior: 2.1062
Epoch 7/10
10/10 - 2s - loss: 973.8900 - loglik: -9.7637e+02 - logprior: 2.4791
Epoch 8/10
10/10 - 2s - loss: 972.2640 - loglik: -9.7499e+02 - logprior: 2.7216
Epoch 9/10
10/10 - 2s - loss: 970.2695 - loglik: -9.7315e+02 - logprior: 2.8758
Epoch 10/10
10/10 - 2s - loss: 970.5770 - loglik: -9.7365e+02 - logprior: 3.0727
Fitted a model with MAP estimate = -970.1884
expansions: [(0, 3), (21, 1), (22, 1), (23, 1), (25, 1), (30, 3), (44, 1), (45, 2), (46, 1), (52, 2), (76, 3), (81, 2), (98, 1), (108, 2), (114, 2), (116, 1), (117, 1), (118, 1), (145, 3), (150, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1050.7780 - loglik: -9.8269e+02 - logprior: -6.8083e+01
Epoch 2/2
10/10 - 2s - loss: 978.5504 - loglik: -9.6307e+02 - logprior: -1.5482e+01
Fitted a model with MAP estimate = -963.0998
expansions: []
discards: [  0   1   2  37  56  66  93 100 174 175 184 185]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 1016.2443 - loglik: -9.6748e+02 - logprior: -4.8764e+01
Epoch 2/2
10/10 - 2s - loss: 968.6860 - loglik: -9.6108e+02 - logprior: -7.6104e+00
Fitted a model with MAP estimate = -960.3413
expansions: [(0, 11), (166, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1011.9614 - loglik: -9.6520e+02 - logprior: -4.6757e+01
Epoch 2/10
10/10 - 2s - loss: 964.7979 - loglik: -9.5793e+02 - logprior: -6.8663e+00
Epoch 3/10
10/10 - 2s - loss: 953.6254 - loglik: -9.5513e+02 - logprior: 1.5082
Epoch 4/10
10/10 - 3s - loss: 944.9313 - loglik: -9.5003e+02 - logprior: 5.0979
Epoch 5/10
10/10 - 2s - loss: 942.7148 - loglik: -9.4975e+02 - logprior: 7.0386
Epoch 6/10
10/10 - 3s - loss: 939.6653 - loglik: -9.4782e+02 - logprior: 8.1521
Epoch 7/10
10/10 - 3s - loss: 935.5627 - loglik: -9.4445e+02 - logprior: 8.8898
Epoch 8/10
10/10 - 3s - loss: 936.6587 - loglik: -9.4616e+02 - logprior: 9.4999
Fitted a model with MAP estimate = -935.4957
Time for alignment: 70.3682
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1187.3192 - loglik: -1.1345e+03 - logprior: -5.2810e+01
Epoch 2/10
10/10 - 2s - loss: 1090.8909 - loglik: -1.0821e+03 - logprior: -8.7802e+00
Epoch 3/10
10/10 - 2s - loss: 1027.1525 - loglik: -1.0256e+03 - logprior: -1.5447e+00
Epoch 4/10
10/10 - 2s - loss: 995.0389 - loglik: -9.9573e+02 - logprior: 0.6910
Epoch 5/10
10/10 - 2s - loss: 981.3928 - loglik: -9.8313e+02 - logprior: 1.7366
Epoch 6/10
10/10 - 2s - loss: 976.4245 - loglik: -9.7876e+02 - logprior: 2.3333
Epoch 7/10
10/10 - 2s - loss: 975.2346 - loglik: -9.7786e+02 - logprior: 2.6263
Epoch 8/10
10/10 - 2s - loss: 973.5753 - loglik: -9.7640e+02 - logprior: 2.8271
Epoch 9/10
10/10 - 2s - loss: 971.6185 - loglik: -9.7457e+02 - logprior: 2.9549
Epoch 10/10
10/10 - 2s - loss: 973.7129 - loglik: -9.7687e+02 - logprior: 3.1553
Fitted a model with MAP estimate = -972.1337
expansions: [(7, 1), (8, 2), (22, 1), (23, 3), (25, 1), (29, 2), (43, 1), (44, 2), (46, 1), (52, 1), (74, 1), (75, 2), (81, 2), (94, 1), (104, 1), (105, 2), (106, 1), (116, 3), (144, 3), (151, 3), (152, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1045.4933 - loglik: -9.8530e+02 - logprior: -6.0195e+01
Epoch 2/2
10/10 - 2s - loss: 986.3552 - loglik: -9.6532e+02 - logprior: -2.1031e+01
Fitted a model with MAP estimate = -976.1136
expansions: [(0, 12), (129, 1)]
discards: [  0  56  98 185]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1016.1001 - loglik: -9.6907e+02 - logprior: -4.7032e+01
Epoch 2/2
10/10 - 3s - loss: 964.3536 - loglik: -9.5659e+02 - logprior: -7.7682e+00
Fitted a model with MAP estimate = -957.9407
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1006.1733 - loglik: -9.6044e+02 - logprior: -4.5734e+01
Epoch 2/10
10/10 - 2s - loss: 964.1426 - loglik: -9.5752e+02 - logprior: -6.6208e+00
Epoch 3/10
10/10 - 2s - loss: 952.6219 - loglik: -9.5417e+02 - logprior: 1.5476
Epoch 4/10
10/10 - 2s - loss: 947.1127 - loglik: -9.5204e+02 - logprior: 4.9313
Epoch 5/10
10/10 - 2s - loss: 941.7604 - loglik: -9.4876e+02 - logprior: 7.0037
Epoch 6/10
10/10 - 2s - loss: 941.8416 - loglik: -9.5005e+02 - logprior: 8.2087
Fitted a model with MAP estimate = -938.8491
Time for alignment: 63.4806
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1187.5267 - loglik: -1.1347e+03 - logprior: -5.2807e+01
Epoch 2/10
10/10 - 2s - loss: 1091.9205 - loglik: -1.0831e+03 - logprior: -8.8120e+00
Epoch 3/10
10/10 - 2s - loss: 1031.3461 - loglik: -1.0296e+03 - logprior: -1.7364e+00
Epoch 4/10
10/10 - 2s - loss: 993.9624 - loglik: -9.9463e+02 - logprior: 0.6677
Epoch 5/10
10/10 - 2s - loss: 982.3947 - loglik: -9.8412e+02 - logprior: 1.7300
Epoch 6/10
10/10 - 2s - loss: 978.5586 - loglik: -9.8088e+02 - logprior: 2.3199
Epoch 7/10
10/10 - 2s - loss: 974.2523 - loglik: -9.7690e+02 - logprior: 2.6457
Epoch 8/10
10/10 - 2s - loss: 973.0578 - loglik: -9.7590e+02 - logprior: 2.8426
Epoch 9/10
10/10 - 2s - loss: 973.9222 - loglik: -9.7699e+02 - logprior: 3.0646
Fitted a model with MAP estimate = -972.6323
expansions: [(9, 2), (19, 2), (22, 1), (23, 2), (30, 2), (44, 3), (46, 1), (52, 1), (74, 1), (81, 2), (94, 1), (102, 2), (105, 2), (106, 1), (116, 2), (145, 3), (151, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1046.8252 - loglik: -9.8596e+02 - logprior: -6.0869e+01
Epoch 2/2
10/10 - 2s - loss: 992.4711 - loglik: -9.7070e+02 - logprior: -2.1768e+01
Fitted a model with MAP estimate = -979.2232
expansions: [(0, 12), (127, 1), (139, 1)]
discards: [  0  21  28  95 119 169 170 171 180]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1017.9699 - loglik: -9.7087e+02 - logprior: -4.7098e+01
Epoch 2/2
10/10 - 2s - loss: 971.1517 - loglik: -9.6331e+02 - logprior: -7.8435e+00
Fitted a model with MAP estimate = -961.9175
expansions: [(20, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 45 66]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 1013.4849 - loglik: -9.6781e+02 - logprior: -4.5677e+01
Epoch 2/10
10/10 - 2s - loss: 967.8188 - loglik: -9.6123e+02 - logprior: -6.5899e+00
Epoch 3/10
10/10 - 2s - loss: 958.3257 - loglik: -9.5991e+02 - logprior: 1.5846
Epoch 4/10
10/10 - 2s - loss: 952.6375 - loglik: -9.5769e+02 - logprior: 5.0505
Epoch 5/10
10/10 - 2s - loss: 949.1530 - loglik: -9.5626e+02 - logprior: 7.1046
Epoch 6/10
10/10 - 2s - loss: 947.8553 - loglik: -9.5619e+02 - logprior: 8.3386
Epoch 7/10
10/10 - 2s - loss: 941.5414 - loglik: -9.5065e+02 - logprior: 9.1100
Epoch 8/10
10/10 - 2s - loss: 941.5359 - loglik: -9.5121e+02 - logprior: 9.6776
Epoch 9/10
10/10 - 2s - loss: 943.7051 - loglik: -9.5387e+02 - logprior: 10.1621
Fitted a model with MAP estimate = -942.0161
Time for alignment: 69.4521
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1187.8704 - loglik: -1.1351e+03 - logprior: -5.2798e+01
Epoch 2/10
10/10 - 2s - loss: 1090.6887 - loglik: -1.0819e+03 - logprior: -8.7792e+00
Epoch 3/10
10/10 - 2s - loss: 1031.6838 - loglik: -1.0301e+03 - logprior: -1.5961e+00
Epoch 4/10
10/10 - 2s - loss: 999.0422 - loglik: -9.9962e+02 - logprior: 0.5784
Epoch 5/10
10/10 - 2s - loss: 991.4870 - loglik: -9.9318e+02 - logprior: 1.6911
Epoch 6/10
10/10 - 2s - loss: 983.4069 - loglik: -9.8588e+02 - logprior: 2.4710
Epoch 7/10
10/10 - 2s - loss: 983.2036 - loglik: -9.8598e+02 - logprior: 2.7733
Epoch 8/10
10/10 - 2s - loss: 981.2957 - loglik: -9.8427e+02 - logprior: 2.9745
Epoch 9/10
10/10 - 2s - loss: 979.9372 - loglik: -9.8310e+02 - logprior: 3.1642
Epoch 10/10
10/10 - 2s - loss: 980.1443 - loglik: -9.8352e+02 - logprior: 3.3781
Fitted a model with MAP estimate = -979.5468
expansions: [(9, 2), (22, 1), (23, 3), (30, 2), (44, 1), (45, 2), (46, 1), (75, 1), (76, 2), (81, 1), (92, 2), (105, 1), (106, 1), (107, 2), (114, 1), (117, 2), (119, 2), (144, 1), (145, 4), (152, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1054.6138 - loglik: -9.9416e+02 - logprior: -6.0458e+01
Epoch 2/2
10/10 - 2s - loss: 994.2562 - loglik: -9.7295e+02 - logprior: -2.1303e+01
Fitted a model with MAP estimate = -982.9003
expansions: [(0, 12)]
discards: [  0  26  54 108 144 170]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1024.1238 - loglik: -9.7702e+02 - logprior: -4.7100e+01
Epoch 2/2
10/10 - 3s - loss: 972.2127 - loglik: -9.6423e+02 - logprior: -7.9786e+00
Fitted a model with MAP estimate = -965.5209
expansions: [(177, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  19 134]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1017.3904 - loglik: -9.7152e+02 - logprior: -4.5873e+01
Epoch 2/10
10/10 - 2s - loss: 973.1400 - loglik: -9.6617e+02 - logprior: -6.9705e+00
Epoch 3/10
10/10 - 2s - loss: 960.5951 - loglik: -9.6173e+02 - logprior: 1.1367
Epoch 4/10
10/10 - 2s - loss: 959.6737 - loglik: -9.6426e+02 - logprior: 4.5826
Epoch 5/10
10/10 - 2s - loss: 951.1536 - loglik: -9.5781e+02 - logprior: 6.6527
Epoch 6/10
10/10 - 2s - loss: 949.6442 - loglik: -9.5748e+02 - logprior: 7.8406
Epoch 7/10
10/10 - 2s - loss: 948.6180 - loglik: -9.5722e+02 - logprior: 8.5996
Epoch 8/10
10/10 - 2s - loss: 947.2953 - loglik: -9.5653e+02 - logprior: 9.2385
Epoch 9/10
10/10 - 2s - loss: 948.4352 - loglik: -9.5816e+02 - logprior: 9.7247
Fitted a model with MAP estimate = -946.4136
Time for alignment: 70.3705
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1189.8944 - loglik: -1.1371e+03 - logprior: -5.2810e+01
Epoch 2/10
10/10 - 2s - loss: 1087.9084 - loglik: -1.0791e+03 - logprior: -8.7849e+00
Epoch 3/10
10/10 - 2s - loss: 1028.2919 - loglik: -1.0266e+03 - logprior: -1.6520e+00
Epoch 4/10
10/10 - 2s - loss: 996.3405 - loglik: -9.9679e+02 - logprior: 0.4482
Epoch 5/10
10/10 - 2s - loss: 981.3385 - loglik: -9.8270e+02 - logprior: 1.3602
Epoch 6/10
10/10 - 2s - loss: 976.8162 - loglik: -9.7886e+02 - logprior: 2.0393
Epoch 7/10
10/10 - 2s - loss: 974.6738 - loglik: -9.7706e+02 - logprior: 2.3816
Epoch 8/10
10/10 - 2s - loss: 972.7098 - loglik: -9.7530e+02 - logprior: 2.5884
Epoch 9/10
10/10 - 2s - loss: 972.1068 - loglik: -9.7495e+02 - logprior: 2.8420
Epoch 10/10
10/10 - 2s - loss: 972.9966 - loglik: -9.7601e+02 - logprior: 3.0173
Fitted a model with MAP estimate = -971.4960
expansions: [(9, 2), (19, 2), (20, 2), (22, 2), (23, 2), (25, 1), (30, 3), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (75, 1), (81, 1), (82, 1), (100, 1), (104, 4), (106, 1), (116, 2), (119, 1), (152, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1043.9163 - loglik: -9.8380e+02 - logprior: -6.0119e+01
Epoch 2/2
10/10 - 3s - loss: 981.7435 - loglik: -9.6095e+02 - logprior: -2.0794e+01
Fitted a model with MAP estimate = -970.0135
expansions: [(0, 12)]
discards: [  0  21  23  28  30  40  69  95 185 186]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1012.7902 - loglik: -9.6616e+02 - logprior: -4.6628e+01
Epoch 2/2
10/10 - 3s - loss: 965.5533 - loglik: -9.5810e+02 - logprior: -7.4530e+00
Fitted a model with MAP estimate = -956.6289
expansions: [(100, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 65]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1005.0231 - loglik: -9.5955e+02 - logprior: -4.5477e+01
Epoch 2/10
10/10 - 2s - loss: 963.1437 - loglik: -9.5663e+02 - logprior: -6.5106e+00
Epoch 3/10
10/10 - 2s - loss: 951.3201 - loglik: -9.5307e+02 - logprior: 1.7494
Epoch 4/10
10/10 - 2s - loss: 946.4037 - loglik: -9.5161e+02 - logprior: 5.2108
Epoch 5/10
10/10 - 2s - loss: 941.3060 - loglik: -9.4849e+02 - logprior: 7.1814
Epoch 6/10
10/10 - 2s - loss: 938.2518 - loglik: -9.4660e+02 - logprior: 8.3452
Epoch 7/10
10/10 - 2s - loss: 936.6647 - loglik: -9.4577e+02 - logprior: 9.1087
Epoch 8/10
10/10 - 2s - loss: 937.1483 - loglik: -9.4682e+02 - logprior: 9.6678
Fitted a model with MAP estimate = -935.4728
Time for alignment: 67.7547
Computed alignments with likelihoods: ['-935.4957', '-938.8491', '-942.0161', '-946.4136', '-935.4728']
Best model has likelihood: -935.4728  (prior= 9.9585 )
time for generating output: 0.2350
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7638780297107115
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 480.6158 - loglik: -4.7752e+02 - logprior: -3.1004e+00
Epoch 2/10
19/19 - 2s - loss: 448.9737 - loglik: -4.4786e+02 - logprior: -1.1176e+00
Epoch 3/10
19/19 - 2s - loss: 439.3592 - loglik: -4.3820e+02 - logprior: -1.1638e+00
Epoch 4/10
19/19 - 2s - loss: 437.1207 - loglik: -4.3597e+02 - logprior: -1.1526e+00
Epoch 5/10
19/19 - 2s - loss: 435.2031 - loglik: -4.3408e+02 - logprior: -1.1182e+00
Epoch 6/10
19/19 - 2s - loss: 435.9702 - loglik: -4.3487e+02 - logprior: -1.1026e+00
Fitted a model with MAP estimate = -414.7583
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 4), (22, 1), (46, 2), (47, 1), (48, 2), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 438.4836 - loglik: -4.3447e+02 - logprior: -4.0161e+00
Epoch 2/2
19/19 - 2s - loss: 431.2462 - loglik: -4.2987e+02 - logprior: -1.3771e+00
Fitted a model with MAP estimate = -409.7189
expansions: [(9, 2), (19, 4)]
discards: [ 0  1 23 57 64]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 433.0645 - loglik: -4.3018e+02 - logprior: -2.8801e+00
Epoch 2/2
19/19 - 2s - loss: 428.7724 - loglik: -4.2780e+02 - logprior: -9.7128e-01
Fitted a model with MAP estimate = -408.1761
expansions: [(0, 2)]
discards: [ 8 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 411.4330 - loglik: -4.0921e+02 - logprior: -2.2271e+00
Epoch 2/10
23/23 - 2s - loss: 407.9824 - loglik: -4.0701e+02 - logprior: -9.7609e-01
Epoch 3/10
23/23 - 2s - loss: 407.9875 - loglik: -4.0706e+02 - logprior: -9.2591e-01
Fitted a model with MAP estimate = -407.4172
Time for alignment: 48.8311
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 480.9227 - loglik: -4.7782e+02 - logprior: -3.1046e+00
Epoch 2/10
19/19 - 2s - loss: 450.3975 - loglik: -4.4926e+02 - logprior: -1.1379e+00
Epoch 3/10
19/19 - 2s - loss: 439.2091 - loglik: -4.3797e+02 - logprior: -1.2408e+00
Epoch 4/10
19/19 - 2s - loss: 437.0168 - loglik: -4.3581e+02 - logprior: -1.2035e+00
Epoch 5/10
19/19 - 2s - loss: 435.7211 - loglik: -4.3458e+02 - logprior: -1.1402e+00
Epoch 6/10
19/19 - 2s - loss: 435.6283 - loglik: -4.3451e+02 - logprior: -1.1185e+00
Epoch 7/10
19/19 - 1s - loss: 435.5592 - loglik: -4.3445e+02 - logprior: -1.1044e+00
Epoch 8/10
19/19 - 2s - loss: 435.3542 - loglik: -4.3425e+02 - logprior: -1.1032e+00
Epoch 9/10
19/19 - 1s - loss: 435.3616 - loglik: -4.3427e+02 - logprior: -1.0963e+00
Fitted a model with MAP estimate = -415.4504
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 4), (18, 2), (22, 1), (24, 1), (46, 2), (47, 1), (48, 2), (49, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 438.2402 - loglik: -4.3410e+02 - logprior: -4.1397e+00
Epoch 2/2
19/19 - 2s - loss: 430.6836 - loglik: -4.2928e+02 - logprior: -1.3993e+00
Fitted a model with MAP estimate = -409.4788
expansions: []
discards: [ 0  1  2 26 61]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 435.5459 - loglik: -4.3182e+02 - logprior: -3.7284e+00
Epoch 2/2
19/19 - 2s - loss: 431.3782 - loglik: -4.2974e+02 - logprior: -1.6389e+00
Fitted a model with MAP estimate = -409.8841
expansions: [(0, 4), (23, 4)]
discards: [ 0 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 411.0564 - loglik: -4.0919e+02 - logprior: -1.8672e+00
Epoch 2/10
23/23 - 2s - loss: 407.9568 - loglik: -4.0701e+02 - logprior: -9.4632e-01
Epoch 3/10
23/23 - 2s - loss: 407.9526 - loglik: -4.0704e+02 - logprior: -9.1016e-01
Epoch 4/10
23/23 - 2s - loss: 406.7158 - loglik: -4.0583e+02 - logprior: -8.8268e-01
Epoch 5/10
23/23 - 2s - loss: 406.4963 - loglik: -4.0562e+02 - logprior: -8.7360e-01
Epoch 6/10
23/23 - 2s - loss: 406.8753 - loglik: -4.0601e+02 - logprior: -8.6082e-01
Fitted a model with MAP estimate = -406.1037
Time for alignment: 58.4714
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 480.9324 - loglik: -4.7782e+02 - logprior: -3.1081e+00
Epoch 2/10
19/19 - 1s - loss: 450.4648 - loglik: -4.4932e+02 - logprior: -1.1422e+00
Epoch 3/10
19/19 - 2s - loss: 440.5125 - loglik: -4.3932e+02 - logprior: -1.1900e+00
Epoch 4/10
19/19 - 2s - loss: 437.7417 - loglik: -4.3658e+02 - logprior: -1.1634e+00
Epoch 5/10
19/19 - 2s - loss: 436.5234 - loglik: -4.3538e+02 - logprior: -1.1399e+00
Epoch 6/10
19/19 - 2s - loss: 436.2033 - loglik: -4.3509e+02 - logprior: -1.1132e+00
Epoch 7/10
19/19 - 2s - loss: 436.3884 - loglik: -4.3530e+02 - logprior: -1.0905e+00
Fitted a model with MAP estimate = -415.4683
expansions: [(0, 2), (3, 1), (4, 1), (16, 1), (17, 3), (18, 1), (23, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 438.6633 - loglik: -4.3459e+02 - logprior: -4.0766e+00
Epoch 2/2
19/19 - 2s - loss: 431.0181 - loglik: -4.2966e+02 - logprior: -1.3538e+00
Fitted a model with MAP estimate = -409.7000
expansions: [(9, 2)]
discards: [ 0  1 58]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 432.5653 - loglik: -4.2967e+02 - logprior: -2.8929e+00
Epoch 2/2
19/19 - 2s - loss: 429.5474 - loglik: -4.2855e+02 - logprior: -9.9429e-01
Fitted a model with MAP estimate = -408.6883
expansions: [(0, 2), (23, 3), (27, 1)]
discards: [ 8  9 24 25]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 411.5113 - loglik: -4.0923e+02 - logprior: -2.2764e+00
Epoch 2/10
23/23 - 2s - loss: 407.8880 - loglik: -4.0687e+02 - logprior: -1.0199e+00
Epoch 3/10
23/23 - 2s - loss: 407.4246 - loglik: -4.0645e+02 - logprior: -9.6973e-01
Epoch 4/10
23/23 - 2s - loss: 407.1783 - loglik: -4.0626e+02 - logprior: -9.1790e-01
Epoch 5/10
23/23 - 2s - loss: 406.5051 - loglik: -4.0560e+02 - logprior: -9.0300e-01
Epoch 6/10
23/23 - 2s - loss: 406.1341 - loglik: -4.0524e+02 - logprior: -8.9078e-01
Epoch 7/10
23/23 - 2s - loss: 406.0784 - loglik: -4.0520e+02 - logprior: -8.7837e-01
Epoch 8/10
23/23 - 2s - loss: 405.3752 - loglik: -4.0450e+02 - logprior: -8.7819e-01
Epoch 9/10
23/23 - 2s - loss: 406.3666 - loglik: -4.0551e+02 - logprior: -8.6094e-01
Fitted a model with MAP estimate = -405.7117
Time for alignment: 63.1446
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 480.8567 - loglik: -4.7775e+02 - logprior: -3.1048e+00
Epoch 2/10
19/19 - 2s - loss: 450.3715 - loglik: -4.4924e+02 - logprior: -1.1359e+00
Epoch 3/10
19/19 - 1s - loss: 439.7630 - loglik: -4.3853e+02 - logprior: -1.2318e+00
Epoch 4/10
19/19 - 2s - loss: 437.5610 - loglik: -4.3640e+02 - logprior: -1.1611e+00
Epoch 5/10
19/19 - 2s - loss: 435.8991 - loglik: -4.3477e+02 - logprior: -1.1260e+00
Epoch 6/10
19/19 - 2s - loss: 435.9229 - loglik: -4.3483e+02 - logprior: -1.0894e+00
Fitted a model with MAP estimate = -415.3775
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 4), (17, 2), (36, 1), (46, 2), (47, 1), (48, 2), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 438.5518 - loglik: -4.3456e+02 - logprior: -3.9937e+00
Epoch 2/2
19/19 - 2s - loss: 430.3051 - loglik: -4.2888e+02 - logprior: -1.4288e+00
Fitted a model with MAP estimate = -409.2585
expansions: [(22, 1), (31, 1)]
discards: [ 0  1  2 24 25 26 27 62 66]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 435.7795 - loglik: -4.3206e+02 - logprior: -3.7217e+00
Epoch 2/2
19/19 - 2s - loss: 431.2608 - loglik: -4.2977e+02 - logprior: -1.4903e+00
Fitted a model with MAP estimate = -409.7482
expansions: [(0, 4), (22, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 410.3999 - loglik: -4.0853e+02 - logprior: -1.8688e+00
Epoch 2/10
23/23 - 2s - loss: 407.1382 - loglik: -4.0617e+02 - logprior: -9.6961e-01
Epoch 3/10
23/23 - 2s - loss: 406.4523 - loglik: -4.0551e+02 - logprior: -9.3741e-01
Epoch 4/10
23/23 - 2s - loss: 406.3031 - loglik: -4.0540e+02 - logprior: -9.0309e-01
Epoch 5/10
23/23 - 2s - loss: 405.8399 - loglik: -4.0493e+02 - logprior: -9.0568e-01
Epoch 6/10
23/23 - 2s - loss: 405.1732 - loglik: -4.0428e+02 - logprior: -8.9047e-01
Epoch 7/10
23/23 - 2s - loss: 405.5189 - loglik: -4.0464e+02 - logprior: -8.7662e-01
Fitted a model with MAP estimate = -404.9948
Time for alignment: 56.0105
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 481.1573 - loglik: -4.7805e+02 - logprior: -3.1091e+00
Epoch 2/10
19/19 - 2s - loss: 451.0752 - loglik: -4.4992e+02 - logprior: -1.1544e+00
Epoch 3/10
19/19 - 2s - loss: 439.5876 - loglik: -4.3834e+02 - logprior: -1.2484e+00
Epoch 4/10
19/19 - 2s - loss: 436.8438 - loglik: -4.3566e+02 - logprior: -1.1809e+00
Epoch 5/10
19/19 - 2s - loss: 436.1726 - loglik: -4.3504e+02 - logprior: -1.1316e+00
Epoch 6/10
19/19 - 2s - loss: 435.7510 - loglik: -4.3463e+02 - logprior: -1.1205e+00
Epoch 7/10
19/19 - 2s - loss: 435.3507 - loglik: -4.3425e+02 - logprior: -1.0991e+00
Epoch 8/10
19/19 - 2s - loss: 435.7123 - loglik: -4.3461e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -415.3318
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 1), (17, 3), (18, 1), (23, 1), (31, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 437.7873 - loglik: -4.3368e+02 - logprior: -4.1102e+00
Epoch 2/2
19/19 - 2s - loss: 430.1676 - loglik: -4.2878e+02 - logprior: -1.3919e+00
Fitted a model with MAP estimate = -408.7943
expansions: [(29, 1)]
discards: [ 0  1 12 60]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 432.5876 - loglik: -4.2967e+02 - logprior: -2.9209e+00
Epoch 2/2
19/19 - 2s - loss: 429.6400 - loglik: -4.2867e+02 - logprior: -9.7373e-01
Fitted a model with MAP estimate = -408.9047
expansions: [(0, 2), (22, 3)]
discards: [23 24]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 410.1894 - loglik: -4.0791e+02 - logprior: -2.2830e+00
Epoch 2/10
23/23 - 2s - loss: 408.0941 - loglik: -4.0707e+02 - logprior: -1.0193e+00
Epoch 3/10
23/23 - 2s - loss: 406.3142 - loglik: -4.0536e+02 - logprior: -9.5919e-01
Epoch 4/10
23/23 - 2s - loss: 406.1709 - loglik: -4.0526e+02 - logprior: -9.1560e-01
Epoch 5/10
23/23 - 2s - loss: 406.6653 - loglik: -4.0576e+02 - logprior: -9.0436e-01
Fitted a model with MAP estimate = -405.8095
Time for alignment: 54.4632
Computed alignments with likelihoods: ['-407.4172', '-406.1037', '-405.7117', '-404.9948', '-405.8095']
Best model has likelihood: -404.9948  (prior= -0.8864 )
time for generating output: 0.1164
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8433179723502304
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1632.7935 - loglik: -1.6271e+03 - logprior: -5.7168e+00
Epoch 2/10
22/22 - 12s - loss: 1515.9855 - loglik: -1.5154e+03 - logprior: -6.0092e-01
Epoch 3/10
22/22 - 12s - loss: 1471.8706 - loglik: -1.4703e+03 - logprior: -1.5852e+00
Epoch 4/10
22/22 - 12s - loss: 1473.6495 - loglik: -1.4721e+03 - logprior: -1.5038e+00
Fitted a model with MAP estimate = -1468.3151
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 2), (45, 1), (48, 1), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (82, 1), (98, 1), (99, 1), (104, 1), (105, 2), (107, 1), (109, 1), (121, 2), (132, 1), (144, 1), (145, 2), (149, 1), (150, 1), (156, 3), (158, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (189, 1), (193, 3), (207, 1), (210, 1), (213, 2), (214, 2), (215, 1), (224, 1), (227, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 1472.0048 - loglik: -1.4638e+03 - logprior: -8.1855e+00
Epoch 2/2
22/22 - 17s - loss: 1449.0304 - loglik: -1.4469e+03 - logprior: -2.0819e+00
Fitted a model with MAP estimate = -1448.1389
expansions: [(0, 3)]
discards: [  0  34 147 176 219 230 270 271]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1455.6451 - loglik: -1.4508e+03 - logprior: -4.8447e+00
Epoch 2/2
22/22 - 16s - loss: 1448.4191 - loglik: -1.4494e+03 - logprior: 1.0064
Fitted a model with MAP estimate = -1445.1323
expansions: [(280, 1)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 1459.7631 - loglik: -1.4522e+03 - logprior: -7.5267e+00
Epoch 2/10
22/22 - 16s - loss: 1453.2972 - loglik: -1.4519e+03 - logprior: -1.4299e+00
Epoch 3/10
22/22 - 16s - loss: 1446.0370 - loglik: -1.4464e+03 - logprior: 0.3828
Epoch 4/10
22/22 - 16s - loss: 1444.4641 - loglik: -1.4471e+03 - logprior: 2.5897
Epoch 5/10
22/22 - 16s - loss: 1441.7034 - loglik: -1.4444e+03 - logprior: 2.7348
Epoch 6/10
22/22 - 16s - loss: 1441.3248 - loglik: -1.4442e+03 - logprior: 2.9109
Epoch 7/10
22/22 - 16s - loss: 1438.2456 - loglik: -1.4414e+03 - logprior: 3.1181
Epoch 8/10
22/22 - 16s - loss: 1440.3497 - loglik: -1.4437e+03 - logprior: 3.3238
Fitted a model with MAP estimate = -1439.7003
Time for alignment: 307.0233
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 1628.1923 - loglik: -1.6225e+03 - logprior: -5.7280e+00
Epoch 2/10
22/22 - 12s - loss: 1513.5792 - loglik: -1.5129e+03 - logprior: -6.8725e-01
Epoch 3/10
22/22 - 12s - loss: 1479.7772 - loglik: -1.4780e+03 - logprior: -1.7598e+00
Epoch 4/10
22/22 - 12s - loss: 1470.5575 - loglik: -1.4689e+03 - logprior: -1.6811e+00
Epoch 5/10
22/22 - 12s - loss: 1472.2134 - loglik: -1.4706e+03 - logprior: -1.6448e+00
Fitted a model with MAP estimate = -1469.0545
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 1), (36, 1), (46, 2), (47, 1), (48, 2), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (79, 1), (82, 1), (99, 1), (104, 1), (105, 2), (110, 1), (119, 1), (121, 2), (137, 1), (144, 1), (145, 2), (149, 1), (150, 1), (156, 1), (158, 3), (159, 1), (177, 1), (179, 3), (180, 1), (181, 1), (184, 1), (185, 1), (186, 2), (194, 3), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1473.2186 - loglik: -1.4649e+03 - logprior: -8.3628e+00
Epoch 2/2
22/22 - 17s - loss: 1448.4774 - loglik: -1.4463e+03 - logprior: -2.2235e+00
Fitted a model with MAP estimate = -1447.8474
expansions: [(0, 3), (120, 1)]
discards: [  0  34  56 177 195 196 197 222 234 246]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1457.9713 - loglik: -1.4532e+03 - logprior: -4.8011e+00
Epoch 2/2
22/22 - 17s - loss: 1450.2560 - loglik: -1.4514e+03 - logprior: 1.1116
Fitted a model with MAP estimate = -1444.8946
expansions: []
discards: [  0   1   2 268]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 1458.7267 - loglik: -1.4513e+03 - logprior: -7.4281e+00
Epoch 2/10
22/22 - 16s - loss: 1452.0688 - loglik: -1.4507e+03 - logprior: -1.3709e+00
Epoch 3/10
22/22 - 16s - loss: 1448.5624 - loglik: -1.4489e+03 - logprior: 0.3145
Epoch 4/10
22/22 - 16s - loss: 1441.3046 - loglik: -1.4439e+03 - logprior: 2.6153
Epoch 5/10
22/22 - 16s - loss: 1443.1023 - loglik: -1.4459e+03 - logprior: 2.7726
Fitted a model with MAP estimate = -1440.6998
Time for alignment: 273.1034
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1626.8302 - loglik: -1.6211e+03 - logprior: -5.7236e+00
Epoch 2/10
22/22 - 12s - loss: 1517.7043 - loglik: -1.5171e+03 - logprior: -6.2583e-01
Epoch 3/10
22/22 - 12s - loss: 1479.8409 - loglik: -1.4784e+03 - logprior: -1.4825e+00
Epoch 4/10
22/22 - 12s - loss: 1472.2316 - loglik: -1.4709e+03 - logprior: -1.3551e+00
Epoch 5/10
22/22 - 12s - loss: 1469.3401 - loglik: -1.4679e+03 - logprior: -1.4008e+00
Epoch 6/10
22/22 - 12s - loss: 1469.2550 - loglik: -1.4678e+03 - logprior: -1.4293e+00
Epoch 7/10
22/22 - 12s - loss: 1466.7087 - loglik: -1.4652e+03 - logprior: -1.4597e+00
Epoch 8/10
22/22 - 12s - loss: 1466.4955 - loglik: -1.4650e+03 - logprior: -1.4476e+00
Epoch 9/10
22/22 - 12s - loss: 1466.3989 - loglik: -1.4649e+03 - logprior: -1.4534e+00
Epoch 10/10
22/22 - 12s - loss: 1472.6537 - loglik: -1.4712e+03 - logprior: -1.4442e+00
Fitted a model with MAP estimate = -1467.1627
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 7), (47, 1), (48, 1), (69, 3), (70, 1), (75, 1), (76, 2), (79, 1), (81, 1), (83, 1), (100, 1), (104, 1), (105, 2), (110, 1), (119, 1), (121, 2), (137, 1), (144, 1), (145, 2), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 2), (178, 3), (179, 1), (180, 1), (183, 2), (184, 1), (185, 1), (193, 2), (194, 1), (205, 1), (208, 1), (209, 1), (212, 1), (213, 2), (214, 1), (223, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 325 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1468.3374 - loglik: -1.4629e+03 - logprior: -5.4238e+00
Epoch 2/2
22/22 - 18s - loss: 1451.3934 - loglik: -1.4519e+03 - logprior: 0.4743
Fitted a model with MAP estimate = -1444.0758
expansions: [(298, 1)]
discards: [ 34  55  56  57  58  87 182 200 201 202 227 237 278 289 290 291]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 1457.4369 - loglik: -1.4527e+03 - logprior: -4.7738e+00
Epoch 2/2
22/22 - 16s - loss: 1446.5701 - loglik: -1.4479e+03 - logprior: 1.2857
Fitted a model with MAP estimate = -1445.4381
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 1457.6351 - loglik: -1.4534e+03 - logprior: -4.1924e+00
Epoch 2/10
22/22 - 16s - loss: 1442.8839 - loglik: -1.4446e+03 - logprior: 1.7321
Epoch 3/10
22/22 - 16s - loss: 1446.7847 - loglik: -1.4492e+03 - logprior: 2.3721
Fitted a model with MAP estimate = -1442.4626
Time for alignment: 299.6865
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1626.1202 - loglik: -1.6204e+03 - logprior: -5.7220e+00
Epoch 2/10
22/22 - 12s - loss: 1518.0739 - loglik: -1.5175e+03 - logprior: -6.0240e-01
Epoch 3/10
22/22 - 12s - loss: 1478.2444 - loglik: -1.4767e+03 - logprior: -1.5707e+00
Epoch 4/10
22/22 - 12s - loss: 1472.3423 - loglik: -1.4707e+03 - logprior: -1.5960e+00
Epoch 5/10
22/22 - 12s - loss: 1465.3864 - loglik: -1.4638e+03 - logprior: -1.6299e+00
Epoch 6/10
22/22 - 12s - loss: 1473.5228 - loglik: -1.4719e+03 - logprior: -1.6383e+00
Fitted a model with MAP estimate = -1468.5831
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 3), (47, 2), (48, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (77, 1), (78, 1), (80, 1), (82, 1), (99, 1), (103, 1), (104, 2), (108, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (177, 1), (178, 2), (179, 1), (180, 1), (184, 1), (185, 2), (193, 1), (194, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (215, 1), (226, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 1470.3344 - loglik: -1.4621e+03 - logprior: -8.2778e+00
Epoch 2/2
22/22 - 17s - loss: 1455.7518 - loglik: -1.4537e+03 - logprior: -2.0567e+00
Fitted a model with MAP estimate = -1448.0873
expansions: [(0, 3)]
discards: [  0  34  55 176 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1455.1553 - loglik: -1.4504e+03 - logprior: -4.7654e+00
Epoch 2/2
22/22 - 17s - loss: 1448.2452 - loglik: -1.4494e+03 - logprior: 1.1963
Fitted a model with MAP estimate = -1445.3410
expansions: []
discards: [  0   1   2 229]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 1460.6261 - loglik: -1.4532e+03 - logprior: -7.4381e+00
Epoch 2/10
22/22 - 16s - loss: 1452.8147 - loglik: -1.4515e+03 - logprior: -1.3454e+00
Epoch 3/10
22/22 - 16s - loss: 1448.6613 - loglik: -1.4490e+03 - logprior: 0.3355
Epoch 4/10
22/22 - 16s - loss: 1442.1876 - loglik: -1.4448e+03 - logprior: 2.6341
Epoch 5/10
22/22 - 16s - loss: 1442.9077 - loglik: -1.4457e+03 - logprior: 2.8299
Fitted a model with MAP estimate = -1441.3747
Time for alignment: 282.5166
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 16s - loss: 1628.1581 - loglik: -1.6224e+03 - logprior: -5.7347e+00
Epoch 2/10
22/22 - 12s - loss: 1519.1832 - loglik: -1.5185e+03 - logprior: -6.8627e-01
Epoch 3/10
22/22 - 12s - loss: 1470.0812 - loglik: -1.4683e+03 - logprior: -1.8140e+00
Epoch 4/10
22/22 - 12s - loss: 1475.9144 - loglik: -1.4741e+03 - logprior: -1.8031e+00
Fitted a model with MAP estimate = -1469.4457
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (65, 1), (69, 1), (70, 1), (75, 1), (76, 2), (79, 1), (81, 1), (97, 1), (99, 1), (105, 1), (106, 1), (109, 1), (121, 2), (132, 1), (138, 1), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (176, 1), (178, 2), (179, 1), (180, 1), (184, 1), (185, 1), (193, 1), (194, 1), (206, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (215, 1), (224, 2), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1472.6152 - loglik: -1.4642e+03 - logprior: -8.3971e+00
Epoch 2/2
22/22 - 17s - loss: 1452.4325 - loglik: -1.4503e+03 - logprior: -2.1042e+00
Fitted a model with MAP estimate = -1447.8060
expansions: [(0, 3), (133, 1)]
discards: [  0  34  57 193 194 195 282 283]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 1460.2111 - loglik: -1.4553e+03 - logprior: -4.8851e+00
Epoch 2/2
22/22 - 16s - loss: 1445.7047 - loglik: -1.4468e+03 - logprior: 1.0618
Fitted a model with MAP estimate = -1444.9339
expansions: [(281, 2)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 1463.5470 - loglik: -1.4562e+03 - logprior: -7.3709e+00
Epoch 2/10
22/22 - 16s - loss: 1444.6505 - loglik: -1.4432e+03 - logprior: -1.4877e+00
Epoch 3/10
22/22 - 16s - loss: 1448.9044 - loglik: -1.4492e+03 - logprior: 0.2578
Fitted a model with MAP estimate = -1442.7132
Time for alignment: 226.4496
Computed alignments with likelihoods: ['-1439.7003', '-1440.6998', '-1442.4626', '-1441.3747', '-1442.7132']
Best model has likelihood: -1439.7003  (prior= 3.4419 )
time for generating output: 0.3056
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9055305103848011
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 256.9901 - loglik: -1.6460e+02 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 0s - loss: 179.7829 - loglik: -1.5319e+02 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 0s - loss: 157.8220 - loglik: -1.4452e+02 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 0s - loss: 148.5388 - loglik: -1.4035e+02 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 143.8111 - loglik: -1.3823e+02 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 1s - loss: 141.6935 - loglik: -1.3754e+02 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 1s - loss: 140.5848 - loglik: -1.3724e+02 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 139.9679 - loglik: -1.3714e+02 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 139.6580 - loglik: -1.3719e+02 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 0s - loss: 139.4415 - loglik: -1.3728e+02 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -139.3502
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 257.5632 - loglik: -1.3391e+02 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 171.0055 - loglik: -1.3087e+02 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -154.1019
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 215.1204 - loglik: -1.2819e+02 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 153.5825 - loglik: -1.2852e+02 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 141.2049 - loglik: -1.2892e+02 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 136.2716 - loglik: -1.2912e+02 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 133.7024 - loglik: -1.2923e+02 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 132.3143 - loglik: -1.2928e+02 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 0s - loss: 131.5633 - loglik: -1.2939e+02 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 0s - loss: 130.9077 - loglik: -1.2937e+02 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 130.0289 - loglik: -1.2895e+02 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 129.6400 - loglik: -1.2891e+02 - logprior: -7.3446e-01
Fitted a model with MAP estimate = -129.5060
Time for alignment: 20.6033
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 256.9901 - loglik: -1.6460e+02 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 0s - loss: 179.7829 - loglik: -1.5319e+02 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 157.8220 - loglik: -1.4452e+02 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 148.5388 - loglik: -1.4035e+02 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 143.8111 - loglik: -1.3823e+02 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 141.6935 - loglik: -1.3754e+02 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 0s - loss: 140.5848 - loglik: -1.3724e+02 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 0s - loss: 139.9679 - loglik: -1.3714e+02 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 0s - loss: 139.6580 - loglik: -1.3719e+02 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 0s - loss: 139.4415 - loglik: -1.3728e+02 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -139.3502
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 257.5632 - loglik: -1.3391e+02 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 1s - loss: 171.0055 - loglik: -1.3087e+02 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -154.1019
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 215.1204 - loglik: -1.2819e+02 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 0s - loss: 153.5825 - loglik: -1.2852e+02 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 0s - loss: 141.2049 - loglik: -1.2892e+02 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 136.2716 - loglik: -1.2912e+02 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 133.7024 - loglik: -1.2923e+02 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 0s - loss: 132.3143 - loglik: -1.2928e+02 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 0s - loss: 131.5633 - loglik: -1.2939e+02 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 130.9078 - loglik: -1.2937e+02 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 130.0289 - loglik: -1.2895e+02 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 0s - loss: 129.6401 - loglik: -1.2891e+02 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -129.5059
Time for alignment: 23.4612
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 256.9901 - loglik: -1.6460e+02 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 179.7829 - loglik: -1.5319e+02 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 157.8220 - loglik: -1.4452e+02 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 148.5388 - loglik: -1.4035e+02 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 143.8111 - loglik: -1.3823e+02 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 1s - loss: 141.6935 - loglik: -1.3754e+02 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 1s - loss: 140.5848 - loglik: -1.3724e+02 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 139.9679 - loglik: -1.3714e+02 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 139.6580 - loglik: -1.3719e+02 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 139.4415 - loglik: -1.3728e+02 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -139.3503
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.5632 - loglik: -1.3391e+02 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 1s - loss: 171.0055 - loglik: -1.3087e+02 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -154.1019
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 215.1204 - loglik: -1.2819e+02 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 153.5825 - loglik: -1.2852e+02 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 141.2049 - loglik: -1.2892e+02 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 136.2716 - loglik: -1.2912e+02 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 1s - loss: 133.7024 - loglik: -1.2923e+02 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 132.3143 - loglik: -1.2928e+02 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 131.5632 - loglik: -1.2939e+02 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 130.9079 - loglik: -1.2937e+02 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 130.0290 - loglik: -1.2895e+02 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 129.6401 - loglik: -1.2891e+02 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -129.5059
Time for alignment: 22.8158
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 256.9901 - loglik: -1.6460e+02 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 179.7829 - loglik: -1.5319e+02 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 157.8220 - loglik: -1.4452e+02 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 0s - loss: 148.5388 - loglik: -1.4035e+02 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 0s - loss: 143.8111 - loglik: -1.3823e+02 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 1s - loss: 141.6935 - loglik: -1.3754e+02 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 1s - loss: 140.5848 - loglik: -1.3724e+02 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 139.9679 - loglik: -1.3714e+02 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 139.6580 - loglik: -1.3719e+02 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 0s - loss: 139.4415 - loglik: -1.3728e+02 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -139.3503
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.5632 - loglik: -1.3391e+02 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 1s - loss: 171.0055 - loglik: -1.3087e+02 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -154.1019
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 215.1204 - loglik: -1.2819e+02 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 153.5825 - loglik: -1.2852e+02 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 141.2049 - loglik: -1.2892e+02 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 1s - loss: 136.2716 - loglik: -1.2912e+02 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 0s - loss: 133.7024 - loglik: -1.2923e+02 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 1s - loss: 132.3143 - loglik: -1.2928e+02 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 1s - loss: 131.5632 - loglik: -1.2939e+02 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 1s - loss: 130.9079 - loglik: -1.2937e+02 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 130.0290 - loglik: -1.2895e+02 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 129.6401 - loglik: -1.2891e+02 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -129.5059
Time for alignment: 22.8948
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 256.9901 - loglik: -1.6460e+02 - logprior: -9.2391e+01
Epoch 2/10
10/10 - 1s - loss: 179.7829 - loglik: -1.5319e+02 - logprior: -2.6590e+01
Epoch 3/10
10/10 - 1s - loss: 157.8220 - loglik: -1.4452e+02 - logprior: -1.3300e+01
Epoch 4/10
10/10 - 1s - loss: 148.5388 - loglik: -1.4035e+02 - logprior: -8.1896e+00
Epoch 5/10
10/10 - 1s - loss: 143.8111 - loglik: -1.3823e+02 - logprior: -5.5854e+00
Epoch 6/10
10/10 - 0s - loss: 141.6935 - loglik: -1.3754e+02 - logprior: -4.1519e+00
Epoch 7/10
10/10 - 1s - loss: 140.5848 - loglik: -1.3724e+02 - logprior: -3.3420e+00
Epoch 8/10
10/10 - 1s - loss: 139.9679 - loglik: -1.3714e+02 - logprior: -2.8285e+00
Epoch 9/10
10/10 - 1s - loss: 139.6580 - loglik: -1.3719e+02 - logprior: -2.4632e+00
Epoch 10/10
10/10 - 1s - loss: 139.4415 - loglik: -1.3728e+02 - logprior: -2.1615e+00
Fitted a model with MAP estimate = -139.3502
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 257.5632 - loglik: -1.3391e+02 - logprior: -1.2365e+02
Epoch 2/2
10/10 - 0s - loss: 171.0055 - loglik: -1.3087e+02 - logprior: -4.0136e+01
Fitted a model with MAP estimate = -154.1019
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 215.1204 - loglik: -1.2819e+02 - logprior: -8.6934e+01
Epoch 2/10
10/10 - 1s - loss: 153.5825 - loglik: -1.2852e+02 - logprior: -2.5061e+01
Epoch 3/10
10/10 - 1s - loss: 141.2049 - loglik: -1.2892e+02 - logprior: -1.2288e+01
Epoch 4/10
10/10 - 0s - loss: 136.2716 - loglik: -1.2912e+02 - logprior: -7.1522e+00
Epoch 5/10
10/10 - 0s - loss: 133.7024 - loglik: -1.2923e+02 - logprior: -4.4725e+00
Epoch 6/10
10/10 - 0s - loss: 132.3143 - loglik: -1.2928e+02 - logprior: -3.0311e+00
Epoch 7/10
10/10 - 0s - loss: 131.5633 - loglik: -1.2939e+02 - logprior: -2.1746e+00
Epoch 8/10
10/10 - 0s - loss: 130.9079 - loglik: -1.2937e+02 - logprior: -1.5408e+00
Epoch 9/10
10/10 - 1s - loss: 130.0289 - loglik: -1.2895e+02 - logprior: -1.0783e+00
Epoch 10/10
10/10 - 1s - loss: 129.6401 - loglik: -1.2891e+02 - logprior: -7.3445e-01
Fitted a model with MAP estimate = -129.5059
Time for alignment: 21.9661
Computed alignments with likelihoods: ['-129.5060', '-129.5059', '-129.5059', '-129.5059', '-129.5059']
Best model has likelihood: -129.5059  (prior= -0.5813 )
time for generating output: 0.1000
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 725.4164 - loglik: -7.1311e+02 - logprior: -1.2302e+01
Epoch 2/10
11/11 - 2s - loss: 671.7564 - loglik: -6.6882e+02 - logprior: -2.9357e+00
Epoch 3/10
11/11 - 2s - loss: 631.1494 - loglik: -6.2921e+02 - logprior: -1.9442e+00
Epoch 4/10
11/11 - 2s - loss: 612.6901 - loglik: -6.1062e+02 - logprior: -2.0659e+00
Epoch 5/10
11/11 - 2s - loss: 608.2007 - loglik: -6.0607e+02 - logprior: -2.1332e+00
Epoch 6/10
11/11 - 2s - loss: 603.3090 - loglik: -6.0117e+02 - logprior: -2.1411e+00
Epoch 7/10
11/11 - 2s - loss: 603.2175 - loglik: -6.0114e+02 - logprior: -2.0765e+00
Epoch 8/10
11/11 - 2s - loss: 602.1458 - loglik: -6.0014e+02 - logprior: -2.0091e+00
Epoch 9/10
11/11 - 2s - loss: 600.4425 - loglik: -5.9843e+02 - logprior: -2.0112e+00
Epoch 10/10
11/11 - 2s - loss: 600.4545 - loglik: -5.9843e+02 - logprior: -2.0284e+00
Fitted a model with MAP estimate = -601.0665
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 617.5259 - loglik: -6.0329e+02 - logprior: -1.4233e+01
Epoch 2/2
11/11 - 2s - loss: 591.7591 - loglik: -5.8571e+02 - logprior: -6.0517e+00
Fitted a model with MAP estimate = -589.2652
expansions: [(0, 20)]
discards: [  0   8  14  84  87 110]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 589.3077 - loglik: -5.8184e+02 - logprior: -7.4667e+00
Epoch 2/2
22/22 - 3s - loss: 577.1120 - loglik: -5.7550e+02 - logprior: -1.6134e+00
Fitted a model with MAP estimate = -574.4373
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 93]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 596.1108 - loglik: -5.8249e+02 - logprior: -1.3623e+01
Epoch 2/10
11/11 - 2s - loss: 585.2167 - loglik: -5.8129e+02 - logprior: -3.9240e+00
Epoch 3/10
11/11 - 2s - loss: 580.8885 - loglik: -5.7954e+02 - logprior: -1.3490e+00
Epoch 4/10
11/11 - 2s - loss: 578.3170 - loglik: -5.7773e+02 - logprior: -5.9108e-01
Epoch 5/10
11/11 - 2s - loss: 578.9047 - loglik: -5.7857e+02 - logprior: -3.3009e-01
Fitted a model with MAP estimate = -577.7961
Time for alignment: 63.9497
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 723.5824 - loglik: -7.1127e+02 - logprior: -1.2308e+01
Epoch 2/10
11/11 - 2s - loss: 674.0053 - loglik: -6.7106e+02 - logprior: -2.9407e+00
Epoch 3/10
11/11 - 2s - loss: 630.6462 - loglik: -6.2869e+02 - logprior: -1.9554e+00
Epoch 4/10
11/11 - 2s - loss: 611.8430 - loglik: -6.0974e+02 - logprior: -2.1033e+00
Epoch 5/10
11/11 - 2s - loss: 604.0510 - loglik: -6.0189e+02 - logprior: -2.1594e+00
Epoch 6/10
11/11 - 2s - loss: 601.7973 - loglik: -5.9963e+02 - logprior: -2.1703e+00
Epoch 7/10
11/11 - 2s - loss: 602.9542 - loglik: -6.0081e+02 - logprior: -2.1458e+00
Fitted a model with MAP estimate = -600.9450
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (64, 2), (65, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 616.6284 - loglik: -6.0250e+02 - logprior: -1.4128e+01
Epoch 2/2
11/11 - 2s - loss: 589.2402 - loglik: -5.8338e+02 - logprior: -5.8625e+00
Fitted a model with MAP estimate = -586.7020
expansions: [(0, 19)]
discards: [ 0  7 76 83]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 595.4318 - loglik: -5.8375e+02 - logprior: -1.1683e+01
Epoch 2/2
11/11 - 3s - loss: 579.5496 - loglik: -5.7625e+02 - logprior: -3.3019e+00
Fitted a model with MAP estimate = -577.1070
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 593.6110 - loglik: -5.8137e+02 - logprior: -1.2242e+01
Epoch 2/10
11/11 - 2s - loss: 582.1180 - loglik: -5.7929e+02 - logprior: -2.8325e+00
Epoch 3/10
11/11 - 2s - loss: 578.1946 - loglik: -5.7706e+02 - logprior: -1.1384e+00
Epoch 4/10
11/11 - 2s - loss: 579.3372 - loglik: -5.7871e+02 - logprior: -6.2332e-01
Fitted a model with MAP estimate = -577.3421
Time for alignment: 54.4530
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 725.0366 - loglik: -7.1273e+02 - logprior: -1.2311e+01
Epoch 2/10
11/11 - 2s - loss: 671.0969 - loglik: -6.6814e+02 - logprior: -2.9519e+00
Epoch 3/10
11/11 - 2s - loss: 629.4914 - loglik: -6.2752e+02 - logprior: -1.9697e+00
Epoch 4/10
11/11 - 2s - loss: 611.8545 - loglik: -6.0982e+02 - logprior: -2.0357e+00
Epoch 5/10
11/11 - 2s - loss: 604.8635 - loglik: -6.0282e+02 - logprior: -2.0437e+00
Epoch 6/10
11/11 - 2s - loss: 603.9274 - loglik: -6.0187e+02 - logprior: -2.0542e+00
Epoch 7/10
11/11 - 2s - loss: 602.4451 - loglik: -6.0042e+02 - logprior: -2.0289e+00
Epoch 8/10
11/11 - 2s - loss: 600.1092 - loglik: -5.9811e+02 - logprior: -2.0021e+00
Epoch 9/10
11/11 - 2s - loss: 599.8266 - loglik: -5.9781e+02 - logprior: -2.0149e+00
Epoch 10/10
11/11 - 2s - loss: 600.8176 - loglik: -5.9880e+02 - logprior: -2.0177e+00
Fitted a model with MAP estimate = -600.1820
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 3), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 616.1611 - loglik: -6.0200e+02 - logprior: -1.4164e+01
Epoch 2/2
11/11 - 2s - loss: 590.0258 - loglik: -5.8413e+02 - logprior: -5.8967e+00
Fitted a model with MAP estimate = -586.4485
expansions: [(0, 19)]
discards: [ 0  8 14 77 86]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 593.3725 - loglik: -5.8173e+02 - logprior: -1.1640e+01
Epoch 2/2
11/11 - 3s - loss: 580.5317 - loglik: -5.7724e+02 - logprior: -3.2879e+00
Fitted a model with MAP estimate = -576.5902
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 594.4980 - loglik: -5.8201e+02 - logprior: -1.2486e+01
Epoch 2/10
11/11 - 2s - loss: 580.6513 - loglik: -5.7773e+02 - logprior: -2.9242e+00
Epoch 3/10
11/11 - 2s - loss: 579.1472 - loglik: -5.7797e+02 - logprior: -1.1744e+00
Epoch 4/10
11/11 - 2s - loss: 578.5848 - loglik: -5.7795e+02 - logprior: -6.3680e-01
Epoch 5/10
11/11 - 2s - loss: 574.7489 - loglik: -5.7433e+02 - logprior: -4.1778e-01
Epoch 6/10
11/11 - 2s - loss: 578.0321 - loglik: -5.7777e+02 - logprior: -2.6550e-01
Fitted a model with MAP estimate = -575.9583
Time for alignment: 63.6811
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 725.6367 - loglik: -7.1333e+02 - logprior: -1.2311e+01
Epoch 2/10
11/11 - 2s - loss: 672.1661 - loglik: -6.6922e+02 - logprior: -2.9450e+00
Epoch 3/10
11/11 - 2s - loss: 629.3232 - loglik: -6.2736e+02 - logprior: -1.9668e+00
Epoch 4/10
11/11 - 2s - loss: 611.2873 - loglik: -6.0917e+02 - logprior: -2.1184e+00
Epoch 5/10
11/11 - 2s - loss: 607.5181 - loglik: -6.0535e+02 - logprior: -2.1727e+00
Epoch 6/10
11/11 - 2s - loss: 600.4512 - loglik: -5.9826e+02 - logprior: -2.1863e+00
Epoch 7/10
11/11 - 2s - loss: 599.3989 - loglik: -5.9724e+02 - logprior: -2.1547e+00
Epoch 8/10
11/11 - 2s - loss: 602.5496 - loglik: -6.0044e+02 - logprior: -2.1073e+00
Fitted a model with MAP estimate = -600.1587
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 5), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 613.1487 - loglik: -5.9902e+02 - logprior: -1.4128e+01
Epoch 2/2
11/11 - 2s - loss: 590.1531 - loglik: -5.8433e+02 - logprior: -5.8263e+00
Fitted a model with MAP estimate = -585.8378
expansions: [(0, 18)]
discards: [ 0  7 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 593.8272 - loglik: -5.8216e+02 - logprior: -1.1664e+01
Epoch 2/2
11/11 - 3s - loss: 581.0355 - loglik: -5.7780e+02 - logprior: -3.2348e+00
Fitted a model with MAP estimate = -576.7194
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 592.6376 - loglik: -5.8026e+02 - logprior: -1.2373e+01
Epoch 2/10
11/11 - 2s - loss: 583.6189 - loglik: -5.8075e+02 - logprior: -2.8662e+00
Epoch 3/10
11/11 - 2s - loss: 578.3746 - loglik: -5.7724e+02 - logprior: -1.1348e+00
Epoch 4/10
11/11 - 2s - loss: 578.5514 - loglik: -5.7795e+02 - logprior: -5.9925e-01
Fitted a model with MAP estimate = -577.3227
Time for alignment: 55.5614
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 725.4349 - loglik: -7.1313e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 2s - loss: 670.0947 - loglik: -6.6714e+02 - logprior: -2.9500e+00
Epoch 3/10
11/11 - 2s - loss: 632.2537 - loglik: -6.3029e+02 - logprior: -1.9594e+00
Epoch 4/10
11/11 - 2s - loss: 609.6675 - loglik: -6.0757e+02 - logprior: -2.0986e+00
Epoch 5/10
11/11 - 2s - loss: 606.7354 - loglik: -6.0457e+02 - logprior: -2.1630e+00
Epoch 6/10
11/11 - 2s - loss: 602.7944 - loglik: -6.0061e+02 - logprior: -2.1829e+00
Epoch 7/10
11/11 - 2s - loss: 600.6647 - loglik: -5.9851e+02 - logprior: -2.1566e+00
Epoch 8/10
11/11 - 2s - loss: 599.7636 - loglik: -5.9763e+02 - logprior: -2.1295e+00
Epoch 9/10
11/11 - 2s - loss: 601.6469 - loglik: -5.9952e+02 - logprior: -2.1310e+00
Fitted a model with MAP estimate = -600.0555
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (64, 2), (65, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 615.0489 - loglik: -6.0088e+02 - logprior: -1.4172e+01
Epoch 2/2
11/11 - 2s - loss: 591.3721 - loglik: -5.8546e+02 - logprior: -5.9157e+00
Fitted a model with MAP estimate = -587.2780
expansions: [(0, 19)]
discards: [ 0  8 14 76 84]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 596.1153 - loglik: -5.8441e+02 - logprior: -1.1708e+01
Epoch 2/2
11/11 - 3s - loss: 583.8757 - loglik: -5.8063e+02 - logprior: -3.2427e+00
Fitted a model with MAP estimate = -578.2334
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 594.0674 - loglik: -5.8168e+02 - logprior: -1.2392e+01
Epoch 2/10
11/11 - 2s - loss: 583.8963 - loglik: -5.8102e+02 - logprior: -2.8794e+00
Epoch 3/10
11/11 - 2s - loss: 579.5379 - loglik: -5.7838e+02 - logprior: -1.1583e+00
Epoch 4/10
11/11 - 2s - loss: 577.9269 - loglik: -5.7728e+02 - logprior: -6.4904e-01
Epoch 5/10
11/11 - 2s - loss: 576.5914 - loglik: -5.7616e+02 - logprior: -4.2837e-01
Epoch 6/10
11/11 - 2s - loss: 577.1536 - loglik: -5.7688e+02 - logprior: -2.7194e-01
Fitted a model with MAP estimate = -576.3907
Time for alignment: 61.5932
Computed alignments with likelihoods: ['-574.4373', '-577.1070', '-575.9583', '-576.7194', '-576.3907']
Best model has likelihood: -574.4373  (prior= -1.3687 )
time for generating output: 0.2217
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.637984496124031
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1591.5140 - loglik: -1.5898e+03 - logprior: -1.6964e+00
Epoch 2/10
39/39 - 16s - loss: 1475.6492 - loglik: -1.4740e+03 - logprior: -1.6845e+00
Epoch 3/10
39/39 - 16s - loss: 1463.7537 - loglik: -1.4620e+03 - logprior: -1.7400e+00
Epoch 4/10
39/39 - 16s - loss: 1460.8898 - loglik: -1.4592e+03 - logprior: -1.7197e+00
Epoch 5/10
39/39 - 16s - loss: 1459.6630 - loglik: -1.4579e+03 - logprior: -1.7267e+00
Epoch 6/10
39/39 - 16s - loss: 1459.1787 - loglik: -1.4574e+03 - logprior: -1.7312e+00
Epoch 7/10
39/39 - 16s - loss: 1459.2894 - loglik: -1.4576e+03 - logprior: -1.7309e+00
Fitted a model with MAP estimate = -1354.5822
expansions: [(12, 3), (14, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (45, 3), (46, 1), (59, 3), (60, 1), (62, 4), (63, 1), (66, 2), (67, 2), (68, 2), (93, 1), (97, 2), (121, 1), (124, 5), (125, 2), (126, 2), (130, 1), (134, 1), (136, 2), (137, 3), (138, 1), (139, 1), (140, 2), (141, 2), (151, 1), (153, 1), (159, 2), (160, 1), (163, 1), (164, 1), (167, 2), (168, 4), (169, 2), (170, 2), (179, 1), (180, 2), (181, 1), (193, 1), (194, 1), (203, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [  0 156 157]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1454.3258 - loglik: -1.4516e+03 - logprior: -2.6959e+00
Epoch 2/2
39/39 - 25s - loss: 1433.3081 - loglik: -1.4322e+03 - logprior: -1.1299e+00
Fitted a model with MAP estimate = -1327.2071
expansions: [(0, 2), (237, 1)]
discards: [  0  12  74  75  88  93 125 159 160 164 178 182 190 192 231 249 288 289]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1436.1498 - loglik: -1.4346e+03 - logprior: -1.5841e+00
Epoch 2/2
39/39 - 23s - loss: 1431.5725 - loglik: -1.4310e+03 - logprior: -5.2809e-01
Fitted a model with MAP estimate = -1327.6514
expansions: []
discards: [  0 219]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1326.7655 - loglik: -1.3251e+03 - logprior: -1.6755e+00
Epoch 2/10
45/45 - 26s - loss: 1327.3275 - loglik: -1.3268e+03 - logprior: -4.8491e-01
Fitted a model with MAP estimate = -1322.5405
Time for alignment: 404.7417
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1594.5684 - loglik: -1.5929e+03 - logprior: -1.6588e+00
Epoch 2/10
39/39 - 16s - loss: 1477.8956 - loglik: -1.4764e+03 - logprior: -1.5322e+00
Epoch 3/10
39/39 - 16s - loss: 1463.0470 - loglik: -1.4614e+03 - logprior: -1.6637e+00
Epoch 4/10
39/39 - 16s - loss: 1459.4177 - loglik: -1.4578e+03 - logprior: -1.6467e+00
Epoch 5/10
39/39 - 16s - loss: 1458.0005 - loglik: -1.4563e+03 - logprior: -1.6762e+00
Epoch 6/10
39/39 - 16s - loss: 1457.2736 - loglik: -1.4556e+03 - logprior: -1.6757e+00
Epoch 7/10
39/39 - 16s - loss: 1457.5187 - loglik: -1.4558e+03 - logprior: -1.6797e+00
Fitted a model with MAP estimate = -1352.9376
expansions: [(12, 3), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (40, 1), (46, 2), (47, 2), (59, 3), (60, 6), (61, 1), (67, 1), (91, 1), (98, 2), (119, 1), (125, 5), (126, 2), (127, 2), (128, 2), (130, 1), (136, 3), (137, 2), (138, 1), (139, 1), (140, 1), (144, 2), (145, 1), (151, 1), (158, 3), (163, 1), (164, 1), (167, 3), (168, 3), (169, 2), (171, 1), (180, 2), (181, 1), (193, 1), (202, 1), (203, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1449.5271 - loglik: -1.4468e+03 - logprior: -2.7222e+00
Epoch 2/2
39/39 - 24s - loss: 1432.6252 - loglik: -1.4316e+03 - logprior: -1.0295e+00
Fitted a model with MAP estimate = -1327.1238
expansions: [(0, 2)]
discards: [  0  12  57  77 122 156 157 160 161 176 191 207 208 209 224 228 245 281]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1438.3442 - loglik: -1.4368e+03 - logprior: -1.5876e+00
Epoch 2/2
39/39 - 22s - loss: 1433.0510 - loglik: -1.4325e+03 - logprior: -5.5116e-01
Fitted a model with MAP estimate = -1328.4201
expansions: [(199, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 1329.4620 - loglik: -1.3279e+03 - logprior: -1.5830e+00
Epoch 2/10
45/45 - 25s - loss: 1323.2135 - loglik: -1.3227e+03 - logprior: -4.6498e-01
Epoch 3/10
45/45 - 26s - loss: 1324.3889 - loglik: -1.3239e+03 - logprior: -4.7677e-01
Fitted a model with MAP estimate = -1321.5197
Time for alignment: 421.6001
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1595.8398 - loglik: -1.5942e+03 - logprior: -1.6360e+00
Epoch 2/10
39/39 - 16s - loss: 1480.7670 - loglik: -1.4793e+03 - logprior: -1.5117e+00
Epoch 3/10
39/39 - 16s - loss: 1466.1663 - loglik: -1.4645e+03 - logprior: -1.6632e+00
Epoch 4/10
39/39 - 16s - loss: 1463.0350 - loglik: -1.4614e+03 - logprior: -1.6290e+00
Epoch 5/10
39/39 - 16s - loss: 1461.9872 - loglik: -1.4604e+03 - logprior: -1.6217e+00
Epoch 6/10
39/39 - 16s - loss: 1462.2017 - loglik: -1.4606e+03 - logprior: -1.6250e+00
Fitted a model with MAP estimate = -1358.0864
expansions: [(12, 2), (14, 1), (16, 1), (17, 1), (19, 1), (35, 1), (36, 1), (38, 1), (44, 3), (45, 1), (57, 2), (58, 2), (61, 4), (62, 1), (65, 2), (66, 2), (67, 2), (119, 14), (122, 2), (126, 1), (130, 1), (134, 1), (137, 1), (144, 2), (163, 3), (166, 1), (168, 3), (169, 2), (181, 2), (190, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [  0 153 154]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1452.8840 - loglik: -1.4501e+03 - logprior: -2.7846e+00
Epoch 2/2
39/39 - 24s - loss: 1435.7603 - loglik: -1.4347e+03 - logprior: -1.0414e+00
Fitted a model with MAP estimate = -1330.3293
expansions: [(0, 2), (154, 3), (203, 2), (220, 2), (221, 1)]
discards: [  0  86  92 163 191 237 276 278]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1435.4994 - loglik: -1.4338e+03 - logprior: -1.6579e+00
Epoch 2/2
39/39 - 24s - loss: 1430.3851 - loglik: -1.4298e+03 - logprior: -6.0406e-01
Fitted a model with MAP estimate = -1327.1291
expansions: []
discards: [  0 156 202 203 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 1328.5370 - loglik: -1.3269e+03 - logprior: -1.6197e+00
Epoch 2/10
45/45 - 26s - loss: 1327.2854 - loglik: -1.3267e+03 - logprior: -5.5202e-01
Epoch 3/10
45/45 - 26s - loss: 1323.9644 - loglik: -1.3235e+03 - logprior: -4.8255e-01
Epoch 4/10
45/45 - 26s - loss: 1321.1034 - loglik: -1.3207e+03 - logprior: -4.0355e-01
Epoch 5/10
45/45 - 26s - loss: 1320.7080 - loglik: -1.3203e+03 - logprior: -3.9043e-01
Epoch 6/10
45/45 - 26s - loss: 1319.7992 - loglik: -1.3195e+03 - logprior: -2.7615e-01
Epoch 7/10
45/45 - 26s - loss: 1320.1395 - loglik: -1.3200e+03 - logprior: -1.8039e-01
Fitted a model with MAP estimate = -1319.6393
Time for alignment: 515.9642
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1590.6281 - loglik: -1.5889e+03 - logprior: -1.7044e+00
Epoch 2/10
39/39 - 16s - loss: 1469.7520 - loglik: -1.4680e+03 - logprior: -1.7593e+00
Epoch 3/10
39/39 - 16s - loss: 1460.0947 - loglik: -1.4583e+03 - logprior: -1.8427e+00
Epoch 4/10
39/39 - 16s - loss: 1457.9329 - loglik: -1.4561e+03 - logprior: -1.7991e+00
Epoch 5/10
39/39 - 16s - loss: 1457.1716 - loglik: -1.4554e+03 - logprior: -1.8051e+00
Epoch 6/10
39/39 - 16s - loss: 1457.0377 - loglik: -1.4552e+03 - logprior: -1.8061e+00
Epoch 7/10
39/39 - 16s - loss: 1456.4796 - loglik: -1.4547e+03 - logprior: -1.8039e+00
Epoch 8/10
39/39 - 16s - loss: 1456.7812 - loglik: -1.4550e+03 - logprior: -1.8080e+00
Fitted a model with MAP estimate = -1353.5698
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 2), (59, 2), (60, 1), (62, 4), (67, 1), (68, 1), (69, 1), (71, 1), (91, 1), (98, 2), (119, 2), (121, 1), (124, 4), (125, 2), (127, 1), (128, 1), (135, 1), (136, 3), (137, 1), (139, 1), (140, 1), (141, 1), (142, 2), (152, 1), (154, 1), (160, 2), (161, 1), (164, 1), (165, 1), (166, 1), (167, 2), (168, 4), (169, 2), (170, 3), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [  0 156 157 158]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1451.8523 - loglik: -1.4492e+03 - logprior: -2.6804e+00
Epoch 2/2
39/39 - 24s - loss: 1433.5822 - loglik: -1.4325e+03 - logprior: -1.0352e+00
Fitted a model with MAP estimate = -1328.1949
expansions: [(0, 2), (209, 3)]
discards: [  0  11  12  58  78 123 147 155 156 176 188 225 226 245 284 285]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1435.7794 - loglik: -1.4342e+03 - logprior: -1.5713e+00
Epoch 2/2
39/39 - 23s - loss: 1431.9077 - loglik: -1.4314e+03 - logprior: -5.0994e-01
Fitted a model with MAP estimate = -1327.0208
expansions: [(57, 1)]
discards: [  0 198 199 200]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1330.8647 - loglik: -1.3292e+03 - logprior: -1.6285e+00
Epoch 2/10
45/45 - 26s - loss: 1325.1128 - loglik: -1.3246e+03 - logprior: -4.7095e-01
Epoch 3/10
45/45 - 26s - loss: 1320.1401 - loglik: -1.3198e+03 - logprior: -3.9014e-01
Epoch 4/10
45/45 - 26s - loss: 1327.6348 - loglik: -1.3272e+03 - logprior: -4.8058e-01
Fitted a model with MAP estimate = -1320.3491
Time for alignment: 467.7822
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1590.8036 - loglik: -1.5891e+03 - logprior: -1.6633e+00
Epoch 2/10
39/39 - 16s - loss: 1473.6543 - loglik: -1.4720e+03 - logprior: -1.6147e+00
Epoch 3/10
39/39 - 16s - loss: 1463.5730 - loglik: -1.4619e+03 - logprior: -1.7165e+00
Epoch 4/10
39/39 - 16s - loss: 1461.3082 - loglik: -1.4596e+03 - logprior: -1.6740e+00
Epoch 5/10
39/39 - 16s - loss: 1460.3822 - loglik: -1.4587e+03 - logprior: -1.6806e+00
Epoch 6/10
39/39 - 16s - loss: 1459.8134 - loglik: -1.4581e+03 - logprior: -1.6997e+00
Epoch 7/10
39/39 - 16s - loss: 1459.5573 - loglik: -1.4579e+03 - logprior: -1.7019e+00
Epoch 8/10
39/39 - 16s - loss: 1459.8077 - loglik: -1.4581e+03 - logprior: -1.7080e+00
Fitted a model with MAP estimate = -1355.9966
expansions: [(12, 2), (13, 1), (14, 1), (16, 1), (17, 1), (19, 1), (37, 1), (39, 1), (45, 3), (46, 1), (58, 2), (59, 2), (61, 5), (66, 1), (67, 1), (68, 1), (70, 1), (98, 2), (119, 1), (124, 1), (125, 1), (126, 4), (127, 1), (138, 2), (140, 3), (141, 3), (142, 2), (143, 2), (147, 2), (160, 4), (161, 1), (164, 1), (165, 1), (166, 1), (167, 4), (168, 3), (169, 3), (180, 2), (181, 1), (190, 1), (194, 1), (202, 1), (209, 1), (211, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1452.3582 - loglik: -1.4496e+03 - logprior: -2.7608e+00
Epoch 2/2
39/39 - 25s - loss: 1434.0544 - loglik: -1.4329e+03 - logprior: -1.1815e+00
Fitted a model with MAP estimate = -1328.9111
expansions: [(0, 2), (227, 1)]
discards: [  0  12  77  78 123 156 157 174 179 186 188 194 208 209 210 211 230 247
 285]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1438.8943 - loglik: -1.4373e+03 - logprior: -1.5583e+00
Epoch 2/2
39/39 - 23s - loss: 1434.4321 - loglik: -1.4339e+03 - logprior: -5.4848e-01
Fitted a model with MAP estimate = -1330.2316
expansions: [(154, 1), (199, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1330.3878 - loglik: -1.3288e+03 - logprior: -1.6120e+00
Epoch 2/10
45/45 - 26s - loss: 1326.1171 - loglik: -1.3257e+03 - logprior: -4.6339e-01
Epoch 3/10
45/45 - 26s - loss: 1323.8259 - loglik: -1.3234e+03 - logprior: -4.1353e-01
Epoch 4/10
45/45 - 26s - loss: 1319.5056 - loglik: -1.3190e+03 - logprior: -4.7505e-01
Epoch 5/10
45/45 - 26s - loss: 1323.4276 - loglik: -1.3231e+03 - logprior: -3.1828e-01
Fitted a model with MAP estimate = -1320.1612
Time for alignment: 494.3967
Computed alignments with likelihoods: ['-1322.5405', '-1321.5197', '-1319.6393', '-1320.3491', '-1320.1612']
Best model has likelihood: -1319.6393  (prior= -0.0812 )
time for generating output: 0.2961
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.9063851014299966
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 936.8196 - loglik: -9.3579e+02 - logprior: -1.0341e+00
Epoch 2/10
30/30 - 7s - loss: 866.7654 - loglik: -8.6566e+02 - logprior: -1.1047e+00
Epoch 3/10
30/30 - 6s - loss: 854.2448 - loglik: -8.5315e+02 - logprior: -1.0987e+00
Epoch 4/10
30/30 - 6s - loss: 851.3672 - loglik: -8.5026e+02 - logprior: -1.1078e+00
Epoch 5/10
30/30 - 6s - loss: 850.8627 - loglik: -8.4978e+02 - logprior: -1.0847e+00
Epoch 6/10
30/30 - 7s - loss: 850.5947 - loglik: -8.4952e+02 - logprior: -1.0698e+00
Epoch 7/10
30/30 - 6s - loss: 850.3417 - loglik: -8.4928e+02 - logprior: -1.0605e+00
Epoch 8/10
30/30 - 6s - loss: 849.6360 - loglik: -8.4858e+02 - logprior: -1.0570e+00
Epoch 9/10
30/30 - 6s - loss: 848.9391 - loglik: -8.4788e+02 - logprior: -1.0554e+00
Epoch 10/10
30/30 - 6s - loss: 850.3766 - loglik: -8.4932e+02 - logprior: -1.0534e+00
Fitted a model with MAP estimate = -813.7642
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 16s - loss: 844.0126 - loglik: -8.4290e+02 - logprior: -1.1130e+00
Epoch 2/2
61/61 - 11s - loss: 836.8043 - loglik: -8.3600e+02 - logprior: -8.0753e-01
Fitted a model with MAP estimate = -799.4256
expansions: []
discards: [ 30  48  51  92  94 102 133 147 151 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 838.1714 - loglik: -8.3719e+02 - logprior: -9.8410e-01
Epoch 2/2
61/61 - 10s - loss: 836.7294 - loglik: -8.3604e+02 - logprior: -6.9277e-01
Fitted a model with MAP estimate = -799.6500
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 798.2955 - loglik: -7.9766e+02 - logprior: -6.3694e-01
Epoch 2/10
87/87 - 14s - loss: 797.5662 - loglik: -7.9704e+02 - logprior: -5.2238e-01
Epoch 3/10
87/87 - 14s - loss: 795.5245 - loglik: -7.9502e+02 - logprior: -5.0221e-01
Epoch 4/10
87/87 - 14s - loss: 796.3243 - loglik: -7.9584e+02 - logprior: -4.8602e-01
Fitted a model with MAP estimate = -794.9954
Time for alignment: 292.1286
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 936.9701 - loglik: -9.3594e+02 - logprior: -1.0330e+00
Epoch 2/10
30/30 - 6s - loss: 864.1956 - loglik: -8.6308e+02 - logprior: -1.1192e+00
Epoch 3/10
30/30 - 7s - loss: 854.0456 - loglik: -8.5293e+02 - logprior: -1.1108e+00
Epoch 4/10
30/30 - 7s - loss: 851.9119 - loglik: -8.5081e+02 - logprior: -1.1055e+00
Epoch 5/10
30/30 - 6s - loss: 850.5952 - loglik: -8.4951e+02 - logprior: -1.0824e+00
Epoch 6/10
30/30 - 7s - loss: 850.6577 - loglik: -8.4959e+02 - logprior: -1.0716e+00
Fitted a model with MAP estimate = -812.9251
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 843.2405 - loglik: -8.4213e+02 - logprior: -1.1056e+00
Epoch 2/2
61/61 - 11s - loss: 837.1205 - loglik: -8.3630e+02 - logprior: -8.1823e-01
Fitted a model with MAP estimate = -799.2213
expansions: []
discards: [ 25  48  51  92  94 126 133 148 151 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 838.2054 - loglik: -8.3721e+02 - logprior: -9.9849e-01
Epoch 2/2
61/61 - 10s - loss: 836.8858 - loglik: -8.3617e+02 - logprior: -7.1306e-01
Fitted a model with MAP estimate = -799.5594
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 18s - loss: 799.4532 - loglik: -7.9880e+02 - logprior: -6.5097e-01
Epoch 2/10
87/87 - 14s - loss: 796.1101 - loglik: -7.9558e+02 - logprior: -5.3317e-01
Epoch 3/10
87/87 - 14s - loss: 795.7380 - loglik: -7.9522e+02 - logprior: -5.1598e-01
Epoch 4/10
87/87 - 14s - loss: 796.1201 - loglik: -7.9562e+02 - logprior: -4.9946e-01
Fitted a model with MAP estimate = -795.0306
Time for alignment: 266.3732
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 937.1552 - loglik: -9.3612e+02 - logprior: -1.0343e+00
Epoch 2/10
30/30 - 6s - loss: 865.2195 - loglik: -8.6410e+02 - logprior: -1.1239e+00
Epoch 3/10
30/30 - 7s - loss: 853.4249 - loglik: -8.5232e+02 - logprior: -1.1029e+00
Epoch 4/10
30/30 - 6s - loss: 851.8764 - loglik: -8.5077e+02 - logprior: -1.1091e+00
Epoch 5/10
30/30 - 6s - loss: 850.5451 - loglik: -8.4946e+02 - logprior: -1.0830e+00
Epoch 6/10
30/30 - 6s - loss: 850.4560 - loglik: -8.4938e+02 - logprior: -1.0735e+00
Epoch 7/10
30/30 - 6s - loss: 849.8308 - loglik: -8.4877e+02 - logprior: -1.0629e+00
Epoch 8/10
30/30 - 7s - loss: 849.4279 - loglik: -8.4837e+02 - logprior: -1.0597e+00
Epoch 9/10
30/30 - 6s - loss: 849.6223 - loglik: -8.4856e+02 - logprior: -1.0597e+00
Fitted a model with MAP estimate = -813.4059
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 2), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 843.4536 - loglik: -8.4235e+02 - logprior: -1.1080e+00
Epoch 2/2
61/61 - 11s - loss: 836.2839 - loglik: -8.3548e+02 - logprior: -8.0807e-01
Fitted a model with MAP estimate = -799.1923
expansions: []
discards: [ 25  48  51  92  94 126 133 147 151 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 837.7985 - loglik: -8.3681e+02 - logprior: -9.8738e-01
Epoch 2/2
61/61 - 10s - loss: 837.4377 - loglik: -8.3673e+02 - logprior: -7.1071e-01
Fitted a model with MAP estimate = -799.6062
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 797.7416 - loglik: -7.9710e+02 - logprior: -6.4019e-01
Epoch 2/10
87/87 - 14s - loss: 797.6320 - loglik: -7.9710e+02 - logprior: -5.3343e-01
Epoch 3/10
87/87 - 14s - loss: 796.6711 - loglik: -7.9616e+02 - logprior: -5.0786e-01
Epoch 4/10
87/87 - 14s - loss: 795.4835 - loglik: -7.9499e+02 - logprior: -4.9467e-01
Epoch 5/10
87/87 - 14s - loss: 795.3578 - loglik: -7.9488e+02 - logprior: -4.7514e-01
Epoch 6/10
87/87 - 14s - loss: 794.4570 - loglik: -7.9400e+02 - logprior: -4.6044e-01
Epoch 7/10
87/87 - 14s - loss: 794.8015 - loglik: -7.9436e+02 - logprior: -4.4011e-01
Fitted a model with MAP estimate = -794.9576
Time for alignment: 324.4118
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 937.2562 - loglik: -9.3622e+02 - logprior: -1.0331e+00
Epoch 2/10
30/30 - 7s - loss: 866.1857 - loglik: -8.6507e+02 - logprior: -1.1202e+00
Epoch 3/10
30/30 - 6s - loss: 854.4296 - loglik: -8.5334e+02 - logprior: -1.0939e+00
Epoch 4/10
30/30 - 6s - loss: 852.3058 - loglik: -8.5122e+02 - logprior: -1.0898e+00
Epoch 5/10
30/30 - 6s - loss: 851.3528 - loglik: -8.5029e+02 - logprior: -1.0670e+00
Epoch 6/10
30/30 - 7s - loss: 850.5392 - loglik: -8.4949e+02 - logprior: -1.0506e+00
Epoch 7/10
30/30 - 6s - loss: 851.2963 - loglik: -8.5025e+02 - logprior: -1.0438e+00
Fitted a model with MAP estimate = -813.7328
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (36, 1), (38, 1), (39, 2), (41, 2), (42, 2), (43, 2), (52, 1), (54, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 844.0969 - loglik: -8.4297e+02 - logprior: -1.1290e+00
Epoch 2/2
61/61 - 10s - loss: 836.5463 - loglik: -8.3572e+02 - logprior: -8.2870e-01
Fitted a model with MAP estimate = -799.3527
expansions: []
discards: [ 25  48  51  57  93  95 103 134 148 152 156]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 838.0112 - loglik: -8.3701e+02 - logprior: -9.9662e-01
Epoch 2/2
61/61 - 10s - loss: 837.0299 - loglik: -8.3632e+02 - logprior: -7.1083e-01
Fitted a model with MAP estimate = -799.6655
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 798.1267 - loglik: -7.9748e+02 - logprior: -6.4878e-01
Epoch 2/10
87/87 - 14s - loss: 797.6553 - loglik: -7.9712e+02 - logprior: -5.3464e-01
Epoch 3/10
87/87 - 14s - loss: 796.0439 - loglik: -7.9553e+02 - logprior: -5.1819e-01
Epoch 4/10
87/87 - 14s - loss: 796.0518 - loglik: -7.9555e+02 - logprior: -5.0010e-01
Fitted a model with MAP estimate = -794.9769
Time for alignment: 271.3206
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 937.4149 - loglik: -9.3639e+02 - logprior: -1.0270e+00
Epoch 2/10
30/30 - 7s - loss: 866.9780 - loglik: -8.6586e+02 - logprior: -1.1143e+00
Epoch 3/10
30/30 - 6s - loss: 854.5992 - loglik: -8.5349e+02 - logprior: -1.1100e+00
Epoch 4/10
30/30 - 6s - loss: 852.7594 - loglik: -8.5164e+02 - logprior: -1.1191e+00
Epoch 5/10
30/30 - 6s - loss: 851.2458 - loglik: -8.5015e+02 - logprior: -1.0978e+00
Epoch 6/10
30/30 - 7s - loss: 850.5803 - loglik: -8.4949e+02 - logprior: -1.0879e+00
Epoch 7/10
30/30 - 6s - loss: 850.4408 - loglik: -8.4936e+02 - logprior: -1.0812e+00
Epoch 8/10
30/30 - 6s - loss: 850.6222 - loglik: -8.4955e+02 - logprior: -1.0757e+00
Fitted a model with MAP estimate = -813.6296
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (36, 1), (38, 1), (39, 2), (40, 2), (41, 1), (42, 2), (43, 2), (52, 1), (54, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (97, 2), (99, 1), (111, 1), (113, 2), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 844.1646 - loglik: -8.4303e+02 - logprior: -1.1358e+00
Epoch 2/2
61/61 - 11s - loss: 837.1682 - loglik: -8.3634e+02 - logprior: -8.2493e-01
Fitted a model with MAP estimate = -799.2357
expansions: []
discards: [ 29  48  51  55  58  94  96 128 149 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 837.7651 - loglik: -8.3678e+02 - logprior: -9.8570e-01
Epoch 2/2
61/61 - 10s - loss: 837.6083 - loglik: -8.3690e+02 - logprior: -7.0481e-01
Fitted a model with MAP estimate = -799.6767
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 798.3287 - loglik: -7.9768e+02 - logprior: -6.4652e-01
Epoch 2/10
87/87 - 14s - loss: 796.8712 - loglik: -7.9634e+02 - logprior: -5.3503e-01
Epoch 3/10
87/87 - 14s - loss: 797.0345 - loglik: -7.9652e+02 - logprior: -5.1533e-01
Fitted a model with MAP estimate = -795.4131
Time for alignment: 264.3030
Computed alignments with likelihoods: ['-794.9954', '-795.0306', '-794.9576', '-794.9769', '-795.4131']
Best model has likelihood: -794.9576  (prior= -0.4325 )
time for generating output: 0.2778
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6808522382292874
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 2564.9717 - loglik: -2.5635e+03 - logprior: -1.4586e+00
Epoch 2/10
40/40 - 39s - loss: 2440.2539 - loglik: -2.4377e+03 - logprior: -2.5103e+00
Epoch 3/10
40/40 - 39s - loss: 2429.1104 - loglik: -2.4266e+03 - logprior: -2.5595e+00
Epoch 4/10
40/40 - 39s - loss: 2423.2573 - loglik: -2.4207e+03 - logprior: -2.5893e+00
Epoch 5/10
40/40 - 39s - loss: 2419.1091 - loglik: -2.4165e+03 - logprior: -2.6142e+00
Epoch 6/10
40/40 - 39s - loss: 2415.2075 - loglik: -2.4126e+03 - logprior: -2.6353e+00
Epoch 7/10
40/40 - 39s - loss: 2414.4673 - loglik: -2.4118e+03 - logprior: -2.6590e+00
Epoch 8/10
40/40 - 39s - loss: 2414.3188 - loglik: -2.4116e+03 - logprior: -2.7387e+00
Epoch 9/10
40/40 - 39s - loss: 2409.8403 - loglik: -2.4071e+03 - logprior: -2.7508e+00
Epoch 10/10
40/40 - 39s - loss: 2411.7576 - loglik: -2.4090e+03 - logprior: -2.7810e+00
Fitted a model with MAP estimate = -1794.1706
expansions: [(125, 1), (153, 1), (229, 1), (330, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 108 109
 110 111 112 113 114 123 238 239 240 241 242 243 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285
 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303
 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321
 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 15s - loss: 2603.1582 - loglik: -2.5988e+03 - logprior: -4.3697e+00
Epoch 2/2
40/40 - 12s - loss: 2575.0354 - loglik: -2.5732e+03 - logprior: -1.8764e+00
Fitted a model with MAP estimate = -1873.1245
expansions: [(14, 315), (15, 2), (113, 2), (128, 1), (130, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9 119 120 121 122 123 124 125 126]
Re-initialized the encoder parameters.
Fitting a model of length 435 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 65s - loss: 2459.2615 - loglik: -2.4576e+03 - logprior: -1.6341e+00
Epoch 2/2
40/40 - 61s - loss: 2373.8188 - loglik: -2.3735e+03 - logprior: -3.2547e-01
Fitted a model with MAP estimate = -1739.8454
expansions: [(124, 1), (125, 1), (161, 3), (239, 1), (247, 1), (272, 1), (285, 2), (286, 1), (287, 1), (291, 1), (292, 1), (294, 2), (300, 1), (313, 1)]
discards: [434]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 1723.0604 - loglik: -1.7225e+03 - logprior: -5.1521e-01
Epoch 2/10
113/113 - 93s - loss: 1732.5575 - loglik: -1.7322e+03 - logprior: -3.6645e-01
Fitted a model with MAP estimate = -1725.1517
Time for alignment: 1030.2861
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2573.4390 - loglik: -2.5725e+03 - logprior: -9.8671e-01
Epoch 2/10
40/40 - 39s - loss: 2475.2986 - loglik: -2.4754e+03 - logprior: 0.1476
Epoch 3/10
40/40 - 39s - loss: 2465.4792 - loglik: -2.4657e+03 - logprior: 0.2045
Epoch 4/10
40/40 - 39s - loss: 2459.7026 - loglik: -2.4599e+03 - logprior: 0.1928
Epoch 5/10
40/40 - 39s - loss: 2453.9575 - loglik: -2.4541e+03 - logprior: 0.1583
Epoch 6/10
40/40 - 39s - loss: 2451.3564 - loglik: -2.4515e+03 - logprior: 0.1045
Epoch 7/10
40/40 - 39s - loss: 2449.1177 - loglik: -2.4492e+03 - logprior: 0.0691
Epoch 8/10
40/40 - 39s - loss: 2449.3833 - loglik: -2.4494e+03 - logprior: 0.0398
Fitted a model with MAP estimate = -1813.1564
expansions: [(124, 1), (125, 1), (162, 2), (296, 7), (297, 2), (298, 3), (299, 1), (300, 4), (302, 1), (314, 1), (316, 2), (317, 2), (318, 5), (329, 10), (330, 88)]
discards: [  1 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 2406.4038 - loglik: -2.4055e+03 - logprior: -9.2963e-01
Epoch 2/2
80/80 - 68s - loss: 2366.4121 - loglik: -2.3664e+03 - logprior: 0.0019
Fitted a model with MAP estimate = -1738.3270
expansions: [(386, 7), (388, 1), (406, 1), (457, 2)]
discards: [305 306 314 337 339 342 343 426 427 428 429 430 431 432 434 435 439 440]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 2368.2207 - loglik: -2.3679e+03 - logprior: -3.1742e-01
Epoch 2/2
80/80 - 66s - loss: 2362.7874 - loglik: -2.3632e+03 - logprior: 0.4627
Fitted a model with MAP estimate = -1737.1353
expansions: [(428, 2)]
discards: [430 431 448 449]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 94s - loss: 1742.7045 - loglik: -1.7424e+03 - logprior: -2.9847e-01
Epoch 2/10
56/56 - 89s - loss: 1723.3877 - loglik: -1.7234e+03 - logprior: -1.0065e-02
Epoch 3/10
56/56 - 89s - loss: 1725.2870 - loglik: -1.7254e+03 - logprior: 0.0844
Fitted a model with MAP estimate = -1723.4616
Time for alignment: 1279.8380
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2562.2810 - loglik: -2.5607e+03 - logprior: -1.5545e+00
Epoch 2/10
40/40 - 39s - loss: 2441.9324 - loglik: -2.4394e+03 - logprior: -2.4963e+00
Epoch 3/10
40/40 - 39s - loss: 2430.1453 - loglik: -2.4276e+03 - logprior: -2.5024e+00
Epoch 4/10
40/40 - 39s - loss: 2424.9038 - loglik: -2.4223e+03 - logprior: -2.5599e+00
Epoch 5/10
40/40 - 39s - loss: 2419.2144 - loglik: -2.4164e+03 - logprior: -2.7678e+00
Epoch 6/10
40/40 - 39s - loss: 2415.8916 - loglik: -2.4131e+03 - logprior: -2.7742e+00
Epoch 7/10
40/40 - 39s - loss: 2415.8000 - loglik: -2.4130e+03 - logprior: -2.7723e+00
Epoch 8/10
40/40 - 39s - loss: 2414.9150 - loglik: -2.4121e+03 - logprior: -2.7684e+00
Epoch 9/10
40/40 - 39s - loss: 2413.0876 - loglik: -2.4103e+03 - logprior: -2.7968e+00
Epoch 10/10
40/40 - 39s - loss: 2413.0393 - loglik: -2.4102e+03 - logprior: -2.7996e+00
Fitted a model with MAP estimate = -1794.9041
expansions: [(124, 1), (202, 1), (210, 1), (229, 1), (330, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 232 233 234 236 237 238 239
 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257
 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 275 276
 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294
 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312
 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 15s - loss: 2602.9346 - loglik: -2.5986e+03 - logprior: -4.3011e+00
Epoch 2/2
40/40 - 12s - loss: 2573.3511 - loglik: -2.5721e+03 - logprior: -1.2886e+00
Fitted a model with MAP estimate = -1870.1195
expansions: [(0, 335), (106, 2), (113, 1)]
discards: [  0   4 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 70s - loss: 2420.3105 - loglik: -2.4193e+03 - logprior: -9.6256e-01
Epoch 2/2
80/80 - 67s - loss: 2367.9106 - loglik: -2.3675e+03 - logprior: -4.4957e-01
Fitted a model with MAP estimate = -1737.0760
expansions: [(257, 1), (266, 1), (289, 1), (295, 1), (302, 1), (308, 1), (311, 1), (313, 2), (328, 1)]
discards: [  0  21  22  31  91 101 137 203 204 205 434 435 440]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 95s - loss: 1737.4987 - loglik: -1.7369e+03 - logprior: -6.2838e-01
Epoch 2/10
113/113 - 93s - loss: 1721.2137 - loglik: -1.7209e+03 - logprior: -3.5143e-01
Epoch 3/10
113/113 - 93s - loss: 1727.2435 - loglik: -1.7268e+03 - logprior: -4.3814e-01
Fitted a model with MAP estimate = -1721.8500
Time for alignment: 1141.9943
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 2557.7744 - loglik: -2.5560e+03 - logprior: -1.7409e+00
Epoch 2/10
40/40 - 39s - loss: 2438.1304 - loglik: -2.4353e+03 - logprior: -2.8361e+00
Epoch 3/10
40/40 - 39s - loss: 2427.1616 - loglik: -2.4245e+03 - logprior: -2.6964e+00
Epoch 4/10
40/40 - 39s - loss: 2422.8101 - loglik: -2.4202e+03 - logprior: -2.6578e+00
Epoch 5/10
40/40 - 39s - loss: 2417.7920 - loglik: -2.4151e+03 - logprior: -2.6650e+00
Epoch 6/10
40/40 - 39s - loss: 2416.0195 - loglik: -2.4133e+03 - logprior: -2.6706e+00
Epoch 7/10
40/40 - 39s - loss: 2414.6287 - loglik: -2.4119e+03 - logprior: -2.6908e+00
Epoch 8/10
40/40 - 39s - loss: 2414.4338 - loglik: -2.4117e+03 - logprior: -2.6880e+00
Epoch 9/10
40/40 - 39s - loss: 2412.8909 - loglik: -2.4102e+03 - logprior: -2.6902e+00
Epoch 10/10
40/40 - 39s - loss: 2412.9536 - loglik: -2.4102e+03 - logprior: -2.7218e+00
Fitted a model with MAP estimate = -1790.3552
expansions: [(117, 1), (170, 5), (171, 1), (207, 1), (330, 8)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 211 212 213 214 215 216 217 218 239 244 245 246 247 248 249
 250 251 252 253 254 255 256 257 258 260 261 262 263 264 265 266 267 268
 269 270 271 272 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 16s - loss: 2602.4868 - loglik: -2.5985e+03 - logprior: -3.9675e+00
Epoch 2/2
40/40 - 13s - loss: 2570.7114 - loglik: -2.5693e+03 - logprior: -1.3852e+00
Fitted a model with MAP estimate = -1867.4804
expansions: [(0, 287), (10, 22), (13, 2), (15, 9), (113, 3), (114, 2), (115, 5), (116, 3), (117, 3)]
discards: [ 64  65  66  67  68 108 120 121 122 123 124 125 126 127 128 129 130 131
 132 133 134 135 136 137 138 139 140 141 142]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 2413.8037 - loglik: -2.4127e+03 - logprior: -1.1479e+00
Epoch 2/2
80/80 - 66s - loss: 2366.7017 - loglik: -2.3663e+03 - logprior: -4.3794e-01
Fitted a model with MAP estimate = -1736.5006
expansions: [(65, 1), (249, 1), (257, 1), (260, 1), (273, 1), (301, 1), (307, 2)]
discards: [  0  43 323 430 431 447 448]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 1732.2145 - loglik: -1.7316e+03 - logprior: -5.8530e-01
Epoch 2/10
113/113 - 93s - loss: 1725.4327 - loglik: -1.7251e+03 - logprior: -2.9424e-01
Epoch 3/10
113/113 - 93s - loss: 1731.2391 - loglik: -1.7310e+03 - logprior: -2.8843e-01
Fitted a model with MAP estimate = -1721.7867
Time for alignment: 1142.8728
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2570.2925 - loglik: -2.5692e+03 - logprior: -1.0877e+00
Epoch 2/10
40/40 - 39s - loss: 2470.0361 - loglik: -2.4702e+03 - logprior: 0.1416
Epoch 3/10
40/40 - 39s - loss: 2460.8188 - loglik: -2.4610e+03 - logprior: 0.1881
Epoch 4/10
40/40 - 39s - loss: 2456.7305 - loglik: -2.4569e+03 - logprior: 0.1726
Epoch 5/10
40/40 - 39s - loss: 2450.5044 - loglik: -2.4507e+03 - logprior: 0.1541
Epoch 6/10
40/40 - 39s - loss: 2446.8540 - loglik: -2.4470e+03 - logprior: 0.1096
Epoch 7/10
40/40 - 39s - loss: 2446.2944 - loglik: -2.4464e+03 - logprior: 0.0677
Epoch 8/10
40/40 - 39s - loss: 2445.7217 - loglik: -2.4458e+03 - logprior: 0.0386
Epoch 9/10
40/40 - 39s - loss: 2446.1399 - loglik: -2.4461e+03 - logprior: -2.5907e-02
Fitted a model with MAP estimate = -1811.7435
expansions: [(25, 1), (117, 2), (123, 1), (124, 1), (198, 1), (295, 9), (297, 1), (317, 2), (318, 1), (319, 10), (327, 23), (328, 1), (329, 12), (330, 63)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 2404.8999 - loglik: -2.4038e+03 - logprior: -1.0511e+00
Epoch 2/2
80/80 - 68s - loss: 2366.8567 - loglik: -2.3668e+03 - logprior: -4.2433e-02
Fitted a model with MAP estimate = -1738.4340
expansions: [(385, 1), (386, 5), (387, 6), (408, 1), (412, 1), (424, 2)]
discards: [118 222 223 224 301 333 339 369 370 441 442 443 444 445 446 447 448 449
 450 451 452 453 454 455]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 68s - loss: 2368.5605 - loglik: -2.3684e+03 - logprior: -2.1033e-01
Epoch 2/2
80/80 - 66s - loss: 2362.8140 - loglik: -2.3636e+03 - logprior: 0.7535
Fitted a model with MAP estimate = -1737.1240
expansions: [(449, 2)]
discards: [381 382 384]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 1735.1703 - loglik: -1.7349e+03 - logprior: -3.1416e-01
Epoch 2/10
56/56 - 89s - loss: 1726.6086 - loglik: -1.7266e+03 - logprior: -2.8552e-02
Epoch 3/10
56/56 - 89s - loss: 1727.8414 - loglik: -1.7279e+03 - logprior: 0.0399
Fitted a model with MAP estimate = -1723.3199
Time for alignment: 1316.4702
Computed alignments with likelihoods: ['-1725.1517', '-1723.4616', '-1721.8500', '-1721.7867', '-1723.3199']
Best model has likelihood: -1721.7867  (prior= -0.1603 )
time for generating output: 0.4499
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7748714785112071
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 972.9468 - loglik: -9.7007e+02 - logprior: -2.8739e+00
Epoch 2/10
19/19 - 4s - loss: 803.9669 - loglik: -8.0237e+02 - logprior: -1.6010e+00
Epoch 3/10
19/19 - 4s - loss: 735.2814 - loglik: -7.3307e+02 - logprior: -2.2135e+00
Epoch 4/10
19/19 - 4s - loss: 725.2349 - loglik: -7.2290e+02 - logprior: -2.3340e+00
Epoch 5/10
19/19 - 4s - loss: 721.8969 - loglik: -7.1967e+02 - logprior: -2.2296e+00
Epoch 6/10
19/19 - 4s - loss: 720.2270 - loglik: -7.1807e+02 - logprior: -2.1578e+00
Epoch 7/10
19/19 - 4s - loss: 717.1646 - loglik: -7.1504e+02 - logprior: -2.1234e+00
Epoch 8/10
19/19 - 4s - loss: 718.6466 - loglik: -7.1653e+02 - logprior: -2.1185e+00
Fitted a model with MAP estimate = -640.2431
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (101, 1), (104, 1), (114, 1), (120, 2), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 703.4245 - loglik: -7.0035e+02 - logprior: -3.0699e+00
Epoch 2/2
19/19 - 5s - loss: 667.7040 - loglik: -6.6643e+02 - logprior: -1.2745e+00
Fitted a model with MAP estimate = -595.2933
expansions: []
discards: [ 50  76 149 154]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 669.6623 - loglik: -6.6666e+02 - logprior: -2.9986e+00
Epoch 2/2
19/19 - 5s - loss: 664.5716 - loglik: -6.6349e+02 - logprior: -1.0799e+00
Fitted a model with MAP estimate = -594.3338
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 596.7742 - loglik: -5.9467e+02 - logprior: -2.1055e+00
Epoch 2/10
22/22 - 6s - loss: 592.7170 - loglik: -5.9177e+02 - logprior: -9.4427e-01
Epoch 3/10
22/22 - 6s - loss: 591.5808 - loglik: -5.9066e+02 - logprior: -9.2403e-01
Epoch 4/10
22/22 - 6s - loss: 587.2731 - loglik: -5.8637e+02 - logprior: -9.0492e-01
Epoch 5/10
22/22 - 6s - loss: 585.9729 - loglik: -5.8511e+02 - logprior: -8.6427e-01
Epoch 6/10
22/22 - 6s - loss: 585.8460 - loglik: -5.8500e+02 - logprior: -8.5108e-01
Epoch 7/10
22/22 - 6s - loss: 584.0183 - loglik: -5.8321e+02 - logprior: -8.0339e-01
Epoch 8/10
22/22 - 6s - loss: 583.0693 - loglik: -5.8229e+02 - logprior: -7.8256e-01
Epoch 9/10
22/22 - 6s - loss: 582.4450 - loglik: -5.8170e+02 - logprior: -7.4270e-01
Epoch 10/10
22/22 - 6s - loss: 582.6774 - loglik: -5.8198e+02 - logprior: -6.9787e-01
Fitted a model with MAP estimate = -582.1421
Time for alignment: 163.5203
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 972.8680 - loglik: -9.7000e+02 - logprior: -2.8727e+00
Epoch 2/10
19/19 - 4s - loss: 805.2537 - loglik: -8.0370e+02 - logprior: -1.5491e+00
Epoch 3/10
19/19 - 4s - loss: 736.7176 - loglik: -7.3457e+02 - logprior: -2.1513e+00
Epoch 4/10
19/19 - 4s - loss: 726.1730 - loglik: -7.2385e+02 - logprior: -2.3266e+00
Epoch 5/10
19/19 - 4s - loss: 722.1149 - loglik: -7.1989e+02 - logprior: -2.2266e+00
Epoch 6/10
19/19 - 4s - loss: 719.6030 - loglik: -7.1746e+02 - logprior: -2.1409e+00
Epoch 7/10
19/19 - 4s - loss: 718.8918 - loglik: -7.1679e+02 - logprior: -2.1042e+00
Epoch 8/10
19/19 - 4s - loss: 717.7985 - loglik: -7.1570e+02 - logprior: -2.1013e+00
Epoch 9/10
19/19 - 4s - loss: 716.3114 - loglik: -7.1422e+02 - logprior: -2.0890e+00
Epoch 10/10
19/19 - 4s - loss: 716.8201 - loglik: -7.1473e+02 - logprior: -2.0870e+00
Fitted a model with MAP estimate = -640.4778
expansions: [(3, 1), (6, 1), (8, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 707.4589 - loglik: -7.0435e+02 - logprior: -3.1072e+00
Epoch 2/2
19/19 - 5s - loss: 668.6522 - loglik: -6.6725e+02 - logprior: -1.4008e+00
Fitted a model with MAP estimate = -595.5764
expansions: []
discards: [ 50  70  77 127 130 141 155 157 162]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 670.2231 - loglik: -6.6722e+02 - logprior: -2.9990e+00
Epoch 2/2
19/19 - 5s - loss: 664.2026 - loglik: -6.6312e+02 - logprior: -1.0786e+00
Fitted a model with MAP estimate = -594.4377
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 597.6729 - loglik: -5.9558e+02 - logprior: -2.0941e+00
Epoch 2/10
22/22 - 6s - loss: 590.2686 - loglik: -5.8934e+02 - logprior: -9.3124e-01
Epoch 3/10
22/22 - 6s - loss: 590.5587 - loglik: -5.8962e+02 - logprior: -9.3684e-01
Fitted a model with MAP estimate = -588.6211
Time for alignment: 129.2576
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 972.9860 - loglik: -9.7012e+02 - logprior: -2.8627e+00
Epoch 2/10
19/19 - 4s - loss: 804.6161 - loglik: -8.0302e+02 - logprior: -1.5966e+00
Epoch 3/10
19/19 - 4s - loss: 734.3637 - loglik: -7.3215e+02 - logprior: -2.2094e+00
Epoch 4/10
19/19 - 4s - loss: 723.9852 - loglik: -7.2165e+02 - logprior: -2.3309e+00
Epoch 5/10
19/19 - 4s - loss: 722.0795 - loglik: -7.1985e+02 - logprior: -2.2258e+00
Epoch 6/10
19/19 - 4s - loss: 718.4465 - loglik: -7.1629e+02 - logprior: -2.1566e+00
Epoch 7/10
19/19 - 4s - loss: 717.8032 - loglik: -7.1568e+02 - logprior: -2.1189e+00
Epoch 8/10
19/19 - 4s - loss: 716.9742 - loglik: -7.1487e+02 - logprior: -2.1089e+00
Epoch 9/10
19/19 - 4s - loss: 716.4217 - loglik: -7.1431e+02 - logprior: -2.1095e+00
Epoch 10/10
19/19 - 4s - loss: 715.5746 - loglik: -7.1347e+02 - logprior: -2.0998e+00
Fitted a model with MAP estimate = -639.8706
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 705.8914 - loglik: -7.0281e+02 - logprior: -3.0853e+00
Epoch 2/2
19/19 - 5s - loss: 667.9702 - loglik: -6.6660e+02 - logprior: -1.3740e+00
Fitted a model with MAP estimate = -595.3985
expansions: []
discards: [ 50  70  77 129 140 154 156 161]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 670.1975 - loglik: -6.6720e+02 - logprior: -2.9951e+00
Epoch 2/2
19/19 - 5s - loss: 664.6450 - loglik: -6.6357e+02 - logprior: -1.0793e+00
Fitted a model with MAP estimate = -594.4680
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 597.7221 - loglik: -5.9563e+02 - logprior: -2.0873e+00
Epoch 2/10
22/22 - 6s - loss: 591.1467 - loglik: -5.9022e+02 - logprior: -9.3061e-01
Epoch 3/10
22/22 - 6s - loss: 592.1503 - loglik: -5.9124e+02 - logprior: -9.1076e-01
Fitted a model with MAP estimate = -588.6755
Time for alignment: 129.1879
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 972.8525 - loglik: -9.6998e+02 - logprior: -2.8766e+00
Epoch 2/10
19/19 - 4s - loss: 805.7205 - loglik: -8.0414e+02 - logprior: -1.5758e+00
Epoch 3/10
19/19 - 4s - loss: 736.7172 - loglik: -7.3452e+02 - logprior: -2.1963e+00
Epoch 4/10
19/19 - 4s - loss: 725.6990 - loglik: -7.2339e+02 - logprior: -2.3111e+00
Epoch 5/10
19/19 - 4s - loss: 722.8725 - loglik: -7.2066e+02 - logprior: -2.2135e+00
Epoch 6/10
19/19 - 4s - loss: 721.4072 - loglik: -7.1928e+02 - logprior: -2.1315e+00
Epoch 7/10
19/19 - 4s - loss: 720.1648 - loglik: -7.1807e+02 - logprior: -2.0913e+00
Epoch 8/10
19/19 - 4s - loss: 717.1055 - loglik: -7.1503e+02 - logprior: -2.0781e+00
Epoch 9/10
19/19 - 4s - loss: 717.2969 - loglik: -7.1523e+02 - logprior: -2.0705e+00
Fitted a model with MAP estimate = -641.2403
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (105, 1), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 704.4242 - loglik: -7.0134e+02 - logprior: -3.0882e+00
Epoch 2/2
19/19 - 5s - loss: 668.5242 - loglik: -6.6722e+02 - logprior: -1.3037e+00
Fitted a model with MAP estimate = -595.7166
expansions: []
discards: [ 50  76 151 153 158]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 669.8638 - loglik: -6.6687e+02 - logprior: -2.9963e+00
Epoch 2/2
19/19 - 5s - loss: 664.2454 - loglik: -6.6315e+02 - logprior: -1.0907e+00
Fitted a model with MAP estimate = -594.4493
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 597.2979 - loglik: -5.9520e+02 - logprior: -2.1026e+00
Epoch 2/10
22/22 - 6s - loss: 591.1679 - loglik: -5.9022e+02 - logprior: -9.4287e-01
Epoch 3/10
22/22 - 6s - loss: 592.1550 - loglik: -5.9123e+02 - logprior: -9.2176e-01
Fitted a model with MAP estimate = -588.6595
Time for alignment: 123.7043
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 973.0891 - loglik: -9.7022e+02 - logprior: -2.8653e+00
Epoch 2/10
19/19 - 4s - loss: 809.4978 - loglik: -8.0790e+02 - logprior: -1.5975e+00
Epoch 3/10
19/19 - 4s - loss: 739.4545 - loglik: -7.3725e+02 - logprior: -2.2052e+00
Epoch 4/10
19/19 - 4s - loss: 726.8710 - loglik: -7.2453e+02 - logprior: -2.3435e+00
Epoch 5/10
19/19 - 4s - loss: 723.6150 - loglik: -7.2135e+02 - logprior: -2.2669e+00
Epoch 6/10
19/19 - 4s - loss: 722.0530 - loglik: -7.1985e+02 - logprior: -2.1984e+00
Epoch 7/10
19/19 - 4s - loss: 720.7471 - loglik: -7.1860e+02 - logprior: -2.1515e+00
Epoch 8/10
19/19 - 4s - loss: 719.5389 - loglik: -7.1741e+02 - logprior: -2.1324e+00
Epoch 9/10
19/19 - 4s - loss: 718.0264 - loglik: -7.1591e+02 - logprior: -2.1184e+00
Epoch 10/10
19/19 - 4s - loss: 718.0511 - loglik: -7.1593e+02 - logprior: -2.1229e+00
Fitted a model with MAP estimate = -641.8238
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 2), (58, 2), (59, 3), (61, 2), (75, 1), (76, 1), (83, 1), (91, 1), (97, 1), (99, 1), (101, 2), (104, 1), (114, 1), (120, 2), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 707.5486 - loglik: -7.0483e+02 - logprior: -2.7201e+00
Epoch 2/2
19/19 - 5s - loss: 668.5422 - loglik: -6.6763e+02 - logprior: -9.1028e-01
Fitted a model with MAP estimate = -596.0188
expansions: []
discards: [ 69  73  77  81 129 152 157]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 670.5096 - loglik: -6.6790e+02 - logprior: -2.6062e+00
Epoch 2/2
19/19 - 5s - loss: 664.8376 - loglik: -6.6414e+02 - logprior: -6.9447e-01
Fitted a model with MAP estimate = -594.6353
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 596.4995 - loglik: -5.9469e+02 - logprior: -1.8085e+00
Epoch 2/10
22/22 - 6s - loss: 595.0626 - loglik: -5.9419e+02 - logprior: -8.7522e-01
Epoch 3/10
22/22 - 6s - loss: 588.6741 - loglik: -5.8776e+02 - logprior: -9.0945e-01
Epoch 4/10
22/22 - 6s - loss: 589.5010 - loglik: -5.8860e+02 - logprior: -8.9851e-01
Fitted a model with MAP estimate = -587.1028
Time for alignment: 134.0528
Computed alignments with likelihoods: ['-582.1421', '-588.6211', '-588.6755', '-588.6595', '-587.1028']
Best model has likelihood: -582.1421  (prior= -0.6756 )
time for generating output: 0.1998
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 1200.3267 - loglik: -1.1682e+03 - logprior: -3.2100e+01
Epoch 2/10
12/12 - 3s - loss: 1115.5734 - loglik: -1.1122e+03 - logprior: -3.3747e+00
Epoch 3/10
12/12 - 3s - loss: 1054.4152 - loglik: -1.0538e+03 - logprior: -5.6604e-01
Epoch 4/10
12/12 - 3s - loss: 1029.6796 - loglik: -1.0295e+03 - logprior: -1.3229e-01
Epoch 5/10
12/12 - 3s - loss: 1015.3036 - loglik: -1.0155e+03 - logprior: 0.2363
Epoch 6/10
12/12 - 3s - loss: 1015.7830 - loglik: -1.0164e+03 - logprior: 0.5681
Fitted a model with MAP estimate = -1014.1610
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (32, 1), (41, 1), (48, 1), (62, 1), (63, 1), (64, 1), (75, 2), (77, 2), (78, 2), (87, 1), (88, 1), (90, 1), (91, 1), (93, 1), (101, 1), (102, 1), (128, 1), (130, 2), (131, 2), (138, 1), (139, 1), (147, 3), (149, 2), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 1047.9414 - loglik: -1.0106e+03 - logprior: -3.7311e+01
Epoch 2/2
12/12 - 4s - loss: 1009.5474 - loglik: -9.9819e+02 - logprior: -1.1360e+01
Fitted a model with MAP estimate = -1003.7866
expansions: [(0, 4)]
discards: [  0  89  93 160 183 184 185 215]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1025.9235 - loglik: -9.9839e+02 - logprior: -2.7531e+01
Epoch 2/2
12/12 - 3s - loss: 996.0809 - loglik: -9.9464e+02 - logprior: -1.4405e+00
Fitted a model with MAP estimate = -992.7849
expansions: []
discards: [ 1  2  3 38]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1021.7012 - loglik: -9.9508e+02 - logprior: -2.6625e+01
Epoch 2/10
12/12 - 3s - loss: 999.7609 - loglik: -9.9886e+02 - logprior: -9.0383e-01
Epoch 3/10
12/12 - 3s - loss: 988.6599 - loglik: -9.9252e+02 - logprior: 3.8628
Epoch 4/10
12/12 - 3s - loss: 990.1909 - loglik: -9.9610e+02 - logprior: 5.9047
Fitted a model with MAP estimate = -987.2492
Time for alignment: 68.4096
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1201.6545 - loglik: -1.1695e+03 - logprior: -3.2105e+01
Epoch 2/10
12/12 - 3s - loss: 1112.7417 - loglik: -1.1094e+03 - logprior: -3.3660e+00
Epoch 3/10
12/12 - 3s - loss: 1054.7544 - loglik: -1.0541e+03 - logprior: -6.6395e-01
Epoch 4/10
12/12 - 3s - loss: 1028.1538 - loglik: -1.0282e+03 - logprior: 0.0601
Epoch 5/10
12/12 - 3s - loss: 1020.7756 - loglik: -1.0213e+03 - logprior: 0.5036
Epoch 6/10
12/12 - 3s - loss: 1020.3655 - loglik: -1.0213e+03 - logprior: 0.8910
Epoch 7/10
12/12 - 3s - loss: 1013.3080 - loglik: -1.0144e+03 - logprior: 1.0508
Epoch 8/10
12/12 - 3s - loss: 1017.0401 - loglik: -1.0183e+03 - logprior: 1.2161
Fitted a model with MAP estimate = -1015.3454
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (45, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 1), (76, 2), (77, 1), (78, 2), (86, 1), (87, 1), (90, 1), (93, 1), (101, 1), (102, 1), (128, 1), (130, 2), (132, 3), (146, 3), (149, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1049.4681 - loglik: -1.0124e+03 - logprior: -3.7093e+01
Epoch 2/2
12/12 - 4s - loss: 1016.4460 - loglik: -1.0054e+03 - logprior: -1.1070e+01
Fitted a model with MAP estimate = -1007.4916
expansions: [(0, 4)]
discards: [  0  35  36 159 185 214]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1028.6283 - loglik: -1.0015e+03 - logprior: -2.7175e+01
Epoch 2/2
12/12 - 3s - loss: 998.5983 - loglik: -9.9752e+02 - logprior: -1.0771e+00
Fitted a model with MAP estimate = -993.6230
expansions: [(155, 5)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 1023.8850 - loglik: -9.9761e+02 - logprior: -2.6277e+01
Epoch 2/10
12/12 - 4s - loss: 994.2148 - loglik: -9.9364e+02 - logprior: -5.7183e-01
Epoch 3/10
12/12 - 4s - loss: 988.9932 - loglik: -9.9313e+02 - logprior: 4.1368
Epoch 4/10
12/12 - 4s - loss: 988.1372 - loglik: -9.9429e+02 - logprior: 6.1545
Epoch 5/10
12/12 - 4s - loss: 985.4451 - loglik: -9.9270e+02 - logprior: 7.2584
Epoch 6/10
12/12 - 4s - loss: 984.6026 - loglik: -9.9243e+02 - logprior: 7.8323
Epoch 7/10
12/12 - 4s - loss: 981.6035 - loglik: -9.8990e+02 - logprior: 8.2997
Epoch 8/10
12/12 - 4s - loss: 982.7224 - loglik: -9.9139e+02 - logprior: 8.6715
Fitted a model with MAP estimate = -982.0727
Time for alignment: 89.5312
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1201.6686 - loglik: -1.1696e+03 - logprior: -3.2107e+01
Epoch 2/10
12/12 - 3s - loss: 1116.1238 - loglik: -1.1128e+03 - logprior: -3.3639e+00
Epoch 3/10
12/12 - 3s - loss: 1053.5084 - loglik: -1.0529e+03 - logprior: -5.7185e-01
Epoch 4/10
12/12 - 3s - loss: 1026.9205 - loglik: -1.0270e+03 - logprior: 0.0319
Epoch 5/10
12/12 - 3s - loss: 1022.5576 - loglik: -1.0231e+03 - logprior: 0.5775
Epoch 6/10
12/12 - 3s - loss: 1018.5845 - loglik: -1.0196e+03 - logprior: 1.0011
Epoch 7/10
12/12 - 3s - loss: 1019.0345 - loglik: -1.0201e+03 - logprior: 1.0989
Fitted a model with MAP estimate = -1017.0968
expansions: [(11, 3), (25, 1), (32, 2), (33, 1), (42, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (76, 3), (77, 2), (78, 1), (86, 1), (87, 1), (90, 1), (92, 1), (101, 1), (102, 1), (127, 5), (128, 1), (130, 2), (131, 2), (132, 1), (137, 2), (152, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1052.5496 - loglik: -1.0153e+03 - logprior: -3.7214e+01
Epoch 2/2
12/12 - 4s - loss: 1014.7380 - loglik: -1.0036e+03 - logprior: -1.1156e+01
Fitted a model with MAP estimate = -1007.0004
expansions: [(0, 4)]
discards: [  0 151 152 153 163 172 189 215]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1030.3091 - loglik: -1.0030e+03 - logprior: -2.7313e+01
Epoch 2/2
12/12 - 3s - loss: 996.7849 - loglik: -9.9544e+02 - logprior: -1.3495e+00
Fitted a model with MAP estimate = -993.1140
expansions: [(181, 3)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1022.4730 - loglik: -9.9596e+02 - logprior: -2.6517e+01
Epoch 2/10
12/12 - 4s - loss: 993.9045 - loglik: -9.9301e+02 - logprior: -8.9276e-01
Epoch 3/10
12/12 - 3s - loss: 990.9057 - loglik: -9.9480e+02 - logprior: 3.8925
Epoch 4/10
12/12 - 3s - loss: 985.2194 - loglik: -9.9116e+02 - logprior: 5.9442
Epoch 5/10
12/12 - 4s - loss: 990.7551 - loglik: -9.9778e+02 - logprior: 7.0276
Fitted a model with MAP estimate = -985.0350
Time for alignment: 74.3786
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 1199.0682 - loglik: -1.1670e+03 - logprior: -3.2103e+01
Epoch 2/10
12/12 - 3s - loss: 1116.8236 - loglik: -1.1135e+03 - logprior: -3.3732e+00
Epoch 3/10
12/12 - 3s - loss: 1054.4980 - loglik: -1.0537e+03 - logprior: -7.6334e-01
Epoch 4/10
12/12 - 3s - loss: 1027.6266 - loglik: -1.0275e+03 - logprior: -1.3090e-01
Epoch 5/10
12/12 - 3s - loss: 1018.1039 - loglik: -1.0185e+03 - logprior: 0.4448
Epoch 6/10
12/12 - 3s - loss: 1015.6351 - loglik: -1.0165e+03 - logprior: 0.8566
Epoch 7/10
12/12 - 3s - loss: 1020.1453 - loglik: -1.0212e+03 - logprior: 1.0894
Fitted a model with MAP estimate = -1015.9050
expansions: [(11, 3), (19, 1), (24, 1), (31, 2), (32, 1), (41, 1), (45, 1), (60, 2), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (90, 1), (92, 1), (101, 1), (102, 1), (127, 3), (129, 3), (139, 1), (146, 2), (149, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1051.2592 - loglik: -1.0140e+03 - logprior: -3.7270e+01
Epoch 2/2
12/12 - 4s - loss: 1013.5576 - loglik: -1.0023e+03 - logprior: -1.1221e+01
Fitted a model with MAP estimate = -1009.4064
expansions: [(0, 4), (109, 1)]
discards: [  0  70 153 183 212]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1027.5752 - loglik: -1.0002e+03 - logprior: -2.7327e+01
Epoch 2/2
12/12 - 4s - loss: 1000.5313 - loglik: -9.9930e+02 - logprior: -1.2332e+00
Fitted a model with MAP estimate = -993.7331
expansions: [(181, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1023.8377 - loglik: -9.9740e+02 - logprior: -2.6433e+01
Epoch 2/10
12/12 - 3s - loss: 992.7493 - loglik: -9.9204e+02 - logprior: -7.0569e-01
Epoch 3/10
12/12 - 3s - loss: 992.3802 - loglik: -9.9644e+02 - logprior: 4.0551
Epoch 4/10
12/12 - 3s - loss: 988.4631 - loglik: -9.9458e+02 - logprior: 6.1139
Epoch 5/10
12/12 - 3s - loss: 981.2020 - loglik: -9.8837e+02 - logprior: 7.1673
Epoch 6/10
12/12 - 3s - loss: 984.2184 - loglik: -9.9201e+02 - logprior: 7.7902
Fitted a model with MAP estimate = -984.1740
Time for alignment: 76.5650
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 1200.2653 - loglik: -1.1682e+03 - logprior: -3.2094e+01
Epoch 2/10
12/12 - 3s - loss: 1116.5300 - loglik: -1.1132e+03 - logprior: -3.3700e+00
Epoch 3/10
12/12 - 3s - loss: 1056.6342 - loglik: -1.0559e+03 - logprior: -7.4301e-01
Epoch 4/10
12/12 - 3s - loss: 1023.6838 - loglik: -1.0235e+03 - logprior: -2.1407e-01
Epoch 5/10
12/12 - 3s - loss: 1021.4812 - loglik: -1.0217e+03 - logprior: 0.2598
Epoch 6/10
12/12 - 3s - loss: 1010.6502 - loglik: -1.0113e+03 - logprior: 0.6498
Epoch 7/10
12/12 - 3s - loss: 1014.5140 - loglik: -1.0154e+03 - logprior: 0.8800
Fitted a model with MAP estimate = -1013.6369
expansions: [(11, 3), (21, 1), (31, 1), (32, 2), (40, 1), (42, 1), (45, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (74, 2), (76, 1), (77, 2), (86, 1), (87, 1), (90, 1), (92, 1), (101, 2), (102, 1), (127, 4), (129, 3), (130, 3), (137, 1), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1049.1097 - loglik: -1.0118e+03 - logprior: -3.7318e+01
Epoch 2/2
12/12 - 4s - loss: 1012.9958 - loglik: -1.0017e+03 - logprior: -1.1339e+01
Fitted a model with MAP estimate = -1005.0234
expansions: [(0, 4)]
discards: [  0  35  89 124 215]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1026.4427 - loglik: -9.9897e+02 - logprior: -2.7473e+01
Epoch 2/2
12/12 - 4s - loss: 999.2529 - loglik: -9.9788e+02 - logprior: -1.3754e+00
Fitted a model with MAP estimate = -992.9493
expansions: [(183, 3)]
discards: [  1   2   3 153 154 185]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1022.7871 - loglik: -9.9627e+02 - logprior: -2.6521e+01
Epoch 2/10
12/12 - 3s - loss: 997.1663 - loglik: -9.9634e+02 - logprior: -8.2802e-01
Epoch 3/10
12/12 - 4s - loss: 990.3967 - loglik: -9.9438e+02 - logprior: 3.9793
Epoch 4/10
12/12 - 3s - loss: 986.2252 - loglik: -9.9229e+02 - logprior: 6.0612
Epoch 5/10
12/12 - 3s - loss: 984.1345 - loglik: -9.9124e+02 - logprior: 7.1019
Epoch 6/10
12/12 - 3s - loss: 984.5835 - loglik: -9.9235e+02 - logprior: 7.7711
Fitted a model with MAP estimate = -984.4553
Time for alignment: 76.6174
Computed alignments with likelihoods: ['-987.2492', '-982.0727', '-985.0350', '-984.1740', '-984.4553']
Best model has likelihood: -982.0727  (prior= 8.9236 )
time for generating output: 0.2474
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.937270626614109
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1251.6740 - loglik: -1.2444e+03 - logprior: -7.2599e+00
Epoch 2/10
21/21 - 6s - loss: 1127.6870 - loglik: -1.1260e+03 - logprior: -1.7268e+00
Epoch 3/10
21/21 - 6s - loss: 1093.0259 - loglik: -1.0908e+03 - logprior: -2.1945e+00
Epoch 4/10
21/21 - 6s - loss: 1081.7240 - loglik: -1.0796e+03 - logprior: -2.0962e+00
Epoch 5/10
21/21 - 6s - loss: 1078.9244 - loglik: -1.0769e+03 - logprior: -2.0305e+00
Epoch 6/10
21/21 - 6s - loss: 1077.5154 - loglik: -1.0755e+03 - logprior: -2.0162e+00
Epoch 7/10
21/21 - 6s - loss: 1077.6959 - loglik: -1.0756e+03 - logprior: -2.0816e+00
Fitted a model with MAP estimate = -1077.3493
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (62, 1), (63, 3), (65, 1), (75, 5), (76, 2), (78, 2), (80, 1), (96, 1), (99, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (124, 1), (143, 1), (145, 1), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 1068.3467 - loglik: -1.0616e+03 - logprior: -6.7648e+00
Epoch 2/2
21/21 - 10s - loss: 1046.6901 - loglik: -1.0464e+03 - logprior: -2.8252e-01
Fitted a model with MAP estimate = -1042.5488
expansions: []
discards: [ 24  80 184 188 219]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 1053.7019 - loglik: -1.0474e+03 - logprior: -6.3400e+00
Epoch 2/2
21/21 - 9s - loss: 1041.7412 - loglik: -1.0419e+03 - logprior: 0.2049
Fitted a model with MAP estimate = -1041.9142
expansions: []
discards: [ 95  96  97  98 100]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1055.0360 - loglik: -1.0491e+03 - logprior: -5.9543e+00
Epoch 2/10
21/21 - 8s - loss: 1043.7646 - loglik: -1.0444e+03 - logprior: 0.6586
Epoch 3/10
21/21 - 8s - loss: 1043.0479 - loglik: -1.0446e+03 - logprior: 1.5075
Epoch 4/10
21/21 - 9s - loss: 1042.1963 - loglik: -1.0440e+03 - logprior: 1.8376
Epoch 5/10
21/21 - 8s - loss: 1039.1906 - loglik: -1.0412e+03 - logprior: 2.0147
Epoch 6/10
21/21 - 8s - loss: 1040.1897 - loglik: -1.0424e+03 - logprior: 2.1766
Fitted a model with MAP estimate = -1039.3636
Time for alignment: 172.0111
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1251.3864 - loglik: -1.2441e+03 - logprior: -7.2537e+00
Epoch 2/10
21/21 - 6s - loss: 1130.6038 - loglik: -1.1289e+03 - logprior: -1.6993e+00
Epoch 3/10
21/21 - 6s - loss: 1093.3258 - loglik: -1.0913e+03 - logprior: -2.0537e+00
Epoch 4/10
21/21 - 6s - loss: 1084.1176 - loglik: -1.0822e+03 - logprior: -1.9554e+00
Epoch 5/10
21/21 - 7s - loss: 1082.1030 - loglik: -1.0801e+03 - logprior: -1.9727e+00
Epoch 6/10
21/21 - 6s - loss: 1079.2841 - loglik: -1.0773e+03 - logprior: -1.9784e+00
Epoch 7/10
21/21 - 6s - loss: 1082.2869 - loglik: -1.0803e+03 - logprior: -1.9994e+00
Fitted a model with MAP estimate = -1080.1730
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (35, 1), (38, 2), (39, 2), (40, 1), (50, 1), (64, 3), (66, 1), (75, 1), (76, 4), (77, 2), (78, 2), (97, 1), (100, 2), (102, 1), (115, 1), (119, 2), (120, 1), (121, 1), (122, 1), (144, 1), (145, 1), (146, 1), (147, 2), (154, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 245 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1070.6888 - loglik: -1.0639e+03 - logprior: -6.7673e+00
Epoch 2/2
21/21 - 9s - loss: 1045.9955 - loglik: -1.0457e+03 - logprior: -2.6684e-01
Fitted a model with MAP estimate = -1042.0373
expansions: []
discards: [ 24  28  49 106 133 155 191 221]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1054.2859 - loglik: -1.0481e+03 - logprior: -6.1690e+00
Epoch 2/2
21/21 - 8s - loss: 1044.8928 - loglik: -1.0453e+03 - logprior: 0.3737
Fitted a model with MAP estimate = -1042.7131
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1053.5494 - loglik: -1.0476e+03 - logprior: -5.9039e+00
Epoch 2/10
21/21 - 9s - loss: 1043.3644 - loglik: -1.0441e+03 - logprior: 0.7364
Epoch 3/10
21/21 - 8s - loss: 1040.7208 - loglik: -1.0423e+03 - logprior: 1.5697
Epoch 4/10
21/21 - 9s - loss: 1039.7480 - loglik: -1.0417e+03 - logprior: 1.9155
Epoch 5/10
21/21 - 9s - loss: 1038.3043 - loglik: -1.0404e+03 - logprior: 2.1023
Epoch 6/10
21/21 - 8s - loss: 1038.2084 - loglik: -1.0405e+03 - logprior: 2.2668
Epoch 7/10
21/21 - 9s - loss: 1039.1718 - loglik: -1.0416e+03 - logprior: 2.4505
Fitted a model with MAP estimate = -1037.3263
Time for alignment: 181.7065
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1250.4556 - loglik: -1.2432e+03 - logprior: -7.2564e+00
Epoch 2/10
21/21 - 7s - loss: 1129.5713 - loglik: -1.1278e+03 - logprior: -1.7884e+00
Epoch 3/10
21/21 - 6s - loss: 1094.3378 - loglik: -1.0918e+03 - logprior: -2.5359e+00
Epoch 4/10
21/21 - 6s - loss: 1085.3177 - loglik: -1.0829e+03 - logprior: -2.3936e+00
Epoch 5/10
21/21 - 7s - loss: 1083.6096 - loglik: -1.0813e+03 - logprior: -2.3546e+00
Epoch 6/10
21/21 - 7s - loss: 1083.4495 - loglik: -1.0811e+03 - logprior: -2.3510e+00
Epoch 7/10
21/21 - 6s - loss: 1082.3059 - loglik: -1.0800e+03 - logprior: -2.3456e+00
Epoch 8/10
21/21 - 6s - loss: 1083.0977 - loglik: -1.0807e+03 - logprior: -2.3487e+00
Fitted a model with MAP estimate = -1081.9677
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (45, 1), (49, 1), (59, 1), (62, 3), (65, 1), (75, 5), (76, 1), (78, 2), (80, 1), (96, 1), (99, 2), (115, 1), (116, 1), (118, 1), (119, 1), (120, 1), (121, 1), (145, 2), (146, 1), (149, 1), (157, 1), (160, 1), (163, 1), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1079.1226 - loglik: -1.0693e+03 - logprior: -9.7855e+00
Epoch 2/2
21/21 - 9s - loss: 1051.9971 - loglik: -1.0488e+03 - logprior: -3.2131e+00
Fitted a model with MAP estimate = -1048.4567
expansions: [(0, 5)]
discards: [  0  13  18  28  50  81  86 133 219]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1060.7925 - loglik: -1.0542e+03 - logprior: -6.5535e+00
Epoch 2/2
21/21 - 9s - loss: 1047.9791 - loglik: -1.0479e+03 - logprior: -1.1425e-01
Fitted a model with MAP estimate = -1045.9716
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1057.2784 - loglik: -1.0512e+03 - logprior: -6.1021e+00
Epoch 2/10
21/21 - 9s - loss: 1046.9388 - loglik: -1.0475e+03 - logprior: 0.5509
Epoch 3/10
21/21 - 8s - loss: 1044.2742 - loglik: -1.0457e+03 - logprior: 1.3914
Epoch 4/10
21/21 - 9s - loss: 1045.4486 - loglik: -1.0472e+03 - logprior: 1.7545
Fitted a model with MAP estimate = -1043.5025
Time for alignment: 164.4350
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1251.1041 - loglik: -1.2438e+03 - logprior: -7.3035e+00
Epoch 2/10
21/21 - 6s - loss: 1123.7244 - loglik: -1.1219e+03 - logprior: -1.8650e+00
Epoch 3/10
21/21 - 6s - loss: 1091.0928 - loglik: -1.0890e+03 - logprior: -2.0646e+00
Epoch 4/10
21/21 - 7s - loss: 1082.7068 - loglik: -1.0808e+03 - logprior: -1.8859e+00
Epoch 5/10
21/21 - 6s - loss: 1081.2529 - loglik: -1.0794e+03 - logprior: -1.8298e+00
Epoch 6/10
21/21 - 7s - loss: 1079.1881 - loglik: -1.0773e+03 - logprior: -1.8989e+00
Epoch 7/10
21/21 - 7s - loss: 1079.5231 - loglik: -1.0775e+03 - logprior: -1.9922e+00
Fitted a model with MAP estimate = -1078.8066
expansions: [(13, 1), (16, 2), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (64, 2), (66, 1), (76, 7), (78, 1), (97, 1), (98, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (145, 2), (147, 1), (149, 1), (157, 1), (160, 1), (162, 2), (163, 1), (164, 1), (169, 3), (171, 1), (178, 1), (181, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1072.8108 - loglik: -1.0660e+03 - logprior: -6.8433e+00
Epoch 2/2
21/21 - 9s - loss: 1049.1954 - loglik: -1.0488e+03 - logprior: -4.1165e-01
Fitted a model with MAP estimate = -1046.2437
expansions: [(81, 1)]
discards: [ 13  17  28  50 149 178 216]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1059.6702 - loglik: -1.0532e+03 - logprior: -6.4741e+00
Epoch 2/2
21/21 - 8s - loss: 1047.8385 - loglik: -1.0480e+03 - logprior: 0.1477
Fitted a model with MAP estimate = -1045.8259
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1055.9199 - loglik: -1.0497e+03 - logprior: -6.1873e+00
Epoch 2/10
21/21 - 9s - loss: 1045.1298 - loglik: -1.0456e+03 - logprior: 0.5024
Epoch 3/10
21/21 - 8s - loss: 1046.6663 - loglik: -1.0480e+03 - logprior: 1.3165
Fitted a model with MAP estimate = -1043.4173
Time for alignment: 145.1618
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1249.3523 - loglik: -1.2421e+03 - logprior: -7.2749e+00
Epoch 2/10
21/21 - 7s - loss: 1127.7198 - loglik: -1.1261e+03 - logprior: -1.6235e+00
Epoch 3/10
21/21 - 6s - loss: 1082.6504 - loglik: -1.0809e+03 - logprior: -1.7007e+00
Epoch 4/10
21/21 - 6s - loss: 1075.4144 - loglik: -1.0738e+03 - logprior: -1.5866e+00
Epoch 5/10
21/21 - 7s - loss: 1074.0706 - loglik: -1.0725e+03 - logprior: -1.5600e+00
Epoch 6/10
21/21 - 6s - loss: 1072.1443 - loglik: -1.0706e+03 - logprior: -1.5886e+00
Epoch 7/10
21/21 - 6s - loss: 1071.9717 - loglik: -1.0704e+03 - logprior: -1.6055e+00
Epoch 8/10
21/21 - 7s - loss: 1073.0111 - loglik: -1.0714e+03 - logprior: -1.6160e+00
Fitted a model with MAP estimate = -1071.2681
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 3), (22, 1), (37, 1), (38, 2), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (75, 8), (97, 1), (100, 2), (116, 1), (119, 1), (121, 4), (143, 1), (145, 2), (146, 1), (148, 1), (153, 1), (159, 1), (162, 2), (163, 1), (164, 1), (167, 1), (168, 2), (171, 1), (178, 1), (181, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 1061.1174 - loglik: -1.0543e+03 - logprior: -6.7734e+00
Epoch 2/2
21/21 - 9s - loss: 1036.4347 - loglik: -1.0360e+03 - logprior: -4.3355e-01
Fitted a model with MAP estimate = -1033.2271
expansions: []
discards: [ 24  28  49  82  87 133 157 219]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1046.2753 - loglik: -1.0400e+03 - logprior: -6.2265e+00
Epoch 2/2
21/21 - 8s - loss: 1034.4993 - loglik: -1.0348e+03 - logprior: 0.3356
Fitted a model with MAP estimate = -1033.3940
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1043.2437 - loglik: -1.0374e+03 - logprior: -5.8934e+00
Epoch 2/10
21/21 - 8s - loss: 1034.1617 - loglik: -1.0349e+03 - logprior: 0.7305
Epoch 3/10
21/21 - 9s - loss: 1032.0120 - loglik: -1.0336e+03 - logprior: 1.6115
Epoch 4/10
21/21 - 9s - loss: 1031.4615 - loglik: -1.0334e+03 - logprior: 1.9197
Epoch 5/10
21/21 - 8s - loss: 1029.1948 - loglik: -1.0313e+03 - logprior: 2.1136
Epoch 6/10
21/21 - 9s - loss: 1030.2252 - loglik: -1.0325e+03 - logprior: 2.2682
Fitted a model with MAP estimate = -1029.1936
Time for alignment: 177.5669
Computed alignments with likelihoods: ['-1039.3636', '-1037.3263', '-1043.5025', '-1043.4173', '-1029.1936']
Best model has likelihood: -1029.1936  (prior= 2.3694 )
time for generating output: 0.3721
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5723969120692901
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 570.2013 - loglik: -5.6721e+02 - logprior: -2.9934e+00
Epoch 2/10
19/19 - 2s - loss: 539.7534 - loglik: -5.3891e+02 - logprior: -8.4351e-01
Epoch 3/10
19/19 - 2s - loss: 530.3342 - loglik: -5.2951e+02 - logprior: -8.2092e-01
Epoch 4/10
19/19 - 2s - loss: 527.7210 - loglik: -5.2698e+02 - logprior: -7.3733e-01
Epoch 5/10
19/19 - 2s - loss: 527.3087 - loglik: -5.2659e+02 - logprior: -7.2201e-01
Epoch 6/10
19/19 - 2s - loss: 525.4063 - loglik: -5.2469e+02 - logprior: -7.2042e-01
Epoch 7/10
19/19 - 2s - loss: 525.5024 - loglik: -5.2480e+02 - logprior: -7.0488e-01
Fitted a model with MAP estimate = -523.0928
expansions: [(0, 10), (16, 3), (17, 1), (18, 2), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 532.8538 - loglik: -5.2914e+02 - logprior: -3.7110e+00
Epoch 2/2
19/19 - 3s - loss: 524.7335 - loglik: -5.2365e+02 - logprior: -1.0797e+00
Fitted a model with MAP estimate = -521.9154
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 26 27 34 78 80]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 530.8085 - loglik: -5.2705e+02 - logprior: -3.7609e+00
Epoch 2/2
19/19 - 3s - loss: 525.9069 - loglik: -5.2471e+02 - logprior: -1.1956e+00
Fitted a model with MAP estimate = -522.6920
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  8 71 74]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 526.7889 - loglik: -5.2365e+02 - logprior: -3.1429e+00
Epoch 2/10
19/19 - 2s - loss: 523.1349 - loglik: -5.2221e+02 - logprior: -9.2442e-01
Epoch 3/10
19/19 - 3s - loss: 521.6587 - loglik: -5.2090e+02 - logprior: -7.5710e-01
Epoch 4/10
19/19 - 3s - loss: 520.5765 - loglik: -5.1988e+02 - logprior: -6.9587e-01
Epoch 5/10
19/19 - 3s - loss: 519.9035 - loglik: -5.1925e+02 - logprior: -6.5284e-01
Epoch 6/10
19/19 - 2s - loss: 518.3026 - loglik: -5.1765e+02 - logprior: -6.5017e-01
Epoch 7/10
19/19 - 3s - loss: 519.1387 - loglik: -5.1850e+02 - logprior: -6.3573e-01
Fitted a model with MAP estimate = -517.8723
Time for alignment: 73.7821
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 570.0518 - loglik: -5.6705e+02 - logprior: -2.9986e+00
Epoch 2/10
19/19 - 2s - loss: 538.6226 - loglik: -5.3778e+02 - logprior: -8.4554e-01
Epoch 3/10
19/19 - 2s - loss: 530.1953 - loglik: -5.2937e+02 - logprior: -8.2367e-01
Epoch 4/10
19/19 - 2s - loss: 527.9662 - loglik: -5.2722e+02 - logprior: -7.4757e-01
Epoch 5/10
19/19 - 2s - loss: 526.1865 - loglik: -5.2546e+02 - logprior: -7.2563e-01
Epoch 6/10
19/19 - 2s - loss: 526.1904 - loglik: -5.2548e+02 - logprior: -7.1387e-01
Fitted a model with MAP estimate = -523.2944
expansions: [(0, 10), (12, 3), (13, 1), (14, 2), (22, 1), (57, 2), (58, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 531.4426 - loglik: -5.2775e+02 - logprior: -3.6914e+00
Epoch 2/2
19/19 - 3s - loss: 525.3829 - loglik: -5.2434e+02 - logprior: -1.0383e+00
Fitted a model with MAP estimate = -521.7876
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 28 77 78 80 82]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 529.6580 - loglik: -5.2597e+02 - logprior: -3.6855e+00
Epoch 2/2
19/19 - 3s - loss: 525.4614 - loglik: -5.2432e+02 - logprior: -1.1457e+00
Fitted a model with MAP estimate = -521.9716
expansions: [(0, 7), (20, 2)]
discards: [ 0  1  2  3  4  8 27 76]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 525.4501 - loglik: -5.2234e+02 - logprior: -3.1058e+00
Epoch 2/10
19/19 - 3s - loss: 521.9162 - loglik: -5.2102e+02 - logprior: -8.9626e-01
Epoch 3/10
19/19 - 3s - loss: 521.0695 - loglik: -5.2033e+02 - logprior: -7.4440e-01
Epoch 4/10
19/19 - 3s - loss: 519.9572 - loglik: -5.1927e+02 - logprior: -6.8241e-01
Epoch 5/10
19/19 - 3s - loss: 519.1479 - loglik: -5.1850e+02 - logprior: -6.5288e-01
Epoch 6/10
19/19 - 3s - loss: 517.8915 - loglik: -5.1726e+02 - logprior: -6.2965e-01
Epoch 7/10
19/19 - 3s - loss: 516.9790 - loglik: -5.1637e+02 - logprior: -6.0908e-01
Epoch 8/10
19/19 - 3s - loss: 517.8843 - loglik: -5.1730e+02 - logprior: -5.8610e-01
Fitted a model with MAP estimate = -516.5622
Time for alignment: 74.3002
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 570.4623 - loglik: -5.6746e+02 - logprior: -3.0065e+00
Epoch 2/10
19/19 - 2s - loss: 538.7358 - loglik: -5.3789e+02 - logprior: -8.4961e-01
Epoch 3/10
19/19 - 2s - loss: 529.6705 - loglik: -5.2882e+02 - logprior: -8.4723e-01
Epoch 4/10
19/19 - 2s - loss: 528.5622 - loglik: -5.2781e+02 - logprior: -7.5229e-01
Epoch 5/10
19/19 - 2s - loss: 527.1746 - loglik: -5.2645e+02 - logprior: -7.2552e-01
Epoch 6/10
19/19 - 2s - loss: 526.6614 - loglik: -5.2594e+02 - logprior: -7.2047e-01
Epoch 7/10
19/19 - 2s - loss: 525.1144 - loglik: -5.2440e+02 - logprior: -7.1327e-01
Epoch 8/10
19/19 - 2s - loss: 525.9682 - loglik: -5.2525e+02 - logprior: -7.1478e-01
Fitted a model with MAP estimate = -522.9756
expansions: [(0, 10), (16, 4), (19, 1), (55, 1), (57, 2), (58, 1), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 532.6555 - loglik: -5.2890e+02 - logprior: -3.7574e+00
Epoch 2/2
19/19 - 3s - loss: 524.8708 - loglik: -5.2379e+02 - logprior: -1.0854e+00
Fitted a model with MAP estimate = -521.2865
expansions: [(0, 8), (27, 1), (28, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 75 78 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 528.6735 - loglik: -5.2498e+02 - logprior: -3.6917e+00
Epoch 2/2
19/19 - 3s - loss: 524.1370 - loglik: -5.2300e+02 - logprior: -1.1353e+00
Fitted a model with MAP estimate = -521.2012
expansions: [(0, 7)]
discards: [0 1 2 3 4 8]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 524.6375 - loglik: -5.2151e+02 - logprior: -3.1276e+00
Epoch 2/10
19/19 - 3s - loss: 521.7554 - loglik: -5.2087e+02 - logprior: -8.9004e-01
Epoch 3/10
19/19 - 3s - loss: 520.2891 - loglik: -5.1956e+02 - logprior: -7.2561e-01
Epoch 4/10
19/19 - 3s - loss: 520.2233 - loglik: -5.1955e+02 - logprior: -6.7622e-01
Epoch 5/10
19/19 - 3s - loss: 518.4196 - loglik: -5.1778e+02 - logprior: -6.3999e-01
Epoch 6/10
19/19 - 3s - loss: 517.6541 - loglik: -5.1703e+02 - logprior: -6.2757e-01
Epoch 7/10
19/19 - 3s - loss: 516.8558 - loglik: -5.1625e+02 - logprior: -6.0976e-01
Epoch 8/10
19/19 - 3s - loss: 517.2125 - loglik: -5.1662e+02 - logprior: -5.9601e-01
Fitted a model with MAP estimate = -516.3147
Time for alignment: 78.7913
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 569.8118 - loglik: -5.6682e+02 - logprior: -2.9902e+00
Epoch 2/10
19/19 - 2s - loss: 539.6857 - loglik: -5.3884e+02 - logprior: -8.4722e-01
Epoch 3/10
19/19 - 2s - loss: 530.4296 - loglik: -5.2958e+02 - logprior: -8.4595e-01
Epoch 4/10
19/19 - 2s - loss: 528.4211 - loglik: -5.2767e+02 - logprior: -7.5462e-01
Epoch 5/10
19/19 - 2s - loss: 527.2922 - loglik: -5.2656e+02 - logprior: -7.3723e-01
Epoch 6/10
19/19 - 2s - loss: 526.0425 - loglik: -5.2531e+02 - logprior: -7.3256e-01
Epoch 7/10
19/19 - 2s - loss: 525.6948 - loglik: -5.2497e+02 - logprior: -7.2660e-01
Epoch 8/10
19/19 - 2s - loss: 525.8117 - loglik: -5.2509e+02 - logprior: -7.2370e-01
Fitted a model with MAP estimate = -523.2079
expansions: [(0, 9), (16, 2), (20, 2), (22, 1), (57, 2), (58, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 533.3853 - loglik: -5.2953e+02 - logprior: -3.8508e+00
Epoch 2/2
19/19 - 3s - loss: 525.4751 - loglik: -5.2438e+02 - logprior: -1.0974e+00
Fitted a model with MAP estimate = -522.4796
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  5  6  7  8 74 75 77 79]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 529.0454 - loglik: -5.2599e+02 - logprior: -3.0586e+00
Epoch 2/2
19/19 - 3s - loss: 525.3953 - loglik: -5.2451e+02 - logprior: -8.8939e-01
Fitted a model with MAP estimate = -522.2283
expansions: [(0, 8), (22, 2)]
discards: [ 1  2  3  4  5  6 73]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 526.1911 - loglik: -5.2281e+02 - logprior: -3.3861e+00
Epoch 2/10
19/19 - 3s - loss: 522.0286 - loglik: -5.2099e+02 - logprior: -1.0391e+00
Epoch 3/10
19/19 - 3s - loss: 521.7114 - loglik: -5.2086e+02 - logprior: -8.4975e-01
Epoch 4/10
19/19 - 3s - loss: 520.1776 - loglik: -5.1942e+02 - logprior: -7.5965e-01
Epoch 5/10
19/19 - 3s - loss: 519.1984 - loglik: -5.1848e+02 - logprior: -7.2108e-01
Epoch 6/10
19/19 - 3s - loss: 518.9673 - loglik: -5.1826e+02 - logprior: -7.0353e-01
Epoch 7/10
19/19 - 3s - loss: 517.6352 - loglik: -5.1693e+02 - logprior: -7.0954e-01
Epoch 8/10
19/19 - 3s - loss: 517.6570 - loglik: -5.1697e+02 - logprior: -6.8993e-01
Fitted a model with MAP estimate = -517.2566
Time for alignment: 78.5109
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 570.5809 - loglik: -5.6759e+02 - logprior: -2.9958e+00
Epoch 2/10
19/19 - 2s - loss: 540.9001 - loglik: -5.4006e+02 - logprior: -8.4183e-01
Epoch 3/10
19/19 - 2s - loss: 530.5184 - loglik: -5.2967e+02 - logprior: -8.4355e-01
Epoch 4/10
19/19 - 2s - loss: 529.1252 - loglik: -5.2838e+02 - logprior: -7.4699e-01
Epoch 5/10
19/19 - 2s - loss: 527.3404 - loglik: -5.2660e+02 - logprior: -7.3863e-01
Epoch 6/10
19/19 - 2s - loss: 527.0978 - loglik: -5.2636e+02 - logprior: -7.3873e-01
Epoch 7/10
19/19 - 2s - loss: 526.2068 - loglik: -5.2547e+02 - logprior: -7.3981e-01
Epoch 8/10
19/19 - 2s - loss: 524.9919 - loglik: -5.2424e+02 - logprior: -7.5247e-01
Epoch 9/10
19/19 - 2s - loss: 525.4414 - loglik: -5.2469e+02 - logprior: -7.5597e-01
Fitted a model with MAP estimate = -522.9098
expansions: [(0, 9), (12, 1), (16, 2), (20, 1), (21, 3), (22, 1), (23, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 533.3882 - loglik: -5.2962e+02 - logprior: -3.7700e+00
Epoch 2/2
19/19 - 3s - loss: 525.1167 - loglik: -5.2403e+02 - logprior: -1.0909e+00
Fitted a model with MAP estimate = -520.9307
expansions: [(0, 8), (35, 1)]
discards: [ 0  1  2  3  4  5 26 81]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 527.6088 - loglik: -5.2452e+02 - logprior: -3.0844e+00
Epoch 2/2
19/19 - 3s - loss: 523.0158 - loglik: -5.2206e+02 - logprior: -9.5985e-01
Fitted a model with MAP estimate = -520.1277
expansions: [(0, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 10 78 85 86]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 526.1978 - loglik: -5.2279e+02 - logprior: -3.4069e+00
Epoch 2/10
19/19 - 3s - loss: 522.1231 - loglik: -5.2118e+02 - logprior: -9.4645e-01
Epoch 3/10
19/19 - 3s - loss: 520.6199 - loglik: -5.1979e+02 - logprior: -8.3401e-01
Epoch 4/10
19/19 - 2s - loss: 519.8500 - loglik: -5.1911e+02 - logprior: -7.4022e-01
Epoch 5/10
19/19 - 3s - loss: 519.0390 - loglik: -5.1838e+02 - logprior: -6.5644e-01
Epoch 6/10
19/19 - 3s - loss: 518.0128 - loglik: -5.1739e+02 - logprior: -6.2134e-01
Epoch 7/10
19/19 - 3s - loss: 517.3317 - loglik: -5.1672e+02 - logprior: -6.0737e-01
Epoch 8/10
19/19 - 3s - loss: 516.9790 - loglik: -5.1639e+02 - logprior: -5.9129e-01
Epoch 9/10
19/19 - 3s - loss: 517.7059 - loglik: -5.1712e+02 - logprior: -5.8747e-01
Fitted a model with MAP estimate = -516.5961
Time for alignment: 83.8369
Computed alignments with likelihoods: ['-517.8723', '-516.5622', '-516.3147', '-517.2566', '-516.5961']
Best model has likelihood: -516.3147  (prior= -0.5823 )
time for generating output: 0.1936
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.5256842105263158
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 1509.7610 - loglik: -1.4979e+03 - logprior: -1.1864e+01
Epoch 2/10
17/17 - 7s - loss: 1333.1326 - loglik: -1.3313e+03 - logprior: -1.8063e+00
Epoch 3/10
17/17 - 6s - loss: 1233.5591 - loglik: -1.2301e+03 - logprior: -3.4876e+00
Epoch 4/10
17/17 - 7s - loss: 1215.1790 - loglik: -1.2115e+03 - logprior: -3.6885e+00
Epoch 5/10
17/17 - 7s - loss: 1210.2964 - loglik: -1.2070e+03 - logprior: -3.2939e+00
Epoch 6/10
17/17 - 6s - loss: 1209.7156 - loglik: -1.2066e+03 - logprior: -3.1009e+00
Epoch 7/10
17/17 - 6s - loss: 1207.1150 - loglik: -1.2040e+03 - logprior: -3.0881e+00
Epoch 8/10
17/17 - 7s - loss: 1202.8524 - loglik: -1.1998e+03 - logprior: -3.0895e+00
Epoch 9/10
17/17 - 7s - loss: 1206.2292 - loglik: -1.2032e+03 - logprior: -3.0684e+00
Fitted a model with MAP estimate = -1205.8425
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (46, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (71, 2), (86, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 1), (140, 1), (141, 1), (155, 1), (160, 1), (161, 1), (162, 1), (172, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1207.4130 - loglik: -1.1917e+03 - logprior: -1.5681e+01
Epoch 2/2
17/17 - 9s - loss: 1172.0895 - loglik: -1.1676e+03 - logprior: -4.4512e+00
Fitted a model with MAP estimate = -1166.3079
expansions: [(0, 9)]
discards: [  0  86 141 156]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1177.1996 - loglik: -1.1666e+03 - logprior: -1.0644e+01
Epoch 2/2
17/17 - 9s - loss: 1161.5459 - loglik: -1.1621e+03 - logprior: 0.5192
Fitted a model with MAP estimate = -1158.1783
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 1172.3572 - loglik: -1.1625e+03 - logprior: -9.8132e+00
Epoch 2/10
17/17 - 8s - loss: 1162.2314 - loglik: -1.1636e+03 - logprior: 1.3741
Epoch 3/10
17/17 - 8s - loss: 1155.1210 - loglik: -1.1582e+03 - logprior: 3.0381
Epoch 4/10
17/17 - 8s - loss: 1155.5150 - loglik: -1.1594e+03 - logprior: 3.8416
Fitted a model with MAP estimate = -1153.5025
Time for alignment: 169.9978
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1510.7606 - loglik: -1.4989e+03 - logprior: -1.1866e+01
Epoch 2/10
17/17 - 7s - loss: 1334.5410 - loglik: -1.3328e+03 - logprior: -1.7459e+00
Epoch 3/10
17/17 - 7s - loss: 1234.7416 - loglik: -1.2315e+03 - logprior: -3.2580e+00
Epoch 4/10
17/17 - 7s - loss: 1215.0063 - loglik: -1.2113e+03 - logprior: -3.6744e+00
Epoch 5/10
17/17 - 7s - loss: 1210.0969 - loglik: -1.2067e+03 - logprior: -3.3573e+00
Epoch 6/10
17/17 - 6s - loss: 1207.4436 - loglik: -1.2043e+03 - logprior: -3.0946e+00
Epoch 7/10
17/17 - 7s - loss: 1207.6912 - loglik: -1.2046e+03 - logprior: -3.0536e+00
Fitted a model with MAP estimate = -1206.3760
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (110, 1), (114, 1), (115, 2), (116, 1), (123, 1), (130, 1), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (163, 1), (173, 1), (179, 1), (181, 2), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 1208.3035 - loglik: -1.1926e+03 - logprior: -1.5665e+01
Epoch 2/2
17/17 - 9s - loss: 1168.3746 - loglik: -1.1641e+03 - logprior: -4.2319e+00
Fitted a model with MAP estimate = -1165.9859
expansions: [(0, 9)]
discards: [  0 137]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1176.3301 - loglik: -1.1656e+03 - logprior: -1.0680e+01
Epoch 2/2
17/17 - 9s - loss: 1160.4182 - loglik: -1.1609e+03 - logprior: 0.4908
Fitted a model with MAP estimate = -1158.1158
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1173.3661 - loglik: -1.1635e+03 - logprior: -9.8752e+00
Epoch 2/10
17/17 - 8s - loss: 1159.6107 - loglik: -1.1609e+03 - logprior: 1.2900
Epoch 3/10
17/17 - 9s - loss: 1156.6481 - loglik: -1.1596e+03 - logprior: 2.9562
Epoch 4/10
17/17 - 8s - loss: 1155.4736 - loglik: -1.1592e+03 - logprior: 3.7575
Epoch 5/10
17/17 - 9s - loss: 1151.7354 - loglik: -1.1558e+03 - logprior: 4.0854
Epoch 6/10
17/17 - 9s - loss: 1152.3088 - loglik: -1.1566e+03 - logprior: 4.2795
Fitted a model with MAP estimate = -1151.1647
Time for alignment: 170.3362
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 1509.8506 - loglik: -1.4980e+03 - logprior: -1.1861e+01
Epoch 2/10
17/17 - 6s - loss: 1334.1368 - loglik: -1.3324e+03 - logprior: -1.7410e+00
Epoch 3/10
17/17 - 6s - loss: 1237.5520 - loglik: -1.2344e+03 - logprior: -3.1305e+00
Epoch 4/10
17/17 - 7s - loss: 1218.3445 - loglik: -1.2147e+03 - logprior: -3.6635e+00
Epoch 5/10
17/17 - 6s - loss: 1208.4189 - loglik: -1.2050e+03 - logprior: -3.3964e+00
Epoch 6/10
17/17 - 7s - loss: 1208.1493 - loglik: -1.2050e+03 - logprior: -3.1330e+00
Epoch 7/10
17/17 - 6s - loss: 1210.1749 - loglik: -1.2071e+03 - logprior: -3.1164e+00
Fitted a model with MAP estimate = -1207.2861
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (28, 1), (33, 1), (44, 1), (45, 1), (47, 1), (56, 1), (57, 1), (58, 1), (59, 1), (63, 1), (69, 1), (86, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (161, 1), (162, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (201, 3), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1209.0870 - loglik: -1.1934e+03 - logprior: -1.5735e+01
Epoch 2/2
17/17 - 9s - loss: 1169.5669 - loglik: -1.1650e+03 - logprior: -4.5854e+00
Fitted a model with MAP estimate = -1165.8497
expansions: [(0, 9)]
discards: [  0 140 155 245]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1177.3706 - loglik: -1.1666e+03 - logprior: -1.0773e+01
Epoch 2/2
17/17 - 9s - loss: 1159.1498 - loglik: -1.1596e+03 - logprior: 0.4006
Fitted a model with MAP estimate = -1157.9023
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1172.8975 - loglik: -1.1630e+03 - logprior: -9.8478e+00
Epoch 2/10
17/17 - 8s - loss: 1158.9703 - loglik: -1.1603e+03 - logprior: 1.3161
Epoch 3/10
17/17 - 9s - loss: 1157.5509 - loglik: -1.1606e+03 - logprior: 3.0094
Epoch 4/10
17/17 - 9s - loss: 1155.8735 - loglik: -1.1597e+03 - logprior: 3.7824
Epoch 5/10
17/17 - 8s - loss: 1152.5168 - loglik: -1.1567e+03 - logprior: 4.1373
Epoch 6/10
17/17 - 8s - loss: 1151.6824 - loglik: -1.1560e+03 - logprior: 4.3265
Epoch 7/10
17/17 - 9s - loss: 1151.5060 - loglik: -1.1561e+03 - logprior: 4.5714
Epoch 8/10
17/17 - 8s - loss: 1149.8392 - loglik: -1.1547e+03 - logprior: 4.8529
Epoch 9/10
17/17 - 9s - loss: 1150.8618 - loglik: -1.1560e+03 - logprior: 5.1380
Fitted a model with MAP estimate = -1149.6813
Time for alignment: 196.6103
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1509.6078 - loglik: -1.4977e+03 - logprior: -1.1860e+01
Epoch 2/10
17/17 - 7s - loss: 1332.3773 - loglik: -1.3306e+03 - logprior: -1.7951e+00
Epoch 3/10
17/17 - 7s - loss: 1236.1794 - loglik: -1.2329e+03 - logprior: -3.2901e+00
Epoch 4/10
17/17 - 7s - loss: 1216.8605 - loglik: -1.2133e+03 - logprior: -3.5973e+00
Epoch 5/10
17/17 - 6s - loss: 1213.2295 - loglik: -1.2099e+03 - logprior: -3.3218e+00
Epoch 6/10
17/17 - 6s - loss: 1211.3757 - loglik: -1.2082e+03 - logprior: -3.1832e+00
Epoch 7/10
17/17 - 7s - loss: 1208.4985 - loglik: -1.2053e+03 - logprior: -3.1743e+00
Epoch 8/10
17/17 - 6s - loss: 1209.6317 - loglik: -1.2065e+03 - logprior: -3.1559e+00
Fitted a model with MAP estimate = -1207.4909
expansions: [(8, 1), (9, 1), (12, 1), (14, 1), (23, 1), (24, 1), (26, 1), (32, 1), (43, 1), (46, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 2), (117, 1), (129, 2), (131, 1), (142, 1), (151, 1), (160, 1), (161, 1), (162, 1), (163, 1), (176, 1), (179, 1), (180, 1), (181, 1), (183, 1), (188, 1), (191, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1208.2533 - loglik: -1.1926e+03 - logprior: -1.5651e+01
Epoch 2/2
17/17 - 8s - loss: 1171.3816 - loglik: -1.1670e+03 - logprior: -4.3873e+00
Fitted a model with MAP estimate = -1166.0082
expansions: [(0, 9)]
discards: [  0 139 155]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1175.9496 - loglik: -1.1653e+03 - logprior: -1.0620e+01
Epoch 2/2
17/17 - 9s - loss: 1161.6483 - loglik: -1.1621e+03 - logprior: 0.4892
Fitted a model with MAP estimate = -1158.3994
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1174.1400 - loglik: -1.1642e+03 - logprior: -9.9396e+00
Epoch 2/10
17/17 - 9s - loss: 1159.4492 - loglik: -1.1607e+03 - logprior: 1.2464
Epoch 3/10
17/17 - 9s - loss: 1157.2312 - loglik: -1.1602e+03 - logprior: 2.9261
Epoch 4/10
17/17 - 8s - loss: 1155.3174 - loglik: -1.1591e+03 - logprior: 3.7746
Epoch 5/10
17/17 - 9s - loss: 1152.8485 - loglik: -1.1570e+03 - logprior: 4.1058
Epoch 6/10
17/17 - 9s - loss: 1153.9714 - loglik: -1.1583e+03 - logprior: 4.2972
Fitted a model with MAP estimate = -1151.3764
Time for alignment: 177.7731
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1510.9470 - loglik: -1.4991e+03 - logprior: -1.1866e+01
Epoch 2/10
17/17 - 6s - loss: 1326.2733 - loglik: -1.3246e+03 - logprior: -1.6906e+00
Epoch 3/10
17/17 - 6s - loss: 1231.1648 - loglik: -1.2282e+03 - logprior: -2.9427e+00
Epoch 4/10
17/17 - 7s - loss: 1216.4274 - loglik: -1.2132e+03 - logprior: -3.2166e+00
Epoch 5/10
17/17 - 7s - loss: 1210.1240 - loglik: -1.2072e+03 - logprior: -2.9674e+00
Epoch 6/10
17/17 - 7s - loss: 1205.1826 - loglik: -1.2024e+03 - logprior: -2.7641e+00
Epoch 7/10
17/17 - 6s - loss: 1211.6615 - loglik: -1.2089e+03 - logprior: -2.7421e+00
Fitted a model with MAP estimate = -1207.3677
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (55, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (87, 4), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (142, 1), (151, 1), (160, 1), (161, 1), (162, 1), (163, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1207.0225 - loglik: -1.1914e+03 - logprior: -1.5658e+01
Epoch 2/2
17/17 - 9s - loss: 1170.5651 - loglik: -1.1662e+03 - logprior: -4.4040e+00
Fitted a model with MAP estimate = -1165.2373
expansions: [(0, 9)]
discards: [  0 103 104 137 157]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1177.7140 - loglik: -1.1670e+03 - logprior: -1.0671e+01
Epoch 2/2
17/17 - 9s - loss: 1158.8646 - loglik: -1.1593e+03 - logprior: 0.4343
Fitted a model with MAP estimate = -1157.8267
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1174.1127 - loglik: -1.1642e+03 - logprior: -9.9455e+00
Epoch 2/10
17/17 - 9s - loss: 1159.0291 - loglik: -1.1603e+03 - logprior: 1.2443
Epoch 3/10
17/17 - 8s - loss: 1157.1061 - loglik: -1.1600e+03 - logprior: 2.9427
Epoch 4/10
17/17 - 9s - loss: 1154.0568 - loglik: -1.1578e+03 - logprior: 3.7508
Epoch 5/10
17/17 - 8s - loss: 1152.2719 - loglik: -1.1564e+03 - logprior: 4.0843
Epoch 6/10
17/17 - 9s - loss: 1147.8298 - loglik: -1.1521e+03 - logprior: 4.2726
Epoch 7/10
17/17 - 9s - loss: 1156.0049 - loglik: -1.1604e+03 - logprior: 4.4084
Fitted a model with MAP estimate = -1150.6605
Time for alignment: 179.7811
Computed alignments with likelihoods: ['-1153.5025', '-1151.1647', '-1149.6813', '-1151.3764', '-1150.6605']
Best model has likelihood: -1149.6813  (prior= 5.3134 )
time for generating output: 0.2682
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.956721562619686
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 346.3151 - loglik: -3.2630e+02 - logprior: -2.0017e+01
Epoch 2/10
10/10 - 1s - loss: 314.8101 - loglik: -3.0909e+02 - logprior: -5.7209e+00
Epoch 3/10
10/10 - 1s - loss: 298.1332 - loglik: -2.9488e+02 - logprior: -3.2564e+00
Epoch 4/10
10/10 - 0s - loss: 286.4579 - loglik: -2.8377e+02 - logprior: -2.6907e+00
Epoch 5/10
10/10 - 0s - loss: 280.8608 - loglik: -2.7824e+02 - logprior: -2.6198e+00
Epoch 6/10
10/10 - 1s - loss: 279.6776 - loglik: -2.7713e+02 - logprior: -2.5482e+00
Epoch 7/10
10/10 - 0s - loss: 278.2576 - loglik: -2.7581e+02 - logprior: -2.4521e+00
Epoch 8/10
10/10 - 0s - loss: 277.7529 - loglik: -2.7539e+02 - logprior: -2.3643e+00
Epoch 9/10
10/10 - 0s - loss: 277.5437 - loglik: -2.7524e+02 - logprior: -2.3078e+00
Epoch 10/10
10/10 - 0s - loss: 277.3273 - loglik: -2.7505e+02 - logprior: -2.2759e+00
Fitted a model with MAP estimate = -277.2934
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 301.7968 - loglik: -2.7937e+02 - logprior: -2.2431e+01
Epoch 2/2
10/10 - 1s - loss: 282.4957 - loglik: -2.7280e+02 - logprior: -9.6927e+00
Fitted a model with MAP estimate = -279.3307
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 288.4498 - loglik: -2.7068e+02 - logprior: -1.7771e+01
Epoch 2/2
10/10 - 1s - loss: 275.1779 - loglik: -2.7032e+02 - logprior: -4.8579e+00
Fitted a model with MAP estimate = -273.0117
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 291.6042 - loglik: -2.7134e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 277.1307 - loglik: -2.7121e+02 - logprior: -5.9167e+00
Epoch 3/10
10/10 - 1s - loss: 273.2628 - loglik: -2.7011e+02 - logprior: -3.1484e+00
Epoch 4/10
10/10 - 1s - loss: 272.2159 - loglik: -2.7017e+02 - logprior: -2.0425e+00
Epoch 5/10
10/10 - 1s - loss: 271.6398 - loglik: -2.7021e+02 - logprior: -1.4265e+00
Epoch 6/10
10/10 - 1s - loss: 271.5227 - loglik: -2.7033e+02 - logprior: -1.1913e+00
Epoch 7/10
10/10 - 1s - loss: 270.7729 - loglik: -2.6979e+02 - logprior: -9.8410e-01
Epoch 8/10
10/10 - 1s - loss: 271.3812 - loglik: -2.7049e+02 - logprior: -8.9102e-01
Fitted a model with MAP estimate = -271.0513
Time for alignment: 31.4272
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 346.3817 - loglik: -3.2636e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 314.8743 - loglik: -3.0915e+02 - logprior: -5.7253e+00
Epoch 3/10
10/10 - 1s - loss: 297.2502 - loglik: -2.9399e+02 - logprior: -3.2601e+00
Epoch 4/10
10/10 - 1s - loss: 286.8784 - loglik: -2.8424e+02 - logprior: -2.6373e+00
Epoch 5/10
10/10 - 1s - loss: 282.6507 - loglik: -2.8020e+02 - logprior: -2.4482e+00
Epoch 6/10
10/10 - 1s - loss: 280.9653 - loglik: -2.7864e+02 - logprior: -2.3257e+00
Epoch 7/10
10/10 - 1s - loss: 280.2721 - loglik: -2.7806e+02 - logprior: -2.2090e+00
Epoch 8/10
10/10 - 1s - loss: 279.9202 - loglik: -2.7784e+02 - logprior: -2.0777e+00
Epoch 9/10
10/10 - 1s - loss: 279.3576 - loglik: -2.7736e+02 - logprior: -1.9934e+00
Epoch 10/10
10/10 - 1s - loss: 279.7451 - loglik: -2.7778e+02 - logprior: -1.9660e+00
Fitted a model with MAP estimate = -279.4423
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 302.2988 - loglik: -2.7988e+02 - logprior: -2.2416e+01
Epoch 2/2
10/10 - 1s - loss: 283.1755 - loglik: -2.7344e+02 - logprior: -9.7337e+00
Fitted a model with MAP estimate = -279.6985
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 289.2230 - loglik: -2.7141e+02 - logprior: -1.7810e+01
Epoch 2/2
10/10 - 1s - loss: 275.0610 - loglik: -2.7019e+02 - logprior: -4.8676e+00
Fitted a model with MAP estimate = -273.1366
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 291.7081 - loglik: -2.7146e+02 - logprior: -2.0247e+01
Epoch 2/10
10/10 - 1s - loss: 276.8822 - loglik: -2.7097e+02 - logprior: -5.9075e+00
Epoch 3/10
10/10 - 1s - loss: 273.3111 - loglik: -2.7015e+02 - logprior: -3.1564e+00
Epoch 4/10
10/10 - 1s - loss: 272.3150 - loglik: -2.7026e+02 - logprior: -2.0532e+00
Epoch 5/10
10/10 - 1s - loss: 271.7299 - loglik: -2.7031e+02 - logprior: -1.4231e+00
Epoch 6/10
10/10 - 1s - loss: 271.1940 - loglik: -2.7000e+02 - logprior: -1.1921e+00
Epoch 7/10
10/10 - 1s - loss: 271.4440 - loglik: -2.7046e+02 - logprior: -9.8417e-01
Fitted a model with MAP estimate = -271.1378
Time for alignment: 27.6241
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 346.4583 - loglik: -3.2644e+02 - logprior: -2.0019e+01
Epoch 2/10
10/10 - 1s - loss: 314.7346 - loglik: -3.0901e+02 - logprior: -5.7225e+00
Epoch 3/10
10/10 - 1s - loss: 297.6247 - loglik: -2.9437e+02 - logprior: -3.2517e+00
Epoch 4/10
10/10 - 1s - loss: 287.3854 - loglik: -2.8474e+02 - logprior: -2.6413e+00
Epoch 5/10
10/10 - 1s - loss: 282.8663 - loglik: -2.8041e+02 - logprior: -2.4601e+00
Epoch 6/10
10/10 - 1s - loss: 281.1566 - loglik: -2.7884e+02 - logprior: -2.3197e+00
Epoch 7/10
10/10 - 1s - loss: 280.3364 - loglik: -2.7816e+02 - logprior: -2.1775e+00
Epoch 8/10
10/10 - 1s - loss: 280.0074 - loglik: -2.7796e+02 - logprior: -2.0461e+00
Epoch 9/10
10/10 - 1s - loss: 280.0833 - loglik: -2.7811e+02 - logprior: -1.9743e+00
Fitted a model with MAP estimate = -279.8353
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 302.6090 - loglik: -2.8020e+02 - logprior: -2.2406e+01
Epoch 2/2
10/10 - 1s - loss: 283.1867 - loglik: -2.7347e+02 - logprior: -9.7150e+00
Fitted a model with MAP estimate = -279.6542
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.1325 - loglik: -2.7133e+02 - logprior: -1.7804e+01
Epoch 2/2
10/10 - 1s - loss: 275.0873 - loglik: -2.7021e+02 - logprior: -4.8732e+00
Fitted a model with MAP estimate = -273.1233
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.8459 - loglik: -2.7160e+02 - logprior: -2.0248e+01
Epoch 2/10
10/10 - 1s - loss: 276.7354 - loglik: -2.7083e+02 - logprior: -5.9085e+00
Epoch 3/10
10/10 - 1s - loss: 273.4509 - loglik: -2.7029e+02 - logprior: -3.1621e+00
Epoch 4/10
10/10 - 1s - loss: 272.1948 - loglik: -2.7015e+02 - logprior: -2.0442e+00
Epoch 5/10
10/10 - 1s - loss: 271.4973 - loglik: -2.7006e+02 - logprior: -1.4346e+00
Epoch 6/10
10/10 - 1s - loss: 271.4009 - loglik: -2.7020e+02 - logprior: -1.2029e+00
Epoch 7/10
10/10 - 1s - loss: 271.5193 - loglik: -2.7053e+02 - logprior: -9.8820e-01
Fitted a model with MAP estimate = -271.1298
Time for alignment: 27.6904
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 346.3366 - loglik: -3.2632e+02 - logprior: -2.0017e+01
Epoch 2/10
10/10 - 1s - loss: 314.8216 - loglik: -3.0909e+02 - logprior: -5.7270e+00
Epoch 3/10
10/10 - 1s - loss: 297.8397 - loglik: -2.9457e+02 - logprior: -3.2688e+00
Epoch 4/10
10/10 - 1s - loss: 287.3291 - loglik: -2.8468e+02 - logprior: -2.6461e+00
Epoch 5/10
10/10 - 1s - loss: 281.6535 - loglik: -2.7908e+02 - logprior: -2.5740e+00
Epoch 6/10
10/10 - 1s - loss: 279.0814 - loglik: -2.7651e+02 - logprior: -2.5754e+00
Epoch 7/10
10/10 - 1s - loss: 278.3703 - loglik: -2.7586e+02 - logprior: -2.5067e+00
Epoch 8/10
10/10 - 1s - loss: 277.7755 - loglik: -2.7538e+02 - logprior: -2.3957e+00
Epoch 9/10
10/10 - 1s - loss: 277.3030 - loglik: -2.7500e+02 - logprior: -2.2999e+00
Epoch 10/10
10/10 - 1s - loss: 277.8144 - loglik: -2.7556e+02 - logprior: -2.2563e+00
Fitted a model with MAP estimate = -277.2981
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 301.9982 - loglik: -2.7957e+02 - logprior: -2.2432e+01
Epoch 2/2
10/10 - 1s - loss: 282.3612 - loglik: -2.7267e+02 - logprior: -9.6928e+00
Fitted a model with MAP estimate = -279.3405
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 288.4240 - loglik: -2.7065e+02 - logprior: -1.7771e+01
Epoch 2/2
10/10 - 1s - loss: 275.0049 - loglik: -2.7014e+02 - logprior: -4.8616e+00
Fitted a model with MAP estimate = -273.0113
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 291.8271 - loglik: -2.7159e+02 - logprior: -2.0236e+01
Epoch 2/10
10/10 - 1s - loss: 276.6370 - loglik: -2.7074e+02 - logprior: -5.8990e+00
Epoch 3/10
10/10 - 1s - loss: 273.3102 - loglik: -2.7015e+02 - logprior: -3.1565e+00
Epoch 4/10
10/10 - 1s - loss: 272.4660 - loglik: -2.7042e+02 - logprior: -2.0482e+00
Epoch 5/10
10/10 - 1s - loss: 271.8522 - loglik: -2.7043e+02 - logprior: -1.4182e+00
Epoch 6/10
10/10 - 1s - loss: 271.2360 - loglik: -2.7005e+02 - logprior: -1.1902e+00
Epoch 7/10
10/10 - 1s - loss: 271.0252 - loglik: -2.7005e+02 - logprior: -9.7832e-01
Epoch 8/10
10/10 - 1s - loss: 270.9904 - loglik: -2.7010e+02 - logprior: -8.8811e-01
Epoch 9/10
10/10 - 1s - loss: 271.2964 - loglik: -2.7046e+02 - logprior: -8.3226e-01
Fitted a model with MAP estimate = -270.9753
Time for alignment: 27.9935
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 346.2623 - loglik: -3.2624e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 314.9431 - loglik: -3.0922e+02 - logprior: -5.7274e+00
Epoch 3/10
10/10 - 1s - loss: 297.8186 - loglik: -2.9452e+02 - logprior: -3.2992e+00
Epoch 4/10
10/10 - 1s - loss: 286.1068 - loglik: -2.8333e+02 - logprior: -2.7747e+00
Epoch 5/10
10/10 - 1s - loss: 280.8679 - loglik: -2.7816e+02 - logprior: -2.7032e+00
Epoch 6/10
10/10 - 1s - loss: 279.1116 - loglik: -2.7648e+02 - logprior: -2.6354e+00
Epoch 7/10
10/10 - 0s - loss: 278.6573 - loglik: -2.7614e+02 - logprior: -2.5125e+00
Epoch 8/10
10/10 - 1s - loss: 278.1874 - loglik: -2.7583e+02 - logprior: -2.3605e+00
Epoch 9/10
10/10 - 1s - loss: 277.7978 - loglik: -2.7553e+02 - logprior: -2.2637e+00
Epoch 10/10
10/10 - 1s - loss: 277.5894 - loglik: -2.7535e+02 - logprior: -2.2376e+00
Fitted a model with MAP estimate = -277.6064
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 302.2386 - loglik: -2.7981e+02 - logprior: -2.2426e+01
Epoch 2/2
10/10 - 1s - loss: 282.3308 - loglik: -2.7264e+02 - logprior: -9.6932e+00
Fitted a model with MAP estimate = -279.3185
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 288.3576 - loglik: -2.7059e+02 - logprior: -1.7772e+01
Epoch 2/2
10/10 - 1s - loss: 274.6888 - loglik: -2.6983e+02 - logprior: -4.8609e+00
Fitted a model with MAP estimate = -273.0014
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 291.6340 - loglik: -2.7139e+02 - logprior: -2.0244e+01
Epoch 2/10
10/10 - 1s - loss: 276.9135 - loglik: -2.7100e+02 - logprior: -5.9095e+00
Epoch 3/10
10/10 - 1s - loss: 273.3452 - loglik: -2.7019e+02 - logprior: -3.1570e+00
Epoch 4/10
10/10 - 1s - loss: 272.0991 - loglik: -2.7006e+02 - logprior: -2.0410e+00
Epoch 5/10
10/10 - 1s - loss: 271.7960 - loglik: -2.7037e+02 - logprior: -1.4300e+00
Epoch 6/10
10/10 - 1s - loss: 271.1181 - loglik: -2.6992e+02 - logprior: -1.1974e+00
Epoch 7/10
10/10 - 1s - loss: 271.4907 - loglik: -2.7051e+02 - logprior: -9.8159e-01
Fitted a model with MAP estimate = -271.1230
Time for alignment: 27.1193
Computed alignments with likelihoods: ['-271.0513', '-271.1378', '-271.1298', '-270.9753', '-271.1230']
Best model has likelihood: -270.9753  (prior= -0.8002 )
time for generating output: 0.1063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.975049504950495
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 1006.7686 - loglik: -1.0050e+03 - logprior: -1.7820e+00
Epoch 2/10
39/39 - 7s - loss: 926.4869 - loglik: -9.2547e+02 - logprior: -1.0141e+00
Epoch 3/10
39/39 - 7s - loss: 919.2031 - loglik: -9.1821e+02 - logprior: -9.9750e-01
Epoch 4/10
39/39 - 7s - loss: 916.5456 - loglik: -9.1553e+02 - logprior: -1.0143e+00
Epoch 5/10
39/39 - 7s - loss: 915.5395 - loglik: -9.1454e+02 - logprior: -9.9522e-01
Epoch 6/10
39/39 - 7s - loss: 914.5617 - loglik: -9.1358e+02 - logprior: -9.8293e-01
Epoch 7/10
39/39 - 7s - loss: 914.2018 - loglik: -9.1322e+02 - logprior: -9.8545e-01
Epoch 8/10
39/39 - 8s - loss: 914.2173 - loglik: -9.1323e+02 - logprior: -9.9062e-01
Fitted a model with MAP estimate = -733.8522
expansions: [(0, 5), (10, 1), (15, 1), (18, 1), (28, 2), (29, 3), (30, 1), (32, 1), (37, 1), (42, 1), (44, 3), (71, 2), (88, 3), (89, 1), (90, 1), (102, 1), (104, 1), (107, 1), (127, 12), (130, 2), (134, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 895.7988 - loglik: -8.9304e+02 - logprior: -2.7597e+00
Epoch 2/2
39/39 - 10s - loss: 880.4705 - loglik: -8.7929e+02 - logprior: -1.1810e+00
Fitted a model with MAP estimate = -707.1543
expansions: []
discards: [  0   1   4   5  37  40  41  62  91 172 178 179 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 895.4044 - loglik: -8.9258e+02 - logprior: -2.8294e+00
Epoch 2/2
39/39 - 9s - loss: 888.7361 - loglik: -8.8808e+02 - logprior: -6.5336e-01
Fitted a model with MAP estimate = -714.4635
expansions: [(0, 8), (168, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 709.2761 - loglik: -7.0794e+02 - logprior: -1.3383e+00
Epoch 2/10
52/52 - 12s - loss: 706.7328 - loglik: -7.0586e+02 - logprior: -8.7186e-01
Epoch 3/10
52/52 - 11s - loss: 699.7335 - loglik: -6.9891e+02 - logprior: -8.2685e-01
Epoch 4/10
52/52 - 12s - loss: 702.2794 - loglik: -7.0148e+02 - logprior: -7.9679e-01
Fitted a model with MAP estimate = -700.4512
Time for alignment: 219.5218
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 1009.3099 - loglik: -1.0075e+03 - logprior: -1.7723e+00
Epoch 2/10
39/39 - 8s - loss: 927.4330 - loglik: -9.2632e+02 - logprior: -1.1139e+00
Epoch 3/10
39/39 - 8s - loss: 918.0674 - loglik: -9.1691e+02 - logprior: -1.1530e+00
Epoch 4/10
39/39 - 8s - loss: 915.4473 - loglik: -9.1434e+02 - logprior: -1.1095e+00
Epoch 5/10
39/39 - 7s - loss: 914.2652 - loglik: -9.1316e+02 - logprior: -1.1077e+00
Epoch 6/10
39/39 - 8s - loss: 914.0197 - loglik: -9.1293e+02 - logprior: -1.0934e+00
Epoch 7/10
39/39 - 8s - loss: 913.4202 - loglik: -9.1233e+02 - logprior: -1.0944e+00
Epoch 8/10
39/39 - 8s - loss: 913.4486 - loglik: -9.1235e+02 - logprior: -1.0958e+00
Fitted a model with MAP estimate = -733.0653
expansions: [(0, 15), (10, 1), (15, 1), (18, 1), (19, 1), (27, 2), (29, 1), (30, 2), (32, 1), (34, 2), (37, 1), (43, 1), (44, 2), (50, 1), (70, 1), (71, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (102, 1), (107, 2), (114, 1), (126, 9), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 895.6347 - loglik: -8.9307e+02 - logprior: -2.5689e+00
Epoch 2/2
39/39 - 11s - loss: 879.5350 - loglik: -8.7824e+02 - logprior: -1.2924e+00
Fitted a model with MAP estimate = -705.6054
expansions: [(172, 2), (173, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  47  52  53  59  73
 147 183]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 887.7139 - loglik: -8.8581e+02 - logprior: -1.8999e+00
Epoch 2/2
39/39 - 10s - loss: 882.3684 - loglik: -8.8161e+02 - logprior: -7.6080e-01
Fitted a model with MAP estimate = -709.5396
expansions: [(38, 2)]
discards: [ 43 156]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 14s - loss: 708.0800 - loglik: -7.0707e+02 - logprior: -1.0126e+00
Epoch 2/10
52/52 - 11s - loss: 700.8536 - loglik: -7.0028e+02 - logprior: -5.7799e-01
Epoch 3/10
52/52 - 12s - loss: 701.4077 - loglik: -7.0087e+02 - logprior: -5.3796e-01
Fitted a model with MAP estimate = -700.2197
Time for alignment: 210.5084
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1010.8785 - loglik: -1.0091e+03 - logprior: -1.7856e+00
Epoch 2/10
39/39 - 7s - loss: 930.2209 - loglik: -9.2907e+02 - logprior: -1.1465e+00
Epoch 3/10
39/39 - 7s - loss: 922.9353 - loglik: -9.2187e+02 - logprior: -1.0690e+00
Epoch 4/10
39/39 - 8s - loss: 921.0607 - loglik: -9.2001e+02 - logprior: -1.0505e+00
Epoch 5/10
39/39 - 7s - loss: 919.1692 - loglik: -9.1814e+02 - logprior: -1.0324e+00
Epoch 6/10
39/39 - 7s - loss: 918.6130 - loglik: -9.1760e+02 - logprior: -1.0163e+00
Epoch 7/10
39/39 - 7s - loss: 918.2700 - loglik: -9.1726e+02 - logprior: -1.0084e+00
Epoch 8/10
39/39 - 7s - loss: 917.4142 - loglik: -9.1638e+02 - logprior: -1.0298e+00
Epoch 9/10
39/39 - 7s - loss: 918.0146 - loglik: -9.1698e+02 - logprior: -1.0307e+00
Fitted a model with MAP estimate = -735.7583
expansions: [(0, 12), (10, 1), (15, 1), (28, 1), (30, 1), (31, 4), (33, 1), (38, 1), (43, 2), (44, 1), (45, 1), (56, 1), (70, 3), (89, 1), (90, 2), (91, 2), (103, 1), (108, 2), (109, 1), (125, 7), (126, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 901.7847 - loglik: -8.9906e+02 - logprior: -2.7218e+00
Epoch 2/2
39/39 - 9s - loss: 884.8494 - loglik: -8.8354e+02 - logprior: -1.3106e+00
Fitted a model with MAP estimate = -710.1691
expansions: [(167, 5)]
discards: [  1   2   3   4   5   6   7   8   9  10  48  65  99 144 178]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 890.0126 - loglik: -8.8806e+02 - logprior: -1.9484e+00
Epoch 2/2
39/39 - 10s - loss: 884.8054 - loglik: -8.8408e+02 - logprior: -7.2664e-01
Fitted a model with MAP estimate = -712.0965
expansions: [(0, 13)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 17s - loss: 708.2325 - loglik: -7.0665e+02 - logprior: -1.5868e+00
Epoch 2/10
52/52 - 13s - loss: 703.1161 - loglik: -7.0190e+02 - logprior: -1.2159e+00
Epoch 3/10
52/52 - 12s - loss: 703.4984 - loglik: -7.0231e+02 - logprior: -1.1913e+00
Fitted a model with MAP estimate = -700.7795
Time for alignment: 219.1472
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 1009.3370 - loglik: -1.0076e+03 - logprior: -1.7474e+00
Epoch 2/10
39/39 - 8s - loss: 931.9678 - loglik: -9.3093e+02 - logprior: -1.0387e+00
Epoch 3/10
39/39 - 8s - loss: 923.0083 - loglik: -9.2195e+02 - logprior: -1.0552e+00
Epoch 4/10
39/39 - 7s - loss: 920.6441 - loglik: -9.1963e+02 - logprior: -1.0119e+00
Epoch 5/10
39/39 - 8s - loss: 919.5605 - loglik: -9.1857e+02 - logprior: -9.8561e-01
Epoch 6/10
39/39 - 8s - loss: 918.6274 - loglik: -9.1763e+02 - logprior: -9.9985e-01
Epoch 7/10
39/39 - 7s - loss: 918.5840 - loglik: -9.1758e+02 - logprior: -1.0069e+00
Epoch 8/10
39/39 - 7s - loss: 917.9196 - loglik: -9.1691e+02 - logprior: -1.0143e+00
Epoch 9/10
39/39 - 8s - loss: 918.3256 - loglik: -9.1731e+02 - logprior: -1.0130e+00
Fitted a model with MAP estimate = -736.1696
expansions: [(0, 12), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (32, 1), (34, 3), (37, 1), (43, 1), (44, 1), (56, 1), (71, 1), (88, 3), (90, 1), (102, 1), (106, 1), (107, 1), (125, 2), (129, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 907.0719 - loglik: -9.0439e+02 - logprior: -2.6833e+00
Epoch 2/2
39/39 - 10s - loss: 893.7228 - loglik: -8.9252e+02 - logprior: -1.2051e+00
Fitted a model with MAP estimate = -717.6321
expansions: [(118, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  52 159]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 899.1559 - loglik: -8.9734e+02 - logprior: -1.8201e+00
Epoch 2/2
39/39 - 8s - loss: 894.6455 - loglik: -8.9398e+02 - logprior: -6.6496e-01
Fitted a model with MAP estimate = -718.3772
expansions: [(0, 14), (41, 1)]
discards: [42 43]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 719.4730 - loglik: -7.1793e+02 - logprior: -1.5441e+00
Epoch 2/10
52/52 - 11s - loss: 709.6218 - loglik: -7.0843e+02 - logprior: -1.1874e+00
Epoch 3/10
52/52 - 12s - loss: 712.0347 - loglik: -7.1087e+02 - logprior: -1.1632e+00
Fitted a model with MAP estimate = -709.6947
Time for alignment: 210.0294
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1008.5524 - loglik: -1.0068e+03 - logprior: -1.7550e+00
Epoch 2/10
39/39 - 8s - loss: 926.6246 - loglik: -9.2556e+02 - logprior: -1.0691e+00
Epoch 3/10
39/39 - 7s - loss: 918.3833 - loglik: -9.1723e+02 - logprior: -1.1541e+00
Epoch 4/10
39/39 - 8s - loss: 916.1269 - loglik: -9.1499e+02 - logprior: -1.1395e+00
Epoch 5/10
39/39 - 7s - loss: 914.7512 - loglik: -9.1361e+02 - logprior: -1.1410e+00
Epoch 6/10
39/39 - 8s - loss: 914.5211 - loglik: -9.1338e+02 - logprior: -1.1391e+00
Epoch 7/10
39/39 - 8s - loss: 913.8269 - loglik: -9.1269e+02 - logprior: -1.1337e+00
Epoch 8/10
39/39 - 7s - loss: 914.2070 - loglik: -9.1308e+02 - logprior: -1.1301e+00
Fitted a model with MAP estimate = -733.1829
expansions: [(0, 12), (10, 1), (15, 1), (18, 1), (28, 1), (30, 1), (31, 3), (33, 1), (34, 1), (43, 1), (44, 1), (45, 1), (60, 1), (70, 3), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (127, 11), (130, 3), (134, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 894.6025 - loglik: -8.9175e+02 - logprior: -2.8496e+00
Epoch 2/2
39/39 - 11s - loss: 876.7421 - loglik: -8.7524e+02 - logprior: -1.4994e+00
Fitted a model with MAP estimate = -705.0622
expansions: [(171, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  96 180 186 187 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 885.4025 - loglik: -8.8331e+02 - logprior: -2.0956e+00
Epoch 2/2
39/39 - 10s - loss: 880.5135 - loglik: -8.7971e+02 - logprior: -8.0827e-01
Fitted a model with MAP estimate = -709.4107
expansions: [(0, 13), (161, 1), (176, 6)]
discards: [171]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 17s - loss: 705.2452 - loglik: -7.0356e+02 - logprior: -1.6890e+00
Epoch 2/10
52/52 - 13s - loss: 696.9615 - loglik: -6.9558e+02 - logprior: -1.3793e+00
Epoch 3/10
52/52 - 14s - loss: 700.8033 - loglik: -6.9951e+02 - logprior: -1.2970e+00
Fitted a model with MAP estimate = -696.3698
Time for alignment: 213.7000
Computed alignments with likelihoods: ['-700.4512', '-700.2197', '-700.7795', '-709.6947', '-696.3698']
Best model has likelihood: -696.3698  (prior= -1.2714 )
time for generating output: 0.5790
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.22916468235070006
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1688.1736 - loglik: -1.6867e+03 - logprior: -1.5023e+00
Epoch 2/10
37/37 - 20s - loss: 1604.0356 - loglik: -1.6036e+03 - logprior: -4.8224e-01
Epoch 3/10
37/37 - 20s - loss: 1590.8810 - loglik: -1.5904e+03 - logprior: -4.8303e-01
Epoch 4/10
37/37 - 20s - loss: 1586.4421 - loglik: -1.5860e+03 - logprior: -4.3119e-01
Epoch 5/10
37/37 - 20s - loss: 1585.2958 - loglik: -1.5848e+03 - logprior: -4.9889e-01
Epoch 6/10
37/37 - 20s - loss: 1584.7876 - loglik: -1.5844e+03 - logprior: -4.3182e-01
Epoch 7/10
37/37 - 20s - loss: 1583.2872 - loglik: -1.5827e+03 - logprior: -5.7937e-01
Epoch 8/10
37/37 - 20s - loss: 1581.2495 - loglik: -1.5808e+03 - logprior: -4.5239e-01
Epoch 9/10
37/37 - 20s - loss: 1583.9020 - loglik: -1.5833e+03 - logprior: -5.8865e-01
Fitted a model with MAP estimate = -1582.1980
expansions: [(0, 4), (30, 1), (32, 1), (33, 2), (34, 1), (35, 1), (39, 1), (61, 1), (74, 3), (89, 14), (134, 1), (198, 9), (204, 1), (210, 3), (222, 1), (239, 1)]
discards: [101 104 105 106 122]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 1597.0718 - loglik: -1.5947e+03 - logprior: -2.4143e+00
Epoch 2/2
37/37 - 24s - loss: 1578.8340 - loglik: -1.5783e+03 - logprior: -5.1362e-01
Fitted a model with MAP estimate = -1577.3690
expansions: [(0, 2), (39, 1), (113, 3), (114, 6), (115, 1), (227, 1), (229, 4)]
discards: [  0  88 178 179 232]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1584.4623 - loglik: -1.5826e+03 - logprior: -1.8953e+00
Epoch 2/2
37/37 - 26s - loss: 1575.7935 - loglik: -1.5755e+03 - logprior: -3.1320e-01
Fitted a model with MAP estimate = -1574.3972
expansions: [(0, 2), (189, 1), (190, 1), (240, 4)]
discards: [  1 115 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 33s - loss: 1579.7787 - loglik: -1.5777e+03 - logprior: -2.0889e+00
Epoch 2/10
37/37 - 26s - loss: 1575.6437 - loglik: -1.5755e+03 - logprior: -1.5378e-01
Epoch 3/10
37/37 - 27s - loss: 1569.8168 - loglik: -1.5699e+03 - logprior: 0.0394
Epoch 4/10
37/37 - 27s - loss: 1569.8075 - loglik: -1.5699e+03 - logprior: 0.0561
Epoch 5/10
37/37 - 27s - loss: 1566.2753 - loglik: -1.5664e+03 - logprior: 0.1267
Epoch 6/10
37/37 - 26s - loss: 1562.4359 - loglik: -1.5626e+03 - logprior: 0.1583
Epoch 7/10
37/37 - 27s - loss: 1566.0872 - loglik: -1.5663e+03 - logprior: 0.2627
Fitted a model with MAP estimate = -1563.3254
Time for alignment: 603.7000
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1688.7083 - loglik: -1.6872e+03 - logprior: -1.4807e+00
Epoch 2/10
37/37 - 20s - loss: 1605.2911 - loglik: -1.6049e+03 - logprior: -4.4000e-01
Epoch 3/10
37/37 - 20s - loss: 1595.8596 - loglik: -1.5955e+03 - logprior: -4.0793e-01
Epoch 4/10
37/37 - 20s - loss: 1589.4067 - loglik: -1.5890e+03 - logprior: -4.1429e-01
Epoch 5/10
37/37 - 20s - loss: 1584.8304 - loglik: -1.5843e+03 - logprior: -5.0231e-01
Epoch 6/10
37/37 - 20s - loss: 1585.6528 - loglik: -1.5851e+03 - logprior: -5.5707e-01
Fitted a model with MAP estimate = -1585.0988
expansions: [(0, 4), (30, 1), (32, 2), (34, 3), (61, 2), (66, 1), (104, 1), (115, 1), (116, 1), (134, 4), (135, 1), (197, 13), (239, 1)]
discards: [ 98  99 100 101 102 151 153 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 280 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 1598.1509 - loglik: -1.5958e+03 - logprior: -2.3551e+00
Epoch 2/2
37/37 - 23s - loss: 1585.1646 - loglik: -1.5846e+03 - logprior: -5.7854e-01
Fitted a model with MAP estimate = -1582.5545
expansions: [(0, 2), (39, 1), (40, 1), (148, 1), (150, 1), (168, 4), (218, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 1587.2513 - loglik: -1.5854e+03 - logprior: -1.8252e+00
Epoch 2/2
37/37 - 24s - loss: 1580.9988 - loglik: -1.5807e+03 - logprior: -2.9165e-01
Fitted a model with MAP estimate = -1578.2604
expansions: [(0, 2), (107, 8), (108, 1)]
discards: [  1 174]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1582.8250 - loglik: -1.5808e+03 - logprior: -2.0409e+00
Epoch 2/10
37/37 - 25s - loss: 1578.7891 - loglik: -1.5787e+03 - logprior: -1.2308e-01
Epoch 3/10
37/37 - 25s - loss: 1572.9651 - loglik: -1.5730e+03 - logprior: 0.0282
Epoch 4/10
37/37 - 25s - loss: 1574.5312 - loglik: -1.5746e+03 - logprior: 0.0414
Fitted a model with MAP estimate = -1568.7781
Time for alignment: 445.1852
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1683.2482 - loglik: -1.6818e+03 - logprior: -1.4762e+00
Epoch 2/10
37/37 - 20s - loss: 1597.2314 - loglik: -1.5969e+03 - logprior: -3.0586e-01
Epoch 3/10
37/37 - 20s - loss: 1589.1154 - loglik: -1.5889e+03 - logprior: -2.3895e-01
Epoch 4/10
37/37 - 20s - loss: 1587.0126 - loglik: -1.5868e+03 - logprior: -2.4891e-01
Epoch 5/10
37/37 - 20s - loss: 1582.9817 - loglik: -1.5827e+03 - logprior: -2.6432e-01
Epoch 6/10
37/37 - 20s - loss: 1580.1948 - loglik: -1.5799e+03 - logprior: -2.8006e-01
Epoch 7/10
37/37 - 20s - loss: 1582.6160 - loglik: -1.5823e+03 - logprior: -2.9477e-01
Fitted a model with MAP estimate = -1580.6409
expansions: [(0, 4), (34, 4), (36, 1), (66, 1), (98, 9), (99, 3), (101, 2), (138, 1), (152, 5), (197, 7), (240, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 292 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1594.6111 - loglik: -1.5923e+03 - logprior: -2.3170e+00
Epoch 2/2
37/37 - 24s - loss: 1579.4373 - loglik: -1.5790e+03 - logprior: -4.6609e-01
Fitted a model with MAP estimate = -1576.9564
expansions: [(0, 2), (38, 2), (180, 2)]
discards: [  0 108 109 110 177 178 183 184 185 186 187 188 189 190]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 1586.2816 - loglik: -1.5846e+03 - logprior: -1.7317e+00
Epoch 2/2
37/37 - 23s - loss: 1578.9128 - loglik: -1.5787e+03 - logprior: -1.9680e-01
Fitted a model with MAP estimate = -1577.5370
expansions: [(0, 2), (38, 1), (39, 1), (180, 1), (183, 1), (184, 1)]
discards: [  1   2   3 177]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 1582.8790 - loglik: -1.5810e+03 - logprior: -1.8628e+00
Epoch 2/10
37/37 - 24s - loss: 1577.6448 - loglik: -1.5776e+03 - logprior: -3.8998e-03
Epoch 3/10
37/37 - 24s - loss: 1574.8424 - loglik: -1.5748e+03 - logprior: -9.4620e-05
Epoch 4/10
37/37 - 24s - loss: 1572.6089 - loglik: -1.5728e+03 - logprior: 0.1976
Epoch 5/10
37/37 - 24s - loss: 1568.3228 - loglik: -1.5684e+03 - logprior: 0.1258
Epoch 6/10
37/37 - 24s - loss: 1568.2018 - loglik: -1.5685e+03 - logprior: 0.2907
Epoch 7/10
37/37 - 24s - loss: 1567.0676 - loglik: -1.5674e+03 - logprior: 0.3524
Epoch 8/10
37/37 - 24s - loss: 1567.2968 - loglik: -1.5678e+03 - logprior: 0.4985
Fitted a model with MAP estimate = -1566.1845
Time for alignment: 555.5203
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 24s - loss: 1686.8035 - loglik: -1.6853e+03 - logprior: -1.4922e+00
Epoch 2/10
37/37 - 20s - loss: 1601.5178 - loglik: -1.6012e+03 - logprior: -3.6391e-01
Epoch 3/10
37/37 - 20s - loss: 1589.4940 - loglik: -1.5892e+03 - logprior: -3.1686e-01
Epoch 4/10
37/37 - 20s - loss: 1587.7051 - loglik: -1.5874e+03 - logprior: -3.4313e-01
Epoch 5/10
37/37 - 20s - loss: 1581.0414 - loglik: -1.5807e+03 - logprior: -3.5927e-01
Epoch 6/10
37/37 - 20s - loss: 1585.0835 - loglik: -1.5847e+03 - logprior: -3.6435e-01
Fitted a model with MAP estimate = -1582.0531
expansions: [(0, 3), (30, 1), (32, 2), (34, 3), (35, 1), (73, 2), (75, 3), (90, 12), (98, 1), (138, 1), (198, 12), (223, 1), (240, 1), (242, 1)]
discards: [105 106 107 148 153 154 184 189 190]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 1593.7412 - loglik: -1.5914e+03 - logprior: -2.3226e+00
Epoch 2/2
37/37 - 24s - loss: 1580.6815 - loglik: -1.5802e+03 - logprior: -4.8871e-01
Fitted a model with MAP estimate = -1576.0632
expansions: [(0, 3), (38, 1), (39, 1), (42, 1), (109, 8), (110, 2), (231, 3)]
discards: [83 84 87]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1582.8607 - loglik: -1.5801e+03 - logprior: -2.7358e+00
Epoch 2/2
37/37 - 26s - loss: 1577.6089 - loglik: -1.5772e+03 - logprior: -4.0486e-01
Fitted a model with MAP estimate = -1572.7856
expansions: []
discards: [  1   2 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1580.3578 - loglik: -1.5787e+03 - logprior: -1.6776e+00
Epoch 2/10
37/37 - 25s - loss: 1575.1731 - loglik: -1.5750e+03 - logprior: -1.9226e-01
Epoch 3/10
37/37 - 25s - loss: 1571.4000 - loglik: -1.5714e+03 - logprior: -2.3167e-02
Epoch 4/10
37/37 - 25s - loss: 1569.8567 - loglik: -1.5699e+03 - logprior: 0.0527
Epoch 5/10
37/37 - 25s - loss: 1565.1760 - loglik: -1.5652e+03 - logprior: 0.0240
Epoch 6/10
37/37 - 25s - loss: 1567.8145 - loglik: -1.5680e+03 - logprior: 0.1654
Fitted a model with MAP estimate = -1564.4203
Time for alignment: 507.4641
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1689.7124 - loglik: -1.6882e+03 - logprior: -1.5007e+00
Epoch 2/10
37/37 - 20s - loss: 1610.7583 - loglik: -1.6103e+03 - logprior: -4.5435e-01
Epoch 3/10
37/37 - 20s - loss: 1592.1184 - loglik: -1.5916e+03 - logprior: -4.7755e-01
Epoch 4/10
37/37 - 20s - loss: 1595.8414 - loglik: -1.5952e+03 - logprior: -6.7627e-01
Fitted a model with MAP estimate = -1589.4526
expansions: [(0, 3), (30, 1), (32, 2), (34, 3), (38, 1), (60, 1), (65, 1), (91, 2), (92, 5), (115, 2), (116, 1), (119, 1), (136, 6), (157, 8), (198, 11), (206, 4), (243, 1)]
discards: [ 98 100 101 102 103 151 152 158 159 160 161 162 163 164 165]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 1596.0610 - loglik: -1.5937e+03 - logprior: -2.3720e+00
Epoch 2/2
37/37 - 24s - loss: 1580.6322 - loglik: -1.5801e+03 - logprior: -5.6453e-01
Fitted a model with MAP estimate = -1577.3681
expansions: [(0, 3), (38, 1), (39, 1), (158, 1), (172, 8), (176, 1), (228, 4), (232, 2)]
discards: [103 182 183 184 185 215 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1584.9731 - loglik: -1.5822e+03 - logprior: -2.7659e+00
Epoch 2/2
37/37 - 26s - loss: 1579.2987 - loglik: -1.5788e+03 - logprior: -5.2639e-01
Fitted a model with MAP estimate = -1575.1573
expansions: [(109, 1), (182, 6), (196, 3), (235, 1), (254, 2), (255, 1)]
discards: [  0   1   2 188 197 241 242 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 31s - loss: 1581.1794 - loglik: -1.5792e+03 - logprior: -1.9766e+00
Epoch 2/10
37/37 - 27s - loss: 1575.8120 - loglik: -1.5756e+03 - logprior: -2.6121e-01
Epoch 3/10
37/37 - 27s - loss: 1570.5135 - loglik: -1.5704e+03 - logprior: -1.1004e-01
Epoch 4/10
37/37 - 27s - loss: 1570.4448 - loglik: -1.5704e+03 - logprior: -2.5372e-02
Epoch 5/10
37/37 - 27s - loss: 1565.0579 - loglik: -1.5651e+03 - logprior: 0.0125
Epoch 6/10
37/37 - 27s - loss: 1565.8480 - loglik: -1.5659e+03 - logprior: 0.0461
Fitted a model with MAP estimate = -1563.9693
Time for alignment: 477.3095
Computed alignments with likelihoods: ['-1563.3254', '-1568.7781', '-1566.1845', '-1564.4203', '-1563.9693']
Best model has likelihood: -1563.3254  (prior= 0.3482 )
time for generating output: 0.2846
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.897981906750174
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 666.4628 - loglik: -5.5331e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 569.5583 - loglik: -5.4226e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 540.4545 - loglik: -5.3063e+02 - logprior: -9.8219e+00
Epoch 4/10
10/10 - 1s - loss: 525.8591 - loglik: -5.2250e+02 - logprior: -3.3629e+00
Epoch 5/10
10/10 - 1s - loss: 518.2465 - loglik: -5.1802e+02 - logprior: -2.2476e-01
Epoch 6/10
10/10 - 1s - loss: 513.0919 - loglik: -5.1461e+02 - logprior: 1.5226
Epoch 7/10
10/10 - 1s - loss: 509.9280 - loglik: -5.1234e+02 - logprior: 2.4092
Epoch 8/10
10/10 - 1s - loss: 508.1389 - loglik: -5.1120e+02 - logprior: 3.0622
Epoch 9/10
10/10 - 1s - loss: 507.0443 - loglik: -5.1071e+02 - logprior: 3.6677
Epoch 10/10
10/10 - 1s - loss: 506.3117 - loglik: -5.1051e+02 - logprior: 4.1952
Fitted a model with MAP estimate = -506.0044
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 658.1928 - loglik: -5.0924e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 548.7093 - loglik: -5.0685e+02 - logprior: -4.1854e+01
Fitted a model with MAP estimate = -528.6039
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 623.7109 - loglik: -5.0577e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 535.0389 - loglik: -5.0515e+02 - logprior: -2.9894e+01
Fitted a model with MAP estimate = -519.4337
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 629.5403 - loglik: -5.0410e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 538.7656 - loglik: -5.0350e+02 - logprior: -3.5271e+01
Epoch 3/10
10/10 - 1s - loss: 514.2479 - loglik: -5.0393e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 503.7903 - loglik: -5.0429e+02 - logprior: 0.4986
Epoch 5/10
10/10 - 1s - loss: 499.0545 - loglik: -5.0434e+02 - logprior: 5.2832
Epoch 6/10
10/10 - 1s - loss: 496.4952 - loglik: -5.0432e+02 - logprior: 7.8287
Epoch 7/10
10/10 - 1s - loss: 494.8896 - loglik: -5.0419e+02 - logprior: 9.3038
Epoch 8/10
10/10 - 1s - loss: 493.7740 - loglik: -5.0407e+02 - logprior: 10.2969
Epoch 9/10
10/10 - 1s - loss: 492.9298 - loglik: -5.0402e+02 - logprior: 11.0938
Epoch 10/10
10/10 - 1s - loss: 492.2302 - loglik: -5.0406e+02 - logprior: 11.8341
Fitted a model with MAP estimate = -491.8818
Time for alignment: 34.4875
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 666.4628 - loglik: -5.5331e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 569.5583 - loglik: -5.4226e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 540.4545 - loglik: -5.3063e+02 - logprior: -9.8219e+00
Epoch 4/10
10/10 - 1s - loss: 525.8591 - loglik: -5.2250e+02 - logprior: -3.3629e+00
Epoch 5/10
10/10 - 1s - loss: 518.2465 - loglik: -5.1802e+02 - logprior: -2.2476e-01
Epoch 6/10
10/10 - 1s - loss: 513.0919 - loglik: -5.1461e+02 - logprior: 1.5226
Epoch 7/10
10/10 - 1s - loss: 509.9280 - loglik: -5.1234e+02 - logprior: 2.4092
Epoch 8/10
10/10 - 1s - loss: 508.1390 - loglik: -5.1120e+02 - logprior: 3.0622
Epoch 9/10
10/10 - 1s - loss: 507.0444 - loglik: -5.1071e+02 - logprior: 3.6677
Epoch 10/10
10/10 - 1s - loss: 506.3117 - loglik: -5.1051e+02 - logprior: 4.1952
Fitted a model with MAP estimate = -506.0044
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 658.1929 - loglik: -5.0924e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 548.7094 - loglik: -5.0685e+02 - logprior: -4.1855e+01
Fitted a model with MAP estimate = -528.6039
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 623.7110 - loglik: -5.0577e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 535.0389 - loglik: -5.0515e+02 - logprior: -2.9894e+01
Fitted a model with MAP estimate = -519.4340
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 629.5404 - loglik: -5.0410e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 538.7657 - loglik: -5.0350e+02 - logprior: -3.5271e+01
Epoch 3/10
10/10 - 1s - loss: 514.2474 - loglik: -5.0393e+02 - logprior: -1.0314e+01
Epoch 4/10
10/10 - 1s - loss: 503.7899 - loglik: -5.0429e+02 - logprior: 0.4987
Epoch 5/10
10/10 - 1s - loss: 499.0543 - loglik: -5.0434e+02 - logprior: 5.2834
Epoch 6/10
10/10 - 1s - loss: 496.4950 - loglik: -5.0432e+02 - logprior: 7.8290
Epoch 7/10
10/10 - 1s - loss: 494.8895 - loglik: -5.0419e+02 - logprior: 9.3041
Epoch 8/10
10/10 - 1s - loss: 493.7738 - loglik: -5.0407e+02 - logprior: 10.2971
Epoch 9/10
10/10 - 1s - loss: 492.9296 - loglik: -5.0402e+02 - logprior: 11.0943
Epoch 10/10
10/10 - 1s - loss: 492.2299 - loglik: -5.0406e+02 - logprior: 11.8345
Fitted a model with MAP estimate = -491.8813
Time for alignment: 38.2313
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 666.4628 - loglik: -5.5331e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 569.5583 - loglik: -5.4226e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 540.4545 - loglik: -5.3063e+02 - logprior: -9.8219e+00
Epoch 4/10
10/10 - 1s - loss: 525.8591 - loglik: -5.2250e+02 - logprior: -3.3629e+00
Epoch 5/10
10/10 - 1s - loss: 518.2465 - loglik: -5.1802e+02 - logprior: -2.2476e-01
Epoch 6/10
10/10 - 1s - loss: 513.0919 - loglik: -5.1461e+02 - logprior: 1.5226
Epoch 7/10
10/10 - 1s - loss: 509.9279 - loglik: -5.1234e+02 - logprior: 2.4092
Epoch 8/10
10/10 - 1s - loss: 508.1389 - loglik: -5.1120e+02 - logprior: 3.0622
Epoch 9/10
10/10 - 1s - loss: 507.0444 - loglik: -5.1071e+02 - logprior: 3.6677
Epoch 10/10
10/10 - 1s - loss: 506.3118 - loglik: -5.1051e+02 - logprior: 4.1952
Fitted a model with MAP estimate = -506.0044
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 658.1927 - loglik: -5.0924e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 548.7092 - loglik: -5.0685e+02 - logprior: -4.1854e+01
Fitted a model with MAP estimate = -528.6039
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 623.7108 - loglik: -5.0577e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 535.0386 - loglik: -5.0514e+02 - logprior: -2.9894e+01
Fitted a model with MAP estimate = -519.4333
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 629.5402 - loglik: -5.0410e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 538.7657 - loglik: -5.0350e+02 - logprior: -3.5270e+01
Epoch 3/10
10/10 - 1s - loss: 514.2474 - loglik: -5.0393e+02 - logprior: -1.0315e+01
Epoch 4/10
10/10 - 1s - loss: 503.7900 - loglik: -5.0429e+02 - logprior: 0.4988
Epoch 5/10
10/10 - 1s - loss: 499.0543 - loglik: -5.0434e+02 - logprior: 5.2833
Epoch 6/10
10/10 - 1s - loss: 496.4948 - loglik: -5.0432e+02 - logprior: 7.8290
Epoch 7/10
10/10 - 1s - loss: 494.8891 - loglik: -5.0419e+02 - logprior: 9.3038
Epoch 8/10
10/10 - 1s - loss: 493.7736 - loglik: -5.0407e+02 - logprior: 10.2969
Epoch 9/10
10/10 - 1s - loss: 492.9296 - loglik: -5.0402e+02 - logprior: 11.0938
Epoch 10/10
10/10 - 1s - loss: 492.2300 - loglik: -5.0406e+02 - logprior: 11.8343
Fitted a model with MAP estimate = -491.8815
Time for alignment: 35.9684
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 666.4628 - loglik: -5.5331e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 569.5583 - loglik: -5.4226e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 540.4545 - loglik: -5.3063e+02 - logprior: -9.8219e+00
Epoch 4/10
10/10 - 1s - loss: 525.8591 - loglik: -5.2250e+02 - logprior: -3.3629e+00
Epoch 5/10
10/10 - 1s - loss: 518.2465 - loglik: -5.1802e+02 - logprior: -2.2476e-01
Epoch 6/10
10/10 - 1s - loss: 513.0919 - loglik: -5.1461e+02 - logprior: 1.5226
Epoch 7/10
10/10 - 1s - loss: 509.9280 - loglik: -5.1234e+02 - logprior: 2.4092
Epoch 8/10
10/10 - 1s - loss: 508.1390 - loglik: -5.1120e+02 - logprior: 3.0622
Epoch 9/10
10/10 - 1s - loss: 507.0444 - loglik: -5.1071e+02 - logprior: 3.6677
Epoch 10/10
10/10 - 1s - loss: 506.3117 - loglik: -5.1051e+02 - logprior: 4.1952
Fitted a model with MAP estimate = -506.0043
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 658.1929 - loglik: -5.0924e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 548.7093 - loglik: -5.0685e+02 - logprior: -4.1854e+01
Fitted a model with MAP estimate = -528.6039
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 623.7110 - loglik: -5.0577e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 535.0389 - loglik: -5.0515e+02 - logprior: -2.9894e+01
Fitted a model with MAP estimate = -519.4338
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 629.5403 - loglik: -5.0410e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 538.7657 - loglik: -5.0350e+02 - logprior: -3.5271e+01
Epoch 3/10
10/10 - 1s - loss: 514.2474 - loglik: -5.0393e+02 - logprior: -1.0314e+01
Epoch 4/10
10/10 - 1s - loss: 503.7899 - loglik: -5.0429e+02 - logprior: 0.4987
Epoch 5/10
10/10 - 1s - loss: 499.0543 - loglik: -5.0434e+02 - logprior: 5.2834
Epoch 6/10
10/10 - 1s - loss: 496.4949 - loglik: -5.0432e+02 - logprior: 7.8290
Epoch 7/10
10/10 - 1s - loss: 494.8893 - loglik: -5.0419e+02 - logprior: 9.3040
Epoch 8/10
10/10 - 1s - loss: 493.7738 - loglik: -5.0407e+02 - logprior: 10.2970
Epoch 9/10
10/10 - 1s - loss: 492.9295 - loglik: -5.0402e+02 - logprior: 11.0940
Epoch 10/10
10/10 - 1s - loss: 492.2299 - loglik: -5.0406e+02 - logprior: 11.8344
Fitted a model with MAP estimate = -491.8814
Time for alignment: 35.6580
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 666.4628 - loglik: -5.5331e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 569.5583 - loglik: -5.4226e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 540.4545 - loglik: -5.3063e+02 - logprior: -9.8219e+00
Epoch 4/10
10/10 - 1s - loss: 525.8591 - loglik: -5.2250e+02 - logprior: -3.3629e+00
Epoch 5/10
10/10 - 1s - loss: 518.2465 - loglik: -5.1802e+02 - logprior: -2.2476e-01
Epoch 6/10
10/10 - 1s - loss: 513.0919 - loglik: -5.1461e+02 - logprior: 1.5226
Epoch 7/10
10/10 - 1s - loss: 509.9280 - loglik: -5.1234e+02 - logprior: 2.4092
Epoch 8/10
10/10 - 1s - loss: 508.1389 - loglik: -5.1120e+02 - logprior: 3.0622
Epoch 9/10
10/10 - 1s - loss: 507.0444 - loglik: -5.1071e+02 - logprior: 3.6677
Epoch 10/10
10/10 - 1s - loss: 506.3117 - loglik: -5.1051e+02 - logprior: 4.1952
Fitted a model with MAP estimate = -506.0044
expansions: [(0, 6), (37, 4), (51, 3), (64, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 92 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 658.1927 - loglik: -5.0924e+02 - logprior: -1.4895e+02
Epoch 2/2
10/10 - 1s - loss: 548.7091 - loglik: -5.0685e+02 - logprior: -4.1854e+01
Fitted a model with MAP estimate = -528.6038
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 623.7106 - loglik: -5.0577e+02 - logprior: -1.1794e+02
Epoch 2/2
10/10 - 1s - loss: 535.0385 - loglik: -5.0515e+02 - logprior: -2.9893e+01
Fitted a model with MAP estimate = -519.4330
expansions: [(0, 4), (42, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 629.5403 - loglik: -5.0410e+02 - logprior: -1.2544e+02
Epoch 2/10
10/10 - 1s - loss: 538.7667 - loglik: -5.0350e+02 - logprior: -3.5270e+01
Epoch 3/10
10/10 - 1s - loss: 514.2478 - loglik: -5.0393e+02 - logprior: -1.0314e+01
Epoch 4/10
10/10 - 1s - loss: 503.7899 - loglik: -5.0429e+02 - logprior: 0.4986
Epoch 5/10
10/10 - 1s - loss: 499.0544 - loglik: -5.0434e+02 - logprior: 5.2831
Epoch 6/10
10/10 - 1s - loss: 496.4953 - loglik: -5.0432e+02 - logprior: 7.8285
Epoch 7/10
10/10 - 1s - loss: 494.8896 - loglik: -5.0419e+02 - logprior: 9.3036
Epoch 8/10
10/10 - 1s - loss: 493.7739 - loglik: -5.0407e+02 - logprior: 10.2963
Epoch 9/10
10/10 - 1s - loss: 492.9296 - loglik: -5.0402e+02 - logprior: 11.0927
Epoch 10/10
10/10 - 1s - loss: 492.2303 - loglik: -5.0406e+02 - logprior: 11.8340
Fitted a model with MAP estimate = -491.8818
Time for alignment: 35.2721
Computed alignments with likelihoods: ['-491.8818', '-491.8813', '-491.8815', '-491.8814', '-491.8818']
Best model has likelihood: -491.8813  (prior= 12.2270 )
time for generating output: 0.1324
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7485226526592252
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2022.2987 - loglik: -2.0148e+03 - logprior: -7.5074e+00
Epoch 2/10
19/19 - 17s - loss: 1898.4730 - loglik: -1.8997e+03 - logprior: 1.2257
Epoch 3/10
19/19 - 17s - loss: 1842.7888 - loglik: -1.8429e+03 - logprior: 0.1474
Epoch 4/10
19/19 - 17s - loss: 1823.2262 - loglik: -1.8229e+03 - logprior: -3.2123e-01
Epoch 5/10
19/19 - 17s - loss: 1812.9332 - loglik: -1.8127e+03 - logprior: -2.6711e-01
Epoch 6/10
19/19 - 17s - loss: 1814.0563 - loglik: -1.8137e+03 - logprior: -3.6083e-01
Fitted a model with MAP estimate = -1812.4794
expansions: [(32, 1), (67, 1), (68, 2), (98, 1), (99, 2), (113, 1), (114, 2), (116, 1), (117, 1), (118, 3), (121, 5), (122, 2), (142, 1), (144, 1), (164, 2), (166, 2), (167, 2), (170, 1), (173, 1), (177, 1), (178, 2), (181, 1), (190, 2), (191, 2), (196, 1), (197, 1), (205, 1), (212, 1), (220, 1), (221, 1), (222, 3), (223, 1), (226, 1), (236, 2), (237, 3), (258, 1), (261, 1), (279, 4), (300, 1), (301, 1), (302, 1), (303, 1), (309, 1), (310, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1822.9227 - loglik: -1.8120e+03 - logprior: -1.0939e+01
Epoch 2/2
19/19 - 23s - loss: 1791.0045 - loglik: -1.7885e+03 - logprior: -2.5054e+00
Fitted a model with MAP estimate = -1783.5329
expansions: [(0, 2), (337, 1)]
discards: [  0  68 103 121 138 191 192 210 226 267 287]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 1795.1259 - loglik: -1.7892e+03 - logprior: -5.9311e+00
Epoch 2/2
19/19 - 23s - loss: 1779.6655 - loglik: -1.7821e+03 - logprior: 2.4475
Fitted a model with MAP estimate = -1774.7722
expansions: [(137, 2)]
discards: [  0 321]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 1794.7574 - loglik: -1.7855e+03 - logprior: -9.2698e+00
Epoch 2/10
19/19 - 23s - loss: 1780.3584 - loglik: -1.7809e+03 - logprior: 0.5398
Epoch 3/10
19/19 - 22s - loss: 1777.8024 - loglik: -1.7817e+03 - logprior: 3.8941
Epoch 4/10
19/19 - 22s - loss: 1768.7325 - loglik: -1.7732e+03 - logprior: 4.4960
Epoch 5/10
19/19 - 23s - loss: 1764.1409 - loglik: -1.7689e+03 - logprior: 4.7334
Epoch 6/10
19/19 - 23s - loss: 1766.1246 - loglik: -1.7712e+03 - logprior: 5.0700
Fitted a model with MAP estimate = -1765.0719
Time for alignment: 407.1078
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2025.4255 - loglik: -2.0179e+03 - logprior: -7.5069e+00
Epoch 2/10
19/19 - 17s - loss: 1903.4685 - loglik: -1.9047e+03 - logprior: 1.2644
Epoch 3/10
19/19 - 17s - loss: 1838.9813 - loglik: -1.8394e+03 - logprior: 0.4274
Epoch 4/10
19/19 - 17s - loss: 1827.9988 - loglik: -1.8282e+03 - logprior: 0.2091
Epoch 5/10
19/19 - 17s - loss: 1819.7043 - loglik: -1.8200e+03 - logprior: 0.2538
Epoch 6/10
19/19 - 17s - loss: 1820.5037 - loglik: -1.8206e+03 - logprior: 0.0560
Fitted a model with MAP estimate = -1817.7083
expansions: [(14, 1), (68, 1), (99, 1), (100, 3), (101, 1), (106, 1), (112, 1), (113, 3), (116, 1), (117, 1), (118, 1), (120, 3), (121, 3), (144, 1), (154, 1), (164, 2), (166, 1), (167, 1), (168, 2), (170, 1), (176, 2), (177, 1), (179, 1), (190, 2), (191, 2), (199, 1), (205, 1), (208, 1), (219, 1), (220, 2), (221, 1), (222, 2), (238, 1), (239, 2), (265, 9), (279, 1), (280, 1), (301, 1), (302, 3), (303, 1), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1828.6445 - loglik: -1.8174e+03 - logprior: -1.1213e+01
Epoch 2/2
19/19 - 23s - loss: 1793.7440 - loglik: -1.7911e+03 - logprior: -2.6648e+00
Fitted a model with MAP estimate = -1786.6680
expansions: [(0, 2), (286, 1), (287, 1), (378, 1)]
discards: [  0 102 103 121 194 205 226 318 319 368]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1795.3682 - loglik: -1.7890e+03 - logprior: -6.3422e+00
Epoch 2/2
19/19 - 23s - loss: 1785.6544 - loglik: -1.7882e+03 - logprior: 2.5159
Fitted a model with MAP estimate = -1777.9128
expansions: [(33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1799.9309 - loglik: -1.7901e+03 - logprior: -9.8298e+00
Epoch 2/10
19/19 - 23s - loss: 1782.2900 - loglik: -1.7826e+03 - logprior: 0.2745
Epoch 3/10
19/19 - 23s - loss: 1779.3206 - loglik: -1.7832e+03 - logprior: 3.9249
Epoch 4/10
19/19 - 23s - loss: 1773.6223 - loglik: -1.7783e+03 - logprior: 4.6893
Epoch 5/10
19/19 - 23s - loss: 1767.3602 - loglik: -1.7723e+03 - logprior: 4.9895
Epoch 6/10
19/19 - 23s - loss: 1769.2321 - loglik: -1.7745e+03 - logprior: 5.2221
Fitted a model with MAP estimate = -1768.0120
Time for alignment: 408.7741
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2020.3116 - loglik: -2.0128e+03 - logprior: -7.4904e+00
Epoch 2/10
19/19 - 17s - loss: 1896.7640 - loglik: -1.8978e+03 - logprior: 1.0821
Epoch 3/10
19/19 - 17s - loss: 1844.3265 - loglik: -1.8445e+03 - logprior: 0.1801
Epoch 4/10
19/19 - 17s - loss: 1816.6205 - loglik: -1.8164e+03 - logprior: -2.0639e-01
Epoch 5/10
19/19 - 17s - loss: 1821.4320 - loglik: -1.8212e+03 - logprior: -2.3409e-01
Fitted a model with MAP estimate = -1813.9620
expansions: [(30, 2), (69, 1), (99, 1), (100, 3), (114, 2), (115, 2), (118, 1), (119, 1), (120, 1), (122, 7), (124, 2), (144, 1), (146, 1), (159, 1), (166, 1), (167, 2), (168, 2), (169, 2), (171, 1), (173, 1), (176, 2), (177, 1), (178, 1), (190, 1), (191, 1), (197, 1), (198, 1), (199, 1), (204, 1), (219, 3), (220, 1), (221, 1), (222, 2), (224, 1), (237, 4), (264, 4), (281, 1), (282, 1), (289, 6), (302, 2), (303, 2), (304, 1), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 1825.5024 - loglik: -1.8141e+03 - logprior: -1.1393e+01
Epoch 2/2
19/19 - 24s - loss: 1786.4198 - loglik: -1.7831e+03 - logprior: -3.3475e+00
Fitted a model with MAP estimate = -1777.4588
expansions: [(0, 2), (291, 1), (321, 1), (388, 1)]
discards: [  0  29 103 104 120 139 140 194 199 351 352 353]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1790.3828 - loglik: -1.7836e+03 - logprior: -6.7574e+00
Epoch 2/2
19/19 - 23s - loss: 1770.7632 - loglik: -1.7727e+03 - logprior: 1.9543
Fitted a model with MAP estimate = -1769.3631
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1789.4309 - loglik: -1.7794e+03 - logprior: -1.0046e+01
Epoch 2/10
19/19 - 23s - loss: 1776.6689 - loglik: -1.7764e+03 - logprior: -2.4061e-01
Epoch 3/10
19/19 - 23s - loss: 1770.5625 - loglik: -1.7743e+03 - logprior: 3.7659
Epoch 4/10
19/19 - 23s - loss: 1765.4396 - loglik: -1.7698e+03 - logprior: 4.3579
Epoch 5/10
19/19 - 23s - loss: 1759.5537 - loglik: -1.7642e+03 - logprior: 4.6297
Epoch 6/10
19/19 - 23s - loss: 1757.3812 - loglik: -1.7622e+03 - logprior: 4.8581
Epoch 7/10
19/19 - 23s - loss: 1760.8732 - loglik: -1.7660e+03 - logprior: 5.1294
Fitted a model with MAP estimate = -1759.4245
Time for alignment: 422.2315
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2023.1593 - loglik: -2.0157e+03 - logprior: -7.5003e+00
Epoch 2/10
19/19 - 17s - loss: 1896.5627 - loglik: -1.8977e+03 - logprior: 1.1585
Epoch 3/10
19/19 - 17s - loss: 1837.2074 - loglik: -1.8374e+03 - logprior: 0.1728
Epoch 4/10
19/19 - 17s - loss: 1817.5465 - loglik: -1.8174e+03 - logprior: -1.5192e-01
Epoch 5/10
19/19 - 17s - loss: 1809.3143 - loglik: -1.8092e+03 - logprior: -1.2421e-01
Epoch 6/10
19/19 - 17s - loss: 1821.3263 - loglik: -1.8211e+03 - logprior: -2.7067e-01
Fitted a model with MAP estimate = -1810.0335
expansions: [(32, 2), (43, 1), (67, 1), (98, 1), (101, 1), (113, 1), (114, 1), (118, 1), (119, 1), (120, 1), (123, 6), (143, 1), (145, 1), (165, 1), (168, 1), (169, 1), (170, 1), (172, 1), (178, 2), (179, 1), (180, 1), (181, 1), (191, 1), (192, 1), (198, 1), (199, 1), (206, 1), (209, 1), (220, 1), (221, 2), (222, 1), (223, 2), (225, 1), (240, 1), (242, 1), (243, 1), (267, 6), (301, 1), (302, 3), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1822.8885 - loglik: -1.8121e+03 - logprior: -1.0752e+01
Epoch 2/2
19/19 - 22s - loss: 1790.9279 - loglik: -1.7889e+03 - logprior: -2.0420e+00
Fitted a model with MAP estimate = -1785.0296
expansions: [(0, 2)]
discards: [  0  32 201]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 1796.0706 - loglik: -1.7897e+03 - logprior: -6.4062e+00
Epoch 2/2
19/19 - 22s - loss: 1785.8053 - loglik: -1.7881e+03 - logprior: 2.3279
Fitted a model with MAP estimate = -1777.4738
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 1797.2230 - loglik: -1.7877e+03 - logprior: -9.5584e+00
Epoch 2/10
19/19 - 22s - loss: 1785.8557 - loglik: -1.7864e+03 - logprior: 0.5526
Epoch 3/10
19/19 - 22s - loss: 1772.9186 - loglik: -1.7769e+03 - logprior: 3.9480
Epoch 4/10
19/19 - 22s - loss: 1769.6620 - loglik: -1.7742e+03 - logprior: 4.5427
Epoch 5/10
19/19 - 22s - loss: 1775.4357 - loglik: -1.7802e+03 - logprior: 4.7847
Fitted a model with MAP estimate = -1769.2952
Time for alignment: 376.3741
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 2027.8281 - loglik: -2.0203e+03 - logprior: -7.4853e+00
Epoch 2/10
19/19 - 17s - loss: 1897.0406 - loglik: -1.8982e+03 - logprior: 1.1571
Epoch 3/10
19/19 - 17s - loss: 1844.0802 - loglik: -1.8443e+03 - logprior: 0.2131
Epoch 4/10
19/19 - 17s - loss: 1825.6168 - loglik: -1.8258e+03 - logprior: 0.1422
Epoch 5/10
19/19 - 17s - loss: 1816.1533 - loglik: -1.8163e+03 - logprior: 0.1113
Epoch 6/10
19/19 - 17s - loss: 1822.5258 - loglik: -1.8225e+03 - logprior: -4.4663e-02
Fitted a model with MAP estimate = -1816.4531
expansions: [(33, 1), (48, 1), (76, 2), (96, 1), (97, 1), (98, 2), (106, 1), (112, 1), (113, 1), (116, 1), (117, 1), (118, 1), (120, 6), (122, 2), (144, 1), (158, 1), (164, 2), (166, 1), (167, 1), (168, 2), (170, 1), (176, 2), (177, 1), (178, 1), (190, 2), (191, 2), (198, 1), (199, 1), (204, 1), (222, 3), (224, 1), (237, 1), (238, 1), (242, 2), (260, 1), (265, 4), (301, 2), (302, 2), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1829.1227 - loglik: -1.8183e+03 - logprior: -1.0799e+01
Epoch 2/2
19/19 - 23s - loss: 1799.3721 - loglik: -1.7967e+03 - logprior: -2.6549e+00
Fitted a model with MAP estimate = -1790.5506
expansions: [(0, 2), (316, 2), (317, 2)]
discards: [  0  33  77 104 137 138 195 206 227]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 1800.0321 - loglik: -1.7933e+03 - logprior: -6.7801e+00
Epoch 2/2
19/19 - 22s - loss: 1789.2094 - loglik: -1.7914e+03 - logprior: 2.2110
Fitted a model with MAP estimate = -1781.6859
expansions: [(30, 1), (137, 1), (252, 2), (256, 1)]
discards: [  0 312 352]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 1800.5721 - loglik: -1.7912e+03 - logprior: -9.3605e+00
Epoch 2/10
19/19 - 22s - loss: 1789.8293 - loglik: -1.7900e+03 - logprior: 0.2046
Epoch 3/10
19/19 - 22s - loss: 1777.3873 - loglik: -1.7813e+03 - logprior: 3.8988
Epoch 4/10
19/19 - 23s - loss: 1775.8468 - loglik: -1.7803e+03 - logprior: 4.4589
Epoch 5/10
19/19 - 22s - loss: 1774.3975 - loglik: -1.7792e+03 - logprior: 4.7900
Epoch 6/10
19/19 - 22s - loss: 1765.5366 - loglik: -1.7705e+03 - logprior: 4.9732
Epoch 7/10
19/19 - 23s - loss: 1762.8477 - loglik: -1.7682e+03 - logprior: 5.3046
Epoch 8/10
19/19 - 22s - loss: 1774.3672 - loglik: -1.7799e+03 - logprior: 5.5071
Fitted a model with MAP estimate = -1767.5841
Time for alignment: 447.8796
Computed alignments with likelihoods: ['-1765.0719', '-1768.0120', '-1759.4245', '-1769.2952', '-1767.5841']
Best model has likelihood: -1759.4245  (prior= 5.2724 )
time for generating output: 0.4081
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7234074519230769
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 694.6261 - loglik: -6.3837e+02 - logprior: -5.6259e+01
Epoch 2/10
10/10 - 1s - loss: 615.5575 - loglik: -6.0221e+02 - logprior: -1.3348e+01
Epoch 3/10
10/10 - 1s - loss: 565.2973 - loglik: -5.5948e+02 - logprior: -5.8156e+00
Epoch 4/10
10/10 - 1s - loss: 537.8073 - loglik: -5.3415e+02 - logprior: -3.6597e+00
Epoch 5/10
10/10 - 1s - loss: 529.2852 - loglik: -5.2685e+02 - logprior: -2.4309e+00
Epoch 6/10
10/10 - 1s - loss: 524.9845 - loglik: -5.2337e+02 - logprior: -1.6152e+00
Epoch 7/10
10/10 - 1s - loss: 522.3969 - loglik: -5.2128e+02 - logprior: -1.1214e+00
Epoch 8/10
10/10 - 1s - loss: 522.0406 - loglik: -5.2120e+02 - logprior: -8.3569e-01
Epoch 9/10
10/10 - 1s - loss: 521.1459 - loglik: -5.2052e+02 - logprior: -6.2949e-01
Epoch 10/10
10/10 - 1s - loss: 520.7750 - loglik: -5.2033e+02 - logprior: -4.4386e-01
Fitted a model with MAP estimate = -520.5676
expansions: [(11, 4), (17, 3), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (56, 5), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 578.0726 - loglik: -5.1473e+02 - logprior: -6.3345e+01
Epoch 2/2
10/10 - 1s - loss: 524.8430 - loglik: -5.0023e+02 - logprior: -2.4609e+01
Fitted a model with MAP estimate = -516.1682
expansions: [(0, 2), (13, 1)]
discards: [ 0 20 21 22 98]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 548.8146 - loglik: -4.9894e+02 - logprior: -4.9876e+01
Epoch 2/2
10/10 - 1s - loss: 506.9594 - loglik: -4.9591e+02 - logprior: -1.1048e+01
Fitted a model with MAP estimate = -500.5419
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 558.6542 - loglik: -4.9815e+02 - logprior: -6.0506e+01
Epoch 2/10
10/10 - 1s - loss: 513.9507 - loglik: -4.9681e+02 - logprior: -1.7138e+01
Epoch 3/10
10/10 - 1s - loss: 499.8519 - loglik: -4.9561e+02 - logprior: -4.2369e+00
Epoch 4/10
10/10 - 1s - loss: 496.8084 - loglik: -4.9700e+02 - logprior: 0.1868
Epoch 5/10
10/10 - 1s - loss: 493.4842 - loglik: -4.9555e+02 - logprior: 2.0630
Epoch 6/10
10/10 - 1s - loss: 492.9051 - loglik: -4.9608e+02 - logprior: 3.1745
Epoch 7/10
10/10 - 1s - loss: 491.9372 - loglik: -4.9599e+02 - logprior: 4.0559
Epoch 8/10
10/10 - 1s - loss: 492.1383 - loglik: -4.9685e+02 - logprior: 4.7162
Fitted a model with MAP estimate = -491.5520
Time for alignment: 42.0715
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 694.8593 - loglik: -6.3860e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 614.7131 - loglik: -6.0135e+02 - logprior: -1.3362e+01
Epoch 3/10
10/10 - 1s - loss: 567.4003 - loglik: -5.6146e+02 - logprior: -5.9392e+00
Epoch 4/10
10/10 - 1s - loss: 538.8959 - loglik: -5.3498e+02 - logprior: -3.9164e+00
Epoch 5/10
10/10 - 1s - loss: 528.1417 - loglik: -5.2534e+02 - logprior: -2.8058e+00
Epoch 6/10
10/10 - 1s - loss: 524.3297 - loglik: -5.2226e+02 - logprior: -2.0692e+00
Epoch 7/10
10/10 - 1s - loss: 522.0909 - loglik: -5.2054e+02 - logprior: -1.5509e+00
Epoch 8/10
10/10 - 1s - loss: 522.0223 - loglik: -5.2079e+02 - logprior: -1.2351e+00
Epoch 9/10
10/10 - 1s - loss: 521.1015 - loglik: -5.2008e+02 - logprior: -1.0253e+00
Epoch 10/10
10/10 - 1s - loss: 520.7523 - loglik: -5.1991e+02 - logprior: -8.3873e-01
Fitted a model with MAP estimate = -520.5877
expansions: [(11, 3), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (59, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 577.6588 - loglik: -5.1430e+02 - logprior: -6.3360e+01
Epoch 2/2
10/10 - 1s - loss: 525.5141 - loglik: -5.0089e+02 - logprior: -2.4624e+01
Fitted a model with MAP estimate = -516.3429
expansions: [(0, 2), (12, 1), (13, 1), (69, 1)]
discards: [ 0 19 20 21 98]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 547.8185 - loglik: -4.9800e+02 - logprior: -4.9819e+01
Epoch 2/2
10/10 - 1s - loss: 506.1731 - loglik: -4.9527e+02 - logprior: -1.0906e+01
Fitted a model with MAP estimate = -499.0038
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 556.8674 - loglik: -4.9649e+02 - logprior: -6.0382e+01
Epoch 2/10
10/10 - 1s - loss: 512.3097 - loglik: -4.9512e+02 - logprior: -1.7191e+01
Epoch 3/10
10/10 - 1s - loss: 498.7739 - loglik: -4.9459e+02 - logprior: -4.1838e+00
Epoch 4/10
10/10 - 1s - loss: 494.1766 - loglik: -4.9450e+02 - logprior: 0.3264
Epoch 5/10
10/10 - 1s - loss: 491.8969 - loglik: -4.9409e+02 - logprior: 2.1949
Epoch 6/10
10/10 - 1s - loss: 491.4257 - loglik: -4.9473e+02 - logprior: 3.3045
Epoch 7/10
10/10 - 1s - loss: 489.5759 - loglik: -4.9376e+02 - logprior: 4.1796
Epoch 8/10
10/10 - 1s - loss: 490.0054 - loglik: -4.9484e+02 - logprior: 4.8389
Fitted a model with MAP estimate = -489.5410
Time for alignment: 42.6359
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 694.3572 - loglik: -6.3810e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 616.5753 - loglik: -6.0322e+02 - logprior: -1.3358e+01
Epoch 3/10
10/10 - 1s - loss: 567.6449 - loglik: -5.6172e+02 - logprior: -5.9294e+00
Epoch 4/10
10/10 - 1s - loss: 538.7284 - loglik: -5.3492e+02 - logprior: -3.8056e+00
Epoch 5/10
10/10 - 1s - loss: 529.1292 - loglik: -5.2649e+02 - logprior: -2.6364e+00
Epoch 6/10
10/10 - 1s - loss: 523.6735 - loglik: -5.2174e+02 - logprior: -1.9310e+00
Epoch 7/10
10/10 - 1s - loss: 522.3830 - loglik: -5.2089e+02 - logprior: -1.4935e+00
Epoch 8/10
10/10 - 1s - loss: 520.0696 - loglik: -5.1887e+02 - logprior: -1.2031e+00
Epoch 9/10
10/10 - 1s - loss: 519.9022 - loglik: -5.1893e+02 - logprior: -9.6833e-01
Epoch 10/10
10/10 - 1s - loss: 519.7928 - loglik: -5.1901e+02 - logprior: -7.7849e-01
Fitted a model with MAP estimate = -519.3271
expansions: [(13, 5), (17, 2), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (62, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 574.8187 - loglik: -5.1158e+02 - logprior: -6.3236e+01
Epoch 2/2
10/10 - 1s - loss: 523.2749 - loglik: -4.9880e+02 - logprior: -2.4473e+01
Fitted a model with MAP estimate = -514.8886
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 546.2680 - loglik: -4.9651e+02 - logprior: -4.9757e+01
Epoch 2/2
10/10 - 1s - loss: 505.4251 - loglik: -4.9458e+02 - logprior: -1.0849e+01
Fitted a model with MAP estimate = -498.7588
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 556.8395 - loglik: -4.9660e+02 - logprior: -6.0240e+01
Epoch 2/10
10/10 - 1s - loss: 512.0057 - loglik: -4.9509e+02 - logprior: -1.6914e+01
Epoch 3/10
10/10 - 1s - loss: 498.6553 - loglik: -4.9462e+02 - logprior: -4.0309e+00
Epoch 4/10
10/10 - 1s - loss: 493.3532 - loglik: -4.9373e+02 - logprior: 0.3778
Epoch 5/10
10/10 - 1s - loss: 491.2113 - loglik: -4.9346e+02 - logprior: 2.2444
Epoch 6/10
10/10 - 1s - loss: 491.3804 - loglik: -4.9472e+02 - logprior: 3.3356
Fitted a model with MAP estimate = -490.3346
Time for alignment: 38.0203
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 694.6676 - loglik: -6.3841e+02 - logprior: -5.6257e+01
Epoch 2/10
10/10 - 1s - loss: 615.5397 - loglik: -6.0217e+02 - logprior: -1.3366e+01
Epoch 3/10
10/10 - 1s - loss: 568.3141 - loglik: -5.6235e+02 - logprior: -5.9666e+00
Epoch 4/10
10/10 - 1s - loss: 541.1288 - loglik: -5.3713e+02 - logprior: -3.9952e+00
Epoch 5/10
10/10 - 1s - loss: 530.4830 - loglik: -5.2766e+02 - logprior: -2.8225e+00
Epoch 6/10
10/10 - 1s - loss: 525.0316 - loglik: -5.2297e+02 - logprior: -2.0645e+00
Epoch 7/10
10/10 - 1s - loss: 522.3214 - loglik: -5.2065e+02 - logprior: -1.6751e+00
Epoch 8/10
10/10 - 1s - loss: 520.8318 - loglik: -5.1940e+02 - logprior: -1.4330e+00
Epoch 9/10
10/10 - 1s - loss: 520.4817 - loglik: -5.1926e+02 - logprior: -1.2225e+00
Epoch 10/10
10/10 - 1s - loss: 520.2892 - loglik: -5.1926e+02 - logprior: -1.0327e+00
Fitted a model with MAP estimate = -520.0948
expansions: [(11, 1), (13, 4), (17, 2), (19, 1), (32, 1), (33, 1), (34, 1), (53, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 575.9010 - loglik: -5.1249e+02 - logprior: -6.3414e+01
Epoch 2/2
10/10 - 1s - loss: 524.6561 - loglik: -5.0000e+02 - logprior: -2.4652e+01
Fitted a model with MAP estimate = -515.8175
expansions: [(0, 2), (69, 1)]
discards: [ 0 21 22 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 547.4688 - loglik: -4.9757e+02 - logprior: -4.9901e+01
Epoch 2/2
10/10 - 1s - loss: 505.3578 - loglik: -4.9434e+02 - logprior: -1.1014e+01
Fitted a model with MAP estimate = -499.3677
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 556.7262 - loglik: -4.9632e+02 - logprior: -6.0404e+01
Epoch 2/10
10/10 - 1s - loss: 512.1556 - loglik: -4.9514e+02 - logprior: -1.7013e+01
Epoch 3/10
10/10 - 1s - loss: 499.2694 - loglik: -4.9508e+02 - logprior: -4.1873e+00
Epoch 4/10
10/10 - 1s - loss: 494.5361 - loglik: -4.9477e+02 - logprior: 0.2326
Epoch 5/10
10/10 - 1s - loss: 492.3659 - loglik: -4.9446e+02 - logprior: 2.0905
Epoch 6/10
10/10 - 1s - loss: 491.0515 - loglik: -4.9425e+02 - logprior: 3.1967
Epoch 7/10
10/10 - 1s - loss: 490.1877 - loglik: -4.9427e+02 - logprior: 4.0865
Epoch 8/10
10/10 - 1s - loss: 490.0466 - loglik: -4.9478e+02 - logprior: 4.7370
Epoch 9/10
10/10 - 1s - loss: 489.6464 - loglik: -4.9483e+02 - logprior: 5.1786
Epoch 10/10
10/10 - 1s - loss: 489.3594 - loglik: -4.9487e+02 - logprior: 5.5146
Fitted a model with MAP estimate = -489.1559
Time for alignment: 41.9688
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 694.8026 - loglik: -6.3855e+02 - logprior: -5.6257e+01
Epoch 2/10
10/10 - 1s - loss: 615.0786 - loglik: -6.0171e+02 - logprior: -1.3366e+01
Epoch 3/10
10/10 - 1s - loss: 567.2700 - loglik: -5.6129e+02 - logprior: -5.9808e+00
Epoch 4/10
10/10 - 1s - loss: 538.0363 - loglik: -5.3426e+02 - logprior: -3.7753e+00
Epoch 5/10
10/10 - 1s - loss: 528.4563 - loglik: -5.2581e+02 - logprior: -2.6416e+00
Epoch 6/10
10/10 - 1s - loss: 523.7674 - loglik: -5.2178e+02 - logprior: -1.9881e+00
Epoch 7/10
10/10 - 1s - loss: 522.4464 - loglik: -5.2096e+02 - logprior: -1.4844e+00
Epoch 8/10
10/10 - 1s - loss: 520.9502 - loglik: -5.1980e+02 - logprior: -1.1463e+00
Epoch 9/10
10/10 - 1s - loss: 520.4584 - loglik: -5.1953e+02 - logprior: -9.2564e-01
Epoch 10/10
10/10 - 1s - loss: 519.7620 - loglik: -5.1894e+02 - logprior: -8.2650e-01
Fitted a model with MAP estimate = -519.3066
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (63, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 574.7264 - loglik: -5.1145e+02 - logprior: -6.3276e+01
Epoch 2/2
10/10 - 1s - loss: 524.2702 - loglik: -4.9979e+02 - logprior: -2.4476e+01
Fitted a model with MAP estimate = -514.9808
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 546.6278 - loglik: -4.9686e+02 - logprior: -4.9765e+01
Epoch 2/2
10/10 - 1s - loss: 504.6854 - loglik: -4.9383e+02 - logprior: -1.0855e+01
Fitted a model with MAP estimate = -498.7628
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 556.4970 - loglik: -4.9626e+02 - logprior: -6.0239e+01
Epoch 2/10
10/10 - 1s - loss: 512.4717 - loglik: -4.9553e+02 - logprior: -1.6945e+01
Epoch 3/10
10/10 - 1s - loss: 498.3500 - loglik: -4.9431e+02 - logprior: -4.0443e+00
Epoch 4/10
10/10 - 1s - loss: 493.5580 - loglik: -4.9395e+02 - logprior: 0.3874
Epoch 5/10
10/10 - 1s - loss: 491.9156 - loglik: -4.9418e+02 - logprior: 2.2600
Epoch 6/10
10/10 - 1s - loss: 491.2287 - loglik: -4.9460e+02 - logprior: 3.3690
Epoch 7/10
10/10 - 1s - loss: 490.2372 - loglik: -4.9449e+02 - logprior: 4.2488
Epoch 8/10
10/10 - 1s - loss: 489.4252 - loglik: -4.9432e+02 - logprior: 4.8933
Epoch 9/10
10/10 - 1s - loss: 489.3792 - loglik: -4.9471e+02 - logprior: 5.3332
Epoch 10/10
10/10 - 1s - loss: 488.1543 - loglik: -4.9383e+02 - logprior: 5.6708
Fitted a model with MAP estimate = -488.7613
Time for alignment: 41.5610
Computed alignments with likelihoods: ['-491.5520', '-489.5410', '-490.3346', '-489.1559', '-488.7613']
Best model has likelihood: -488.7613  (prior= 5.8433 )
time for generating output: 0.1412
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9311943393924607
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 795.7011 - loglik: -7.8765e+02 - logprior: -8.0556e+00
Epoch 2/10
13/13 - 2s - loss: 752.5010 - loglik: -7.5068e+02 - logprior: -1.8168e+00
Epoch 3/10
13/13 - 2s - loss: 724.6171 - loglik: -7.2310e+02 - logprior: -1.5122e+00
Epoch 4/10
13/13 - 2s - loss: 711.5231 - loglik: -7.0981e+02 - logprior: -1.7123e+00
Epoch 5/10
13/13 - 2s - loss: 705.6884 - loglik: -7.0403e+02 - logprior: -1.6563e+00
Epoch 6/10
13/13 - 2s - loss: 703.0947 - loglik: -7.0149e+02 - logprior: -1.6006e+00
Epoch 7/10
13/13 - 2s - loss: 700.6058 - loglik: -6.9898e+02 - logprior: -1.6219e+00
Epoch 8/10
13/13 - 2s - loss: 701.7181 - loglik: -7.0010e+02 - logprior: -1.6152e+00
Fitted a model with MAP estimate = -700.5748
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (82, 1), (92, 1), (99, 1), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 712.6027 - loglik: -7.0321e+02 - logprior: -9.3879e+00
Epoch 2/2
13/13 - 3s - loss: 696.4375 - loglik: -6.9244e+02 - logprior: -3.9927e+00
Fitted a model with MAP estimate = -693.4770
expansions: [(0, 2)]
discards: [  0  23  37  81 127]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 697.8740 - loglik: -6.9087e+02 - logprior: -7.0056e+00
Epoch 2/2
13/13 - 2s - loss: 689.8300 - loglik: -6.8816e+02 - logprior: -1.6669e+00
Fitted a model with MAP estimate = -688.4663
expansions: []
discards: [ 0 73]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 700.9975 - loglik: -6.9223e+02 - logprior: -8.7656e+00
Epoch 2/10
13/13 - 2s - loss: 691.6006 - loglik: -6.8930e+02 - logprior: -2.3034e+00
Epoch 3/10
13/13 - 2s - loss: 689.5286 - loglik: -6.8866e+02 - logprior: -8.6442e-01
Epoch 4/10
13/13 - 2s - loss: 686.7859 - loglik: -6.8636e+02 - logprior: -4.2905e-01
Epoch 5/10
13/13 - 2s - loss: 686.5127 - loglik: -6.8623e+02 - logprior: -2.8725e-01
Epoch 6/10
13/13 - 2s - loss: 687.1154 - loglik: -6.8690e+02 - logprior: -2.1830e-01
Fitted a model with MAP estimate = -685.7772
Time for alignment: 67.1139
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 794.6790 - loglik: -7.8662e+02 - logprior: -8.0637e+00
Epoch 2/10
13/13 - 2s - loss: 754.5530 - loglik: -7.5274e+02 - logprior: -1.8162e+00
Epoch 3/10
13/13 - 2s - loss: 725.8985 - loglik: -7.2441e+02 - logprior: -1.4914e+00
Epoch 4/10
13/13 - 2s - loss: 713.4778 - loglik: -7.1176e+02 - logprior: -1.7139e+00
Epoch 5/10
13/13 - 2s - loss: 706.7670 - loglik: -7.0516e+02 - logprior: -1.6112e+00
Epoch 6/10
13/13 - 2s - loss: 703.6520 - loglik: -7.0211e+02 - logprior: -1.5412e+00
Epoch 7/10
13/13 - 2s - loss: 703.4434 - loglik: -7.0187e+02 - logprior: -1.5775e+00
Epoch 8/10
13/13 - 2s - loss: 702.0589 - loglik: -7.0048e+02 - logprior: -1.5757e+00
Epoch 9/10
13/13 - 2s - loss: 703.2719 - loglik: -7.0171e+02 - logprior: -1.5590e+00
Fitted a model with MAP estimate = -702.1182
expansions: [(6, 1), (15, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (79, 1), (80, 1), (81, 2), (82, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 713.6116 - loglik: -7.0420e+02 - logprior: -9.4077e+00
Epoch 2/2
13/13 - 2s - loss: 697.7123 - loglik: -6.9370e+02 - logprior: -4.0143e+00
Fitted a model with MAP estimate = -694.2826
expansions: [(0, 2)]
discards: [  0  36  73  74  81 124]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 698.3457 - loglik: -6.9137e+02 - logprior: -6.9735e+00
Epoch 2/2
13/13 - 2s - loss: 692.0173 - loglik: -6.9038e+02 - logprior: -1.6338e+00
Fitted a model with MAP estimate = -689.8010
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 700.7465 - loglik: -6.9200e+02 - logprior: -8.7421e+00
Epoch 2/10
13/13 - 2s - loss: 694.0178 - loglik: -6.9174e+02 - logprior: -2.2780e+00
Epoch 3/10
13/13 - 2s - loss: 689.8892 - loglik: -6.8903e+02 - logprior: -8.6391e-01
Epoch 4/10
13/13 - 2s - loss: 688.6287 - loglik: -6.8818e+02 - logprior: -4.4677e-01
Epoch 5/10
13/13 - 2s - loss: 687.7715 - loglik: -6.8749e+02 - logprior: -2.8289e-01
Epoch 6/10
13/13 - 2s - loss: 687.0167 - loglik: -6.8680e+02 - logprior: -2.1891e-01
Epoch 7/10
13/13 - 2s - loss: 686.5137 - loglik: -6.8631e+02 - logprior: -2.0086e-01
Epoch 8/10
13/13 - 2s - loss: 686.4011 - loglik: -6.8623e+02 - logprior: -1.7444e-01
Epoch 9/10
13/13 - 2s - loss: 686.8969 - loglik: -6.8676e+02 - logprior: -1.3921e-01
Fitted a model with MAP estimate = -686.3321
Time for alignment: 75.0759
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 795.1112 - loglik: -7.8707e+02 - logprior: -8.0460e+00
Epoch 2/10
13/13 - 2s - loss: 753.9456 - loglik: -7.5214e+02 - logprior: -1.8091e+00
Epoch 3/10
13/13 - 2s - loss: 727.8577 - loglik: -7.2637e+02 - logprior: -1.4910e+00
Epoch 4/10
13/13 - 2s - loss: 714.4567 - loglik: -7.1276e+02 - logprior: -1.6938e+00
Epoch 5/10
13/13 - 2s - loss: 706.9882 - loglik: -7.0531e+02 - logprior: -1.6748e+00
Epoch 6/10
13/13 - 2s - loss: 705.6591 - loglik: -7.0404e+02 - logprior: -1.6183e+00
Epoch 7/10
13/13 - 2s - loss: 704.0611 - loglik: -7.0242e+02 - logprior: -1.6388e+00
Epoch 8/10
13/13 - 2s - loss: 703.9504 - loglik: -7.0231e+02 - logprior: -1.6396e+00
Epoch 9/10
13/13 - 2s - loss: 703.4965 - loglik: -7.0189e+02 - logprior: -1.6113e+00
Epoch 10/10
13/13 - 2s - loss: 703.5089 - loglik: -7.0191e+02 - logprior: -1.6012e+00
Fitted a model with MAP estimate = -702.9074
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (27, 3), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (80, 1), (81, 1), (82, 2), (83, 1), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 715.7888 - loglik: -7.0633e+02 - logprior: -9.4540e+00
Epoch 2/2
13/13 - 3s - loss: 698.2416 - loglik: -6.9418e+02 - logprior: -4.0650e+00
Fitted a model with MAP estimate = -695.9868
expansions: [(0, 2)]
discards: [  0  34  74  81 101]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 702.0094 - loglik: -6.9499e+02 - logprior: -7.0239e+00
Epoch 2/2
13/13 - 2s - loss: 691.9958 - loglik: -6.9030e+02 - logprior: -1.6951e+00
Fitted a model with MAP estimate = -691.3813
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 703.4990 - loglik: -6.9469e+02 - logprior: -8.8075e+00
Epoch 2/10
13/13 - 2s - loss: 693.7526 - loglik: -6.9144e+02 - logprior: -2.3132e+00
Epoch 3/10
13/13 - 2s - loss: 690.5319 - loglik: -6.8963e+02 - logprior: -8.9996e-01
Epoch 4/10
13/13 - 2s - loss: 689.4944 - loglik: -6.8901e+02 - logprior: -4.8214e-01
Epoch 5/10
13/13 - 2s - loss: 688.5887 - loglik: -6.8827e+02 - logprior: -3.2288e-01
Epoch 6/10
13/13 - 2s - loss: 688.3754 - loglik: -6.8810e+02 - logprior: -2.7056e-01
Epoch 7/10
13/13 - 2s - loss: 687.3889 - loglik: -6.8713e+02 - logprior: -2.5720e-01
Epoch 8/10
13/13 - 2s - loss: 687.3630 - loglik: -6.8714e+02 - logprior: -2.1935e-01
Epoch 9/10
13/13 - 2s - loss: 687.5333 - loglik: -6.8734e+02 - logprior: -1.9749e-01
Fitted a model with MAP estimate = -687.1615
Time for alignment: 77.1326
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 794.8835 - loglik: -7.8682e+02 - logprior: -8.0620e+00
Epoch 2/10
13/13 - 2s - loss: 754.4641 - loglik: -7.5266e+02 - logprior: -1.8077e+00
Epoch 3/10
13/13 - 2s - loss: 728.9150 - loglik: -7.2744e+02 - logprior: -1.4736e+00
Epoch 4/10
13/13 - 2s - loss: 713.6553 - loglik: -7.1199e+02 - logprior: -1.6695e+00
Epoch 5/10
13/13 - 2s - loss: 709.8799 - loglik: -7.0834e+02 - logprior: -1.5435e+00
Epoch 6/10
13/13 - 2s - loss: 707.5696 - loglik: -7.0610e+02 - logprior: -1.4679e+00
Epoch 7/10
13/13 - 2s - loss: 705.6041 - loglik: -7.0409e+02 - logprior: -1.5167e+00
Epoch 8/10
13/13 - 2s - loss: 704.6920 - loglik: -7.0318e+02 - logprior: -1.5163e+00
Epoch 9/10
13/13 - 2s - loss: 705.2079 - loglik: -7.0369e+02 - logprior: -1.5130e+00
Fitted a model with MAP estimate = -704.6056
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 2), (51, 1), (52, 1), (55, 1), (57, 1), (64, 2), (81, 4), (93, 1), (99, 2), (100, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 716.9265 - loglik: -7.0745e+02 - logprior: -9.4789e+00
Epoch 2/2
13/13 - 3s - loss: 700.1908 - loglik: -6.9606e+02 - logprior: -4.1357e+00
Fitted a model with MAP estimate = -696.8436
expansions: [(0, 2)]
discards: [  0  23  37  44  81 124 127]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 700.5240 - loglik: -6.9352e+02 - logprior: -6.9996e+00
Epoch 2/2
13/13 - 2s - loss: 692.9089 - loglik: -6.9126e+02 - logprior: -1.6501e+00
Fitted a model with MAP estimate = -690.8623
expansions: [(121, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 702.9207 - loglik: -6.9395e+02 - logprior: -8.9679e+00
Epoch 2/10
13/13 - 2s - loss: 693.9916 - loglik: -6.9141e+02 - logprior: -2.5811e+00
Epoch 3/10
13/13 - 2s - loss: 689.7356 - loglik: -6.8884e+02 - logprior: -8.9787e-01
Epoch 4/10
13/13 - 2s - loss: 688.8389 - loglik: -6.8838e+02 - logprior: -4.5613e-01
Epoch 5/10
13/13 - 2s - loss: 687.6755 - loglik: -6.8737e+02 - logprior: -3.0997e-01
Epoch 6/10
13/13 - 2s - loss: 686.9797 - loglik: -6.8673e+02 - logprior: -2.5266e-01
Epoch 7/10
13/13 - 2s - loss: 686.7324 - loglik: -6.8650e+02 - logprior: -2.2951e-01
Epoch 8/10
13/13 - 2s - loss: 686.5461 - loglik: -6.8632e+02 - logprior: -2.2219e-01
Epoch 9/10
13/13 - 2s - loss: 686.1042 - loglik: -6.8591e+02 - logprior: -1.9026e-01
Epoch 10/10
13/13 - 2s - loss: 686.9485 - loglik: -6.8680e+02 - logprior: -1.4600e-01
Fitted a model with MAP estimate = -686.1267
Time for alignment: 77.4154
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 795.3811 - loglik: -7.8732e+02 - logprior: -8.0580e+00
Epoch 2/10
13/13 - 2s - loss: 753.5830 - loglik: -7.5177e+02 - logprior: -1.8090e+00
Epoch 3/10
13/13 - 2s - loss: 727.6688 - loglik: -7.2617e+02 - logprior: -1.5003e+00
Epoch 4/10
13/13 - 2s - loss: 712.2039 - loglik: -7.1044e+02 - logprior: -1.7628e+00
Epoch 5/10
13/13 - 2s - loss: 706.0084 - loglik: -7.0432e+02 - logprior: -1.6884e+00
Epoch 6/10
13/13 - 2s - loss: 704.2473 - loglik: -7.0265e+02 - logprior: -1.5959e+00
Epoch 7/10
13/13 - 2s - loss: 702.2399 - loglik: -7.0063e+02 - logprior: -1.6088e+00
Epoch 8/10
13/13 - 2s - loss: 702.8778 - loglik: -7.0126e+02 - logprior: -1.6190e+00
Fitted a model with MAP estimate = -701.6758
expansions: [(12, 1), (15, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (45, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (81, 1), (82, 1), (83, 1), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 714.2773 - loglik: -7.0482e+02 - logprior: -9.4537e+00
Epoch 2/2
13/13 - 3s - loss: 698.4272 - loglik: -6.9437e+02 - logprior: -4.0570e+00
Fitted a model with MAP estimate = -695.0067
expansions: [(0, 2)]
discards: [ 0 23 37 81]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 698.4851 - loglik: -6.9143e+02 - logprior: -7.0541e+00
Epoch 2/2
13/13 - 3s - loss: 691.3065 - loglik: -6.8962e+02 - logprior: -1.6889e+00
Fitted a model with MAP estimate = -689.0753
expansions: []
discards: [  0  73 125]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 701.2888 - loglik: -6.9251e+02 - logprior: -8.7746e+00
Epoch 2/10
13/13 - 2s - loss: 691.8497 - loglik: -6.8957e+02 - logprior: -2.2768e+00
Epoch 3/10
13/13 - 2s - loss: 688.3438 - loglik: -6.8747e+02 - logprior: -8.7027e-01
Epoch 4/10
13/13 - 2s - loss: 687.4867 - loglik: -6.8704e+02 - logprior: -4.4661e-01
Epoch 5/10
13/13 - 2s - loss: 687.8635 - loglik: -6.8757e+02 - logprior: -2.9597e-01
Fitted a model with MAP estimate = -686.1401
Time for alignment: 63.4088
Computed alignments with likelihoods: ['-685.7772', '-686.3321', '-687.1615', '-686.1267', '-686.1401']
Best model has likelihood: -685.7772  (prior= -0.1791 )
time for generating output: 0.1618
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.888295505117935
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 450.9385 - loglik: -4.4235e+02 - logprior: -8.5889e+00
Epoch 2/10
13/13 - 1s - loss: 402.2597 - loglik: -3.9999e+02 - logprior: -2.2650e+00
Epoch 3/10
13/13 - 1s - loss: 374.7237 - loglik: -3.7285e+02 - logprior: -1.8757e+00
Epoch 4/10
13/13 - 1s - loss: 366.9993 - loglik: -3.6524e+02 - logprior: -1.7573e+00
Epoch 5/10
13/13 - 1s - loss: 363.2977 - loglik: -3.6163e+02 - logprior: -1.6727e+00
Epoch 6/10
13/13 - 1s - loss: 364.1382 - loglik: -3.6255e+02 - logprior: -1.5852e+00
Fitted a model with MAP estimate = -362.7681
expansions: [(0, 3), (13, 1), (14, 3), (15, 1), (33, 1), (35, 2), (36, 1), (37, 2), (43, 2), (44, 3), (45, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 364.3909 - loglik: -3.5426e+02 - logprior: -1.0130e+01
Epoch 2/2
13/13 - 1s - loss: 347.0334 - loglik: -3.4392e+02 - logprior: -3.1100e+00
Fitted a model with MAP estimate = -343.4830
expansions: []
discards: [45 49 62 63 64 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 353.7102 - loglik: -3.4581e+02 - logprior: -7.9007e+00
Epoch 2/2
13/13 - 1s - loss: 346.9984 - loglik: -3.4467e+02 - logprior: -2.3280e+00
Fitted a model with MAP estimate = -345.6619
expansions: [(60, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 351.6478 - loglik: -3.4383e+02 - logprior: -7.8209e+00
Epoch 2/10
13/13 - 1s - loss: 345.4705 - loglik: -3.4315e+02 - logprior: -2.3206e+00
Epoch 3/10
13/13 - 1s - loss: 342.9557 - loglik: -3.4120e+02 - logprior: -1.7591e+00
Epoch 4/10
13/13 - 1s - loss: 342.1608 - loglik: -3.4070e+02 - logprior: -1.4590e+00
Epoch 5/10
13/13 - 1s - loss: 341.5967 - loglik: -3.4021e+02 - logprior: -1.3893e+00
Epoch 6/10
13/13 - 1s - loss: 341.8606 - loglik: -3.4054e+02 - logprior: -1.3205e+00
Fitted a model with MAP estimate = -341.5029
Time for alignment: 43.0996
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 450.3519 - loglik: -4.4176e+02 - logprior: -8.5875e+00
Epoch 2/10
13/13 - 2s - loss: 400.7722 - loglik: -3.9854e+02 - logprior: -2.2275e+00
Epoch 3/10
13/13 - 1s - loss: 372.0404 - loglik: -3.7035e+02 - logprior: -1.6943e+00
Epoch 4/10
13/13 - 1s - loss: 364.8730 - loglik: -3.6340e+02 - logprior: -1.4760e+00
Epoch 5/10
13/13 - 1s - loss: 362.0190 - loglik: -3.6060e+02 - logprior: -1.4190e+00
Epoch 6/10
13/13 - 1s - loss: 362.4591 - loglik: -3.6109e+02 - logprior: -1.3711e+00
Fitted a model with MAP estimate = -361.3567
expansions: [(0, 4), (13, 1), (36, 4), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 365.9103 - loglik: -3.5591e+02 - logprior: -1.0002e+01
Epoch 2/2
13/13 - 1s - loss: 349.8470 - loglik: -3.4687e+02 - logprior: -2.9747e+00
Fitted a model with MAP estimate = -347.1154
expansions: [(0, 2)]
discards: [46 49]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 355.2246 - loglik: -3.4516e+02 - logprior: -1.0067e+01
Epoch 2/2
13/13 - 1s - loss: 346.5388 - loglik: -3.4334e+02 - logprior: -3.1990e+00
Fitted a model with MAP estimate = -344.6031
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 350.7708 - loglik: -3.4285e+02 - logprior: -7.9194e+00
Epoch 2/10
13/13 - 1s - loss: 344.5239 - loglik: -3.4228e+02 - logprior: -2.2458e+00
Epoch 3/10
13/13 - 1s - loss: 343.1076 - loglik: -3.4141e+02 - logprior: -1.6986e+00
Epoch 4/10
13/13 - 1s - loss: 343.6973 - loglik: -3.4235e+02 - logprior: -1.3473e+00
Fitted a model with MAP estimate = -342.8326
Time for alignment: 39.0529
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 450.6378 - loglik: -4.4205e+02 - logprior: -8.5877e+00
Epoch 2/10
13/13 - 2s - loss: 399.5609 - loglik: -3.9734e+02 - logprior: -2.2220e+00
Epoch 3/10
13/13 - 1s - loss: 371.6120 - loglik: -3.6996e+02 - logprior: -1.6559e+00
Epoch 4/10
13/13 - 1s - loss: 364.3667 - loglik: -3.6293e+02 - logprior: -1.4396e+00
Epoch 5/10
13/13 - 1s - loss: 361.9568 - loglik: -3.6060e+02 - logprior: -1.3612e+00
Epoch 6/10
13/13 - 1s - loss: 361.4571 - loglik: -3.6014e+02 - logprior: -1.3180e+00
Epoch 7/10
13/13 - 1s - loss: 361.1078 - loglik: -3.5981e+02 - logprior: -1.2932e+00
Epoch 8/10
13/13 - 1s - loss: 361.1003 - loglik: -3.5981e+02 - logprior: -1.2866e+00
Epoch 9/10
13/13 - 1s - loss: 360.6364 - loglik: -3.5936e+02 - logprior: -1.2756e+00
Epoch 10/10
13/13 - 1s - loss: 360.2539 - loglik: -3.5900e+02 - logprior: -1.2562e+00
Fitted a model with MAP estimate = -360.5354
expansions: [(0, 4), (13, 1), (36, 4), (37, 1), (42, 1), (43, 2), (44, 2), (45, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 365.4985 - loglik: -3.5532e+02 - logprior: -1.0178e+01
Epoch 2/2
13/13 - 1s - loss: 348.9837 - loglik: -3.4575e+02 - logprior: -3.2324e+00
Fitted a model with MAP estimate = -345.5984
expansions: [(0, 2), (44, 2)]
discards: [54]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 352.9205 - loglik: -3.4277e+02 - logprior: -1.0153e+01
Epoch 2/2
13/13 - 1s - loss: 342.6167 - loglik: -3.3919e+02 - logprior: -3.4315e+00
Fitted a model with MAP estimate = -340.8172
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 347.2471 - loglik: -3.3918e+02 - logprior: -8.0701e+00
Epoch 2/10
13/13 - 1s - loss: 340.3458 - loglik: -3.3798e+02 - logprior: -2.3640e+00
Epoch 3/10
13/13 - 1s - loss: 339.5631 - loglik: -3.3773e+02 - logprior: -1.8303e+00
Epoch 4/10
13/13 - 1s - loss: 339.0291 - loglik: -3.3755e+02 - logprior: -1.4837e+00
Epoch 5/10
13/13 - 2s - loss: 338.8827 - loglik: -3.3749e+02 - logprior: -1.3883e+00
Epoch 6/10
13/13 - 1s - loss: 338.4863 - loglik: -3.3716e+02 - logprior: -1.3287e+00
Epoch 7/10
13/13 - 1s - loss: 338.2601 - loglik: -3.3696e+02 - logprior: -1.2971e+00
Epoch 8/10
13/13 - 1s - loss: 338.5892 - loglik: -3.3734e+02 - logprior: -1.2540e+00
Fitted a model with MAP estimate = -338.2467
Time for alignment: 50.8458
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 450.8085 - loglik: -4.4221e+02 - logprior: -8.5939e+00
Epoch 2/10
13/13 - 1s - loss: 400.3388 - loglik: -3.9810e+02 - logprior: -2.2414e+00
Epoch 3/10
13/13 - 1s - loss: 371.9007 - loglik: -3.7015e+02 - logprior: -1.7512e+00
Epoch 4/10
13/13 - 1s - loss: 364.0774 - loglik: -3.6253e+02 - logprior: -1.5484e+00
Epoch 5/10
13/13 - 1s - loss: 362.0131 - loglik: -3.6055e+02 - logprior: -1.4621e+00
Epoch 6/10
13/13 - 1s - loss: 360.8045 - loglik: -3.5940e+02 - logprior: -1.4089e+00
Epoch 7/10
13/13 - 1s - loss: 360.3420 - loglik: -3.5893e+02 - logprior: -1.4086e+00
Epoch 8/10
13/13 - 1s - loss: 360.4802 - loglik: -3.5910e+02 - logprior: -1.3832e+00
Fitted a model with MAP estimate = -360.2233
expansions: [(0, 4), (13, 1), (16, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 365.0087 - loglik: -3.5491e+02 - logprior: -1.0095e+01
Epoch 2/2
13/13 - 1s - loss: 350.0197 - loglik: -3.4697e+02 - logprior: -3.0482e+00
Fitted a model with MAP estimate = -346.7906
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 354.6840 - loglik: -3.4463e+02 - logprior: -1.0053e+01
Epoch 2/2
13/13 - 1s - loss: 346.1319 - loglik: -3.4289e+02 - logprior: -3.2465e+00
Fitted a model with MAP estimate = -344.2984
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 350.6564 - loglik: -3.4274e+02 - logprior: -7.9163e+00
Epoch 2/10
13/13 - 1s - loss: 344.0925 - loglik: -3.4187e+02 - logprior: -2.2205e+00
Epoch 3/10
13/13 - 1s - loss: 342.6236 - loglik: -3.4094e+02 - logprior: -1.6799e+00
Epoch 4/10
13/13 - 1s - loss: 343.1920 - loglik: -3.4185e+02 - logprior: -1.3380e+00
Fitted a model with MAP estimate = -342.4169
Time for alignment: 41.4781
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 450.5467 - loglik: -4.4195e+02 - logprior: -8.5946e+00
Epoch 2/10
13/13 - 1s - loss: 400.6602 - loglik: -3.9841e+02 - logprior: -2.2501e+00
Epoch 3/10
13/13 - 1s - loss: 372.8199 - loglik: -3.7104e+02 - logprior: -1.7796e+00
Epoch 4/10
13/13 - 1s - loss: 364.3333 - loglik: -3.6276e+02 - logprior: -1.5723e+00
Epoch 5/10
13/13 - 1s - loss: 362.4977 - loglik: -3.6100e+02 - logprior: -1.4996e+00
Epoch 6/10
13/13 - 1s - loss: 361.0381 - loglik: -3.5956e+02 - logprior: -1.4795e+00
Epoch 7/10
13/13 - 1s - loss: 361.0405 - loglik: -3.5959e+02 - logprior: -1.4477e+00
Fitted a model with MAP estimate = -360.6632
expansions: [(0, 4), (13, 1), (16, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 365.7218 - loglik: -3.5565e+02 - logprior: -1.0076e+01
Epoch 2/2
13/13 - 1s - loss: 350.6735 - loglik: -3.4766e+02 - logprior: -3.0157e+00
Fitted a model with MAP estimate = -347.3555
expansions: [(0, 2)]
discards: [46 49]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 355.2036 - loglik: -3.4514e+02 - logprior: -1.0064e+01
Epoch 2/2
13/13 - 1s - loss: 346.8711 - loglik: -3.4365e+02 - logprior: -3.2232e+00
Fitted a model with MAP estimate = -344.6814
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 350.3416 - loglik: -3.4241e+02 - logprior: -7.9309e+00
Epoch 2/10
13/13 - 1s - loss: 345.1425 - loglik: -3.4290e+02 - logprior: -2.2443e+00
Epoch 3/10
13/13 - 1s - loss: 343.6467 - loglik: -3.4195e+02 - logprior: -1.6956e+00
Epoch 4/10
13/13 - 1s - loss: 342.8156 - loglik: -3.4147e+02 - logprior: -1.3488e+00
Epoch 5/10
13/13 - 1s - loss: 342.7774 - loglik: -3.4153e+02 - logprior: -1.2505e+00
Epoch 6/10
13/13 - 2s - loss: 342.9405 - loglik: -3.4174e+02 - logprior: -1.1959e+00
Fitted a model with MAP estimate = -342.4515
Time for alignment: 43.6898
Computed alignments with likelihoods: ['-341.5029', '-342.8326', '-338.2467', '-342.4169', '-342.4515']
Best model has likelihood: -338.2467  (prior= -1.2314 )
time for generating output: 0.1820
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.4998118178396688
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 132.8981 - loglik: -1.3207e+02 - logprior: -8.3025e-01
Epoch 2/10
41/41 - 1s - loss: 119.8983 - loglik: -1.1909e+02 - logprior: -8.0846e-01
Epoch 3/10
41/41 - 1s - loss: 119.1885 - loglik: -1.1839e+02 - logprior: -7.9422e-01
Epoch 4/10
41/41 - 1s - loss: 119.0394 - loglik: -1.1825e+02 - logprior: -7.8566e-01
Epoch 5/10
41/41 - 1s - loss: 118.8827 - loglik: -1.1810e+02 - logprior: -7.8083e-01
Epoch 6/10
41/41 - 1s - loss: 118.9382 - loglik: -1.1816e+02 - logprior: -7.7817e-01
Fitted a model with MAP estimate = -117.2406
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 118.7451 - loglik: -1.1775e+02 - logprior: -9.9441e-01
Epoch 2/2
41/41 - 1s - loss: 117.0863 - loglik: -1.1633e+02 - logprior: -7.5552e-01
Fitted a model with MAP estimate = -115.3397
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 117.8028 - loglik: -1.1683e+02 - logprior: -9.7305e-01
Epoch 2/2
41/41 - 1s - loss: 117.0361 - loglik: -1.1630e+02 - logprior: -7.4047e-01
Fitted a model with MAP estimate = -115.3114
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 115.2337 - loglik: -1.1463e+02 - logprior: -6.0646e-01
Epoch 2/10
58/58 - 2s - loss: 114.7697 - loglik: -1.1427e+02 - logprior: -5.0438e-01
Epoch 3/10
58/58 - 2s - loss: 114.6262 - loglik: -1.1413e+02 - logprior: -5.0072e-01
Epoch 4/10
58/58 - 2s - loss: 114.5370 - loglik: -1.1404e+02 - logprior: -4.9638e-01
Epoch 5/10
58/58 - 2s - loss: 114.4700 - loglik: -1.1398e+02 - logprior: -4.9484e-01
Epoch 6/10
58/58 - 2s - loss: 114.4418 - loglik: -1.1395e+02 - logprior: -4.9398e-01
Epoch 7/10
58/58 - 2s - loss: 114.3829 - loglik: -1.1389e+02 - logprior: -4.8988e-01
Epoch 8/10
58/58 - 2s - loss: 114.3216 - loglik: -1.1383e+02 - logprior: -4.9012e-01
Epoch 9/10
58/58 - 2s - loss: 114.3660 - loglik: -1.1388e+02 - logprior: -4.8934e-01
Fitted a model with MAP estimate = -114.3778
Time for alignment: 64.2214
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 132.9135 - loglik: -1.3208e+02 - logprior: -8.2925e-01
Epoch 2/10
41/41 - 1s - loss: 120.1451 - loglik: -1.1934e+02 - logprior: -8.0436e-01
Epoch 3/10
41/41 - 1s - loss: 119.2325 - loglik: -1.1844e+02 - logprior: -7.9361e-01
Epoch 4/10
41/41 - 1s - loss: 119.1477 - loglik: -1.1836e+02 - logprior: -7.8526e-01
Epoch 5/10
41/41 - 1s - loss: 119.0058 - loglik: -1.1822e+02 - logprior: -7.8246e-01
Epoch 6/10
41/41 - 1s - loss: 118.9958 - loglik: -1.1822e+02 - logprior: -7.8056e-01
Epoch 7/10
41/41 - 1s - loss: 118.9548 - loglik: -1.1818e+02 - logprior: -7.7841e-01
Epoch 8/10
41/41 - 1s - loss: 118.7726 - loglik: -1.1800e+02 - logprior: -7.7754e-01
Epoch 9/10
41/41 - 1s - loss: 118.7980 - loglik: -1.1802e+02 - logprior: -7.7561e-01
Fitted a model with MAP estimate = -117.3302
expansions: [(4, 1), (7, 2), (10, 2), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 119.1518 - loglik: -1.1815e+02 - logprior: -1.0012e+00
Epoch 2/2
41/41 - 1s - loss: 117.1044 - loglik: -1.1634e+02 - logprior: -7.6113e-01
Fitted a model with MAP estimate = -115.2789
expansions: []
discards: [ 8 14]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.8654 - loglik: -1.1689e+02 - logprior: -9.7429e-01
Epoch 2/2
41/41 - 1s - loss: 117.0293 - loglik: -1.1629e+02 - logprior: -7.3855e-01
Fitted a model with MAP estimate = -115.3292
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 115.1391 - loglik: -1.1453e+02 - logprior: -6.0627e-01
Epoch 2/10
58/58 - 2s - loss: 114.7739 - loglik: -1.1427e+02 - logprior: -5.0528e-01
Epoch 3/10
58/58 - 2s - loss: 114.7088 - loglik: -1.1421e+02 - logprior: -4.9917e-01
Epoch 4/10
58/58 - 2s - loss: 114.4958 - loglik: -1.1400e+02 - logprior: -4.9629e-01
Epoch 5/10
58/58 - 1s - loss: 114.4862 - loglik: -1.1399e+02 - logprior: -4.9293e-01
Epoch 6/10
58/58 - 2s - loss: 114.4631 - loglik: -1.1397e+02 - logprior: -4.9315e-01
Epoch 7/10
58/58 - 2s - loss: 114.2186 - loglik: -1.1373e+02 - logprior: -4.9038e-01
Epoch 8/10
58/58 - 2s - loss: 114.3239 - loglik: -1.1383e+02 - logprior: -4.8932e-01
Fitted a model with MAP estimate = -114.3669
Time for alignment: 64.8259
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 132.9136 - loglik: -1.3208e+02 - logprior: -8.3122e-01
Epoch 2/10
41/41 - 1s - loss: 119.8443 - loglik: -1.1904e+02 - logprior: -8.0892e-01
Epoch 3/10
41/41 - 1s - loss: 119.2081 - loglik: -1.1842e+02 - logprior: -7.9259e-01
Epoch 4/10
41/41 - 1s - loss: 119.0974 - loglik: -1.1831e+02 - logprior: -7.8539e-01
Epoch 5/10
41/41 - 1s - loss: 118.8094 - loglik: -1.1803e+02 - logprior: -7.8229e-01
Epoch 6/10
41/41 - 1s - loss: 118.9475 - loglik: -1.1817e+02 - logprior: -7.7973e-01
Fitted a model with MAP estimate = -117.2573
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 118.7744 - loglik: -1.1778e+02 - logprior: -9.9465e-01
Epoch 2/2
41/41 - 1s - loss: 117.0998 - loglik: -1.1634e+02 - logprior: -7.5564e-01
Fitted a model with MAP estimate = -115.2938
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.8059 - loglik: -1.1683e+02 - logprior: -9.7293e-01
Epoch 2/2
41/41 - 1s - loss: 117.0570 - loglik: -1.1632e+02 - logprior: -7.4151e-01
Fitted a model with MAP estimate = -115.3380
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 115.1435 - loglik: -1.1453e+02 - logprior: -6.0853e-01
Epoch 2/10
58/58 - 1s - loss: 114.8326 - loglik: -1.1433e+02 - logprior: -5.0300e-01
Epoch 3/10
58/58 - 1s - loss: 114.6613 - loglik: -1.1416e+02 - logprior: -5.0070e-01
Epoch 4/10
58/58 - 2s - loss: 114.4490 - loglik: -1.1395e+02 - logprior: -4.9783e-01
Epoch 5/10
58/58 - 2s - loss: 114.5031 - loglik: -1.1401e+02 - logprior: -4.9380e-01
Fitted a model with MAP estimate = -114.4154
Time for alignment: 56.3704
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 133.1344 - loglik: -1.3231e+02 - logprior: -8.2559e-01
Epoch 2/10
41/41 - 1s - loss: 119.9623 - loglik: -1.1916e+02 - logprior: -8.0562e-01
Epoch 3/10
41/41 - 1s - loss: 119.1935 - loglik: -1.1840e+02 - logprior: -7.9211e-01
Epoch 4/10
41/41 - 1s - loss: 119.1921 - loglik: -1.1841e+02 - logprior: -7.8652e-01
Epoch 5/10
41/41 - 1s - loss: 118.8005 - loglik: -1.1802e+02 - logprior: -7.8374e-01
Epoch 6/10
41/41 - 1s - loss: 118.9971 - loglik: -1.1822e+02 - logprior: -7.8012e-01
Fitted a model with MAP estimate = -117.2679
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 118.6736 - loglik: -1.1768e+02 - logprior: -9.9420e-01
Epoch 2/2
41/41 - 1s - loss: 117.1928 - loglik: -1.1644e+02 - logprior: -7.5648e-01
Fitted a model with MAP estimate = -115.2958
expansions: []
discards: [11]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.7373 - loglik: -1.1676e+02 - logprior: -9.7422e-01
Epoch 2/2
41/41 - 1s - loss: 117.2808 - loglik: -1.1654e+02 - logprior: -7.3929e-01
Fitted a model with MAP estimate = -115.3316
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 115.0661 - loglik: -1.1446e+02 - logprior: -6.0669e-01
Epoch 2/10
58/58 - 1s - loss: 114.7765 - loglik: -1.1427e+02 - logprior: -5.0311e-01
Epoch 3/10
58/58 - 1s - loss: 114.7929 - loglik: -1.1429e+02 - logprior: -5.0124e-01
Fitted a model with MAP estimate = -114.5074
Time for alignment: 53.0917
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 133.6038 - loglik: -1.3278e+02 - logprior: -8.2245e-01
Epoch 2/10
41/41 - 1s - loss: 120.5489 - loglik: -1.1974e+02 - logprior: -8.0747e-01
Epoch 3/10
41/41 - 1s - loss: 119.2157 - loglik: -1.1842e+02 - logprior: -7.9205e-01
Epoch 4/10
41/41 - 1s - loss: 119.0485 - loglik: -1.1826e+02 - logprior: -7.8436e-01
Epoch 5/10
41/41 - 1s - loss: 118.8600 - loglik: -1.1808e+02 - logprior: -7.7826e-01
Epoch 6/10
41/41 - 1s - loss: 118.9786 - loglik: -1.1820e+02 - logprior: -7.7975e-01
Fitted a model with MAP estimate = -117.1716
expansions: [(8, 2), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 119.0367 - loglik: -1.1802e+02 - logprior: -1.0117e+00
Epoch 2/2
41/41 - 1s - loss: 117.3936 - loglik: -1.1662e+02 - logprior: -7.7191e-01
Fitted a model with MAP estimate = -115.3090
expansions: []
discards: [ 8 12 14]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 117.7648 - loglik: -1.1679e+02 - logprior: -9.7533e-01
Epoch 2/2
41/41 - 1s - loss: 117.1361 - loglik: -1.1640e+02 - logprior: -7.3936e-01
Fitted a model with MAP estimate = -115.3424
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 115.1928 - loglik: -1.1458e+02 - logprior: -6.1011e-01
Epoch 2/10
58/58 - 2s - loss: 114.7094 - loglik: -1.1421e+02 - logprior: -5.0251e-01
Epoch 3/10
58/58 - 2s - loss: 114.7274 - loglik: -1.1423e+02 - logprior: -5.0072e-01
Fitted a model with MAP estimate = -114.4986
Time for alignment: 53.8699
Computed alignments with likelihoods: ['-114.3778', '-114.3669', '-114.4154', '-114.5074', '-114.4986']
Best model has likelihood: -114.3669  (prior= -0.4903 )
time for generating output: 0.0772
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 447.9256 - loglik: -3.2375e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 335.6385 - loglik: -3.0185e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 1s - loss: 300.7708 - loglik: -2.8473e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 284.9565 - loglik: -2.7534e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 277.0293 - loglik: -2.7096e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 273.3319 - loglik: -2.6935e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 1s - loss: 271.5578 - loglik: -2.6892e+02 - logprior: -2.6387e+00
Epoch 8/10
10/10 - 0s - loss: 270.4499 - loglik: -2.6870e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 269.3305 - loglik: -2.6819e+02 - logprior: -1.1391e+00
Epoch 10/10
10/10 - 1s - loss: 268.2439 - loglik: -2.6754e+02 - logprior: -7.0209e-01
Fitted a model with MAP estimate = -267.9404
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 406.1203 - loglik: -2.6711e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 319.0757 - loglik: -2.6158e+02 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -304.8205
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 370.6198 - loglik: -2.5908e+02 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 287.7429 - loglik: -2.5775e+02 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -275.6143
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 395.3246 - loglik: -2.5961e+02 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 1s - loss: 306.3986 - loglik: -2.5860e+02 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 276.8526 - loglik: -2.5836e+02 - logprior: -1.8496e+01
Epoch 4/10
10/10 - 1s - loss: 264.9867 - loglik: -2.5830e+02 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 1s - loss: 260.3473 - loglik: -2.5829e+02 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 0s - loss: 258.0164 - loglik: -2.5832e+02 - logprior: 0.3050
Epoch 7/10
10/10 - 0s - loss: 256.7046 - loglik: -2.5841e+02 - logprior: 1.7014
Epoch 8/10
10/10 - 1s - loss: 255.8682 - loglik: -2.5852e+02 - logprior: 2.6562
Epoch 9/10
10/10 - 0s - loss: 255.2695 - loglik: -2.5867e+02 - logprior: 3.3968
Epoch 10/10
10/10 - 1s - loss: 254.7976 - loglik: -2.5881e+02 - logprior: 4.0167
Fitted a model with MAP estimate = -254.5694
Time for alignment: 28.9692
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 447.9256 - loglik: -3.2375e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 335.6385 - loglik: -3.0185e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 1s - loss: 300.7708 - loglik: -2.8473e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 284.9565 - loglik: -2.7534e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 1s - loss: 277.0293 - loglik: -2.7096e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 1s - loss: 273.3319 - loglik: -2.6935e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 0s - loss: 271.5578 - loglik: -2.6892e+02 - logprior: -2.6387e+00
Epoch 8/10
10/10 - 0s - loss: 270.4500 - loglik: -2.6870e+02 - logprior: -1.7476e+00
Epoch 9/10
10/10 - 0s - loss: 269.3305 - loglik: -2.6819e+02 - logprior: -1.1391e+00
Epoch 10/10
10/10 - 1s - loss: 268.2439 - loglik: -2.6754e+02 - logprior: -7.0210e-01
Fitted a model with MAP estimate = -267.9404
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.1203 - loglik: -2.6711e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 0s - loss: 319.0757 - loglik: -2.6158e+02 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -304.8205
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 370.6198 - loglik: -2.5908e+02 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 287.7429 - loglik: -2.5775e+02 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -275.6143
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 395.3246 - loglik: -2.5961e+02 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 1s - loss: 306.3986 - loglik: -2.5860e+02 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 276.8526 - loglik: -2.5836e+02 - logprior: -1.8496e+01
Epoch 4/10
10/10 - 1s - loss: 264.9867 - loglik: -2.5830e+02 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 260.3472 - loglik: -2.5829e+02 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 0s - loss: 258.0164 - loglik: -2.5832e+02 - logprior: 0.3050
Epoch 7/10
10/10 - 1s - loss: 256.7046 - loglik: -2.5841e+02 - logprior: 1.7014
Epoch 8/10
10/10 - 1s - loss: 255.8681 - loglik: -2.5852e+02 - logprior: 2.6562
Epoch 9/10
10/10 - 0s - loss: 255.2694 - loglik: -2.5867e+02 - logprior: 3.3968
Epoch 10/10
10/10 - 0s - loss: 254.7976 - loglik: -2.5881e+02 - logprior: 4.0167
Fitted a model with MAP estimate = -254.5694
Time for alignment: 27.5893
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 447.9256 - loglik: -3.2375e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 335.6385 - loglik: -3.0185e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 1s - loss: 300.7708 - loglik: -2.8473e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 1s - loss: 284.9565 - loglik: -2.7534e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 1s - loss: 277.0293 - loglik: -2.7096e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 1s - loss: 273.3319 - loglik: -2.6935e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 1s - loss: 271.5578 - loglik: -2.6892e+02 - logprior: -2.6387e+00
Epoch 8/10
10/10 - 0s - loss: 270.4499 - loglik: -2.6870e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 1s - loss: 269.3305 - loglik: -2.6819e+02 - logprior: -1.1391e+00
Epoch 10/10
10/10 - 1s - loss: 268.2438 - loglik: -2.6754e+02 - logprior: -7.0209e-01
Fitted a model with MAP estimate = -267.9404
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.1203 - loglik: -2.6711e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 0s - loss: 319.0757 - loglik: -2.6158e+02 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -304.8205
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 370.6198 - loglik: -2.5908e+02 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 287.7429 - loglik: -2.5775e+02 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -275.6143
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 395.3246 - loglik: -2.5961e+02 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 0s - loss: 306.3986 - loglik: -2.5860e+02 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 0s - loss: 276.8526 - loglik: -2.5836e+02 - logprior: -1.8496e+01
Epoch 4/10
10/10 - 0s - loss: 264.9867 - loglik: -2.5830e+02 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 260.3473 - loglik: -2.5829e+02 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 0s - loss: 258.0164 - loglik: -2.5832e+02 - logprior: 0.3050
Epoch 7/10
10/10 - 0s - loss: 256.7046 - loglik: -2.5841e+02 - logprior: 1.7013
Epoch 8/10
10/10 - 1s - loss: 255.8682 - loglik: -2.5852e+02 - logprior: 2.6562
Epoch 9/10
10/10 - 0s - loss: 255.2695 - loglik: -2.5867e+02 - logprior: 3.3968
Epoch 10/10
10/10 - 1s - loss: 254.7976 - loglik: -2.5881e+02 - logprior: 4.0167
Fitted a model with MAP estimate = -254.5694
Time for alignment: 27.5624
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 447.9256 - loglik: -3.2375e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 335.6385 - loglik: -3.0185e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 300.7708 - loglik: -2.8473e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 284.9565 - loglik: -2.7534e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 277.0293 - loglik: -2.7096e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 273.3319 - loglik: -2.6935e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 0s - loss: 271.5578 - loglik: -2.6892e+02 - logprior: -2.6387e+00
Epoch 8/10
10/10 - 0s - loss: 270.4499 - loglik: -2.6870e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 0s - loss: 269.3305 - loglik: -2.6819e+02 - logprior: -1.1391e+00
Epoch 10/10
10/10 - 0s - loss: 268.2439 - loglik: -2.6754e+02 - logprior: -7.0210e-01
Fitted a model with MAP estimate = -267.9404
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.1203 - loglik: -2.6711e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 319.0757 - loglik: -2.6158e+02 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -304.8205
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 370.6198 - loglik: -2.5908e+02 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 287.7429 - loglik: -2.5775e+02 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -275.6143
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 395.3246 - loglik: -2.5961e+02 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 0s - loss: 306.3986 - loglik: -2.5860e+02 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 276.8526 - loglik: -2.5836e+02 - logprior: -1.8496e+01
Epoch 4/10
10/10 - 1s - loss: 264.9867 - loglik: -2.5830e+02 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 0s - loss: 260.3473 - loglik: -2.5829e+02 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 1s - loss: 258.0164 - loglik: -2.5832e+02 - logprior: 0.3050
Epoch 7/10
10/10 - 1s - loss: 256.7046 - loglik: -2.5841e+02 - logprior: 1.7013
Epoch 8/10
10/10 - 0s - loss: 255.8681 - loglik: -2.5852e+02 - logprior: 2.6562
Epoch 9/10
10/10 - 1s - loss: 255.2694 - loglik: -2.5867e+02 - logprior: 3.3968
Epoch 10/10
10/10 - 1s - loss: 254.7976 - loglik: -2.5881e+02 - logprior: 4.0167
Fitted a model with MAP estimate = -254.5695
Time for alignment: 27.2369
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 447.9256 - loglik: -3.2375e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 335.6385 - loglik: -3.0185e+02 - logprior: -3.3791e+01
Epoch 3/10
10/10 - 0s - loss: 300.7708 - loglik: -2.8473e+02 - logprior: -1.6040e+01
Epoch 4/10
10/10 - 0s - loss: 284.9565 - loglik: -2.7534e+02 - logprior: -9.6174e+00
Epoch 5/10
10/10 - 0s - loss: 277.0293 - loglik: -2.7096e+02 - logprior: -6.0660e+00
Epoch 6/10
10/10 - 0s - loss: 273.3319 - loglik: -2.6935e+02 - logprior: -3.9845e+00
Epoch 7/10
10/10 - 0s - loss: 271.5578 - loglik: -2.6892e+02 - logprior: -2.6387e+00
Epoch 8/10
10/10 - 1s - loss: 270.4499 - loglik: -2.6870e+02 - logprior: -1.7475e+00
Epoch 9/10
10/10 - 1s - loss: 269.3305 - loglik: -2.6819e+02 - logprior: -1.1391e+00
Epoch 10/10
10/10 - 1s - loss: 268.2439 - loglik: -2.6754e+02 - logprior: -7.0209e-01
Fitted a model with MAP estimate = -267.9403
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.1203 - loglik: -2.6711e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 0s - loss: 319.0757 - loglik: -2.6158e+02 - logprior: -5.7497e+01
Fitted a model with MAP estimate = -304.8205
expansions: [(0, 2)]
discards: [ 0  8 20 29 40 43]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 370.6198 - loglik: -2.5908e+02 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 0s - loss: 287.7429 - loglik: -2.5775e+02 - logprior: -2.9995e+01
Fitted a model with MAP estimate = -275.6144
expansions: [(38, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 395.3246 - loglik: -2.5961e+02 - logprior: -1.3572e+02
Epoch 2/10
10/10 - 1s - loss: 306.3986 - loglik: -2.5860e+02 - logprior: -4.7803e+01
Epoch 3/10
10/10 - 1s - loss: 276.8526 - loglik: -2.5836e+02 - logprior: -1.8496e+01
Epoch 4/10
10/10 - 1s - loss: 264.9867 - loglik: -2.5830e+02 - logprior: -6.6901e+00
Epoch 5/10
10/10 - 1s - loss: 260.3472 - loglik: -2.5829e+02 - logprior: -2.0581e+00
Epoch 6/10
10/10 - 1s - loss: 258.0164 - loglik: -2.5832e+02 - logprior: 0.3050
Epoch 7/10
10/10 - 1s - loss: 256.7046 - loglik: -2.5841e+02 - logprior: 1.7013
Epoch 8/10
10/10 - 1s - loss: 255.8682 - loglik: -2.5852e+02 - logprior: 2.6562
Epoch 9/10
10/10 - 1s - loss: 255.2694 - loglik: -2.5867e+02 - logprior: 3.3968
Epoch 10/10
10/10 - 1s - loss: 254.7976 - loglik: -2.5881e+02 - logprior: 4.0167
Fitted a model with MAP estimate = -254.5695
Time for alignment: 27.3806
Computed alignments with likelihoods: ['-254.5694', '-254.5694', '-254.5694', '-254.5695', '-254.5695']
Best model has likelihood: -254.5694  (prior= 4.3186 )
time for generating output: 0.0961
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8919239904988123
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 401.7680 - loglik: -3.9865e+02 - logprior: -3.1166e+00
Epoch 2/10
19/19 - 1s - loss: 364.5143 - loglik: -3.6335e+02 - logprior: -1.1644e+00
Epoch 3/10
19/19 - 1s - loss: 348.9789 - loglik: -3.4773e+02 - logprior: -1.2534e+00
Epoch 4/10
19/19 - 1s - loss: 345.8777 - loglik: -3.4465e+02 - logprior: -1.2274e+00
Epoch 5/10
19/19 - 1s - loss: 345.0125 - loglik: -3.4384e+02 - logprior: -1.1734e+00
Epoch 6/10
19/19 - 1s - loss: 344.6876 - loglik: -3.4354e+02 - logprior: -1.1512e+00
Epoch 7/10
19/19 - 1s - loss: 344.3907 - loglik: -3.4326e+02 - logprior: -1.1319e+00
Epoch 8/10
19/19 - 1s - loss: 344.5227 - loglik: -3.4341e+02 - logprior: -1.1164e+00
Fitted a model with MAP estimate = -341.9320
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 1), (24, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 347.4400 - loglik: -3.4318e+02 - logprior: -4.2606e+00
Epoch 2/2
19/19 - 1s - loss: 337.9602 - loglik: -3.3654e+02 - logprior: -1.4155e+00
Fitted a model with MAP estimate = -334.9597
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.0307 - loglik: -3.3603e+02 - logprior: -4.0038e+00
Epoch 2/2
19/19 - 1s - loss: 335.4752 - loglik: -3.3425e+02 - logprior: -1.2287e+00
Fitted a model with MAP estimate = -332.9766
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 336.7193 - loglik: -3.3336e+02 - logprior: -3.3633e+00
Epoch 2/10
21/21 - 1s - loss: 333.4254 - loglik: -3.3198e+02 - logprior: -1.4458e+00
Epoch 3/10
21/21 - 1s - loss: 332.4120 - loglik: -3.3134e+02 - logprior: -1.0684e+00
Epoch 4/10
21/21 - 1s - loss: 331.9112 - loglik: -3.3088e+02 - logprior: -1.0284e+00
Epoch 5/10
21/21 - 1s - loss: 331.3893 - loglik: -3.3038e+02 - logprior: -1.0130e+00
Epoch 6/10
21/21 - 1s - loss: 331.6004 - loglik: -3.3061e+02 - logprior: -9.9442e-01
Fitted a model with MAP estimate = -331.3168
Time for alignment: 46.3170
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 401.6702 - loglik: -3.9856e+02 - logprior: -3.1152e+00
Epoch 2/10
19/19 - 1s - loss: 361.0505 - loglik: -3.5988e+02 - logprior: -1.1690e+00
Epoch 3/10
19/19 - 1s - loss: 347.5246 - loglik: -3.4626e+02 - logprior: -1.2652e+00
Epoch 4/10
19/19 - 1s - loss: 345.8173 - loglik: -3.4460e+02 - logprior: -1.2204e+00
Epoch 5/10
19/19 - 1s - loss: 344.8842 - loglik: -3.4373e+02 - logprior: -1.1573e+00
Epoch 6/10
19/19 - 1s - loss: 344.9307 - loglik: -3.4380e+02 - logprior: -1.1348e+00
Fitted a model with MAP estimate = -342.1034
expansions: [(0, 3), (13, 2), (14, 1), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 347.5473 - loglik: -3.4339e+02 - logprior: -4.1549e+00
Epoch 2/2
19/19 - 1s - loss: 338.0750 - loglik: -3.3668e+02 - logprior: -1.3980e+00
Fitted a model with MAP estimate = -335.2621
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.1497 - loglik: -3.3618e+02 - logprior: -3.9697e+00
Epoch 2/2
19/19 - 1s - loss: 334.9567 - loglik: -3.3370e+02 - logprior: -1.2589e+00
Fitted a model with MAP estimate = -332.1553
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 335.9812 - loglik: -3.3259e+02 - logprior: -3.3888e+00
Epoch 2/10
21/21 - 1s - loss: 332.6042 - loglik: -3.3112e+02 - logprior: -1.4854e+00
Epoch 3/10
21/21 - 1s - loss: 331.7149 - loglik: -3.3064e+02 - logprior: -1.0771e+00
Epoch 4/10
21/21 - 1s - loss: 331.1781 - loglik: -3.3015e+02 - logprior: -1.0279e+00
Epoch 5/10
21/21 - 1s - loss: 330.7719 - loglik: -3.2974e+02 - logprior: -1.0289e+00
Epoch 6/10
21/21 - 1s - loss: 330.4953 - loglik: -3.2949e+02 - logprior: -1.0053e+00
Epoch 7/10
21/21 - 1s - loss: 330.5925 - loglik: -3.2961e+02 - logprior: -9.7759e-01
Fitted a model with MAP estimate = -330.4506
Time for alignment: 43.6763
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 401.5785 - loglik: -3.9846e+02 - logprior: -3.1147e+00
Epoch 2/10
19/19 - 1s - loss: 360.7966 - loglik: -3.5964e+02 - logprior: -1.1608e+00
Epoch 3/10
19/19 - 1s - loss: 348.0005 - loglik: -3.4675e+02 - logprior: -1.2536e+00
Epoch 4/10
19/19 - 1s - loss: 346.1194 - loglik: -3.4490e+02 - logprior: -1.2162e+00
Epoch 5/10
19/19 - 1s - loss: 345.5397 - loglik: -3.4438e+02 - logprior: -1.1632e+00
Epoch 6/10
19/19 - 1s - loss: 345.0809 - loglik: -3.4394e+02 - logprior: -1.1458e+00
Epoch 7/10
19/19 - 1s - loss: 344.9064 - loglik: -3.4378e+02 - logprior: -1.1272e+00
Epoch 8/10
19/19 - 1s - loss: 344.9270 - loglik: -3.4381e+02 - logprior: -1.1130e+00
Fitted a model with MAP estimate = -342.3173
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 348.3917 - loglik: -3.4412e+02 - logprior: -4.2707e+00
Epoch 2/2
19/19 - 1s - loss: 338.1930 - loglik: -3.3675e+02 - logprior: -1.4440e+00
Fitted a model with MAP estimate = -335.0706
expansions: [(0, 2)]
discards: [27 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.2031 - loglik: -3.3621e+02 - logprior: -3.9976e+00
Epoch 2/2
19/19 - 1s - loss: 335.6870 - loglik: -3.3445e+02 - logprior: -1.2373e+00
Fitted a model with MAP estimate = -333.0267
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 336.6902 - loglik: -3.3333e+02 - logprior: -3.3569e+00
Epoch 2/10
21/21 - 1s - loss: 333.7429 - loglik: -3.3229e+02 - logprior: -1.4517e+00
Epoch 3/10
21/21 - 1s - loss: 332.2364 - loglik: -3.3117e+02 - logprior: -1.0702e+00
Epoch 4/10
21/21 - 1s - loss: 331.8582 - loglik: -3.3083e+02 - logprior: -1.0264e+00
Epoch 5/10
21/21 - 1s - loss: 331.3618 - loglik: -3.3034e+02 - logprior: -1.0189e+00
Epoch 6/10
21/21 - 1s - loss: 331.2009 - loglik: -3.3020e+02 - logprior: -9.9699e-01
Epoch 7/10
21/21 - 1s - loss: 330.9600 - loglik: -3.2999e+02 - logprior: -9.7331e-01
Epoch 8/10
21/21 - 1s - loss: 331.0416 - loglik: -3.3009e+02 - logprior: -9.5399e-01
Fitted a model with MAP estimate = -330.9201
Time for alignment: 47.5081
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 401.8215 - loglik: -3.9870e+02 - logprior: -3.1175e+00
Epoch 2/10
19/19 - 1s - loss: 364.3029 - loglik: -3.6314e+02 - logprior: -1.1649e+00
Epoch 3/10
19/19 - 1s - loss: 350.1680 - loglik: -3.4894e+02 - logprior: -1.2328e+00
Epoch 4/10
19/19 - 1s - loss: 347.8792 - loglik: -3.4669e+02 - logprior: -1.1872e+00
Epoch 5/10
19/19 - 1s - loss: 347.3275 - loglik: -3.4619e+02 - logprior: -1.1338e+00
Epoch 6/10
19/19 - 1s - loss: 347.0498 - loglik: -3.4594e+02 - logprior: -1.1138e+00
Epoch 7/10
19/19 - 1s - loss: 346.7599 - loglik: -3.4567e+02 - logprior: -1.0922e+00
Epoch 8/10
19/19 - 1s - loss: 346.8939 - loglik: -3.4581e+02 - logprior: -1.0808e+00
Fitted a model with MAP estimate = -344.2883
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 2), (30, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 350.4763 - loglik: -3.4619e+02 - logprior: -4.2840e+00
Epoch 2/2
19/19 - 1s - loss: 340.1210 - loglik: -3.3864e+02 - logprior: -1.4772e+00
Fitted a model with MAP estimate = -337.2133
expansions: [(0, 2)]
discards: [25 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 342.1168 - loglik: -3.3809e+02 - logprior: -4.0246e+00
Epoch 2/2
19/19 - 1s - loss: 337.5340 - loglik: -3.3628e+02 - logprior: -1.2523e+00
Fitted a model with MAP estimate = -334.9776
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 338.7865 - loglik: -3.3540e+02 - logprior: -3.3830e+00
Epoch 2/10
21/21 - 1s - loss: 335.5930 - loglik: -3.3411e+02 - logprior: -1.4807e+00
Epoch 3/10
21/21 - 1s - loss: 334.6946 - loglik: -3.3362e+02 - logprior: -1.0790e+00
Epoch 4/10
21/21 - 1s - loss: 333.8211 - loglik: -3.3279e+02 - logprior: -1.0357e+00
Epoch 5/10
21/21 - 1s - loss: 333.7506 - loglik: -3.3273e+02 - logprior: -1.0201e+00
Epoch 6/10
21/21 - 1s - loss: 333.3320 - loglik: -3.3232e+02 - logprior: -1.0102e+00
Epoch 7/10
21/21 - 1s - loss: 333.4177 - loglik: -3.3243e+02 - logprior: -9.8326e-01
Fitted a model with MAP estimate = -333.3187
Time for alignment: 46.6547
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 401.5877 - loglik: -3.9847e+02 - logprior: -3.1140e+00
Epoch 2/10
19/19 - 1s - loss: 361.5981 - loglik: -3.6043e+02 - logprior: -1.1672e+00
Epoch 3/10
19/19 - 1s - loss: 347.8478 - loglik: -3.4659e+02 - logprior: -1.2575e+00
Epoch 4/10
19/19 - 1s - loss: 345.5369 - loglik: -3.4432e+02 - logprior: -1.2137e+00
Epoch 5/10
19/19 - 1s - loss: 345.0297 - loglik: -3.4387e+02 - logprior: -1.1627e+00
Epoch 6/10
19/19 - 1s - loss: 344.7945 - loglik: -3.4366e+02 - logprior: -1.1345e+00
Epoch 7/10
19/19 - 1s - loss: 344.7859 - loglik: -3.4367e+02 - logprior: -1.1168e+00
Epoch 8/10
19/19 - 1s - loss: 344.5844 - loglik: -3.4349e+02 - logprior: -1.0949e+00
Epoch 9/10
19/19 - 1s - loss: 344.6433 - loglik: -3.4355e+02 - logprior: -1.0902e+00
Fitted a model with MAP estimate = -342.0734
expansions: [(0, 3), (13, 2), (14, 2), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 347.9155 - loglik: -3.4361e+02 - logprior: -4.3042e+00
Epoch 2/2
19/19 - 1s - loss: 338.1574 - loglik: -3.3671e+02 - logprior: -1.4469e+00
Fitted a model with MAP estimate = -335.2999
expansions: [(0, 2)]
discards: [19 49 60 65]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 340.6499 - loglik: -3.3665e+02 - logprior: -4.0042e+00
Epoch 2/2
19/19 - 1s - loss: 335.3281 - loglik: -3.3407e+02 - logprior: -1.2593e+00
Fitted a model with MAP estimate = -332.5868
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 336.1328 - loglik: -3.3276e+02 - logprior: -3.3728e+00
Epoch 2/10
21/21 - 1s - loss: 332.8823 - loglik: -3.3140e+02 - logprior: -1.4834e+00
Epoch 3/10
21/21 - 1s - loss: 331.8354 - loglik: -3.3077e+02 - logprior: -1.0617e+00
Epoch 4/10
21/21 - 1s - loss: 331.0764 - loglik: -3.3006e+02 - logprior: -1.0137e+00
Epoch 5/10
21/21 - 1s - loss: 330.9839 - loglik: -3.2998e+02 - logprior: -1.0089e+00
Epoch 6/10
21/21 - 1s - loss: 330.6693 - loglik: -3.2968e+02 - logprior: -9.8903e-01
Epoch 7/10
21/21 - 1s - loss: 331.1457 - loglik: -3.3018e+02 - logprior: -9.6797e-01
Fitted a model with MAP estimate = -330.6289
Time for alignment: 47.4265
Computed alignments with likelihoods: ['-331.3168', '-330.4506', '-330.9201', '-333.3187', '-330.6289']
Best model has likelihood: -330.4506  (prior= -0.9446 )
time for generating output: 0.1340
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9488459139114162
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 786.0146 - loglik: -7.7791e+02 - logprior: -8.0999e+00
Epoch 2/10
13/13 - 2s - loss: 736.2245 - loglik: -7.3436e+02 - logprior: -1.8690e+00
Epoch 3/10
13/13 - 2s - loss: 695.9146 - loglik: -6.9418e+02 - logprior: -1.7332e+00
Epoch 4/10
13/13 - 2s - loss: 680.7885 - loglik: -6.7883e+02 - logprior: -1.9607e+00
Epoch 5/10
13/13 - 2s - loss: 676.1625 - loglik: -6.7416e+02 - logprior: -2.0065e+00
Epoch 6/10
13/13 - 2s - loss: 674.5551 - loglik: -6.7262e+02 - logprior: -1.9393e+00
Epoch 7/10
13/13 - 2s - loss: 672.1550 - loglik: -6.7023e+02 - logprior: -1.9221e+00
Epoch 8/10
13/13 - 2s - loss: 673.1862 - loglik: -6.7126e+02 - logprior: -1.9217e+00
Fitted a model with MAP estimate = -672.8701
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (35, 1), (38, 1), (39, 1), (49, 1), (52, 1), (61, 2), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 680.8157 - loglik: -6.7132e+02 - logprior: -9.4947e+00
Epoch 2/2
13/13 - 2s - loss: 661.5341 - loglik: -6.5756e+02 - logprior: -3.9759e+00
Fitted a model with MAP estimate = -658.3641
expansions: [(0, 2)]
discards: [ 0  9 76 88 89 95]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 664.6855 - loglik: -6.5746e+02 - logprior: -7.2279e+00
Epoch 2/2
13/13 - 2s - loss: 653.5815 - loglik: -6.5187e+02 - logprior: -1.7138e+00
Fitted a model with MAP estimate = -653.0782
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 665.4317 - loglik: -6.5619e+02 - logprior: -9.2408e+00
Epoch 2/10
13/13 - 2s - loss: 656.8690 - loglik: -6.5396e+02 - logprior: -2.9118e+00
Epoch 3/10
13/13 - 2s - loss: 654.0685 - loglik: -6.5310e+02 - logprior: -9.7055e-01
Epoch 4/10
13/13 - 2s - loss: 651.7806 - loglik: -6.5141e+02 - logprior: -3.6684e-01
Epoch 5/10
13/13 - 2s - loss: 650.6288 - loglik: -6.5039e+02 - logprior: -2.3859e-01
Epoch 6/10
13/13 - 2s - loss: 650.4664 - loglik: -6.5024e+02 - logprior: -2.2212e-01
Epoch 7/10
13/13 - 2s - loss: 650.7806 - loglik: -6.5052e+02 - logprior: -2.5571e-01
Fitted a model with MAP estimate = -649.8175
Time for alignment: 69.0540
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 787.5516 - loglik: -7.7944e+02 - logprior: -8.1131e+00
Epoch 2/10
13/13 - 2s - loss: 733.8502 - loglik: -7.3197e+02 - logprior: -1.8780e+00
Epoch 3/10
13/13 - 2s - loss: 694.5771 - loglik: -6.9283e+02 - logprior: -1.7487e+00
Epoch 4/10
13/13 - 2s - loss: 679.2910 - loglik: -6.7728e+02 - logprior: -2.0114e+00
Epoch 5/10
13/13 - 2s - loss: 675.1432 - loglik: -6.7307e+02 - logprior: -2.0755e+00
Epoch 6/10
13/13 - 2s - loss: 671.6959 - loglik: -6.6967e+02 - logprior: -2.0304e+00
Epoch 7/10
13/13 - 2s - loss: 672.8589 - loglik: -6.7084e+02 - logprior: -2.0157e+00
Fitted a model with MAP estimate = -671.6824
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 2), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 678.6301 - loglik: -6.6918e+02 - logprior: -9.4459e+00
Epoch 2/2
13/13 - 2s - loss: 662.2680 - loglik: -6.5844e+02 - logprior: -3.8251e+00
Fitted a model with MAP estimate = -658.5871
expansions: [(0, 3)]
discards: [ 0 77]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 663.0916 - loglik: -6.5577e+02 - logprior: -7.3171e+00
Epoch 2/2
13/13 - 2s - loss: 653.6087 - loglik: -6.5187e+02 - logprior: -1.7361e+00
Fitted a model with MAP estimate = -652.6068
expansions: []
discards: [ 0  2 24 89]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 665.5374 - loglik: -6.5641e+02 - logprior: -9.1281e+00
Epoch 2/10
13/13 - 2s - loss: 657.6506 - loglik: -6.5496e+02 - logprior: -2.6918e+00
Epoch 3/10
13/13 - 2s - loss: 653.5072 - loglik: -6.5263e+02 - logprior: -8.8143e-01
Epoch 4/10
13/13 - 2s - loss: 652.1169 - loglik: -6.5175e+02 - logprior: -3.6496e-01
Epoch 5/10
13/13 - 2s - loss: 650.9249 - loglik: -6.5067e+02 - logprior: -2.5875e-01
Epoch 6/10
13/13 - 2s - loss: 650.1722 - loglik: -6.4994e+02 - logprior: -2.2943e-01
Epoch 7/10
13/13 - 2s - loss: 650.7451 - loglik: -6.5051e+02 - logprior: -2.3091e-01
Fitted a model with MAP estimate = -649.8968
Time for alignment: 66.1750
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 786.3801 - loglik: -7.7827e+02 - logprior: -8.1103e+00
Epoch 2/10
13/13 - 2s - loss: 733.5165 - loglik: -7.3165e+02 - logprior: -1.8715e+00
Epoch 3/10
13/13 - 2s - loss: 692.9578 - loglik: -6.9125e+02 - logprior: -1.7106e+00
Epoch 4/10
13/13 - 2s - loss: 680.0979 - loglik: -6.7816e+02 - logprior: -1.9333e+00
Epoch 5/10
13/13 - 2s - loss: 675.7269 - loglik: -6.7379e+02 - logprior: -1.9333e+00
Epoch 6/10
13/13 - 2s - loss: 674.7985 - loglik: -6.7293e+02 - logprior: -1.8705e+00
Epoch 7/10
13/13 - 2s - loss: 672.5087 - loglik: -6.7062e+02 - logprior: -1.8854e+00
Epoch 8/10
13/13 - 2s - loss: 673.8163 - loglik: -6.7192e+02 - logprior: -1.8938e+00
Fitted a model with MAP estimate = -672.8681
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 681.3121 - loglik: -6.7180e+02 - logprior: -9.5152e+00
Epoch 2/2
13/13 - 2s - loss: 661.3145 - loglik: -6.5727e+02 - logprior: -4.0493e+00
Fitted a model with MAP estimate = -658.4945
expansions: [(0, 3)]
discards: [  0   9  66  88  89  95 129]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 663.4385 - loglik: -6.5614e+02 - logprior: -7.2966e+00
Epoch 2/2
13/13 - 2s - loss: 655.5219 - loglik: -6.5383e+02 - logprior: -1.6869e+00
Fitted a model with MAP estimate = -653.2267
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 665.3855 - loglik: -6.5622e+02 - logprior: -9.1612e+00
Epoch 2/10
13/13 - 2s - loss: 656.6560 - loglik: -6.5397e+02 - logprior: -2.6850e+00
Epoch 3/10
13/13 - 2s - loss: 653.9466 - loglik: -6.5307e+02 - logprior: -8.8054e-01
Epoch 4/10
13/13 - 2s - loss: 651.6124 - loglik: -6.5126e+02 - logprior: -3.4855e-01
Epoch 5/10
13/13 - 2s - loss: 651.8378 - loglik: -6.5160e+02 - logprior: -2.4286e-01
Fitted a model with MAP estimate = -650.7171
Time for alignment: 63.8940
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 786.6728 - loglik: -7.7857e+02 - logprior: -8.1032e+00
Epoch 2/10
13/13 - 2s - loss: 734.6161 - loglik: -7.3273e+02 - logprior: -1.8828e+00
Epoch 3/10
13/13 - 2s - loss: 692.2888 - loglik: -6.9054e+02 - logprior: -1.7459e+00
Epoch 4/10
13/13 - 2s - loss: 681.9895 - loglik: -6.8003e+02 - logprior: -1.9609e+00
Epoch 5/10
13/13 - 2s - loss: 675.4776 - loglik: -6.7346e+02 - logprior: -2.0208e+00
Epoch 6/10
13/13 - 2s - loss: 674.3890 - loglik: -6.7246e+02 - logprior: -1.9283e+00
Epoch 7/10
13/13 - 2s - loss: 672.4991 - loglik: -6.7058e+02 - logprior: -1.9158e+00
Epoch 8/10
13/13 - 2s - loss: 672.6149 - loglik: -6.7069e+02 - logprior: -1.9221e+00
Fitted a model with MAP estimate = -672.6075
expansions: [(7, 2), (8, 3), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 680.0635 - loglik: -6.7058e+02 - logprior: -9.4826e+00
Epoch 2/2
13/13 - 2s - loss: 661.2571 - loglik: -6.5725e+02 - logprior: -4.0037e+00
Fitted a model with MAP estimate = -658.1958
expansions: [(0, 3)]
discards: [ 0  9 77 89 90 96]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 664.0932 - loglik: -6.5678e+02 - logprior: -7.3090e+00
Epoch 2/2
13/13 - 2s - loss: 655.0123 - loglik: -6.5328e+02 - logprior: -1.7308e+00
Fitted a model with MAP estimate = -653.1318
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 665.5533 - loglik: -6.5634e+02 - logprior: -9.2130e+00
Epoch 2/10
13/13 - 2s - loss: 657.0292 - loglik: -6.5423e+02 - logprior: -2.7957e+00
Epoch 3/10
13/13 - 2s - loss: 652.8089 - loglik: -6.5187e+02 - logprior: -9.3590e-01
Epoch 4/10
13/13 - 2s - loss: 652.0616 - loglik: -6.5164e+02 - logprior: -4.1822e-01
Epoch 5/10
13/13 - 2s - loss: 652.0677 - loglik: -6.5176e+02 - logprior: -3.0702e-01
Fitted a model with MAP estimate = -650.6813
Time for alignment: 65.4843
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 786.7476 - loglik: -7.7865e+02 - logprior: -8.0975e+00
Epoch 2/10
13/13 - 2s - loss: 734.2872 - loglik: -7.3243e+02 - logprior: -1.8611e+00
Epoch 3/10
13/13 - 2s - loss: 690.6132 - loglik: -6.8889e+02 - logprior: -1.7272e+00
Epoch 4/10
13/13 - 2s - loss: 680.0058 - loglik: -6.7805e+02 - logprior: -1.9605e+00
Epoch 5/10
13/13 - 2s - loss: 675.6215 - loglik: -6.7363e+02 - logprior: -1.9899e+00
Epoch 6/10
13/13 - 2s - loss: 674.0745 - loglik: -6.7216e+02 - logprior: -1.9156e+00
Epoch 7/10
13/13 - 2s - loss: 674.1539 - loglik: -6.7224e+02 - logprior: -1.9137e+00
Fitted a model with MAP estimate = -673.2750
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 1), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 679.8279 - loglik: -6.7035e+02 - logprior: -9.4789e+00
Epoch 2/2
13/13 - 2s - loss: 661.5652 - loglik: -6.5764e+02 - logprior: -3.9288e+00
Fitted a model with MAP estimate = -658.2682
expansions: [(0, 2)]
discards: [ 0  9 87 88 94]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 663.4319 - loglik: -6.5620e+02 - logprior: -7.2323e+00
Epoch 2/2
13/13 - 2s - loss: 654.0566 - loglik: -6.5234e+02 - logprior: -1.7211e+00
Fitted a model with MAP estimate = -653.0654
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 664.7763 - loglik: -6.5555e+02 - logprior: -9.2247e+00
Epoch 2/10
13/13 - 2s - loss: 657.7099 - loglik: -6.5480e+02 - logprior: -2.9128e+00
Epoch 3/10
13/13 - 2s - loss: 654.3124 - loglik: -6.5333e+02 - logprior: -9.7959e-01
Epoch 4/10
13/13 - 2s - loss: 651.3312 - loglik: -6.5096e+02 - logprior: -3.7306e-01
Epoch 5/10
13/13 - 2s - loss: 651.5886 - loglik: -6.5130e+02 - logprior: -2.8726e-01
Fitted a model with MAP estimate = -650.7445
Time for alignment: 58.5049
Computed alignments with likelihoods: ['-649.8175', '-649.8968', '-650.7171', '-650.6813', '-650.7445']
Best model has likelihood: -649.8175  (prior= -0.2787 )
time for generating output: 0.1623
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9423324150596878
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2269.6709 - loglik: -2.2663e+03 - logprior: -3.3895e+00
Epoch 2/10
25/25 - 28s - loss: 2016.3281 - loglik: -2.0155e+03 - logprior: -8.4655e-01
Epoch 3/10
25/25 - 29s - loss: 1961.7639 - loglik: -1.9600e+03 - logprior: -1.7360e+00
Epoch 4/10
25/25 - 29s - loss: 1957.2606 - loglik: -1.9553e+03 - logprior: -1.9253e+00
Epoch 5/10
25/25 - 29s - loss: 1953.6339 - loglik: -1.9518e+03 - logprior: -1.8416e+00
Epoch 6/10
25/25 - 28s - loss: 1951.9098 - loglik: -1.9500e+03 - logprior: -1.9309e+00
Epoch 7/10
25/25 - 29s - loss: 1953.1692 - loglik: -1.9514e+03 - logprior: -1.7512e+00
Fitted a model with MAP estimate = -1950.0447
expansions: [(0, 2), (50, 1), (125, 1), (134, 1), (139, 1), (143, 1), (162, 1), (163, 2), (169, 1), (173, 4), (174, 2), (175, 2), (190, 4), (191, 2), (192, 1), (193, 2), (196, 1), (197, 3), (198, 2), (200, 1), (201, 1), (203, 1), (205, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (213, 1), (214, 1), (215, 1), (220, 1), (221, 1), (222, 1), (226, 1), (228, 1), (229, 1), (230, 1), (233, 1), (236, 1), (237, 1), (238, 1), (249, 1), (252, 1), (253, 1), (254, 3), (255, 2), (257, 1), (258, 1), (281, 1), (282, 1), (283, 1), (297, 1), (299, 1), (300, 2), (301, 2), (318, 2), (319, 2), (321, 2), (326, 1), (339, 2), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 1937.4280 - loglik: -1.9318e+03 - logprior: -5.6451e+00
Epoch 2/2
25/25 - 41s - loss: 1897.0270 - loglik: -1.8966e+03 - logprior: -4.1319e-01
Fitted a model with MAP estimate = -1894.1859
expansions: [(422, 1)]
discards: [  2 187 188 211 230 373 401]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 1904.0962 - loglik: -1.9016e+03 - logprior: -2.5224e+00
Epoch 2/2
25/25 - 40s - loss: 1891.8395 - loglik: -1.8933e+03 - logprior: 1.4817
Fitted a model with MAP estimate = -1891.3349
expansions: []
discards: [185]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1895.5027 - loglik: -1.8935e+03 - logprior: -1.9982e+00
Epoch 2/10
25/25 - 40s - loss: 1901.6333 - loglik: -1.9037e+03 - logprior: 2.0170
Fitted a model with MAP estimate = -1891.2866
Time for alignment: 601.7626
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2273.2695 - loglik: -2.2699e+03 - logprior: -3.3874e+00
Epoch 2/10
25/25 - 29s - loss: 2011.3778 - loglik: -2.0105e+03 - logprior: -8.6788e-01
Epoch 3/10
25/25 - 29s - loss: 1963.9478 - loglik: -1.9622e+03 - logprior: -1.7307e+00
Epoch 4/10
25/25 - 29s - loss: 1951.6305 - loglik: -1.9498e+03 - logprior: -1.8547e+00
Epoch 5/10
25/25 - 29s - loss: 1953.8896 - loglik: -1.9520e+03 - logprior: -1.8592e+00
Fitted a model with MAP estimate = -1950.8357
expansions: [(0, 2), (46, 1), (132, 1), (144, 1), (145, 1), (163, 1), (164, 1), (171, 1), (175, 4), (176, 2), (177, 2), (178, 1), (191, 1), (192, 1), (193, 5), (194, 2), (197, 1), (198, 3), (199, 2), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 2), (227, 1), (229, 1), (230, 1), (231, 1), (236, 1), (238, 1), (239, 2), (250, 1), (255, 3), (256, 2), (257, 1), (259, 5), (267, 1), (281, 1), (282, 1), (283, 1), (299, 2), (300, 1), (301, 2), (303, 1), (314, 1), (319, 1), (322, 2), (325, 2), (327, 2), (341, 1), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 1934.2684 - loglik: -1.9286e+03 - logprior: -5.6205e+00
Epoch 2/2
25/25 - 41s - loss: 1903.9694 - loglik: -1.9036e+03 - logprior: -3.9924e-01
Fitted a model with MAP estimate = -1896.9320
expansions: [(408, 1)]
discards: [  2 186 216 230 322 323 402]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 1905.8879 - loglik: -1.9030e+03 - logprior: -2.8516e+00
Epoch 2/2
25/25 - 40s - loss: 1891.7545 - loglik: -1.8929e+03 - logprior: 1.1292
Fitted a model with MAP estimate = -1889.1463
expansions: [(434, 3)]
discards: [185 310 419]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1896.6530 - loglik: -1.8937e+03 - logprior: -2.9626e+00
Epoch 2/10
25/25 - 40s - loss: 1894.1597 - loglik: -1.8954e+03 - logprior: 1.2838
Epoch 3/10
25/25 - 40s - loss: 1885.0845 - loglik: -1.8871e+03 - logprior: 2.0613
Epoch 4/10
25/25 - 40s - loss: 1884.5057 - loglik: -1.8868e+03 - logprior: 2.3268
Epoch 5/10
25/25 - 40s - loss: 1881.0663 - loglik: -1.8836e+03 - logprior: 2.5644
Epoch 6/10
25/25 - 40s - loss: 1890.3628 - loglik: -1.8932e+03 - logprior: 2.8201
Fitted a model with MAP estimate = -1882.5247
Time for alignment: 704.6939
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2270.1458 - loglik: -2.2668e+03 - logprior: -3.3823e+00
Epoch 2/10
25/25 - 28s - loss: 2011.3448 - loglik: -2.0103e+03 - logprior: -9.9641e-01
Epoch 3/10
25/25 - 29s - loss: 1968.8491 - loglik: -1.9670e+03 - logprior: -1.8861e+00
Epoch 4/10
25/25 - 28s - loss: 1951.6919 - loglik: -1.9497e+03 - logprior: -1.9712e+00
Epoch 5/10
25/25 - 29s - loss: 1960.7640 - loglik: -1.9588e+03 - logprior: -1.9416e+00
Fitted a model with MAP estimate = -1953.7830
expansions: [(0, 2), (136, 1), (145, 1), (164, 1), (165, 2), (171, 1), (176, 1), (177, 2), (178, 2), (179, 1), (192, 1), (193, 1), (194, 3), (195, 1), (196, 2), (199, 1), (200, 1), (201, 1), (202, 1), (204, 1), (205, 2), (208, 1), (210, 1), (211, 1), (214, 1), (215, 1), (217, 1), (219, 1), (220, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (230, 1), (231, 1), (232, 1), (237, 1), (239, 1), (240, 1), (251, 1), (252, 1), (254, 1), (255, 1), (256, 3), (257, 3), (259, 4), (283, 1), (284, 1), (301, 1), (302, 2), (303, 2), (305, 1), (316, 1), (318, 1), (319, 1), (320, 1), (322, 2), (327, 1), (340, 2), (341, 1), (356, 1), (358, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 1943.1724 - loglik: -1.9375e+03 - logprior: -5.6500e+00
Epoch 2/2
25/25 - 40s - loss: 1905.3505 - loglik: -1.9047e+03 - logprior: -6.7812e-01
Fitted a model with MAP estimate = -1899.5626
expansions: [(245, 1), (345, 1), (418, 1), (436, 1)]
discards: [  2 178 190 211 212 216 312 313 318 369 397]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 1911.6438 - loglik: -1.9081e+03 - logprior: -3.5813e+00
Epoch 2/2
25/25 - 39s - loss: 1896.6038 - loglik: -1.8971e+03 - logprior: 0.5457
Fitted a model with MAP estimate = -1894.6108
expansions: [(129, 1), (208, 2), (425, 1)]
discards: [177 426]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 1904.6572 - loglik: -1.9016e+03 - logprior: -3.0267e+00
Epoch 2/10
25/25 - 39s - loss: 1898.1663 - loglik: -1.8995e+03 - logprior: 1.3694
Epoch 3/10
25/25 - 39s - loss: 1886.7535 - loglik: -1.8889e+03 - logprior: 2.0986
Epoch 4/10
25/25 - 39s - loss: 1893.1296 - loglik: -1.8955e+03 - logprior: 2.3393
Fitted a model with MAP estimate = -1889.0685
Time for alignment: 615.3041
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 31s - loss: 2269.4473 - loglik: -2.2660e+03 - logprior: -3.4821e+00
Epoch 2/10
25/25 - 29s - loss: 2022.5844 - loglik: -2.0217e+03 - logprior: -9.0625e-01
Epoch 3/10
25/25 - 29s - loss: 1969.7953 - loglik: -1.9682e+03 - logprior: -1.5815e+00
Epoch 4/10
25/25 - 29s - loss: 1959.6836 - loglik: -1.9578e+03 - logprior: -1.8347e+00
Epoch 5/10
25/25 - 28s - loss: 1951.2792 - loglik: -1.9494e+03 - logprior: -1.8422e+00
Epoch 6/10
25/25 - 29s - loss: 1951.1431 - loglik: -1.9493e+03 - logprior: -1.8777e+00
Epoch 7/10
25/25 - 29s - loss: 1958.2471 - loglik: -1.9564e+03 - logprior: -1.8324e+00
Fitted a model with MAP estimate = -1951.6788
expansions: [(0, 2), (89, 2), (135, 1), (144, 1), (147, 1), (163, 1), (164, 4), (169, 1), (173, 5), (174, 3), (175, 1), (176, 1), (191, 1), (192, 1), (193, 2), (194, 1), (197, 1), (198, 3), (199, 2), (201, 1), (203, 1), (207, 2), (208, 1), (209, 1), (212, 1), (213, 1), (215, 1), (218, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (232, 1), (237, 2), (238, 1), (249, 1), (251, 1), (255, 3), (256, 2), (258, 4), (281, 1), (282, 1), (283, 1), (299, 2), (301, 2), (302, 1), (304, 1), (318, 2), (319, 1), (322, 2), (325, 2), (327, 2), (354, 2), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 1934.1647 - loglik: -1.9280e+03 - logprior: -6.1397e+00
Epoch 2/2
25/25 - 41s - loss: 1903.8553 - loglik: -1.9031e+03 - logprior: -7.7473e-01
Fitted a model with MAP estimate = -1894.8572
expansions: [(251, 1), (409, 1)]
discards: [  1   2 173 190 191 218 231 244 323 374 403 440]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 1904.5782 - loglik: -1.9020e+03 - logprior: -2.5597e+00
Epoch 2/2
25/25 - 40s - loss: 1898.0144 - loglik: -1.8996e+03 - logprior: 1.5538
Fitted a model with MAP estimate = -1893.4686
expansions: [(0, 2), (187, 1), (429, 2)]
discards: [89]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1906.4257 - loglik: -1.9019e+03 - logprior: -4.5417e+00
Epoch 2/10
25/25 - 40s - loss: 1891.7219 - loglik: -1.8930e+03 - logprior: 1.2316
Epoch 3/10
25/25 - 40s - loss: 1890.3035 - loglik: -1.8926e+03 - logprior: 2.2709
Epoch 4/10
25/25 - 40s - loss: 1888.3455 - loglik: -1.8909e+03 - logprior: 2.5653
Epoch 5/10
25/25 - 40s - loss: 1881.4672 - loglik: -1.8842e+03 - logprior: 2.7497
Epoch 6/10
25/25 - 40s - loss: 1890.2532 - loglik: -1.8933e+03 - logprior: 3.0676
Fitted a model with MAP estimate = -1885.3952
Time for alignment: 761.0570
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 31s - loss: 2270.9014 - loglik: -2.2676e+03 - logprior: -3.3393e+00
Epoch 2/10
25/25 - 29s - loss: 2011.7158 - loglik: -2.0109e+03 - logprior: -8.2364e-01
Epoch 3/10
25/25 - 29s - loss: 1963.9996 - loglik: -1.9623e+03 - logprior: -1.7027e+00
Epoch 4/10
25/25 - 29s - loss: 1944.4012 - loglik: -1.9426e+03 - logprior: -1.8188e+00
Epoch 5/10
25/25 - 29s - loss: 1947.0898 - loglik: -1.9453e+03 - logprior: -1.8175e+00
Fitted a model with MAP estimate = -1946.6709
expansions: [(0, 2), (46, 1), (132, 1), (144, 2), (146, 1), (164, 1), (165, 1), (170, 1), (174, 5), (175, 3), (176, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (196, 1), (197, 1), (198, 2), (199, 1), (201, 1), (203, 1), (206, 1), (208, 1), (209, 1), (211, 1), (212, 1), (213, 1), (215, 1), (217, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (238, 2), (250, 1), (251, 1), (253, 1), (254, 3), (255, 2), (256, 2), (258, 4), (281, 1), (282, 2), (283, 1), (299, 1), (300, 1), (301, 2), (304, 1), (315, 1), (317, 1), (318, 1), (319, 1), (321, 2), (325, 2), (327, 2), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 1928.0535 - loglik: -1.9228e+03 - logprior: -5.2851e+00
Epoch 2/2
25/25 - 41s - loss: 1897.2192 - loglik: -1.8973e+03 - logprior: 0.0618
Fitted a model with MAP estimate = -1892.4706
expansions: [(53, 1)]
discards: [  2 188 216 317 318 321 374 401]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 1902.7777 - loglik: -1.8999e+03 - logprior: -2.8555e+00
Epoch 2/2
25/25 - 40s - loss: 1895.1724 - loglik: -1.8963e+03 - logprior: 1.1719
Fitted a model with MAP estimate = -1890.9923
expansions: []
discards: [52]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 1903.2781 - loglik: -1.9010e+03 - logprior: -2.3034e+00
Epoch 2/10
25/25 - 40s - loss: 1889.6925 - loglik: -1.8914e+03 - logprior: 1.7551
Epoch 3/10
25/25 - 40s - loss: 1890.3506 - loglik: -1.8927e+03 - logprior: 2.3803
Fitted a model with MAP estimate = -1889.1897
Time for alignment: 582.2738
Computed alignments with likelihoods: ['-1891.2866', '-1882.5247', '-1889.0685', '-1885.3952', '-1889.1897']
Best model has likelihood: -1882.5247  (prior= 3.0753 )
time for generating output: 0.4461
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9072109332853803
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 671.6172 - loglik: -6.6712e+02 - logprior: -4.4977e+00
Epoch 2/10
16/16 - 3s - loss: 601.2829 - loglik: -5.9978e+02 - logprior: -1.5006e+00
Epoch 3/10
16/16 - 4s - loss: 569.2625 - loglik: -5.6759e+02 - logprior: -1.6696e+00
Epoch 4/10
16/16 - 4s - loss: 562.9410 - loglik: -5.6143e+02 - logprior: -1.5091e+00
Epoch 5/10
16/16 - 3s - loss: 555.8390 - loglik: -5.5435e+02 - logprior: -1.4913e+00
Epoch 6/10
16/16 - 3s - loss: 554.3829 - loglik: -5.5291e+02 - logprior: -1.4715e+00
Epoch 7/10
16/16 - 3s - loss: 549.5601 - loglik: -5.4808e+02 - logprior: -1.4774e+00
Epoch 8/10
16/16 - 3s - loss: 548.6010 - loglik: -5.4713e+02 - logprior: -1.4742e+00
Epoch 9/10
16/16 - 3s - loss: 546.9048 - loglik: -5.4542e+02 - logprior: -1.4828e+00
Epoch 10/10
16/16 - 4s - loss: 546.1505 - loglik: -5.4464e+02 - logprior: -1.5076e+00
Fitted a model with MAP estimate = -545.2557
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (18, 1), (72, 1), (96, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 549.8830 - loglik: -5.4594e+02 - logprior: -3.9473e+00
Epoch 2/2
33/33 - 4s - loss: 541.3113 - loglik: -5.3978e+02 - logprior: -1.5283e+00
Fitted a model with MAP estimate = -539.5572
expansions: [(20, 1)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 543.7028 - loglik: -5.4050e+02 - logprior: -3.2009e+00
Epoch 2/2
33/33 - 5s - loss: 540.4518 - loglik: -5.3904e+02 - logprior: -1.4128e+00
Fitted a model with MAP estimate = -538.9637
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 543.7495 - loglik: -5.4067e+02 - logprior: -3.0838e+00
Epoch 2/10
33/33 - 5s - loss: 539.3678 - loglik: -5.3809e+02 - logprior: -1.2744e+00
Epoch 3/10
33/33 - 4s - loss: 538.6855 - loglik: -5.3753e+02 - logprior: -1.1532e+00
Epoch 4/10
33/33 - 5s - loss: 539.8799 - loglik: -5.3883e+02 - logprior: -1.0483e+00
Fitted a model with MAP estimate = -536.7091
Time for alignment: 105.3854
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 671.8175 - loglik: -6.6734e+02 - logprior: -4.4792e+00
Epoch 2/10
16/16 - 3s - loss: 599.2959 - loglik: -5.9782e+02 - logprior: -1.4733e+00
Epoch 3/10
16/16 - 3s - loss: 571.1777 - loglik: -5.6953e+02 - logprior: -1.6510e+00
Epoch 4/10
16/16 - 3s - loss: 562.3542 - loglik: -5.6089e+02 - logprior: -1.4674e+00
Epoch 5/10
16/16 - 3s - loss: 557.0422 - loglik: -5.5553e+02 - logprior: -1.5162e+00
Epoch 6/10
16/16 - 3s - loss: 554.1283 - loglik: -5.5270e+02 - logprior: -1.4323e+00
Epoch 7/10
16/16 - 3s - loss: 549.6066 - loglik: -5.4817e+02 - logprior: -1.4325e+00
Epoch 8/10
16/16 - 3s - loss: 548.2116 - loglik: -5.4676e+02 - logprior: -1.4553e+00
Epoch 9/10
16/16 - 3s - loss: 546.4457 - loglik: -5.4500e+02 - logprior: -1.4481e+00
Epoch 10/10
16/16 - 3s - loss: 545.8213 - loglik: -5.4436e+02 - logprior: -1.4564e+00
Fitted a model with MAP estimate = -546.2957
expansions: [(0, 1), (12, 1), (15, 1), (16, 3), (79, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 555.6409 - loglik: -5.4975e+02 - logprior: -5.8889e+00
Epoch 2/2
16/16 - 4s - loss: 546.8178 - loglik: -5.4453e+02 - logprior: -2.2842e+00
Fitted a model with MAP estimate = -543.3455
expansions: [(20, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 548.4985 - loglik: -5.4348e+02 - logprior: -5.0194e+00
Epoch 2/2
16/16 - 3s - loss: 543.6779 - loglik: -5.4208e+02 - logprior: -1.5933e+00
Fitted a model with MAP estimate = -541.9847
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 544.4580 - loglik: -5.4095e+02 - logprior: -3.5078e+00
Epoch 2/10
33/33 - 5s - loss: 543.8055 - loglik: -5.4244e+02 - logprior: -1.3675e+00
Epoch 3/10
33/33 - 4s - loss: 539.1393 - loglik: -5.3790e+02 - logprior: -1.2425e+00
Epoch 4/10
33/33 - 4s - loss: 538.1696 - loglik: -5.3701e+02 - logprior: -1.1596e+00
Epoch 5/10
33/33 - 4s - loss: 539.8945 - loglik: -5.3886e+02 - logprior: -1.0389e+00
Fitted a model with MAP estimate = -537.8194
Time for alignment: 101.7073
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 672.4890 - loglik: -6.6799e+02 - logprior: -4.5029e+00
Epoch 2/10
16/16 - 4s - loss: 599.3487 - loglik: -5.9784e+02 - logprior: -1.5071e+00
Epoch 3/10
16/16 - 3s - loss: 572.1538 - loglik: -5.7044e+02 - logprior: -1.7132e+00
Epoch 4/10
16/16 - 3s - loss: 558.8494 - loglik: -5.5729e+02 - logprior: -1.5632e+00
Epoch 5/10
16/16 - 3s - loss: 554.6939 - loglik: -5.5314e+02 - logprior: -1.5538e+00
Epoch 6/10
16/16 - 3s - loss: 550.8618 - loglik: -5.4932e+02 - logprior: -1.5384e+00
Epoch 7/10
16/16 - 3s - loss: 548.5056 - loglik: -5.4697e+02 - logprior: -1.5343e+00
Epoch 8/10
16/16 - 3s - loss: 547.6873 - loglik: -5.4617e+02 - logprior: -1.5203e+00
Epoch 9/10
16/16 - 3s - loss: 547.7211 - loglik: -5.4620e+02 - logprior: -1.5174e+00
Fitted a model with MAP estimate = -547.6514
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (21, 1), (70, 1), (72, 1), (96, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 550.6685 - loglik: -5.4674e+02 - logprior: -3.9326e+00
Epoch 2/2
33/33 - 5s - loss: 543.6277 - loglik: -5.4205e+02 - logprior: -1.5740e+00
Fitted a model with MAP estimate = -541.3166
expansions: [(22, 1)]
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 545.4318 - loglik: -5.4220e+02 - logprior: -3.2342e+00
Epoch 2/2
33/33 - 5s - loss: 541.8171 - loglik: -5.4037e+02 - logprior: -1.4469e+00
Fitted a model with MAP estimate = -539.0974
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 542.8882 - loglik: -5.3977e+02 - logprior: -3.1170e+00
Epoch 2/10
33/33 - 5s - loss: 539.5342 - loglik: -5.3823e+02 - logprior: -1.3067e+00
Epoch 3/10
33/33 - 5s - loss: 539.4639 - loglik: -5.3828e+02 - logprior: -1.1851e+00
Epoch 4/10
33/33 - 5s - loss: 536.8304 - loglik: -5.3575e+02 - logprior: -1.0807e+00
Epoch 5/10
33/33 - 5s - loss: 537.4389 - loglik: -5.3649e+02 - logprior: -9.4970e-01
Fitted a model with MAP estimate = -535.8459
Time for alignment: 108.8748
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 671.4040 - loglik: -6.6691e+02 - logprior: -4.4961e+00
Epoch 2/10
16/16 - 3s - loss: 603.9583 - loglik: -6.0245e+02 - logprior: -1.5053e+00
Epoch 3/10
16/16 - 3s - loss: 570.9879 - loglik: -5.6930e+02 - logprior: -1.6831e+00
Epoch 4/10
16/16 - 3s - loss: 560.7424 - loglik: -5.5926e+02 - logprior: -1.4796e+00
Epoch 5/10
16/16 - 3s - loss: 558.8092 - loglik: -5.5730e+02 - logprior: -1.5088e+00
Epoch 6/10
16/16 - 3s - loss: 556.2544 - loglik: -5.5481e+02 - logprior: -1.4477e+00
Epoch 7/10
16/16 - 4s - loss: 549.4148 - loglik: -5.4794e+02 - logprior: -1.4757e+00
Epoch 8/10
16/16 - 3s - loss: 549.4586 - loglik: -5.4798e+02 - logprior: -1.4816e+00
Fitted a model with MAP estimate = -548.0830
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (18, 1), (69, 1), (94, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 553.1013 - loglik: -5.4924e+02 - logprior: -3.8575e+00
Epoch 2/2
33/33 - 5s - loss: 543.4831 - loglik: -5.4195e+02 - logprior: -1.5373e+00
Fitted a model with MAP estimate = -540.4879
expansions: [(20, 1)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 545.4793 - loglik: -5.4229e+02 - logprior: -3.1867e+00
Epoch 2/2
33/33 - 5s - loss: 540.7800 - loglik: -5.3938e+02 - logprior: -1.4010e+00
Fitted a model with MAP estimate = -540.0632
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 545.1357 - loglik: -5.4207e+02 - logprior: -3.0700e+00
Epoch 2/10
33/33 - 5s - loss: 540.5689 - loglik: -5.3929e+02 - logprior: -1.2743e+00
Epoch 3/10
33/33 - 5s - loss: 539.1149 - loglik: -5.3797e+02 - logprior: -1.1453e+00
Epoch 4/10
33/33 - 4s - loss: 538.9944 - loglik: -5.3796e+02 - logprior: -1.0392e+00
Epoch 5/10
33/33 - 5s - loss: 536.6662 - loglik: -5.3575e+02 - logprior: -9.1874e-01
Epoch 6/10
33/33 - 5s - loss: 539.3087 - loglik: -5.3847e+02 - logprior: -8.3420e-01
Fitted a model with MAP estimate = -536.9258
Time for alignment: 105.8959
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 671.8592 - loglik: -6.6738e+02 - logprior: -4.4763e+00
Epoch 2/10
16/16 - 3s - loss: 597.1111 - loglik: -5.9563e+02 - logprior: -1.4813e+00
Epoch 3/10
16/16 - 4s - loss: 571.2535 - loglik: -5.6955e+02 - logprior: -1.6986e+00
Epoch 4/10
16/16 - 3s - loss: 560.8299 - loglik: -5.5930e+02 - logprior: -1.5268e+00
Epoch 5/10
16/16 - 3s - loss: 554.8841 - loglik: -5.5338e+02 - logprior: -1.4992e+00
Epoch 6/10
16/16 - 3s - loss: 551.4811 - loglik: -5.4996e+02 - logprior: -1.5183e+00
Epoch 7/10
16/16 - 3s - loss: 548.7761 - loglik: -5.4724e+02 - logprior: -1.5394e+00
Epoch 8/10
16/16 - 3s - loss: 546.8715 - loglik: -5.4534e+02 - logprior: -1.5304e+00
Epoch 9/10
16/16 - 3s - loss: 545.1354 - loglik: -5.4361e+02 - logprior: -1.5229e+00
Epoch 10/10
16/16 - 3s - loss: 544.7325 - loglik: -5.4320e+02 - logprior: -1.5344e+00
Fitted a model with MAP estimate = -544.7406
expansions: [(0, 1), (10, 1), (11, 1), (15, 1), (16, 2), (69, 1), (94, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 550.2160 - loglik: -5.4632e+02 - logprior: -3.8982e+00
Epoch 2/2
33/33 - 4s - loss: 542.2202 - loglik: -5.4069e+02 - logprior: -1.5321e+00
Fitted a model with MAP estimate = -540.0588
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 544.4966 - loglik: -5.4128e+02 - logprior: -3.2188e+00
Epoch 2/10
33/33 - 4s - loss: 539.2527 - loglik: -5.3784e+02 - logprior: -1.4142e+00
Epoch 3/10
33/33 - 5s - loss: 540.8818 - loglik: -5.3959e+02 - logprior: -1.2931e+00
Fitted a model with MAP estimate = -538.0719
Time for alignment: 82.0034
Computed alignments with likelihoods: ['-536.7091', '-537.8194', '-535.8459', '-536.9258', '-538.0719']
Best model has likelihood: -535.8459  (prior= -0.8746 )
time for generating output: 0.2612
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.4856286111314269
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 621.7700 - loglik: -6.1865e+02 - logprior: -3.1188e+00
Epoch 2/10
19/19 - 2s - loss: 592.0765 - loglik: -5.9093e+02 - logprior: -1.1478e+00
Epoch 3/10
19/19 - 2s - loss: 576.5358 - loglik: -5.7506e+02 - logprior: -1.4731e+00
Epoch 4/10
19/19 - 2s - loss: 571.5438 - loglik: -5.7012e+02 - logprior: -1.4231e+00
Epoch 5/10
19/19 - 2s - loss: 570.0651 - loglik: -5.6868e+02 - logprior: -1.3851e+00
Epoch 6/10
19/19 - 2s - loss: 569.8410 - loglik: -5.6847e+02 - logprior: -1.3754e+00
Epoch 7/10
19/19 - 2s - loss: 568.5781 - loglik: -5.6722e+02 - logprior: -1.3590e+00
Epoch 8/10
19/19 - 2s - loss: 568.6858 - loglik: -5.6734e+02 - logprior: -1.3504e+00
Fitted a model with MAP estimate = -538.6786
expansions: [(6, 3), (7, 2), (10, 1), (29, 1), (33, 10), (38, 2), (43, 2), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 573.6765 - loglik: -5.6976e+02 - logprior: -3.9181e+00
Epoch 2/2
19/19 - 2s - loss: 564.5210 - loglik: -5.6248e+02 - logprior: -2.0362e+00
Fitted a model with MAP estimate = -532.2142
expansions: [(0, 2)]
discards: [ 0  8 42 43 44 45 46 47 48 54 80 87]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 567.0429 - loglik: -5.6418e+02 - logprior: -2.8660e+00
Epoch 2/2
19/19 - 2s - loss: 563.1285 - loglik: -5.6204e+02 - logprior: -1.0851e+00
Fitted a model with MAP estimate = -532.0403
expansions: [(42, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 533.8617 - loglik: -5.3130e+02 - logprior: -2.5624e+00
Epoch 2/10
23/23 - 3s - loss: 530.3367 - loglik: -5.2929e+02 - logprior: -1.0446e+00
Epoch 3/10
23/23 - 3s - loss: 528.1412 - loglik: -5.2720e+02 - logprior: -9.4250e-01
Epoch 4/10
23/23 - 3s - loss: 528.4799 - loglik: -5.2756e+02 - logprior: -9.2009e-01
Fitted a model with MAP estimate = -527.1938
Time for alignment: 68.1258
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 622.2240 - loglik: -6.1911e+02 - logprior: -3.1161e+00
Epoch 2/10
19/19 - 2s - loss: 590.3200 - loglik: -5.8920e+02 - logprior: -1.1242e+00
Epoch 3/10
19/19 - 2s - loss: 574.6559 - loglik: -5.7321e+02 - logprior: -1.4467e+00
Epoch 4/10
19/19 - 2s - loss: 572.0009 - loglik: -5.7059e+02 - logprior: -1.4059e+00
Epoch 5/10
19/19 - 2s - loss: 569.3379 - loglik: -5.6798e+02 - logprior: -1.3623e+00
Epoch 6/10
19/19 - 2s - loss: 569.9781 - loglik: -5.6864e+02 - logprior: -1.3421e+00
Fitted a model with MAP estimate = -538.5700
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 1), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 573.9350 - loglik: -5.7003e+02 - logprior: -3.9049e+00
Epoch 2/2
19/19 - 2s - loss: 564.1044 - loglik: -5.6211e+02 - logprior: -1.9964e+00
Fitted a model with MAP estimate = -532.5764
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 27 54 86]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 564.4540 - loglik: -5.6155e+02 - logprior: -2.8991e+00
Epoch 2/2
19/19 - 2s - loss: 561.3921 - loglik: -5.6028e+02 - logprior: -1.1087e+00
Fitted a model with MAP estimate = -530.7438
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 533.3551 - loglik: -5.3082e+02 - logprior: -2.5356e+00
Epoch 2/10
23/23 - 3s - loss: 530.0896 - loglik: -5.2908e+02 - logprior: -1.0129e+00
Epoch 3/10
23/23 - 3s - loss: 528.3488 - loglik: -5.2738e+02 - logprior: -9.7211e-01
Epoch 4/10
23/23 - 3s - loss: 527.8692 - loglik: -5.2694e+02 - logprior: -9.3379e-01
Epoch 5/10
23/23 - 3s - loss: 527.6381 - loglik: -5.2672e+02 - logprior: -9.1376e-01
Epoch 6/10
23/23 - 3s - loss: 527.0287 - loglik: -5.2611e+02 - logprior: -9.1576e-01
Epoch 7/10
23/23 - 3s - loss: 526.5547 - loglik: -5.2564e+02 - logprior: -9.1082e-01
Epoch 8/10
23/23 - 3s - loss: 526.1285 - loglik: -5.2524e+02 - logprior: -8.9303e-01
Epoch 9/10
23/23 - 3s - loss: 526.1343 - loglik: -5.2525e+02 - logprior: -8.8920e-01
Fitted a model with MAP estimate = -526.1803
Time for alignment: 77.6561
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 621.6991 - loglik: -6.1858e+02 - logprior: -3.1164e+00
Epoch 2/10
19/19 - 2s - loss: 590.7749 - loglik: -5.8963e+02 - logprior: -1.1434e+00
Epoch 3/10
19/19 - 2s - loss: 575.9790 - loglik: -5.7451e+02 - logprior: -1.4714e+00
Epoch 4/10
19/19 - 2s - loss: 571.3481 - loglik: -5.6993e+02 - logprior: -1.4167e+00
Epoch 5/10
19/19 - 2s - loss: 570.2068 - loglik: -5.6884e+02 - logprior: -1.3688e+00
Epoch 6/10
19/19 - 2s - loss: 569.4153 - loglik: -5.6806e+02 - logprior: -1.3512e+00
Epoch 7/10
19/19 - 2s - loss: 568.5523 - loglik: -5.6722e+02 - logprior: -1.3341e+00
Epoch 8/10
19/19 - 2s - loss: 570.2321 - loglik: -5.6891e+02 - logprior: -1.3253e+00
Fitted a model with MAP estimate = -538.6057
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (21, 2), (33, 9), (38, 2), (43, 2), (55, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 575.4920 - loglik: -5.7155e+02 - logprior: -3.9437e+00
Epoch 2/2
19/19 - 2s - loss: 564.2978 - loglik: -5.6219e+02 - logprior: -2.1047e+00
Fitted a model with MAP estimate = -532.6572
expansions: [(0, 2)]
discards: [ 0  8 10 15 29 46 47 48 49 56 77 83 90]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 566.6476 - loglik: -5.6376e+02 - logprior: -2.8871e+00
Epoch 2/2
19/19 - 2s - loss: 562.6753 - loglik: -5.6157e+02 - logprior: -1.1006e+00
Fitted a model with MAP estimate = -531.7171
expansions: [(43, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 533.4251 - loglik: -5.3085e+02 - logprior: -2.5716e+00
Epoch 2/10
23/23 - 3s - loss: 530.3971 - loglik: -5.2935e+02 - logprior: -1.0475e+00
Epoch 3/10
23/23 - 3s - loss: 528.3080 - loglik: -5.2734e+02 - logprior: -9.6671e-01
Epoch 4/10
23/23 - 3s - loss: 527.5705 - loglik: -5.2664e+02 - logprior: -9.3394e-01
Epoch 5/10
23/23 - 3s - loss: 527.2095 - loglik: -5.2629e+02 - logprior: -9.1948e-01
Epoch 6/10
23/23 - 3s - loss: 526.8376 - loglik: -5.2593e+02 - logprior: -9.0957e-01
Epoch 7/10
23/23 - 3s - loss: 526.6777 - loglik: -5.2577e+02 - logprior: -9.1057e-01
Epoch 8/10
23/23 - 3s - loss: 525.7395 - loglik: -5.2485e+02 - logprior: -8.9130e-01
Epoch 9/10
23/23 - 3s - loss: 525.9785 - loglik: -5.2509e+02 - logprior: -8.8706e-01
Fitted a model with MAP estimate = -525.8968
Time for alignment: 81.6938
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 621.8926 - loglik: -6.1878e+02 - logprior: -3.1171e+00
Epoch 2/10
19/19 - 2s - loss: 591.1888 - loglik: -5.9006e+02 - logprior: -1.1333e+00
Epoch 3/10
19/19 - 2s - loss: 576.9208 - loglik: -5.7547e+02 - logprior: -1.4482e+00
Epoch 4/10
19/19 - 2s - loss: 572.1132 - loglik: -5.7069e+02 - logprior: -1.4203e+00
Epoch 5/10
19/19 - 2s - loss: 570.3145 - loglik: -5.6895e+02 - logprior: -1.3659e+00
Epoch 6/10
19/19 - 2s - loss: 569.0702 - loglik: -5.6772e+02 - logprior: -1.3508e+00
Epoch 7/10
19/19 - 2s - loss: 569.4414 - loglik: -5.6811e+02 - logprior: -1.3339e+00
Fitted a model with MAP estimate = -538.6727
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (43, 2), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 574.4199 - loglik: -5.7050e+02 - logprior: -3.9170e+00
Epoch 2/2
19/19 - 2s - loss: 564.7251 - loglik: -5.6270e+02 - logprior: -2.0288e+00
Fitted a model with MAP estimate = -532.5750
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 27 54 80 87]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 564.7401 - loglik: -5.6184e+02 - logprior: -2.8992e+00
Epoch 2/2
19/19 - 2s - loss: 561.2291 - loglik: -5.6012e+02 - logprior: -1.1108e+00
Fitted a model with MAP estimate = -530.3512
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 533.4815 - loglik: -5.3098e+02 - logprior: -2.4992e+00
Epoch 2/10
23/23 - 3s - loss: 529.3542 - loglik: -5.2834e+02 - logprior: -1.0147e+00
Epoch 3/10
23/23 - 3s - loss: 527.7206 - loglik: -5.2676e+02 - logprior: -9.5902e-01
Epoch 4/10
23/23 - 3s - loss: 529.4417 - loglik: -5.2852e+02 - logprior: -9.2529e-01
Fitted a model with MAP estimate = -527.5085
Time for alignment: 65.9926
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 621.8337 - loglik: -6.1872e+02 - logprior: -3.1156e+00
Epoch 2/10
19/19 - 2s - loss: 589.9905 - loglik: -5.8886e+02 - logprior: -1.1299e+00
Epoch 3/10
19/19 - 2s - loss: 574.3457 - loglik: -5.7290e+02 - logprior: -1.4503e+00
Epoch 4/10
19/19 - 2s - loss: 571.2175 - loglik: -5.6981e+02 - logprior: -1.4097e+00
Epoch 5/10
19/19 - 2s - loss: 569.8392 - loglik: -5.6848e+02 - logprior: -1.3544e+00
Epoch 6/10
19/19 - 2s - loss: 569.8397 - loglik: -5.6850e+02 - logprior: -1.3379e+00
Fitted a model with MAP estimate = -538.5798
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 573.8126 - loglik: -5.6989e+02 - logprior: -3.9204e+00
Epoch 2/2
19/19 - 2s - loss: 564.7838 - loglik: -5.6275e+02 - logprior: -2.0357e+00
Fitted a model with MAP estimate = -532.5473
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 27 54 80 87]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 564.7983 - loglik: -5.6189e+02 - logprior: -2.9079e+00
Epoch 2/2
19/19 - 2s - loss: 560.6611 - loglik: -5.5955e+02 - logprior: -1.1137e+00
Fitted a model with MAP estimate = -530.2686
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 533.0733 - loglik: -5.3059e+02 - logprior: -2.4827e+00
Epoch 2/10
23/23 - 3s - loss: 529.0463 - loglik: -5.2804e+02 - logprior: -1.0033e+00
Epoch 3/10
23/23 - 3s - loss: 528.9896 - loglik: -5.2803e+02 - logprior: -9.6394e-01
Epoch 4/10
23/23 - 3s - loss: 528.0634 - loglik: -5.2713e+02 - logprior: -9.3271e-01
Epoch 5/10
23/23 - 3s - loss: 527.5081 - loglik: -5.2659e+02 - logprior: -9.1944e-01
Epoch 6/10
23/23 - 3s - loss: 527.3071 - loglik: -5.2639e+02 - logprior: -9.1225e-01
Epoch 7/10
23/23 - 3s - loss: 526.4921 - loglik: -5.2559e+02 - logprior: -9.0627e-01
Epoch 8/10
23/23 - 3s - loss: 526.4083 - loglik: -5.2551e+02 - logprior: -8.9552e-01
Epoch 9/10
23/23 - 3s - loss: 525.8076 - loglik: -5.2492e+02 - logprior: -8.8449e-01
Epoch 10/10
23/23 - 3s - loss: 527.3304 - loglik: -5.2646e+02 - logprior: -8.7165e-01
Fitted a model with MAP estimate = -526.1550
Time for alignment: 79.7208
Computed alignments with likelihoods: ['-527.1938', '-526.1803', '-525.8968', '-527.5085', '-526.1550']
Best model has likelihood: -525.8968  (prior= -0.8494 )
time for generating output: 0.1636
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.7852494577006508
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 823.8240 - loglik: -7.7834e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 762.0184 - loglik: -7.5172e+02 - logprior: -1.0303e+01
Epoch 3/10
10/10 - 2s - loss: 724.7291 - loglik: -7.2056e+02 - logprior: -4.1682e+00
Epoch 4/10
10/10 - 2s - loss: 701.0980 - loglik: -6.9896e+02 - logprior: -2.1419e+00
Epoch 5/10
10/10 - 2s - loss: 693.5419 - loglik: -6.9233e+02 - logprior: -1.2149e+00
Epoch 6/10
10/10 - 2s - loss: 689.5900 - loglik: -6.8896e+02 - logprior: -6.2611e-01
Epoch 7/10
10/10 - 2s - loss: 689.4852 - loglik: -6.8933e+02 - logprior: -1.5563e-01
Epoch 8/10
10/10 - 2s - loss: 687.0008 - loglik: -6.8715e+02 - logprior: 0.1508
Epoch 9/10
10/10 - 2s - loss: 687.0453 - loglik: -6.8732e+02 - logprior: 0.2715
Fitted a model with MAP estimate = -686.9382
expansions: [(7, 2), (10, 2), (17, 1), (18, 1), (29, 3), (42, 2), (48, 2), (59, 3), (62, 1), (71, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 738.9006 - loglik: -6.8824e+02 - logprior: -5.0663e+01
Epoch 2/2
10/10 - 2s - loss: 697.2709 - loglik: -6.7808e+02 - logprior: -1.9186e+01
Fitted a model with MAP estimate = -690.2880
expansions: [(9, 3)]
discards: [  0  11  59  72 110]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 726.6656 - loglik: -6.7705e+02 - logprior: -4.9614e+01
Epoch 2/2
10/10 - 2s - loss: 689.9791 - loglik: -6.7285e+02 - logprior: -1.7131e+01
Fitted a model with MAP estimate = -682.8911
expansions: [(0, 3)]
discards: [ 0 36 52]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 711.0170 - loglik: -6.7226e+02 - logprior: -3.8752e+01
Epoch 2/10
10/10 - 2s - loss: 677.5719 - loglik: -6.7018e+02 - logprior: -7.3891e+00
Epoch 3/10
10/10 - 2s - loss: 669.9820 - loglik: -6.6902e+02 - logprior: -9.5961e-01
Epoch 4/10
10/10 - 2s - loss: 667.1830 - loglik: -6.6885e+02 - logprior: 1.6712
Epoch 5/10
10/10 - 2s - loss: 665.1351 - loglik: -6.6825e+02 - logprior: 3.1192
Epoch 6/10
10/10 - 2s - loss: 665.6263 - loglik: -6.6957e+02 - logprior: 3.9447
Fitted a model with MAP estimate = -664.5360
Time for alignment: 57.5127
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 824.3041 - loglik: -7.7882e+02 - logprior: -4.5481e+01
Epoch 2/10
10/10 - 2s - loss: 761.5073 - loglik: -7.5119e+02 - logprior: -1.0312e+01
Epoch 3/10
10/10 - 2s - loss: 725.6001 - loglik: -7.2146e+02 - logprior: -4.1395e+00
Epoch 4/10
10/10 - 2s - loss: 703.9427 - loglik: -7.0185e+02 - logprior: -2.0972e+00
Epoch 5/10
10/10 - 2s - loss: 694.6488 - loglik: -6.9336e+02 - logprior: -1.2848e+00
Epoch 6/10
10/10 - 2s - loss: 690.5625 - loglik: -6.8963e+02 - logprior: -9.3131e-01
Epoch 7/10
10/10 - 2s - loss: 689.0353 - loglik: -6.8848e+02 - logprior: -5.5564e-01
Epoch 8/10
10/10 - 2s - loss: 687.4877 - loglik: -6.8727e+02 - logprior: -2.1617e-01
Epoch 9/10
10/10 - 2s - loss: 687.2709 - loglik: -6.8725e+02 - logprior: -2.5947e-02
Epoch 10/10
10/10 - 2s - loss: 686.4456 - loglik: -6.8652e+02 - logprior: 0.0739
Fitted a model with MAP estimate = -686.6193
expansions: [(12, 1), (13, 1), (20, 1), (27, 1), (28, 3), (43, 1), (49, 2), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 739.5865 - loglik: -6.8886e+02 - logprior: -5.0729e+01
Epoch 2/2
10/10 - 2s - loss: 697.1476 - loglik: -6.7802e+02 - logprior: -1.9123e+01
Fitted a model with MAP estimate = -691.4079
expansions: []
discards: [ 0 31 56 68 74]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 729.0804 - loglik: -6.7949e+02 - logprior: -4.9592e+01
Epoch 2/2
10/10 - 2s - loss: 692.9911 - loglik: -6.7675e+02 - logprior: -1.6242e+01
Fitted a model with MAP estimate = -685.1893
expansions: [(0, 6), (5, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 713.1870 - loglik: -6.7414e+02 - logprior: -3.9048e+01
Epoch 2/10
10/10 - 2s - loss: 679.5304 - loglik: -6.7201e+02 - logprior: -7.5248e+00
Epoch 3/10
10/10 - 2s - loss: 671.2250 - loglik: -6.7014e+02 - logprior: -1.0878e+00
Epoch 4/10
10/10 - 2s - loss: 667.7988 - loglik: -6.6929e+02 - logprior: 1.4922
Epoch 5/10
10/10 - 2s - loss: 666.1832 - loglik: -6.6908e+02 - logprior: 2.8964
Epoch 6/10
10/10 - 2s - loss: 665.2277 - loglik: -6.6897e+02 - logprior: 3.7473
Epoch 7/10
10/10 - 2s - loss: 665.2975 - loglik: -6.6957e+02 - logprior: 4.2720
Fitted a model with MAP estimate = -664.6543
Time for alignment: 57.9958
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 824.3284 - loglik: -7.7885e+02 - logprior: -4.5480e+01
Epoch 2/10
10/10 - 2s - loss: 761.1389 - loglik: -7.5083e+02 - logprior: -1.0305e+01
Epoch 3/10
10/10 - 2s - loss: 724.0304 - loglik: -7.1992e+02 - logprior: -4.1109e+00
Epoch 4/10
10/10 - 2s - loss: 703.1018 - loglik: -7.0109e+02 - logprior: -2.0110e+00
Epoch 5/10
10/10 - 2s - loss: 694.8834 - loglik: -6.9388e+02 - logprior: -1.0025e+00
Epoch 6/10
10/10 - 2s - loss: 690.5431 - loglik: -6.9014e+02 - logprior: -4.0429e-01
Epoch 7/10
10/10 - 2s - loss: 688.6052 - loglik: -6.8859e+02 - logprior: -1.7334e-02
Epoch 8/10
10/10 - 2s - loss: 688.7548 - loglik: -6.8894e+02 - logprior: 0.1876
Fitted a model with MAP estimate = -687.9038
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (40, 2), (49, 2), (58, 2), (59, 1), (62, 2), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 738.7422 - loglik: -6.8792e+02 - logprior: -5.0826e+01
Epoch 2/2
10/10 - 2s - loss: 698.7321 - loglik: -6.7953e+02 - logprior: -1.9203e+01
Fitted a model with MAP estimate = -691.4683
expansions: [(9, 3), (33, 3)]
discards: [ 45  55  67 106]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 725.0150 - loglik: -6.7638e+02 - logprior: -4.8631e+01
Epoch 2/2
10/10 - 2s - loss: 685.0239 - loglik: -6.7151e+02 - logprior: -1.3514e+01
Fitted a model with MAP estimate = -676.5548
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 710.4615 - loglik: -6.7037e+02 - logprior: -4.0092e+01
Epoch 2/10
10/10 - 2s - loss: 678.8133 - loglik: -6.7100e+02 - logprior: -7.8142e+00
Epoch 3/10
10/10 - 2s - loss: 671.4178 - loglik: -6.7015e+02 - logprior: -1.2636e+00
Epoch 4/10
10/10 - 2s - loss: 667.1584 - loglik: -6.6844e+02 - logprior: 1.2814
Epoch 5/10
10/10 - 2s - loss: 666.2901 - loglik: -6.6882e+02 - logprior: 2.5346
Epoch 6/10
10/10 - 2s - loss: 664.8459 - loglik: -6.6824e+02 - logprior: 3.3913
Epoch 7/10
10/10 - 2s - loss: 664.6868 - loglik: -6.6878e+02 - logprior: 4.0952
Epoch 8/10
10/10 - 2s - loss: 664.4801 - loglik: -6.6904e+02 - logprior: 4.5631
Epoch 9/10
10/10 - 2s - loss: 664.2516 - loglik: -6.6913e+02 - logprior: 4.8767
Epoch 10/10
10/10 - 2s - loss: 663.1616 - loglik: -6.6831e+02 - logprior: 5.1505
Fitted a model with MAP estimate = -663.5919
Time for alignment: 60.4826
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 824.4869 - loglik: -7.7900e+02 - logprior: -4.5485e+01
Epoch 2/10
10/10 - 2s - loss: 761.4257 - loglik: -7.5112e+02 - logprior: -1.0307e+01
Epoch 3/10
10/10 - 2s - loss: 726.4684 - loglik: -7.2232e+02 - logprior: -4.1533e+00
Epoch 4/10
10/10 - 2s - loss: 704.0484 - loglik: -7.0193e+02 - logprior: -2.1175e+00
Epoch 5/10
10/10 - 2s - loss: 694.8521 - loglik: -6.9368e+02 - logprior: -1.1726e+00
Epoch 6/10
10/10 - 2s - loss: 690.1536 - loglik: -6.8945e+02 - logprior: -6.9984e-01
Epoch 7/10
10/10 - 2s - loss: 688.7210 - loglik: -6.8843e+02 - logprior: -2.9558e-01
Epoch 8/10
10/10 - 2s - loss: 688.3260 - loglik: -6.8826e+02 - logprior: -6.8123e-02
Epoch 9/10
10/10 - 2s - loss: 687.5703 - loglik: -6.8763e+02 - logprior: 0.0589
Epoch 10/10
10/10 - 2s - loss: 687.0220 - loglik: -6.8720e+02 - logprior: 0.1743
Fitted a model with MAP estimate = -686.8703
expansions: [(10, 2), (15, 1), (17, 2), (19, 1), (26, 1), (27, 3), (42, 1), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 739.4353 - loglik: -6.8866e+02 - logprior: -5.0780e+01
Epoch 2/2
10/10 - 2s - loss: 699.0444 - loglik: -6.7973e+02 - logprior: -1.9310e+01
Fitted a model with MAP estimate = -691.8495
expansions: [(6, 2)]
discards: [  0   9  20  33  69  75 109]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 728.4108 - loglik: -6.7868e+02 - logprior: -4.9731e+01
Epoch 2/2
10/10 - 2s - loss: 694.6818 - loglik: -6.7774e+02 - logprior: -1.6941e+01
Fitted a model with MAP estimate = -685.9389
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 714.6033 - loglik: -6.7554e+02 - logprior: -3.9060e+01
Epoch 2/10
10/10 - 2s - loss: 680.1903 - loglik: -6.7253e+02 - logprior: -7.6629e+00
Epoch 3/10
10/10 - 2s - loss: 673.3217 - loglik: -6.7199e+02 - logprior: -1.3330e+00
Epoch 4/10
10/10 - 2s - loss: 667.3159 - loglik: -6.6847e+02 - logprior: 1.1495
Epoch 5/10
10/10 - 2s - loss: 667.4283 - loglik: -6.7001e+02 - logprior: 2.5788
Fitted a model with MAP estimate = -666.3649
Time for alignment: 53.1368
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 824.5281 - loglik: -7.7904e+02 - logprior: -4.5484e+01
Epoch 2/10
10/10 - 2s - loss: 761.2212 - loglik: -7.5092e+02 - logprior: -1.0302e+01
Epoch 3/10
10/10 - 2s - loss: 724.6915 - loglik: -7.2057e+02 - logprior: -4.1247e+00
Epoch 4/10
10/10 - 2s - loss: 703.0466 - loglik: -7.0091e+02 - logprior: -2.1345e+00
Epoch 5/10
10/10 - 2s - loss: 695.2016 - loglik: -6.9401e+02 - logprior: -1.1878e+00
Epoch 6/10
10/10 - 2s - loss: 689.5544 - loglik: -6.8877e+02 - logprior: -7.8884e-01
Epoch 7/10
10/10 - 2s - loss: 688.9824 - loglik: -6.8849e+02 - logprior: -4.9326e-01
Epoch 8/10
10/10 - 2s - loss: 687.3287 - loglik: -6.8712e+02 - logprior: -2.0471e-01
Epoch 9/10
10/10 - 2s - loss: 687.0908 - loglik: -6.8703e+02 - logprior: -6.1124e-02
Epoch 10/10
10/10 - 2s - loss: 686.4780 - loglik: -6.8654e+02 - logprior: 0.0626
Fitted a model with MAP estimate = -686.4908
expansions: [(7, 2), (10, 1), (11, 1), (16, 1), (19, 1), (26, 1), (49, 2), (59, 3), (62, 1), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 738.1823 - loglik: -6.8745e+02 - logprior: -5.0731e+01
Epoch 2/2
10/10 - 2s - loss: 696.8324 - loglik: -6.7780e+02 - logprior: -1.9029e+01
Fitted a model with MAP estimate = -690.6256
expansions: [(9, 3), (33, 3)]
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 725.9252 - loglik: -6.7639e+02 - logprior: -4.9533e+01
Epoch 2/2
10/10 - 2s - loss: 691.0250 - loglik: -6.7384e+02 - logprior: -1.7184e+01
Fitted a model with MAP estimate = -683.3258
expansions: [(0, 3)]
discards: [ 0 35 72]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 711.1416 - loglik: -6.7246e+02 - logprior: -3.8681e+01
Epoch 2/10
10/10 - 2s - loss: 676.8206 - loglik: -6.6948e+02 - logprior: -7.3426e+00
Epoch 3/10
10/10 - 2s - loss: 670.3288 - loglik: -6.6940e+02 - logprior: -9.2735e-01
Epoch 4/10
10/10 - 2s - loss: 666.6029 - loglik: -6.6831e+02 - logprior: 1.7046
Epoch 5/10
10/10 - 2s - loss: 666.3965 - loglik: -6.6954e+02 - logprior: 3.1460
Epoch 6/10
10/10 - 2s - loss: 663.8424 - loglik: -6.6782e+02 - logprior: 3.9781
Epoch 7/10
10/10 - 2s - loss: 664.6916 - loglik: -6.6918e+02 - logprior: 4.4858
Fitted a model with MAP estimate = -663.8712
Time for alignment: 55.5113
Computed alignments with likelihoods: ['-664.5360', '-664.6543', '-663.5919', '-666.3649', '-663.8712']
Best model has likelihood: -663.5919  (prior= 5.3046 )
time for generating output: 0.1470
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.8878576952822892
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 650.7308 - loglik: -3.8382e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 428.0336 - loglik: -3.5810e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 368.2524 - loglik: -3.3751e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 340.2330 - loglik: -3.2355e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 326.6939 - loglik: -3.1768e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 319.3060 - loglik: -3.1557e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 314.4265 - loglik: -3.1406e+02 - logprior: -3.7088e-01
Epoch 8/10
10/10 - 1s - loss: 311.6570 - loglik: -3.1345e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 310.0823 - loglik: -3.1346e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 309.0099 - loglik: -3.1358e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -308.5427
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.7739 - loglik: -3.0730e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 401.4385 - loglik: -2.9511e+02 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -353.1051
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 595.3898 - loglik: -2.9503e+02 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 407.6827 - loglik: -2.9340e+02 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -377.8107
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 567.3713 - loglik: -2.9254e+02 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 364.8348 - loglik: -2.9178e+02 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 314.1261 - loglik: -2.9201e+02 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 296.0015 - loglik: -2.9249e+02 - logprior: -3.5110e+00
Epoch 5/10
10/10 - 1s - loss: 286.8472 - loglik: -2.9289e+02 - logprior: 6.0478
Epoch 6/10
10/10 - 1s - loss: 281.5307 - loglik: -2.9319e+02 - logprior: 11.6587
Epoch 7/10
10/10 - 1s - loss: 278.1523 - loglik: -2.9338e+02 - logprior: 15.2275
Epoch 8/10
10/10 - 1s - loss: 275.7672 - loglik: -2.9349e+02 - logprior: 17.7251
Epoch 9/10
10/10 - 1s - loss: 273.9446 - loglik: -2.9359e+02 - logprior: 19.6496
Epoch 10/10
10/10 - 1s - loss: 272.4350 - loglik: -2.9370e+02 - logprior: 21.2613
Fitted a model with MAP estimate = -271.6851
Time for alignment: 31.7466
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 650.7308 - loglik: -3.8382e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 428.0336 - loglik: -3.5810e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 368.2524 - loglik: -3.3751e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 340.2329 - loglik: -3.2355e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 326.6938 - loglik: -3.1768e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 319.3060 - loglik: -3.1557e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 314.4265 - loglik: -3.1406e+02 - logprior: -3.7088e-01
Epoch 8/10
10/10 - 1s - loss: 311.6570 - loglik: -3.1345e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 310.0823 - loglik: -3.1346e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 309.0099 - loglik: -3.1358e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -308.5426
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 661.7738 - loglik: -3.0730e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 401.4385 - loglik: -2.9511e+02 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -353.1051
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 595.3898 - loglik: -2.9503e+02 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 407.6827 - loglik: -2.9340e+02 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -377.8107
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 567.3713 - loglik: -2.9254e+02 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 364.8348 - loglik: -2.9178e+02 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 314.1262 - loglik: -2.9201e+02 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 296.0016 - loglik: -2.9249e+02 - logprior: -3.5111e+00
Epoch 5/10
10/10 - 1s - loss: 286.8473 - loglik: -2.9289e+02 - logprior: 6.0476
Epoch 6/10
10/10 - 1s - loss: 281.5309 - loglik: -2.9319e+02 - logprior: 11.6585
Epoch 7/10
10/10 - 1s - loss: 278.1525 - loglik: -2.9338e+02 - logprior: 15.2273
Epoch 8/10
10/10 - 1s - loss: 275.7674 - loglik: -2.9349e+02 - logprior: 17.7249
Epoch 9/10
10/10 - 1s - loss: 273.9449 - loglik: -2.9359e+02 - logprior: 19.6493
Epoch 10/10
10/10 - 1s - loss: 272.4354 - loglik: -2.9370e+02 - logprior: 21.2610
Fitted a model with MAP estimate = -271.6854
Time for alignment: 29.3207
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 650.7308 - loglik: -3.8382e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 428.0336 - loglik: -3.5810e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 368.2524 - loglik: -3.3751e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 340.2330 - loglik: -3.2355e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 326.6938 - loglik: -3.1768e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 319.3061 - loglik: -3.1557e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 314.4265 - loglik: -3.1406e+02 - logprior: -3.7089e-01
Epoch 8/10
10/10 - 1s - loss: 311.6570 - loglik: -3.1345e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 310.0822 - loglik: -3.1346e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 309.0099 - loglik: -3.1358e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -308.5426
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 661.7739 - loglik: -3.0730e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 401.4385 - loglik: -2.9511e+02 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -353.1051
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 595.3898 - loglik: -2.9503e+02 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 407.6827 - loglik: -2.9340e+02 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -377.8107
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 567.3713 - loglik: -2.9254e+02 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 364.8349 - loglik: -2.9178e+02 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 314.1263 - loglik: -2.9201e+02 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 296.0016 - loglik: -2.9249e+02 - logprior: -3.5112e+00
Epoch 5/10
10/10 - 1s - loss: 286.8475 - loglik: -2.9289e+02 - logprior: 6.0475
Epoch 6/10
10/10 - 1s - loss: 281.5311 - loglik: -2.9319e+02 - logprior: 11.6583
Epoch 7/10
10/10 - 1s - loss: 278.1527 - loglik: -2.9338e+02 - logprior: 15.2271
Epoch 8/10
10/10 - 1s - loss: 275.7677 - loglik: -2.9349e+02 - logprior: 17.7247
Epoch 9/10
10/10 - 1s - loss: 273.9452 - loglik: -2.9359e+02 - logprior: 19.6491
Epoch 10/10
10/10 - 1s - loss: 272.4355 - loglik: -2.9370e+02 - logprior: 21.2608
Fitted a model with MAP estimate = -271.6856
Time for alignment: 29.0349
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 650.7308 - loglik: -3.8382e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 428.0336 - loglik: -3.5810e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 368.2524 - loglik: -3.3751e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 340.2329 - loglik: -3.2355e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 326.6939 - loglik: -3.1768e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 319.3060 - loglik: -3.1557e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 314.4266 - loglik: -3.1406e+02 - logprior: -3.7088e-01
Epoch 8/10
10/10 - 1s - loss: 311.6570 - loglik: -3.1345e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 310.0822 - loglik: -3.1346e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 309.0099 - loglik: -3.1358e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -308.5426
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 661.7739 - loglik: -3.0730e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 401.4385 - loglik: -2.9511e+02 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -353.1051
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 595.3898 - loglik: -2.9503e+02 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 407.6827 - loglik: -2.9340e+02 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -377.8106
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 567.3713 - loglik: -2.9254e+02 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 364.8348 - loglik: -2.9178e+02 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 314.1260 - loglik: -2.9201e+02 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 296.0013 - loglik: -2.9249e+02 - logprior: -3.5109e+00
Epoch 5/10
10/10 - 1s - loss: 286.8469 - loglik: -2.9289e+02 - logprior: 6.0480
Epoch 6/10
10/10 - 1s - loss: 281.5303 - loglik: -2.9319e+02 - logprior: 11.6590
Epoch 7/10
10/10 - 1s - loss: 278.1519 - loglik: -2.9338e+02 - logprior: 15.2278
Epoch 8/10
10/10 - 1s - loss: 275.7668 - loglik: -2.9349e+02 - logprior: 17.7255
Epoch 9/10
10/10 - 1s - loss: 273.9442 - loglik: -2.9359e+02 - logprior: 19.6500
Epoch 10/10
10/10 - 1s - loss: 272.4345 - loglik: -2.9370e+02 - logprior: 21.2618
Fitted a model with MAP estimate = -271.6846
Time for alignment: 27.1977
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 650.7308 - loglik: -3.8382e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 428.0336 - loglik: -3.5810e+02 - logprior: -6.9932e+01
Epoch 3/10
10/10 - 1s - loss: 368.2524 - loglik: -3.3751e+02 - logprior: -3.0747e+01
Epoch 4/10
10/10 - 1s - loss: 340.2330 - loglik: -3.2355e+02 - logprior: -1.6686e+01
Epoch 5/10
10/10 - 1s - loss: 326.6939 - loglik: -3.1768e+02 - logprior: -9.0148e+00
Epoch 6/10
10/10 - 1s - loss: 319.3060 - loglik: -3.1557e+02 - logprior: -3.7312e+00
Epoch 7/10
10/10 - 1s - loss: 314.4265 - loglik: -3.1406e+02 - logprior: -3.7088e-01
Epoch 8/10
10/10 - 1s - loss: 311.6570 - loglik: -3.1345e+02 - logprior: 1.7892
Epoch 9/10
10/10 - 1s - loss: 310.0822 - loglik: -3.1346e+02 - logprior: 3.3756
Epoch 10/10
10/10 - 1s - loss: 309.0099 - loglik: -3.1358e+02 - logprior: 4.5684
Fitted a model with MAP estimate = -308.5429
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 661.7739 - loglik: -3.0730e+02 - logprior: -3.5447e+02
Epoch 2/2
10/10 - 1s - loss: 401.4385 - loglik: -2.9511e+02 - logprior: -1.0633e+02
Fitted a model with MAP estimate = -353.1051
expansions: []
discards: [ 0 21 37 48 53]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 595.3898 - loglik: -2.9503e+02 - logprior: -3.0036e+02
Epoch 2/2
10/10 - 1s - loss: 407.6827 - loglik: -2.9340e+02 - logprior: -1.1428e+02
Fitted a model with MAP estimate = -377.8107
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 567.3713 - loglik: -2.9254e+02 - logprior: -2.7483e+02
Epoch 2/10
10/10 - 1s - loss: 364.8347 - loglik: -2.9178e+02 - logprior: -7.3053e+01
Epoch 3/10
10/10 - 1s - loss: 314.1259 - loglik: -2.9201e+02 - logprior: -2.2114e+01
Epoch 4/10
10/10 - 1s - loss: 296.0012 - loglik: -2.9249e+02 - logprior: -3.5108e+00
Epoch 5/10
10/10 - 1s - loss: 286.8467 - loglik: -2.9289e+02 - logprior: 6.0481
Epoch 6/10
10/10 - 1s - loss: 281.5302 - loglik: -2.9319e+02 - logprior: 11.6591
Epoch 7/10
10/10 - 1s - loss: 278.1516 - loglik: -2.9338e+02 - logprior: 15.2280
Epoch 8/10
10/10 - 1s - loss: 275.7665 - loglik: -2.9349e+02 - logprior: 17.7257
Epoch 9/10
10/10 - 1s - loss: 273.9439 - loglik: -2.9359e+02 - logprior: 19.6503
Epoch 10/10
10/10 - 1s - loss: 272.4342 - loglik: -2.9370e+02 - logprior: 21.2621
Fitted a model with MAP estimate = -271.6843
Time for alignment: 29.0494
Computed alignments with likelihoods: ['-271.6851', '-271.6854', '-271.6856', '-271.6846', '-271.6843']
Best model has likelihood: -271.6843  (prior= 22.0678 )
time for generating output: 0.1362
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.823262839879154
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1434.0546 - loglik: -1.4300e+03 - logprior: -4.0148e+00
Epoch 2/10
26/26 - 9s - loss: 1330.6552 - loglik: -1.3294e+03 - logprior: -1.2615e+00
Epoch 3/10
26/26 - 8s - loss: 1318.6674 - loglik: -1.3174e+03 - logprior: -1.2919e+00
Epoch 4/10
26/26 - 9s - loss: 1318.4143 - loglik: -1.3171e+03 - logprior: -1.3007e+00
Epoch 5/10
26/26 - 9s - loss: 1318.3502 - loglik: -1.3170e+03 - logprior: -1.3148e+00
Epoch 6/10
26/26 - 9s - loss: 1314.6202 - loglik: -1.3133e+03 - logprior: -1.3309e+00
Epoch 7/10
26/26 - 9s - loss: 1314.3237 - loglik: -1.3130e+03 - logprior: -1.3114e+00
Epoch 8/10
26/26 - 9s - loss: 1315.1945 - loglik: -1.3139e+03 - logprior: -1.3268e+00
Fitted a model with MAP estimate = -1315.0706
expansions: [(47, 1), (106, 1), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1322.4243 - loglik: -1.3178e+03 - logprior: -4.6693e+00
Epoch 2/2
26/26 - 9s - loss: 1316.0447 - loglik: -1.3149e+03 - logprior: -1.1897e+00
Fitted a model with MAP estimate = -1315.0605
expansions: [(109, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1319.7975 - loglik: -1.3153e+03 - logprior: -4.5001e+00
Epoch 2/2
26/26 - 9s - loss: 1317.6794 - loglik: -1.3167e+03 - logprior: -9.3750e-01
Fitted a model with MAP estimate = -1314.7466
expansions: [(0, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1319.1355 - loglik: -1.3135e+03 - logprior: -5.5948e+00
Epoch 2/10
26/26 - 9s - loss: 1317.9729 - loglik: -1.3167e+03 - logprior: -1.2248e+00
Epoch 3/10
26/26 - 9s - loss: 1310.1061 - loglik: -1.3091e+03 - logprior: -1.0034e+00
Epoch 4/10
26/26 - 9s - loss: 1315.4415 - loglik: -1.3147e+03 - logprior: -7.7327e-01
Fitted a model with MAP estimate = -1311.1450
Time for alignment: 192.2161
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1434.4590 - loglik: -1.4304e+03 - logprior: -4.0201e+00
Epoch 2/10
26/26 - 9s - loss: 1331.5242 - loglik: -1.3305e+03 - logprior: -1.0195e+00
Epoch 3/10
26/26 - 9s - loss: 1321.3630 - loglik: -1.3204e+03 - logprior: -9.4123e-01
Epoch 4/10
26/26 - 9s - loss: 1320.5249 - loglik: -1.3196e+03 - logprior: -9.3604e-01
Epoch 5/10
26/26 - 9s - loss: 1313.1223 - loglik: -1.3122e+03 - logprior: -9.3539e-01
Epoch 6/10
26/26 - 9s - loss: 1317.1714 - loglik: -1.3162e+03 - logprior: -9.4980e-01
Fitted a model with MAP estimate = -1315.7517
expansions: [(56, 2), (94, 2), (156, 1), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1322.8257 - loglik: -1.3184e+03 - logprior: -4.4383e+00
Epoch 2/2
26/26 - 9s - loss: 1314.9351 - loglik: -1.3140e+03 - logprior: -9.4971e-01
Fitted a model with MAP estimate = -1313.5774
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1319.3601 - loglik: -1.3151e+03 - logprior: -4.2760e+00
Epoch 2/10
26/26 - 9s - loss: 1312.9974 - loglik: -1.3123e+03 - logprior: -7.0869e-01
Epoch 3/10
26/26 - 9s - loss: 1315.4154 - loglik: -1.3150e+03 - logprior: -4.6271e-01
Fitted a model with MAP estimate = -1312.0017
Time for alignment: 130.4832
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1432.8137 - loglik: -1.4288e+03 - logprior: -4.0550e+00
Epoch 2/10
26/26 - 9s - loss: 1328.9182 - loglik: -1.3275e+03 - logprior: -1.4161e+00
Epoch 3/10
26/26 - 9s - loss: 1320.4485 - loglik: -1.3191e+03 - logprior: -1.3287e+00
Epoch 4/10
26/26 - 9s - loss: 1314.1857 - loglik: -1.3128e+03 - logprior: -1.3747e+00
Epoch 5/10
26/26 - 9s - loss: 1311.4934 - loglik: -1.3101e+03 - logprior: -1.4097e+00
Epoch 6/10
26/26 - 9s - loss: 1315.7075 - loglik: -1.3143e+03 - logprior: -1.4077e+00
Fitted a model with MAP estimate = -1313.1229
expansions: [(173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1322.6923 - loglik: -1.3180e+03 - logprior: -4.7390e+00
Epoch 2/2
26/26 - 9s - loss: 1314.3455 - loglik: -1.3131e+03 - logprior: -1.2419e+00
Fitted a model with MAP estimate = -1314.3563
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1319.4407 - loglik: -1.3149e+03 - logprior: -4.5857e+00
Epoch 2/2
26/26 - 9s - loss: 1318.3298 - loglik: -1.3173e+03 - logprior: -1.0517e+00
Fitted a model with MAP estimate = -1314.5015
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 11s - loss: 1321.7841 - loglik: -1.3174e+03 - logprior: -4.3614e+00
Epoch 2/10
26/26 - 9s - loss: 1314.4236 - loglik: -1.3136e+03 - logprior: -8.0770e-01
Epoch 3/10
26/26 - 9s - loss: 1314.3549 - loglik: -1.3137e+03 - logprior: -6.2272e-01
Epoch 4/10
26/26 - 9s - loss: 1311.9351 - loglik: -1.3114e+03 - logprior: -5.7072e-01
Epoch 5/10
26/26 - 9s - loss: 1311.6140 - loglik: -1.3112e+03 - logprior: -4.5156e-01
Epoch 6/10
26/26 - 9s - loss: 1313.6327 - loglik: -1.3133e+03 - logprior: -3.2681e-01
Fitted a model with MAP estimate = -1311.3253
Time for alignment: 188.8126
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1434.2404 - loglik: -1.4302e+03 - logprior: -4.0292e+00
Epoch 2/10
26/26 - 9s - loss: 1329.4208 - loglik: -1.3281e+03 - logprior: -1.3507e+00
Epoch 3/10
26/26 - 9s - loss: 1314.4015 - loglik: -1.3130e+03 - logprior: -1.4353e+00
Epoch 4/10
26/26 - 9s - loss: 1315.6174 - loglik: -1.3141e+03 - logprior: -1.5039e+00
Fitted a model with MAP estimate = -1311.8675
expansions: [(9, 2), (77, 1), (108, 4), (160, 3), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1319.5839 - loglik: -1.3127e+03 - logprior: -6.8561e+00
Epoch 2/2
26/26 - 9s - loss: 1311.0559 - loglik: -1.3080e+03 - logprior: -3.0774e+00
Fitted a model with MAP estimate = -1308.0814
expansions: [(0, 5), (112, 1)]
discards: [  0   9 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1315.9761 - loglik: -1.3113e+03 - logprior: -4.7154e+00
Epoch 2/2
26/26 - 9s - loss: 1306.3978 - loglik: -1.3050e+03 - logprior: -1.3485e+00
Fitted a model with MAP estimate = -1305.6645
expansions: [(170, 1), (171, 1)]
discards: [0 1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1313.0438 - loglik: -1.3066e+03 - logprior: -6.4089e+00
Epoch 2/10
26/26 - 9s - loss: 1309.8074 - loglik: -1.3082e+03 - logprior: -1.5728e+00
Epoch 3/10
26/26 - 9s - loss: 1303.2120 - loglik: -1.3026e+03 - logprior: -5.6566e-01
Epoch 4/10
26/26 - 9s - loss: 1307.5798 - loglik: -1.3070e+03 - logprior: -5.6936e-01
Fitted a model with MAP estimate = -1303.1707
Time for alignment: 156.4499
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 1431.4746 - loglik: -1.4274e+03 - logprior: -4.0394e+00
Epoch 2/10
26/26 - 9s - loss: 1327.9015 - loglik: -1.3267e+03 - logprior: -1.2259e+00
Epoch 3/10
26/26 - 9s - loss: 1316.5280 - loglik: -1.3153e+03 - logprior: -1.2757e+00
Epoch 4/10
26/26 - 9s - loss: 1307.1024 - loglik: -1.3058e+03 - logprior: -1.2908e+00
Epoch 5/10
26/26 - 9s - loss: 1306.5807 - loglik: -1.3053e+03 - logprior: -1.2937e+00
Epoch 6/10
26/26 - 9s - loss: 1308.4902 - loglik: -1.3072e+03 - logprior: -1.2718e+00
Fitted a model with MAP estimate = -1307.6519
expansions: [(116, 1), (173, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1316.7997 - loglik: -1.3121e+03 - logprior: -4.7046e+00
Epoch 2/2
26/26 - 9s - loss: 1311.1438 - loglik: -1.3100e+03 - logprior: -1.1648e+00
Fitted a model with MAP estimate = -1309.3184
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1312.9963 - loglik: -1.3085e+03 - logprior: -4.5191e+00
Epoch 2/10
26/26 - 9s - loss: 1311.9675 - loglik: -1.3110e+03 - logprior: -9.2966e-01
Epoch 3/10
26/26 - 9s - loss: 1313.4542 - loglik: -1.3127e+03 - logprior: -7.6196e-01
Fitted a model with MAP estimate = -1307.8339
Time for alignment: 130.8093
Computed alignments with likelihoods: ['-1311.1450', '-1312.0017', '-1311.3253', '-1303.1707', '-1307.6519']
Best model has likelihood: -1303.1707  (prior= -0.5170 )
time for generating output: 0.3244
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.7025806451612904
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 771.8069 - loglik: -6.9507e+02 - logprior: -7.6735e+01
Epoch 2/10
10/10 - 1s - loss: 687.2686 - loglik: -6.6966e+02 - logprior: -1.7605e+01
Epoch 3/10
10/10 - 1s - loss: 648.1996 - loglik: -6.4175e+02 - logprior: -6.4541e+00
Epoch 4/10
10/10 - 1s - loss: 628.8258 - loglik: -6.2617e+02 - logprior: -2.6523e+00
Epoch 5/10
10/10 - 1s - loss: 621.1471 - loglik: -6.2051e+02 - logprior: -6.3701e-01
Epoch 6/10
10/10 - 1s - loss: 618.0616 - loglik: -6.1847e+02 - logprior: 0.4129
Epoch 7/10
10/10 - 1s - loss: 615.1115 - loglik: -6.1615e+02 - logprior: 1.0418
Epoch 8/10
10/10 - 1s - loss: 612.9623 - loglik: -6.1445e+02 - logprior: 1.4858
Epoch 9/10
10/10 - 1s - loss: 611.4131 - loglik: -6.1329e+02 - logprior: 1.8733
Epoch 10/10
10/10 - 1s - loss: 610.6747 - loglik: -6.1281e+02 - logprior: 2.1402
Fitted a model with MAP estimate = -610.2707
expansions: [(10, 1), (11, 4), (12, 1), (18, 2), (36, 2), (45, 3), (48, 1), (54, 1), (63, 4), (78, 1), (82, 2), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 698.1887 - loglik: -6.1241e+02 - logprior: -8.5779e+01
Epoch 2/2
10/10 - 1s - loss: 634.0199 - loglik: -6.0131e+02 - logprior: -3.2712e+01
Fitted a model with MAP estimate = -622.6135
expansions: [(0, 3), (109, 3)]
discards: [  0  77 101]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 665.3204 - loglik: -5.9780e+02 - logprior: -6.7520e+01
Epoch 2/2
10/10 - 1s - loss: 607.2552 - loglik: -5.9287e+02 - logprior: -1.4384e+01
Fitted a model with MAP estimate = -597.9598
expansions: [(113, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 676.2117 - loglik: -5.9333e+02 - logprior: -8.2884e+01
Epoch 2/10
10/10 - 1s - loss: 616.4338 - loglik: -5.9038e+02 - logprior: -2.6052e+01
Epoch 3/10
10/10 - 1s - loss: 596.5409 - loglik: -5.8995e+02 - logprior: -6.5863e+00
Epoch 4/10
10/10 - 1s - loss: 585.7953 - loglik: -5.8737e+02 - logprior: 1.5771
Epoch 5/10
10/10 - 1s - loss: 583.0874 - loglik: -5.8772e+02 - logprior: 4.6340
Epoch 6/10
10/10 - 1s - loss: 581.2390 - loglik: -5.8744e+02 - logprior: 6.1998
Epoch 7/10
10/10 - 1s - loss: 580.7833 - loglik: -5.8797e+02 - logprior: 7.1842
Epoch 8/10
10/10 - 1s - loss: 580.2408 - loglik: -5.8820e+02 - logprior: 7.9552
Epoch 9/10
10/10 - 1s - loss: 578.8928 - loglik: -5.8751e+02 - logprior: 8.6213
Epoch 10/10
10/10 - 1s - loss: 578.9505 - loglik: -5.8813e+02 - logprior: 9.1816
Fitted a model with MAP estimate = -578.5950
Time for alignment: 50.1876
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 771.8143 - loglik: -6.9508e+02 - logprior: -7.6737e+01
Epoch 2/10
10/10 - 1s - loss: 687.1802 - loglik: -6.6957e+02 - logprior: -1.7608e+01
Epoch 3/10
10/10 - 1s - loss: 648.3502 - loglik: -6.4193e+02 - logprior: -6.4245e+00
Epoch 4/10
10/10 - 1s - loss: 627.9120 - loglik: -6.2553e+02 - logprior: -2.3840e+00
Epoch 5/10
10/10 - 1s - loss: 621.3407 - loglik: -6.2099e+02 - logprior: -3.5170e-01
Epoch 6/10
10/10 - 1s - loss: 617.5370 - loglik: -6.1834e+02 - logprior: 0.8039
Epoch 7/10
10/10 - 1s - loss: 614.7397 - loglik: -6.1622e+02 - logprior: 1.4826
Epoch 8/10
10/10 - 1s - loss: 614.2725 - loglik: -6.1629e+02 - logprior: 2.0150
Epoch 9/10
10/10 - 1s - loss: 613.6481 - loglik: -6.1603e+02 - logprior: 2.3822
Epoch 10/10
10/10 - 1s - loss: 612.8115 - loglik: -6.1546e+02 - logprior: 2.6450
Fitted a model with MAP estimate = -612.6134
expansions: [(5, 3), (6, 2), (12, 4), (19, 2), (36, 4), (45, 3), (68, 1), (83, 4), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 701.0020 - loglik: -6.1526e+02 - logprior: -8.5739e+01
Epoch 2/2
10/10 - 1s - loss: 636.0558 - loglik: -6.0334e+02 - logprior: -3.2712e+01
Fitted a model with MAP estimate = -625.0592
expansions: [(0, 3), (110, 3), (111, 2)]
discards: [  0   8  46  47 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 668.1589 - loglik: -6.0067e+02 - logprior: -6.7489e+01
Epoch 2/2
10/10 - 1s - loss: 607.5314 - loglik: -5.9344e+02 - logprior: -1.4091e+01
Fitted a model with MAP estimate = -597.7161
expansions: [(78, 3), (111, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 675.9354 - loglik: -5.9323e+02 - logprior: -8.2709e+01
Epoch 2/10
10/10 - 1s - loss: 616.6223 - loglik: -5.8981e+02 - logprior: -2.6810e+01
Epoch 3/10
10/10 - 1s - loss: 594.5463 - loglik: -5.8729e+02 - logprior: -7.2582e+00
Epoch 4/10
10/10 - 1s - loss: 584.7406 - loglik: -5.8659e+02 - logprior: 1.8511
Epoch 5/10
10/10 - 1s - loss: 580.7180 - loglik: -5.8582e+02 - logprior: 5.1051
Epoch 6/10
10/10 - 1s - loss: 578.6755 - loglik: -5.8537e+02 - logprior: 6.6961
Epoch 7/10
10/10 - 1s - loss: 577.6559 - loglik: -5.8535e+02 - logprior: 7.6920
Epoch 8/10
10/10 - 2s - loss: 577.8365 - loglik: -5.8630e+02 - logprior: 8.4621
Fitted a model with MAP estimate = -576.7590
Time for alignment: 45.9636
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 772.5811 - loglik: -6.9584e+02 - logprior: -7.6739e+01
Epoch 2/10
10/10 - 1s - loss: 685.6633 - loglik: -6.6805e+02 - logprior: -1.7609e+01
Epoch 3/10
10/10 - 1s - loss: 646.3768 - loglik: -6.3995e+02 - logprior: -6.4304e+00
Epoch 4/10
10/10 - 1s - loss: 627.7723 - loglik: -6.2526e+02 - logprior: -2.5156e+00
Epoch 5/10
10/10 - 1s - loss: 619.9411 - loglik: -6.1935e+02 - logprior: -5.9096e-01
Epoch 6/10
10/10 - 1s - loss: 616.3732 - loglik: -6.1666e+02 - logprior: 0.2843
Epoch 7/10
10/10 - 1s - loss: 613.0753 - loglik: -6.1409e+02 - logprior: 1.0142
Epoch 8/10
10/10 - 1s - loss: 612.6523 - loglik: -6.1428e+02 - logprior: 1.6318
Epoch 9/10
10/10 - 1s - loss: 611.0461 - loglik: -6.1300e+02 - logprior: 1.9557
Epoch 10/10
10/10 - 1s - loss: 610.7698 - loglik: -6.1293e+02 - logprior: 2.1628
Fitted a model with MAP estimate = -610.1033
expansions: [(5, 1), (6, 1), (10, 2), (12, 4), (18, 2), (36, 4), (45, 3), (48, 2), (63, 4), (78, 1), (82, 1), (86, 3), (88, 3), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.0196 - loglik: -6.0891e+02 - logprior: -8.5112e+01
Epoch 2/2
10/10 - 1s - loss: 626.7916 - loglik: -5.9490e+02 - logprior: -3.1887e+01
Fitted a model with MAP estimate = -615.2939
expansions: [(0, 3), (64, 1), (110, 1)]
discards: [ 0 47]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 658.4567 - loglik: -5.9154e+02 - logprior: -6.6920e+01
Epoch 2/2
10/10 - 2s - loss: 600.2831 - loglik: -5.8659e+02 - logprior: -1.3690e+01
Fitted a model with MAP estimate = -590.8793
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 669.6998 - loglik: -5.8848e+02 - logprior: -8.1221e+01
Epoch 2/10
10/10 - 2s - loss: 608.5688 - loglik: -5.8649e+02 - logprior: -2.2080e+01
Epoch 3/10
10/10 - 2s - loss: 588.2009 - loglik: -5.8414e+02 - logprior: -4.0601e+00
Epoch 4/10
10/10 - 1s - loss: 582.3453 - loglik: -5.8468e+02 - logprior: 2.3376
Epoch 5/10
10/10 - 2s - loss: 578.9949 - loglik: -5.8416e+02 - logprior: 5.1644
Epoch 6/10
10/10 - 2s - loss: 577.0333 - loglik: -5.8373e+02 - logprior: 6.6978
Epoch 7/10
10/10 - 2s - loss: 576.5739 - loglik: -5.8427e+02 - logprior: 7.6998
Epoch 8/10
10/10 - 2s - loss: 575.8578 - loglik: -5.8436e+02 - logprior: 8.4999
Epoch 9/10
10/10 - 2s - loss: 575.0405 - loglik: -5.8423e+02 - logprior: 9.1864
Epoch 10/10
10/10 - 2s - loss: 574.9633 - loglik: -5.8472e+02 - logprior: 9.7584
Fitted a model with MAP estimate = -574.4368
Time for alignment: 50.9340
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 771.8853 - loglik: -6.9515e+02 - logprior: -7.6736e+01
Epoch 2/10
10/10 - 1s - loss: 686.8612 - loglik: -6.6926e+02 - logprior: -1.7601e+01
Epoch 3/10
10/10 - 1s - loss: 648.8990 - loglik: -6.4248e+02 - logprior: -6.4186e+00
Epoch 4/10
10/10 - 1s - loss: 629.7811 - loglik: -6.2744e+02 - logprior: -2.3413e+00
Epoch 5/10
10/10 - 1s - loss: 621.5565 - loglik: -6.2123e+02 - logprior: -3.2799e-01
Epoch 6/10
10/10 - 1s - loss: 618.0795 - loglik: -6.1892e+02 - logprior: 0.8391
Epoch 7/10
10/10 - 1s - loss: 616.1902 - loglik: -6.1774e+02 - logprior: 1.5490
Epoch 8/10
10/10 - 1s - loss: 614.6562 - loglik: -6.1668e+02 - logprior: 2.0210
Epoch 9/10
10/10 - 1s - loss: 613.2386 - loglik: -6.1558e+02 - logprior: 2.3452
Epoch 10/10
10/10 - 1s - loss: 612.3514 - loglik: -6.1495e+02 - logprior: 2.5950
Fitted a model with MAP estimate = -612.1608
expansions: [(5, 3), (6, 2), (12, 4), (19, 2), (36, 4), (45, 3), (64, 2), (68, 1), (83, 4), (86, 1), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 700.2798 - loglik: -6.1464e+02 - logprior: -8.5641e+01
Epoch 2/2
10/10 - 1s - loss: 636.1368 - loglik: -6.0338e+02 - logprior: -3.2754e+01
Fitted a model with MAP estimate = -624.9762
expansions: [(0, 3), (112, 3)]
discards: [  0   8  46  60  81 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 668.6159 - loglik: -6.0106e+02 - logprior: -6.7558e+01
Epoch 2/2
10/10 - 1s - loss: 610.2978 - loglik: -5.9603e+02 - logprior: -1.4264e+01
Fitted a model with MAP estimate = -600.7355
expansions: [(112, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 679.1790 - loglik: -5.9634e+02 - logprior: -8.2840e+01
Epoch 2/10
10/10 - 1s - loss: 619.4372 - loglik: -5.9322e+02 - logprior: -2.6222e+01
Epoch 3/10
10/10 - 1s - loss: 597.9003 - loglik: -5.9122e+02 - logprior: -6.6779e+00
Epoch 4/10
10/10 - 1s - loss: 589.2020 - loglik: -5.9089e+02 - logprior: 1.6837
Epoch 5/10
10/10 - 1s - loss: 585.3708 - loglik: -5.9014e+02 - logprior: 4.7667
Epoch 6/10
10/10 - 1s - loss: 583.6341 - loglik: -5.8996e+02 - logprior: 6.3253
Epoch 7/10
10/10 - 1s - loss: 582.4355 - loglik: -5.8976e+02 - logprior: 7.3293
Epoch 8/10
10/10 - 1s - loss: 582.1810 - loglik: -5.9030e+02 - logprior: 8.1177
Epoch 9/10
10/10 - 1s - loss: 581.5145 - loglik: -5.9031e+02 - logprior: 8.7938
Epoch 10/10
10/10 - 1s - loss: 581.0248 - loglik: -5.9037e+02 - logprior: 9.3500
Fitted a model with MAP estimate = -580.7442
Time for alignment: 48.6249
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 772.0743 - loglik: -6.9534e+02 - logprior: -7.6739e+01
Epoch 2/10
10/10 - 1s - loss: 686.0677 - loglik: -6.6844e+02 - logprior: -1.7625e+01
Epoch 3/10
10/10 - 1s - loss: 645.8944 - loglik: -6.3950e+02 - logprior: -6.3965e+00
Epoch 4/10
10/10 - 1s - loss: 626.3878 - loglik: -6.2404e+02 - logprior: -2.3511e+00
Epoch 5/10
10/10 - 1s - loss: 618.9078 - loglik: -6.1862e+02 - logprior: -2.8371e-01
Epoch 6/10
10/10 - 1s - loss: 615.2570 - loglik: -6.1597e+02 - logprior: 0.7177
Epoch 7/10
10/10 - 1s - loss: 613.7521 - loglik: -6.1516e+02 - logprior: 1.4041
Epoch 8/10
10/10 - 1s - loss: 611.9395 - loglik: -6.1391e+02 - logprior: 1.9708
Epoch 9/10
10/10 - 1s - loss: 611.0992 - loglik: -6.1346e+02 - logprior: 2.3564
Epoch 10/10
10/10 - 1s - loss: 610.7453 - loglik: -6.1332e+02 - logprior: 2.5734
Fitted a model with MAP estimate = -610.2799
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (54, 2), (63, 4), (83, 3), (88, 5), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 698.1793 - loglik: -6.1267e+02 - logprior: -8.5505e+01
Epoch 2/2
10/10 - 1s - loss: 631.8588 - loglik: -5.9938e+02 - logprior: -3.2476e+01
Fitted a model with MAP estimate = -619.8258
expansions: [(0, 3), (111, 2)]
discards: [  0  43  44 104]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 662.0468 - loglik: -5.9490e+02 - logprior: -6.7151e+01
Epoch 2/2
10/10 - 2s - loss: 603.4775 - loglik: -5.8955e+02 - logprior: -1.3927e+01
Fitted a model with MAP estimate = -594.3252
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 672.1530 - loglik: -5.9128e+02 - logprior: -8.0876e+01
Epoch 2/10
10/10 - 1s - loss: 610.5094 - loglik: -5.8931e+02 - logprior: -2.1199e+01
Epoch 3/10
10/10 - 1s - loss: 592.3605 - loglik: -5.8841e+02 - logprior: -3.9498e+00
Epoch 4/10
10/10 - 1s - loss: 585.9142 - loglik: -5.8801e+02 - logprior: 2.0988
Epoch 5/10
10/10 - 1s - loss: 582.2169 - loglik: -5.8709e+02 - logprior: 4.8764
Epoch 6/10
10/10 - 1s - loss: 580.8284 - loglik: -5.8724e+02 - logprior: 6.4123
Epoch 7/10
10/10 - 1s - loss: 579.8604 - loglik: -5.8726e+02 - logprior: 7.3984
Epoch 8/10
10/10 - 1s - loss: 579.3174 - loglik: -5.8750e+02 - logprior: 8.1778
Epoch 9/10
10/10 - 1s - loss: 578.7590 - loglik: -5.8762e+02 - logprior: 8.8562
Epoch 10/10
10/10 - 1s - loss: 578.2273 - loglik: -5.8766e+02 - logprior: 9.4286
Fitted a model with MAP estimate = -577.9212
Time for alignment: 49.4611
Computed alignments with likelihoods: ['-578.5950', '-576.7590', '-574.4368', '-580.7442', '-577.9212']
Best model has likelihood: -574.4368  (prior= 10.0433 )
time for generating output: 0.1925
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7669064748201438
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 198.7992 - loglik: -1.9436e+02 - logprior: -4.4346e+00
Epoch 2/10
17/17 - 1s - loss: 178.8973 - loglik: -1.7733e+02 - logprior: -1.5648e+00
Epoch 3/10
17/17 - 1s - loss: 169.2090 - loglik: -1.6772e+02 - logprior: -1.4919e+00
Epoch 4/10
17/17 - 1s - loss: 166.4753 - loglik: -1.6483e+02 - logprior: -1.6411e+00
Epoch 5/10
17/17 - 1s - loss: 165.7577 - loglik: -1.6424e+02 - logprior: -1.5222e+00
Epoch 6/10
17/17 - 1s - loss: 165.4659 - loglik: -1.6396e+02 - logprior: -1.5044e+00
Epoch 7/10
17/17 - 1s - loss: 165.3095 - loglik: -1.6381e+02 - logprior: -1.5035e+00
Epoch 8/10
17/17 - 1s - loss: 165.2409 - loglik: -1.6375e+02 - logprior: -1.4894e+00
Epoch 9/10
17/17 - 1s - loss: 165.2797 - loglik: -1.6380e+02 - logprior: -1.4773e+00
Fitted a model with MAP estimate = -165.1919
expansions: [(2, 3), (10, 2), (11, 2), (13, 2), (16, 1), (18, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 35 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 166.1883 - loglik: -1.6194e+02 - logprior: -4.2493e+00
Epoch 2/2
17/17 - 1s - loss: 160.1529 - loglik: -1.5857e+02 - logprior: -1.5855e+00
Fitted a model with MAP estimate = -159.5701
expansions: []
discards: [ 0 14 17 20]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 168.3540 - loglik: -1.6294e+02 - logprior: -5.4092e+00
Epoch 2/2
17/17 - 1s - loss: 163.4233 - loglik: -1.6101e+02 - logprior: -2.4086e+00
Fitted a model with MAP estimate = -161.0654
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.6192 - loglik: -1.5927e+02 - logprior: -4.3511e+00
Epoch 2/10
17/17 - 1s - loss: 160.1450 - loglik: -1.5855e+02 - logprior: -1.5935e+00
Epoch 3/10
17/17 - 1s - loss: 159.7092 - loglik: -1.5836e+02 - logprior: -1.3492e+00
Epoch 4/10
17/17 - 1s - loss: 159.4687 - loglik: -1.5820e+02 - logprior: -1.2727e+00
Epoch 5/10
17/17 - 1s - loss: 159.3511 - loglik: -1.5810e+02 - logprior: -1.2536e+00
Epoch 6/10
17/17 - 1s - loss: 159.4024 - loglik: -1.5819e+02 - logprior: -1.2106e+00
Fitted a model with MAP estimate = -159.3195
Time for alignment: 32.6618
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 198.7610 - loglik: -1.9433e+02 - logprior: -4.4304e+00
Epoch 2/10
17/17 - 1s - loss: 177.0422 - loglik: -1.7551e+02 - logprior: -1.5354e+00
Epoch 3/10
17/17 - 1s - loss: 167.1227 - loglik: -1.6551e+02 - logprior: -1.6118e+00
Epoch 4/10
17/17 - 1s - loss: 165.2990 - loglik: -1.6367e+02 - logprior: -1.6339e+00
Epoch 5/10
17/17 - 1s - loss: 164.8402 - loglik: -1.6331e+02 - logprior: -1.5328e+00
Epoch 6/10
17/17 - 1s - loss: 164.5552 - loglik: -1.6300e+02 - logprior: -1.5526e+00
Epoch 7/10
17/17 - 1s - loss: 164.5759 - loglik: -1.6305e+02 - logprior: -1.5263e+00
Fitted a model with MAP estimate = -164.4560
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 172.2084 - loglik: -1.6669e+02 - logprior: -5.5175e+00
Epoch 2/2
17/17 - 1s - loss: 163.2476 - loglik: -1.6075e+02 - logprior: -2.4961e+00
Fitted a model with MAP estimate = -160.9763
expansions: []
discards: [13 16 26]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.9594 - loglik: -1.5959e+02 - logprior: -4.3702e+00
Epoch 2/2
17/17 - 1s - loss: 159.9969 - loglik: -1.5839e+02 - logprior: -1.6024e+00
Fitted a model with MAP estimate = -159.7457
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 163.1593 - loglik: -1.5889e+02 - logprior: -4.2723e+00
Epoch 2/10
17/17 - 1s - loss: 160.0465 - loglik: -1.5847e+02 - logprior: -1.5732e+00
Epoch 3/10
17/17 - 1s - loss: 159.6996 - loglik: -1.5836e+02 - logprior: -1.3416e+00
Epoch 4/10
17/17 - 1s - loss: 159.2109 - loglik: -1.5793e+02 - logprior: -1.2810e+00
Epoch 5/10
17/17 - 1s - loss: 159.7870 - loglik: -1.5855e+02 - logprior: -1.2372e+00
Fitted a model with MAP estimate = -159.3669
Time for alignment: 29.2222
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 198.8505 - loglik: -1.9442e+02 - logprior: -4.4301e+00
Epoch 2/10
17/17 - 1s - loss: 176.5913 - loglik: -1.7503e+02 - logprior: -1.5619e+00
Epoch 3/10
17/17 - 1s - loss: 166.6380 - loglik: -1.6503e+02 - logprior: -1.6128e+00
Epoch 4/10
17/17 - 1s - loss: 165.0309 - loglik: -1.6339e+02 - logprior: -1.6451e+00
Epoch 5/10
17/17 - 1s - loss: 164.7112 - loglik: -1.6319e+02 - logprior: -1.5230e+00
Epoch 6/10
17/17 - 1s - loss: 164.3914 - loglik: -1.6285e+02 - logprior: -1.5435e+00
Epoch 7/10
17/17 - 1s - loss: 164.2347 - loglik: -1.6271e+02 - logprior: -1.5228e+00
Epoch 8/10
17/17 - 1s - loss: 164.3004 - loglik: -1.6279e+02 - logprior: -1.5068e+00
Fitted a model with MAP estimate = -164.1929
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 171.3547 - loglik: -1.6578e+02 - logprior: -5.5713e+00
Epoch 2/2
17/17 - 1s - loss: 163.1783 - loglik: -1.6049e+02 - logprior: -2.6924e+00
Fitted a model with MAP estimate = -161.0770
expansions: [(2, 1)]
discards: [ 0 12 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 165.4657 - loglik: -1.6052e+02 - logprior: -4.9412e+00
Epoch 2/2
17/17 - 1s - loss: 160.3272 - loglik: -1.5865e+02 - logprior: -1.6748e+00
Fitted a model with MAP estimate = -159.8062
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.2310 - loglik: -1.5896e+02 - logprior: -4.2731e+00
Epoch 2/10
17/17 - 1s - loss: 159.9986 - loglik: -1.5842e+02 - logprior: -1.5740e+00
Epoch 3/10
17/17 - 1s - loss: 159.6689 - loglik: -1.5833e+02 - logprior: -1.3369e+00
Epoch 4/10
17/17 - 1s - loss: 159.4503 - loglik: -1.5817e+02 - logprior: -1.2837e+00
Epoch 5/10
17/17 - 1s - loss: 159.4940 - loglik: -1.5826e+02 - logprior: -1.2352e+00
Fitted a model with MAP estimate = -159.3665
Time for alignment: 29.5248
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 198.8602 - loglik: -1.9443e+02 - logprior: -4.4320e+00
Epoch 2/10
17/17 - 1s - loss: 176.5326 - loglik: -1.7498e+02 - logprior: -1.5550e+00
Epoch 3/10
17/17 - 1s - loss: 166.3824 - loglik: -1.6479e+02 - logprior: -1.5945e+00
Epoch 4/10
17/17 - 1s - loss: 165.0511 - loglik: -1.6341e+02 - logprior: -1.6381e+00
Epoch 5/10
17/17 - 1s - loss: 164.6186 - loglik: -1.6310e+02 - logprior: -1.5159e+00
Epoch 6/10
17/17 - 1s - loss: 164.3665 - loglik: -1.6283e+02 - logprior: -1.5375e+00
Epoch 7/10
17/17 - 1s - loss: 164.2981 - loglik: -1.6278e+02 - logprior: -1.5165e+00
Epoch 8/10
17/17 - 1s - loss: 164.3038 - loglik: -1.6280e+02 - logprior: -1.5043e+00
Fitted a model with MAP estimate = -164.1898
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 171.5355 - loglik: -1.6596e+02 - logprior: -5.5744e+00
Epoch 2/2
17/17 - 1s - loss: 162.7983 - loglik: -1.6010e+02 - logprior: -2.6938e+00
Fitted a model with MAP estimate = -161.0895
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 165.6380 - loglik: -1.6069e+02 - logprior: -4.9453e+00
Epoch 2/2
17/17 - 1s - loss: 160.4274 - loglik: -1.5876e+02 - logprior: -1.6651e+00
Fitted a model with MAP estimate = -159.8161
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 163.1091 - loglik: -1.5883e+02 - logprior: -4.2757e+00
Epoch 2/10
17/17 - 1s - loss: 160.1055 - loglik: -1.5853e+02 - logprior: -1.5726e+00
Epoch 3/10
17/17 - 1s - loss: 159.6160 - loglik: -1.5828e+02 - logprior: -1.3344e+00
Epoch 4/10
17/17 - 1s - loss: 159.2981 - loglik: -1.5801e+02 - logprior: -1.2869e+00
Epoch 5/10
17/17 - 1s - loss: 159.4651 - loglik: -1.5823e+02 - logprior: -1.2330e+00
Fitted a model with MAP estimate = -159.3638
Time for alignment: 29.0589
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 198.7583 - loglik: -1.9433e+02 - logprior: -4.4321e+00
Epoch 2/10
17/17 - 1s - loss: 178.3966 - loglik: -1.7685e+02 - logprior: -1.5419e+00
Epoch 3/10
17/17 - 1s - loss: 167.6481 - loglik: -1.6611e+02 - logprior: -1.5362e+00
Epoch 4/10
17/17 - 1s - loss: 165.1823 - loglik: -1.6352e+02 - logprior: -1.6599e+00
Epoch 5/10
17/17 - 1s - loss: 164.4980 - loglik: -1.6298e+02 - logprior: -1.5202e+00
Epoch 6/10
17/17 - 1s - loss: 164.3666 - loglik: -1.6283e+02 - logprior: -1.5359e+00
Epoch 7/10
17/17 - 1s - loss: 164.3840 - loglik: -1.6286e+02 - logprior: -1.5242e+00
Fitted a model with MAP estimate = -164.2493
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 171.5076 - loglik: -1.6594e+02 - logprior: -5.5661e+00
Epoch 2/2
17/17 - 1s - loss: 162.8600 - loglik: -1.6020e+02 - logprior: -2.6588e+00
Fitted a model with MAP estimate = -161.0783
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 165.6133 - loglik: -1.6066e+02 - logprior: -4.9541e+00
Epoch 2/2
17/17 - 1s - loss: 160.4871 - loglik: -1.5883e+02 - logprior: -1.6618e+00
Fitted a model with MAP estimate = -159.8143
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 163.1510 - loglik: -1.5887e+02 - logprior: -4.2787e+00
Epoch 2/10
17/17 - 1s - loss: 159.9984 - loglik: -1.5843e+02 - logprior: -1.5707e+00
Epoch 3/10
17/17 - 1s - loss: 159.8094 - loglik: -1.5847e+02 - logprior: -1.3388e+00
Epoch 4/10
17/17 - 1s - loss: 159.2288 - loglik: -1.5795e+02 - logprior: -1.2808e+00
Epoch 5/10
17/17 - 1s - loss: 159.6066 - loglik: -1.5836e+02 - logprior: -1.2467e+00
Fitted a model with MAP estimate = -159.3642
Time for alignment: 28.4361
Computed alignments with likelihoods: ['-159.3195', '-159.3669', '-159.3665', '-159.3638', '-159.3642']
Best model has likelihood: -159.3195  (prior= -1.1971 )
time for generating output: 0.0980
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7610961018911617
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 387.7012 - loglik: -3.7960e+02 - logprior: -8.0979e+00
Epoch 2/10
13/13 - 1s - loss: 358.4318 - loglik: -3.5617e+02 - logprior: -2.2633e+00
Epoch 3/10
13/13 - 1s - loss: 341.3252 - loglik: -3.3940e+02 - logprior: -1.9281e+00
Epoch 4/10
13/13 - 1s - loss: 333.3939 - loglik: -3.3131e+02 - logprior: -2.0827e+00
Epoch 5/10
13/13 - 1s - loss: 329.8579 - loglik: -3.2783e+02 - logprior: -2.0268e+00
Epoch 6/10
13/13 - 1s - loss: 328.3678 - loglik: -3.2644e+02 - logprior: -1.9316e+00
Epoch 7/10
13/13 - 1s - loss: 327.0281 - loglik: -3.2508e+02 - logprior: -1.9511e+00
Epoch 8/10
13/13 - 1s - loss: 327.4602 - loglik: -3.2552e+02 - logprior: -1.9378e+00
Fitted a model with MAP estimate = -327.0425
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 336.7458 - loglik: -3.2717e+02 - logprior: -9.5739e+00
Epoch 2/2
13/13 - 1s - loss: 323.0146 - loglik: -3.1860e+02 - logprior: -4.4157e+00
Fitted a model with MAP estimate = -321.3941
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 324.2458 - loglik: -3.1694e+02 - logprior: -7.3064e+00
Epoch 2/2
13/13 - 1s - loss: 318.8517 - loglik: -3.1667e+02 - logprior: -2.1816e+00
Fitted a model with MAP estimate = -317.3608
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 327.9334 - loglik: -3.1858e+02 - logprior: -9.3520e+00
Epoch 2/10
13/13 - 1s - loss: 319.4584 - loglik: -3.1602e+02 - logprior: -3.4421e+00
Epoch 3/10
13/13 - 1s - loss: 316.8567 - loglik: -3.1518e+02 - logprior: -1.6809e+00
Epoch 4/10
13/13 - 1s - loss: 315.1942 - loglik: -3.1396e+02 - logprior: -1.2297e+00
Epoch 5/10
13/13 - 1s - loss: 314.5666 - loglik: -3.1342e+02 - logprior: -1.1454e+00
Epoch 6/10
13/13 - 1s - loss: 313.6458 - loglik: -3.1248e+02 - logprior: -1.1687e+00
Epoch 7/10
13/13 - 1s - loss: 313.6833 - loglik: -3.1252e+02 - logprior: -1.1653e+00
Fitted a model with MAP estimate = -313.5381
Time for alignment: 33.7390
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 387.9341 - loglik: -3.7983e+02 - logprior: -8.0992e+00
Epoch 2/10
13/13 - 1s - loss: 357.8232 - loglik: -3.5557e+02 - logprior: -2.2550e+00
Epoch 3/10
13/13 - 1s - loss: 341.1516 - loglik: -3.3926e+02 - logprior: -1.8933e+00
Epoch 4/10
13/13 - 1s - loss: 333.8401 - loglik: -3.3186e+02 - logprior: -1.9779e+00
Epoch 5/10
13/13 - 1s - loss: 330.6116 - loglik: -3.2873e+02 - logprior: -1.8807e+00
Epoch 6/10
13/13 - 1s - loss: 328.9809 - loglik: -3.2717e+02 - logprior: -1.8142e+00
Epoch 7/10
13/13 - 1s - loss: 328.3999 - loglik: -3.2656e+02 - logprior: -1.8428e+00
Epoch 8/10
13/13 - 1s - loss: 328.1232 - loglik: -3.2630e+02 - logprior: -1.8282e+00
Epoch 9/10
13/13 - 1s - loss: 327.7778 - loglik: -3.2597e+02 - logprior: -1.8084e+00
Epoch 10/10
13/13 - 1s - loss: 327.9617 - loglik: -3.2616e+02 - logprior: -1.8060e+00
Fitted a model with MAP estimate = -327.8284
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 336.8576 - loglik: -3.2727e+02 - logprior: -9.5879e+00
Epoch 2/2
13/13 - 1s - loss: 323.2033 - loglik: -3.1878e+02 - logprior: -4.4264e+00
Fitted a model with MAP estimate = -321.4736
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 324.6711 - loglik: -3.1737e+02 - logprior: -7.2996e+00
Epoch 2/2
13/13 - 1s - loss: 318.5341 - loglik: -3.1636e+02 - logprior: -2.1753e+00
Fitted a model with MAP estimate = -317.3403
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 327.8506 - loglik: -3.1850e+02 - logprior: -9.3500e+00
Epoch 2/10
13/13 - 1s - loss: 319.2245 - loglik: -3.1579e+02 - logprior: -3.4338e+00
Epoch 3/10
13/13 - 1s - loss: 316.9218 - loglik: -3.1525e+02 - logprior: -1.6681e+00
Epoch 4/10
13/13 - 1s - loss: 315.6025 - loglik: -3.1437e+02 - logprior: -1.2372e+00
Epoch 5/10
13/13 - 1s - loss: 314.2975 - loglik: -3.1316e+02 - logprior: -1.1345e+00
Epoch 6/10
13/13 - 1s - loss: 314.2674 - loglik: -3.1311e+02 - logprior: -1.1603e+00
Epoch 7/10
13/13 - 1s - loss: 313.0791 - loglik: -3.1192e+02 - logprior: -1.1631e+00
Epoch 8/10
13/13 - 1s - loss: 313.7136 - loglik: -3.1258e+02 - logprior: -1.1376e+00
Fitted a model with MAP estimate = -313.4006
Time for alignment: 34.9408
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 387.4480 - loglik: -3.7935e+02 - logprior: -8.0981e+00
Epoch 2/10
13/13 - 1s - loss: 358.3576 - loglik: -3.5611e+02 - logprior: -2.2474e+00
Epoch 3/10
13/13 - 1s - loss: 340.5760 - loglik: -3.3868e+02 - logprior: -1.8928e+00
Epoch 4/10
13/13 - 1s - loss: 333.5477 - loglik: -3.3155e+02 - logprior: -2.0013e+00
Epoch 5/10
13/13 - 1s - loss: 330.7278 - loglik: -3.2883e+02 - logprior: -1.9021e+00
Epoch 6/10
13/13 - 1s - loss: 329.6145 - loglik: -3.2780e+02 - logprior: -1.8149e+00
Epoch 7/10
13/13 - 1s - loss: 328.6634 - loglik: -3.2682e+02 - logprior: -1.8390e+00
Epoch 8/10
13/13 - 1s - loss: 328.5956 - loglik: -3.2678e+02 - logprior: -1.8206e+00
Epoch 9/10
13/13 - 1s - loss: 328.3239 - loglik: -3.2653e+02 - logprior: -1.7978e+00
Epoch 10/10
13/13 - 1s - loss: 328.4880 - loglik: -3.2669e+02 - logprior: -1.7951e+00
Fitted a model with MAP estimate = -328.3077
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 337.6405 - loglik: -3.2806e+02 - logprior: -9.5852e+00
Epoch 2/2
13/13 - 1s - loss: 322.8090 - loglik: -3.1838e+02 - logprior: -4.4320e+00
Fitted a model with MAP estimate = -321.5251
expansions: [(0, 2)]
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 325.0219 - loglik: -3.1772e+02 - logprior: -7.2981e+00
Epoch 2/2
13/13 - 1s - loss: 318.1844 - loglik: -3.1601e+02 - logprior: -2.1784e+00
Fitted a model with MAP estimate = -317.3418
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 327.6919 - loglik: -3.1836e+02 - logprior: -9.3339e+00
Epoch 2/10
13/13 - 1s - loss: 319.7048 - loglik: -3.1631e+02 - logprior: -3.3987e+00
Epoch 3/10
13/13 - 1s - loss: 316.1485 - loglik: -3.1448e+02 - logprior: -1.6681e+00
Epoch 4/10
13/13 - 1s - loss: 315.1765 - loglik: -3.1395e+02 - logprior: -1.2240e+00
Epoch 5/10
13/13 - 1s - loss: 315.2440 - loglik: -3.1411e+02 - logprior: -1.1366e+00
Fitted a model with MAP estimate = -314.1750
Time for alignment: 32.7340
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 387.5187 - loglik: -3.7942e+02 - logprior: -8.0977e+00
Epoch 2/10
13/13 - 1s - loss: 358.4911 - loglik: -3.5624e+02 - logprior: -2.2481e+00
Epoch 3/10
13/13 - 1s - loss: 340.6256 - loglik: -3.3872e+02 - logprior: -1.9066e+00
Epoch 4/10
13/13 - 1s - loss: 333.4747 - loglik: -3.3144e+02 - logprior: -2.0347e+00
Epoch 5/10
13/13 - 1s - loss: 330.2290 - loglik: -3.2829e+02 - logprior: -1.9376e+00
Epoch 6/10
13/13 - 1s - loss: 328.8708 - loglik: -3.2703e+02 - logprior: -1.8399e+00
Epoch 7/10
13/13 - 1s - loss: 328.2407 - loglik: -3.2639e+02 - logprior: -1.8544e+00
Epoch 8/10
13/13 - 1s - loss: 328.0193 - loglik: -3.2619e+02 - logprior: -1.8297e+00
Epoch 9/10
13/13 - 1s - loss: 328.1610 - loglik: -3.2635e+02 - logprior: -1.8115e+00
Fitted a model with MAP estimate = -327.8777
expansions: [(12, 1), (17, 5), (18, 1), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 336.3665 - loglik: -3.2679e+02 - logprior: -9.5773e+00
Epoch 2/2
13/13 - 1s - loss: 322.9153 - loglik: -3.1854e+02 - logprior: -4.3776e+00
Fitted a model with MAP estimate = -321.4704
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 324.6291 - loglik: -3.1733e+02 - logprior: -7.2975e+00
Epoch 2/2
13/13 - 1s - loss: 318.1750 - loglik: -3.1599e+02 - logprior: -2.1841e+00
Fitted a model with MAP estimate = -317.3815
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 328.3240 - loglik: -3.1897e+02 - logprior: -9.3559e+00
Epoch 2/10
13/13 - 1s - loss: 318.8677 - loglik: -3.1541e+02 - logprior: -3.4551e+00
Epoch 3/10
13/13 - 1s - loss: 316.9081 - loglik: -3.1522e+02 - logprior: -1.6896e+00
Epoch 4/10
13/13 - 1s - loss: 315.2421 - loglik: -3.1401e+02 - logprior: -1.2320e+00
Epoch 5/10
13/13 - 1s - loss: 314.9435 - loglik: -3.1380e+02 - logprior: -1.1436e+00
Epoch 6/10
13/13 - 1s - loss: 313.7936 - loglik: -3.1263e+02 - logprior: -1.1609e+00
Epoch 7/10
13/13 - 1s - loss: 313.9255 - loglik: -3.1276e+02 - logprior: -1.1679e+00
Fitted a model with MAP estimate = -313.6079
Time for alignment: 33.3371
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 387.6931 - loglik: -3.7959e+02 - logprior: -8.0997e+00
Epoch 2/10
13/13 - 1s - loss: 358.6671 - loglik: -3.5642e+02 - logprior: -2.2502e+00
Epoch 3/10
13/13 - 1s - loss: 341.5996 - loglik: -3.3969e+02 - logprior: -1.9078e+00
Epoch 4/10
13/13 - 1s - loss: 334.7867 - loglik: -3.3277e+02 - logprior: -2.0167e+00
Epoch 5/10
13/13 - 1s - loss: 330.4785 - loglik: -3.2858e+02 - logprior: -1.8957e+00
Epoch 6/10
13/13 - 1s - loss: 329.4793 - loglik: -3.2768e+02 - logprior: -1.7977e+00
Epoch 7/10
13/13 - 1s - loss: 328.6803 - loglik: -3.2686e+02 - logprior: -1.8184e+00
Epoch 8/10
13/13 - 1s - loss: 328.8068 - loglik: -3.2700e+02 - logprior: -1.8103e+00
Fitted a model with MAP estimate = -328.5264
expansions: [(12, 1), (17, 5), (18, 1), (19, 2), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 336.6792 - loglik: -3.2710e+02 - logprior: -9.5768e+00
Epoch 2/2
13/13 - 1s - loss: 323.6747 - loglik: -3.1926e+02 - logprior: -4.4178e+00
Fitted a model with MAP estimate = -321.4927
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 324.9221 - loglik: -3.1762e+02 - logprior: -7.3018e+00
Epoch 2/2
13/13 - 1s - loss: 318.2768 - loglik: -3.1610e+02 - logprior: -2.1783e+00
Fitted a model with MAP estimate = -317.3441
expansions: [(17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 328.3055 - loglik: -3.1895e+02 - logprior: -9.3515e+00
Epoch 2/10
13/13 - 1s - loss: 319.1948 - loglik: -3.1574e+02 - logprior: -3.4518e+00
Epoch 3/10
13/13 - 1s - loss: 316.3456 - loglik: -3.1467e+02 - logprior: -1.6784e+00
Epoch 4/10
13/13 - 1s - loss: 315.2757 - loglik: -3.1404e+02 - logprior: -1.2321e+00
Epoch 5/10
13/13 - 1s - loss: 314.6836 - loglik: -3.1354e+02 - logprior: -1.1448e+00
Epoch 6/10
13/13 - 1s - loss: 314.1813 - loglik: -3.1302e+02 - logprior: -1.1607e+00
Epoch 7/10
13/13 - 1s - loss: 313.5961 - loglik: -3.1244e+02 - logprior: -1.1601e+00
Epoch 8/10
13/13 - 1s - loss: 313.4717 - loglik: -3.1233e+02 - logprior: -1.1453e+00
Epoch 9/10
13/13 - 1s - loss: 313.1768 - loglik: -3.1205e+02 - logprior: -1.1290e+00
Epoch 10/10
13/13 - 1s - loss: 313.7220 - loglik: -3.1263e+02 - logprior: -1.0948e+00
Fitted a model with MAP estimate = -313.2418
Time for alignment: 34.5715
Computed alignments with likelihoods: ['-313.5381', '-313.4006', '-314.1750', '-313.6079', '-313.2418']
Best model has likelihood: -313.2418  (prior= -1.0976 )
time for generating output: 0.1134
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.8824362606232294
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 476.0903 - loglik: -4.6355e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 443.2270 - loglik: -4.3995e+02 - logprior: -3.2802e+00
Epoch 3/10
11/11 - 1s - loss: 416.5599 - loglik: -4.1450e+02 - logprior: -2.0584e+00
Epoch 4/10
11/11 - 1s - loss: 400.6975 - loglik: -3.9914e+02 - logprior: -1.5537e+00
Epoch 5/10
11/11 - 1s - loss: 395.3790 - loglik: -3.9404e+02 - logprior: -1.3432e+00
Epoch 6/10
11/11 - 1s - loss: 393.7739 - loglik: -3.9261e+02 - logprior: -1.1631e+00
Epoch 7/10
11/11 - 1s - loss: 392.5686 - loglik: -3.9157e+02 - logprior: -9.9729e-01
Epoch 8/10
11/11 - 1s - loss: 392.3897 - loglik: -3.9153e+02 - logprior: -8.6318e-01
Epoch 9/10
11/11 - 1s - loss: 391.6617 - loglik: -3.9088e+02 - logprior: -7.8543e-01
Epoch 10/10
11/11 - 1s - loss: 391.6141 - loglik: -3.9089e+02 - logprior: -7.2622e-01
Fitted a model with MAP estimate = -391.5506
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 405.5755 - loglik: -3.8991e+02 - logprior: -1.5664e+01
Epoch 2/2
11/11 - 1s - loss: 385.8383 - loglik: -3.8149e+02 - logprior: -4.3491e+00
Fitted a model with MAP estimate = -382.8475
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 397.4744 - loglik: -3.8338e+02 - logprior: -1.4094e+01
Epoch 2/2
11/11 - 1s - loss: 386.7487 - loglik: -3.8119e+02 - logprior: -5.5549e+00
Fitted a model with MAP estimate = -384.1695
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 393.6030 - loglik: -3.8132e+02 - logprior: -1.2288e+01
Epoch 2/10
11/11 - 1s - loss: 383.2100 - loglik: -3.7998e+02 - logprior: -3.2328e+00
Epoch 3/10
11/11 - 1s - loss: 381.0205 - loglik: -3.7931e+02 - logprior: -1.7124e+00
Epoch 4/10
11/11 - 1s - loss: 379.8297 - loglik: -3.7882e+02 - logprior: -1.0132e+00
Epoch 5/10
11/11 - 1s - loss: 379.2015 - loglik: -3.7838e+02 - logprior: -8.2427e-01
Epoch 6/10
11/11 - 1s - loss: 378.8680 - loglik: -3.7822e+02 - logprior: -6.4827e-01
Epoch 7/10
11/11 - 1s - loss: 378.6427 - loglik: -3.7806e+02 - logprior: -5.7850e-01
Epoch 8/10
11/11 - 1s - loss: 378.6575 - loglik: -3.7814e+02 - logprior: -5.2134e-01
Fitted a model with MAP estimate = -378.4145
Time for alignment: 32.0206
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 476.2769 - loglik: -4.6374e+02 - logprior: -1.2541e+01
Epoch 2/10
11/11 - 1s - loss: 443.0581 - loglik: -4.3977e+02 - logprior: -3.2834e+00
Epoch 3/10
11/11 - 1s - loss: 416.2378 - loglik: -4.1417e+02 - logprior: -2.0659e+00
Epoch 4/10
11/11 - 1s - loss: 401.0448 - loglik: -3.9949e+02 - logprior: -1.5594e+00
Epoch 5/10
11/11 - 1s - loss: 395.6074 - loglik: -3.9430e+02 - logprior: -1.3034e+00
Epoch 6/10
11/11 - 1s - loss: 393.6577 - loglik: -3.9255e+02 - logprior: -1.1076e+00
Epoch 7/10
11/11 - 1s - loss: 392.9753 - loglik: -3.9205e+02 - logprior: -9.2431e-01
Epoch 8/10
11/11 - 1s - loss: 392.6188 - loglik: -3.9182e+02 - logprior: -7.9432e-01
Epoch 9/10
11/11 - 1s - loss: 392.1236 - loglik: -3.9140e+02 - logprior: -7.2411e-01
Epoch 10/10
11/11 - 1s - loss: 391.9247 - loglik: -3.9126e+02 - logprior: -6.6797e-01
Fitted a model with MAP estimate = -391.8089
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 406.6975 - loglik: -3.9103e+02 - logprior: -1.5672e+01
Epoch 2/2
11/11 - 1s - loss: 385.9149 - loglik: -3.8148e+02 - logprior: -4.4374e+00
Fitted a model with MAP estimate = -383.0692
expansions: []
discards: [ 0 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.3009 - loglik: -3.8320e+02 - logprior: -1.4105e+01
Epoch 2/2
11/11 - 1s - loss: 387.0911 - loglik: -3.8153e+02 - logprior: -5.5625e+00
Fitted a model with MAP estimate = -384.2449
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 393.4049 - loglik: -3.8112e+02 - logprior: -1.2284e+01
Epoch 2/10
11/11 - 1s - loss: 383.3003 - loglik: -3.8004e+02 - logprior: -3.2592e+00
Epoch 3/10
11/11 - 1s - loss: 380.8799 - loglik: -3.7918e+02 - logprior: -1.7023e+00
Epoch 4/10
11/11 - 1s - loss: 380.1162 - loglik: -3.7909e+02 - logprior: -1.0247e+00
Epoch 5/10
11/11 - 1s - loss: 379.0172 - loglik: -3.7819e+02 - logprior: -8.2798e-01
Epoch 6/10
11/11 - 1s - loss: 378.8622 - loglik: -3.7821e+02 - logprior: -6.5550e-01
Epoch 7/10
11/11 - 1s - loss: 379.2355 - loglik: -3.7866e+02 - logprior: -5.7719e-01
Fitted a model with MAP estimate = -378.5203
Time for alignment: 36.0185
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 476.3791 - loglik: -4.6384e+02 - logprior: -1.2542e+01
Epoch 2/10
11/11 - 1s - loss: 443.3069 - loglik: -4.4003e+02 - logprior: -3.2789e+00
Epoch 3/10
11/11 - 1s - loss: 417.5031 - loglik: -4.1543e+02 - logprior: -2.0752e+00
Epoch 4/10
11/11 - 1s - loss: 401.2271 - loglik: -3.9967e+02 - logprior: -1.5611e+00
Epoch 5/10
11/11 - 1s - loss: 396.1515 - loglik: -3.9484e+02 - logprior: -1.3099e+00
Epoch 6/10
11/11 - 1s - loss: 393.6583 - loglik: -3.9253e+02 - logprior: -1.1238e+00
Epoch 7/10
11/11 - 1s - loss: 393.3569 - loglik: -3.9243e+02 - logprior: -9.2892e-01
Epoch 8/10
11/11 - 1s - loss: 392.2324 - loglik: -3.9144e+02 - logprior: -7.9608e-01
Epoch 9/10
11/11 - 1s - loss: 392.5248 - loglik: -3.9179e+02 - logprior: -7.3272e-01
Fitted a model with MAP estimate = -392.1467
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 406.7899 - loglik: -3.9120e+02 - logprior: -1.5593e+01
Epoch 2/2
11/11 - 1s - loss: 386.2533 - loglik: -3.8185e+02 - logprior: -4.4072e+00
Fitted a model with MAP estimate = -383.1028
expansions: []
discards: [ 0 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.4018 - loglik: -3.8329e+02 - logprior: -1.4108e+01
Epoch 2/2
11/11 - 1s - loss: 386.8721 - loglik: -3.8132e+02 - logprior: -5.5541e+00
Fitted a model with MAP estimate = -384.2206
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 393.7295 - loglik: -3.8145e+02 - logprior: -1.2282e+01
Epoch 2/10
11/11 - 1s - loss: 382.8681 - loglik: -3.7963e+02 - logprior: -3.2408e+00
Epoch 3/10
11/11 - 1s - loss: 381.1483 - loglik: -3.7944e+02 - logprior: -1.7104e+00
Epoch 4/10
11/11 - 1s - loss: 379.8028 - loglik: -3.7878e+02 - logprior: -1.0229e+00
Epoch 5/10
11/11 - 1s - loss: 379.2649 - loglik: -3.7844e+02 - logprior: -8.2217e-01
Epoch 6/10
11/11 - 1s - loss: 379.1748 - loglik: -3.7851e+02 - logprior: -6.6216e-01
Epoch 7/10
11/11 - 1s - loss: 378.4776 - loglik: -3.7790e+02 - logprior: -5.8030e-01
Epoch 8/10
11/11 - 1s - loss: 378.7004 - loglik: -3.7817e+02 - logprior: -5.2845e-01
Fitted a model with MAP estimate = -378.4171
Time for alignment: 34.7627
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 476.2428 - loglik: -4.6370e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 442.5755 - loglik: -4.3929e+02 - logprior: -3.2825e+00
Epoch 3/10
11/11 - 1s - loss: 415.3174 - loglik: -4.1326e+02 - logprior: -2.0617e+00
Epoch 4/10
11/11 - 1s - loss: 400.2939 - loglik: -3.9874e+02 - logprior: -1.5529e+00
Epoch 5/10
11/11 - 1s - loss: 395.3510 - loglik: -3.9400e+02 - logprior: -1.3505e+00
Epoch 6/10
11/11 - 1s - loss: 393.2058 - loglik: -3.9203e+02 - logprior: -1.1740e+00
Epoch 7/10
11/11 - 1s - loss: 392.6523 - loglik: -3.9165e+02 - logprior: -1.0003e+00
Epoch 8/10
11/11 - 1s - loss: 392.3535 - loglik: -3.9149e+02 - logprior: -8.6194e-01
Epoch 9/10
11/11 - 1s - loss: 391.6374 - loglik: -3.9085e+02 - logprior: -7.8969e-01
Epoch 10/10
11/11 - 1s - loss: 391.7948 - loglik: -3.9106e+02 - logprior: -7.3356e-01
Fitted a model with MAP estimate = -391.4842
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 405.7512 - loglik: -3.9010e+02 - logprior: -1.5653e+01
Epoch 2/2
11/11 - 1s - loss: 385.4825 - loglik: -3.8114e+02 - logprior: -4.3426e+00
Fitted a model with MAP estimate = -382.8093
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.3921 - loglik: -3.8330e+02 - logprior: -1.4091e+01
Epoch 2/2
11/11 - 1s - loss: 386.3930 - loglik: -3.8084e+02 - logprior: -5.5540e+00
Fitted a model with MAP estimate = -384.1613
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 393.5237 - loglik: -3.8125e+02 - logprior: -1.2275e+01
Epoch 2/10
11/11 - 1s - loss: 383.0270 - loglik: -3.7978e+02 - logprior: -3.2457e+00
Epoch 3/10
11/11 - 1s - loss: 381.1784 - loglik: -3.7948e+02 - logprior: -1.7005e+00
Epoch 4/10
11/11 - 1s - loss: 379.7523 - loglik: -3.7874e+02 - logprior: -1.0152e+00
Epoch 5/10
11/11 - 1s - loss: 379.4733 - loglik: -3.7865e+02 - logprior: -8.2214e-01
Epoch 6/10
11/11 - 1s - loss: 378.9669 - loglik: -3.7832e+02 - logprior: -6.5179e-01
Epoch 7/10
11/11 - 1s - loss: 378.1331 - loglik: -3.7756e+02 - logprior: -5.7358e-01
Epoch 8/10
11/11 - 1s - loss: 378.5360 - loglik: -3.7801e+02 - logprior: -5.2784e-01
Fitted a model with MAP estimate = -378.4193
Time for alignment: 35.2993
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 476.4989 - loglik: -4.6396e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 443.5865 - loglik: -4.4030e+02 - logprior: -3.2848e+00
Epoch 3/10
11/11 - 1s - loss: 419.0158 - loglik: -4.1696e+02 - logprior: -2.0597e+00
Epoch 4/10
11/11 - 1s - loss: 403.1926 - loglik: -4.0164e+02 - logprior: -1.5498e+00
Epoch 5/10
11/11 - 1s - loss: 395.6647 - loglik: -3.9431e+02 - logprior: -1.3546e+00
Epoch 6/10
11/11 - 1s - loss: 393.9544 - loglik: -3.9277e+02 - logprior: -1.1842e+00
Epoch 7/10
11/11 - 1s - loss: 393.2348 - loglik: -3.9224e+02 - logprior: -9.9947e-01
Epoch 8/10
11/11 - 1s - loss: 392.6374 - loglik: -3.9178e+02 - logprior: -8.5249e-01
Epoch 9/10
11/11 - 1s - loss: 391.7269 - loglik: -3.9095e+02 - logprior: -7.7275e-01
Epoch 10/10
11/11 - 1s - loss: 392.0171 - loglik: -3.9131e+02 - logprior: -7.1159e-01
Fitted a model with MAP estimate = -391.8022
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 406.2288 - loglik: -3.9056e+02 - logprior: -1.5666e+01
Epoch 2/2
11/11 - 1s - loss: 386.3747 - loglik: -3.8198e+02 - logprior: -4.3908e+00
Fitted a model with MAP estimate = -382.9469
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 397.6445 - loglik: -3.8355e+02 - logprior: -1.4098e+01
Epoch 2/2
11/11 - 1s - loss: 387.0043 - loglik: -3.8145e+02 - logprior: -5.5533e+00
Fitted a model with MAP estimate = -384.4222
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 393.7863 - loglik: -3.8151e+02 - logprior: -1.2276e+01
Epoch 2/10
11/11 - 1s - loss: 383.1748 - loglik: -3.7993e+02 - logprior: -3.2443e+00
Epoch 3/10
11/11 - 1s - loss: 381.0965 - loglik: -3.7940e+02 - logprior: -1.7003e+00
Epoch 4/10
11/11 - 1s - loss: 379.8347 - loglik: -3.7882e+02 - logprior: -1.0182e+00
Epoch 5/10
11/11 - 1s - loss: 379.3001 - loglik: -3.7848e+02 - logprior: -8.1625e-01
Epoch 6/10
11/11 - 1s - loss: 378.9221 - loglik: -3.7827e+02 - logprior: -6.5378e-01
Epoch 7/10
11/11 - 1s - loss: 378.8783 - loglik: -3.7830e+02 - logprior: -5.7614e-01
Epoch 8/10
11/11 - 1s - loss: 378.2411 - loglik: -3.7773e+02 - logprior: -5.1512e-01
Epoch 9/10
11/11 - 1s - loss: 378.6774 - loglik: -3.7818e+02 - logprior: -4.9683e-01
Fitted a model with MAP estimate = -378.3245
Time for alignment: 36.0542
Computed alignments with likelihoods: ['-378.4145', '-378.5203', '-378.4171', '-378.4193', '-378.3245']
Best model has likelihood: -378.3245  (prior= -0.4610 )
time for generating output: 0.1124
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 1117.0601 - loglik: -1.1117e+03 - logprior: -5.3296e+00
Epoch 2/10
24/24 - 6s - loss: 964.3727 - loglik: -9.6219e+02 - logprior: -2.1827e+00
Epoch 3/10
24/24 - 6s - loss: 937.4915 - loglik: -9.3508e+02 - logprior: -2.4104e+00
Epoch 4/10
24/24 - 6s - loss: 933.8381 - loglik: -9.3161e+02 - logprior: -2.2310e+00
Epoch 5/10
24/24 - 6s - loss: 930.5112 - loglik: -9.2826e+02 - logprior: -2.2539e+00
Epoch 6/10
24/24 - 6s - loss: 929.9792 - loglik: -9.2770e+02 - logprior: -2.2840e+00
Epoch 7/10
24/24 - 6s - loss: 933.3842 - loglik: -9.3108e+02 - logprior: -2.3075e+00
Fitted a model with MAP estimate = -930.9278
expansions: [(11, 1), (12, 2), (13, 3), (16, 2), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (65, 1), (75, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 1), (175, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 924.5764 - loglik: -9.1730e+02 - logprior: -7.2814e+00
Epoch 2/2
24/24 - 8s - loss: 904.1808 - loglik: -9.0161e+02 - logprior: -2.5749e+00
Fitted a model with MAP estimate = -902.3122
expansions: [(0, 3), (192, 1), (194, 1)]
discards: [ 0 14 25 64]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 904.7983 - loglik: -9.0033e+02 - logprior: -4.4677e+00
Epoch 2/2
24/24 - 8s - loss: 898.1950 - loglik: -8.9821e+02 - logprior: 0.0153
Fitted a model with MAP estimate = -895.5801
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 909.0859 - loglik: -9.0270e+02 - logprior: -6.3817e+00
Epoch 2/10
24/24 - 8s - loss: 898.0690 - loglik: -8.9730e+02 - logprior: -7.7304e-01
Epoch 3/10
24/24 - 8s - loss: 897.3259 - loglik: -8.9858e+02 - logprior: 1.2504
Epoch 4/10
24/24 - 8s - loss: 895.1232 - loglik: -8.9642e+02 - logprior: 1.2952
Epoch 5/10
24/24 - 8s - loss: 893.9011 - loglik: -8.9517e+02 - logprior: 1.2700
Epoch 6/10
24/24 - 8s - loss: 893.4493 - loglik: -8.9488e+02 - logprior: 1.4280
Epoch 7/10
24/24 - 8s - loss: 893.5552 - loglik: -8.9518e+02 - logprior: 1.6263
Fitted a model with MAP estimate = -892.7264
Time for alignment: 169.0891
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1114.6976 - loglik: -1.1094e+03 - logprior: -5.2904e+00
Epoch 2/10
24/24 - 6s - loss: 966.4251 - loglik: -9.6425e+02 - logprior: -2.1787e+00
Epoch 3/10
24/24 - 6s - loss: 936.6860 - loglik: -9.3412e+02 - logprior: -2.5681e+00
Epoch 4/10
24/24 - 6s - loss: 932.2314 - loglik: -9.2984e+02 - logprior: -2.3886e+00
Epoch 5/10
24/24 - 6s - loss: 931.8801 - loglik: -9.2949e+02 - logprior: -2.3939e+00
Epoch 6/10
24/24 - 6s - loss: 930.5056 - loglik: -9.2808e+02 - logprior: -2.4296e+00
Epoch 7/10
24/24 - 6s - loss: 932.5533 - loglik: -9.3009e+02 - logprior: -2.4645e+00
Fitted a model with MAP estimate = -930.5375
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (34, 2), (35, 1), (36, 1), (37, 1), (39, 2), (46, 1), (47, 1), (48, 1), (61, 1), (65, 1), (75, 1), (82, 1), (86, 1), (87, 1), (90, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (190, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 920.7148 - loglik: -9.1574e+02 - logprior: -4.9705e+00
Epoch 2/2
24/24 - 8s - loss: 901.8699 - loglik: -9.0131e+02 - logprior: -5.5498e-01
Fitted a model with MAP estimate = -899.7657
expansions: [(196, 1), (217, 1)]
discards: [13 26 54]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 905.2114 - loglik: -9.0073e+02 - logprior: -4.4791e+00
Epoch 2/2
24/24 - 8s - loss: 897.6608 - loglik: -8.9768e+02 - logprior: 0.0182
Fitted a model with MAP estimate = -896.5490
expansions: [(150, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 901.4814 - loglik: -8.9742e+02 - logprior: -4.0654e+00
Epoch 2/10
24/24 - 8s - loss: 900.8306 - loglik: -9.0129e+02 - logprior: 0.4636
Epoch 3/10
24/24 - 8s - loss: 893.1528 - loglik: -8.9411e+02 - logprior: 0.9613
Epoch 4/10
24/24 - 8s - loss: 896.5474 - loglik: -8.9771e+02 - logprior: 1.1622
Fitted a model with MAP estimate = -893.5742
Time for alignment: 147.0202
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1116.6266 - loglik: -1.1113e+03 - logprior: -5.2981e+00
Epoch 2/10
24/24 - 6s - loss: 967.1774 - loglik: -9.6508e+02 - logprior: -2.0989e+00
Epoch 3/10
24/24 - 6s - loss: 941.6504 - loglik: -9.3914e+02 - logprior: -2.5141e+00
Epoch 4/10
24/24 - 6s - loss: 934.5342 - loglik: -9.3213e+02 - logprior: -2.4058e+00
Epoch 5/10
24/24 - 6s - loss: 934.9359 - loglik: -9.3251e+02 - logprior: -2.4226e+00
Fitted a model with MAP estimate = -933.6201
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (76, 1), (78, 1), (85, 1), (88, 1), (91, 1), (111, 1), (113, 1), (114, 1), (116, 1), (119, 2), (120, 1), (121, 1), (123, 1), (124, 1), (153, 2), (154, 4), (155, 1), (156, 1), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (185, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 924.3506 - loglik: -9.1710e+02 - logprior: -7.2468e+00
Epoch 2/2
24/24 - 8s - loss: 903.1592 - loglik: -9.0073e+02 - logprior: -2.4313e+00
Fitted a model with MAP estimate = -901.0101
expansions: [(0, 2)]
discards: [ 0 12 63]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 906.1450 - loglik: -9.0160e+02 - logprior: -4.5405e+00
Epoch 2/2
24/24 - 8s - loss: 898.7075 - loglik: -8.9865e+02 - logprior: -5.8572e-02
Fitted a model with MAP estimate = -896.5214
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 905.3849 - loglik: -9.0123e+02 - logprior: -4.1562e+00
Epoch 2/10
24/24 - 8s - loss: 897.0882 - loglik: -8.9741e+02 - logprior: 0.3195
Epoch 3/10
24/24 - 8s - loss: 895.8608 - loglik: -8.9667e+02 - logprior: 0.8059
Epoch 4/10
24/24 - 8s - loss: 892.6683 - loglik: -8.9365e+02 - logprior: 0.9771
Epoch 5/10
24/24 - 8s - loss: 895.2375 - loglik: -8.9638e+02 - logprior: 1.1447
Fitted a model with MAP estimate = -893.4000
Time for alignment: 141.1582
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1115.0183 - loglik: -1.1097e+03 - logprior: -5.3362e+00
Epoch 2/10
24/24 - 6s - loss: 969.0569 - loglik: -9.6684e+02 - logprior: -2.2138e+00
Epoch 3/10
24/24 - 6s - loss: 937.4913 - loglik: -9.3488e+02 - logprior: -2.6159e+00
Epoch 4/10
24/24 - 6s - loss: 934.0477 - loglik: -9.3163e+02 - logprior: -2.4146e+00
Epoch 5/10
24/24 - 6s - loss: 931.4265 - loglik: -9.2904e+02 - logprior: -2.3901e+00
Epoch 6/10
24/24 - 6s - loss: 930.1544 - loglik: -9.2774e+02 - logprior: -2.4098e+00
Epoch 7/10
24/24 - 6s - loss: 934.3239 - loglik: -9.3190e+02 - logprior: -2.4263e+00
Fitted a model with MAP estimate = -931.1703
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 2), (49, 2), (63, 1), (65, 1), (76, 1), (82, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 2), (172, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 926.7126 - loglik: -9.1937e+02 - logprior: -7.3442e+00
Epoch 2/2
24/24 - 8s - loss: 902.4144 - loglik: -8.9974e+02 - logprior: -2.6746e+00
Fitted a model with MAP estimate = -902.4344
expansions: [(0, 3), (192, 1), (194, 1)]
discards: [  0  12  25  64 218 219]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 906.1177 - loglik: -9.0154e+02 - logprior: -4.5796e+00
Epoch 2/2
24/24 - 8s - loss: 897.7593 - loglik: -8.9766e+02 - logprior: -9.6766e-02
Fitted a model with MAP estimate = -896.3936
expansions: [(151, 1), (219, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 905.9141 - loglik: -8.9951e+02 - logprior: -6.4000e+00
Epoch 2/10
24/24 - 8s - loss: 902.3956 - loglik: -9.0150e+02 - logprior: -9.0009e-01
Epoch 3/10
24/24 - 8s - loss: 897.4644 - loglik: -8.9852e+02 - logprior: 1.0585
Epoch 4/10
24/24 - 8s - loss: 895.4091 - loglik: -8.9654e+02 - logprior: 1.1333
Epoch 5/10
24/24 - 8s - loss: 892.2991 - loglik: -8.9358e+02 - logprior: 1.2784
Epoch 6/10
24/24 - 8s - loss: 892.2309 - loglik: -8.9365e+02 - logprior: 1.4147
Epoch 7/10
24/24 - 8s - loss: 895.1342 - loglik: -8.9675e+02 - logprior: 1.6136
Fitted a model with MAP estimate = -892.7031
Time for alignment: 166.9655
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1113.7340 - loglik: -1.1084e+03 - logprior: -5.3048e+00
Epoch 2/10
24/24 - 6s - loss: 972.6122 - loglik: -9.7046e+02 - logprior: -2.1547e+00
Epoch 3/10
24/24 - 6s - loss: 937.7206 - loglik: -9.3509e+02 - logprior: -2.6350e+00
Epoch 4/10
24/24 - 6s - loss: 934.1317 - loglik: -9.3164e+02 - logprior: -2.4959e+00
Epoch 5/10
24/24 - 6s - loss: 931.8909 - loglik: -9.2939e+02 - logprior: -2.4987e+00
Epoch 6/10
24/24 - 6s - loss: 930.8728 - loglik: -9.2836e+02 - logprior: -2.5140e+00
Epoch 7/10
24/24 - 6s - loss: 930.2705 - loglik: -9.2770e+02 - logprior: -2.5740e+00
Epoch 8/10
24/24 - 6s - loss: 929.3295 - loglik: -9.2677e+02 - logprior: -2.5635e+00
Epoch 9/10
24/24 - 6s - loss: 932.0862 - loglik: -9.2952e+02 - logprior: -2.5644e+00
Fitted a model with MAP estimate = -930.0231
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1), (48, 2), (49, 2), (66, 1), (76, 1), (83, 1), (85, 1), (88, 1), (91, 1), (93, 1), (110, 1), (113, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 1), (172, 1), (173, 3), (174, 1), (186, 1), (187, 1), (189, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 916.7960 - loglik: -9.1188e+02 - logprior: -4.9184e+00
Epoch 2/2
24/24 - 8s - loss: 901.7878 - loglik: -9.0142e+02 - logprior: -3.6941e-01
Fitted a model with MAP estimate = -897.7400
expansions: [(197, 1)]
discards: [13 26 65]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 905.5334 - loglik: -9.0119e+02 - logprior: -4.3475e+00
Epoch 2/2
24/24 - 8s - loss: 898.2153 - loglik: -8.9833e+02 - logprior: 0.1115
Fitted a model with MAP estimate = -896.1533
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 903.7795 - loglik: -8.9975e+02 - logprior: -4.0258e+00
Epoch 2/10
24/24 - 8s - loss: 897.8623 - loglik: -8.9834e+02 - logprior: 0.4823
Epoch 3/10
24/24 - 8s - loss: 897.4060 - loglik: -8.9840e+02 - logprior: 0.9894
Epoch 4/10
24/24 - 8s - loss: 893.1144 - loglik: -8.9430e+02 - logprior: 1.1840
Epoch 5/10
24/24 - 8s - loss: 890.8565 - loglik: -8.9219e+02 - logprior: 1.3371
Epoch 6/10
24/24 - 8s - loss: 894.3162 - loglik: -8.9568e+02 - logprior: 1.3631
Fitted a model with MAP estimate = -892.3670
Time for alignment: 171.1248
Computed alignments with likelihoods: ['-892.7264', '-893.5742', '-893.4000', '-892.7031', '-892.3670']
Best model has likelihood: -892.3670  (prior= 1.5028 )
time for generating output: 0.2666
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.97874072733852
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 594.9538 - loglik: -5.6788e+02 - logprior: -2.7071e+01
Epoch 2/10
10/10 - 1s - loss: 537.5709 - loglik: -5.3067e+02 - logprior: -6.9034e+00
Epoch 3/10
10/10 - 1s - loss: 499.9958 - loglik: -4.9632e+02 - logprior: -3.6761e+00
Epoch 4/10
10/10 - 1s - loss: 480.4470 - loglik: -4.7768e+02 - logprior: -2.7662e+00
Epoch 5/10
10/10 - 1s - loss: 472.0971 - loglik: -4.6982e+02 - logprior: -2.2751e+00
Epoch 6/10
10/10 - 1s - loss: 467.1649 - loglik: -4.6525e+02 - logprior: -1.9188e+00
Epoch 7/10
10/10 - 1s - loss: 466.8805 - loglik: -4.6518e+02 - logprior: -1.6968e+00
Epoch 8/10
10/10 - 1s - loss: 465.6768 - loglik: -4.6407e+02 - logprior: -1.6078e+00
Epoch 9/10
10/10 - 1s - loss: 464.1604 - loglik: -4.6260e+02 - logprior: -1.5635e+00
Epoch 10/10
10/10 - 1s - loss: 463.8201 - loglik: -4.6230e+02 - logprior: -1.5241e+00
Fitted a model with MAP estimate = -463.7003
expansions: [(0, 3), (10, 2), (24, 1), (33, 1), (35, 3), (36, 2), (50, 3), (58, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.1783 - loglik: -4.6161e+02 - logprior: -3.4572e+01
Epoch 2/2
10/10 - 1s - loss: 461.4175 - loglik: -4.5092e+02 - logprior: -1.0501e+01
Fitted a model with MAP estimate = -455.2417
expansions: [(63, 1)]
discards: [13 43 90]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.4953 - loglik: -4.4819e+02 - logprior: -2.5306e+01
Epoch 2/2
10/10 - 1s - loss: 451.9564 - loglik: -4.4562e+02 - logprior: -6.3399e+00
Fitted a model with MAP estimate = -449.0168
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 469.3630 - loglik: -4.4531e+02 - logprior: -2.4051e+01
Epoch 2/10
10/10 - 1s - loss: 451.0638 - loglik: -4.4506e+02 - logprior: -6.0077e+00
Epoch 3/10
10/10 - 1s - loss: 447.8148 - loglik: -4.4538e+02 - logprior: -2.4393e+00
Epoch 4/10
10/10 - 1s - loss: 444.7660 - loglik: -4.4368e+02 - logprior: -1.0905e+00
Epoch 5/10
10/10 - 1s - loss: 444.4067 - loglik: -4.4397e+02 - logprior: -4.3795e-01
Epoch 6/10
10/10 - 1s - loss: 444.0448 - loglik: -4.4395e+02 - logprior: -9.9425e-02
Epoch 7/10
10/10 - 1s - loss: 443.6016 - loglik: -4.4377e+02 - logprior: 0.1719
Epoch 8/10
10/10 - 1s - loss: 443.4342 - loglik: -4.4383e+02 - logprior: 0.3925
Epoch 9/10
10/10 - 1s - loss: 443.7799 - loglik: -4.4433e+02 - logprior: 0.5510
Fitted a model with MAP estimate = -443.2500
Time for alignment: 34.7280
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 594.5439 - loglik: -5.6747e+02 - logprior: -2.7070e+01
Epoch 2/10
10/10 - 1s - loss: 538.3278 - loglik: -5.3142e+02 - logprior: -6.9057e+00
Epoch 3/10
10/10 - 1s - loss: 500.7989 - loglik: -4.9713e+02 - logprior: -3.6681e+00
Epoch 4/10
10/10 - 1s - loss: 480.0663 - loglik: -4.7737e+02 - logprior: -2.6934e+00
Epoch 5/10
10/10 - 1s - loss: 471.7103 - loglik: -4.6947e+02 - logprior: -2.2382e+00
Epoch 6/10
10/10 - 1s - loss: 468.6068 - loglik: -4.6678e+02 - logprior: -1.8235e+00
Epoch 7/10
10/10 - 1s - loss: 465.7249 - loglik: -4.6415e+02 - logprior: -1.5702e+00
Epoch 8/10
10/10 - 1s - loss: 465.5060 - loglik: -4.6403e+02 - logprior: -1.4721e+00
Epoch 9/10
10/10 - 1s - loss: 464.7319 - loglik: -4.6335e+02 - logprior: -1.3838e+00
Epoch 10/10
10/10 - 1s - loss: 464.4965 - loglik: -4.6319e+02 - logprior: -1.3099e+00
Fitted a model with MAP estimate = -464.3814
expansions: [(0, 3), (10, 1), (24, 1), (35, 3), (36, 3), (50, 3), (58, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 495.4626 - loglik: -4.6102e+02 - logprior: -3.4447e+01
Epoch 2/2
10/10 - 1s - loss: 461.4372 - loglik: -4.5110e+02 - logprior: -1.0341e+01
Fitted a model with MAP estimate = -455.1388
expansions: [(61, 1), (62, 1)]
discards: [89]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 474.3061 - loglik: -4.4899e+02 - logprior: -2.5314e+01
Epoch 2/2
10/10 - 1s - loss: 452.4979 - loglik: -4.4614e+02 - logprior: -6.3588e+00
Fitted a model with MAP estimate = -449.0665
expansions: []
discards: [44]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 469.9621 - loglik: -4.4594e+02 - logprior: -2.4017e+01
Epoch 2/10
10/10 - 1s - loss: 451.4404 - loglik: -4.4547e+02 - logprior: -5.9670e+00
Epoch 3/10
10/10 - 1s - loss: 445.6528 - loglik: -4.4326e+02 - logprior: -2.3976e+00
Epoch 4/10
10/10 - 1s - loss: 445.3329 - loglik: -4.4429e+02 - logprior: -1.0452e+00
Epoch 5/10
10/10 - 1s - loss: 443.8932 - loglik: -4.4350e+02 - logprior: -3.9725e-01
Epoch 6/10
10/10 - 1s - loss: 443.4460 - loglik: -4.4340e+02 - logprior: -4.8857e-02
Epoch 7/10
10/10 - 1s - loss: 443.1766 - loglik: -4.4340e+02 - logprior: 0.2206
Epoch 8/10
10/10 - 1s - loss: 443.2787 - loglik: -4.4372e+02 - logprior: 0.4363
Fitted a model with MAP estimate = -442.9489
Time for alignment: 38.2400
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 594.6119 - loglik: -5.6754e+02 - logprior: -2.7070e+01
Epoch 2/10
10/10 - 1s - loss: 538.3150 - loglik: -5.3142e+02 - logprior: -6.8994e+00
Epoch 3/10
10/10 - 1s - loss: 500.0319 - loglik: -4.9637e+02 - logprior: -3.6597e+00
Epoch 4/10
10/10 - 1s - loss: 479.3393 - loglik: -4.7659e+02 - logprior: -2.7526e+00
Epoch 5/10
10/10 - 1s - loss: 472.3080 - loglik: -4.6994e+02 - logprior: -2.3705e+00
Epoch 6/10
10/10 - 1s - loss: 468.7125 - loglik: -4.6677e+02 - logprior: -1.9431e+00
Epoch 7/10
10/10 - 1s - loss: 466.7407 - loglik: -4.6505e+02 - logprior: -1.6908e+00
Epoch 8/10
10/10 - 1s - loss: 466.9684 - loglik: -4.6530e+02 - logprior: -1.6727e+00
Fitted a model with MAP estimate = -465.8947
expansions: [(0, 2), (5, 1), (10, 1), (21, 1), (35, 3), (36, 2), (48, 2), (49, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 494.9768 - loglik: -4.6100e+02 - logprior: -3.3982e+01
Epoch 2/2
10/10 - 1s - loss: 460.4122 - loglik: -4.5045e+02 - logprior: -9.9626e+00
Fitted a model with MAP estimate = -453.8517
expansions: [(59, 1)]
discards: [90]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 472.3830 - loglik: -4.4719e+02 - logprior: -2.5191e+01
Epoch 2/2
10/10 - 1s - loss: 452.0374 - loglik: -4.4572e+02 - logprior: -6.3211e+00
Fitted a model with MAP estimate = -448.7493
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 469.1960 - loglik: -4.4517e+02 - logprior: -2.4029e+01
Epoch 2/10
10/10 - 1s - loss: 449.9277 - loglik: -4.4395e+02 - logprior: -5.9804e+00
Epoch 3/10
10/10 - 1s - loss: 446.7433 - loglik: -4.4432e+02 - logprior: -2.4213e+00
Epoch 4/10
10/10 - 1s - loss: 445.0252 - loglik: -4.4395e+02 - logprior: -1.0756e+00
Epoch 5/10
10/10 - 1s - loss: 443.4321 - loglik: -4.4299e+02 - logprior: -4.4374e-01
Epoch 6/10
10/10 - 1s - loss: 443.2959 - loglik: -4.4320e+02 - logprior: -9.6984e-02
Epoch 7/10
10/10 - 1s - loss: 442.5354 - loglik: -4.4270e+02 - logprior: 0.1674
Epoch 8/10
10/10 - 1s - loss: 443.4672 - loglik: -4.4387e+02 - logprior: 0.3999
Fitted a model with MAP estimate = -442.6605
Time for alignment: 36.0114
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 594.4933 - loglik: -5.6742e+02 - logprior: -2.7070e+01
Epoch 2/10
10/10 - 1s - loss: 537.3389 - loglik: -5.3043e+02 - logprior: -6.9057e+00
Epoch 3/10
10/10 - 1s - loss: 500.4746 - loglik: -4.9679e+02 - logprior: -3.6877e+00
Epoch 4/10
10/10 - 1s - loss: 478.3599 - loglik: -4.7564e+02 - logprior: -2.7158e+00
Epoch 5/10
10/10 - 1s - loss: 470.3925 - loglik: -4.6812e+02 - logprior: -2.2728e+00
Epoch 6/10
10/10 - 1s - loss: 467.7333 - loglik: -4.6583e+02 - logprior: -1.9056e+00
Epoch 7/10
10/10 - 1s - loss: 467.2164 - loglik: -4.6557e+02 - logprior: -1.6428e+00
Epoch 8/10
10/10 - 1s - loss: 465.2977 - loglik: -4.6377e+02 - logprior: -1.5237e+00
Epoch 9/10
10/10 - 1s - loss: 466.8418 - loglik: -4.6539e+02 - logprior: -1.4493e+00
Fitted a model with MAP estimate = -465.5652
expansions: [(0, 2), (11, 1), (12, 1), (34, 1), (36, 4), (45, 1), (48, 2), (49, 2), (70, 1), (71, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 494.4491 - loglik: -4.6032e+02 - logprior: -3.4126e+01
Epoch 2/2
10/10 - 1s - loss: 460.8094 - loglik: -4.5081e+02 - logprior: -1.0001e+01
Fitted a model with MAP estimate = -453.9950
expansions: [(59, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.2941 - loglik: -4.4805e+02 - logprior: -2.5239e+01
Epoch 2/2
10/10 - 1s - loss: 450.4902 - loglik: -4.4410e+02 - logprior: -6.3891e+00
Fitted a model with MAP estimate = -448.3885
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 468.8550 - loglik: -4.4478e+02 - logprior: -2.4073e+01
Epoch 2/10
10/10 - 1s - loss: 450.1122 - loglik: -4.4407e+02 - logprior: -6.0445e+00
Epoch 3/10
10/10 - 1s - loss: 445.4080 - loglik: -4.4292e+02 - logprior: -2.4888e+00
Epoch 4/10
10/10 - 1s - loss: 444.6187 - loglik: -4.4349e+02 - logprior: -1.1332e+00
Epoch 5/10
10/10 - 1s - loss: 444.1720 - loglik: -4.4368e+02 - logprior: -4.9327e-01
Epoch 6/10
10/10 - 1s - loss: 442.6523 - loglik: -4.4250e+02 - logprior: -1.5626e-01
Epoch 7/10
10/10 - 1s - loss: 442.7773 - loglik: -4.4289e+02 - logprior: 0.1163
Fitted a model with MAP estimate = -442.4632
Time for alignment: 35.4115
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 594.5840 - loglik: -5.6751e+02 - logprior: -2.7070e+01
Epoch 2/10
10/10 - 1s - loss: 537.3659 - loglik: -5.3047e+02 - logprior: -6.8998e+00
Epoch 3/10
10/10 - 1s - loss: 499.8293 - loglik: -4.9616e+02 - logprior: -3.6661e+00
Epoch 4/10
10/10 - 1s - loss: 480.3480 - loglik: -4.7761e+02 - logprior: -2.7383e+00
Epoch 5/10
10/10 - 1s - loss: 472.4005 - loglik: -4.7010e+02 - logprior: -2.2954e+00
Epoch 6/10
10/10 - 1s - loss: 467.9449 - loglik: -4.6602e+02 - logprior: -1.9289e+00
Epoch 7/10
10/10 - 1s - loss: 467.1176 - loglik: -4.6540e+02 - logprior: -1.7196e+00
Epoch 8/10
10/10 - 1s - loss: 466.2762 - loglik: -4.6463e+02 - logprior: -1.6434e+00
Epoch 9/10
10/10 - 1s - loss: 466.2342 - loglik: -4.6467e+02 - logprior: -1.5605e+00
Epoch 10/10
10/10 - 1s - loss: 464.6598 - loglik: -4.6315e+02 - logprior: -1.5071e+00
Fitted a model with MAP estimate = -465.1965
expansions: [(0, 2), (11, 1), (12, 1), (21, 1), (35, 3), (36, 2), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 494.4331 - loglik: -4.6026e+02 - logprior: -3.4175e+01
Epoch 2/2
10/10 - 1s - loss: 460.6276 - loglik: -4.5055e+02 - logprior: -1.0078e+01
Fitted a model with MAP estimate = -454.2275
expansions: []
discards: [44 92]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 473.6406 - loglik: -4.4838e+02 - logprior: -2.5264e+01
Epoch 2/2
10/10 - 1s - loss: 452.1395 - loglik: -4.4581e+02 - logprior: -6.3260e+00
Fitted a model with MAP estimate = -448.8883
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 469.9970 - loglik: -4.4601e+02 - logprior: -2.3990e+01
Epoch 2/10
10/10 - 1s - loss: 449.9387 - loglik: -4.4400e+02 - logprior: -5.9352e+00
Epoch 3/10
10/10 - 1s - loss: 447.4725 - loglik: -4.4508e+02 - logprior: -2.3932e+00
Epoch 4/10
10/10 - 1s - loss: 444.3000 - loglik: -4.4326e+02 - logprior: -1.0434e+00
Epoch 5/10
10/10 - 1s - loss: 443.8544 - loglik: -4.4345e+02 - logprior: -4.0329e-01
Epoch 6/10
10/10 - 1s - loss: 443.1600 - loglik: -4.4309e+02 - logprior: -7.1652e-02
Epoch 7/10
10/10 - 1s - loss: 443.0494 - loglik: -4.4325e+02 - logprior: 0.2049
Epoch 8/10
10/10 - 1s - loss: 443.6140 - loglik: -4.4404e+02 - logprior: 0.4220
Fitted a model with MAP estimate = -442.7705
Time for alignment: 37.6844
Computed alignments with likelihoods: ['-443.2500', '-442.9489', '-442.6605', '-442.4632', '-442.7705']
Best model has likelihood: -442.4632  (prior= 0.2481 )
time for generating output: 0.1506
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7721518987341772
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 303.4068 - loglik: -2.9831e+02 - logprior: -5.0959e+00
Epoch 2/10
16/16 - 1s - loss: 280.3773 - loglik: -2.7875e+02 - logprior: -1.6230e+00
Epoch 3/10
16/16 - 1s - loss: 267.5927 - loglik: -2.6593e+02 - logprior: -1.6593e+00
Epoch 4/10
16/16 - 1s - loss: 264.0870 - loglik: -2.6241e+02 - logprior: -1.6730e+00
Epoch 5/10
16/16 - 1s - loss: 262.4357 - loglik: -2.6076e+02 - logprior: -1.6714e+00
Epoch 6/10
16/16 - 1s - loss: 261.4519 - loglik: -2.5979e+02 - logprior: -1.6617e+00
Epoch 7/10
16/16 - 1s - loss: 261.0426 - loglik: -2.5943e+02 - logprior: -1.6169e+00
Epoch 8/10
16/16 - 1s - loss: 261.2276 - loglik: -2.5963e+02 - logprior: -1.5939e+00
Fitted a model with MAP estimate = -260.9143
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 6), (24, 2), (33, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 266.3994 - loglik: -2.6007e+02 - logprior: -6.3272e+00
Epoch 2/2
16/16 - 1s - loss: 258.7223 - loglik: -2.5570e+02 - logprior: -3.0197e+00
Fitted a model with MAP estimate = -256.8947
expansions: [(0, 1)]
discards: [ 0 31 32 33]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 259.3798 - loglik: -2.5478e+02 - logprior: -4.6022e+00
Epoch 2/2
16/16 - 1s - loss: 255.5832 - loglik: -2.5397e+02 - logprior: -1.6156e+00
Fitted a model with MAP estimate = -254.9629
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 258.3019 - loglik: -2.5372e+02 - logprior: -4.5867e+00
Epoch 2/10
16/16 - 1s - loss: 255.2818 - loglik: -2.5372e+02 - logprior: -1.5604e+00
Epoch 3/10
16/16 - 1s - loss: 254.3967 - loglik: -2.5310e+02 - logprior: -1.2972e+00
Epoch 4/10
16/16 - 1s - loss: 253.6953 - loglik: -2.5247e+02 - logprior: -1.2301e+00
Epoch 5/10
16/16 - 1s - loss: 253.7997 - loglik: -2.5260e+02 - logprior: -1.2037e+00
Fitted a model with MAP estimate = -253.4573
Time for alignment: 43.0001
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 303.5227 - loglik: -2.9843e+02 - logprior: -5.0908e+00
Epoch 2/10
16/16 - 1s - loss: 278.0714 - loglik: -2.7646e+02 - logprior: -1.6133e+00
Epoch 3/10
16/16 - 1s - loss: 265.1558 - loglik: -2.6348e+02 - logprior: -1.6750e+00
Epoch 4/10
16/16 - 1s - loss: 262.8956 - loglik: -2.6123e+02 - logprior: -1.6652e+00
Epoch 5/10
16/16 - 1s - loss: 261.6793 - loglik: -2.6003e+02 - logprior: -1.6488e+00
Epoch 6/10
16/16 - 1s - loss: 260.9830 - loglik: -2.5934e+02 - logprior: -1.6436e+00
Epoch 7/10
16/16 - 1s - loss: 261.0460 - loglik: -2.5945e+02 - logprior: -1.5976e+00
Fitted a model with MAP estimate = -260.9786
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 267.1340 - loglik: -2.6080e+02 - logprior: -6.3340e+00
Epoch 2/2
16/16 - 1s - loss: 257.9351 - loglik: -2.5491e+02 - logprior: -3.0203e+00
Fitted a model with MAP estimate = -256.7533
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 259.6994 - loglik: -2.5508e+02 - logprior: -4.6153e+00
Epoch 2/2
16/16 - 1s - loss: 255.2305 - loglik: -2.5360e+02 - logprior: -1.6288e+00
Fitted a model with MAP estimate = -254.9350
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 261.7256 - loglik: -2.5547e+02 - logprior: -6.2528e+00
Epoch 2/10
16/16 - 1s - loss: 256.8900 - loglik: -2.5418e+02 - logprior: -2.7070e+00
Epoch 3/10
16/16 - 1s - loss: 255.2235 - loglik: -2.5373e+02 - logprior: -1.4906e+00
Epoch 4/10
16/16 - 1s - loss: 254.4842 - loglik: -2.5326e+02 - logprior: -1.2265e+00
Epoch 5/10
16/16 - 1s - loss: 254.4143 - loglik: -2.5321e+02 - logprior: -1.2040e+00
Epoch 6/10
16/16 - 1s - loss: 253.5353 - loglik: -2.5235e+02 - logprior: -1.1814e+00
Epoch 7/10
16/16 - 1s - loss: 253.7286 - loglik: -2.5256e+02 - logprior: -1.1653e+00
Fitted a model with MAP estimate = -253.5677
Time for alignment: 42.9801
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 303.6527 - loglik: -2.9856e+02 - logprior: -5.0903e+00
Epoch 2/10
16/16 - 1s - loss: 281.3974 - loglik: -2.7978e+02 - logprior: -1.6190e+00
Epoch 3/10
16/16 - 1s - loss: 268.2064 - loglik: -2.6658e+02 - logprior: -1.6279e+00
Epoch 4/10
16/16 - 1s - loss: 264.0223 - loglik: -2.6242e+02 - logprior: -1.6028e+00
Epoch 5/10
16/16 - 1s - loss: 262.4403 - loglik: -2.6080e+02 - logprior: -1.6390e+00
Epoch 6/10
16/16 - 1s - loss: 261.5052 - loglik: -2.5990e+02 - logprior: -1.6035e+00
Epoch 7/10
16/16 - 1s - loss: 261.9360 - loglik: -2.6033e+02 - logprior: -1.6037e+00
Fitted a model with MAP estimate = -261.1945
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 6), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 266.8572 - loglik: -2.6050e+02 - logprior: -6.3523e+00
Epoch 2/2
16/16 - 1s - loss: 258.3947 - loglik: -2.5535e+02 - logprior: -3.0479e+00
Fitted a model with MAP estimate = -256.7479
expansions: [(0, 1)]
discards: [ 0 31 32 33 38]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 259.4460 - loglik: -2.5483e+02 - logprior: -4.6147e+00
Epoch 2/2
16/16 - 1s - loss: 255.4255 - loglik: -2.5378e+02 - logprior: -1.6412e+00
Fitted a model with MAP estimate = -254.9458
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 261.2103 - loglik: -2.5494e+02 - logprior: -6.2697e+00
Epoch 2/10
16/16 - 1s - loss: 256.9807 - loglik: -2.5428e+02 - logprior: -2.7049e+00
Epoch 3/10
16/16 - 1s - loss: 255.3117 - loglik: -2.5382e+02 - logprior: -1.4899e+00
Epoch 4/10
16/16 - 1s - loss: 254.7674 - loglik: -2.5352e+02 - logprior: -1.2435e+00
Epoch 5/10
16/16 - 1s - loss: 254.2417 - loglik: -2.5302e+02 - logprior: -1.2182e+00
Epoch 6/10
16/16 - 1s - loss: 253.6729 - loglik: -2.5247e+02 - logprior: -1.2008e+00
Epoch 7/10
16/16 - 1s - loss: 253.0538 - loglik: -2.5187e+02 - logprior: -1.1806e+00
Epoch 8/10
16/16 - 1s - loss: 253.8424 - loglik: -2.5269e+02 - logprior: -1.1548e+00
Fitted a model with MAP estimate = -253.3919
Time for alignment: 45.1340
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 303.3474 - loglik: -2.9825e+02 - logprior: -5.0927e+00
Epoch 2/10
16/16 - 1s - loss: 281.1909 - loglik: -2.7957e+02 - logprior: -1.6202e+00
Epoch 3/10
16/16 - 1s - loss: 269.1766 - loglik: -2.6752e+02 - logprior: -1.6548e+00
Epoch 4/10
16/16 - 1s - loss: 263.5206 - loglik: -2.6187e+02 - logprior: -1.6542e+00
Epoch 5/10
16/16 - 1s - loss: 262.2727 - loglik: -2.6063e+02 - logprior: -1.6430e+00
Epoch 6/10
16/16 - 1s - loss: 261.5975 - loglik: -2.5997e+02 - logprior: -1.6304e+00
Epoch 7/10
16/16 - 1s - loss: 260.9223 - loglik: -2.5933e+02 - logprior: -1.5906e+00
Epoch 8/10
16/16 - 1s - loss: 261.4611 - loglik: -2.5989e+02 - logprior: -1.5750e+00
Fitted a model with MAP estimate = -261.0670
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 266.5803 - loglik: -2.6024e+02 - logprior: -6.3386e+00
Epoch 2/2
16/16 - 1s - loss: 257.9968 - loglik: -2.5495e+02 - logprior: -3.0433e+00
Fitted a model with MAP estimate = -256.7606
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 259.3597 - loglik: -2.5475e+02 - logprior: -4.6125e+00
Epoch 2/2
16/16 - 1s - loss: 255.3597 - loglik: -2.5372e+02 - logprior: -1.6363e+00
Fitted a model with MAP estimate = -254.9301
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 261.1847 - loglik: -2.5492e+02 - logprior: -6.2654e+00
Epoch 2/10
16/16 - 1s - loss: 257.1949 - loglik: -2.5448e+02 - logprior: -2.7148e+00
Epoch 3/10
16/16 - 1s - loss: 255.2956 - loglik: -2.5380e+02 - logprior: -1.4989e+00
Epoch 4/10
16/16 - 1s - loss: 254.7700 - loglik: -2.5352e+02 - logprior: -1.2497e+00
Epoch 5/10
16/16 - 1s - loss: 253.8457 - loglik: -2.5263e+02 - logprior: -1.2171e+00
Epoch 6/10
16/16 - 1s - loss: 253.8559 - loglik: -2.5266e+02 - logprior: -1.1991e+00
Fitted a model with MAP estimate = -253.6347
Time for alignment: 42.9392
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 303.5462 - loglik: -2.9845e+02 - logprior: -5.0952e+00
Epoch 2/10
16/16 - 1s - loss: 281.0697 - loglik: -2.7946e+02 - logprior: -1.6118e+00
Epoch 3/10
16/16 - 1s - loss: 268.5995 - loglik: -2.6694e+02 - logprior: -1.6588e+00
Epoch 4/10
16/16 - 1s - loss: 262.9498 - loglik: -2.6130e+02 - logprior: -1.6493e+00
Epoch 5/10
16/16 - 1s - loss: 261.9927 - loglik: -2.6034e+02 - logprior: -1.6486e+00
Epoch 6/10
16/16 - 1s - loss: 261.2372 - loglik: -2.5961e+02 - logprior: -1.6231e+00
Epoch 7/10
16/16 - 1s - loss: 261.1260 - loglik: -2.5952e+02 - logprior: -1.6077e+00
Epoch 8/10
16/16 - 1s - loss: 260.9689 - loglik: -2.5939e+02 - logprior: -1.5787e+00
Epoch 9/10
16/16 - 1s - loss: 260.8784 - loglik: -2.5930e+02 - logprior: -1.5753e+00
Epoch 10/10
16/16 - 1s - loss: 261.0996 - loglik: -2.5953e+02 - logprior: -1.5684e+00
Fitted a model with MAP estimate = -260.9124
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 266.6633 - loglik: -2.6032e+02 - logprior: -6.3407e+00
Epoch 2/2
16/16 - 1s - loss: 258.2537 - loglik: -2.5520e+02 - logprior: -3.0526e+00
Fitted a model with MAP estimate = -256.8465
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 259.3338 - loglik: -2.5472e+02 - logprior: -4.6094e+00
Epoch 2/2
16/16 - 1s - loss: 255.4664 - loglik: -2.5384e+02 - logprior: -1.6307e+00
Fitted a model with MAP estimate = -254.9440
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 261.5884 - loglik: -2.5533e+02 - logprior: -6.2594e+00
Epoch 2/10
16/16 - 1s - loss: 257.3806 - loglik: -2.5466e+02 - logprior: -2.7224e+00
Epoch 3/10
16/16 - 1s - loss: 254.5442 - loglik: -2.5305e+02 - logprior: -1.4982e+00
Epoch 4/10
16/16 - 1s - loss: 255.1230 - loglik: -2.5389e+02 - logprior: -1.2368e+00
Fitted a model with MAP estimate = -254.4080
Time for alignment: 42.6769
Computed alignments with likelihoods: ['-253.4573', '-253.5677', '-253.3919', '-253.6347', '-254.4080']
Best model has likelihood: -253.3919  (prior= -1.1461 )
time for generating output: 0.1108
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.8998384491114702
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1203.1014 - loglik: -1.2014e+03 - logprior: -1.6609e+00
Epoch 2/10
39/39 - 9s - loss: 1156.3795 - loglik: -1.1554e+03 - logprior: -9.3415e-01
Epoch 3/10
39/39 - 9s - loss: 1147.3490 - loglik: -1.1464e+03 - logprior: -9.4384e-01
Epoch 4/10
39/39 - 9s - loss: 1144.9828 - loglik: -1.1440e+03 - logprior: -9.3885e-01
Epoch 5/10
39/39 - 9s - loss: 1143.5579 - loglik: -1.1426e+03 - logprior: -9.6733e-01
Epoch 6/10
39/39 - 9s - loss: 1143.0271 - loglik: -1.1420e+03 - logprior: -9.8142e-01
Epoch 7/10
39/39 - 9s - loss: 1142.8127 - loglik: -1.1418e+03 - logprior: -9.9454e-01
Epoch 8/10
39/39 - 9s - loss: 1143.1073 - loglik: -1.1421e+03 - logprior: -9.9611e-01
Fitted a model with MAP estimate = -1050.2325
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (20, 2), (23, 6), (25, 1), (54, 1), (56, 2), (71, 2), (78, 3), (85, 1), (91, 1), (122, 1), (124, 2), (125, 3), (126, 2), (128, 1), (146, 3)]
discards: [  0  37 135 136 137 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1150.2297 - loglik: -1.1476e+03 - logprior: -2.6183e+00
Epoch 2/2
39/39 - 11s - loss: 1138.8385 - loglik: -1.1381e+03 - logprior: -6.9543e-01
Fitted a model with MAP estimate = -1038.3573
expansions: [(0, 2), (40, 1), (95, 1), (131, 2), (166, 1), (168, 10)]
discards: [  0  16  25  70 148]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1136.8210 - loglik: -1.1351e+03 - logprior: -1.7451e+00
Epoch 2/2
39/39 - 12s - loss: 1131.3915 - loglik: -1.1307e+03 - logprior: -6.8620e-01
Fitted a model with MAP estimate = -1033.4748
expansions: [(38, 1), (150, 1), (174, 3)]
discards: [  0 164 165 166 167 168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 1030.9646 - loglik: -1.0298e+03 - logprior: -1.1856e+00
Epoch 2/10
51/51 - 15s - loss: 1028.1337 - loglik: -1.0277e+03 - logprior: -3.8788e-01
Epoch 3/10
51/51 - 15s - loss: 1026.9370 - loglik: -1.0266e+03 - logprior: -3.4886e-01
Epoch 4/10
51/51 - 14s - loss: 1023.0970 - loglik: -1.0228e+03 - logprior: -3.1375e-01
Epoch 5/10
51/51 - 14s - loss: 1022.4658 - loglik: -1.0221e+03 - logprior: -3.5358e-01
Epoch 6/10
51/51 - 14s - loss: 1023.8252 - loglik: -1.0235e+03 - logprior: -3.6636e-01
Fitted a model with MAP estimate = -1021.8837
Time for alignment: 281.9956
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1201.2438 - loglik: -1.1996e+03 - logprior: -1.6556e+00
Epoch 2/10
39/39 - 9s - loss: 1151.1007 - loglik: -1.1502e+03 - logprior: -9.0095e-01
Epoch 3/10
39/39 - 9s - loss: 1144.8683 - loglik: -1.1439e+03 - logprior: -9.6074e-01
Epoch 4/10
39/39 - 9s - loss: 1142.7062 - loglik: -1.1418e+03 - logprior: -9.5132e-01
Epoch 5/10
39/39 - 9s - loss: 1141.7775 - loglik: -1.1408e+03 - logprior: -9.4000e-01
Epoch 6/10
39/39 - 9s - loss: 1141.3612 - loglik: -1.1404e+03 - logprior: -9.2290e-01
Epoch 7/10
39/39 - 9s - loss: 1140.9874 - loglik: -1.1401e+03 - logprior: -9.2480e-01
Epoch 8/10
39/39 - 9s - loss: 1140.8033 - loglik: -1.1399e+03 - logprior: -9.1845e-01
Epoch 9/10
39/39 - 9s - loss: 1140.9281 - loglik: -1.1400e+03 - logprior: -9.1691e-01
Fitted a model with MAP estimate = -1049.6280
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (21, 2), (24, 3), (25, 1), (53, 1), (54, 2), (55, 1), (71, 2), (76, 4), (84, 1), (121, 3), (124, 3), (125, 1), (141, 5), (146, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1145.4303 - loglik: -1.1428e+03 - logprior: -2.6766e+00
Epoch 2/2
39/39 - 11s - loss: 1135.8833 - loglik: -1.1351e+03 - logprior: -8.1459e-01
Fitted a model with MAP estimate = -1036.8388
expansions: [(0, 2), (93, 3), (129, 1), (146, 2), (172, 2), (173, 1)]
discards: [  0  15  67  86  89 158 159 160 161 162 163 164 165]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1140.9518 - loglik: -1.1392e+03 - logprior: -1.7634e+00
Epoch 2/2
39/39 - 11s - loss: 1135.5482 - loglik: -1.1349e+03 - logprior: -6.8674e-01
Fitted a model with MAP estimate = -1036.6783
expansions: [(26, 1), (36, 1), (37, 1), (87, 1), (146, 3), (147, 1), (148, 2), (168, 1)]
discards: [  0  90 152 153 154 155 160 161 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 1033.5164 - loglik: -1.0323e+03 - logprior: -1.2160e+00
Epoch 2/10
51/51 - 14s - loss: 1028.4773 - loglik: -1.0281e+03 - logprior: -4.1164e-01
Epoch 3/10
51/51 - 14s - loss: 1027.0145 - loglik: -1.0266e+03 - logprior: -3.8376e-01
Epoch 4/10
51/51 - 14s - loss: 1025.2261 - loglik: -1.0248e+03 - logprior: -3.8216e-01
Epoch 5/10
51/51 - 14s - loss: 1024.5325 - loglik: -1.0242e+03 - logprior: -3.8102e-01
Epoch 6/10
51/51 - 14s - loss: 1023.1030 - loglik: -1.0227e+03 - logprior: -3.6735e-01
Epoch 7/10
51/51 - 14s - loss: 1023.3850 - loglik: -1.0231e+03 - logprior: -3.2664e-01
Fitted a model with MAP estimate = -1023.0352
Time for alignment: 301.5745
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1203.1394 - loglik: -1.2015e+03 - logprior: -1.6453e+00
Epoch 2/10
39/39 - 9s - loss: 1155.5232 - loglik: -1.1546e+03 - logprior: -9.0178e-01
Epoch 3/10
39/39 - 9s - loss: 1145.0771 - loglik: -1.1441e+03 - logprior: -1.0102e+00
Epoch 4/10
39/39 - 9s - loss: 1142.2279 - loglik: -1.1412e+03 - logprior: -9.9883e-01
Epoch 5/10
39/39 - 9s - loss: 1140.9731 - loglik: -1.1400e+03 - logprior: -9.9847e-01
Epoch 6/10
39/39 - 9s - loss: 1140.5123 - loglik: -1.1395e+03 - logprior: -1.0029e+00
Epoch 7/10
39/39 - 9s - loss: 1140.2069 - loglik: -1.1392e+03 - logprior: -1.0055e+00
Epoch 8/10
39/39 - 9s - loss: 1140.2614 - loglik: -1.1393e+03 - logprior: -9.9931e-01
Fitted a model with MAP estimate = -1048.6522
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (21, 2), (24, 6), (25, 1), (55, 2), (73, 2), (77, 5), (79, 2), (82, 2), (83, 1), (86, 1), (99, 1), (119, 5), (124, 1), (125, 2), (126, 1), (127, 3), (128, 2), (146, 3)]
discards: [  0  66 131 132 133 134 135 136 137 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 191 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1148.2246 - loglik: -1.1456e+03 - logprior: -2.6577e+00
Epoch 2/2
39/39 - 11s - loss: 1136.3569 - loglik: -1.1356e+03 - logprior: -8.0667e-01
Fitted a model with MAP estimate = -1037.5005
expansions: [(0, 2), (39, 1), (40, 1), (165, 1)]
discards: [  0  15  25  70 100 105 149 150 151 160 168 169 170 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1140.3295 - loglik: -1.1387e+03 - logprior: -1.6709e+00
Epoch 2/2
39/39 - 11s - loss: 1136.3351 - loglik: -1.1359e+03 - logprior: -4.7026e-01
Fitted a model with MAP estimate = -1036.7651
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 16s - loss: 1032.2383 - loglik: -1.0312e+03 - logprior: -1.0409e+00
Epoch 2/10
51/51 - 13s - loss: 1029.9009 - loglik: -1.0296e+03 - logprior: -3.0046e-01
Epoch 3/10
51/51 - 13s - loss: 1030.6152 - loglik: -1.0303e+03 - logprior: -3.0656e-01
Fitted a model with MAP estimate = -1026.6927
Time for alignment: 231.7589
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1203.2744 - loglik: -1.2016e+03 - logprior: -1.6453e+00
Epoch 2/10
39/39 - 9s - loss: 1156.2966 - loglik: -1.1554e+03 - logprior: -9.4304e-01
Epoch 3/10
39/39 - 9s - loss: 1148.8510 - loglik: -1.1478e+03 - logprior: -1.0135e+00
Epoch 4/10
39/39 - 9s - loss: 1146.4010 - loglik: -1.1453e+03 - logprior: -1.0513e+00
Epoch 5/10
39/39 - 9s - loss: 1144.5812 - loglik: -1.1435e+03 - logprior: -1.0422e+00
Epoch 6/10
39/39 - 9s - loss: 1143.8118 - loglik: -1.1428e+03 - logprior: -1.0529e+00
Epoch 7/10
39/39 - 9s - loss: 1143.3458 - loglik: -1.1423e+03 - logprior: -1.0582e+00
Epoch 8/10
39/39 - 9s - loss: 1143.3373 - loglik: -1.1423e+03 - logprior: -1.0626e+00
Epoch 9/10
39/39 - 9s - loss: 1142.9504 - loglik: -1.1419e+03 - logprior: -1.0641e+00
Epoch 10/10
39/39 - 9s - loss: 1143.1738 - loglik: -1.1421e+03 - logprior: -1.0573e+00
Fitted a model with MAP estimate = -1051.0521
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 3), (24, 1), (25, 1), (26, 1), (27, 2), (55, 2), (71, 2), (78, 5), (81, 1), (114, 1), (124, 2), (125, 2), (126, 3), (127, 2), (128, 1), (140, 8)]
discards: [ 0 92 93 94]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1148.6361 - loglik: -1.1459e+03 - logprior: -2.6932e+00
Epoch 2/2
39/39 - 12s - loss: 1135.6954 - loglik: -1.1349e+03 - logprior: -7.9838e-01
Fitted a model with MAP estimate = -1036.7035
expansions: [(0, 2), (26, 1), (131, 2)]
discards: [  0  13  37  69 163 164 165 166 167 168 169]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1139.6799 - loglik: -1.1380e+03 - logprior: -1.7288e+00
Epoch 2/2
39/39 - 11s - loss: 1135.6973 - loglik: -1.1351e+03 - logprior: -6.4088e-01
Fitted a model with MAP estimate = -1036.4301
expansions: [(163, 1), (164, 1), (166, 4)]
discards: [  0 159 160 161]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 1032.5647 - loglik: -1.0314e+03 - logprior: -1.1856e+00
Epoch 2/10
51/51 - 14s - loss: 1026.5479 - loglik: -1.0262e+03 - logprior: -3.9517e-01
Epoch 3/10
51/51 - 14s - loss: 1028.3962 - loglik: -1.0280e+03 - logprior: -3.7701e-01
Fitted a model with MAP estimate = -1024.9323
Time for alignment: 254.7296
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1202.9131 - loglik: -1.2013e+03 - logprior: -1.6379e+00
Epoch 2/10
39/39 - 9s - loss: 1155.1415 - loglik: -1.1543e+03 - logprior: -8.2615e-01
Epoch 3/10
39/39 - 9s - loss: 1146.5996 - loglik: -1.1457e+03 - logprior: -8.9211e-01
Epoch 4/10
39/39 - 9s - loss: 1144.4203 - loglik: -1.1435e+03 - logprior: -8.9740e-01
Epoch 5/10
39/39 - 9s - loss: 1142.4817 - loglik: -1.1416e+03 - logprior: -9.1750e-01
Epoch 6/10
39/39 - 9s - loss: 1142.4877 - loglik: -1.1416e+03 - logprior: -9.1554e-01
Fitted a model with MAP estimate = -1048.4260
expansions: [(4, 1), (5, 1), (6, 1), (10, 1), (20, 3), (24, 1), (25, 1), (26, 1), (30, 1), (56, 1), (67, 1), (70, 1), (78, 5), (87, 1), (92, 2), (111, 1), (123, 2), (125, 3), (127, 2), (128, 2), (129, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 1146.4427 - loglik: -1.1439e+03 - logprior: -2.5753e+00
Epoch 2/2
39/39 - 11s - loss: 1137.9208 - loglik: -1.1373e+03 - logprior: -5.9921e-01
Fitted a model with MAP estimate = -1038.9296
expansions: [(0, 2), (25, 1), (175, 7), (179, 3)]
discards: [  0  93  94 165 166 167 168 169]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1139.0769 - loglik: -1.1374e+03 - logprior: -1.6894e+00
Epoch 2/2
39/39 - 11s - loss: 1134.6185 - loglik: -1.1340e+03 - logprior: -6.1574e-01
Fitted a model with MAP estimate = -1035.3175
expansions: [(151, 1), (175, 5)]
discards: [158 159 160 161 165 166 167 168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 188 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 1032.9470 - loglik: -1.0319e+03 - logprior: -1.0375e+00
Epoch 2/10
51/51 - 14s - loss: 1028.0378 - loglik: -1.0274e+03 - logprior: -5.9523e-01
Epoch 3/10
51/51 - 14s - loss: 1026.0325 - loglik: -1.0255e+03 - logprior: -5.7439e-01
Epoch 4/10
51/51 - 14s - loss: 1026.1564 - loglik: -1.0256e+03 - logprior: -5.3791e-01
Fitted a model with MAP estimate = -1024.5471
Time for alignment: 233.6468
Computed alignments with likelihoods: ['-1021.8837', '-1023.0352', '-1026.6927', '-1024.9323', '-1024.5471']
Best model has likelihood: -1021.8837  (prior= -0.3029 )
time for generating output: 0.2319
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6375488917861799
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f3ca483acd0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f3ca483af40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f5e0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54fbb0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f9d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc10>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f3b7c54f8e0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f790>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f400>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f160>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f040>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f1c0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f490>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f580>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd30>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54ffa0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f3b7c54ff70> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f3b7c54fd00>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fc40> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f3b9003f310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54f760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54fd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f3b7c54faf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f3caf63e790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f3ca4004310> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f3b7c54fee0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 992.7007 - loglik: -9.8676e+02 - logprior: -5.9405e+00
Epoch 2/10
14/14 - 3s - loss: 938.4601 - loglik: -9.3721e+02 - logprior: -1.2470e+00
Epoch 3/10
14/14 - 3s - loss: 901.0389 - loglik: -8.9988e+02 - logprior: -1.1556e+00
Epoch 4/10
14/14 - 3s - loss: 891.6134 - loglik: -8.9062e+02 - logprior: -9.9152e-01
Epoch 5/10
14/14 - 3s - loss: 887.1318 - loglik: -8.8618e+02 - logprior: -9.4708e-01
Epoch 6/10
14/14 - 3s - loss: 883.8214 - loglik: -8.8290e+02 - logprior: -9.1665e-01
Epoch 7/10
14/14 - 3s - loss: 885.7758 - loglik: -8.8486e+02 - logprior: -9.2018e-01
Fitted a model with MAP estimate = -883.2767
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 3), (75, 1), (89, 1), (102, 4), (117, 1), (120, 2), (128, 1), (131, 2), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 882.9559 - loglik: -8.7869e+02 - logprior: -4.2697e+00
Epoch 2/2
29/29 - 5s - loss: 869.7423 - loglik: -8.6872e+02 - logprior: -1.0266e+00
Fitted a model with MAP estimate = -867.2399
expansions: [(83, 1), (84, 1), (125, 1), (126, 1), (128, 1)]
discards: [  0  41 146 164]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 875.4128 - loglik: -8.7074e+02 - logprior: -4.6746e+00
Epoch 2/2
29/29 - 5s - loss: 865.7350 - loglik: -8.6504e+02 - logprior: -6.9886e-01
Fitted a model with MAP estimate = -864.8304
expansions: []
discards: [  0 112]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 873.0645 - loglik: -8.6930e+02 - logprior: -3.7627e+00
Epoch 2/10
29/29 - 5s - loss: 866.4803 - loglik: -8.6633e+02 - logprior: -1.5219e-01
Epoch 3/10
29/29 - 5s - loss: 865.1855 - loglik: -8.6524e+02 - logprior: 0.0546
Epoch 4/10
29/29 - 5s - loss: 862.7430 - loglik: -8.6285e+02 - logprior: 0.1036
Epoch 5/10
29/29 - 5s - loss: 861.6858 - loglik: -8.6186e+02 - logprior: 0.1773
Epoch 6/10
29/29 - 5s - loss: 861.1388 - loglik: -8.6140e+02 - logprior: 0.2649
Epoch 7/10
29/29 - 5s - loss: 861.2732 - loglik: -8.6162e+02 - logprior: 0.3469
Fitted a model with MAP estimate = -860.4459
Time for alignment: 121.2425
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 991.8746 - loglik: -9.8594e+02 - logprior: -5.9350e+00
Epoch 2/10
14/14 - 3s - loss: 939.0978 - loglik: -9.3785e+02 - logprior: -1.2521e+00
Epoch 3/10
14/14 - 3s - loss: 903.8578 - loglik: -9.0264e+02 - logprior: -1.2147e+00
Epoch 4/10
14/14 - 3s - loss: 890.1937 - loglik: -8.8908e+02 - logprior: -1.1093e+00
Epoch 5/10
14/14 - 3s - loss: 885.6674 - loglik: -8.8454e+02 - logprior: -1.1309e+00
Epoch 6/10
14/14 - 3s - loss: 883.2344 - loglik: -8.8213e+02 - logprior: -1.1020e+00
Epoch 7/10
14/14 - 3s - loss: 884.7553 - loglik: -8.8368e+02 - logprior: -1.0758e+00
Fitted a model with MAP estimate = -882.8358
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (47, 1), (50, 1), (52, 1), (53, 1), (58, 1), (61, 1), (69, 1), (71, 1), (74, 1), (76, 1), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 882.4738 - loglik: -8.7827e+02 - logprior: -4.2033e+00
Epoch 2/2
29/29 - 5s - loss: 869.3622 - loglik: -8.6846e+02 - logprior: -8.9875e-01
Fitted a model with MAP estimate = -867.7685
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [ 41 146 163]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 872.4764 - loglik: -8.6943e+02 - logprior: -3.0496e+00
Epoch 2/2
29/29 - 5s - loss: 870.3687 - loglik: -8.6981e+02 - logprior: -5.5776e-01
Fitted a model with MAP estimate = -867.1517
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 871.7914 - loglik: -8.6887e+02 - logprior: -2.9230e+00
Epoch 2/10
29/29 - 5s - loss: 868.4578 - loglik: -8.6807e+02 - logprior: -3.8615e-01
Epoch 3/10
29/29 - 5s - loss: 867.0197 - loglik: -8.6682e+02 - logprior: -2.0452e-01
Epoch 4/10
29/29 - 5s - loss: 864.5035 - loglik: -8.6437e+02 - logprior: -1.3024e-01
Epoch 5/10
29/29 - 5s - loss: 863.1620 - loglik: -8.6312e+02 - logprior: -4.6219e-02
Epoch 6/10
29/29 - 5s - loss: 863.5535 - loglik: -8.6360e+02 - logprior: 0.0448
Fitted a model with MAP estimate = -862.5912
Time for alignment: 116.1661
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 992.1913 - loglik: -9.8627e+02 - logprior: -5.9212e+00
Epoch 2/10
14/14 - 3s - loss: 939.4615 - loglik: -9.3822e+02 - logprior: -1.2377e+00
Epoch 3/10
14/14 - 3s - loss: 902.5312 - loglik: -9.0137e+02 - logprior: -1.1625e+00
Epoch 4/10
14/14 - 3s - loss: 891.5631 - loglik: -8.9051e+02 - logprior: -1.0535e+00
Epoch 5/10
14/14 - 3s - loss: 887.1949 - loglik: -8.8612e+02 - logprior: -1.0717e+00
Epoch 6/10
14/14 - 3s - loss: 884.4445 - loglik: -8.8339e+02 - logprior: -1.0589e+00
Epoch 7/10
14/14 - 3s - loss: 883.1790 - loglik: -8.8213e+02 - logprior: -1.0536e+00
Epoch 8/10
14/14 - 3s - loss: 883.3849 - loglik: -8.8234e+02 - logprior: -1.0421e+00
Fitted a model with MAP estimate = -882.9138
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (51, 1), (53, 1), (54, 1), (57, 1), (58, 1), (61, 1), (69, 1), (70, 2), (71, 1), (82, 1), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 882.4874 - loglik: -8.7821e+02 - logprior: -4.2789e+00
Epoch 2/2
29/29 - 5s - loss: 869.2513 - loglik: -8.6827e+02 - logprior: -9.7822e-01
Fitted a model with MAP estimate = -867.3573
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41 147 164]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 873.1531 - loglik: -8.7005e+02 - logprior: -3.1053e+00
Epoch 2/2
29/29 - 5s - loss: 868.0484 - loglik: -8.6744e+02 - logprior: -6.0803e-01
Fitted a model with MAP estimate = -866.5686
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 872.1022 - loglik: -8.6913e+02 - logprior: -2.9728e+00
Epoch 2/10
29/29 - 5s - loss: 866.7540 - loglik: -8.6632e+02 - logprior: -4.3003e-01
Epoch 3/10
29/29 - 5s - loss: 866.4844 - loglik: -8.6621e+02 - logprior: -2.7159e-01
Epoch 4/10
29/29 - 5s - loss: 864.6809 - loglik: -8.6450e+02 - logprior: -1.7800e-01
Epoch 5/10
29/29 - 5s - loss: 862.4178 - loglik: -8.6232e+02 - logprior: -9.3692e-02
Epoch 6/10
29/29 - 5s - loss: 863.4650 - loglik: -8.6345e+02 - logprior: -1.5089e-02
Fitted a model with MAP estimate = -862.0446
Time for alignment: 119.2836
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 993.1948 - loglik: -9.8727e+02 - logprior: -5.9296e+00
Epoch 2/10
14/14 - 3s - loss: 938.6840 - loglik: -9.3745e+02 - logprior: -1.2374e+00
Epoch 3/10
14/14 - 3s - loss: 905.1168 - loglik: -9.0388e+02 - logprior: -1.2411e+00
Epoch 4/10
14/14 - 3s - loss: 894.4225 - loglik: -8.9321e+02 - logprior: -1.2154e+00
Epoch 5/10
14/14 - 3s - loss: 886.8190 - loglik: -8.8560e+02 - logprior: -1.2157e+00
Epoch 6/10
14/14 - 3s - loss: 888.2702 - loglik: -8.8706e+02 - logprior: -1.2089e+00
Fitted a model with MAP estimate = -885.5108
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (70, 2), (71, 2), (83, 2), (102, 2), (103, 4), (117, 1), (124, 1), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 882.5753 - loglik: -8.7836e+02 - logprior: -4.2178e+00
Epoch 2/2
29/29 - 5s - loss: 869.7186 - loglik: -8.6887e+02 - logprior: -8.5096e-01
Fitted a model with MAP estimate = -867.3034
expansions: [(129, 1), (130, 1)]
discards: [ 41  91 127 167]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 872.3543 - loglik: -8.6936e+02 - logprior: -2.9931e+00
Epoch 2/2
29/29 - 5s - loss: 868.4032 - loglik: -8.6783e+02 - logprior: -5.6953e-01
Fitted a model with MAP estimate = -866.4130
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 870.9338 - loglik: -8.6799e+02 - logprior: -2.9465e+00
Epoch 2/10
29/29 - 5s - loss: 868.4056 - loglik: -8.6799e+02 - logprior: -4.2012e-01
Epoch 3/10
29/29 - 5s - loss: 865.3287 - loglik: -8.6507e+02 - logprior: -2.5912e-01
Epoch 4/10
29/29 - 5s - loss: 863.4104 - loglik: -8.6322e+02 - logprior: -1.9064e-01
Epoch 5/10
29/29 - 5s - loss: 863.5134 - loglik: -8.6340e+02 - logprior: -1.1003e-01
Fitted a model with MAP estimate = -861.9622
Time for alignment: 107.9484
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 992.1563 - loglik: -9.8623e+02 - logprior: -5.9262e+00
Epoch 2/10
14/14 - 3s - loss: 938.1052 - loglik: -9.3687e+02 - logprior: -1.2388e+00
Epoch 3/10
14/14 - 3s - loss: 906.5082 - loglik: -9.0534e+02 - logprior: -1.1657e+00
Epoch 4/10
14/14 - 3s - loss: 893.6904 - loglik: -8.9261e+02 - logprior: -1.0840e+00
Epoch 5/10
14/14 - 4s - loss: 888.6606 - loglik: -8.8757e+02 - logprior: -1.0937e+00
Epoch 6/10
14/14 - 4s - loss: 884.9663 - loglik: -8.8389e+02 - logprior: -1.0771e+00
Epoch 7/10
14/14 - 3s - loss: 885.6198 - loglik: -8.8453e+02 - logprior: -1.0866e+00
Fitted a model with MAP estimate = -884.2264
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (30, 1), (32, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (71, 2), (83, 2), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (136, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 884.4559 - loglik: -8.8023e+02 - logprior: -4.2225e+00
Epoch 2/2
29/29 - 5s - loss: 869.7292 - loglik: -8.6881e+02 - logprior: -9.2428e-01
Fitted a model with MAP estimate = -868.1192
expansions: [(125, 1), (126, 1), (128, 1), (164, 1)]
discards: [  0 146]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 874.8975 - loglik: -8.7021e+02 - logprior: -4.6896e+00
Epoch 2/2
29/29 - 5s - loss: 868.8524 - loglik: -8.6811e+02 - logprior: -7.3912e-01
Fitted a model with MAP estimate = -865.8523
expansions: [(88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 872.7727 - loglik: -8.6881e+02 - logprior: -3.9630e+00
Epoch 2/10
29/29 - 5s - loss: 867.1381 - loglik: -8.6694e+02 - logprior: -1.9664e-01
Epoch 3/10
29/29 - 5s - loss: 865.1580 - loglik: -8.6518e+02 - logprior: 0.0190
Epoch 4/10
29/29 - 5s - loss: 863.1221 - loglik: -8.6319e+02 - logprior: 0.0685
Epoch 5/10
29/29 - 5s - loss: 861.2864 - loglik: -8.6143e+02 - logprior: 0.1480
Epoch 6/10
29/29 - 5s - loss: 862.7527 - loglik: -8.6299e+02 - logprior: 0.2354
Fitted a model with MAP estimate = -860.8707
Time for alignment: 115.7922
Computed alignments with likelihoods: ['-860.4459', '-862.5912', '-862.0446', '-861.9622', '-860.8707']
Best model has likelihood: -860.4459  (prior= 0.4067 )
time for generating output: 0.2310
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.7980721606278813
