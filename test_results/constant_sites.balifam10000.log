Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52b08fc490>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50384f9070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f503850a220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f503850a2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 772.2610 - loglik: -7.7003e+02 - logprior: -2.2265e+00
Epoch 2/10
39/39 - 33s - loss: 645.8531 - loglik: -6.4385e+02 - logprior: -2.0063e+00
Epoch 3/10
39/39 - 33s - loss: 635.7776 - loglik: -6.3378e+02 - logprior: -1.9983e+00
Epoch 4/10
39/39 - 33s - loss: 633.3712 - loglik: -6.3134e+02 - logprior: -2.0339e+00
Epoch 5/10
39/39 - 34s - loss: 631.8991 - loglik: -6.2971e+02 - logprior: -2.1872e+00
Epoch 6/10
39/39 - 34s - loss: 630.2159 - loglik: -6.2781e+02 - logprior: -2.4020e+00
Epoch 7/10
39/39 - 33s - loss: 630.3268 - loglik: -6.2771e+02 - logprior: -2.6130e+00
Fitted a model with MAP estimate = -629.4609
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (34, 1), (50, 1), (51, 3), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (81, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (94, 1), (99, 1), (100, 1), (113, 1), (118, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (161, 1), (163, 1), (165, 2), (166, 1), (172, 1), (173, 1), (184, 1), (188, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (214, 1), (217, 1), (218, 1), (224, 1), (229, 1), (235, 1), (236, 2), (247, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 621.5229 - loglik: -6.1963e+02 - logprior: -1.8943e+00
Epoch 2/2
39/39 - 48s - loss: 609.0851 - loglik: -6.0859e+02 - logprior: -4.9912e-01
Fitted a model with MAP estimate = -606.2190
expansions: [(143, 1), (203, 1)]
discards: [ 56 249 335 336]
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 611.6180 - loglik: -6.1041e+02 - logprior: -1.2044e+00
Epoch 2/2
39/39 - 48s - loss: 607.8998 - loglik: -6.0777e+02 - logprior: -1.3219e-01
Fitted a model with MAP estimate = -605.2830
expansions: [(56, 2)]
discards: [144 145]
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 611.6297 - loglik: -6.1063e+02 - logprior: -9.9564e-01
Epoch 2/10
39/39 - 48s - loss: 607.7858 - loglik: -6.0780e+02 - logprior: 0.0110
Epoch 3/10
39/39 - 48s - loss: 605.5381 - loglik: -6.0565e+02 - logprior: 0.1159
Epoch 4/10
39/39 - 47s - loss: 603.0850 - loglik: -6.0328e+02 - logprior: 0.1920
Epoch 5/10
39/39 - 48s - loss: 600.8573 - loglik: -6.0100e+02 - logprior: 0.1463
Epoch 6/10
39/39 - 48s - loss: 600.3029 - loglik: -6.0034e+02 - logprior: 0.0336
Epoch 7/10
39/39 - 48s - loss: 599.4067 - loglik: -5.9936e+02 - logprior: -4.3748e-02
Epoch 8/10
39/39 - 48s - loss: 598.7884 - loglik: -5.9867e+02 - logprior: -1.1941e-01
Epoch 9/10
39/39 - 47s - loss: 598.7156 - loglik: -5.9848e+02 - logprior: -2.3595e-01
Epoch 10/10
39/39 - 48s - loss: 598.1304 - loglik: -5.9783e+02 - logprior: -2.9655e-01
Fitted a model with MAP estimate = -597.4604
Time for alignment: 1112.3053
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 771.2033 - loglik: -7.6900e+02 - logprior: -2.2079e+00
Epoch 2/10
39/39 - 34s - loss: 646.1297 - loglik: -6.4414e+02 - logprior: -1.9891e+00
Epoch 3/10
39/39 - 33s - loss: 635.7379 - loglik: -6.3371e+02 - logprior: -2.0283e+00
Epoch 4/10
39/39 - 33s - loss: 633.5612 - loglik: -6.3151e+02 - logprior: -2.0472e+00
Epoch 5/10
39/39 - 34s - loss: 631.8270 - loglik: -6.2963e+02 - logprior: -2.1982e+00
Epoch 6/10
39/39 - 34s - loss: 630.3866 - loglik: -6.2797e+02 - logprior: -2.4167e+00
Epoch 7/10
39/39 - 33s - loss: 630.3747 - loglik: -6.2772e+02 - logprior: -2.6562e+00
Epoch 8/10
39/39 - 33s - loss: 630.0043 - loglik: -6.2699e+02 - logprior: -3.0173e+00
Epoch 9/10
39/39 - 34s - loss: 629.8023 - loglik: -6.2673e+02 - logprior: -3.0712e+00
Epoch 10/10
39/39 - 33s - loss: 629.4156 - loglik: -6.2618e+02 - logprior: -3.2345e+00
Fitted a model with MAP estimate = -628.9499
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (50, 1), (51, 3), (53, 2), (54, 1), (60, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (99, 1), (100, 1), (101, 1), (106, 1), (107, 1), (118, 1), (120, 1), (122, 1), (133, 1), (135, 1), (140, 1), (143, 1), (158, 1), (159, 1), (162, 1), (164, 1), (165, 1), (167, 1), (168, 1), (174, 1), (185, 1), (186, 1), (190, 1), (193, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (201, 1), (202, 1), (214, 1), (215, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 2), (268, 2), (269, 1), (278, 1), (279, 2), (280, 3)]
discards: [1]
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 620.6263 - loglik: -6.1878e+02 - logprior: -1.8425e+00
Epoch 2/2
39/39 - 49s - loss: 608.1323 - loglik: -6.0773e+02 - logprior: -3.9909e-01
Fitted a model with MAP estimate = -605.3899
expansions: []
discards: [ 58  61 250 338]
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 611.6927 - loglik: -6.1052e+02 - logprior: -1.1698e+00
Epoch 2/2
39/39 - 48s - loss: 607.6637 - loglik: -6.0756e+02 - logprior: -1.0572e-01
Fitted a model with MAP estimate = -605.5487
expansions: []
discards: []
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 610.6739 - loglik: -6.0971e+02 - logprior: -9.6435e-01
Epoch 2/10
39/39 - 48s - loss: 607.4081 - loglik: -6.0755e+02 - logprior: 0.1448
Epoch 3/10
39/39 - 48s - loss: 605.3973 - loglik: -6.0557e+02 - logprior: 0.1733
Epoch 4/10
39/39 - 48s - loss: 602.2167 - loglik: -6.0244e+02 - logprior: 0.2283
Epoch 5/10
39/39 - 47s - loss: 600.7708 - loglik: -6.0101e+02 - logprior: 0.2374
Epoch 6/10
39/39 - 49s - loss: 599.4478 - loglik: -5.9963e+02 - logprior: 0.1800
Epoch 7/10
39/39 - 48s - loss: 599.1188 - loglik: -5.9914e+02 - logprior: 0.0166
Epoch 8/10
39/39 - 48s - loss: 598.5496 - loglik: -5.9850e+02 - logprior: -4.7115e-02
Epoch 9/10
39/39 - 48s - loss: 598.1851 - loglik: -5.9805e+02 - logprior: -1.3952e-01
Epoch 10/10
39/39 - 48s - loss: 597.2240 - loglik: -5.9705e+02 - logprior: -1.7203e-01
Fitted a model with MAP estimate = -596.8090
Time for alignment: 1213.7171
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 770.9392 - loglik: -7.6878e+02 - logprior: -2.1570e+00
Epoch 2/10
39/39 - 33s - loss: 645.3662 - loglik: -6.4361e+02 - logprior: -1.7603e+00
Epoch 3/10
39/39 - 34s - loss: 635.0857 - loglik: -6.3334e+02 - logprior: -1.7501e+00
Epoch 4/10
39/39 - 34s - loss: 632.5524 - loglik: -6.3076e+02 - logprior: -1.7883e+00
Epoch 5/10
39/39 - 34s - loss: 631.1762 - loglik: -6.2925e+02 - logprior: -1.9214e+00
Epoch 6/10
39/39 - 34s - loss: 629.7945 - loglik: -6.2766e+02 - logprior: -2.1349e+00
Epoch 7/10
39/39 - 33s - loss: 629.1296 - loglik: -6.2681e+02 - logprior: -2.3227e+00
Epoch 8/10
39/39 - 34s - loss: 628.8843 - loglik: -6.2637e+02 - logprior: -2.5131e+00
Epoch 9/10
39/39 - 34s - loss: 629.4377 - loglik: -6.2675e+02 - logprior: -2.6846e+00
Fitted a model with MAP estimate = -628.1550
expansions: [(0, 2), (11, 1), (13, 1), (14, 1), (45, 1), (51, 3), (55, 1), (60, 1), (61, 1), (62, 2), (63, 1), (66, 1), (67, 1), (76, 1), (77, 1), (82, 1), (83, 2), (84, 1), (89, 1), (90, 1), (97, 1), (98, 1), (99, 1), (101, 1), (105, 1), (116, 1), (121, 1), (134, 1), (139, 1), (142, 1), (143, 1), (157, 1), (158, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (176, 1), (184, 1), (185, 1), (188, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 1), (236, 2), (262, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 621.3440 - loglik: -6.1918e+02 - logprior: -2.1673e+00
Epoch 2/2
39/39 - 49s - loss: 607.8571 - loglik: -6.0740e+02 - logprior: -4.5768e-01
Fitted a model with MAP estimate = -605.3140
expansions: []
discards: [  1 251]
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 611.4254 - loglik: -6.1024e+02 - logprior: -1.1866e+00
Epoch 2/2
39/39 - 48s - loss: 607.5430 - loglik: -6.0744e+02 - logprior: -1.0045e-01
Fitted a model with MAP estimate = -605.2475
expansions: []
discards: [335 336]
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 610.9669 - loglik: -6.0999e+02 - logprior: -9.7495e-01
Epoch 2/10
39/39 - 48s - loss: 607.2039 - loglik: -6.0733e+02 - logprior: 0.1215
Epoch 3/10
39/39 - 48s - loss: 605.0844 - loglik: -6.0531e+02 - logprior: 0.2259
Epoch 4/10
39/39 - 48s - loss: 602.5276 - loglik: -6.0270e+02 - logprior: 0.1701
Epoch 5/10
39/39 - 48s - loss: 600.6089 - loglik: -6.0082e+02 - logprior: 0.2074
Epoch 6/10
39/39 - 48s - loss: 599.5745 - loglik: -5.9967e+02 - logprior: 0.0940
Epoch 7/10
39/39 - 47s - loss: 599.0836 - loglik: -5.9910e+02 - logprior: 0.0167
Epoch 8/10
39/39 - 47s - loss: 598.6462 - loglik: -5.9860e+02 - logprior: -4.2530e-02
Epoch 9/10
39/39 - 48s - loss: 598.0452 - loglik: -5.9789e+02 - logprior: -1.5657e-01
Epoch 10/10
39/39 - 47s - loss: 597.2451 - loglik: -5.9711e+02 - logprior: -1.3677e-01
Fitted a model with MAP estimate = -596.9623
Time for alignment: 1179.3026
Computed alignments with likelihoods: ['-597.4604', '-596.8090', '-596.9623']
Best model has likelihood: -596.8090
SP score = 0.9417
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f524dfa4b20>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f4a5cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52945730a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529513efa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.1458 - loglik: -1.4676e+02 - logprior: -3.3891e+00
Epoch 2/10
19/19 - 1s - loss: 127.8659 - loglik: -1.2647e+02 - logprior: -1.3918e+00
Epoch 3/10
19/19 - 1s - loss: 119.5866 - loglik: -1.1802e+02 - logprior: -1.5688e+00
Epoch 4/10
19/19 - 1s - loss: 118.1296 - loglik: -1.1668e+02 - logprior: -1.4492e+00
Epoch 5/10
19/19 - 1s - loss: 117.7119 - loglik: -1.1628e+02 - logprior: -1.4317e+00
Epoch 6/10
19/19 - 1s - loss: 117.3595 - loglik: -1.1593e+02 - logprior: -1.4276e+00
Epoch 7/10
19/19 - 1s - loss: 117.2501 - loglik: -1.1581e+02 - logprior: -1.4357e+00
Epoch 8/10
19/19 - 1s - loss: 117.1101 - loglik: -1.1566e+02 - logprior: -1.4524e+00
Epoch 9/10
19/19 - 1s - loss: 117.0760 - loglik: -1.1561e+02 - logprior: -1.4662e+00
Epoch 10/10
19/19 - 1s - loss: 116.9417 - loglik: -1.1547e+02 - logprior: -1.4760e+00
Fitted a model with MAP estimate = -116.7945
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 122.7192 - loglik: -1.1847e+02 - logprior: -4.2537e+00
Epoch 2/2
19/19 - 1s - loss: 114.2437 - loglik: -1.1210e+02 - logprior: -2.1443e+00
Fitted a model with MAP estimate = -112.6314
expansions: [(0, 2)]
discards: [ 0 23 34 36 40 46]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.1400 - loglik: -1.1113e+02 - logprior: -3.0137e+00
Epoch 2/2
19/19 - 1s - loss: 110.6275 - loglik: -1.0943e+02 - logprior: -1.1964e+00
Fitted a model with MAP estimate = -110.0723
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 115.4190 - loglik: -1.1166e+02 - logprior: -3.7573e+00
Epoch 2/10
19/19 - 1s - loss: 111.2332 - loglik: -1.0984e+02 - logprior: -1.3931e+00
Epoch 3/10
19/19 - 1s - loss: 110.4542 - loglik: -1.0923e+02 - logprior: -1.2268e+00
Epoch 4/10
19/19 - 1s - loss: 110.2140 - loglik: -1.0902e+02 - logprior: -1.1915e+00
Epoch 5/10
19/19 - 1s - loss: 109.6864 - loglik: -1.0850e+02 - logprior: -1.1899e+00
Epoch 6/10
19/19 - 1s - loss: 109.4726 - loglik: -1.0827e+02 - logprior: -1.1984e+00
Epoch 7/10
19/19 - 1s - loss: 109.4213 - loglik: -1.0822e+02 - logprior: -1.2004e+00
Epoch 8/10
19/19 - 1s - loss: 109.2369 - loglik: -1.0802e+02 - logprior: -1.2137e+00
Epoch 9/10
19/19 - 1s - loss: 109.1160 - loglik: -1.0789e+02 - logprior: -1.2305e+00
Epoch 10/10
19/19 - 1s - loss: 109.0762 - loglik: -1.0786e+02 - logprior: -1.2208e+00
Fitted a model with MAP estimate = -108.9255
Time for alignment: 52.7935
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.2171 - loglik: -1.4683e+02 - logprior: -3.3892e+00
Epoch 2/10
19/19 - 1s - loss: 128.6309 - loglik: -1.2724e+02 - logprior: -1.3956e+00
Epoch 3/10
19/19 - 1s - loss: 120.1661 - loglik: -1.1859e+02 - logprior: -1.5743e+00
Epoch 4/10
19/19 - 1s - loss: 117.6447 - loglik: -1.1617e+02 - logprior: -1.4789e+00
Epoch 5/10
19/19 - 1s - loss: 117.0049 - loglik: -1.1553e+02 - logprior: -1.4731e+00
Epoch 6/10
19/19 - 1s - loss: 116.9308 - loglik: -1.1546e+02 - logprior: -1.4679e+00
Epoch 7/10
19/19 - 1s - loss: 116.6560 - loglik: -1.1519e+02 - logprior: -1.4688e+00
Epoch 8/10
19/19 - 1s - loss: 116.5401 - loglik: -1.1506e+02 - logprior: -1.4803e+00
Epoch 9/10
19/19 - 1s - loss: 116.4802 - loglik: -1.1499e+02 - logprior: -1.4895e+00
Epoch 10/10
19/19 - 1s - loss: 116.3610 - loglik: -1.1485e+02 - logprior: -1.5089e+00
Fitted a model with MAP estimate = -116.2386
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.7618 - loglik: -1.1754e+02 - logprior: -4.2239e+00
Epoch 2/2
19/19 - 1s - loss: 113.9286 - loglik: -1.1184e+02 - logprior: -2.0879e+00
Fitted a model with MAP estimate = -112.5032
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6275 - loglik: -1.1061e+02 - logprior: -3.0134e+00
Epoch 2/2
19/19 - 1s - loss: 110.6188 - loglik: -1.0942e+02 - logprior: -1.1972e+00
Fitted a model with MAP estimate = -110.0494
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.3888 - loglik: -1.1164e+02 - logprior: -3.7489e+00
Epoch 2/10
19/19 - 1s - loss: 111.1567 - loglik: -1.0976e+02 - logprior: -1.3918e+00
Epoch 3/10
19/19 - 1s - loss: 110.4779 - loglik: -1.0925e+02 - logprior: -1.2280e+00
Epoch 4/10
19/19 - 1s - loss: 110.1516 - loglik: -1.0895e+02 - logprior: -1.1976e+00
Epoch 5/10
19/19 - 1s - loss: 109.8411 - loglik: -1.0865e+02 - logprior: -1.1898e+00
Epoch 6/10
19/19 - 1s - loss: 109.4801 - loglik: -1.0828e+02 - logprior: -1.2029e+00
Epoch 7/10
19/19 - 1s - loss: 109.2818 - loglik: -1.0807e+02 - logprior: -1.2071e+00
Epoch 8/10
19/19 - 1s - loss: 109.2436 - loglik: -1.0802e+02 - logprior: -1.2223e+00
Epoch 9/10
19/19 - 1s - loss: 109.1745 - loglik: -1.0795e+02 - logprior: -1.2205e+00
Epoch 10/10
19/19 - 1s - loss: 108.9717 - loglik: -1.0774e+02 - logprior: -1.2325e+00
Fitted a model with MAP estimate = -108.9301
Time for alignment: 52.3450
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.2549 - loglik: -1.4687e+02 - logprior: -3.3886e+00
Epoch 2/10
19/19 - 1s - loss: 128.1123 - loglik: -1.2672e+02 - logprior: -1.3960e+00
Epoch 3/10
19/19 - 1s - loss: 119.3943 - loglik: -1.1781e+02 - logprior: -1.5868e+00
Epoch 4/10
19/19 - 1s - loss: 118.1003 - loglik: -1.1665e+02 - logprior: -1.4507e+00
Epoch 5/10
19/19 - 1s - loss: 117.6507 - loglik: -1.1622e+02 - logprior: -1.4351e+00
Epoch 6/10
19/19 - 1s - loss: 117.4487 - loglik: -1.1602e+02 - logprior: -1.4299e+00
Epoch 7/10
19/19 - 1s - loss: 117.1803 - loglik: -1.1575e+02 - logprior: -1.4317e+00
Epoch 8/10
19/19 - 1s - loss: 117.1917 - loglik: -1.1574e+02 - logprior: -1.4522e+00
Fitted a model with MAP estimate = -116.9076
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.5970 - loglik: -1.1836e+02 - logprior: -4.2393e+00
Epoch 2/2
19/19 - 1s - loss: 114.1310 - loglik: -1.1200e+02 - logprior: -2.1267e+00
Fitted a model with MAP estimate = -112.5166
expansions: [(0, 2)]
discards: [ 0 22 34 36 40 46]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8549 - loglik: -1.1083e+02 - logprior: -3.0212e+00
Epoch 2/2
19/19 - 1s - loss: 110.7228 - loglik: -1.0952e+02 - logprior: -1.1999e+00
Fitted a model with MAP estimate = -110.0506
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 115.2611 - loglik: -1.1151e+02 - logprior: -3.7482e+00
Epoch 2/10
19/19 - 1s - loss: 111.2365 - loglik: -1.0985e+02 - logprior: -1.3899e+00
Epoch 3/10
19/19 - 1s - loss: 110.5276 - loglik: -1.0930e+02 - logprior: -1.2266e+00
Epoch 4/10
19/19 - 1s - loss: 110.0947 - loglik: -1.0890e+02 - logprior: -1.1983e+00
Epoch 5/10
19/19 - 1s - loss: 109.7981 - loglik: -1.0861e+02 - logprior: -1.1923e+00
Epoch 6/10
19/19 - 1s - loss: 109.5257 - loglik: -1.0833e+02 - logprior: -1.1968e+00
Epoch 7/10
19/19 - 1s - loss: 109.3542 - loglik: -1.0815e+02 - logprior: -1.2020e+00
Epoch 8/10
19/19 - 1s - loss: 109.3265 - loglik: -1.0811e+02 - logprior: -1.2138e+00
Epoch 9/10
19/19 - 1s - loss: 109.1099 - loglik: -1.0788e+02 - logprior: -1.2259e+00
Epoch 10/10
19/19 - 1s - loss: 108.9451 - loglik: -1.0771e+02 - logprior: -1.2338e+00
Fitted a model with MAP estimate = -108.9263
Time for alignment: 50.2643
Computed alignments with likelihoods: ['-108.9255', '-108.9301', '-108.9263']
Best model has likelihood: -108.9255
SP score = 0.6803
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f524df43e20>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f525ea03760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52947f6fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52947f6160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.3961 - loglik: -3.5690e+02 - logprior: -3.4928e+00
Epoch 2/10
19/19 - 3s - loss: 301.0080 - loglik: -2.9977e+02 - logprior: -1.2361e+00
Epoch 3/10
19/19 - 3s - loss: 278.2036 - loglik: -2.7679e+02 - logprior: -1.4128e+00
Epoch 4/10
19/19 - 3s - loss: 272.9729 - loglik: -2.7151e+02 - logprior: -1.4590e+00
Epoch 5/10
19/19 - 3s - loss: 270.5187 - loglik: -2.6904e+02 - logprior: -1.4773e+00
Epoch 6/10
19/19 - 3s - loss: 269.2442 - loglik: -2.6777e+02 - logprior: -1.4697e+00
Epoch 7/10
19/19 - 3s - loss: 268.9224 - loglik: -2.6744e+02 - logprior: -1.4845e+00
Epoch 8/10
19/19 - 3s - loss: 267.5005 - loglik: -2.6597e+02 - logprior: -1.5317e+00
Epoch 9/10
19/19 - 3s - loss: 267.6267 - loglik: -2.6608e+02 - logprior: -1.5496e+00
Fitted a model with MAP estimate = -267.3719
expansions: [(12, 2), (14, 3), (16, 1), (23, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.9705 - loglik: -2.6582e+02 - logprior: -4.1493e+00
Epoch 2/2
19/19 - 4s - loss: 259.6474 - loglik: -2.5765e+02 - logprior: -2.0011e+00
Fitted a model with MAP estimate = -258.0334
expansions: [(0, 3)]
discards: [ 0 12 38 76]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.0070 - loglik: -2.5819e+02 - logprior: -2.8214e+00
Epoch 2/2
19/19 - 4s - loss: 257.5444 - loglik: -2.5648e+02 - logprior: -1.0638e+00
Fitted a model with MAP estimate = -256.7216
expansions: []
discards: [ 0  2 17]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.9245 - loglik: -2.5912e+02 - logprior: -3.8049e+00
Epoch 2/10
19/19 - 4s - loss: 259.0139 - loglik: -2.5752e+02 - logprior: -1.4975e+00
Epoch 3/10
19/19 - 4s - loss: 257.3384 - loglik: -2.5655e+02 - logprior: -7.8878e-01
Epoch 4/10
19/19 - 4s - loss: 256.4988 - loglik: -2.5576e+02 - logprior: -7.4276e-01
Epoch 5/10
19/19 - 4s - loss: 255.8175 - loglik: -2.5503e+02 - logprior: -7.8877e-01
Epoch 6/10
19/19 - 4s - loss: 255.6624 - loglik: -2.5487e+02 - logprior: -7.9727e-01
Epoch 7/10
19/19 - 4s - loss: 254.9996 - loglik: -2.5420e+02 - logprior: -7.9877e-01
Epoch 8/10
19/19 - 4s - loss: 255.1089 - loglik: -2.5430e+02 - logprior: -8.0742e-01
Fitted a model with MAP estimate = -254.8401
Time for alignment: 113.0406
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.3210 - loglik: -3.5684e+02 - logprior: -3.4837e+00
Epoch 2/10
19/19 - 3s - loss: 301.5269 - loglik: -3.0027e+02 - logprior: -1.2556e+00
Epoch 3/10
19/19 - 3s - loss: 277.2162 - loglik: -2.7576e+02 - logprior: -1.4555e+00
Epoch 4/10
19/19 - 3s - loss: 270.5247 - loglik: -2.6899e+02 - logprior: -1.5371e+00
Epoch 5/10
19/19 - 3s - loss: 269.1838 - loglik: -2.6767e+02 - logprior: -1.5154e+00
Epoch 6/10
19/19 - 3s - loss: 267.8690 - loglik: -2.6636e+02 - logprior: -1.5100e+00
Epoch 7/10
19/19 - 3s - loss: 266.9044 - loglik: -2.6537e+02 - logprior: -1.5339e+00
Epoch 8/10
19/19 - 3s - loss: 267.2776 - loglik: -2.6573e+02 - logprior: -1.5475e+00
Fitted a model with MAP estimate = -266.7432
expansions: [(12, 1), (14, 3), (17, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.2512 - loglik: -2.6512e+02 - logprior: -4.1280e+00
Epoch 2/2
19/19 - 4s - loss: 259.3642 - loglik: -2.5740e+02 - logprior: -1.9625e+00
Fitted a model with MAP estimate = -257.8710
expansions: [(0, 3)]
discards: [ 0 15 37 75]
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.8806 - loglik: -2.5807e+02 - logprior: -2.8138e+00
Epoch 2/2
19/19 - 4s - loss: 257.3978 - loglik: -2.5635e+02 - logprior: -1.0502e+00
Fitted a model with MAP estimate = -256.5007
expansions: []
discards: [0 2]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.0441 - loglik: -2.5827e+02 - logprior: -3.7728e+00
Epoch 2/10
19/19 - 4s - loss: 258.5146 - loglik: -2.5707e+02 - logprior: -1.4435e+00
Epoch 3/10
19/19 - 4s - loss: 256.5129 - loglik: -2.5574e+02 - logprior: -7.7180e-01
Epoch 4/10
19/19 - 4s - loss: 255.8168 - loglik: -2.5509e+02 - logprior: -7.2878e-01
Epoch 5/10
19/19 - 4s - loss: 255.4239 - loglik: -2.5466e+02 - logprior: -7.5948e-01
Epoch 6/10
19/19 - 4s - loss: 255.1055 - loglik: -2.5433e+02 - logprior: -7.7467e-01
Epoch 7/10
19/19 - 4s - loss: 254.8861 - loglik: -2.5409e+02 - logprior: -7.9849e-01
Epoch 8/10
19/19 - 4s - loss: 254.3641 - loglik: -2.5356e+02 - logprior: -7.9998e-01
Epoch 9/10
19/19 - 4s - loss: 254.6801 - loglik: -2.5388e+02 - logprior: -7.9848e-01
Fitted a model with MAP estimate = -254.1720
Time for alignment: 114.4921
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.4299 - loglik: -3.5694e+02 - logprior: -3.4935e+00
Epoch 2/10
19/19 - 3s - loss: 300.9591 - loglik: -2.9971e+02 - logprior: -1.2492e+00
Epoch 3/10
19/19 - 3s - loss: 277.4604 - loglik: -2.7601e+02 - logprior: -1.4456e+00
Epoch 4/10
19/19 - 3s - loss: 271.9057 - loglik: -2.7041e+02 - logprior: -1.4958e+00
Epoch 5/10
19/19 - 3s - loss: 269.4229 - loglik: -2.6791e+02 - logprior: -1.5122e+00
Epoch 6/10
19/19 - 3s - loss: 269.3802 - loglik: -2.6786e+02 - logprior: -1.5162e+00
Epoch 7/10
19/19 - 3s - loss: 267.7850 - loglik: -2.6626e+02 - logprior: -1.5203e+00
Epoch 8/10
19/19 - 3s - loss: 267.5851 - loglik: -2.6603e+02 - logprior: -1.5562e+00
Epoch 9/10
19/19 - 3s - loss: 267.9806 - loglik: -2.6638e+02 - logprior: -1.5982e+00
Fitted a model with MAP estimate = -267.3747
expansions: [(12, 1), (14, 4), (16, 1), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.6820 - loglik: -2.6553e+02 - logprior: -4.1552e+00
Epoch 2/2
19/19 - 4s - loss: 259.4485 - loglik: -2.5743e+02 - logprior: -2.0170e+00
Fitted a model with MAP estimate = -257.6071
expansions: [(0, 3)]
discards: [ 0 14 15 38 76]
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.0049 - loglik: -2.5816e+02 - logprior: -2.8470e+00
Epoch 2/2
19/19 - 4s - loss: 257.8481 - loglik: -2.5676e+02 - logprior: -1.0915e+00
Fitted a model with MAP estimate = -256.7231
expansions: []
discards: [0 2]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.2737 - loglik: -2.5845e+02 - logprior: -3.8191e+00
Epoch 2/10
19/19 - 4s - loss: 258.4930 - loglik: -2.5698e+02 - logprior: -1.5099e+00
Epoch 3/10
19/19 - 4s - loss: 257.4391 - loglik: -2.5662e+02 - logprior: -8.1696e-01
Epoch 4/10
19/19 - 4s - loss: 256.2268 - loglik: -2.5546e+02 - logprior: -7.7089e-01
Epoch 5/10
19/19 - 4s - loss: 255.8367 - loglik: -2.5504e+02 - logprior: -8.0081e-01
Epoch 6/10
19/19 - 4s - loss: 254.6603 - loglik: -2.5384e+02 - logprior: -8.1622e-01
Epoch 7/10
19/19 - 4s - loss: 255.1998 - loglik: -2.5438e+02 - logprior: -8.2028e-01
Fitted a model with MAP estimate = -254.7256
Time for alignment: 110.0078
Computed alignments with likelihoods: ['-254.8401', '-254.1720', '-254.7256']
Best model has likelihood: -254.1720
SP score = 0.8079
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5294149220>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5239b735e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236ce2bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5239524d60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 275.0471 - loglik: -2.7164e+02 - logprior: -3.4076e+00
Epoch 2/10
19/19 - 3s - loss: 236.8663 - loglik: -2.3558e+02 - logprior: -1.2911e+00
Epoch 3/10
19/19 - 3s - loss: 220.9838 - loglik: -2.1949e+02 - logprior: -1.4891e+00
Epoch 4/10
19/19 - 3s - loss: 217.0936 - loglik: -2.1562e+02 - logprior: -1.4728e+00
Epoch 5/10
19/19 - 3s - loss: 216.0507 - loglik: -2.1462e+02 - logprior: -1.4258e+00
Epoch 6/10
19/19 - 3s - loss: 215.5968 - loglik: -2.1420e+02 - logprior: -1.4002e+00
Epoch 7/10
19/19 - 3s - loss: 215.4319 - loglik: -2.1403e+02 - logprior: -1.4061e+00
Epoch 8/10
19/19 - 3s - loss: 214.9259 - loglik: -2.1348e+02 - logprior: -1.4456e+00
Epoch 9/10
19/19 - 3s - loss: 215.2321 - loglik: -2.1376e+02 - logprior: -1.4721e+00
Fitted a model with MAP estimate = -214.5467
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 216.9898 - loglik: -2.1297e+02 - logprior: -4.0246e+00
Epoch 2/2
19/19 - 3s - loss: 207.1791 - loglik: -2.0586e+02 - logprior: -1.3175e+00
Fitted a model with MAP estimate = -205.7627
expansions: []
discards: [ 0 46 75 79 95]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.1659 - loglik: -2.0719e+02 - logprior: -3.9719e+00
Epoch 2/2
19/19 - 3s - loss: 207.4084 - loglik: -2.0596e+02 - logprior: -1.4492e+00
Fitted a model with MAP estimate = -205.9802
expansions: []
discards: [0]
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.6642 - loglik: -2.0657e+02 - logprior: -3.0974e+00
Epoch 2/10
19/19 - 3s - loss: 206.6438 - loglik: -2.0563e+02 - logprior: -1.0162e+00
Epoch 3/10
19/19 - 3s - loss: 206.1044 - loglik: -2.0520e+02 - logprior: -9.0708e-01
Epoch 4/10
19/19 - 3s - loss: 205.5676 - loglik: -2.0467e+02 - logprior: -8.9307e-01
Epoch 5/10
19/19 - 3s - loss: 205.3380 - loglik: -2.0449e+02 - logprior: -8.5175e-01
Epoch 6/10
19/19 - 3s - loss: 204.6999 - loglik: -2.0383e+02 - logprior: -8.6952e-01
Epoch 7/10
19/19 - 3s - loss: 204.4775 - loglik: -2.0360e+02 - logprior: -8.7944e-01
Epoch 8/10
19/19 - 3s - loss: 204.4981 - loglik: -2.0362e+02 - logprior: -8.7927e-01
Fitted a model with MAP estimate = -204.2678
Time for alignment: 91.4195
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 275.0717 - loglik: -2.7166e+02 - logprior: -3.4078e+00
Epoch 2/10
19/19 - 3s - loss: 236.5895 - loglik: -2.3530e+02 - logprior: -1.2918e+00
Epoch 3/10
19/19 - 3s - loss: 221.0587 - loglik: -2.1960e+02 - logprior: -1.4634e+00
Epoch 4/10
19/19 - 3s - loss: 216.8905 - loglik: -2.1542e+02 - logprior: -1.4685e+00
Epoch 5/10
19/19 - 3s - loss: 215.5527 - loglik: -2.1410e+02 - logprior: -1.4566e+00
Epoch 6/10
19/19 - 3s - loss: 215.0662 - loglik: -2.1361e+02 - logprior: -1.4520e+00
Epoch 7/10
19/19 - 3s - loss: 215.1887 - loglik: -2.1373e+02 - logprior: -1.4601e+00
Fitted a model with MAP estimate = -214.3977
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (27, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 2), (64, 2), (70, 1), (71, 2), (77, 1)]
discards: []
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 217.4458 - loglik: -2.1349e+02 - logprior: -3.9540e+00
Epoch 2/2
19/19 - 3s - loss: 207.0630 - loglik: -2.0571e+02 - logprior: -1.3540e+00
Fitted a model with MAP estimate = -205.6396
expansions: []
discards: [ 0 46 75 79 85 96]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.5697 - loglik: -2.0759e+02 - logprior: -3.9766e+00
Epoch 2/2
19/19 - 3s - loss: 207.5083 - loglik: -2.0598e+02 - logprior: -1.5234e+00
Fitted a model with MAP estimate = -206.0450
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.4762 - loglik: -2.0560e+02 - logprior: -2.8713e+00
Epoch 2/10
19/19 - 3s - loss: 206.0106 - loglik: -2.0495e+02 - logprior: -1.0633e+00
Epoch 3/10
19/19 - 3s - loss: 205.4043 - loglik: -2.0438e+02 - logprior: -1.0265e+00
Epoch 4/10
19/19 - 3s - loss: 204.5994 - loglik: -2.0361e+02 - logprior: -9.8913e-01
Epoch 5/10
19/19 - 3s - loss: 204.6973 - loglik: -2.0373e+02 - logprior: -9.6243e-01
Fitted a model with MAP estimate = -204.1159
Time for alignment: 77.5106
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6419 - loglik: -2.7124e+02 - logprior: -3.4035e+00
Epoch 2/10
19/19 - 3s - loss: 234.5249 - loglik: -2.3325e+02 - logprior: -1.2719e+00
Epoch 3/10
19/19 - 3s - loss: 221.4352 - loglik: -2.1997e+02 - logprior: -1.4656e+00
Epoch 4/10
19/19 - 3s - loss: 218.4965 - loglik: -2.1693e+02 - logprior: -1.5620e+00
Epoch 5/10
19/19 - 3s - loss: 217.5235 - loglik: -2.1602e+02 - logprior: -1.5049e+00
Epoch 6/10
19/19 - 3s - loss: 216.7092 - loglik: -2.1520e+02 - logprior: -1.5094e+00
Epoch 7/10
19/19 - 3s - loss: 216.6389 - loglik: -2.1511e+02 - logprior: -1.5309e+00
Epoch 8/10
19/19 - 3s - loss: 216.3113 - loglik: -2.1476e+02 - logprior: -1.5500e+00
Epoch 9/10
19/19 - 3s - loss: 216.4406 - loglik: -2.1487e+02 - logprior: -1.5733e+00
Fitted a model with MAP estimate = -215.8365
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (10, 1), (17, 1), (18, 3), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 215.7145 - loglik: -2.1256e+02 - logprior: -3.1582e+00
Epoch 2/2
19/19 - 3s - loss: 206.8860 - loglik: -2.0567e+02 - logprior: -1.2196e+00
Fitted a model with MAP estimate = -205.5409
expansions: []
discards: [ 1 46 75 79 95]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 209.5362 - loglik: -2.0678e+02 - logprior: -2.7560e+00
Epoch 2/2
19/19 - 3s - loss: 206.5165 - loglik: -2.0536e+02 - logprior: -1.1557e+00
Fitted a model with MAP estimate = -205.6270
expansions: []
discards: []
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.4869 - loglik: -2.0564e+02 - logprior: -2.8482e+00
Epoch 2/10
19/19 - 3s - loss: 205.9693 - loglik: -2.0489e+02 - logprior: -1.0773e+00
Epoch 3/10
19/19 - 3s - loss: 205.4520 - loglik: -2.0440e+02 - logprior: -1.0548e+00
Epoch 4/10
19/19 - 3s - loss: 205.0879 - loglik: -2.0408e+02 - logprior: -1.0039e+00
Epoch 5/10
19/19 - 3s - loss: 204.7566 - loglik: -2.0377e+02 - logprior: -9.8514e-01
Epoch 6/10
19/19 - 3s - loss: 204.3745 - loglik: -2.0339e+02 - logprior: -9.8590e-01
Epoch 7/10
19/19 - 3s - loss: 203.8818 - loglik: -2.0288e+02 - logprior: -1.0061e+00
Epoch 8/10
19/19 - 3s - loss: 203.9607 - loglik: -2.0295e+02 - logprior: -1.0120e+00
Fitted a model with MAP estimate = -203.7616
Time for alignment: 91.7575
Computed alignments with likelihoods: ['-204.2678', '-204.1159', '-203.7616']
Best model has likelihood: -203.7616
SP score = 0.6643
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5239ed0700>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52391a2cd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236d9d5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236eb8970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.5838 - loglik: -1.7313e+02 - logprior: -3.4519e+00
Epoch 2/10
19/19 - 1s - loss: 133.9265 - loglik: -1.3246e+02 - logprior: -1.4708e+00
Epoch 3/10
19/19 - 1s - loss: 117.2737 - loglik: -1.1582e+02 - logprior: -1.4569e+00
Epoch 4/10
19/19 - 1s - loss: 114.5312 - loglik: -1.1302e+02 - logprior: -1.5076e+00
Epoch 5/10
19/19 - 1s - loss: 113.5829 - loglik: -1.1210e+02 - logprior: -1.4841e+00
Epoch 6/10
19/19 - 1s - loss: 113.2411 - loglik: -1.1178e+02 - logprior: -1.4618e+00
Epoch 7/10
19/19 - 1s - loss: 112.8528 - loglik: -1.1138e+02 - logprior: -1.4759e+00
Epoch 8/10
19/19 - 2s - loss: 112.9958 - loglik: -1.1150e+02 - logprior: -1.4916e+00
Fitted a model with MAP estimate = -112.6777
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.1597 - loglik: -1.0993e+02 - logprior: -4.2323e+00
Epoch 2/2
19/19 - 2s - loss: 104.9669 - loglik: -1.0367e+02 - logprior: -1.2977e+00
Fitted a model with MAP estimate = -103.7141
expansions: []
discards: [ 0 35]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.3667 - loglik: -1.0529e+02 - logprior: -4.0738e+00
Epoch 2/2
19/19 - 2s - loss: 105.1246 - loglik: -1.0361e+02 - logprior: -1.5160e+00
Fitted a model with MAP estimate = -104.3337
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.4109 - loglik: -1.0450e+02 - logprior: -2.9073e+00
Epoch 2/10
19/19 - 1s - loss: 104.3897 - loglik: -1.0334e+02 - logprior: -1.0464e+00
Epoch 3/10
19/19 - 2s - loss: 103.5151 - loglik: -1.0225e+02 - logprior: -1.2632e+00
Epoch 4/10
19/19 - 2s - loss: 102.9912 - loglik: -1.0186e+02 - logprior: -1.1362e+00
Epoch 5/10
19/19 - 2s - loss: 102.8106 - loglik: -1.0170e+02 - logprior: -1.1095e+00
Epoch 6/10
19/19 - 2s - loss: 102.4287 - loglik: -1.0131e+02 - logprior: -1.1195e+00
Epoch 7/10
19/19 - 2s - loss: 101.9811 - loglik: -1.0087e+02 - logprior: -1.1124e+00
Epoch 8/10
19/19 - 2s - loss: 102.1729 - loglik: -1.0105e+02 - logprior: -1.1248e+00
Fitted a model with MAP estimate = -101.8539
Time for alignment: 54.3830
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.6592 - loglik: -1.7321e+02 - logprior: -3.4499e+00
Epoch 2/10
19/19 - 1s - loss: 132.9435 - loglik: -1.3145e+02 - logprior: -1.4910e+00
Epoch 3/10
19/19 - 1s - loss: 117.1783 - loglik: -1.1572e+02 - logprior: -1.4554e+00
Epoch 4/10
19/19 - 1s - loss: 115.1278 - loglik: -1.1365e+02 - logprior: -1.4804e+00
Epoch 5/10
19/19 - 1s - loss: 113.9673 - loglik: -1.1251e+02 - logprior: -1.4541e+00
Epoch 6/10
19/19 - 1s - loss: 113.4481 - loglik: -1.1201e+02 - logprior: -1.4386e+00
Epoch 7/10
19/19 - 1s - loss: 113.3699 - loglik: -1.1192e+02 - logprior: -1.4543e+00
Epoch 8/10
19/19 - 1s - loss: 113.5151 - loglik: -1.1205e+02 - logprior: -1.4691e+00
Fitted a model with MAP estimate = -113.0822
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6937 - loglik: -1.0947e+02 - logprior: -4.2198e+00
Epoch 2/2
19/19 - 2s - loss: 104.7248 - loglik: -1.0343e+02 - logprior: -1.2914e+00
Fitted a model with MAP estimate = -103.8759
expansions: []
discards: [ 0 38]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.4259 - loglik: -1.0535e+02 - logprior: -4.0781e+00
Epoch 2/2
19/19 - 1s - loss: 105.2909 - loglik: -1.0377e+02 - logprior: -1.5224e+00
Fitted a model with MAP estimate = -104.3798
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5086 - loglik: -1.0459e+02 - logprior: -2.9170e+00
Epoch 2/10
19/19 - 1s - loss: 104.2600 - loglik: -1.0322e+02 - logprior: -1.0406e+00
Epoch 3/10
19/19 - 1s - loss: 103.6824 - loglik: -1.0242e+02 - logprior: -1.2618e+00
Epoch 4/10
19/19 - 2s - loss: 102.9251 - loglik: -1.0179e+02 - logprior: -1.1368e+00
Epoch 5/10
19/19 - 1s - loss: 102.7589 - loglik: -1.0165e+02 - logprior: -1.1112e+00
Epoch 6/10
19/19 - 2s - loss: 102.3778 - loglik: -1.0126e+02 - logprior: -1.1203e+00
Epoch 7/10
19/19 - 2s - loss: 102.2481 - loglik: -1.0113e+02 - logprior: -1.1131e+00
Epoch 8/10
19/19 - 2s - loss: 102.0781 - loglik: -1.0095e+02 - logprior: -1.1292e+00
Epoch 9/10
19/19 - 1s - loss: 101.5110 - loglik: -1.0038e+02 - logprior: -1.1285e+00
Epoch 10/10
19/19 - 2s - loss: 101.9664 - loglik: -1.0083e+02 - logprior: -1.1377e+00
Fitted a model with MAP estimate = -101.6946
Time for alignment: 56.0218
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 176.6375 - loglik: -1.7318e+02 - logprior: -3.4539e+00
Epoch 2/10
19/19 - 2s - loss: 133.6310 - loglik: -1.3216e+02 - logprior: -1.4689e+00
Epoch 3/10
19/19 - 1s - loss: 118.0676 - loglik: -1.1664e+02 - logprior: -1.4256e+00
Epoch 4/10
19/19 - 1s - loss: 115.3765 - loglik: -1.1390e+02 - logprior: -1.4733e+00
Epoch 5/10
19/19 - 1s - loss: 114.5236 - loglik: -1.1306e+02 - logprior: -1.4595e+00
Epoch 6/10
19/19 - 1s - loss: 114.1677 - loglik: -1.1273e+02 - logprior: -1.4387e+00
Epoch 7/10
19/19 - 1s - loss: 113.7981 - loglik: -1.1235e+02 - logprior: -1.4521e+00
Epoch 8/10
19/19 - 2s - loss: 113.5097 - loglik: -1.1205e+02 - logprior: -1.4599e+00
Epoch 9/10
19/19 - 2s - loss: 113.4259 - loglik: -1.1195e+02 - logprior: -1.4753e+00
Epoch 10/10
19/19 - 1s - loss: 113.2046 - loglik: -1.1172e+02 - logprior: -1.4868e+00
Fitted a model with MAP estimate = -113.2253
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.0358 - loglik: -1.0975e+02 - logprior: -4.2878e+00
Epoch 2/2
19/19 - 2s - loss: 104.9192 - loglik: -1.0363e+02 - logprior: -1.2894e+00
Fitted a model with MAP estimate = -103.8590
expansions: []
discards: [ 0 38]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4187 - loglik: -1.0535e+02 - logprior: -4.0680e+00
Epoch 2/2
19/19 - 2s - loss: 105.0669 - loglik: -1.0356e+02 - logprior: -1.5108e+00
Fitted a model with MAP estimate = -104.3639
expansions: []
discards: []
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.5556 - loglik: -1.0436e+02 - logprior: -3.1979e+00
Epoch 2/10
19/19 - 2s - loss: 104.7399 - loglik: -1.0335e+02 - logprior: -1.3906e+00
Epoch 3/10
19/19 - 2s - loss: 104.0635 - loglik: -1.0277e+02 - logprior: -1.2965e+00
Epoch 4/10
19/19 - 2s - loss: 103.4736 - loglik: -1.0221e+02 - logprior: -1.2621e+00
Epoch 5/10
19/19 - 2s - loss: 103.1504 - loglik: -1.0191e+02 - logprior: -1.2417e+00
Epoch 6/10
19/19 - 2s - loss: 102.5867 - loglik: -1.0135e+02 - logprior: -1.2386e+00
Epoch 7/10
19/19 - 2s - loss: 102.4953 - loglik: -1.0126e+02 - logprior: -1.2388e+00
Epoch 8/10
19/19 - 2s - loss: 102.3902 - loglik: -1.0113e+02 - logprior: -1.2568e+00
Epoch 9/10
19/19 - 2s - loss: 102.2087 - loglik: -1.0095e+02 - logprior: -1.2636e+00
Epoch 10/10
19/19 - 2s - loss: 102.1622 - loglik: -1.0090e+02 - logprior: -1.2668e+00
Fitted a model with MAP estimate = -101.9955
Time for alignment: 60.9641
Computed alignments with likelihoods: ['-101.8539', '-101.6946', '-101.9955']
Best model has likelihood: -101.6946
SP score = 0.8204
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52377a01c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f524576fc10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5295549850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f366550>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 616.7543 - loglik: -6.1343e+02 - logprior: -3.3255e+00
Epoch 2/10
19/19 - 12s - loss: 516.5734 - loglik: -5.1528e+02 - logprior: -1.2887e+00
Epoch 3/10
19/19 - 12s - loss: 483.6270 - loglik: -4.8222e+02 - logprior: -1.4097e+00
Epoch 4/10
19/19 - 12s - loss: 476.6200 - loglik: -4.7518e+02 - logprior: -1.4441e+00
Epoch 5/10
19/19 - 12s - loss: 472.1433 - loglik: -4.7065e+02 - logprior: -1.4974e+00
Epoch 6/10
19/19 - 12s - loss: 471.3787 - loglik: -4.6983e+02 - logprior: -1.5495e+00
Epoch 7/10
19/19 - 12s - loss: 471.2121 - loglik: -4.6961e+02 - logprior: -1.6020e+00
Epoch 8/10
19/19 - 11s - loss: 468.8805 - loglik: -4.6720e+02 - logprior: -1.6768e+00
Epoch 9/10
19/19 - 12s - loss: 471.2833 - loglik: -4.6955e+02 - logprior: -1.7348e+00
Fitted a model with MAP estimate = -468.0396
expansions: [(25, 1), (30, 4), (55, 1), (56, 1), (76, 1), (82, 1), (101, 1), (102, 1), (103, 1), (106, 1), (137, 3)]
discards: []
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 474.5776 - loglik: -4.7142e+02 - logprior: -3.1580e+00
Epoch 2/2
19/19 - 13s - loss: 466.1217 - loglik: -4.6497e+02 - logprior: -1.1491e+00
Fitted a model with MAP estimate = -463.4703
expansions: []
discards: [ 0 33 34]
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 472.8438 - loglik: -4.6896e+02 - logprior: -3.8800e+00
Epoch 2/2
19/19 - 13s - loss: 468.1430 - loglik: -4.6617e+02 - logprior: -1.9746e+00
Fitted a model with MAP estimate = -466.0060
expansions: [(0, 2), (151, 1)]
discards: [  0 149]
Fitting a model of length 185 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 468.4442 - loglik: -4.6554e+02 - logprior: -2.8996e+00
Epoch 2/10
19/19 - 13s - loss: 464.5880 - loglik: -4.6358e+02 - logprior: -1.0119e+00
Epoch 3/10
19/19 - 13s - loss: 462.3653 - loglik: -4.6148e+02 - logprior: -8.8638e-01
Epoch 4/10
19/19 - 13s - loss: 462.9294 - loglik: -4.6206e+02 - logprior: -8.6987e-01
Fitted a model with MAP estimate = -461.5479
Time for alignment: 282.3462
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 617.1158 - loglik: -6.1379e+02 - logprior: -3.3289e+00
Epoch 2/10
19/19 - 12s - loss: 525.1674 - loglik: -5.2389e+02 - logprior: -1.2782e+00
Epoch 3/10
19/19 - 12s - loss: 488.9174 - loglik: -4.8759e+02 - logprior: -1.3291e+00
Epoch 4/10
19/19 - 12s - loss: 479.5104 - loglik: -4.7819e+02 - logprior: -1.3184e+00
Epoch 5/10
19/19 - 12s - loss: 478.0650 - loglik: -4.7669e+02 - logprior: -1.3761e+00
Epoch 6/10
19/19 - 11s - loss: 474.5132 - loglik: -4.7306e+02 - logprior: -1.4488e+00
Epoch 7/10
19/19 - 12s - loss: 475.1399 - loglik: -4.7366e+02 - logprior: -1.4809e+00
Fitted a model with MAP estimate = -472.8070
expansions: [(25, 1), (57, 1), (76, 2), (77, 1), (103, 1), (104, 2), (139, 6)]
discards: [2]
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 478.8658 - loglik: -4.7594e+02 - logprior: -2.9213e+00
Epoch 2/2
19/19 - 13s - loss: 468.8122 - loglik: -4.6760e+02 - logprior: -1.2102e+00
Fitted a model with MAP estimate = -465.5331
expansions: []
discards: [78]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 470.9680 - loglik: -4.6793e+02 - logprior: -3.0421e+00
Epoch 2/2
19/19 - 13s - loss: 467.5337 - loglik: -4.6635e+02 - logprior: -1.1795e+00
Fitted a model with MAP estimate = -465.0818
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 183 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 471.7638 - loglik: -4.6790e+02 - logprior: -3.8608e+00
Epoch 2/10
19/19 - 13s - loss: 467.3060 - loglik: -4.6517e+02 - logprior: -2.1323e+00
Epoch 3/10
19/19 - 13s - loss: 464.8915 - loglik: -4.6300e+02 - logprior: -1.8888e+00
Epoch 4/10
19/19 - 12s - loss: 465.6548 - loglik: -4.6440e+02 - logprior: -1.2523e+00
Fitted a model with MAP estimate = -463.2111
Time for alignment: 257.5001
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 617.4893 - loglik: -6.1416e+02 - logprior: -3.3249e+00
Epoch 2/10
19/19 - 12s - loss: 521.3322 - loglik: -5.2004e+02 - logprior: -1.2948e+00
Epoch 3/10
19/19 - 12s - loss: 491.3438 - loglik: -4.9001e+02 - logprior: -1.3307e+00
Epoch 4/10
19/19 - 12s - loss: 477.0489 - loglik: -4.7575e+02 - logprior: -1.2995e+00
Epoch 5/10
19/19 - 12s - loss: 473.0025 - loglik: -4.7163e+02 - logprior: -1.3742e+00
Epoch 6/10
19/19 - 12s - loss: 471.2348 - loglik: -4.6980e+02 - logprior: -1.4342e+00
Epoch 7/10
19/19 - 12s - loss: 469.9690 - loglik: -4.6850e+02 - logprior: -1.4658e+00
Epoch 8/10
19/19 - 12s - loss: 467.9819 - loglik: -4.6644e+02 - logprior: -1.5411e+00
Epoch 9/10
19/19 - 12s - loss: 469.1514 - loglik: -4.6755e+02 - logprior: -1.6039e+00
Fitted a model with MAP estimate = -467.2329
expansions: [(22, 1), (36, 1), (37, 1), (56, 1), (57, 1), (75, 1), (93, 3), (103, 1), (138, 3)]
discards: [2]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 474.3768 - loglik: -4.7149e+02 - logprior: -2.8875e+00
Epoch 2/2
19/19 - 13s - loss: 467.6092 - loglik: -4.6653e+02 - logprior: -1.0835e+00
Fitted a model with MAP estimate = -464.2257
expansions: []
discards: [99]
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 469.4444 - loglik: -4.6656e+02 - logprior: -2.8863e+00
Epoch 2/2
19/19 - 12s - loss: 466.2804 - loglik: -4.6528e+02 - logprior: -1.0026e+00
Fitted a model with MAP estimate = -463.8601
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 182 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 470.9721 - loglik: -4.6726e+02 - logprior: -3.7100e+00
Epoch 2/10
19/19 - 12s - loss: 466.1632 - loglik: -4.6419e+02 - logprior: -1.9690e+00
Epoch 3/10
19/19 - 13s - loss: 464.8854 - loglik: -4.6317e+02 - logprior: -1.7183e+00
Epoch 4/10
19/19 - 12s - loss: 463.5216 - loglik: -4.6247e+02 - logprior: -1.0542e+00
Epoch 5/10
19/19 - 12s - loss: 462.9055 - loglik: -4.6207e+02 - logprior: -8.3768e-01
Epoch 6/10
19/19 - 13s - loss: 462.1506 - loglik: -4.6126e+02 - logprior: -8.9152e-01
Epoch 7/10
19/19 - 12s - loss: 462.1120 - loglik: -4.6121e+02 - logprior: -9.0356e-01
Epoch 8/10
19/19 - 12s - loss: 460.1807 - loglik: -4.5923e+02 - logprior: -9.4572e-01
Epoch 9/10
19/19 - 12s - loss: 459.2130 - loglik: -4.5824e+02 - logprior: -9.7003e-01
Epoch 10/10
19/19 - 13s - loss: 464.1781 - loglik: -4.6318e+02 - logprior: -9.9985e-01
Fitted a model with MAP estimate = -460.6724
Time for alignment: 354.5797
Computed alignments with likelihoods: ['-461.5479', '-463.2111', '-460.6724']
Best model has likelihood: -460.6724
SP score = 0.7749
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5236936e50>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f529538d7c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529538d940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523824dfd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8545 - loglik: -1.3946e+02 - logprior: -3.3909e+00
Epoch 2/10
19/19 - 1s - loss: 114.0061 - loglik: -1.1255e+02 - logprior: -1.4528e+00
Epoch 3/10
19/19 - 1s - loss: 103.9339 - loglik: -1.0234e+02 - logprior: -1.5968e+00
Epoch 4/10
19/19 - 1s - loss: 101.9101 - loglik: -1.0045e+02 - logprior: -1.4592e+00
Epoch 5/10
19/19 - 1s - loss: 101.3071 - loglik: -9.9823e+01 - logprior: -1.4842e+00
Epoch 6/10
19/19 - 1s - loss: 100.9784 - loglik: -9.9500e+01 - logprior: -1.4784e+00
Epoch 7/10
19/19 - 1s - loss: 100.8155 - loglik: -9.9342e+01 - logprior: -1.4738e+00
Epoch 8/10
19/19 - 1s - loss: 100.8404 - loglik: -9.9352e+01 - logprior: -1.4883e+00
Fitted a model with MAP estimate = -100.6914
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.9285 - loglik: -1.0367e+02 - logprior: -4.2554e+00
Epoch 2/2
19/19 - 1s - loss: 99.8809 - loglik: -9.7731e+01 - logprior: -2.1495e+00
Fitted a model with MAP estimate = -98.3128
expansions: [(0, 1)]
discards: [ 0  8 17 44]
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5281 - loglik: -9.7297e+01 - logprior: -3.2314e+00
Epoch 2/2
19/19 - 1s - loss: 96.7722 - loglik: -9.5277e+01 - logprior: -1.4955e+00
Fitted a model with MAP estimate = -96.2480
expansions: []
discards: [26 27 38]
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.9376 - loglik: -9.6689e+01 - logprior: -3.2491e+00
Epoch 2/10
19/19 - 1s - loss: 97.2961 - loglik: -9.5856e+01 - logprior: -1.4405e+00
Epoch 3/10
19/19 - 1s - loss: 96.8201 - loglik: -9.5466e+01 - logprior: -1.3544e+00
Epoch 4/10
19/19 - 1s - loss: 96.6271 - loglik: -9.5315e+01 - logprior: -1.3120e+00
Epoch 5/10
19/19 - 1s - loss: 96.3096 - loglik: -9.5005e+01 - logprior: -1.3041e+00
Epoch 6/10
19/19 - 1s - loss: 96.3897 - loglik: -9.5088e+01 - logprior: -1.3017e+00
Fitted a model with MAP estimate = -96.1627
Time for alignment: 46.4484
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 142.8441 - loglik: -1.3945e+02 - logprior: -3.3922e+00
Epoch 2/10
19/19 - 1s - loss: 114.1931 - loglik: -1.1275e+02 - logprior: -1.4477e+00
Epoch 3/10
19/19 - 1s - loss: 104.7234 - loglik: -1.0313e+02 - logprior: -1.5901e+00
Epoch 4/10
19/19 - 1s - loss: 102.4662 - loglik: -1.0102e+02 - logprior: -1.4488e+00
Epoch 5/10
19/19 - 1s - loss: 102.0200 - loglik: -1.0054e+02 - logprior: -1.4787e+00
Epoch 6/10
19/19 - 1s - loss: 101.6706 - loglik: -1.0021e+02 - logprior: -1.4652e+00
Epoch 7/10
19/19 - 1s - loss: 101.4801 - loglik: -1.0002e+02 - logprior: -1.4615e+00
Epoch 8/10
19/19 - 1s - loss: 101.5917 - loglik: -1.0012e+02 - logprior: -1.4752e+00
Fitted a model with MAP estimate = -101.4181
expansions: [(6, 1), (7, 2), (8, 1), (13, 1), (20, 2), (21, 1), (27, 1), (28, 3), (31, 2)]
discards: [0]
Fitting a model of length 53 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.0025 - loglik: -1.0278e+02 - logprior: -4.2274e+00
Epoch 2/2
19/19 - 1s - loss: 99.7884 - loglik: -9.7710e+01 - logprior: -2.0782e+00
Fitted a model with MAP estimate = -98.2421
expansions: [(0, 1)]
discards: [ 0  8 25 37 42]
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.9702 - loglik: -9.7778e+01 - logprior: -3.1923e+00
Epoch 2/2
19/19 - 1s - loss: 97.5301 - loglik: -9.6071e+01 - logprior: -1.4593e+00
Fitted a model with MAP estimate = -97.0128
expansions: []
discards: []
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.6463 - loglik: -9.6399e+01 - logprior: -3.2470e+00
Epoch 2/10
19/19 - 1s - loss: 97.3062 - loglik: -9.5866e+01 - logprior: -1.4398e+00
Epoch 3/10
19/19 - 1s - loss: 96.9322 - loglik: -9.5583e+01 - logprior: -1.3495e+00
Epoch 4/10
19/19 - 1s - loss: 96.4080 - loglik: -9.5095e+01 - logprior: -1.3130e+00
Epoch 5/10
19/19 - 1s - loss: 96.5360 - loglik: -9.5232e+01 - logprior: -1.3037e+00
Fitted a model with MAP estimate = -96.2772
Time for alignment: 44.1244
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 143.0398 - loglik: -1.3965e+02 - logprior: -3.3894e+00
Epoch 2/10
19/19 - 1s - loss: 114.5165 - loglik: -1.1307e+02 - logprior: -1.4461e+00
Epoch 3/10
19/19 - 1s - loss: 104.1558 - loglik: -1.0256e+02 - logprior: -1.5941e+00
Epoch 4/10
19/19 - 1s - loss: 102.0277 - loglik: -1.0057e+02 - logprior: -1.4566e+00
Epoch 5/10
19/19 - 1s - loss: 101.2712 - loglik: -9.9788e+01 - logprior: -1.4829e+00
Epoch 6/10
19/19 - 1s - loss: 100.8660 - loglik: -9.9390e+01 - logprior: -1.4762e+00
Epoch 7/10
19/19 - 1s - loss: 101.0758 - loglik: -9.9596e+01 - logprior: -1.4795e+00
Fitted a model with MAP estimate = -100.7496
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.8999 - loglik: -1.0365e+02 - logprior: -4.2531e+00
Epoch 2/2
19/19 - 1s - loss: 99.9261 - loglik: -9.7789e+01 - logprior: -2.1368e+00
Fitted a model with MAP estimate = -98.2505
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.6838 - loglik: -9.7470e+01 - logprior: -3.2142e+00
Epoch 2/2
19/19 - 1s - loss: 97.0427 - loglik: -9.5566e+01 - logprior: -1.4765e+00
Fitted a model with MAP estimate = -96.5453
expansions: []
discards: [36]
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.7942 - loglik: -9.6544e+01 - logprior: -3.2499e+00
Epoch 2/10
19/19 - 1s - loss: 97.2182 - loglik: -9.5783e+01 - logprior: -1.4347e+00
Epoch 3/10
19/19 - 1s - loss: 96.8585 - loglik: -9.5506e+01 - logprior: -1.3526e+00
Epoch 4/10
19/19 - 1s - loss: 96.6140 - loglik: -9.5296e+01 - logprior: -1.3179e+00
Epoch 5/10
19/19 - 1s - loss: 96.3243 - loglik: -9.5024e+01 - logprior: -1.3006e+00
Epoch 6/10
19/19 - 1s - loss: 96.2438 - loglik: -9.4943e+01 - logprior: -1.3010e+00
Epoch 7/10
19/19 - 1s - loss: 96.2989 - loglik: -9.4993e+01 - logprior: -1.3063e+00
Fitted a model with MAP estimate = -96.1119
Time for alignment: 45.9069
Computed alignments with likelihoods: ['-96.1627', '-96.2772', '-96.1119']
Best model has likelihood: -96.1119
SP score = 0.7730
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f514976af40>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f51286d0d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52391cc040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52391cc1c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 458.1248 - loglik: -4.5463e+02 - logprior: -3.4920e+00
Epoch 2/10
19/19 - 5s - loss: 379.4468 - loglik: -3.7800e+02 - logprior: -1.4479e+00
Epoch 3/10
19/19 - 5s - loss: 350.5713 - loglik: -3.4880e+02 - logprior: -1.7726e+00
Epoch 4/10
19/19 - 5s - loss: 343.9403 - loglik: -3.4207e+02 - logprior: -1.8657e+00
Epoch 5/10
19/19 - 5s - loss: 341.2130 - loglik: -3.3944e+02 - logprior: -1.7755e+00
Epoch 6/10
19/19 - 5s - loss: 340.9583 - loglik: -3.3922e+02 - logprior: -1.7414e+00
Epoch 7/10
19/19 - 5s - loss: 339.9812 - loglik: -3.3823e+02 - logprior: -1.7482e+00
Epoch 8/10
19/19 - 5s - loss: 339.1599 - loglik: -3.3739e+02 - logprior: -1.7717e+00
Epoch 9/10
19/19 - 5s - loss: 339.8333 - loglik: -3.3806e+02 - logprior: -1.7764e+00
Fitted a model with MAP estimate = -336.0747
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (70, 1), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 334.5604 - loglik: -3.3063e+02 - logprior: -3.9310e+00
Epoch 2/2
19/19 - 7s - loss: 317.8795 - loglik: -3.1675e+02 - logprior: -1.1269e+00
Fitted a model with MAP estimate = -313.0987
expansions: []
discards: [  0  54 112 144]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 323.3352 - loglik: -3.1950e+02 - logprior: -3.8344e+00
Epoch 2/2
19/19 - 7s - loss: 317.8333 - loglik: -3.1622e+02 - logprior: -1.6178e+00
Fitted a model with MAP estimate = -313.3765
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 184 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 316.2742 - loglik: -3.1351e+02 - logprior: -2.7689e+00
Epoch 2/10
19/19 - 7s - loss: 312.8825 - loglik: -3.1201e+02 - logprior: -8.7237e-01
Epoch 3/10
19/19 - 7s - loss: 312.2385 - loglik: -3.1144e+02 - logprior: -7.9369e-01
Epoch 4/10
19/19 - 7s - loss: 311.0022 - loglik: -3.1025e+02 - logprior: -7.4893e-01
Epoch 5/10
19/19 - 7s - loss: 310.3098 - loglik: -3.0957e+02 - logprior: -7.3742e-01
Epoch 6/10
19/19 - 7s - loss: 309.0458 - loglik: -3.0832e+02 - logprior: -7.2586e-01
Epoch 7/10
19/19 - 7s - loss: 309.9329 - loglik: -3.0921e+02 - logprior: -7.2615e-01
Fitted a model with MAP estimate = -308.6919
Time for alignment: 173.7655
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 458.6462 - loglik: -4.5516e+02 - logprior: -3.4892e+00
Epoch 2/10
19/19 - 5s - loss: 382.4311 - loglik: -3.8102e+02 - logprior: -1.4063e+00
Epoch 3/10
19/19 - 5s - loss: 351.0919 - loglik: -3.4938e+02 - logprior: -1.7155e+00
Epoch 4/10
19/19 - 5s - loss: 345.0165 - loglik: -3.4318e+02 - logprior: -1.8340e+00
Epoch 5/10
19/19 - 5s - loss: 342.2830 - loglik: -3.4049e+02 - logprior: -1.7928e+00
Epoch 6/10
19/19 - 5s - loss: 342.0042 - loglik: -3.4023e+02 - logprior: -1.7740e+00
Epoch 7/10
19/19 - 5s - loss: 340.8936 - loglik: -3.3912e+02 - logprior: -1.7753e+00
Epoch 8/10
19/19 - 5s - loss: 341.0543 - loglik: -3.3926e+02 - logprior: -1.7972e+00
Fitted a model with MAP estimate = -337.5391
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (23, 1), (38, 1), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 1), (80, 1), (87, 1), (88, 1), (89, 2), (99, 2), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 336.4773 - loglik: -3.3263e+02 - logprior: -3.8439e+00
Epoch 2/2
19/19 - 7s - loss: 320.7315 - loglik: -3.1963e+02 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -316.2077
expansions: []
discards: [  0 114]
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 324.8600 - loglik: -3.2122e+02 - logprior: -3.6421e+00
Epoch 2/2
19/19 - 7s - loss: 319.7982 - loglik: -3.1866e+02 - logprior: -1.1386e+00
Fitted a model with MAP estimate = -316.0498
expansions: []
discards: [0]
Fitting a model of length 181 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 320.5505 - loglik: -3.1765e+02 - logprior: -2.9036e+00
Epoch 2/10
19/19 - 7s - loss: 316.8257 - loglik: -3.1601e+02 - logprior: -8.1455e-01
Epoch 3/10
19/19 - 7s - loss: 315.6560 - loglik: -3.1496e+02 - logprior: -6.9199e-01
Epoch 4/10
19/19 - 7s - loss: 314.7709 - loglik: -3.1411e+02 - logprior: -6.6245e-01
Epoch 5/10
19/19 - 7s - loss: 313.7281 - loglik: -3.1308e+02 - logprior: -6.4752e-01
Epoch 6/10
19/19 - 7s - loss: 312.8317 - loglik: -3.1221e+02 - logprior: -6.2495e-01
Epoch 7/10
19/19 - 7s - loss: 312.5721 - loglik: -3.1193e+02 - logprior: -6.4186e-01
Epoch 8/10
19/19 - 7s - loss: 312.1822 - loglik: -3.1154e+02 - logprior: -6.3941e-01
Epoch 9/10
19/19 - 7s - loss: 311.7612 - loglik: -3.1111e+02 - logprior: -6.4752e-01
Epoch 10/10
19/19 - 7s - loss: 312.4228 - loglik: -3.1176e+02 - logprior: -6.5783e-01
Fitted a model with MAP estimate = -311.5146
Time for alignment: 187.8133
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.9878 - loglik: -4.5551e+02 - logprior: -3.4799e+00
Epoch 2/10
19/19 - 5s - loss: 382.4216 - loglik: -3.8097e+02 - logprior: -1.4534e+00
Epoch 3/10
19/19 - 5s - loss: 350.1996 - loglik: -3.4842e+02 - logprior: -1.7837e+00
Epoch 4/10
19/19 - 5s - loss: 343.1383 - loglik: -3.4127e+02 - logprior: -1.8635e+00
Epoch 5/10
19/19 - 5s - loss: 341.1791 - loglik: -3.3937e+02 - logprior: -1.8052e+00
Epoch 6/10
19/19 - 5s - loss: 339.6754 - loglik: -3.3789e+02 - logprior: -1.7859e+00
Epoch 7/10
19/19 - 5s - loss: 339.0968 - loglik: -3.3731e+02 - logprior: -1.7879e+00
Epoch 8/10
19/19 - 5s - loss: 338.3758 - loglik: -3.3656e+02 - logprior: -1.8141e+00
Epoch 9/10
19/19 - 5s - loss: 338.2965 - loglik: -3.3646e+02 - logprior: -1.8318e+00
Epoch 10/10
19/19 - 5s - loss: 338.6837 - loglik: -3.3683e+02 - logprior: -1.8585e+00
Fitted a model with MAP estimate = -335.0049
expansions: [(0, 2), (11, 1), (13, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 2), (51, 1), (58, 1), (66, 1), (68, 1), (70, 1), (80, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 334.3024 - loglik: -3.3033e+02 - logprior: -3.9768e+00
Epoch 2/2
19/19 - 7s - loss: 318.8377 - loglik: -3.1766e+02 - logprior: -1.1791e+00
Fitted a model with MAP estimate = -313.5639
expansions: []
discards: [  0 114]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 322.6708 - loglik: -3.1876e+02 - logprior: -3.9155e+00
Epoch 2/2
19/19 - 7s - loss: 318.5357 - loglik: -3.1684e+02 - logprior: -1.6933e+00
Fitted a model with MAP estimate = -313.8168
expansions: [(0, 2)]
discards: [ 0 53]
Fitting a model of length 183 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 316.8851 - loglik: -3.1405e+02 - logprior: -2.8356e+00
Epoch 2/10
19/19 - 7s - loss: 313.5457 - loglik: -3.1262e+02 - logprior: -9.2121e-01
Epoch 3/10
19/19 - 7s - loss: 312.1364 - loglik: -3.1127e+02 - logprior: -8.6421e-01
Epoch 4/10
19/19 - 7s - loss: 312.0935 - loglik: -3.1128e+02 - logprior: -8.1619e-01
Epoch 5/10
19/19 - 7s - loss: 310.6627 - loglik: -3.0986e+02 - logprior: -8.0654e-01
Epoch 6/10
19/19 - 7s - loss: 310.0254 - loglik: -3.0923e+02 - logprior: -7.9806e-01
Epoch 7/10
19/19 - 7s - loss: 309.7169 - loglik: -3.0892e+02 - logprior: -7.9863e-01
Epoch 8/10
19/19 - 7s - loss: 309.4872 - loglik: -3.0864e+02 - logprior: -8.5209e-01
Epoch 9/10
19/19 - 7s - loss: 307.6279 - loglik: -3.0677e+02 - logprior: -8.5971e-01
Epoch 10/10
19/19 - 7s - loss: 310.5469 - loglik: -3.0968e+02 - logprior: -8.6803e-01
Fitted a model with MAP estimate = -308.6012
Time for alignment: 198.2742
Computed alignments with likelihoods: ['-308.6919', '-311.5146', '-308.6012']
Best model has likelihood: -308.6012
SP score = 0.3180
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f51286c3790>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f521d05b220>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5237bfdb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5237bfdf70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 904.7537 - loglik: -9.0267e+02 - logprior: -2.0872e+00
Epoch 2/10
39/39 - 35s - loss: 727.1204 - loglik: -7.2532e+02 - logprior: -1.7979e+00
Epoch 3/10
39/39 - 35s - loss: 709.2244 - loglik: -7.0729e+02 - logprior: -1.9301e+00
Epoch 4/10
39/39 - 35s - loss: 703.9549 - loglik: -7.0192e+02 - logprior: -2.0345e+00
Epoch 5/10
39/39 - 35s - loss: 700.8394 - loglik: -6.9863e+02 - logprior: -2.2116e+00
Epoch 6/10
39/39 - 35s - loss: 699.1450 - loglik: -6.9673e+02 - logprior: -2.4134e+00
Epoch 7/10
39/39 - 35s - loss: 698.3374 - loglik: -6.9552e+02 - logprior: -2.8156e+00
Epoch 8/10
39/39 - 35s - loss: 697.6874 - loglik: -6.9466e+02 - logprior: -3.0227e+00
Epoch 9/10
39/39 - 35s - loss: 697.1683 - loglik: -6.9406e+02 - logprior: -3.1069e+00
Epoch 10/10
39/39 - 35s - loss: 696.6757 - loglik: -6.9340e+02 - logprior: -3.2724e+00
Fitted a model with MAP estimate = -696.0191
expansions: [(0, 5), (36, 1), (45, 2), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 3), (84, 3), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (228, 1), (229, 1), (232, 3), (233, 1), (258, 4), (259, 2), (260, 2), (261, 1), (277, 1), (279, 1), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 681.4661 - loglik: -6.7878e+02 - logprior: -2.6908e+00
Epoch 2/2
39/39 - 51s - loss: 659.5967 - loglik: -6.5864e+02 - logprior: -9.5394e-01
Fitted a model with MAP estimate = -655.7254
expansions: []
discards: [  1   2  99 102 149 154 233 324]
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 664.7747 - loglik: -6.6303e+02 - logprior: -1.7483e+00
Epoch 2/2
39/39 - 49s - loss: 659.2542 - loglik: -6.5866e+02 - logprior: -5.9796e-01
Fitted a model with MAP estimate = -656.3999
expansions: []
discards: []
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 663.2585 - loglik: -6.6179e+02 - logprior: -1.4677e+00
Epoch 2/10
39/39 - 49s - loss: 658.4299 - loglik: -6.5820e+02 - logprior: -2.3494e-01
Epoch 3/10
39/39 - 49s - loss: 655.5474 - loglik: -6.5541e+02 - logprior: -1.3538e-01
Epoch 4/10
39/39 - 49s - loss: 652.2841 - loglik: -6.5213e+02 - logprior: -1.5447e-01
Epoch 5/10
39/39 - 49s - loss: 649.0035 - loglik: -6.4898e+02 - logprior: -2.2184e-02
Epoch 6/10
39/39 - 49s - loss: 648.1480 - loglik: -6.4816e+02 - logprior: 0.0086
Epoch 7/10
39/39 - 49s - loss: 646.8765 - loglik: -6.4684e+02 - logprior: -3.5785e-02
Epoch 8/10
39/39 - 49s - loss: 645.9279 - loglik: -6.4588e+02 - logprior: -5.0971e-02
Epoch 9/10
39/39 - 49s - loss: 646.3204 - loglik: -6.4600e+02 - logprior: -3.1971e-01
Fitted a model with MAP estimate = -645.2981
Time for alignment: 1230.8165
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 903.4164 - loglik: -9.0132e+02 - logprior: -2.0964e+00
Epoch 2/10
39/39 - 35s - loss: 722.1333 - loglik: -7.2018e+02 - logprior: -1.9497e+00
Epoch 3/10
39/39 - 35s - loss: 708.3802 - loglik: -7.0628e+02 - logprior: -2.0962e+00
Epoch 4/10
39/39 - 35s - loss: 703.0886 - loglik: -7.0092e+02 - logprior: -2.1651e+00
Epoch 5/10
39/39 - 35s - loss: 700.5997 - loglik: -6.9824e+02 - logprior: -2.3644e+00
Epoch 6/10
39/39 - 35s - loss: 698.7443 - loglik: -6.9620e+02 - logprior: -2.5439e+00
Epoch 7/10
39/39 - 35s - loss: 697.4344 - loglik: -6.9454e+02 - logprior: -2.8966e+00
Epoch 8/10
39/39 - 35s - loss: 696.7362 - loglik: -6.9365e+02 - logprior: -3.0893e+00
Epoch 9/10
39/39 - 35s - loss: 696.3710 - loglik: -6.9311e+02 - logprior: -3.2655e+00
Epoch 10/10
39/39 - 35s - loss: 696.1320 - loglik: -6.9279e+02 - logprior: -3.3465e+00
Fitted a model with MAP estimate = -694.9809
expansions: [(0, 5), (36, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 3), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (232, 1), (256, 1), (257, 4), (258, 1), (259, 1), (260, 1), (261, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 681.7017 - loglik: -6.7884e+02 - logprior: -2.8636e+00
Epoch 2/2
39/39 - 51s - loss: 659.2415 - loglik: -6.5817e+02 - logprior: -1.0695e+00
Fitted a model with MAP estimate = -654.7790
expansions: []
discards: [  1   2 148 153 232 319 350 365 366]
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 663.7827 - loglik: -6.6209e+02 - logprior: -1.6950e+00
Epoch 2/2
39/39 - 49s - loss: 658.5200 - loglik: -6.5807e+02 - logprior: -4.4760e-01
Fitted a model with MAP estimate = -655.3781
expansions: []
discards: [100]
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 662.9064 - loglik: -6.6155e+02 - logprior: -1.3608e+00
Epoch 2/10
39/39 - 49s - loss: 658.5360 - loglik: -6.5825e+02 - logprior: -2.8798e-01
Epoch 3/10
39/39 - 49s - loss: 655.2036 - loglik: -6.5505e+02 - logprior: -1.5048e-01
Epoch 4/10
39/39 - 49s - loss: 651.8066 - loglik: -6.5178e+02 - logprior: -2.4011e-02
Epoch 5/10
39/39 - 49s - loss: 649.2644 - loglik: -6.4930e+02 - logprior: 0.0391
Epoch 6/10
39/39 - 49s - loss: 647.1981 - loglik: -6.4716e+02 - logprior: -3.7382e-02
Epoch 7/10
39/39 - 49s - loss: 646.4169 - loglik: -6.4618e+02 - logprior: -2.4102e-01
Epoch 8/10
39/39 - 49s - loss: 646.3477 - loglik: -6.4611e+02 - logprior: -2.3369e-01
Epoch 9/10
39/39 - 49s - loss: 645.0388 - loglik: -6.4486e+02 - logprior: -1.8124e-01
Epoch 10/10
39/39 - 49s - loss: 644.9460 - loglik: -6.4481e+02 - logprior: -1.3250e-01
Fitted a model with MAP estimate = -644.7287
Time for alignment: 1279.3737
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 906.1294 - loglik: -9.0401e+02 - logprior: -2.1238e+00
Epoch 2/10
39/39 - 35s - loss: 722.6832 - loglik: -7.2075e+02 - logprior: -1.9300e+00
Epoch 3/10
39/39 - 35s - loss: 705.8091 - loglik: -7.0369e+02 - logprior: -2.1223e+00
Epoch 4/10
39/39 - 35s - loss: 701.2803 - loglik: -6.9904e+02 - logprior: -2.2441e+00
Epoch 5/10
39/39 - 35s - loss: 698.4809 - loglik: -6.9613e+02 - logprior: -2.3488e+00
Epoch 6/10
39/39 - 35s - loss: 696.6450 - loglik: -6.9405e+02 - logprior: -2.5916e+00
Epoch 7/10
39/39 - 35s - loss: 696.5624 - loglik: -6.9362e+02 - logprior: -2.9424e+00
Epoch 8/10
39/39 - 35s - loss: 695.1483 - loglik: -6.9203e+02 - logprior: -3.1209e+00
Epoch 9/10
39/39 - 35s - loss: 695.2669 - loglik: -6.9202e+02 - logprior: -3.2442e+00
Fitted a model with MAP estimate = -693.8201
expansions: [(0, 5), (43, 1), (46, 1), (61, 1), (63, 2), (67, 1), (73, 1), (74, 1), (80, 1), (83, 1), (84, 1), (85, 3), (91, 1), (92, 1), (103, 1), (113, 1), (114, 1), (119, 1), (123, 2), (126, 2), (127, 1), (143, 1), (145, 1), (147, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (177, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (228, 1), (229, 1), (230, 1), (231, 4), (233, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 1), (277, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 682.0704 - loglik: -6.7931e+02 - logprior: -2.7646e+00
Epoch 2/2
39/39 - 51s - loss: 659.6955 - loglik: -6.5863e+02 - logprior: -1.0703e+00
Fitted a model with MAP estimate = -655.4208
expansions: []
discards: [  1   2   3 148 153 232 288 324 351]
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 664.6306 - loglik: -6.6304e+02 - logprior: -1.5881e+00
Epoch 2/2
39/39 - 49s - loss: 659.5286 - loglik: -6.5910e+02 - logprior: -4.2461e-01
Fitted a model with MAP estimate = -656.1773
expansions: [(1, 1)]
discards: []
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 662.6450 - loglik: -6.6117e+02 - logprior: -1.4707e+00
Epoch 2/10
39/39 - 49s - loss: 657.5372 - loglik: -6.5725e+02 - logprior: -2.9118e-01
Epoch 3/10
39/39 - 49s - loss: 654.8288 - loglik: -6.5469e+02 - logprior: -1.4201e-01
Epoch 4/10
39/39 - 49s - loss: 651.0874 - loglik: -6.5093e+02 - logprior: -1.5917e-01
Epoch 5/10
39/39 - 49s - loss: 649.7985 - loglik: -6.4972e+02 - logprior: -7.4566e-02
Epoch 6/10
39/39 - 49s - loss: 647.0864 - loglik: -6.4686e+02 - logprior: -2.3100e-01
Epoch 7/10
39/39 - 49s - loss: 645.2509 - loglik: -6.4509e+02 - logprior: -1.6208e-01
Epoch 8/10
39/39 - 49s - loss: 645.1310 - loglik: -6.4507e+02 - logprior: -5.9391e-02
Epoch 9/10
39/39 - 49s - loss: 644.8979 - loglik: -6.4487e+02 - logprior: -2.3878e-02
Epoch 10/10
39/39 - 49s - loss: 643.8914 - loglik: -6.4389e+02 - logprior: 0.0012
Fitted a model with MAP estimate = -643.7729
Time for alignment: 1247.8689
Computed alignments with likelihoods: ['-645.2981', '-644.7287', '-643.7729']
Best model has likelihood: -643.7729
SP score = 0.9448
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f523c9434c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52369baac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514a8318e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f521d3022e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 691.7238 - loglik: -6.8820e+02 - logprior: -3.5189e+00
Epoch 2/10
19/19 - 13s - loss: 601.4980 - loglik: -6.0023e+02 - logprior: -1.2676e+00
Epoch 3/10
19/19 - 13s - loss: 558.8639 - loglik: -5.5730e+02 - logprior: -1.5645e+00
Epoch 4/10
19/19 - 13s - loss: 550.4826 - loglik: -5.4866e+02 - logprior: -1.8265e+00
Epoch 5/10
19/19 - 13s - loss: 546.2563 - loglik: -5.4428e+02 - logprior: -1.9729e+00
Epoch 6/10
19/19 - 13s - loss: 543.8166 - loglik: -5.4173e+02 - logprior: -2.0854e+00
Epoch 7/10
19/19 - 13s - loss: 544.6265 - loglik: -5.4246e+02 - logprior: -2.1684e+00
Fitted a model with MAP estimate = -543.2334
expansions: [(4, 1), (6, 1), (33, 1), (37, 1), (84, 3), (89, 1), (90, 1), (91, 1), (115, 1), (116, 4), (117, 1), (118, 3), (119, 1), (120, 2), (128, 2), (130, 2), (131, 4), (135, 1), (138, 1), (140, 2), (141, 1), (142, 8), (143, 1), (145, 1), (147, 1), (154, 1), (156, 4), (159, 1), (160, 3), (161, 1), (172, 4), (174, 1), (186, 1)]
discards: [  0 162 163 164 165 166 167 168 169 170 175 176 177 178 179 180 181 187
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Fitting a model of length 232 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 570.7717 - loglik: -5.6664e+02 - logprior: -4.1354e+00
Epoch 2/2
19/19 - 15s - loss: 554.4178 - loglik: -5.5264e+02 - logprior: -1.7812e+00
Fitted a model with MAP estimate = -550.1026
expansions: [(210, 1), (215, 2), (232, 22)]
discards: [  0   1 127 128 134 155 178 179 180 194 195 196 197 198 199 204 205 217
 218 219 220 221 222 223]
Fitting a model of length 233 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 18s - loss: 559.9615 - loglik: -5.5617e+02 - logprior: -3.7946e+00
Epoch 2/2
19/19 - 15s - loss: 548.3387 - loglik: -5.4679e+02 - logprior: -1.5488e+00
Fitted a model with MAP estimate = -544.6473
expansions: [(0, 2), (2, 1), (198, 9), (199, 2), (200, 1), (206, 2), (233, 4)]
discards: [  0 171 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226
 227 228 229 230 231 232]
Fitting a model of length 230 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 556.5823 - loglik: -5.5374e+02 - logprior: -2.8392e+00
Epoch 2/10
19/19 - 15s - loss: 547.9606 - loglik: -5.4701e+02 - logprior: -9.4794e-01
Epoch 3/10
19/19 - 15s - loss: 544.2629 - loglik: -5.4359e+02 - logprior: -6.7562e-01
Epoch 4/10
19/19 - 15s - loss: 542.9169 - loglik: -5.4223e+02 - logprior: -6.9024e-01
Epoch 5/10
19/19 - 15s - loss: 542.2950 - loglik: -5.4161e+02 - logprior: -6.8956e-01
Epoch 6/10
19/19 - 15s - loss: 541.1068 - loglik: -5.4040e+02 - logprior: -7.0646e-01
Epoch 7/10
19/19 - 15s - loss: 541.6484 - loglik: -5.4092e+02 - logprior: -7.2857e-01
Fitted a model with MAP estimate = -540.2322
Time for alignment: 350.6879
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 691.0093 - loglik: -6.8749e+02 - logprior: -3.5182e+00
Epoch 2/10
19/19 - 13s - loss: 600.0466 - loglik: -5.9884e+02 - logprior: -1.2028e+00
Epoch 3/10
19/19 - 13s - loss: 557.9950 - loglik: -5.5653e+02 - logprior: -1.4691e+00
Epoch 4/10
19/19 - 13s - loss: 547.6024 - loglik: -5.4591e+02 - logprior: -1.6920e+00
Epoch 5/10
19/19 - 13s - loss: 546.9117 - loglik: -5.4509e+02 - logprior: -1.8188e+00
Epoch 6/10
19/19 - 13s - loss: 543.9034 - loglik: -5.4199e+02 - logprior: -1.9167e+00
Epoch 7/10
19/19 - 13s - loss: 542.2417 - loglik: -5.4025e+02 - logprior: -1.9910e+00
Epoch 8/10
19/19 - 13s - loss: 542.7695 - loglik: -5.4069e+02 - logprior: -2.0832e+00
Fitted a model with MAP estimate = -541.9992
expansions: [(4, 1), (6, 1), (33, 1), (83, 7), (85, 2), (90, 1), (91, 1), (92, 1), (116, 1), (117, 7), (118, 2), (119, 1), (120, 2), (127, 1), (128, 1), (130, 6), (133, 1), (134, 1), (136, 1), (139, 11), (140, 1), (142, 1), (144, 1), (148, 1), (151, 2), (154, 1), (157, 1), (158, 3), (159, 1), (170, 5)]
discards: [  0 160 161 162 163 164 165 166 167 168 174 175 176 177 178 179 180 185
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Fitting a model of length 235 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 569.5573 - loglik: -5.6544e+02 - logprior: -4.1152e+00
Epoch 2/2
19/19 - 15s - loss: 552.4869 - loglik: -5.5077e+02 - logprior: -1.7147e+00
Fitted a model with MAP estimate = -548.8642
expansions: [(214, 1), (228, 1), (235, 22)]
discards: [  0   1  89 134 135 136 161 183 184 221 222 223 224 225]
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 552.2551 - loglik: -5.4857e+02 - logprior: -3.6886e+00
Epoch 2/2
19/19 - 16s - loss: 540.7892 - loglik: -5.3928e+02 - logprior: -1.5079e+00
Fitted a model with MAP estimate = -537.9696
expansions: [(0, 2), (2, 2), (219, 3), (245, 4)]
discards: [  0 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238
 239 240 241 242 243 244]
Fitting a model of length 232 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 548.1651 - loglik: -5.4530e+02 - logprior: -2.8661e+00
Epoch 2/10
19/19 - 15s - loss: 543.3501 - loglik: -5.4243e+02 - logprior: -9.2267e-01
Epoch 3/10
19/19 - 15s - loss: 540.6867 - loglik: -5.4004e+02 - logprior: -6.4587e-01
Epoch 4/10
19/19 - 15s - loss: 539.3136 - loglik: -5.3872e+02 - logprior: -5.8971e-01
Epoch 5/10
19/19 - 15s - loss: 538.8306 - loglik: -5.3822e+02 - logprior: -6.1507e-01
Epoch 6/10
19/19 - 15s - loss: 538.6743 - loglik: -5.3805e+02 - logprior: -6.2376e-01
Epoch 7/10
19/19 - 15s - loss: 537.2560 - loglik: -5.3658e+02 - logprior: -6.7907e-01
Epoch 8/10
19/19 - 15s - loss: 537.0096 - loglik: -5.3629e+02 - logprior: -7.1642e-01
Epoch 9/10
19/19 - 15s - loss: 536.6336 - loglik: -5.3584e+02 - logprior: -7.9384e-01
Epoch 10/10
19/19 - 15s - loss: 537.0171 - loglik: -5.3616e+02 - logprior: -8.5231e-01
Fitted a model with MAP estimate = -536.0990
Time for alignment: 416.4475
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 17s - loss: 690.3230 - loglik: -6.8680e+02 - logprior: -3.5211e+00
Epoch 2/10
19/19 - 13s - loss: 598.5444 - loglik: -5.9735e+02 - logprior: -1.1966e+00
Epoch 3/10
19/19 - 13s - loss: 559.1298 - loglik: -5.5769e+02 - logprior: -1.4357e+00
Epoch 4/10
19/19 - 13s - loss: 549.7781 - loglik: -5.4813e+02 - logprior: -1.6454e+00
Epoch 5/10
19/19 - 13s - loss: 546.8134 - loglik: -5.4500e+02 - logprior: -1.8099e+00
Epoch 6/10
19/19 - 13s - loss: 545.5024 - loglik: -5.4357e+02 - logprior: -1.9286e+00
Epoch 7/10
19/19 - 13s - loss: 544.5421 - loglik: -5.4251e+02 - logprior: -2.0282e+00
Epoch 8/10
19/19 - 13s - loss: 543.6735 - loglik: -5.4154e+02 - logprior: -2.1361e+00
Epoch 9/10
19/19 - 13s - loss: 543.6073 - loglik: -5.4139e+02 - logprior: -2.2215e+00
Epoch 10/10
19/19 - 13s - loss: 541.3654 - loglik: -5.3906e+02 - logprior: -2.3018e+00
Fitted a model with MAP estimate = -542.2633
expansions: [(4, 1), (6, 1), (33, 1), (87, 1), (89, 1), (90, 2), (92, 2), (116, 1), (117, 8), (118, 2), (119, 1), (120, 2), (127, 1), (128, 1), (130, 5), (133, 1), (134, 1), (136, 1), (139, 3), (140, 8), (141, 1), (143, 1), (145, 1), (158, 7), (170, 5), (172, 1), (183, 1)]
discards: [  0 149 150 151 152 153 154 155 156 159 160 161 162 163 164 168 173 174
 175 176 177 178 179 184 185 186 187 188 189 190 191 192 193 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Fitting a model of length 222 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 17s - loss: 573.5567 - loglik: -5.6940e+02 - logprior: -4.1580e+00
Epoch 2/2
19/19 - 14s - loss: 555.7889 - loglik: -5.5407e+02 - logprior: -1.7145e+00
Fitted a model with MAP estimate = -551.6626
expansions: [(85, 2), (202, 2), (214, 13), (217, 1), (219, 1), (222, 13)]
discards: [  0   1 128 129 130 131 177 178 179 203 204]
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 551.0789 - loglik: -5.4729e+02 - logprior: -3.7905e+00
Epoch 2/2
19/19 - 16s - loss: 538.6047 - loglik: -5.3698e+02 - logprior: -1.6262e+00
Fitted a model with MAP estimate = -535.7410
expansions: [(0, 2), (2, 1), (85, 4), (195, 1), (196, 1), (198, 2), (199, 2), (206, 7)]
discards: [  0 187 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233 234 235 236 237 238 239 240 241 242]
Fitting a model of length 234 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 552.1432 - loglik: -5.4928e+02 - logprior: -2.8679e+00
Epoch 2/10
19/19 - 15s - loss: 543.1514 - loglik: -5.4214e+02 - logprior: -1.0120e+00
Epoch 3/10
19/19 - 15s - loss: 538.5887 - loglik: -5.3785e+02 - logprior: -7.4190e-01
Epoch 4/10
19/19 - 15s - loss: 537.5627 - loglik: -5.3683e+02 - logprior: -7.3370e-01
Epoch 5/10
19/19 - 15s - loss: 536.5908 - loglik: -5.3586e+02 - logprior: -7.2684e-01
Epoch 6/10
19/19 - 15s - loss: 536.2061 - loglik: -5.3545e+02 - logprior: -7.5655e-01
Epoch 7/10
19/19 - 15s - loss: 534.7682 - loglik: -5.3401e+02 - logprior: -7.6012e-01
Epoch 8/10
19/19 - 15s - loss: 534.7863 - loglik: -5.3399e+02 - logprior: -7.9391e-01
Fitted a model with MAP estimate = -534.2305
Time for alignment: 406.1214
Computed alignments with likelihoods: ['-540.2322', '-536.0990', '-534.2305']
Best model has likelihood: -534.2305
SP score = 0.8542
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f524d815ee0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f523c4f5eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52369d5130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f5b8910>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 270.1824 - loglik: -2.6670e+02 - logprior: -3.4831e+00
Epoch 2/10
19/19 - 3s - loss: 199.7295 - loglik: -1.9834e+02 - logprior: -1.3939e+00
Epoch 3/10
19/19 - 3s - loss: 177.2205 - loglik: -1.7549e+02 - logprior: -1.7312e+00
Epoch 4/10
19/19 - 3s - loss: 173.2024 - loglik: -1.7154e+02 - logprior: -1.6578e+00
Epoch 5/10
19/19 - 3s - loss: 172.1284 - loglik: -1.7051e+02 - logprior: -1.6167e+00
Epoch 6/10
19/19 - 3s - loss: 171.3244 - loglik: -1.6971e+02 - logprior: -1.6118e+00
Epoch 7/10
19/19 - 3s - loss: 171.0081 - loglik: -1.6940e+02 - logprior: -1.6051e+00
Epoch 8/10
19/19 - 3s - loss: 170.7411 - loglik: -1.6910e+02 - logprior: -1.6403e+00
Epoch 9/10
19/19 - 3s - loss: 170.5578 - loglik: -1.6891e+02 - logprior: -1.6448e+00
Epoch 10/10
19/19 - 3s - loss: 170.1914 - loglik: -1.6852e+02 - logprior: -1.6712e+00
Fitted a model with MAP estimate = -170.0602
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 169.1970 - loglik: -1.6509e+02 - logprior: -4.1055e+00
Epoch 2/2
19/19 - 3s - loss: 156.7726 - loglik: -1.5470e+02 - logprior: -2.0763e+00
Fitted a model with MAP estimate = -155.1449
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.8323 - loglik: -1.5389e+02 - logprior: -2.9423e+00
Epoch 2/2
19/19 - 3s - loss: 152.6948 - loglik: -1.5157e+02 - logprior: -1.1290e+00
Fitted a model with MAP estimate = -151.9479
expansions: []
discards: [0]
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 158.9628 - loglik: -1.5510e+02 - logprior: -3.8591e+00
Epoch 2/10
19/19 - 3s - loss: 154.1015 - loglik: -1.5265e+02 - logprior: -1.4494e+00
Epoch 3/10
19/19 - 3s - loss: 152.3902 - loglik: -1.5135e+02 - logprior: -1.0373e+00
Epoch 4/10
19/19 - 3s - loss: 151.8906 - loglik: -1.5089e+02 - logprior: -9.9819e-01
Epoch 5/10
19/19 - 3s - loss: 151.0287 - loglik: -1.5005e+02 - logprior: -9.7796e-01
Epoch 6/10
19/19 - 3s - loss: 150.8712 - loglik: -1.4988e+02 - logprior: -9.9439e-01
Epoch 7/10
19/19 - 3s - loss: 150.2709 - loglik: -1.4927e+02 - logprior: -1.0037e+00
Epoch 8/10
19/19 - 3s - loss: 150.4481 - loglik: -1.4942e+02 - logprior: -1.0292e+00
Fitted a model with MAP estimate = -150.2333
Time for alignment: 97.5811
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 270.2161 - loglik: -2.6673e+02 - logprior: -3.4852e+00
Epoch 2/10
19/19 - 3s - loss: 199.7934 - loglik: -1.9840e+02 - logprior: -1.3928e+00
Epoch 3/10
19/19 - 3s - loss: 176.8291 - loglik: -1.7508e+02 - logprior: -1.7454e+00
Epoch 4/10
19/19 - 3s - loss: 173.0530 - loglik: -1.7138e+02 - logprior: -1.6683e+00
Epoch 5/10
19/19 - 3s - loss: 172.0454 - loglik: -1.7043e+02 - logprior: -1.6170e+00
Epoch 6/10
19/19 - 3s - loss: 170.9664 - loglik: -1.6936e+02 - logprior: -1.6062e+00
Epoch 7/10
19/19 - 3s - loss: 170.9069 - loglik: -1.6931e+02 - logprior: -1.6017e+00
Epoch 8/10
19/19 - 3s - loss: 170.9674 - loglik: -1.6935e+02 - logprior: -1.6214e+00
Fitted a model with MAP estimate = -170.4180
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 169.5975 - loglik: -1.6549e+02 - logprior: -4.1079e+00
Epoch 2/2
19/19 - 3s - loss: 157.2826 - loglik: -1.5521e+02 - logprior: -2.0769e+00
Fitted a model with MAP estimate = -155.2603
expansions: [(0, 2)]
discards: [ 0 42 61 62 64]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 157.0400 - loglik: -1.5410e+02 - logprior: -2.9443e+00
Epoch 2/2
19/19 - 3s - loss: 152.8094 - loglik: -1.5168e+02 - logprior: -1.1340e+00
Fitted a model with MAP estimate = -151.9931
expansions: []
discards: [0]
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 159.0381 - loglik: -1.5518e+02 - logprior: -3.8535e+00
Epoch 2/10
19/19 - 3s - loss: 154.4740 - loglik: -1.5298e+02 - logprior: -1.4891e+00
Epoch 3/10
19/19 - 3s - loss: 152.6659 - loglik: -1.5163e+02 - logprior: -1.0362e+00
Epoch 4/10
19/19 - 3s - loss: 151.6982 - loglik: -1.5069e+02 - logprior: -1.0125e+00
Epoch 5/10
19/19 - 3s - loss: 151.9031 - loglik: -1.5090e+02 - logprior: -1.0049e+00
Fitted a model with MAP estimate = -151.1524
Time for alignment: 82.6068
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 270.0439 - loglik: -2.6656e+02 - logprior: -3.4882e+00
Epoch 2/10
19/19 - 3s - loss: 199.7740 - loglik: -1.9838e+02 - logprior: -1.3953e+00
Epoch 3/10
19/19 - 3s - loss: 176.4993 - loglik: -1.7476e+02 - logprior: -1.7431e+00
Epoch 4/10
19/19 - 3s - loss: 173.3481 - loglik: -1.7169e+02 - logprior: -1.6605e+00
Epoch 5/10
19/19 - 3s - loss: 171.9192 - loglik: -1.7030e+02 - logprior: -1.6159e+00
Epoch 6/10
19/19 - 3s - loss: 171.1737 - loglik: -1.6956e+02 - logprior: -1.6141e+00
Epoch 7/10
19/19 - 3s - loss: 171.1042 - loglik: -1.6949e+02 - logprior: -1.6168e+00
Epoch 8/10
19/19 - 3s - loss: 170.7468 - loglik: -1.6911e+02 - logprior: -1.6391e+00
Epoch 9/10
19/19 - 3s - loss: 170.1781 - loglik: -1.6853e+02 - logprior: -1.6528e+00
Epoch 10/10
19/19 - 3s - loss: 170.6901 - loglik: -1.6903e+02 - logprior: -1.6581e+00
Fitted a model with MAP estimate = -170.0827
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 169.0640 - loglik: -1.6495e+02 - logprior: -4.1101e+00
Epoch 2/2
19/19 - 3s - loss: 156.9595 - loglik: -1.5488e+02 - logprior: -2.0802e+00
Fitted a model with MAP estimate = -155.1555
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.6445 - loglik: -1.5371e+02 - logprior: -2.9380e+00
Epoch 2/2
19/19 - 3s - loss: 152.7524 - loglik: -1.5162e+02 - logprior: -1.1275e+00
Fitted a model with MAP estimate = -151.8893
expansions: []
discards: [0]
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 158.9335 - loglik: -1.5509e+02 - logprior: -3.8460e+00
Epoch 2/10
19/19 - 3s - loss: 154.3741 - loglik: -1.5290e+02 - logprior: -1.4705e+00
Epoch 3/10
19/19 - 3s - loss: 152.3345 - loglik: -1.5130e+02 - logprior: -1.0304e+00
Epoch 4/10
19/19 - 3s - loss: 152.0431 - loglik: -1.5104e+02 - logprior: -1.0012e+00
Epoch 5/10
19/19 - 3s - loss: 150.9301 - loglik: -1.4994e+02 - logprior: -9.8907e-01
Epoch 6/10
19/19 - 3s - loss: 150.6086 - loglik: -1.4961e+02 - logprior: -9.9705e-01
Epoch 7/10
19/19 - 3s - loss: 150.6189 - loglik: -1.4961e+02 - logprior: -1.0088e+00
Fitted a model with MAP estimate = -150.3848
Time for alignment: 92.9020
Computed alignments with likelihoods: ['-150.2333', '-151.1524', '-150.3848']
Best model has likelihood: -150.2333
SP score = 0.9177
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f514abe29a0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f523c685400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5239016640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523c9bfd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 232.4333 - loglik: -2.2902e+02 - logprior: -3.4134e+00
Epoch 2/10
19/19 - 2s - loss: 196.7417 - loglik: -1.9540e+02 - logprior: -1.3414e+00
Epoch 3/10
19/19 - 2s - loss: 185.1912 - loglik: -1.8381e+02 - logprior: -1.3837e+00
Epoch 4/10
19/19 - 2s - loss: 182.0111 - loglik: -1.8064e+02 - logprior: -1.3701e+00
Epoch 5/10
19/19 - 2s - loss: 180.7110 - loglik: -1.7932e+02 - logprior: -1.3922e+00
Epoch 6/10
19/19 - 2s - loss: 180.1179 - loglik: -1.7873e+02 - logprior: -1.3833e+00
Epoch 7/10
19/19 - 2s - loss: 179.6570 - loglik: -1.7825e+02 - logprior: -1.4099e+00
Epoch 8/10
19/19 - 2s - loss: 179.7125 - loglik: -1.7828e+02 - logprior: -1.4361e+00
Fitted a model with MAP estimate = -179.4659
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (31, 1), (32, 1), (34, 1), (35, 1), (37, 1), (46, 1), (48, 1), (52, 1), (55, 3)]
discards: [0]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 182.9967 - loglik: -1.7896e+02 - logprior: -4.0366e+00
Epoch 2/2
19/19 - 2s - loss: 176.1940 - loglik: -1.7409e+02 - logprior: -2.1067e+00
Fitted a model with MAP estimate = -174.9228
expansions: [(0, 2)]
discards: [ 0 71]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 177.2657 - loglik: -1.7434e+02 - logprior: -2.9211e+00
Epoch 2/2
19/19 - 2s - loss: 173.9815 - loglik: -1.7283e+02 - logprior: -1.1513e+00
Fitted a model with MAP estimate = -173.5009
expansions: []
discards: [ 0 14]
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.4651 - loglik: -1.7480e+02 - logprior: -3.6681e+00
Epoch 2/10
19/19 - 2s - loss: 174.8939 - loglik: -1.7362e+02 - logprior: -1.2752e+00
Epoch 3/10
19/19 - 2s - loss: 173.8973 - loglik: -1.7280e+02 - logprior: -1.0946e+00
Epoch 4/10
19/19 - 2s - loss: 172.9066 - loglik: -1.7183e+02 - logprior: -1.0799e+00
Epoch 5/10
19/19 - 2s - loss: 172.7908 - loglik: -1.7172e+02 - logprior: -1.0688e+00
Epoch 6/10
19/19 - 2s - loss: 171.8805 - loglik: -1.7079e+02 - logprior: -1.0859e+00
Epoch 7/10
19/19 - 2s - loss: 171.6581 - loglik: -1.7056e+02 - logprior: -1.0994e+00
Epoch 8/10
19/19 - 2s - loss: 171.5800 - loglik: -1.7047e+02 - logprior: -1.1141e+00
Epoch 9/10
19/19 - 2s - loss: 171.4843 - loglik: -1.7036e+02 - logprior: -1.1287e+00
Epoch 10/10
19/19 - 2s - loss: 171.3104 - loglik: -1.7018e+02 - logprior: -1.1286e+00
Fitted a model with MAP estimate = -171.1529
Time for alignment: 68.8023
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9705 - loglik: -2.2856e+02 - logprior: -3.4137e+00
Epoch 2/10
19/19 - 2s - loss: 194.0934 - loglik: -1.9276e+02 - logprior: -1.3364e+00
Epoch 3/10
19/19 - 2s - loss: 183.6957 - loglik: -1.8230e+02 - logprior: -1.3973e+00
Epoch 4/10
19/19 - 2s - loss: 181.2808 - loglik: -1.7989e+02 - logprior: -1.3888e+00
Epoch 5/10
19/19 - 2s - loss: 179.8837 - loglik: -1.7849e+02 - logprior: -1.3922e+00
Epoch 6/10
19/19 - 2s - loss: 179.8303 - loglik: -1.7845e+02 - logprior: -1.3813e+00
Epoch 7/10
19/19 - 2s - loss: 179.4662 - loglik: -1.7805e+02 - logprior: -1.4139e+00
Epoch 8/10
19/19 - 2s - loss: 179.1230 - loglik: -1.7769e+02 - logprior: -1.4335e+00
Epoch 9/10
19/19 - 2s - loss: 179.1700 - loglik: -1.7772e+02 - logprior: -1.4510e+00
Fitted a model with MAP estimate = -178.9234
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 2), (35, 1), (37, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.0808 - loglik: -1.7903e+02 - logprior: -4.0524e+00
Epoch 2/2
19/19 - 2s - loss: 176.1864 - loglik: -1.7406e+02 - logprior: -2.1238e+00
Fitted a model with MAP estimate = -174.5254
expansions: [(0, 2)]
discards: [ 0 43 72]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 176.4462 - loglik: -1.7354e+02 - logprior: -2.9099e+00
Epoch 2/2
19/19 - 2s - loss: 173.3689 - loglik: -1.7223e+02 - logprior: -1.1393e+00
Fitted a model with MAP estimate = -172.6623
expansions: []
discards: [ 0 14]
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 177.6847 - loglik: -1.7404e+02 - logprior: -3.6462e+00
Epoch 2/10
19/19 - 2s - loss: 173.8441 - loglik: -1.7260e+02 - logprior: -1.2490e+00
Epoch 3/10
19/19 - 2s - loss: 173.1843 - loglik: -1.7210e+02 - logprior: -1.0796e+00
Epoch 4/10
19/19 - 2s - loss: 172.2206 - loglik: -1.7116e+02 - logprior: -1.0636e+00
Epoch 5/10
19/19 - 2s - loss: 171.8100 - loglik: -1.7075e+02 - logprior: -1.0580e+00
Epoch 6/10
19/19 - 2s - loss: 171.6409 - loglik: -1.7057e+02 - logprior: -1.0734e+00
Epoch 7/10
19/19 - 2s - loss: 171.0701 - loglik: -1.6999e+02 - logprior: -1.0802e+00
Epoch 8/10
19/19 - 2s - loss: 170.3158 - loglik: -1.6922e+02 - logprior: -1.0991e+00
Epoch 9/10
19/19 - 2s - loss: 170.6601 - loglik: -1.6955e+02 - logprior: -1.1146e+00
Fitted a model with MAP estimate = -170.4349
Time for alignment: 67.9270
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 231.9786 - loglik: -2.2856e+02 - logprior: -3.4167e+00
Epoch 2/10
19/19 - 2s - loss: 195.9789 - loglik: -1.9464e+02 - logprior: -1.3345e+00
Epoch 3/10
19/19 - 2s - loss: 184.7205 - loglik: -1.8334e+02 - logprior: -1.3762e+00
Epoch 4/10
19/19 - 2s - loss: 181.5186 - loglik: -1.8014e+02 - logprior: -1.3748e+00
Epoch 5/10
19/19 - 2s - loss: 180.5612 - loglik: -1.7917e+02 - logprior: -1.3935e+00
Epoch 6/10
19/19 - 2s - loss: 179.8276 - loglik: -1.7845e+02 - logprior: -1.3747e+00
Epoch 7/10
19/19 - 2s - loss: 179.6140 - loglik: -1.7820e+02 - logprior: -1.4125e+00
Epoch 8/10
19/19 - 2s - loss: 179.6003 - loglik: -1.7817e+02 - logprior: -1.4291e+00
Epoch 9/10
19/19 - 2s - loss: 179.0158 - loglik: -1.7757e+02 - logprior: -1.4434e+00
Epoch 10/10
19/19 - 2s - loss: 179.3876 - loglik: -1.7793e+02 - logprior: -1.4624e+00
Fitted a model with MAP estimate = -179.0692
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 182.9404 - loglik: -1.7890e+02 - logprior: -4.0427e+00
Epoch 2/2
19/19 - 2s - loss: 175.9931 - loglik: -1.7388e+02 - logprior: -2.1099e+00
Fitted a model with MAP estimate = -174.2441
expansions: [(0, 2)]
discards: [ 0 71]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 176.4626 - loglik: -1.7355e+02 - logprior: -2.9083e+00
Epoch 2/2
19/19 - 2s - loss: 173.3767 - loglik: -1.7224e+02 - logprior: -1.1406e+00
Fitted a model with MAP estimate = -172.6562
expansions: []
discards: [ 0 14]
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.7982 - loglik: -1.7416e+02 - logprior: -3.6422e+00
Epoch 2/10
19/19 - 2s - loss: 173.9462 - loglik: -1.7270e+02 - logprior: -1.2454e+00
Epoch 3/10
19/19 - 2s - loss: 172.8658 - loglik: -1.7179e+02 - logprior: -1.0798e+00
Epoch 4/10
19/19 - 2s - loss: 172.5018 - loglik: -1.7144e+02 - logprior: -1.0578e+00
Epoch 5/10
19/19 - 2s - loss: 171.6495 - loglik: -1.7059e+02 - logprior: -1.0578e+00
Epoch 6/10
19/19 - 2s - loss: 171.3219 - loglik: -1.7025e+02 - logprior: -1.0709e+00
Epoch 7/10
19/19 - 2s - loss: 170.9875 - loglik: -1.6991e+02 - logprior: -1.0785e+00
Epoch 8/10
19/19 - 2s - loss: 170.5948 - loglik: -1.6949e+02 - logprior: -1.1059e+00
Epoch 9/10
19/19 - 2s - loss: 170.9951 - loglik: -1.6989e+02 - logprior: -1.1039e+00
Fitted a model with MAP estimate = -170.4287
Time for alignment: 68.8351
Computed alignments with likelihoods: ['-171.1529', '-170.4349', '-170.4287']
Best model has likelihood: -170.4287
SP score = 0.7898
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5236b4b880>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f524d993d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f524d9934f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529460cb20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 667.3267 - loglik: -6.6387e+02 - logprior: -3.4535e+00
Epoch 2/10
19/19 - 13s - loss: 620.8607 - loglik: -6.1986e+02 - logprior: -9.9935e-01
Epoch 3/10
19/19 - 13s - loss: 598.0186 - loglik: -5.9690e+02 - logprior: -1.1169e+00
Epoch 4/10
19/19 - 13s - loss: 589.0170 - loglik: -5.8785e+02 - logprior: -1.1651e+00
Epoch 5/10
19/19 - 13s - loss: 585.3112 - loglik: -5.8413e+02 - logprior: -1.1812e+00
Epoch 6/10
19/19 - 13s - loss: 581.6134 - loglik: -5.8037e+02 - logprior: -1.2464e+00
Epoch 7/10
19/19 - 13s - loss: 579.1268 - loglik: -5.7777e+02 - logprior: -1.3593e+00
Epoch 8/10
19/19 - 13s - loss: 577.2266 - loglik: -5.7569e+02 - logprior: -1.5335e+00
Epoch 9/10
19/19 - 13s - loss: 574.6613 - loglik: -5.7293e+02 - logprior: -1.7335e+00
Epoch 10/10
19/19 - 13s - loss: 571.3140 - loglik: -5.6942e+02 - logprior: -1.8986e+00
Fitted a model with MAP estimate = -570.3981
expansions: [(21, 1), (22, 1), (23, 6), (27, 1), (46, 5), (48, 1), (55, 1), (58, 1), (62, 1), (64, 1), (67, 1), (81, 1), (82, 2), (83, 2), (96, 1), (104, 2), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (135, 1), (145, 1), (154, 1), (155, 2), (160, 1), (162, 1), (164, 1), (167, 1), (169, 1), (174, 1), (179, 1)]
discards: [0]
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 627.5706 - loglik: -6.2327e+02 - logprior: -4.3027e+00
Epoch 2/2
19/19 - 17s - loss: 592.0634 - loglik: -5.9024e+02 - logprior: -1.8214e+00
Fitted a model with MAP estimate = -586.3472
expansions: [(0, 3), (32, 3), (76, 1), (77, 1), (185, 2), (192, 2), (237, 6)]
discards: [  0  29  30  62 104 131 193 217 223 234 235 236]
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 589.2330 - loglik: -5.8647e+02 - logprior: -2.7659e+00
Epoch 2/2
19/19 - 18s - loss: 583.2118 - loglik: -5.8243e+02 - logprior: -7.7890e-01
Fitted a model with MAP estimate = -581.1450
expansions: [(59, 1), (224, 1)]
discards: [  0   2  28  29  60  61  62 196 238 239 240 241 242]
Fitting a model of length 232 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 590.5563 - loglik: -5.8715e+02 - logprior: -3.4079e+00
Epoch 2/10
19/19 - 17s - loss: 585.3399 - loglik: -5.8434e+02 - logprior: -9.9994e-01
Epoch 3/10
19/19 - 17s - loss: 582.7045 - loglik: -5.8251e+02 - logprior: -1.9796e-01
Epoch 4/10
19/19 - 17s - loss: 580.4952 - loglik: -5.8039e+02 - logprior: -1.0297e-01
Epoch 5/10
19/19 - 17s - loss: 576.1362 - loglik: -5.7603e+02 - logprior: -1.0694e-01
Epoch 6/10
19/19 - 17s - loss: 572.7678 - loglik: -5.7264e+02 - logprior: -1.2746e-01
Epoch 7/10
19/19 - 17s - loss: 569.3593 - loglik: -5.6912e+02 - logprior: -2.3816e-01
Epoch 8/10
19/19 - 17s - loss: 565.2233 - loglik: -5.6485e+02 - logprior: -3.7547e-01
Epoch 9/10
19/19 - 17s - loss: 562.2952 - loglik: -5.6171e+02 - logprior: -5.8484e-01
Epoch 10/10
19/19 - 17s - loss: 559.5566 - loglik: -5.5878e+02 - logprior: -7.8096e-01
Fitted a model with MAP estimate = -557.0063
Time for alignment: 467.9929
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 668.0247 - loglik: -6.6457e+02 - logprior: -3.4573e+00
Epoch 2/10
19/19 - 13s - loss: 622.0057 - loglik: -6.2101e+02 - logprior: -9.9211e-01
Epoch 3/10
19/19 - 13s - loss: 597.2560 - loglik: -5.9611e+02 - logprior: -1.1500e+00
Epoch 4/10
19/19 - 13s - loss: 588.8856 - loglik: -5.8763e+02 - logprior: -1.2602e+00
Epoch 5/10
19/19 - 13s - loss: 586.0394 - loglik: -5.8477e+02 - logprior: -1.2717e+00
Epoch 6/10
19/19 - 13s - loss: 582.6792 - loglik: -5.8134e+02 - logprior: -1.3376e+00
Epoch 7/10
19/19 - 13s - loss: 579.6375 - loglik: -5.7819e+02 - logprior: -1.4507e+00
Epoch 8/10
19/19 - 13s - loss: 576.6146 - loglik: -5.7502e+02 - logprior: -1.5976e+00
Epoch 9/10
19/19 - 13s - loss: 573.2636 - loglik: -5.7145e+02 - logprior: -1.8136e+00
Epoch 10/10
19/19 - 13s - loss: 572.5135 - loglik: -5.7054e+02 - logprior: -1.9737e+00
Fitted a model with MAP estimate = -570.8405
expansions: [(23, 1), (24, 4), (26, 1), (27, 1), (28, 1), (48, 6), (60, 1), (63, 1), (66, 1), (69, 1), (81, 1), (82, 1), (84, 1), (85, 1), (97, 1), (105, 4), (106, 2), (110, 1), (121, 1), (124, 1), (126, 1), (127, 1), (135, 1), (156, 3), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (179, 1)]
discards: [0]
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 627.8535 - loglik: -6.2355e+02 - logprior: -4.3003e+00
Epoch 2/2
19/19 - 17s - loss: 592.3613 - loglik: -5.9055e+02 - logprior: -1.8112e+00
Fitted a model with MAP estimate = -586.6104
expansions: [(0, 3), (76, 2), (184, 1), (185, 1), (189, 1), (236, 6)]
discards: [  0  25  26  27  59  60  61  62  78 130 131 193 222 234 235]
Fitting a model of length 235 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 589.9977 - loglik: -5.8723e+02 - logprior: -2.7678e+00
Epoch 2/2
19/19 - 17s - loss: 583.8488 - loglik: -5.8308e+02 - logprior: -7.6713e-01
Fitted a model with MAP estimate = -582.3634
expansions: [(57, 1), (215, 1), (218, 1)]
discards: [  0   2 229 230 231 232 233 234]
Fitting a model of length 230 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 590.2739 - loglik: -5.8684e+02 - logprior: -3.4318e+00
Epoch 2/10
19/19 - 17s - loss: 586.0598 - loglik: -5.8505e+02 - logprior: -1.0123e+00
Epoch 3/10
19/19 - 17s - loss: 582.8068 - loglik: -5.8263e+02 - logprior: -1.7420e-01
Epoch 4/10
19/19 - 16s - loss: 579.9086 - loglik: -5.7984e+02 - logprior: -6.9289e-02
Epoch 5/10
19/19 - 17s - loss: 578.1127 - loglik: -5.7802e+02 - logprior: -9.4911e-02
Epoch 6/10
19/19 - 16s - loss: 572.0298 - loglik: -5.7190e+02 - logprior: -1.3324e-01
Epoch 7/10
19/19 - 17s - loss: 569.9515 - loglik: -5.6971e+02 - logprior: -2.3794e-01
Epoch 8/10
19/19 - 17s - loss: 564.7143 - loglik: -5.6433e+02 - logprior: -3.8754e-01
Epoch 9/10
19/19 - 16s - loss: 563.3192 - loglik: -5.6275e+02 - logprior: -5.6805e-01
Epoch 10/10
19/19 - 17s - loss: 559.1832 - loglik: -5.5839e+02 - logprior: -7.8954e-01
Fitted a model with MAP estimate = -557.5651
Time for alignment: 460.4268
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 668.2858 - loglik: -6.6483e+02 - logprior: -3.4563e+00
Epoch 2/10
19/19 - 13s - loss: 621.0630 - loglik: -6.2008e+02 - logprior: -9.7878e-01
Epoch 3/10
19/19 - 13s - loss: 596.4249 - loglik: -5.9530e+02 - logprior: -1.1217e+00
Epoch 4/10
19/19 - 13s - loss: 589.8167 - loglik: -5.8862e+02 - logprior: -1.1922e+00
Epoch 5/10
19/19 - 13s - loss: 585.1971 - loglik: -5.8399e+02 - logprior: -1.2026e+00
Epoch 6/10
19/19 - 13s - loss: 581.2877 - loglik: -5.8001e+02 - logprior: -1.2783e+00
Epoch 7/10
19/19 - 13s - loss: 579.5312 - loglik: -5.7815e+02 - logprior: -1.3841e+00
Epoch 8/10
19/19 - 13s - loss: 575.9093 - loglik: -5.7434e+02 - logprior: -1.5715e+00
Epoch 9/10
19/19 - 13s - loss: 575.3929 - loglik: -5.7361e+02 - logprior: -1.7790e+00
Epoch 10/10
19/19 - 13s - loss: 569.5453 - loglik: -5.6757e+02 - logprior: -1.9734e+00
Fitted a model with MAP estimate = -570.3308
expansions: [(21, 1), (22, 1), (24, 5), (28, 1), (47, 5), (49, 1), (53, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (84, 1), (85, 1), (106, 4), (107, 2), (109, 1), (110, 1), (121, 1), (124, 1), (127, 1), (134, 1), (138, 1), (148, 1), (150, 1), (156, 3), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (176, 2), (179, 1)]
discards: [0]
Fitting a model of length 242 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 627.1853 - loglik: -6.2290e+02 - logprior: -4.2901e+00
Epoch 2/2
19/19 - 18s - loss: 591.5903 - loglik: -5.8978e+02 - logprior: -1.8073e+00
Fitted a model with MAP estimate = -586.1486
expansions: [(0, 3), (32, 3), (193, 2), (242, 6)]
discards: [  0  26  27  30  62  81 132 133 188 189 197 225 226 237 238 239 240 241]
Fitting a model of length 238 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 590.6976 - loglik: -5.8796e+02 - logprior: -2.7329e+00
Epoch 2/2
19/19 - 17s - loss: 583.7018 - loglik: -5.8294e+02 - logprior: -7.6462e-01
Fitted a model with MAP estimate = -581.7821
expansions: [(58, 1)]
discards: [  0   2  27  28  37  61  62 235 236 237]
Fitting a model of length 229 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 591.1856 - loglik: -5.8776e+02 - logprior: -3.4224e+00
Epoch 2/10
19/19 - 16s - loss: 585.7573 - loglik: -5.8473e+02 - logprior: -1.0288e+00
Epoch 3/10
19/19 - 16s - loss: 582.8682 - loglik: -5.8259e+02 - logprior: -2.7499e-01
Epoch 4/10
19/19 - 16s - loss: 579.5376 - loglik: -5.7936e+02 - logprior: -1.7433e-01
Epoch 5/10
19/19 - 16s - loss: 579.0519 - loglik: -5.7887e+02 - logprior: -1.8612e-01
Epoch 6/10
19/19 - 17s - loss: 572.5078 - loglik: -5.7231e+02 - logprior: -2.0115e-01
Epoch 7/10
19/19 - 16s - loss: 568.9588 - loglik: -5.6865e+02 - logprior: -3.1305e-01
Epoch 8/10
19/19 - 17s - loss: 565.3172 - loglik: -5.6486e+02 - logprior: -4.5590e-01
Epoch 9/10
19/19 - 17s - loss: 564.6592 - loglik: -5.6401e+02 - logprior: -6.4969e-01
Epoch 10/10
19/19 - 17s - loss: 559.1068 - loglik: -5.5825e+02 - logprior: -8.5636e-01
Fitted a model with MAP estimate = -557.7008
Time for alignment: 462.0877
Computed alignments with likelihoods: ['-557.0063', '-557.5651', '-557.7008']
Best model has likelihood: -557.0063
SP score = 0.6393
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5294cbb700>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f523c5fe340>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514ad50e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52393edb20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1006 - loglik: -1.5067e+02 - logprior: -3.4292e+00
Epoch 2/10
19/19 - 1s - loss: 123.7041 - loglik: -1.2218e+02 - logprior: -1.5258e+00
Epoch 3/10
19/19 - 1s - loss: 112.1344 - loglik: -1.1057e+02 - logprior: -1.5599e+00
Epoch 4/10
19/19 - 1s - loss: 108.8019 - loglik: -1.0720e+02 - logprior: -1.5980e+00
Epoch 5/10
19/19 - 1s - loss: 107.9699 - loglik: -1.0638e+02 - logprior: -1.5858e+00
Epoch 6/10
19/19 - 1s - loss: 107.4688 - loglik: -1.0588e+02 - logprior: -1.5882e+00
Epoch 7/10
19/19 - 1s - loss: 107.0753 - loglik: -1.0548e+02 - logprior: -1.5914e+00
Epoch 8/10
19/19 - 1s - loss: 107.3719 - loglik: -1.0577e+02 - logprior: -1.5983e+00
Fitted a model with MAP estimate = -106.9516
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 110.7495 - loglik: -1.0652e+02 - logprior: -4.2315e+00
Epoch 2/2
19/19 - 1s - loss: 101.8300 - loglik: -9.9600e+01 - logprior: -2.2299e+00
Fitted a model with MAP estimate = -100.4237
expansions: [(3, 1)]
discards: [ 0 20 38]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.5190 - loglik: -1.0056e+02 - logprior: -3.9614e+00
Epoch 2/2
19/19 - 1s - loss: 100.3812 - loglik: -9.8865e+01 - logprior: -1.5158e+00
Fitted a model with MAP estimate = -99.6515
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.8744 - loglik: -1.0044e+02 - logprior: -4.4343e+00
Epoch 2/10
19/19 - 1s - loss: 100.7803 - loglik: -9.8724e+01 - logprior: -2.0558e+00
Epoch 3/10
19/19 - 1s - loss: 99.5528 - loglik: -9.8148e+01 - logprior: -1.4043e+00
Epoch 4/10
19/19 - 1s - loss: 98.8921 - loglik: -9.7672e+01 - logprior: -1.2196e+00
Epoch 5/10
19/19 - 1s - loss: 98.6038 - loglik: -9.7402e+01 - logprior: -1.2016e+00
Epoch 6/10
19/19 - 1s - loss: 98.4535 - loglik: -9.7259e+01 - logprior: -1.1940e+00
Epoch 7/10
19/19 - 1s - loss: 98.0971 - loglik: -9.6905e+01 - logprior: -1.1925e+00
Epoch 8/10
19/19 - 1s - loss: 98.1089 - loglik: -9.6906e+01 - logprior: -1.2030e+00
Fitted a model with MAP estimate = -97.9840
Time for alignment: 51.3027
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.2863 - loglik: -1.5086e+02 - logprior: -3.4246e+00
Epoch 2/10
19/19 - 1s - loss: 124.2671 - loglik: -1.2274e+02 - logprior: -1.5279e+00
Epoch 3/10
19/19 - 1s - loss: 111.3432 - loglik: -1.0978e+02 - logprior: -1.5676e+00
Epoch 4/10
19/19 - 1s - loss: 107.9091 - loglik: -1.0629e+02 - logprior: -1.6208e+00
Epoch 5/10
19/19 - 1s - loss: 107.0263 - loglik: -1.0546e+02 - logprior: -1.5694e+00
Epoch 6/10
19/19 - 1s - loss: 106.3693 - loglik: -1.0479e+02 - logprior: -1.5800e+00
Epoch 7/10
19/19 - 1s - loss: 106.2836 - loglik: -1.0470e+02 - logprior: -1.5819e+00
Epoch 8/10
19/19 - 1s - loss: 105.9462 - loglik: -1.0435e+02 - logprior: -1.5936e+00
Epoch 9/10
19/19 - 1s - loss: 105.9870 - loglik: -1.0437e+02 - logprior: -1.6191e+00
Fitted a model with MAP estimate = -105.8473
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.2025 - loglik: -1.0401e+02 - logprior: -3.1961e+00
Epoch 2/2
19/19 - 1s - loss: 99.8209 - loglik: -9.8466e+01 - logprior: -1.3553e+00
Fitted a model with MAP estimate = -98.9726
expansions: []
discards: [22 37 40]
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.6624 - loglik: -9.8686e+01 - logprior: -2.9762e+00
Epoch 2/2
19/19 - 1s - loss: 99.1144 - loglik: -9.7909e+01 - logprior: -1.2054e+00
Fitted a model with MAP estimate = -98.6526
expansions: []
discards: []
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.2719 - loglik: -9.8328e+01 - logprior: -2.9439e+00
Epoch 2/10
19/19 - 1s - loss: 98.9565 - loglik: -9.7774e+01 - logprior: -1.1821e+00
Epoch 3/10
19/19 - 1s - loss: 98.6278 - loglik: -9.7487e+01 - logprior: -1.1407e+00
Epoch 4/10
19/19 - 1s - loss: 98.0569 - loglik: -9.6950e+01 - logprior: -1.1066e+00
Epoch 5/10
19/19 - 1s - loss: 97.8347 - loglik: -9.6747e+01 - logprior: -1.0881e+00
Epoch 6/10
19/19 - 1s - loss: 97.6660 - loglik: -9.6576e+01 - logprior: -1.0897e+00
Epoch 7/10
19/19 - 1s - loss: 97.1615 - loglik: -9.6081e+01 - logprior: -1.0806e+00
Epoch 8/10
19/19 - 1s - loss: 97.4177 - loglik: -9.6319e+01 - logprior: -1.0985e+00
Fitted a model with MAP estimate = -97.1734
Time for alignment: 53.2904
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.2480 - loglik: -1.5082e+02 - logprior: -3.4296e+00
Epoch 2/10
19/19 - 1s - loss: 125.0546 - loglik: -1.2353e+02 - logprior: -1.5278e+00
Epoch 3/10
19/19 - 1s - loss: 112.7809 - loglik: -1.1122e+02 - logprior: -1.5609e+00
Epoch 4/10
19/19 - 1s - loss: 108.9454 - loglik: -1.0736e+02 - logprior: -1.5872e+00
Epoch 5/10
19/19 - 1s - loss: 108.1432 - loglik: -1.0655e+02 - logprior: -1.5922e+00
Epoch 6/10
19/19 - 1s - loss: 107.1834 - loglik: -1.0559e+02 - logprior: -1.5889e+00
Epoch 7/10
19/19 - 1s - loss: 107.1332 - loglik: -1.0554e+02 - logprior: -1.5930e+00
Epoch 8/10
19/19 - 1s - loss: 106.8295 - loglik: -1.0523e+02 - logprior: -1.5950e+00
Epoch 9/10
19/19 - 1s - loss: 107.1825 - loglik: -1.0557e+02 - logprior: -1.6112e+00
Fitted a model with MAP estimate = -106.7990
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.8129 - loglik: -1.0657e+02 - logprior: -4.2437e+00
Epoch 2/2
19/19 - 1s - loss: 101.6554 - loglik: -9.9410e+01 - logprior: -2.2450e+00
Fitted a model with MAP estimate = -100.4784
expansions: [(3, 1)]
discards: [ 0 21 38]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.6715 - loglik: -1.0070e+02 - logprior: -3.9696e+00
Epoch 2/2
19/19 - 1s - loss: 100.4781 - loglik: -9.8966e+01 - logprior: -1.5126e+00
Fitted a model with MAP estimate = -99.6708
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.9645 - loglik: -1.0052e+02 - logprior: -4.4425e+00
Epoch 2/10
19/19 - 1s - loss: 100.6642 - loglik: -9.8625e+01 - logprior: -2.0389e+00
Epoch 3/10
19/19 - 1s - loss: 99.6454 - loglik: -9.8254e+01 - logprior: -1.3916e+00
Epoch 4/10
19/19 - 1s - loss: 99.0290 - loglik: -9.7806e+01 - logprior: -1.2228e+00
Epoch 5/10
19/19 - 1s - loss: 98.3772 - loglik: -9.7175e+01 - logprior: -1.2023e+00
Epoch 6/10
19/19 - 1s - loss: 98.5326 - loglik: -9.7341e+01 - logprior: -1.1919e+00
Fitted a model with MAP estimate = -98.1591
Time for alignment: 50.7108
Computed alignments with likelihoods: ['-97.9840', '-97.1734', '-98.1591']
Best model has likelihood: -97.1734
SP score = 0.9630
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52396f1100>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f336f40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f521d7a9a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5239ed7af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 366.5024 - loglik: -3.6307e+02 - logprior: -3.4330e+00
Epoch 2/10
19/19 - 3s - loss: 338.0388 - loglik: -3.3697e+02 - logprior: -1.0648e+00
Epoch 3/10
19/19 - 3s - loss: 325.1646 - loglik: -3.2389e+02 - logprior: -1.2726e+00
Epoch 4/10
19/19 - 3s - loss: 320.8322 - loglik: -3.1955e+02 - logprior: -1.2819e+00
Epoch 5/10
19/19 - 3s - loss: 318.1155 - loglik: -3.1672e+02 - logprior: -1.3940e+00
Epoch 6/10
19/19 - 3s - loss: 316.6593 - loglik: -3.1521e+02 - logprior: -1.4512e+00
Epoch 7/10
19/19 - 3s - loss: 313.6657 - loglik: -3.1215e+02 - logprior: -1.5146e+00
Epoch 8/10
19/19 - 3s - loss: 312.7094 - loglik: -3.1113e+02 - logprior: -1.5774e+00
Epoch 9/10
19/19 - 3s - loss: 311.5892 - loglik: -3.0993e+02 - logprior: -1.6557e+00
Epoch 10/10
19/19 - 3s - loss: 309.9545 - loglik: -3.0825e+02 - logprior: -1.7083e+00
Fitted a model with MAP estimate = -309.5945
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (24, 3), (27, 1), (28, 1), (29, 1), (32, 1), (51, 2), (54, 4), (74, 1), (75, 4), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Fitting a model of length 135 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 340.4803 - loglik: -3.3639e+02 - logprior: -4.0921e+00
Epoch 2/2
19/19 - 4s - loss: 319.3477 - loglik: -3.1725e+02 - logprior: -2.1000e+00
Fitted a model with MAP estimate = -316.0001
expansions: [(0, 2), (34, 1)]
discards: [  0 112]
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 317.2472 - loglik: -3.1439e+02 - logprior: -2.8560e+00
Epoch 2/2
19/19 - 4s - loss: 313.1635 - loglik: -3.1214e+02 - logprior: -1.0251e+00
Fitted a model with MAP estimate = -312.0948
expansions: []
discards: [26 30 70]
Fitting a model of length 133 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 316.6173 - loglik: -3.1385e+02 - logprior: -2.7651e+00
Epoch 2/10
19/19 - 4s - loss: 313.1827 - loglik: -3.1220e+02 - logprior: -9.8504e-01
Epoch 3/10
19/19 - 4s - loss: 312.0119 - loglik: -3.1106e+02 - logprior: -9.5570e-01
Epoch 4/10
19/19 - 4s - loss: 311.2013 - loglik: -3.1026e+02 - logprior: -9.3849e-01
Epoch 5/10
19/19 - 4s - loss: 308.8951 - loglik: -3.0795e+02 - logprior: -9.4348e-01
Epoch 6/10
19/19 - 4s - loss: 306.8684 - loglik: -3.0592e+02 - logprior: -9.5259e-01
Epoch 7/10
19/19 - 4s - loss: 304.6557 - loglik: -3.0366e+02 - logprior: -9.9313e-01
Epoch 8/10
19/19 - 4s - loss: 302.6706 - loglik: -3.0162e+02 - logprior: -1.0499e+00
Epoch 9/10
19/19 - 4s - loss: 300.8167 - loglik: -2.9971e+02 - logprior: -1.1065e+00
Epoch 10/10
19/19 - 4s - loss: 299.2384 - loglik: -2.9807e+02 - logprior: -1.1641e+00
Fitted a model with MAP estimate = -298.5788
Time for alignment: 128.6545
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 366.4656 - loglik: -3.6303e+02 - logprior: -3.4356e+00
Epoch 2/10
19/19 - 3s - loss: 338.3587 - loglik: -3.3728e+02 - logprior: -1.0746e+00
Epoch 3/10
19/19 - 3s - loss: 326.1570 - loglik: -3.2487e+02 - logprior: -1.2831e+00
Epoch 4/10
19/19 - 3s - loss: 322.8604 - loglik: -3.2159e+02 - logprior: -1.2659e+00
Epoch 5/10
19/19 - 3s - loss: 318.9143 - loglik: -3.1758e+02 - logprior: -1.3337e+00
Epoch 6/10
19/19 - 3s - loss: 316.2244 - loglik: -3.1483e+02 - logprior: -1.3994e+00
Epoch 7/10
19/19 - 3s - loss: 314.7014 - loglik: -3.1322e+02 - logprior: -1.4828e+00
Epoch 8/10
19/19 - 3s - loss: 312.1959 - loglik: -3.1065e+02 - logprior: -1.5463e+00
Epoch 9/10
19/19 - 3s - loss: 311.0996 - loglik: -3.0949e+02 - logprior: -1.6109e+00
Epoch 10/10
19/19 - 3s - loss: 310.1484 - loglik: -3.0847e+02 - logprior: -1.6753e+00
Fitted a model with MAP estimate = -309.4725
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 2), (29, 1), (32, 1), (53, 2), (68, 1), (74, 1), (75, 4), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 341.7488 - loglik: -3.3766e+02 - logprior: -4.0903e+00
Epoch 2/2
19/19 - 4s - loss: 320.9034 - loglik: -3.1876e+02 - logprior: -2.1385e+00
Fitted a model with MAP estimate = -317.1848
expansions: [(0, 2), (66, 1), (68, 1)]
discards: [  0  37 108]
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 318.6331 - loglik: -3.1579e+02 - logprior: -2.8395e+00
Epoch 2/2
19/19 - 4s - loss: 314.1163 - loglik: -3.1311e+02 - logprior: -1.0073e+00
Fitted a model with MAP estimate = -313.2020
expansions: []
discards: [ 1 26]
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.6292 - loglik: -3.1499e+02 - logprior: -2.6388e+00
Epoch 2/10
19/19 - 4s - loss: 314.3753 - loglik: -3.1342e+02 - logprior: -9.5305e-01
Epoch 3/10
19/19 - 4s - loss: 313.3869 - loglik: -3.1237e+02 - logprior: -1.0198e+00
Epoch 4/10
19/19 - 4s - loss: 311.5802 - loglik: -3.1064e+02 - logprior: -9.4187e-01
Epoch 5/10
19/19 - 4s - loss: 310.2071 - loglik: -3.0924e+02 - logprior: -9.6572e-01
Epoch 6/10
19/19 - 4s - loss: 307.7228 - loglik: -3.0675e+02 - logprior: -9.7105e-01
Epoch 7/10
19/19 - 4s - loss: 306.3612 - loglik: -3.0535e+02 - logprior: -1.0099e+00
Epoch 8/10
19/19 - 4s - loss: 303.5537 - loglik: -3.0249e+02 - logprior: -1.0649e+00
Epoch 9/10
19/19 - 4s - loss: 302.5680 - loglik: -3.0144e+02 - logprior: -1.1261e+00
Epoch 10/10
19/19 - 4s - loss: 300.6229 - loglik: -2.9945e+02 - logprior: -1.1778e+00
Fitted a model with MAP estimate = -299.9652
Time for alignment: 126.5595
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 366.3503 - loglik: -3.6292e+02 - logprior: -3.4350e+00
Epoch 2/10
19/19 - 3s - loss: 337.5725 - loglik: -3.3651e+02 - logprior: -1.0654e+00
Epoch 3/10
19/19 - 3s - loss: 326.0092 - loglik: -3.2475e+02 - logprior: -1.2633e+00
Epoch 4/10
19/19 - 3s - loss: 321.8900 - loglik: -3.2067e+02 - logprior: -1.2227e+00
Epoch 5/10
19/19 - 3s - loss: 319.2237 - loglik: -3.1792e+02 - logprior: -1.3036e+00
Epoch 6/10
19/19 - 3s - loss: 316.6067 - loglik: -3.1524e+02 - logprior: -1.3644e+00
Epoch 7/10
19/19 - 3s - loss: 314.1423 - loglik: -3.1268e+02 - logprior: -1.4617e+00
Epoch 8/10
19/19 - 3s - loss: 312.4772 - loglik: -3.1094e+02 - logprior: -1.5400e+00
Epoch 9/10
19/19 - 3s - loss: 310.9308 - loglik: -3.0933e+02 - logprior: -1.6025e+00
Epoch 10/10
19/19 - 3s - loss: 309.5573 - loglik: -3.0789e+02 - logprior: -1.6715e+00
Fitted a model with MAP estimate = -309.2814
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (24, 3), (25, 1), (28, 1), (31, 1), (54, 4), (75, 1), (76, 4), (82, 1), (84, 3), (86, 1)]
discards: [0]
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 340.7953 - loglik: -3.3670e+02 - logprior: -4.0919e+00
Epoch 2/2
19/19 - 4s - loss: 320.3315 - loglik: -3.1824e+02 - logprior: -2.0911e+00
Fitted a model with MAP estimate = -316.7435
expansions: [(0, 2), (104, 3)]
discards: [  0  20 110]
Fitting a model of length 135 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 317.9777 - loglik: -3.1512e+02 - logprior: -2.8546e+00
Epoch 2/2
19/19 - 4s - loss: 314.2620 - loglik: -3.1322e+02 - logprior: -1.0450e+00
Fitted a model with MAP estimate = -312.7420
expansions: []
discards: [ 26  27 104 105 106]
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.3327 - loglik: -3.1460e+02 - logprior: -2.7357e+00
Epoch 2/10
19/19 - 4s - loss: 313.9788 - loglik: -3.1302e+02 - logprior: -9.5557e-01
Epoch 3/10
19/19 - 4s - loss: 313.0169 - loglik: -3.1209e+02 - logprior: -9.3039e-01
Epoch 4/10
19/19 - 4s - loss: 311.4706 - loglik: -3.1054e+02 - logprior: -9.3498e-01
Epoch 5/10
19/19 - 4s - loss: 309.9582 - loglik: -3.0903e+02 - logprior: -9.3198e-01
Epoch 6/10
19/19 - 4s - loss: 307.7976 - loglik: -3.0684e+02 - logprior: -9.5667e-01
Epoch 7/10
19/19 - 4s - loss: 305.1599 - loglik: -3.0418e+02 - logprior: -9.8467e-01
Epoch 8/10
19/19 - 4s - loss: 304.0290 - loglik: -3.0298e+02 - logprior: -1.0457e+00
Epoch 9/10
19/19 - 4s - loss: 302.4324 - loglik: -3.0135e+02 - logprior: -1.0860e+00
Epoch 10/10
19/19 - 4s - loss: 300.7287 - loglik: -2.9959e+02 - logprior: -1.1433e+00
Fitted a model with MAP estimate = -299.9025
Time for alignment: 127.0827
Computed alignments with likelihoods: ['-298.5788', '-299.9652', '-299.9025']
Best model has likelihood: -298.5788
SP score = 0.8041
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5238f99a90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f521d7c7d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f524dd1beb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514abd9fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 920.9562 - loglik: -9.1888e+02 - logprior: -2.0735e+00
Epoch 2/10
39/39 - 27s - loss: 824.4968 - loglik: -8.2307e+02 - logprior: -1.4246e+00
Epoch 3/10
39/39 - 27s - loss: 812.6209 - loglik: -8.1106e+02 - logprior: -1.5622e+00
Epoch 4/10
39/39 - 27s - loss: 808.4369 - loglik: -8.0678e+02 - logprior: -1.6616e+00
Epoch 5/10
39/39 - 27s - loss: 805.1100 - loglik: -8.0317e+02 - logprior: -1.9401e+00
Epoch 6/10
39/39 - 27s - loss: 803.2145 - loglik: -8.0102e+02 - logprior: -2.1985e+00
Epoch 7/10
39/39 - 27s - loss: 802.0179 - loglik: -7.9947e+02 - logprior: -2.5496e+00
Epoch 8/10
39/39 - 27s - loss: 800.8598 - loglik: -7.9803e+02 - logprior: -2.8347e+00
Epoch 9/10
39/39 - 27s - loss: 800.1768 - loglik: -7.9706e+02 - logprior: -3.1211e+00
Epoch 10/10
39/39 - 27s - loss: 799.3126 - loglik: -7.9599e+02 - logprior: -3.3239e+00
Fitted a model with MAP estimate = -790.9873
expansions: [(0, 2), (12, 1), (15, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (30, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (88, 1), (95, 2), (101, 1), (103, 1), (121, 1), (124, 2), (130, 1), (145, 1), (148, 1), (150, 1), (155, 1), (156, 2), (162, 1), (182, 1), (185, 1), (186, 2), (187, 1), (188, 2), (189, 1), (190, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (219, 1), (220, 1), (226, 2), (227, 1), (240, 3), (242, 1), (245, 1), (250, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 818.1481 - loglik: -8.1550e+02 - logprior: -2.6502e+00
Epoch 2/2
39/39 - 39s - loss: 790.5017 - loglik: -7.8987e+02 - logprior: -6.2747e-01
Fitted a model with MAP estimate = -780.1263
expansions: [(117, 1), (157, 1)]
discards: [  0   1  32 229 236 258 287 318 342]
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 794.7733 - loglik: -7.9335e+02 - logprior: -1.4229e+00
Epoch 2/2
39/39 - 39s - loss: 789.7829 - loglik: -7.8964e+02 - logprior: -1.3895e-01
Fitted a model with MAP estimate = -779.8499
expansions: [(0, 2)]
discards: []
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 785.9537 - loglik: -7.8452e+02 - logprior: -1.4358e+00
Epoch 2/10
39/39 - 40s - loss: 781.4371 - loglik: -7.8136e+02 - logprior: -7.5404e-02
Epoch 3/10
39/39 - 40s - loss: 778.2391 - loglik: -7.7825e+02 - logprior: 0.0129
Epoch 4/10
39/39 - 40s - loss: 774.3762 - loglik: -7.7439e+02 - logprior: 0.0130
Epoch 5/10
39/39 - 40s - loss: 769.6794 - loglik: -7.6953e+02 - logprior: -1.4592e-01
Epoch 6/10
39/39 - 40s - loss: 768.7332 - loglik: -7.6843e+02 - logprior: -3.0125e-01
Epoch 7/10
39/39 - 40s - loss: 766.1669 - loglik: -7.6560e+02 - logprior: -5.7104e-01
Epoch 8/10
39/39 - 39s - loss: 763.1517 - loglik: -7.6233e+02 - logprior: -8.1823e-01
Epoch 9/10
39/39 - 40s - loss: 762.4839 - loglik: -7.6153e+02 - logprior: -9.5620e-01
Epoch 10/10
39/39 - 40s - loss: 760.9022 - loglik: -7.5976e+02 - logprior: -1.1376e+00
Fitted a model with MAP estimate = -759.3498
Time for alignment: 1030.6521
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 921.0056 - loglik: -9.1892e+02 - logprior: -2.0851e+00
Epoch 2/10
39/39 - 27s - loss: 825.5699 - loglik: -8.2411e+02 - logprior: -1.4563e+00
Epoch 3/10
39/39 - 27s - loss: 813.7582 - loglik: -8.1219e+02 - logprior: -1.5684e+00
Epoch 4/10
39/39 - 27s - loss: 809.9014 - loglik: -8.0827e+02 - logprior: -1.6275e+00
Epoch 5/10
39/39 - 27s - loss: 806.2557 - loglik: -8.0441e+02 - logprior: -1.8474e+00
Epoch 6/10
39/39 - 27s - loss: 804.0837 - loglik: -8.0195e+02 - logprior: -2.1351e+00
Epoch 7/10
39/39 - 27s - loss: 802.8771 - loglik: -8.0037e+02 - logprior: -2.5040e+00
Epoch 8/10
39/39 - 27s - loss: 801.4586 - loglik: -7.9866e+02 - logprior: -2.8003e+00
Epoch 9/10
39/39 - 27s - loss: 800.3997 - loglik: -7.9737e+02 - logprior: -3.0293e+00
Epoch 10/10
39/39 - 27s - loss: 799.9316 - loglik: -7.9669e+02 - logprior: -3.2401e+00
Fitted a model with MAP estimate = -791.2281
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (40, 1), (41, 1), (42, 1), (43, 1), (45, 3), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (91, 2), (92, 1), (93, 2), (94, 1), (101, 1), (103, 1), (121, 1), (129, 2), (131, 1), (132, 2), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (162, 1), (182, 1), (184, 1), (186, 2), (187, 1), (188, 2), (206, 4), (207, 1), (208, 1), (209, 1), (213, 1), (219, 1), (220, 1), (228, 2), (241, 3), (243, 1), (246, 1), (251, 1), (258, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Fitting a model of length 362 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 819.9141 - loglik: -8.1724e+02 - logprior: -2.6734e+00
Epoch 2/2
39/39 - 40s - loss: 789.0709 - loglik: -7.8847e+02 - logprior: -6.0294e-01
Fitted a model with MAP estimate = -778.3733
expansions: []
discards: [  1  30  33 118 123 169 262 307 322 346]
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 794.0901 - loglik: -7.9289e+02 - logprior: -1.1967e+00
Epoch 2/2
39/39 - 38s - loss: 789.1816 - loglik: -7.8889e+02 - logprior: -2.8925e-01
Fitted a model with MAP estimate = -779.2682
expansions: [(119, 1)]
discards: [115 116 191]
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 785.7401 - loglik: -7.8455e+02 - logprior: -1.1867e+00
Epoch 2/10
39/39 - 40s - loss: 781.3343 - loglik: -7.8122e+02 - logprior: -1.1889e-01
Epoch 3/10
39/39 - 40s - loss: 778.0092 - loglik: -7.7801e+02 - logprior: 0.0034
Epoch 4/10
39/39 - 39s - loss: 773.6158 - loglik: -7.7359e+02 - logprior: -2.6054e-02
Epoch 5/10
39/39 - 40s - loss: 770.5887 - loglik: -7.7041e+02 - logprior: -1.7402e-01
Epoch 6/10
39/39 - 40s - loss: 767.2291 - loglik: -7.6688e+02 - logprior: -3.5049e-01
Epoch 7/10
39/39 - 40s - loss: 765.2114 - loglik: -7.6461e+02 - logprior: -5.9741e-01
Epoch 8/10
39/39 - 39s - loss: 763.2792 - loglik: -7.6246e+02 - logprior: -8.2015e-01
Epoch 9/10
39/39 - 40s - loss: 762.4270 - loglik: -7.6143e+02 - logprior: -9.9877e-01
Epoch 10/10
39/39 - 40s - loss: 760.3845 - loglik: -7.5922e+02 - logprior: -1.1652e+00
Fitted a model with MAP estimate = -759.1680
Time for alignment: 1033.8652
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 920.5493 - loglik: -9.1847e+02 - logprior: -2.0801e+00
Epoch 2/10
39/39 - 27s - loss: 826.3569 - loglik: -8.2489e+02 - logprior: -1.4625e+00
Epoch 3/10
39/39 - 27s - loss: 814.6520 - loglik: -8.1315e+02 - logprior: -1.5040e+00
Epoch 4/10
39/39 - 27s - loss: 810.1211 - loglik: -8.0849e+02 - logprior: -1.6272e+00
Epoch 5/10
39/39 - 27s - loss: 806.8873 - loglik: -8.0501e+02 - logprior: -1.8775e+00
Epoch 6/10
39/39 - 27s - loss: 804.8311 - loglik: -8.0267e+02 - logprior: -2.1620e+00
Epoch 7/10
39/39 - 27s - loss: 803.8615 - loglik: -8.0143e+02 - logprior: -2.4273e+00
Epoch 8/10
39/39 - 27s - loss: 802.8623 - loglik: -8.0016e+02 - logprior: -2.6985e+00
Epoch 9/10
39/39 - 27s - loss: 802.0588 - loglik: -7.9917e+02 - logprior: -2.8885e+00
Epoch 10/10
39/39 - 27s - loss: 801.3824 - loglik: -7.9824e+02 - logprior: -3.1381e+00
Fitted a model with MAP estimate = -792.9202
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (30, 1), (32, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 3), (103, 1), (120, 1), (131, 1), (134, 1), (145, 1), (148, 1), (150, 1), (155, 1), (156, 2), (169, 1), (182, 1), (185, 1), (186, 2), (187, 1), (188, 2), (189, 1), (190, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (223, 1), (224, 2), (226, 2), (227, 1), (229, 1), (240, 3), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (269, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Fitting a model of length 360 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 821.0581 - loglik: -8.1847e+02 - logprior: -2.5838e+00
Epoch 2/2
39/39 - 40s - loss: 792.5217 - loglik: -7.9189e+02 - logprior: -6.2701e-01
Fitted a model with MAP estimate = -781.7767
expansions: []
discards: [  0   1  32 105 122 229 236 258 284 288 305 320 344]
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 797.5262 - loglik: -7.9608e+02 - logprior: -1.4495e+00
Epoch 2/2
39/39 - 39s - loss: 792.1450 - loglik: -7.9196e+02 - logprior: -1.8164e-01
Fitted a model with MAP estimate = -781.8261
expansions: [(0, 2)]
discards: []
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 788.1867 - loglik: -7.8671e+02 - logprior: -1.4776e+00
Epoch 2/10
39/39 - 39s - loss: 783.5718 - loglik: -7.8346e+02 - logprior: -1.1565e-01
Epoch 3/10
39/39 - 39s - loss: 780.4139 - loglik: -7.8041e+02 - logprior: -1.7300e-03
Epoch 4/10
39/39 - 39s - loss: 776.8734 - loglik: -7.7686e+02 - logprior: -1.5709e-02
Epoch 5/10
39/39 - 39s - loss: 772.7260 - loglik: -7.7256e+02 - logprior: -1.6327e-01
Epoch 6/10
39/39 - 40s - loss: 769.7436 - loglik: -7.6942e+02 - logprior: -3.2025e-01
Epoch 7/10
39/39 - 39s - loss: 767.9164 - loglik: -7.6736e+02 - logprior: -5.6068e-01
Epoch 8/10
39/39 - 40s - loss: 766.1801 - loglik: -7.6538e+02 - logprior: -8.0193e-01
Epoch 9/10
39/39 - 40s - loss: 764.0252 - loglik: -7.6304e+02 - logprior: -9.8505e-01
Epoch 10/10
39/39 - 39s - loss: 763.5466 - loglik: -7.6242e+02 - logprior: -1.1232e+00
Fitted a model with MAP estimate = -761.9383
Time for alignment: 1027.8163
Computed alignments with likelihoods: ['-759.3498', '-759.1680', '-761.9383']
Best model has likelihood: -759.1680
SP score = 0.4568
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f529459d070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5237be25e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294522eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5238681b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 895.2003 - loglik: -8.9292e+02 - logprior: -2.2815e+00
Epoch 2/10
39/39 - 41s - loss: 756.3568 - loglik: -7.5468e+02 - logprior: -1.6743e+00
Epoch 3/10
39/39 - 41s - loss: 744.8613 - loglik: -7.4302e+02 - logprior: -1.8400e+00
Epoch 4/10
39/39 - 41s - loss: 742.2361 - loglik: -7.4034e+02 - logprior: -1.9005e+00
Epoch 5/10
39/39 - 41s - loss: 740.9081 - loglik: -7.3882e+02 - logprior: -2.0867e+00
Epoch 6/10
39/39 - 41s - loss: 739.7026 - loglik: -7.3749e+02 - logprior: -2.2159e+00
Epoch 7/10
39/39 - 41s - loss: 741.1713 - loglik: -7.3871e+02 - logprior: -2.4641e+00
Fitted a model with MAP estimate = -739.0831
expansions: [(9, 1), (19, 1), (21, 1), (34, 1), (66, 1), (77, 1), (100, 2), (103, 1), (105, 1), (106, 1), (107, 1), (113, 1), (118, 1), (119, 1), (137, 2), (138, 1), (139, 1), (140, 3), (141, 2), (142, 1), (157, 1), (159, 2), (160, 1), (165, 1), (172, 1), (174, 1), (175, 5), (178, 1), (179, 4), (181, 1), (183, 1), (185, 1), (186, 2), (187, 2), (188, 1), (189, 1), (201, 1), (203, 1), (209, 3), (217, 1), (218, 2), (219, 1), (227, 1), (236, 1), (239, 1), (241, 1), (245, 1), (251, 1), (253, 1), (255, 1), (266, 1), (275, 2), (276, 2), (277, 1), (280, 2), (281, 3), (295, 1), (311, 1), (313, 1), (317, 1), (319, 1), (321, 1)]
discards: [  1 190 191 192 193 194 195 205 206]
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 731.8259 - loglik: -7.2979e+02 - logprior: -2.0333e+00
Epoch 2/2
39/39 - 57s - loss: 717.1020 - loglik: -7.1653e+02 - logprior: -5.6735e-01
Fitted a model with MAP estimate = -714.4218
expansions: [(232, 1), (244, 1), (284, 1), (285, 2), (295, 1)]
discards: [160 162 208 209 218 240 245 246 247 248 249 250 251 254 266 339 346 347]
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 724.1384 - loglik: -7.2281e+02 - logprior: -1.3279e+00
Epoch 2/2
39/39 - 54s - loss: 717.8390 - loglik: -7.1775e+02 - logprior: -9.1806e-02
Fitted a model with MAP estimate = -714.7776
expansions: [(5, 1)]
discards: [214 215 271]
Fitting a model of length 388 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 719.4658 - loglik: -7.1840e+02 - logprior: -1.0699e+00
Epoch 2/10
39/39 - 54s - loss: 715.4545 - loglik: -7.1560e+02 - logprior: 0.1444
Epoch 3/10
39/39 - 54s - loss: 713.8151 - loglik: -7.1412e+02 - logprior: 0.3032
Epoch 4/10
39/39 - 54s - loss: 712.5645 - loglik: -7.1285e+02 - logprior: 0.2896
Epoch 5/10
39/39 - 54s - loss: 711.0447 - loglik: -7.1125e+02 - logprior: 0.2097
Epoch 6/10
39/39 - 54s - loss: 710.0493 - loglik: -7.1028e+02 - logprior: 0.2261
Epoch 7/10
39/39 - 54s - loss: 711.4948 - loglik: -7.1169e+02 - logprior: 0.1972
Fitted a model with MAP estimate = -709.8977
Time for alignment: 1138.7552
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 894.1760 - loglik: -8.9191e+02 - logprior: -2.2648e+00
Epoch 2/10
39/39 - 41s - loss: 755.9760 - loglik: -7.5435e+02 - logprior: -1.6277e+00
Epoch 3/10
39/39 - 41s - loss: 744.7484 - loglik: -7.4295e+02 - logprior: -1.7947e+00
Epoch 4/10
39/39 - 41s - loss: 741.1351 - loglik: -7.3925e+02 - logprior: -1.8885e+00
Epoch 5/10
39/39 - 41s - loss: 740.2136 - loglik: -7.3815e+02 - logprior: -2.0655e+00
Epoch 6/10
39/39 - 41s - loss: 739.7823 - loglik: -7.3756e+02 - logprior: -2.2174e+00
Epoch 7/10
39/39 - 41s - loss: 738.7114 - loglik: -7.3629e+02 - logprior: -2.4255e+00
Epoch 8/10
39/39 - 41s - loss: 738.9461 - loglik: -7.3637e+02 - logprior: -2.5743e+00
Fitted a model with MAP estimate = -737.6799
expansions: [(9, 1), (19, 2), (31, 1), (66, 1), (77, 1), (100, 1), (104, 2), (105, 1), (108, 1), (115, 1), (120, 1), (139, 2), (140, 1), (141, 1), (142, 3), (143, 2), (144, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (166, 1), (173, 1), (175, 1), (176, 5), (179, 1), (180, 5), (182, 1), (184, 1), (186, 1), (187, 2), (188, 1), (189, 1), (202, 3), (210, 3), (218, 1), (219, 2), (220, 1), (225, 1), (233, 1), (237, 1), (240, 1), (241, 2), (242, 2), (245, 1), (251, 1), (253, 1), (267, 1), (277, 1), (279, 2), (282, 1), (283, 1), (284, 1), (289, 1), (298, 1), (307, 1), (311, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195 206 207 208]
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 730.5939 - loglik: -7.2871e+02 - logprior: -1.8866e+00
Epoch 2/2
39/39 - 57s - loss: 717.3975 - loglik: -7.1698e+02 - logprior: -4.2225e-01
Fitted a model with MAP estimate = -714.5862
expansions: [(248, 2), (249, 1), (296, 1)]
discards: [160 162 208 209 218 254 266 297 298]
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 720.8779 - loglik: -7.1950e+02 - logprior: -1.3744e+00
Epoch 2/2
39/39 - 56s - loss: 716.8382 - loglik: -7.1674e+02 - logprior: -1.0273e-01
Fitted a model with MAP estimate = -714.2433
expansions: [(5, 1)]
discards: [102 214 243 247 248]
Fitting a model of length 393 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 720.6964 - loglik: -7.1950e+02 - logprior: -1.1956e+00
Epoch 2/10
39/39 - 55s - loss: 716.3034 - loglik: -7.1631e+02 - logprior: 0.0083
Epoch 3/10
39/39 - 55s - loss: 714.9526 - loglik: -7.1511e+02 - logprior: 0.1620
Epoch 4/10
39/39 - 55s - loss: 713.3746 - loglik: -7.1355e+02 - logprior: 0.1742
Epoch 5/10
39/39 - 55s - loss: 712.2567 - loglik: -7.1244e+02 - logprior: 0.1844
Epoch 6/10
39/39 - 55s - loss: 711.1755 - loglik: -7.1131e+02 - logprior: 0.1313
Epoch 7/10
39/39 - 55s - loss: 711.4681 - loglik: -7.1150e+02 - logprior: 0.0363
Fitted a model with MAP estimate = -710.9297
Time for alignment: 1190.2517
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 891.7320 - loglik: -8.8946e+02 - logprior: -2.2704e+00
Epoch 2/10
39/39 - 41s - loss: 756.1718 - loglik: -7.5464e+02 - logprior: -1.5354e+00
Epoch 3/10
39/39 - 41s - loss: 745.2884 - loglik: -7.4360e+02 - logprior: -1.6926e+00
Epoch 4/10
39/39 - 41s - loss: 742.0549 - loglik: -7.4018e+02 - logprior: -1.8792e+00
Epoch 5/10
39/39 - 41s - loss: 740.8168 - loglik: -7.3882e+02 - logprior: -1.9986e+00
Epoch 6/10
39/39 - 41s - loss: 740.3403 - loglik: -7.3818e+02 - logprior: -2.1581e+00
Epoch 7/10
39/39 - 41s - loss: 739.2249 - loglik: -7.3689e+02 - logprior: -2.3398e+00
Epoch 8/10
39/39 - 41s - loss: 740.1437 - loglik: -7.3762e+02 - logprior: -2.5258e+00
Fitted a model with MAP estimate = -738.3924
expansions: [(9, 1), (19, 2), (34, 1), (63, 2), (66, 1), (68, 1), (96, 1), (99, 1), (102, 1), (103, 1), (104, 1), (105, 1), (113, 1), (118, 1), (119, 1), (137, 2), (138, 1), (139, 1), (141, 3), (142, 2), (157, 1), (159, 2), (160, 1), (161, 1), (165, 1), (174, 1), (175, 5), (178, 5), (179, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 2), (199, 1), (209, 3), (217, 1), (218, 1), (219, 1), (227, 1), (232, 1), (233, 2), (236, 1), (239, 1), (241, 1), (248, 2), (249, 1), (255, 1), (276, 1), (277, 1), (278, 1), (281, 1), (282, 2), (283, 2), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 192 193 194 195]
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 728.9160 - loglik: -7.2694e+02 - logprior: -1.9743e+00
Epoch 2/2
39/39 - 57s - loss: 715.7254 - loglik: -7.1533e+02 - logprior: -3.9231e-01
Fitted a model with MAP estimate = -712.3041
expansions: [(5, 1), (224, 1), (236, 2), (237, 1), (299, 1)]
discards: [ 67 209 210 219 241 242 256 287 288 350 351]
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 720.7850 - loglik: -7.1953e+02 - logprior: -1.2570e+00
Epoch 2/2
39/39 - 56s - loss: 715.0126 - loglik: -7.1497e+02 - logprior: -3.8795e-02
Fitted a model with MAP estimate = -712.6010
expansions: [(234, 1), (246, 1), (285, 2)]
discards: [213 214 238 239 240 241]
Fitting a model of length 397 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 719.6553 - loglik: -7.1861e+02 - logprior: -1.0440e+00
Epoch 2/10
39/39 - 55s - loss: 714.3067 - loglik: -7.1452e+02 - logprior: 0.2142
Epoch 3/10
39/39 - 55s - loss: 711.2488 - loglik: -7.1149e+02 - logprior: 0.2401
Epoch 4/10
39/39 - 55s - loss: 710.3432 - loglik: -7.1068e+02 - logprior: 0.3332
Epoch 5/10
39/39 - 56s - loss: 709.4753 - loglik: -7.0979e+02 - logprior: 0.3152
Epoch 6/10
39/39 - 55s - loss: 709.6135 - loglik: -7.0978e+02 - logprior: 0.1628
Fitted a model with MAP estimate = -708.7172
Time for alignment: 1145.0715
Computed alignments with likelihoods: ['-709.8977', '-710.9297', '-708.7172']
Best model has likelihood: -708.7172
SP score = 0.8057
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5149dcadf0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5237a20d60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5294840730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529427bd00>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.7688 - loglik: -2.4939e+02 - logprior: -3.3780e+00
Epoch 2/10
19/19 - 2s - loss: 223.4462 - loglik: -2.2219e+02 - logprior: -1.2521e+00
Epoch 3/10
19/19 - 2s - loss: 210.9495 - loglik: -2.0958e+02 - logprior: -1.3694e+00
Epoch 4/10
19/19 - 2s - loss: 207.7654 - loglik: -2.0648e+02 - logprior: -1.2809e+00
Epoch 5/10
19/19 - 2s - loss: 206.4619 - loglik: -2.0514e+02 - logprior: -1.3208e+00
Epoch 6/10
19/19 - 2s - loss: 206.0291 - loglik: -2.0467e+02 - logprior: -1.3566e+00
Epoch 7/10
19/19 - 2s - loss: 205.7688 - loglik: -2.0439e+02 - logprior: -1.3831e+00
Epoch 8/10
19/19 - 2s - loss: 205.4638 - loglik: -2.0406e+02 - logprior: -1.4054e+00
Epoch 9/10
19/19 - 2s - loss: 205.3440 - loglik: -2.0391e+02 - logprior: -1.4344e+00
Epoch 10/10
19/19 - 2s - loss: 205.3326 - loglik: -2.0387e+02 - logprior: -1.4633e+00
Fitted a model with MAP estimate = -205.0914
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (17, 1), (18, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (60, 4)]
discards: [0]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.4824 - loglik: -2.0441e+02 - logprior: -4.0739e+00
Epoch 2/2
19/19 - 2s - loss: 200.5838 - loglik: -1.9849e+02 - logprior: -2.0954e+00
Fitted a model with MAP estimate = -199.1027
expansions: [(0, 2)]
discards: [ 0 13 65]
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 200.5144 - loglik: -1.9763e+02 - logprior: -2.8865e+00
Epoch 2/2
19/19 - 2s - loss: 197.4528 - loglik: -1.9636e+02 - logprior: -1.0915e+00
Fitted a model with MAP estimate = -196.7263
expansions: []
discards: [ 0 51]
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.8638 - loglik: -1.9819e+02 - logprior: -3.6730e+00
Epoch 2/10
19/19 - 2s - loss: 197.6782 - loglik: -1.9647e+02 - logprior: -1.2103e+00
Epoch 3/10
19/19 - 2s - loss: 197.0523 - loglik: -1.9603e+02 - logprior: -1.0249e+00
Epoch 4/10
19/19 - 2s - loss: 196.4896 - loglik: -1.9548e+02 - logprior: -1.0049e+00
Epoch 5/10
19/19 - 2s - loss: 196.0035 - loglik: -1.9499e+02 - logprior: -1.0148e+00
Epoch 6/10
19/19 - 2s - loss: 195.6141 - loglik: -1.9459e+02 - logprior: -1.0283e+00
Epoch 7/10
19/19 - 2s - loss: 195.6121 - loglik: -1.9456e+02 - logprior: -1.0490e+00
Epoch 8/10
19/19 - 2s - loss: 195.0939 - loglik: -1.9403e+02 - logprior: -1.0601e+00
Epoch 9/10
19/19 - 2s - loss: 195.1488 - loglik: -1.9405e+02 - logprior: -1.0973e+00
Fitted a model with MAP estimate = -194.9070
Time for alignment: 78.4075
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.1626 - loglik: -2.4979e+02 - logprior: -3.3720e+00
Epoch 2/10
19/19 - 2s - loss: 221.4679 - loglik: -2.2018e+02 - logprior: -1.2832e+00
Epoch 3/10
19/19 - 2s - loss: 207.6722 - loglik: -2.0615e+02 - logprior: -1.5182e+00
Epoch 4/10
19/19 - 2s - loss: 205.1705 - loglik: -2.0375e+02 - logprior: -1.4242e+00
Epoch 5/10
19/19 - 2s - loss: 204.3238 - loglik: -2.0286e+02 - logprior: -1.4594e+00
Epoch 6/10
19/19 - 2s - loss: 204.0578 - loglik: -2.0256e+02 - logprior: -1.5017e+00
Epoch 7/10
19/19 - 2s - loss: 203.9432 - loglik: -2.0241e+02 - logprior: -1.5305e+00
Epoch 8/10
19/19 - 2s - loss: 203.5683 - loglik: -2.0201e+02 - logprior: -1.5550e+00
Epoch 9/10
19/19 - 2s - loss: 203.6371 - loglik: -2.0206e+02 - logprior: -1.5800e+00
Fitted a model with MAP estimate = -203.3395
expansions: [(9, 1), (10, 1), (11, 1), (13, 2), (14, 1), (15, 1), (32, 2), (34, 1), (39, 2), (40, 3), (42, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.5193 - loglik: -2.0441e+02 - logprior: -4.1129e+00
Epoch 2/2
19/19 - 2s - loss: 200.6577 - loglik: -1.9854e+02 - logprior: -2.1151e+00
Fitted a model with MAP estimate = -199.1583
expansions: [(0, 2)]
discards: [ 0 39 49 52 66 68]
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.6272 - loglik: -1.9775e+02 - logprior: -2.8731e+00
Epoch 2/2
19/19 - 2s - loss: 197.5175 - loglik: -1.9643e+02 - logprior: -1.0833e+00
Fitted a model with MAP estimate = -196.7798
expansions: [(54, 1)]
discards: [0]
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 201.8855 - loglik: -1.9817e+02 - logprior: -3.7188e+00
Epoch 2/10
19/19 - 2s - loss: 197.6333 - loglik: -1.9638e+02 - logprior: -1.2567e+00
Epoch 3/10
19/19 - 2s - loss: 196.7197 - loglik: -1.9571e+02 - logprior: -1.0119e+00
Epoch 4/10
19/19 - 2s - loss: 196.2979 - loglik: -1.9530e+02 - logprior: -1.0015e+00
Epoch 5/10
19/19 - 2s - loss: 195.6549 - loglik: -1.9465e+02 - logprior: -1.0068e+00
Epoch 6/10
19/19 - 2s - loss: 195.5592 - loglik: -1.9452e+02 - logprior: -1.0366e+00
Epoch 7/10
19/19 - 2s - loss: 195.1382 - loglik: -1.9409e+02 - logprior: -1.0436e+00
Epoch 8/10
19/19 - 2s - loss: 195.0669 - loglik: -1.9401e+02 - logprior: -1.0598e+00
Epoch 9/10
19/19 - 2s - loss: 194.8809 - loglik: -1.9380e+02 - logprior: -1.0852e+00
Epoch 10/10
19/19 - 2s - loss: 194.6555 - loglik: -1.9352e+02 - logprior: -1.1316e+00
Fitted a model with MAP estimate = -194.4617
Time for alignment: 79.1000
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 253.1138 - loglik: -2.4973e+02 - logprior: -3.3807e+00
Epoch 2/10
19/19 - 2s - loss: 222.8139 - loglik: -2.2151e+02 - logprior: -1.3071e+00
Epoch 3/10
19/19 - 2s - loss: 208.5999 - loglik: -2.0711e+02 - logprior: -1.4920e+00
Epoch 4/10
19/19 - 2s - loss: 205.8669 - loglik: -2.0450e+02 - logprior: -1.3712e+00
Epoch 5/10
19/19 - 2s - loss: 204.8724 - loglik: -2.0346e+02 - logprior: -1.4169e+00
Epoch 6/10
19/19 - 2s - loss: 204.7864 - loglik: -2.0335e+02 - logprior: -1.4400e+00
Epoch 7/10
19/19 - 2s - loss: 204.2537 - loglik: -2.0278e+02 - logprior: -1.4772e+00
Epoch 8/10
19/19 - 2s - loss: 203.9911 - loglik: -2.0249e+02 - logprior: -1.5042e+00
Epoch 9/10
19/19 - 2s - loss: 203.9616 - loglik: -2.0244e+02 - logprior: -1.5229e+00
Epoch 10/10
19/19 - 2s - loss: 204.0849 - loglik: -2.0253e+02 - logprior: -1.5576e+00
Fitted a model with MAP estimate = -203.7394
expansions: [(8, 1), (9, 3), (10, 1), (12, 1), (14, 1), (19, 1), (24, 1), (34, 1), (39, 2), (40, 3), (42, 2), (47, 3), (48, 1), (49, 1), (60, 2)]
discards: [0]
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.8091 - loglik: -2.0371e+02 - logprior: -4.0991e+00
Epoch 2/2
19/19 - 2s - loss: 200.2637 - loglik: -1.9817e+02 - logprior: -2.0985e+00
Fitted a model with MAP estimate = -198.9970
expansions: [(0, 2)]
discards: [ 0 10 49 52 64]
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.3036 - loglik: -1.9743e+02 - logprior: -2.8708e+00
Epoch 2/2
19/19 - 2s - loss: 197.1039 - loglik: -1.9602e+02 - logprior: -1.0867e+00
Fitted a model with MAP estimate = -196.5488
expansions: []
discards: [0]
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.5971 - loglik: -1.9792e+02 - logprior: -3.6729e+00
Epoch 2/10
19/19 - 2s - loss: 197.5782 - loglik: -1.9636e+02 - logprior: -1.2205e+00
Epoch 3/10
19/19 - 2s - loss: 196.7158 - loglik: -1.9570e+02 - logprior: -1.0177e+00
Epoch 4/10
19/19 - 2s - loss: 196.3344 - loglik: -1.9534e+02 - logprior: -9.9423e-01
Epoch 5/10
19/19 - 2s - loss: 195.6510 - loglik: -1.9464e+02 - logprior: -1.0104e+00
Epoch 6/10
19/19 - 2s - loss: 195.4681 - loglik: -1.9443e+02 - logprior: -1.0389e+00
Epoch 7/10
19/19 - 2s - loss: 195.1012 - loglik: -1.9406e+02 - logprior: -1.0418e+00
Epoch 8/10
19/19 - 2s - loss: 195.1862 - loglik: -1.9412e+02 - logprior: -1.0684e+00
Fitted a model with MAP estimate = -194.7929
Time for alignment: 76.1010
Computed alignments with likelihoods: ['-194.9070', '-194.4617', '-194.7929']
Best model has likelihood: -194.4617
SP score = 0.8369
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f511f122310>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52943671c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50db99cfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523ce28a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 302.4587 - loglik: -2.9905e+02 - logprior: -3.4042e+00
Epoch 2/10
19/19 - 2s - loss: 276.0998 - loglik: -2.7484e+02 - logprior: -1.2624e+00
Epoch 3/10
19/19 - 2s - loss: 260.4893 - loglik: -2.5923e+02 - logprior: -1.2630e+00
Epoch 4/10
19/19 - 2s - loss: 256.9656 - loglik: -2.5579e+02 - logprior: -1.1747e+00
Epoch 5/10
19/19 - 2s - loss: 255.8635 - loglik: -2.5467e+02 - logprior: -1.1967e+00
Epoch 6/10
19/19 - 2s - loss: 255.2174 - loglik: -2.5397e+02 - logprior: -1.2499e+00
Epoch 7/10
19/19 - 2s - loss: 254.5958 - loglik: -2.5330e+02 - logprior: -1.2935e+00
Epoch 8/10
19/19 - 2s - loss: 253.8672 - loglik: -2.5256e+02 - logprior: -1.3032e+00
Epoch 9/10
19/19 - 2s - loss: 254.0687 - loglik: -2.5272e+02 - logprior: -1.3537e+00
Fitted a model with MAP estimate = -253.1071
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (32, 1), (41, 3), (42, 1), (52, 1), (53, 1), (54, 1), (56, 1), (75, 3), (76, 2), (77, 2)]
discards: []
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 260.3141 - loglik: -2.5599e+02 - logprior: -4.3202e+00
Epoch 2/2
19/19 - 3s - loss: 253.4175 - loglik: -2.5215e+02 - logprior: -1.2660e+00
Fitted a model with MAP estimate = -251.8908
expansions: [(52, 1)]
discards: [1]
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.1588 - loglik: -2.5231e+02 - logprior: -2.8464e+00
Epoch 2/2
19/19 - 3s - loss: 252.7140 - loglik: -2.5161e+02 - logprior: -1.1044e+00
Fitted a model with MAP estimate = -251.3944
expansions: []
discards: []
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 254.3123 - loglik: -2.5155e+02 - logprior: -2.7657e+00
Epoch 2/10
19/19 - 3s - loss: 251.7500 - loglik: -2.5068e+02 - logprior: -1.0745e+00
Epoch 3/10
19/19 - 3s - loss: 251.2404 - loglik: -2.5029e+02 - logprior: -9.4596e-01
Epoch 4/10
19/19 - 3s - loss: 250.3880 - loglik: -2.4943e+02 - logprior: -9.5448e-01
Epoch 5/10
19/19 - 3s - loss: 249.6532 - loglik: -2.4866e+02 - logprior: -9.9569e-01
Epoch 6/10
19/19 - 3s - loss: 248.9356 - loglik: -2.4793e+02 - logprior: -1.0096e+00
Epoch 7/10
19/19 - 3s - loss: 248.7417 - loglik: -2.4772e+02 - logprior: -1.0181e+00
Epoch 8/10
19/19 - 3s - loss: 247.6682 - loglik: -2.4661e+02 - logprior: -1.0533e+00
Epoch 9/10
19/19 - 3s - loss: 247.4316 - loglik: -2.4635e+02 - logprior: -1.0780e+00
Epoch 10/10
19/19 - 3s - loss: 246.9654 - loglik: -2.4585e+02 - logprior: -1.1168e+00
Fitted a model with MAP estimate = -246.8566
Time for alignment: 92.3857
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 302.3906 - loglik: -2.9899e+02 - logprior: -3.4013e+00
Epoch 2/10
19/19 - 2s - loss: 273.6289 - loglik: -2.7236e+02 - logprior: -1.2689e+00
Epoch 3/10
19/19 - 2s - loss: 260.6192 - loglik: -2.5934e+02 - logprior: -1.2779e+00
Epoch 4/10
19/19 - 2s - loss: 257.8022 - loglik: -2.5665e+02 - logprior: -1.1567e+00
Epoch 5/10
19/19 - 2s - loss: 256.9132 - loglik: -2.5574e+02 - logprior: -1.1723e+00
Epoch 6/10
19/19 - 2s - loss: 255.8910 - loglik: -2.5469e+02 - logprior: -1.2007e+00
Epoch 7/10
19/19 - 2s - loss: 255.5149 - loglik: -2.5426e+02 - logprior: -1.2563e+00
Epoch 8/10
19/19 - 2s - loss: 255.2944 - loglik: -2.5403e+02 - logprior: -1.2691e+00
Epoch 9/10
19/19 - 2s - loss: 254.8807 - loglik: -2.5356e+02 - logprior: -1.3181e+00
Epoch 10/10
19/19 - 2s - loss: 254.7224 - loglik: -2.5333e+02 - logprior: -1.3902e+00
Fitted a model with MAP estimate = -253.8006
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (28, 1), (38, 1), (42, 2), (44, 1), (51, 2), (52, 5), (74, 2), (75, 2), (76, 3), (77, 2)]
discards: []
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.5356 - loglik: -2.5715e+02 - logprior: -4.3841e+00
Epoch 2/2
19/19 - 3s - loss: 253.8266 - loglik: -2.5255e+02 - logprior: -1.2762e+00
Fitted a model with MAP estimate = -251.7924
expansions: []
discards: [ 1 28 96]
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.0232 - loglik: -2.5219e+02 - logprior: -2.8310e+00
Epoch 2/2
19/19 - 3s - loss: 252.3679 - loglik: -2.5126e+02 - logprior: -1.1030e+00
Fitted a model with MAP estimate = -251.0955
expansions: []
discards: []
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 253.9496 - loglik: -2.5118e+02 - logprior: -2.7679e+00
Epoch 2/10
19/19 - 3s - loss: 251.6786 - loglik: -2.5058e+02 - logprior: -1.0961e+00
Epoch 3/10
19/19 - 3s - loss: 250.7534 - loglik: -2.4979e+02 - logprior: -9.6811e-01
Epoch 4/10
19/19 - 3s - loss: 250.4151 - loglik: -2.4944e+02 - logprior: -9.7903e-01
Epoch 5/10
19/19 - 3s - loss: 249.3311 - loglik: -2.4833e+02 - logprior: -9.9967e-01
Epoch 6/10
19/19 - 3s - loss: 248.5032 - loglik: -2.4749e+02 - logprior: -1.0137e+00
Epoch 7/10
19/19 - 3s - loss: 248.0038 - loglik: -2.4698e+02 - logprior: -1.0210e+00
Epoch 8/10
19/19 - 3s - loss: 247.7216 - loglik: -2.4667e+02 - logprior: -1.0492e+00
Epoch 9/10
19/19 - 3s - loss: 247.4187 - loglik: -2.4635e+02 - logprior: -1.0701e+00
Epoch 10/10
19/19 - 3s - loss: 246.8887 - loglik: -2.4577e+02 - logprior: -1.1144e+00
Fitted a model with MAP estimate = -246.5899
Time for alignment: 93.5914
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 302.2231 - loglik: -2.9882e+02 - logprior: -3.3988e+00
Epoch 2/10
19/19 - 2s - loss: 271.7875 - loglik: -2.7053e+02 - logprior: -1.2586e+00
Epoch 3/10
19/19 - 2s - loss: 259.2650 - loglik: -2.5804e+02 - logprior: -1.2222e+00
Epoch 4/10
19/19 - 2s - loss: 256.5694 - loglik: -2.5543e+02 - logprior: -1.1358e+00
Epoch 5/10
19/19 - 2s - loss: 255.3571 - loglik: -2.5421e+02 - logprior: -1.1515e+00
Epoch 6/10
19/19 - 2s - loss: 254.7174 - loglik: -2.5352e+02 - logprior: -1.1967e+00
Epoch 7/10
19/19 - 2s - loss: 253.6216 - loglik: -2.5239e+02 - logprior: -1.2325e+00
Epoch 8/10
19/19 - 2s - loss: 253.8349 - loglik: -2.5257e+02 - logprior: -1.2604e+00
Fitted a model with MAP estimate = -252.8957
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (52, 2), (53, 3), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 259.9844 - loglik: -2.5570e+02 - logprior: -4.2857e+00
Epoch 2/2
19/19 - 3s - loss: 253.3063 - loglik: -2.5203e+02 - logprior: -1.2802e+00
Fitted a model with MAP estimate = -251.6512
expansions: []
discards: [ 1 27 67 96 99]
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.1436 - loglik: -2.5230e+02 - logprior: -2.8404e+00
Epoch 2/2
19/19 - 3s - loss: 252.4452 - loglik: -2.5134e+02 - logprior: -1.1046e+00
Fitted a model with MAP estimate = -251.2617
expansions: [(51, 1)]
discards: []
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.1946 - loglik: -2.5143e+02 - logprior: -2.7669e+00
Epoch 2/10
19/19 - 3s - loss: 251.6116 - loglik: -2.5052e+02 - logprior: -1.0896e+00
Epoch 3/10
19/19 - 3s - loss: 251.0125 - loglik: -2.5005e+02 - logprior: -9.6560e-01
Epoch 4/10
19/19 - 3s - loss: 250.3423 - loglik: -2.4937e+02 - logprior: -9.7524e-01
Epoch 5/10
19/19 - 3s - loss: 249.1558 - loglik: -2.4815e+02 - logprior: -1.0050e+00
Epoch 6/10
19/19 - 3s - loss: 249.0384 - loglik: -2.4802e+02 - logprior: -1.0209e+00
Epoch 7/10
19/19 - 3s - loss: 247.9621 - loglik: -2.4693e+02 - logprior: -1.0328e+00
Epoch 8/10
19/19 - 3s - loss: 248.0275 - loglik: -2.4697e+02 - logprior: -1.0602e+00
Fitted a model with MAP estimate = -247.3819
Time for alignment: 82.6541
Computed alignments with likelihoods: ['-246.8566', '-246.5899', '-247.3819']
Best model has likelihood: -246.5899
SP score = 0.8858
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5203286610>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50db95ab80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523cd54c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50ec759700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.6370 - loglik: -1.8320e+02 - logprior: -3.4388e+00
Epoch 2/10
19/19 - 2s - loss: 154.2905 - loglik: -1.5283e+02 - logprior: -1.4622e+00
Epoch 3/10
19/19 - 2s - loss: 143.2331 - loglik: -1.4164e+02 - logprior: -1.5924e+00
Epoch 4/10
19/19 - 2s - loss: 139.5505 - loglik: -1.3806e+02 - logprior: -1.4932e+00
Epoch 5/10
19/19 - 2s - loss: 138.7668 - loglik: -1.3724e+02 - logprior: -1.5234e+00
Epoch 6/10
19/19 - 2s - loss: 137.9519 - loglik: -1.3642e+02 - logprior: -1.5356e+00
Epoch 7/10
19/19 - 1s - loss: 137.4990 - loglik: -1.3592e+02 - logprior: -1.5785e+00
Epoch 8/10
19/19 - 2s - loss: 137.1740 - loglik: -1.3555e+02 - logprior: -1.6192e+00
Epoch 9/10
19/19 - 2s - loss: 136.5375 - loglik: -1.3488e+02 - logprior: -1.6530e+00
Epoch 10/10
19/19 - 2s - loss: 136.4321 - loglik: -1.3474e+02 - logprior: -1.6879e+00
Fitted a model with MAP estimate = -136.3071
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 2), (23, 1), (24, 1), (34, 1), (38, 1), (42, 1), (43, 1), (44, 1), (46, 1)]
discards: [0]
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 140.1633 - loglik: -1.3596e+02 - logprior: -4.2033e+00
Epoch 2/2
19/19 - 2s - loss: 131.1055 - loglik: -1.2905e+02 - logprior: -2.0512e+00
Fitted a model with MAP estimate = -129.5896
expansions: [(0, 2)]
discards: [ 0 27]
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.8030 - loglik: -1.2780e+02 - logprior: -3.0060e+00
Epoch 2/2
19/19 - 2s - loss: 127.6193 - loglik: -1.2646e+02 - logprior: -1.1597e+00
Fitted a model with MAP estimate = -126.9822
expansions: []
discards: [0]
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.1067 - loglik: -1.2932e+02 - logprior: -3.7864e+00
Epoch 2/10
19/19 - 2s - loss: 128.5582 - loglik: -1.2722e+02 - logprior: -1.3374e+00
Epoch 3/10
19/19 - 2s - loss: 127.5565 - loglik: -1.2642e+02 - logprior: -1.1406e+00
Epoch 4/10
19/19 - 2s - loss: 126.6626 - loglik: -1.2554e+02 - logprior: -1.1189e+00
Epoch 5/10
19/19 - 2s - loss: 126.1000 - loglik: -1.2498e+02 - logprior: -1.1190e+00
Epoch 6/10
19/19 - 2s - loss: 125.8034 - loglik: -1.2466e+02 - logprior: -1.1398e+00
Epoch 7/10
19/19 - 2s - loss: 125.4096 - loglik: -1.2426e+02 - logprior: -1.1449e+00
Epoch 8/10
19/19 - 2s - loss: 125.0794 - loglik: -1.2391e+02 - logprior: -1.1691e+00
Epoch 9/10
19/19 - 2s - loss: 124.9810 - loglik: -1.2378e+02 - logprior: -1.1975e+00
Epoch 10/10
19/19 - 2s - loss: 124.6895 - loglik: -1.2348e+02 - logprior: -1.2126e+00
Fitted a model with MAP estimate = -124.7817
Time for alignment: 63.2376
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.7417 - loglik: -1.8330e+02 - logprior: -3.4388e+00
Epoch 2/10
19/19 - 2s - loss: 155.5273 - loglik: -1.5390e+02 - logprior: -1.6256e+00
Epoch 3/10
19/19 - 2s - loss: 143.5727 - loglik: -1.4198e+02 - logprior: -1.5897e+00
Epoch 4/10
19/19 - 2s - loss: 140.1714 - loglik: -1.3859e+02 - logprior: -1.5812e+00
Epoch 5/10
19/19 - 2s - loss: 139.0041 - loglik: -1.3744e+02 - logprior: -1.5613e+00
Epoch 6/10
19/19 - 2s - loss: 138.3122 - loglik: -1.3673e+02 - logprior: -1.5844e+00
Epoch 7/10
19/19 - 2s - loss: 138.1358 - loglik: -1.3652e+02 - logprior: -1.6162e+00
Epoch 8/10
19/19 - 2s - loss: 137.7613 - loglik: -1.3614e+02 - logprior: -1.6225e+00
Epoch 9/10
19/19 - 1s - loss: 137.9086 - loglik: -1.3627e+02 - logprior: -1.6385e+00
Fitted a model with MAP estimate = -137.5431
expansions: [(16, 1), (17, 1), (18, 1), (22, 2), (23, 3), (24, 1), (34, 1), (35, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 136.5823 - loglik: -1.3319e+02 - logprior: -3.3881e+00
Epoch 2/2
19/19 - 2s - loss: 128.6859 - loglik: -1.2729e+02 - logprior: -1.3946e+00
Fitted a model with MAP estimate = -127.6102
expansions: []
discards: [28]
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.0175 - loglik: -1.2783e+02 - logprior: -3.1896e+00
Epoch 2/2
19/19 - 2s - loss: 127.9508 - loglik: -1.2664e+02 - logprior: -1.3075e+00
Fitted a model with MAP estimate = -127.2886
expansions: []
discards: []
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.6085 - loglik: -1.2745e+02 - logprior: -3.1630e+00
Epoch 2/10
19/19 - 2s - loss: 127.8648 - loglik: -1.2658e+02 - logprior: -1.2861e+00
Epoch 3/10
19/19 - 2s - loss: 127.1083 - loglik: -1.2596e+02 - logprior: -1.1516e+00
Epoch 4/10
19/19 - 2s - loss: 126.5716 - loglik: -1.2545e+02 - logprior: -1.1174e+00
Epoch 5/10
19/19 - 1s - loss: 125.9426 - loglik: -1.2482e+02 - logprior: -1.1203e+00
Epoch 6/10
19/19 - 2s - loss: 125.3809 - loglik: -1.2425e+02 - logprior: -1.1301e+00
Epoch 7/10
19/19 - 2s - loss: 125.0742 - loglik: -1.2394e+02 - logprior: -1.1343e+00
Epoch 8/10
19/19 - 2s - loss: 124.8722 - loglik: -1.2371e+02 - logprior: -1.1602e+00
Epoch 9/10
19/19 - 2s - loss: 124.7150 - loglik: -1.2353e+02 - logprior: -1.1835e+00
Epoch 10/10
19/19 - 1s - loss: 124.7655 - loglik: -1.2356e+02 - logprior: -1.2092e+00
Fitted a model with MAP estimate = -124.5273
Time for alignment: 60.3725
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.6737 - loglik: -1.8323e+02 - logprior: -3.4430e+00
Epoch 2/10
19/19 - 2s - loss: 154.7587 - loglik: -1.5309e+02 - logprior: -1.6723e+00
Epoch 3/10
19/19 - 2s - loss: 142.0143 - loglik: -1.4040e+02 - logprior: -1.6133e+00
Epoch 4/10
19/19 - 2s - loss: 138.3247 - loglik: -1.3668e+02 - logprior: -1.6455e+00
Epoch 5/10
19/19 - 2s - loss: 137.0504 - loglik: -1.3542e+02 - logprior: -1.6271e+00
Epoch 6/10
19/19 - 2s - loss: 136.3359 - loglik: -1.3469e+02 - logprior: -1.6482e+00
Epoch 7/10
19/19 - 2s - loss: 135.9348 - loglik: -1.3427e+02 - logprior: -1.6656e+00
Epoch 8/10
19/19 - 2s - loss: 136.1502 - loglik: -1.3446e+02 - logprior: -1.6912e+00
Fitted a model with MAP estimate = -135.7258
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (23, 1), (24, 1), (26, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 136.3341 - loglik: -1.3292e+02 - logprior: -3.4094e+00
Epoch 2/2
19/19 - 2s - loss: 128.5532 - loglik: -1.2717e+02 - logprior: -1.3798e+00
Fitted a model with MAP estimate = -127.5757
expansions: []
discards: [25]
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.8628 - loglik: -1.2768e+02 - logprior: -3.1856e+00
Epoch 2/2
19/19 - 2s - loss: 127.9792 - loglik: -1.2667e+02 - logprior: -1.3119e+00
Fitted a model with MAP estimate = -127.3413
expansions: []
discards: []
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.6002 - loglik: -1.2744e+02 - logprior: -3.1611e+00
Epoch 2/10
19/19 - 2s - loss: 127.8669 - loglik: -1.2658e+02 - logprior: -1.2838e+00
Epoch 3/10
19/19 - 2s - loss: 127.2181 - loglik: -1.2607e+02 - logprior: -1.1490e+00
Epoch 4/10
19/19 - 2s - loss: 126.4688 - loglik: -1.2535e+02 - logprior: -1.1183e+00
Epoch 5/10
19/19 - 2s - loss: 125.9885 - loglik: -1.2488e+02 - logprior: -1.1124e+00
Epoch 6/10
19/19 - 2s - loss: 125.1492 - loglik: -1.2402e+02 - logprior: -1.1298e+00
Epoch 7/10
19/19 - 2s - loss: 125.2243 - loglik: -1.2408e+02 - logprior: -1.1433e+00
Fitted a model with MAP estimate = -124.8771
Time for alignment: 54.7809
Computed alignments with likelihoods: ['-124.7817', '-124.5273', '-124.8771']
Best model has likelihood: -124.5273
SP score = 0.9249
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5237e2d160>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52398459d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523866a280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50e4055310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 789.4217 - loglik: -7.8597e+02 - logprior: -3.4553e+00
Epoch 2/10
19/19 - 18s - loss: 673.8780 - loglik: -6.7285e+02 - logprior: -1.0305e+00
Epoch 3/10
19/19 - 18s - loss: 619.7875 - loglik: -6.1820e+02 - logprior: -1.5834e+00
Epoch 4/10
19/19 - 18s - loss: 606.3528 - loglik: -6.0458e+02 - logprior: -1.7748e+00
Epoch 5/10
19/19 - 18s - loss: 604.1115 - loglik: -6.0220e+02 - logprior: -1.9152e+00
Epoch 6/10
19/19 - 18s - loss: 597.9009 - loglik: -5.9589e+02 - logprior: -2.0096e+00
Epoch 7/10
19/19 - 18s - loss: 596.8228 - loglik: -5.9469e+02 - logprior: -2.1378e+00
Epoch 8/10
19/19 - 18s - loss: 598.0404 - loglik: -5.9576e+02 - logprior: -2.2826e+00
Fitted a model with MAP estimate = -595.4644
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (30, 1), (34, 1), (46, 1), (47, 1), (52, 1), (53, 1), (69, 1), (70, 2), (71, 1), (77, 1), (87, 1), (113, 1), (115, 1), (116, 1), (117, 1), (120, 4), (122, 1), (133, 1), (135, 1), (137, 2), (138, 1), (150, 1), (154, 3), (163, 1), (165, 2), (166, 1), (174, 2), (175, 1), (184, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 591.2575 - loglik: -5.8847e+02 - logprior: -2.7911e+00
Epoch 2/2
39/39 - 27s - loss: 576.4679 - loglik: -5.7499e+02 - logprior: -1.4777e+00
Fitted a model with MAP estimate = -572.1433
expansions: [(0, 3), (145, 2), (241, 1)]
discards: [  0 190 264]
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 576.0292 - loglik: -5.7450e+02 - logprior: -1.5323e+00
Epoch 2/2
39/39 - 27s - loss: 571.4559 - loglik: -5.7097e+02 - logprior: -4.8926e-01
Fitted a model with MAP estimate = -568.3343
expansions: []
discards: [  0   1 147]
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 577.4139 - loglik: -5.7530e+02 - logprior: -2.1102e+00
Epoch 2/10
39/39 - 27s - loss: 571.7513 - loglik: -5.7140e+02 - logprior: -3.5434e-01
Epoch 3/10
39/39 - 27s - loss: 568.3363 - loglik: -5.6830e+02 - logprior: -4.0050e-02
Epoch 4/10
39/39 - 27s - loss: 565.5056 - loglik: -5.6548e+02 - logprior: -2.8421e-02
Epoch 5/10
39/39 - 27s - loss: 563.2060 - loglik: -5.6308e+02 - logprior: -1.3063e-01
Epoch 6/10
39/39 - 27s - loss: 562.0967 - loglik: -5.6180e+02 - logprior: -2.9520e-01
Epoch 7/10
39/39 - 27s - loss: 561.1693 - loglik: -5.6078e+02 - logprior: -3.9042e-01
Epoch 8/10
39/39 - 27s - loss: 560.6768 - loglik: -5.6018e+02 - logprior: -4.9902e-01
Epoch 9/10
39/39 - 27s - loss: 560.1409 - loglik: -5.5964e+02 - logprior: -4.9860e-01
Epoch 10/10
39/39 - 27s - loss: 559.8252 - loglik: -5.5927e+02 - logprior: -5.5646e-01
Fitted a model with MAP estimate = -559.2577
Time for alignment: 664.9835
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 789.7700 - loglik: -7.8632e+02 - logprior: -3.4541e+00
Epoch 2/10
19/19 - 18s - loss: 673.6406 - loglik: -6.7260e+02 - logprior: -1.0374e+00
Epoch 3/10
19/19 - 18s - loss: 622.8605 - loglik: -6.2126e+02 - logprior: -1.5985e+00
Epoch 4/10
19/19 - 18s - loss: 607.3494 - loglik: -6.0558e+02 - logprior: -1.7732e+00
Epoch 5/10
19/19 - 18s - loss: 602.4277 - loglik: -6.0049e+02 - logprior: -1.9396e+00
Epoch 6/10
19/19 - 18s - loss: 598.9387 - loglik: -5.9693e+02 - logprior: -2.0055e+00
Epoch 7/10
19/19 - 18s - loss: 599.7589 - loglik: -5.9763e+02 - logprior: -2.1338e+00
Fitted a model with MAP estimate = -597.3464
expansions: [(8, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (30, 1), (34, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 1), (72, 1), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (122, 1), (132, 1), (136, 1), (138, 1), (146, 2), (147, 1), (154, 6), (163, 1), (164, 3), (168, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [  0 184]
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 592.6171 - loglik: -5.8982e+02 - logprior: -2.7983e+00
Epoch 2/2
39/39 - 27s - loss: 577.2371 - loglik: -5.7577e+02 - logprior: -1.4671e+00
Fitted a model with MAP estimate = -572.0961
expansions: [(148, 1)]
discards: [180 191 194]
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 577.7401 - loglik: -5.7612e+02 - logprior: -1.6192e+00
Epoch 2/2
39/39 - 26s - loss: 573.5117 - loglik: -5.7305e+02 - logprior: -4.6336e-01
Fitted a model with MAP estimate = -570.6801
expansions: []
discards: []
Fitting a model of length 301 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 576.2041 - loglik: -5.7480e+02 - logprior: -1.4044e+00
Epoch 2/10
39/39 - 26s - loss: 572.3693 - loglik: -5.7211e+02 - logprior: -2.5656e-01
Epoch 3/10
39/39 - 26s - loss: 570.1037 - loglik: -5.6995e+02 - logprior: -1.5209e-01
Epoch 4/10
39/39 - 26s - loss: 567.3765 - loglik: -5.6722e+02 - logprior: -1.5741e-01
Epoch 5/10
39/39 - 26s - loss: 564.9749 - loglik: -5.6472e+02 - logprior: -2.5023e-01
Epoch 6/10
39/39 - 26s - loss: 563.2150 - loglik: -5.6285e+02 - logprior: -3.6353e-01
Epoch 7/10
39/39 - 26s - loss: 562.9670 - loglik: -5.6247e+02 - logprior: -4.9220e-01
Epoch 8/10
39/39 - 26s - loss: 561.4841 - loglik: -5.6092e+02 - logprior: -5.5934e-01
Epoch 9/10
39/39 - 26s - loss: 561.3251 - loglik: -5.6073e+02 - logprior: -5.9382e-01
Epoch 10/10
39/39 - 26s - loss: 561.5399 - loglik: -5.6094e+02 - logprior: -5.9635e-01
Fitted a model with MAP estimate = -560.5414
Time for alignment: 640.6655
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 789.9531 - loglik: -7.8649e+02 - logprior: -3.4585e+00
Epoch 2/10
19/19 - 18s - loss: 672.3192 - loglik: -6.7129e+02 - logprior: -1.0244e+00
Epoch 3/10
19/19 - 18s - loss: 620.8120 - loglik: -6.1929e+02 - logprior: -1.5219e+00
Epoch 4/10
19/19 - 18s - loss: 609.4296 - loglik: -6.0776e+02 - logprior: -1.6654e+00
Epoch 5/10
19/19 - 18s - loss: 604.9141 - loglik: -6.0308e+02 - logprior: -1.8332e+00
Epoch 6/10
19/19 - 18s - loss: 601.3989 - loglik: -5.9946e+02 - logprior: -1.9347e+00
Epoch 7/10
19/19 - 18s - loss: 600.4874 - loglik: -5.9840e+02 - logprior: -2.0903e+00
Epoch 8/10
19/19 - 18s - loss: 599.3799 - loglik: -5.9715e+02 - logprior: -2.2280e+00
Epoch 9/10
19/19 - 18s - loss: 598.7952 - loglik: -5.9639e+02 - logprior: -2.4011e+00
Epoch 10/10
19/19 - 18s - loss: 596.6936 - loglik: -5.9417e+02 - logprior: -2.5261e+00
Fitted a model with MAP estimate = -596.9401
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (30, 1), (34, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (113, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (122, 1), (132, 1), (135, 2), (136, 3), (137, 1), (147, 1), (154, 2), (163, 1), (164, 2), (173, 2), (175, 1), (180, 1), (184, 1), (185, 2), (192, 3), (193, 1), (196, 1), (210, 1), (211, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 592.4048 - loglik: -5.8949e+02 - logprior: -2.9188e+00
Epoch 2/2
39/39 - 27s - loss: 575.4792 - loglik: -5.7399e+02 - logprior: -1.4919e+00
Fitted a model with MAP estimate = -570.2916
expansions: [(52, 1), (149, 1)]
discards: [ 86 168 169 170 232]
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 575.9443 - loglik: -5.7424e+02 - logprior: -1.7061e+00
Epoch 2/2
39/39 - 27s - loss: 571.1729 - loglik: -5.7073e+02 - logprior: -4.4762e-01
Fitted a model with MAP estimate = -568.3059
expansions: [(195, 5)]
discards: [204]
Fitting a model of length 309 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 573.2562 - loglik: -5.7185e+02 - logprior: -1.4089e+00
Epoch 2/10
39/39 - 27s - loss: 568.6545 - loglik: -5.6840e+02 - logprior: -2.5537e-01
Epoch 3/10
39/39 - 27s - loss: 565.9773 - loglik: -5.6582e+02 - logprior: -1.5247e-01
Epoch 4/10
39/39 - 27s - loss: 563.2081 - loglik: -5.6309e+02 - logprior: -1.1790e-01
Epoch 5/10
39/39 - 27s - loss: 560.2338 - loglik: -5.6001e+02 - logprior: -2.2120e-01
Epoch 6/10
39/39 - 27s - loss: 559.3087 - loglik: -5.5892e+02 - logprior: -3.8918e-01
Epoch 7/10
39/39 - 27s - loss: 557.9352 - loglik: -5.5743e+02 - logprior: -5.0649e-01
Epoch 8/10
39/39 - 27s - loss: 557.3486 - loglik: -5.5676e+02 - logprior: -5.8755e-01
Epoch 9/10
39/39 - 27s - loss: 557.2397 - loglik: -5.5661e+02 - logprior: -6.2691e-01
Epoch 10/10
39/39 - 27s - loss: 556.7966 - loglik: -5.5614e+02 - logprior: -6.5272e-01
Fitted a model with MAP estimate = -556.0641
Time for alignment: 706.4145
Computed alignments with likelihoods: ['-559.2577', '-560.5414', '-556.0641']
Best model has likelihood: -556.0641
SP score = 0.4701
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f514a6246d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5149f13c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52953c7190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50f5170f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 969.2155 - loglik: -9.6712e+02 - logprior: -2.0913e+00
Epoch 2/10
39/39 - 38s - loss: 809.2532 - loglik: -8.0801e+02 - logprior: -1.2451e+00
Epoch 3/10
39/39 - 38s - loss: 797.2212 - loglik: -7.9580e+02 - logprior: -1.4218e+00
Epoch 4/10
39/39 - 38s - loss: 794.3745 - loglik: -7.9280e+02 - logprior: -1.5787e+00
Epoch 5/10
39/39 - 38s - loss: 792.7164 - loglik: -7.9082e+02 - logprior: -1.8962e+00
Epoch 6/10
39/39 - 38s - loss: 792.1436 - loglik: -7.8999e+02 - logprior: -2.1490e+00
Epoch 7/10
39/39 - 38s - loss: 791.4648 - loglik: -7.8910e+02 - logprior: -2.3688e+00
Epoch 8/10
39/39 - 38s - loss: 791.6693 - loglik: -7.8910e+02 - logprior: -2.5734e+00
Fitted a model with MAP estimate = -782.8837
expansions: [(0, 3), (5, 1), (44, 1), (52, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (73, 2), (99, 1), (120, 2), (121, 2), (122, 2), (146, 3), (148, 1), (163, 1), (170, 1), (171, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (261, 1), (263, 1), (264, 1), (266, 2), (285, 6), (286, 3), (287, 1), (288, 1), (296, 4), (309, 1), (313, 2), (314, 3)]
discards: []
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 778.6954 - loglik: -7.7611e+02 - logprior: -2.5858e+00
Epoch 2/2
39/39 - 49s - loss: 760.4401 - loglik: -7.5973e+02 - logprior: -7.0606e-01
Fitted a model with MAP estimate = -750.1671
expansions: [(346, 2)]
discards: [  1   4 135 139 263 318 388]
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 763.1901 - loglik: -7.6191e+02 - logprior: -1.2752e+00
Epoch 2/2
39/39 - 49s - loss: 758.7620 - loglik: -7.5861e+02 - logprior: -1.5571e-01
Fitted a model with MAP estimate = -749.1620
expansions: [(0, 2), (258, 1)]
discards: [336]
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 755.4694 - loglik: -7.5349e+02 - logprior: -1.9763e+00
Epoch 2/10
39/39 - 49s - loss: 750.3141 - loglik: -7.5036e+02 - logprior: 0.0507
Epoch 3/10
39/39 - 49s - loss: 748.7108 - loglik: -7.4884e+02 - logprior: 0.1326
Epoch 4/10
39/39 - 49s - loss: 747.4763 - loglik: -7.4757e+02 - logprior: 0.0978
Epoch 5/10
39/39 - 49s - loss: 744.8915 - loglik: -7.4485e+02 - logprior: -4.3870e-02
Epoch 6/10
39/39 - 49s - loss: 745.0099 - loglik: -7.4485e+02 - logprior: -1.6487e-01
Fitted a model with MAP estimate = -744.3576
Time for alignment: 1054.7274
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 969.9205 - loglik: -9.6781e+02 - logprior: -2.1066e+00
Epoch 2/10
39/39 - 38s - loss: 811.6503 - loglik: -8.1039e+02 - logprior: -1.2566e+00
Epoch 3/10
39/39 - 38s - loss: 799.7047 - loglik: -7.9819e+02 - logprior: -1.5190e+00
Epoch 4/10
39/39 - 38s - loss: 796.7666 - loglik: -7.9505e+02 - logprior: -1.7151e+00
Epoch 5/10
39/39 - 38s - loss: 795.0634 - loglik: -7.9306e+02 - logprior: -2.0035e+00
Epoch 6/10
39/39 - 38s - loss: 794.2742 - loglik: -7.9203e+02 - logprior: -2.2464e+00
Epoch 7/10
39/39 - 38s - loss: 793.9031 - loglik: -7.9142e+02 - logprior: -2.4807e+00
Epoch 8/10
39/39 - 38s - loss: 793.7990 - loglik: -7.9098e+02 - logprior: -2.8187e+00
Epoch 9/10
39/39 - 38s - loss: 793.7281 - loglik: -7.9081e+02 - logprior: -2.9167e+00
Epoch 10/10
39/39 - 38s - loss: 793.6606 - loglik: -7.9055e+02 - logprior: -3.1091e+00
Fitted a model with MAP estimate = -784.7576
expansions: [(0, 2), (20, 1), (34, 1), (51, 2), (52, 1), (58, 1), (63, 1), (69, 1), (70, 1), (74, 2), (120, 1), (121, 2), (122, 1), (123, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (262, 1), (266, 2), (285, 4), (286, 3), (287, 3), (288, 1), (289, 1), (297, 4), (299, 1), (309, 1), (313, 2), (314, 3)]
discards: [281]
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 779.5448 - loglik: -7.7671e+02 - logprior: -2.8308e+00
Epoch 2/2
39/39 - 50s - loss: 760.6872 - loglik: -7.5987e+02 - logprior: -8.2113e-01
Fitted a model with MAP estimate = -750.3737
expansions: [(8, 1), (334, 1)]
discards: [  1   3   4  85 136 168 262 317 336 337 386]
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 764.4677 - loglik: -7.6333e+02 - logprior: -1.1406e+00
Epoch 2/2
39/39 - 48s - loss: 760.2629 - loglik: -7.6021e+02 - logprior: -5.7445e-02
Fitted a model with MAP estimate = -750.5628
expansions: [(0, 3), (256, 1), (330, 2)]
discards: []
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 756.0624 - loglik: -7.5408e+02 - logprior: -1.9782e+00
Epoch 2/10
39/39 - 49s - loss: 751.2886 - loglik: -7.5139e+02 - logprior: 0.1026
Epoch 3/10
39/39 - 49s - loss: 748.9922 - loglik: -7.4919e+02 - logprior: 0.2021
Epoch 4/10
39/39 - 49s - loss: 747.0780 - loglik: -7.4724e+02 - logprior: 0.1667
Epoch 5/10
39/39 - 49s - loss: 746.2784 - loglik: -7.4631e+02 - logprior: 0.0323
Epoch 6/10
39/39 - 49s - loss: 746.0424 - loglik: -7.4595e+02 - logprior: -8.9246e-02
Epoch 7/10
39/39 - 49s - loss: 744.3776 - loglik: -7.4421e+02 - logprior: -1.6288e-01
Epoch 8/10
39/39 - 49s - loss: 744.7094 - loglik: -7.4445e+02 - logprior: -2.5939e-01
Fitted a model with MAP estimate = -743.9413
Time for alignment: 1226.0304
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 973.8828 - loglik: -9.7180e+02 - logprior: -2.0835e+00
Epoch 2/10
39/39 - 38s - loss: 809.3698 - loglik: -8.0812e+02 - logprior: -1.2525e+00
Epoch 3/10
39/39 - 38s - loss: 797.0237 - loglik: -7.9554e+02 - logprior: -1.4833e+00
Epoch 4/10
39/39 - 38s - loss: 793.6469 - loglik: -7.9193e+02 - logprior: -1.7218e+00
Epoch 5/10
39/39 - 38s - loss: 792.3485 - loglik: -7.9037e+02 - logprior: -1.9766e+00
Epoch 6/10
39/39 - 38s - loss: 791.6679 - loglik: -7.8948e+02 - logprior: -2.1854e+00
Epoch 7/10
39/39 - 38s - loss: 791.3339 - loglik: -7.8890e+02 - logprior: -2.4366e+00
Epoch 8/10
39/39 - 38s - loss: 791.1461 - loglik: -7.8849e+02 - logprior: -2.6553e+00
Epoch 9/10
39/39 - 38s - loss: 790.5869 - loglik: -7.8775e+02 - logprior: -2.8406e+00
Epoch 10/10
39/39 - 38s - loss: 790.8865 - loglik: -7.8788e+02 - logprior: -3.0030e+00
Fitted a model with MAP estimate = -782.1607
expansions: [(0, 2), (6, 1), (20, 1), (34, 1), (47, 1), (51, 1), (52, 1), (56, 1), (67, 1), (69, 1), (70, 1), (72, 2), (73, 2), (97, 1), (118, 1), (119, 2), (121, 1), (145, 3), (146, 1), (147, 1), (166, 1), (173, 1), (175, 1), (176, 1), (177, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 1), (264, 1), (266, 2), (285, 2), (286, 2), (287, 8), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 780.7097 - loglik: -7.7780e+02 - logprior: -2.9121e+00
Epoch 2/2
39/39 - 50s - loss: 760.2618 - loglik: -7.5925e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -749.9056
expansions: [(0, 2)]
discards: [  1   2   3   6  85 137 170 223 265 313 321 368 393]
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 764.7997 - loglik: -7.6259e+02 - logprior: -2.2113e+00
Epoch 2/2
39/39 - 48s - loss: 758.9642 - loglik: -7.5882e+02 - logprior: -1.4836e-01
Fitted a model with MAP estimate = -749.4515
expansions: [(0, 2), (5, 1), (258, 1)]
discards: [1 2 3]
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 755.9408 - loglik: -7.5374e+02 - logprior: -2.1970e+00
Epoch 2/10
39/39 - 49s - loss: 750.1310 - loglik: -7.5021e+02 - logprior: 0.0774
Epoch 3/10
39/39 - 49s - loss: 748.1002 - loglik: -7.4834e+02 - logprior: 0.2386
Epoch 4/10
39/39 - 49s - loss: 745.6714 - loglik: -7.4590e+02 - logprior: 0.2303
Epoch 5/10
39/39 - 49s - loss: 745.7548 - loglik: -7.4585e+02 - logprior: 0.0983
Fitted a model with MAP estimate = -744.2422
Time for alignment: 1083.1434
Computed alignments with likelihoods: ['-744.3576', '-743.9413', '-744.2422']
Best model has likelihood: -743.9413
SP score = 0.4871
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f520309b760>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5203700520>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236c54070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236b2e7f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7087 - loglik: -1.3033e+02 - logprior: -3.3830e+00
Epoch 2/10
19/19 - 1s - loss: 108.9255 - loglik: -1.0751e+02 - logprior: -1.4137e+00
Epoch 3/10
19/19 - 1s - loss: 101.0163 - loglik: -9.9460e+01 - logprior: -1.5567e+00
Epoch 4/10
19/19 - 1s - loss: 99.0133 - loglik: -9.7573e+01 - logprior: -1.4405e+00
Epoch 5/10
19/19 - 1s - loss: 98.5072 - loglik: -9.7075e+01 - logprior: -1.4317e+00
Epoch 6/10
19/19 - 1s - loss: 98.1858 - loglik: -9.6743e+01 - logprior: -1.4433e+00
Epoch 7/10
19/19 - 1s - loss: 98.2212 - loglik: -9.6771e+01 - logprior: -1.4497e+00
Fitted a model with MAP estimate = -98.0167
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4147 - loglik: -9.8151e+01 - logprior: -4.2641e+00
Epoch 2/2
19/19 - 1s - loss: 95.3637 - loglik: -9.3268e+01 - logprior: -2.0959e+00
Fitted a model with MAP estimate = -93.9975
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 95.5972 - loglik: -9.2513e+01 - logprior: -3.0837e+00
Epoch 2/2
19/19 - 1s - loss: 92.7581 - loglik: -9.1489e+01 - logprior: -1.2687e+00
Fitted a model with MAP estimate = -92.3608
expansions: []
discards: [0]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8494 - loglik: -9.3286e+01 - logprior: -3.5636e+00
Epoch 2/10
19/19 - 1s - loss: 93.5581 - loglik: -9.2108e+01 - logprior: -1.4506e+00
Epoch 3/10
19/19 - 1s - loss: 92.9697 - loglik: -9.1605e+01 - logprior: -1.3645e+00
Epoch 4/10
19/19 - 1s - loss: 92.8019 - loglik: -9.1477e+01 - logprior: -1.3249e+00
Epoch 5/10
19/19 - 1s - loss: 92.5733 - loglik: -9.1267e+01 - logprior: -1.3067e+00
Epoch 6/10
19/19 - 1s - loss: 92.4613 - loglik: -9.1160e+01 - logprior: -1.3014e+00
Epoch 7/10
19/19 - 1s - loss: 92.4522 - loglik: -9.1152e+01 - logprior: -1.2997e+00
Epoch 8/10
19/19 - 1s - loss: 92.3656 - loglik: -9.1065e+01 - logprior: -1.3008e+00
Epoch 9/10
19/19 - 1s - loss: 92.2442 - loglik: -9.0923e+01 - logprior: -1.3215e+00
Epoch 10/10
19/19 - 1s - loss: 92.2913 - loglik: -9.0972e+01 - logprior: -1.3198e+00
Fitted a model with MAP estimate = -92.2304
Time for alignment: 47.6765
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.7438 - loglik: -1.3036e+02 - logprior: -3.3861e+00
Epoch 2/10
19/19 - 1s - loss: 109.0410 - loglik: -1.0762e+02 - logprior: -1.4228e+00
Epoch 3/10
19/19 - 1s - loss: 101.7438 - loglik: -1.0020e+02 - logprior: -1.5449e+00
Epoch 4/10
19/19 - 1s - loss: 99.5440 - loglik: -9.8112e+01 - logprior: -1.4321e+00
Epoch 5/10
19/19 - 1s - loss: 98.6166 - loglik: -9.7205e+01 - logprior: -1.4116e+00
Epoch 6/10
19/19 - 1s - loss: 98.2957 - loglik: -9.6882e+01 - logprior: -1.4133e+00
Epoch 7/10
19/19 - 1s - loss: 98.1541 - loglik: -9.6724e+01 - logprior: -1.4298e+00
Epoch 8/10
19/19 - 1s - loss: 98.2170 - loglik: -9.6789e+01 - logprior: -1.4285e+00
Fitted a model with MAP estimate = -98.0203
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.5385 - loglik: -9.8252e+01 - logprior: -4.2866e+00
Epoch 2/2
19/19 - 1s - loss: 95.3782 - loglik: -9.3229e+01 - logprior: -2.1488e+00
Fitted a model with MAP estimate = -94.0479
expansions: [(0, 2)]
discards: [ 0  9 12 29 32 40 42]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.6574 - loglik: -9.2575e+01 - logprior: -3.0828e+00
Epoch 2/2
19/19 - 1s - loss: 92.7295 - loglik: -9.1460e+01 - logprior: -1.2693e+00
Fitted a model with MAP estimate = -92.3741
expansions: []
discards: [0]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.9014 - loglik: -9.3333e+01 - logprior: -3.5687e+00
Epoch 2/10
19/19 - 1s - loss: 93.5099 - loglik: -9.2060e+01 - logprior: -1.4498e+00
Epoch 3/10
19/19 - 1s - loss: 93.0024 - loglik: -9.1640e+01 - logprior: -1.3624e+00
Epoch 4/10
19/19 - 1s - loss: 92.7028 - loglik: -9.1379e+01 - logprior: -1.3241e+00
Epoch 5/10
19/19 - 1s - loss: 92.6078 - loglik: -9.1302e+01 - logprior: -1.3061e+00
Epoch 6/10
19/19 - 1s - loss: 92.6010 - loglik: -9.1299e+01 - logprior: -1.3020e+00
Epoch 7/10
19/19 - 1s - loss: 92.2444 - loglik: -9.0944e+01 - logprior: -1.3009e+00
Epoch 8/10
19/19 - 1s - loss: 92.3286 - loglik: -9.1028e+01 - logprior: -1.3001e+00
Fitted a model with MAP estimate = -92.2800
Time for alignment: 46.6503
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.7738 - loglik: -1.3039e+02 - logprior: -3.3846e+00
Epoch 2/10
19/19 - 1s - loss: 108.7748 - loglik: -1.0735e+02 - logprior: -1.4210e+00
Epoch 3/10
19/19 - 1s - loss: 100.9068 - loglik: -9.9356e+01 - logprior: -1.5505e+00
Epoch 4/10
19/19 - 1s - loss: 98.9242 - loglik: -9.7480e+01 - logprior: -1.4447e+00
Epoch 5/10
19/19 - 1s - loss: 98.4725 - loglik: -9.7045e+01 - logprior: -1.4273e+00
Epoch 6/10
19/19 - 1s - loss: 98.3145 - loglik: -9.6872e+01 - logprior: -1.4428e+00
Epoch 7/10
19/19 - 1s - loss: 98.1566 - loglik: -9.6708e+01 - logprior: -1.4488e+00
Epoch 8/10
19/19 - 1s - loss: 98.1296 - loglik: -9.6682e+01 - logprior: -1.4472e+00
Epoch 9/10
19/19 - 1s - loss: 98.0583 - loglik: -9.6609e+01 - logprior: -1.4496e+00
Epoch 10/10
19/19 - 1s - loss: 97.9833 - loglik: -9.6528e+01 - logprior: -1.4553e+00
Fitted a model with MAP estimate = -97.9228
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.3574 - loglik: -9.8081e+01 - logprior: -4.2763e+00
Epoch 2/2
19/19 - 1s - loss: 95.4478 - loglik: -9.3303e+01 - logprior: -2.1445e+00
Fitted a model with MAP estimate = -94.0894
expansions: [(0, 2)]
discards: [ 0  9 12 29 41]
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.4911 - loglik: -9.2395e+01 - logprior: -3.0965e+00
Epoch 2/2
19/19 - 1s - loss: 92.6604 - loglik: -9.1381e+01 - logprior: -1.2790e+00
Fitted a model with MAP estimate = -92.2654
expansions: []
discards: [ 0 37]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.9811 - loglik: -9.3426e+01 - logprior: -3.5553e+00
Epoch 2/10
19/19 - 1s - loss: 93.3979 - loglik: -9.1949e+01 - logprior: -1.4494e+00
Epoch 3/10
19/19 - 1s - loss: 93.0022 - loglik: -9.1641e+01 - logprior: -1.3609e+00
Epoch 4/10
19/19 - 1s - loss: 92.8386 - loglik: -9.1515e+01 - logprior: -1.3236e+00
Epoch 5/10
19/19 - 1s - loss: 92.5771 - loglik: -9.1272e+01 - logprior: -1.3046e+00
Epoch 6/10
19/19 - 1s - loss: 92.4127 - loglik: -9.1115e+01 - logprior: -1.2976e+00
Epoch 7/10
19/19 - 1s - loss: 92.4154 - loglik: -9.1116e+01 - logprior: -1.2994e+00
Fitted a model with MAP estimate = -92.3298
Time for alignment: 48.3629
Computed alignments with likelihoods: ['-92.2304', '-92.2800', '-92.2654']
Best model has likelihood: -92.2304
SP score = 0.8371
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f514a935940>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50e3f283a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514a27d730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514a27d130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 525.7358 - loglik: -5.2232e+02 - logprior: -3.4206e+00
Epoch 2/10
19/19 - 7s - loss: 424.0779 - loglik: -4.2303e+02 - logprior: -1.0465e+00
Epoch 3/10
19/19 - 7s - loss: 378.9839 - loglik: -3.7727e+02 - logprior: -1.7103e+00
Epoch 4/10
19/19 - 7s - loss: 366.1675 - loglik: -3.6417e+02 - logprior: -2.0009e+00
Epoch 5/10
19/19 - 7s - loss: 364.3824 - loglik: -3.6242e+02 - logprior: -1.9593e+00
Epoch 6/10
19/19 - 7s - loss: 361.2338 - loglik: -3.5931e+02 - logprior: -1.9244e+00
Epoch 7/10
19/19 - 7s - loss: 361.9454 - loglik: -3.6003e+02 - logprior: -1.9176e+00
Fitted a model with MAP estimate = -360.0562
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 1), (80, 1), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 353.2242 - loglik: -3.5032e+02 - logprior: -2.9007e+00
Epoch 2/2
19/19 - 10s - loss: 331.5995 - loglik: -3.3070e+02 - logprior: -9.0253e-01
Fitted a model with MAP estimate = -329.0211
expansions: []
discards: [118 169]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 334.5655 - loglik: -3.3194e+02 - logprior: -2.6229e+00
Epoch 2/2
19/19 - 10s - loss: 329.0631 - loglik: -3.2844e+02 - logprior: -6.2193e-01
Fitted a model with MAP estimate = -328.2483
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 333.8354 - loglik: -3.3127e+02 - logprior: -2.5680e+00
Epoch 2/10
19/19 - 10s - loss: 329.4088 - loglik: -3.2888e+02 - logprior: -5.3136e-01
Epoch 3/10
19/19 - 10s - loss: 326.9631 - loglik: -3.2644e+02 - logprior: -5.2385e-01
Epoch 4/10
19/19 - 10s - loss: 327.0897 - loglik: -3.2652e+02 - logprior: -5.6718e-01
Fitted a model with MAP estimate = -325.6011
Time for alignment: 197.2774
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 525.8377 - loglik: -5.2242e+02 - logprior: -3.4185e+00
Epoch 2/10
19/19 - 7s - loss: 421.6391 - loglik: -4.2061e+02 - logprior: -1.0280e+00
Epoch 3/10
19/19 - 7s - loss: 378.2841 - loglik: -3.7660e+02 - logprior: -1.6845e+00
Epoch 4/10
19/19 - 7s - loss: 367.6631 - loglik: -3.6573e+02 - logprior: -1.9321e+00
Epoch 5/10
19/19 - 7s - loss: 363.2825 - loglik: -3.6141e+02 - logprior: -1.8724e+00
Epoch 6/10
19/19 - 7s - loss: 363.6899 - loglik: -3.6184e+02 - logprior: -1.8491e+00
Fitted a model with MAP estimate = -361.1377
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (54, 1), (58, 1), (60, 1), (63, 1), (66, 1), (72, 1), (74, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 353.9504 - loglik: -3.5104e+02 - logprior: -2.9144e+00
Epoch 2/2
19/19 - 10s - loss: 331.9687 - loglik: -3.3098e+02 - logprior: -9.9150e-01
Fitted a model with MAP estimate = -329.4202
expansions: []
discards: [118 169]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 334.8864 - loglik: -3.3224e+02 - logprior: -2.6465e+00
Epoch 2/2
19/19 - 10s - loss: 330.0100 - loglik: -3.2925e+02 - logprior: -7.5667e-01
Fitted a model with MAP estimate = -328.5090
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 333.7332 - loglik: -3.3114e+02 - logprior: -2.5883e+00
Epoch 2/10
19/19 - 10s - loss: 328.9632 - loglik: -3.2838e+02 - logprior: -5.8480e-01
Epoch 3/10
19/19 - 10s - loss: 327.9000 - loglik: -3.2736e+02 - logprior: -5.4004e-01
Epoch 4/10
19/19 - 10s - loss: 328.1225 - loglik: -3.2766e+02 - logprior: -4.5895e-01
Fitted a model with MAP estimate = -326.0047
Time for alignment: 189.1798
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 525.7729 - loglik: -5.2235e+02 - logprior: -3.4197e+00
Epoch 2/10
19/19 - 7s - loss: 420.6588 - loglik: -4.1961e+02 - logprior: -1.0502e+00
Epoch 3/10
19/19 - 7s - loss: 375.4753 - loglik: -3.7382e+02 - logprior: -1.6595e+00
Epoch 4/10
19/19 - 7s - loss: 366.3853 - loglik: -3.6442e+02 - logprior: -1.9675e+00
Epoch 5/10
19/19 - 7s - loss: 364.5471 - loglik: -3.6264e+02 - logprior: -1.9028e+00
Epoch 6/10
19/19 - 7s - loss: 360.9686 - loglik: -3.5909e+02 - logprior: -1.8805e+00
Epoch 7/10
19/19 - 7s - loss: 360.6560 - loglik: -3.5880e+02 - logprior: -1.8522e+00
Epoch 8/10
19/19 - 7s - loss: 360.3384 - loglik: -3.5846e+02 - logprior: -1.8793e+00
Epoch 9/10
19/19 - 7s - loss: 358.5515 - loglik: -3.5664e+02 - logprior: -1.9130e+00
Epoch 10/10
19/19 - 7s - loss: 357.8753 - loglik: -3.5595e+02 - logprior: -1.9269e+00
Fitted a model with MAP estimate = -358.4185
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 352.7761 - loglik: -3.4977e+02 - logprior: -3.0044e+00
Epoch 2/2
19/19 - 10s - loss: 330.2573 - loglik: -3.2926e+02 - logprior: -1.0017e+00
Fitted a model with MAP estimate = -328.1432
expansions: []
discards: [118 169]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 333.5582 - loglik: -3.3088e+02 - logprior: -2.6774e+00
Epoch 2/2
19/19 - 10s - loss: 329.1799 - loglik: -3.2839e+02 - logprior: -7.8971e-01
Fitted a model with MAP estimate = -327.6384
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 332.8714 - loglik: -3.3029e+02 - logprior: -2.5822e+00
Epoch 2/10
19/19 - 10s - loss: 328.3988 - loglik: -3.2770e+02 - logprior: -6.9864e-01
Epoch 3/10
19/19 - 10s - loss: 327.3247 - loglik: -3.2677e+02 - logprior: -5.5772e-01
Epoch 4/10
19/19 - 10s - loss: 326.0839 - loglik: -3.2558e+02 - logprior: -5.0044e-01
Epoch 5/10
19/19 - 10s - loss: 324.6514 - loglik: -3.2419e+02 - logprior: -4.6386e-01
Epoch 6/10
19/19 - 10s - loss: 324.2342 - loglik: -3.2381e+02 - logprior: -4.2421e-01
Epoch 7/10
19/19 - 10s - loss: 322.0288 - loglik: -3.2164e+02 - logprior: -3.9322e-01
Epoch 8/10
19/19 - 10s - loss: 322.2759 - loglik: -3.2190e+02 - logprior: -3.7440e-01
Fitted a model with MAP estimate = -321.5999
Time for alignment: 258.7761
Computed alignments with likelihoods: ['-325.6011', '-326.0047', '-321.5999']
Best model has likelihood: -321.5999
SP score = 0.8005
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f51282f5ee0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f525eef3f70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50ecb8e700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50ec97ac40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.0941 - loglik: -2.5467e+02 - logprior: -3.4284e+00
Epoch 2/10
19/19 - 2s - loss: 226.2889 - loglik: -2.2495e+02 - logprior: -1.3344e+00
Epoch 3/10
19/19 - 2s - loss: 214.0913 - loglik: -2.1270e+02 - logprior: -1.3950e+00
Epoch 4/10
19/19 - 2s - loss: 210.1204 - loglik: -2.0880e+02 - logprior: -1.3184e+00
Epoch 5/10
19/19 - 2s - loss: 209.1157 - loglik: -2.0779e+02 - logprior: -1.3291e+00
Epoch 6/10
19/19 - 2s - loss: 208.1404 - loglik: -2.0689e+02 - logprior: -1.2515e+00
Epoch 7/10
19/19 - 2s - loss: 207.1896 - loglik: -2.0599e+02 - logprior: -1.2023e+00
Epoch 8/10
19/19 - 2s - loss: 204.7576 - loglik: -2.0350e+02 - logprior: -1.2559e+00
Epoch 9/10
19/19 - 2s - loss: 204.2745 - loglik: -2.0301e+02 - logprior: -1.2601e+00
Epoch 10/10
19/19 - 2s - loss: 204.3901 - loglik: -2.0312e+02 - logprior: -1.2672e+00
Fitted a model with MAP estimate = -203.6030
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 2), (37, 2), (38, 1), (43, 1), (54, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 208.2095 - loglik: -2.0413e+02 - logprior: -4.0772e+00
Epoch 2/2
19/19 - 2s - loss: 198.1949 - loglik: -1.9625e+02 - logprior: -1.9487e+00
Fitted a model with MAP estimate = -196.2191
expansions: [(0, 2)]
discards: [ 0  7 37 46]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 198.0845 - loglik: -1.9524e+02 - logprior: -2.8478e+00
Epoch 2/2
19/19 - 2s - loss: 194.8475 - loglik: -1.9383e+02 - logprior: -1.0208e+00
Fitted a model with MAP estimate = -193.8961
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 199.3983 - loglik: -1.9598e+02 - logprior: -3.4188e+00
Epoch 2/10
19/19 - 2s - loss: 195.3062 - loglik: -1.9413e+02 - logprior: -1.1774e+00
Epoch 3/10
19/19 - 2s - loss: 194.4128 - loglik: -1.9337e+02 - logprior: -1.0388e+00
Epoch 4/10
19/19 - 2s - loss: 193.9531 - loglik: -1.9296e+02 - logprior: -9.9313e-01
Epoch 5/10
19/19 - 2s - loss: 193.1253 - loglik: -1.9216e+02 - logprior: -9.6506e-01
Epoch 6/10
19/19 - 2s - loss: 193.0086 - loglik: -1.9206e+02 - logprior: -9.4446e-01
Epoch 7/10
19/19 - 2s - loss: 192.6678 - loglik: -1.9174e+02 - logprior: -9.2504e-01
Epoch 8/10
19/19 - 2s - loss: 192.6180 - loglik: -1.9169e+02 - logprior: -9.2876e-01
Epoch 9/10
19/19 - 2s - loss: 192.5469 - loglik: -1.9161e+02 - logprior: -9.3202e-01
Epoch 10/10
19/19 - 2s - loss: 192.6025 - loglik: -1.9167e+02 - logprior: -9.3456e-01
Fitted a model with MAP estimate = -192.2658
Time for alignment: 86.1041
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.3676 - loglik: -2.5494e+02 - logprior: -3.4273e+00
Epoch 2/10
19/19 - 2s - loss: 226.0974 - loglik: -2.2476e+02 - logprior: -1.3333e+00
Epoch 3/10
19/19 - 2s - loss: 211.2858 - loglik: -2.0986e+02 - logprior: -1.4289e+00
Epoch 4/10
19/19 - 2s - loss: 207.1834 - loglik: -2.0579e+02 - logprior: -1.3972e+00
Epoch 5/10
19/19 - 2s - loss: 205.6615 - loglik: -2.0421e+02 - logprior: -1.4498e+00
Epoch 6/10
19/19 - 2s - loss: 205.2840 - loglik: -2.0387e+02 - logprior: -1.4142e+00
Epoch 7/10
19/19 - 2s - loss: 205.1516 - loglik: -2.0374e+02 - logprior: -1.4136e+00
Epoch 8/10
19/19 - 2s - loss: 205.0771 - loglik: -2.0367e+02 - logprior: -1.4107e+00
Epoch 9/10
19/19 - 2s - loss: 204.7154 - loglik: -2.0330e+02 - logprior: -1.4158e+00
Epoch 10/10
19/19 - 2s - loss: 204.9417 - loglik: -2.0350e+02 - logprior: -1.4425e+00
Fitted a model with MAP estimate = -204.4231
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (24, 1), (29, 1), (37, 1), (38, 1), (48, 1), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.2158 - loglik: -2.0314e+02 - logprior: -4.0806e+00
Epoch 2/2
19/19 - 2s - loss: 197.0842 - loglik: -1.9513e+02 - logprior: -1.9493e+00
Fitted a model with MAP estimate = -195.0667
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 81]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.2131 - loglik: -1.9437e+02 - logprior: -2.8429e+00
Epoch 2/2
19/19 - 2s - loss: 193.8363 - loglik: -1.9282e+02 - logprior: -1.0118e+00
Fitted a model with MAP estimate = -192.9833
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.4549 - loglik: -1.9503e+02 - logprior: -3.4274e+00
Epoch 2/10
19/19 - 2s - loss: 194.4703 - loglik: -1.9330e+02 - logprior: -1.1733e+00
Epoch 3/10
19/19 - 2s - loss: 193.3196 - loglik: -1.9228e+02 - logprior: -1.0351e+00
Epoch 4/10
19/19 - 2s - loss: 193.0071 - loglik: -1.9202e+02 - logprior: -9.8593e-01
Epoch 5/10
19/19 - 2s - loss: 192.4583 - loglik: -1.9150e+02 - logprior: -9.6049e-01
Epoch 6/10
19/19 - 2s - loss: 191.9104 - loglik: -1.9097e+02 - logprior: -9.4132e-01
Epoch 7/10
19/19 - 2s - loss: 191.9883 - loglik: -1.9105e+02 - logprior: -9.3572e-01
Fitted a model with MAP estimate = -191.6614
Time for alignment: 78.5700
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.3244 - loglik: -2.5490e+02 - logprior: -3.4240e+00
Epoch 2/10
19/19 - 2s - loss: 225.6416 - loglik: -2.2432e+02 - logprior: -1.3233e+00
Epoch 3/10
19/19 - 2s - loss: 212.5657 - loglik: -2.1118e+02 - logprior: -1.3885e+00
Epoch 4/10
19/19 - 2s - loss: 207.9973 - loglik: -2.0666e+02 - logprior: -1.3372e+00
Epoch 5/10
19/19 - 2s - loss: 206.6906 - loglik: -2.0528e+02 - logprior: -1.4136e+00
Epoch 6/10
19/19 - 2s - loss: 206.0582 - loglik: -2.0466e+02 - logprior: -1.3957e+00
Epoch 7/10
19/19 - 2s - loss: 205.5674 - loglik: -2.0417e+02 - logprior: -1.3996e+00
Epoch 8/10
19/19 - 2s - loss: 205.7391 - loglik: -2.0433e+02 - logprior: -1.4121e+00
Fitted a model with MAP estimate = -205.0734
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (18, 1), (30, 2), (37, 2), (38, 2), (54, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 207.5992 - loglik: -2.0352e+02 - logprior: -4.0773e+00
Epoch 2/2
19/19 - 2s - loss: 197.2314 - loglik: -1.9530e+02 - logprior: -1.9294e+00
Fitted a model with MAP estimate = -195.1881
expansions: [(0, 2)]
discards: [ 0  7 37 46 80]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.0584 - loglik: -1.9422e+02 - logprior: -2.8382e+00
Epoch 2/2
19/19 - 2s - loss: 194.1161 - loglik: -1.9310e+02 - logprior: -1.0145e+00
Fitted a model with MAP estimate = -192.9884
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.3904 - loglik: -1.9499e+02 - logprior: -3.4040e+00
Epoch 2/10
19/19 - 2s - loss: 194.3395 - loglik: -1.9317e+02 - logprior: -1.1744e+00
Epoch 3/10
19/19 - 2s - loss: 193.4341 - loglik: -1.9241e+02 - logprior: -1.0271e+00
Epoch 4/10
19/19 - 2s - loss: 192.9363 - loglik: -1.9194e+02 - logprior: -9.9152e-01
Epoch 5/10
19/19 - 2s - loss: 192.4599 - loglik: -1.9150e+02 - logprior: -9.6283e-01
Epoch 6/10
19/19 - 2s - loss: 191.7981 - loglik: -1.9086e+02 - logprior: -9.4160e-01
Epoch 7/10
19/19 - 2s - loss: 192.1355 - loglik: -1.9119e+02 - logprior: -9.4524e-01
Fitted a model with MAP estimate = -191.6472
Time for alignment: 74.7463
Computed alignments with likelihoods: ['-192.2658', '-191.6614', '-191.6472']
Best model has likelihood: -191.6472
SP score = 0.9720
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50f5022d00>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50f52166a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50d3352610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52032a1df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 605.0222 - loglik: -6.0161e+02 - logprior: -3.4101e+00
Epoch 2/10
19/19 - 8s - loss: 532.3179 - loglik: -5.3133e+02 - logprior: -9.8671e-01
Epoch 3/10
19/19 - 8s - loss: 493.6680 - loglik: -4.9226e+02 - logprior: -1.4122e+00
Epoch 4/10
19/19 - 8s - loss: 482.5389 - loglik: -4.8106e+02 - logprior: -1.4798e+00
Epoch 5/10
19/19 - 8s - loss: 479.3194 - loglik: -4.7776e+02 - logprior: -1.5586e+00
Epoch 6/10
19/19 - 8s - loss: 477.1246 - loglik: -4.7549e+02 - logprior: -1.6361e+00
Epoch 7/10
19/19 - 8s - loss: 475.8688 - loglik: -4.7416e+02 - logprior: -1.7133e+00
Epoch 8/10
19/19 - 8s - loss: 475.8383 - loglik: -4.7402e+02 - logprior: -1.8230e+00
Epoch 9/10
19/19 - 8s - loss: 474.7229 - loglik: -4.7281e+02 - logprior: -1.9127e+00
Epoch 10/10
19/19 - 8s - loss: 474.9611 - loglik: -4.7298e+02 - logprior: -1.9813e+00
Fitted a model with MAP estimate = -472.2054
expansions: [(0, 3), (12, 1), (18, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (58, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (119, 1), (125, 1), (126, 1), (132, 1), (141, 5), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Fitting a model of length 222 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 480.0315 - loglik: -4.7566e+02 - logprior: -4.3704e+00
Epoch 2/2
19/19 - 11s - loss: 461.4096 - loglik: -4.6012e+02 - logprior: -1.2859e+00
Fitted a model with MAP estimate = -456.3154
expansions: []
discards: [ 26  45  57 113 179 180 192]
Fitting a model of length 215 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 463.0685 - loglik: -4.6026e+02 - logprior: -2.8056e+00
Epoch 2/2
19/19 - 11s - loss: 459.2181 - loglik: -4.5832e+02 - logprior: -9.0232e-01
Fitted a model with MAP estimate = -455.5065
expansions: [(181, 1)]
discards: []
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 459.6277 - loglik: -4.5692e+02 - logprior: -2.7073e+00
Epoch 2/10
19/19 - 11s - loss: 456.2281 - loglik: -4.5540e+02 - logprior: -8.2995e-01
Epoch 3/10
19/19 - 11s - loss: 454.9226 - loglik: -4.5426e+02 - logprior: -6.5759e-01
Epoch 4/10
19/19 - 11s - loss: 453.1771 - loglik: -4.5253e+02 - logprior: -6.5059e-01
Epoch 5/10
19/19 - 11s - loss: 452.2766 - loglik: -4.5160e+02 - logprior: -6.7637e-01
Epoch 6/10
19/19 - 11s - loss: 451.5446 - loglik: -4.5083e+02 - logprior: -7.1629e-01
Epoch 7/10
19/19 - 11s - loss: 450.0614 - loglik: -4.4925e+02 - logprior: -8.1020e-01
Epoch 8/10
19/19 - 11s - loss: 450.4675 - loglik: -4.4958e+02 - logprior: -8.8805e-01
Fitted a model with MAP estimate = -449.4165
Time for alignment: 284.6103
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 604.9666 - loglik: -6.0155e+02 - logprior: -3.4122e+00
Epoch 2/10
19/19 - 8s - loss: 530.5568 - loglik: -5.2960e+02 - logprior: -9.5400e-01
Epoch 3/10
19/19 - 8s - loss: 492.6339 - loglik: -4.9131e+02 - logprior: -1.3279e+00
Epoch 4/10
19/19 - 8s - loss: 483.8441 - loglik: -4.8247e+02 - logprior: -1.3733e+00
Epoch 5/10
19/19 - 8s - loss: 480.6644 - loglik: -4.7923e+02 - logprior: -1.4394e+00
Epoch 6/10
19/19 - 8s - loss: 479.0708 - loglik: -4.7754e+02 - logprior: -1.5320e+00
Epoch 7/10
19/19 - 8s - loss: 478.6004 - loglik: -4.7697e+02 - logprior: -1.6310e+00
Epoch 8/10
19/19 - 8s - loss: 477.5700 - loglik: -4.7585e+02 - logprior: -1.7233e+00
Epoch 9/10
19/19 - 8s - loss: 477.4085 - loglik: -4.7560e+02 - logprior: -1.8103e+00
Epoch 10/10
19/19 - 8s - loss: 476.9441 - loglik: -4.7505e+02 - logprior: -1.8929e+00
Fitted a model with MAP estimate = -474.3250
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (103, 1), (107, 1), (108, 1), (116, 1), (125, 3), (126, 1), (142, 4), (150, 2), (151, 4), (153, 1), (157, 1), (158, 1)]
discards: []
Fitting a model of length 221 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 482.0014 - loglik: -4.7762e+02 - logprior: -4.3839e+00
Epoch 2/2
19/19 - 11s - loss: 462.2919 - loglik: -4.6097e+02 - logprior: -1.3217e+00
Fitted a model with MAP estimate = -457.2103
expansions: [(187, 1)]
discards: [ 25  44  56  94 113 179 180]
Fitting a model of length 215 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 463.6459 - loglik: -4.6086e+02 - logprior: -2.7876e+00
Epoch 2/2
19/19 - 11s - loss: 459.1198 - loglik: -4.5821e+02 - logprior: -9.0686e-01
Fitted a model with MAP estimate = -455.7341
expansions: []
discards: []
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 460.1387 - loglik: -4.5744e+02 - logprior: -2.6982e+00
Epoch 2/10
19/19 - 11s - loss: 456.5645 - loglik: -4.5575e+02 - logprior: -8.1374e-01
Epoch 3/10
19/19 - 11s - loss: 454.8597 - loglik: -4.5421e+02 - logprior: -6.5467e-01
Epoch 4/10
19/19 - 11s - loss: 454.2911 - loglik: -4.5365e+02 - logprior: -6.4259e-01
Epoch 5/10
19/19 - 11s - loss: 453.1610 - loglik: -4.5248e+02 - logprior: -6.7632e-01
Epoch 6/10
19/19 - 11s - loss: 451.5234 - loglik: -4.5079e+02 - logprior: -7.3648e-01
Epoch 7/10
19/19 - 11s - loss: 451.0086 - loglik: -4.5020e+02 - logprior: -8.1082e-01
Epoch 8/10
19/19 - 11s - loss: 450.8004 - loglik: -4.4991e+02 - logprior: -8.9032e-01
Epoch 9/10
19/19 - 11s - loss: 450.1342 - loglik: -4.4919e+02 - logprior: -9.4741e-01
Epoch 10/10
19/19 - 11s - loss: 449.7154 - loglik: -4.4871e+02 - logprior: -1.0067e+00
Fitted a model with MAP estimate = -449.4446
Time for alignment: 304.8225
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 605.2753 - loglik: -6.0188e+02 - logprior: -3.3952e+00
Epoch 2/10
19/19 - 8s - loss: 531.0209 - loglik: -5.3008e+02 - logprior: -9.4492e-01
Epoch 3/10
19/19 - 8s - loss: 493.5671 - loglik: -4.9221e+02 - logprior: -1.3532e+00
Epoch 4/10
19/19 - 8s - loss: 483.9661 - loglik: -4.8254e+02 - logprior: -1.4284e+00
Epoch 5/10
19/19 - 8s - loss: 479.8603 - loglik: -4.7831e+02 - logprior: -1.5498e+00
Epoch 6/10
19/19 - 8s - loss: 478.5270 - loglik: -4.7688e+02 - logprior: -1.6473e+00
Epoch 7/10
19/19 - 8s - loss: 477.3904 - loglik: -4.7566e+02 - logprior: -1.7351e+00
Epoch 8/10
19/19 - 8s - loss: 476.5108 - loglik: -4.7471e+02 - logprior: -1.7980e+00
Epoch 9/10
19/19 - 8s - loss: 477.1783 - loglik: -4.7529e+02 - logprior: -1.8925e+00
Fitted a model with MAP estimate = -473.7872
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (116, 1), (125, 2), (129, 1), (131, 1), (141, 2), (149, 2), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Fitting a model of length 218 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 480.5059 - loglik: -4.7618e+02 - logprior: -4.3302e+00
Epoch 2/2
19/19 - 11s - loss: 463.6065 - loglik: -4.6230e+02 - logprior: -1.3110e+00
Fitted a model with MAP estimate = -458.9307
expansions: []
discards: [ 25  44  56 112 186 188]
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 465.3706 - loglik: -4.6255e+02 - logprior: -2.8195e+00
Epoch 2/2
19/19 - 10s - loss: 461.5482 - loglik: -4.6064e+02 - logprior: -9.1172e-01
Fitted a model with MAP estimate = -457.9950
expansions: []
discards: [172]
Fitting a model of length 211 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 462.3844 - loglik: -4.5967e+02 - logprior: -2.7150e+00
Epoch 2/10
19/19 - 10s - loss: 458.6870 - loglik: -4.5786e+02 - logprior: -8.2732e-01
Epoch 3/10
19/19 - 10s - loss: 457.6095 - loglik: -4.5692e+02 - logprior: -6.8854e-01
Epoch 4/10
19/19 - 10s - loss: 456.4887 - loglik: -4.5584e+02 - logprior: -6.4688e-01
Epoch 5/10
19/19 - 10s - loss: 454.5650 - loglik: -4.5388e+02 - logprior: -6.8257e-01
Epoch 6/10
19/19 - 10s - loss: 454.0766 - loglik: -4.5332e+02 - logprior: -7.5561e-01
Epoch 7/10
19/19 - 10s - loss: 452.5773 - loglik: -4.5176e+02 - logprior: -8.1251e-01
Epoch 8/10
19/19 - 10s - loss: 452.4145 - loglik: -4.5151e+02 - logprior: -9.0815e-01
Epoch 9/10
19/19 - 10s - loss: 452.4293 - loglik: -4.5146e+02 - logprior: -9.6519e-01
Fitted a model with MAP estimate = -451.6223
Time for alignment: 282.6485
Computed alignments with likelihoods: ['-449.4165', '-449.4446', '-451.6223']
Best model has likelihood: -449.4165
SP score = 0.7883
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50a02f10d0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5236f970d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5236f97af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5237e2a970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.0058 - loglik: -1.6162e+02 - logprior: -3.3857e+00
Epoch 2/10
19/19 - 1s - loss: 136.4789 - loglik: -1.3506e+02 - logprior: -1.4218e+00
Epoch 3/10
19/19 - 1s - loss: 128.6643 - loglik: -1.2715e+02 - logprior: -1.5122e+00
Epoch 4/10
19/19 - 1s - loss: 126.9922 - loglik: -1.2557e+02 - logprior: -1.4198e+00
Epoch 5/10
19/19 - 1s - loss: 126.5736 - loglik: -1.2511e+02 - logprior: -1.4612e+00
Epoch 6/10
19/19 - 1s - loss: 126.2386 - loglik: -1.2478e+02 - logprior: -1.4538e+00
Epoch 7/10
19/19 - 1s - loss: 126.0471 - loglik: -1.2458e+02 - logprior: -1.4640e+00
Epoch 8/10
19/19 - 1s - loss: 125.9925 - loglik: -1.2453e+02 - logprior: -1.4675e+00
Epoch 9/10
19/19 - 1s - loss: 125.8613 - loglik: -1.2439e+02 - logprior: -1.4686e+00
Epoch 10/10
19/19 - 1s - loss: 125.9501 - loglik: -1.2447e+02 - logprior: -1.4778e+00
Fitted a model with MAP estimate = -125.8246
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.9243 - loglik: -1.2969e+02 - logprior: -4.2347e+00
Epoch 2/2
19/19 - 1s - loss: 127.1252 - loglik: -1.2486e+02 - logprior: -2.2676e+00
Fitted a model with MAP estimate = -124.7911
expansions: []
discards: [12 13 41 42 51]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.6313 - loglik: -1.2321e+02 - logprior: -3.4188e+00
Epoch 2/2
19/19 - 1s - loss: 123.2848 - loglik: -1.2181e+02 - logprior: -1.4706e+00
Fitted a model with MAP estimate = -122.8325
expansions: []
discards: []
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5269 - loglik: -1.2228e+02 - logprior: -3.2482e+00
Epoch 2/10
19/19 - 1s - loss: 123.2290 - loglik: -1.2178e+02 - logprior: -1.4496e+00
Epoch 3/10
19/19 - 1s - loss: 122.7545 - loglik: -1.2138e+02 - logprior: -1.3756e+00
Epoch 4/10
19/19 - 1s - loss: 122.3360 - loglik: -1.2100e+02 - logprior: -1.3344e+00
Epoch 5/10
19/19 - 1s - loss: 122.1870 - loglik: -1.2085e+02 - logprior: -1.3331e+00
Epoch 6/10
19/19 - 1s - loss: 121.9412 - loglik: -1.2060e+02 - logprior: -1.3452e+00
Epoch 7/10
19/19 - 1s - loss: 121.8323 - loglik: -1.2049e+02 - logprior: -1.3439e+00
Epoch 8/10
19/19 - 1s - loss: 121.6924 - loglik: -1.2035e+02 - logprior: -1.3407e+00
Epoch 9/10
19/19 - 1s - loss: 121.9283 - loglik: -1.2060e+02 - logprior: -1.3328e+00
Fitted a model with MAP estimate = -121.5954
Time for alignment: 55.8668
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 165.0358 - loglik: -1.6165e+02 - logprior: -3.3878e+00
Epoch 2/10
19/19 - 1s - loss: 136.3629 - loglik: -1.3494e+02 - logprior: -1.4231e+00
Epoch 3/10
19/19 - 1s - loss: 129.2406 - loglik: -1.2772e+02 - logprior: -1.5186e+00
Epoch 4/10
19/19 - 1s - loss: 127.7677 - loglik: -1.2635e+02 - logprior: -1.4210e+00
Epoch 5/10
19/19 - 1s - loss: 126.7623 - loglik: -1.2531e+02 - logprior: -1.4550e+00
Epoch 6/10
19/19 - 1s - loss: 126.3510 - loglik: -1.2488e+02 - logprior: -1.4668e+00
Epoch 7/10
19/19 - 1s - loss: 126.1577 - loglik: -1.2468e+02 - logprior: -1.4756e+00
Epoch 8/10
19/19 - 1s - loss: 126.1531 - loglik: -1.2468e+02 - logprior: -1.4747e+00
Epoch 9/10
19/19 - 1s - loss: 126.0302 - loglik: -1.2455e+02 - logprior: -1.4825e+00
Epoch 10/10
19/19 - 1s - loss: 126.1290 - loglik: -1.2464e+02 - logprior: -1.4896e+00
Fitted a model with MAP estimate = -125.9505
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (32, 1), (34, 1), (36, 2)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7642 - loglik: -1.2954e+02 - logprior: -4.2233e+00
Epoch 2/2
19/19 - 1s - loss: 127.0962 - loglik: -1.2485e+02 - logprior: -2.2453e+00
Fitted a model with MAP estimate = -124.7889
expansions: []
discards: [12 13 41 50]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.5375 - loglik: -1.2313e+02 - logprior: -3.4080e+00
Epoch 2/2
19/19 - 1s - loss: 123.4230 - loglik: -1.2194e+02 - logprior: -1.4801e+00
Fitted a model with MAP estimate = -122.8673
expansions: []
discards: []
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6520 - loglik: -1.2239e+02 - logprior: -3.2580e+00
Epoch 2/10
19/19 - 1s - loss: 123.1309 - loglik: -1.2167e+02 - logprior: -1.4579e+00
Epoch 3/10
19/19 - 1s - loss: 122.7745 - loglik: -1.2139e+02 - logprior: -1.3799e+00
Epoch 4/10
19/19 - 1s - loss: 122.4141 - loglik: -1.2108e+02 - logprior: -1.3345e+00
Epoch 5/10
19/19 - 1s - loss: 122.2492 - loglik: -1.2091e+02 - logprior: -1.3346e+00
Epoch 6/10
19/19 - 1s - loss: 122.1334 - loglik: -1.2079e+02 - logprior: -1.3425e+00
Epoch 7/10
19/19 - 1s - loss: 121.6954 - loglik: -1.2035e+02 - logprior: -1.3454e+00
Epoch 8/10
19/19 - 1s - loss: 121.6185 - loglik: -1.2028e+02 - logprior: -1.3392e+00
Epoch 9/10
19/19 - 1s - loss: 121.8639 - loglik: -1.2053e+02 - logprior: -1.3371e+00
Fitted a model with MAP estimate = -121.5915
Time for alignment: 56.9959
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.1590 - loglik: -1.6177e+02 - logprior: -3.3881e+00
Epoch 2/10
19/19 - 1s - loss: 138.7467 - loglik: -1.3732e+02 - logprior: -1.4239e+00
Epoch 3/10
19/19 - 1s - loss: 130.1114 - loglik: -1.2859e+02 - logprior: -1.5188e+00
Epoch 4/10
19/19 - 1s - loss: 127.5940 - loglik: -1.2619e+02 - logprior: -1.4011e+00
Epoch 5/10
19/19 - 1s - loss: 127.0016 - loglik: -1.2554e+02 - logprior: -1.4633e+00
Epoch 6/10
19/19 - 1s - loss: 126.5698 - loglik: -1.2513e+02 - logprior: -1.4445e+00
Epoch 7/10
19/19 - 1s - loss: 126.6551 - loglik: -1.2520e+02 - logprior: -1.4560e+00
Fitted a model with MAP estimate = -126.4242
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.9648 - loglik: -1.2971e+02 - logprior: -4.2501e+00
Epoch 2/2
19/19 - 1s - loss: 127.0353 - loglik: -1.2480e+02 - logprior: -2.2398e+00
Fitted a model with MAP estimate = -124.4872
expansions: []
discards: [11 13 14 42 48 52]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4981 - loglik: -1.2312e+02 - logprior: -3.3744e+00
Epoch 2/2
19/19 - 1s - loss: 123.2782 - loglik: -1.2180e+02 - logprior: -1.4783e+00
Fitted a model with MAP estimate = -122.8324
expansions: []
discards: []
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.5815 - loglik: -1.2233e+02 - logprior: -3.2544e+00
Epoch 2/10
19/19 - 1s - loss: 123.1808 - loglik: -1.2173e+02 - logprior: -1.4550e+00
Epoch 3/10
19/19 - 1s - loss: 122.6443 - loglik: -1.2127e+02 - logprior: -1.3723e+00
Epoch 4/10
19/19 - 1s - loss: 122.4503 - loglik: -1.2111e+02 - logprior: -1.3427e+00
Epoch 5/10
19/19 - 1s - loss: 122.2587 - loglik: -1.2092e+02 - logprior: -1.3372e+00
Epoch 6/10
19/19 - 1s - loss: 121.9817 - loglik: -1.2064e+02 - logprior: -1.3446e+00
Epoch 7/10
19/19 - 1s - loss: 121.8542 - loglik: -1.2051e+02 - logprior: -1.3444e+00
Epoch 8/10
19/19 - 1s - loss: 121.5856 - loglik: -1.2024e+02 - logprior: -1.3442e+00
Epoch 9/10
19/19 - 1s - loss: 121.8630 - loglik: -1.2052e+02 - logprior: -1.3392e+00
Fitted a model with MAP estimate = -121.5855
Time for alignment: 52.2377
Computed alignments with likelihoods: ['-121.5954', '-121.5915', '-121.5855']
Best model has likelihood: -121.5855
SP score = 0.8855
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f6ddf70>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f525effcbb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50e4466ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f4025b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 902.3721 - loglik: -9.0006e+02 - logprior: -2.3088e+00
Epoch 2/10
39/39 - 45s - loss: 720.0367 - loglik: -7.1856e+02 - logprior: -1.4744e+00
Epoch 3/10
39/39 - 45s - loss: 707.9298 - loglik: -7.0611e+02 - logprior: -1.8223e+00
Epoch 4/10
39/39 - 45s - loss: 705.0251 - loglik: -7.0319e+02 - logprior: -1.8311e+00
Epoch 5/10
39/39 - 46s - loss: 703.5645 - loglik: -7.0171e+02 - logprior: -1.8577e+00
Epoch 6/10
39/39 - 46s - loss: 703.1812 - loglik: -7.0127e+02 - logprior: -1.9134e+00
Epoch 7/10
39/39 - 46s - loss: 702.9008 - loglik: -7.0087e+02 - logprior: -2.0314e+00
Epoch 8/10
39/39 - 46s - loss: 702.2783 - loglik: -7.0024e+02 - logprior: -2.0385e+00
Epoch 9/10
39/39 - 46s - loss: 703.0211 - loglik: -7.0093e+02 - logprior: -2.0910e+00
Fitted a model with MAP estimate = -701.6599
expansions: [(0, 3), (43, 1), (135, 1), (140, 1), (144, 1), (164, 1), (165, 1), (176, 2), (177, 8), (178, 1), (188, 1), (190, 2), (191, 5), (192, 1), (193, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (211, 1), (212, 1), (217, 3), (221, 2), (222, 5), (223, 2), (226, 1), (227, 2), (228, 3), (229, 2), (230, 1), (241, 1), (243, 1), (244, 2), (245, 2), (246, 3), (248, 2), (249, 5), (250, 2), (251, 1), (253, 1), (269, 1), (270, 1), (271, 1), (286, 2), (287, 2), (288, 1), (291, 2), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (325, 1), (329, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [  1 128]
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 677.1060 - loglik: -6.7468e+02 - logprior: -2.4219e+00
Epoch 2/2
39/39 - 70s - loss: 658.8893 - loglik: -6.5813e+02 - logprior: -7.5674e-01
Fitted a model with MAP estimate = -655.7239
expansions: [(217, 1), (282, 1), (456, 1)]
discards: [  2   3 187 188 189 190 213 214 271 274 276 277 278 279 288 309 321 326
 327 328 329 374]
Fitting a model of length 440 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 669.4705 - loglik: -6.6820e+02 - logprior: -1.2702e+00
Epoch 2/2
39/39 - 65s - loss: 663.6575 - loglik: -6.6342e+02 - logprior: -2.4029e-01
Fitted a model with MAP estimate = -661.6422
expansions: [(266, 4), (267, 1), (310, 4)]
discards: [358]
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 663.3275 - loglik: -6.6209e+02 - logprior: -1.2341e+00
Epoch 2/10
39/39 - 67s - loss: 659.1087 - loglik: -6.5912e+02 - logprior: 0.0144
Epoch 3/10
39/39 - 67s - loss: 657.2349 - loglik: -6.5750e+02 - logprior: 0.2643
Epoch 4/10
39/39 - 67s - loss: 655.5594 - loglik: -6.5577e+02 - logprior: 0.2104
Epoch 5/10
39/39 - 67s - loss: 654.7738 - loglik: -6.5486e+02 - logprior: 0.0831
Epoch 6/10
39/39 - 67s - loss: 654.1771 - loglik: -6.5452e+02 - logprior: 0.3468
Epoch 7/10
39/39 - 67s - loss: 654.5782 - loglik: -6.5502e+02 - logprior: 0.4383
Fitted a model with MAP estimate = -653.4190
Time for alignment: 1455.1815
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 903.9719 - loglik: -9.0168e+02 - logprior: -2.2883e+00
Epoch 2/10
39/39 - 45s - loss: 717.4843 - loglik: -7.1612e+02 - logprior: -1.3629e+00
Epoch 3/10
39/39 - 45s - loss: 703.0120 - loglik: -7.0153e+02 - logprior: -1.4797e+00
Epoch 4/10
39/39 - 45s - loss: 700.3292 - loglik: -6.9888e+02 - logprior: -1.4516e+00
Epoch 5/10
39/39 - 46s - loss: 698.9667 - loglik: -6.9742e+02 - logprior: -1.5431e+00
Epoch 6/10
39/39 - 46s - loss: 698.4104 - loglik: -6.9680e+02 - logprior: -1.6145e+00
Epoch 7/10
39/39 - 46s - loss: 698.7355 - loglik: -6.9704e+02 - logprior: -1.6947e+00
Fitted a model with MAP estimate = -697.4645
expansions: [(0, 3), (24, 1), (43, 1), (132, 1), (134, 1), (136, 1), (146, 1), (163, 1), (164, 1), (168, 1), (169, 1), (173, 8), (174, 1), (175, 1), (176, 1), (187, 1), (188, 1), (189, 4), (190, 1), (191, 1), (194, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 1), (202, 1), (203, 1), (206, 1), (208, 1), (211, 1), (217, 1), (218, 1), (222, 2), (223, 7), (226, 1), (227, 2), (228, 3), (229, 2), (230, 1), (241, 1), (243, 1), (244, 2), (245, 2), (246, 3), (248, 2), (249, 5), (250, 1), (253, 1), (254, 2), (266, 1), (268, 1), (285, 1), (286, 2), (287, 2), (288, 2), (290, 1), (301, 1), (303, 1), (304, 1), (305, 1), (307, 1), (312, 1), (326, 1), (329, 1), (354, 1), (355, 6)]
discards: [  1   2   3 128]
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 677.5950 - loglik: -6.7512e+02 - logprior: -2.4709e+00
Epoch 2/2
39/39 - 69s - loss: 658.7849 - loglik: -6.5786e+02 - logprior: -9.2748e-01
Fitted a model with MAP estimate = -656.1095
expansions: [(212, 1), (279, 1)]
discards: [185 186 213 270 285 306 313 314 315 316 331 371 457]
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 663.9535 - loglik: -6.6237e+02 - logprior: -1.5840e+00
Epoch 2/2
39/39 - 67s - loss: 659.0967 - loglik: -6.5868e+02 - logprior: -4.1925e-01
Fitted a model with MAP estimate = -656.9693
expansions: [(212, 1), (306, 2)]
discards: [2 3]
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 661.6791 - loglik: -6.6053e+02 - logprior: -1.1494e+00
Epoch 2/10
39/39 - 67s - loss: 658.0162 - loglik: -6.5807e+02 - logprior: 0.0578
Epoch 3/10
39/39 - 67s - loss: 656.2190 - loglik: -6.5652e+02 - logprior: 0.2963
Epoch 4/10
39/39 - 67s - loss: 654.6594 - loglik: -6.5485e+02 - logprior: 0.1936
Epoch 5/10
39/39 - 67s - loss: 654.5013 - loglik: -6.5470e+02 - logprior: 0.2019
Epoch 6/10
39/39 - 67s - loss: 653.6168 - loglik: -6.5409e+02 - logprior: 0.4740
Epoch 7/10
39/39 - 67s - loss: 653.3090 - loglik: -6.5367e+02 - logprior: 0.3633
Epoch 8/10
39/39 - 67s - loss: 653.2593 - loglik: -6.5375e+02 - logprior: 0.4914
Epoch 9/10
39/39 - 67s - loss: 652.4600 - loglik: -6.5300e+02 - logprior: 0.5387
Epoch 10/10
39/39 - 67s - loss: 653.4999 - loglik: -6.5421e+02 - logprior: 0.7133
Fitted a model with MAP estimate = -651.7928
Time for alignment: 1571.8156
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 902.9401 - loglik: -9.0057e+02 - logprior: -2.3715e+00
Epoch 2/10
39/39 - 45s - loss: 718.3304 - loglik: -7.1673e+02 - logprior: -1.6049e+00
Epoch 3/10
39/39 - 45s - loss: 705.2473 - loglik: -7.0321e+02 - logprior: -2.0396e+00
Epoch 4/10
39/39 - 45s - loss: 702.7221 - loglik: -7.0079e+02 - logprior: -1.9271e+00
Epoch 5/10
39/39 - 45s - loss: 700.3957 - loglik: -6.9837e+02 - logprior: -2.0243e+00
Epoch 6/10
39/39 - 45s - loss: 701.0113 - loglik: -6.9896e+02 - logprior: -2.0483e+00
Fitted a model with MAP estimate = -699.6251
expansions: [(0, 3), (37, 1), (43, 1), (66, 1), (105, 1), (133, 1), (142, 1), (145, 1), (162, 1), (163, 1), (167, 1), (173, 8), (174, 1), (175, 1), (176, 1), (184, 1), (186, 1), (187, 1), (188, 4), (189, 1), (190, 1), (193, 1), (194, 3), (195, 1), (197, 1), (198, 2), (199, 2), (200, 1), (203, 1), (205, 1), (207, 1), (211, 1), (214, 1), (219, 2), (220, 5), (221, 1), (224, 1), (225, 3), (226, 1), (227, 1), (228, 2), (230, 1), (240, 1), (242, 1), (243, 2), (244, 3), (245, 2), (248, 6), (250, 1), (252, 1), (253, 1), (266, 1), (268, 1), (269, 1), (270, 1), (284, 1), (286, 2), (287, 2), (289, 1), (300, 1), (302, 1), (303, 1), (304, 1), (306, 1), (326, 1), (329, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [  2   3 126]
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 676.5143 - loglik: -6.7417e+02 - logprior: -2.3457e+00
Epoch 2/2
39/39 - 69s - loss: 659.1279 - loglik: -6.5843e+02 - logprior: -7.0293e-01
Fitted a model with MAP estimate = -656.2085
expansions: [(216, 1), (321, 1), (322, 2), (455, 1)]
discards: [  2 186 187 213 214 237 270 289 309 316 317 318 326 327 372]
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 665.3856 - loglik: -6.6404e+02 - logprior: -1.3463e+00
Epoch 2/2
39/39 - 67s - loss: 660.5626 - loglik: -6.6019e+02 - logprior: -3.7112e-01
Fitted a model with MAP estimate = -658.7595
expansions: [(316, 2)]
discards: [  2 307 308 310 311 317]
Fitting a model of length 444 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 664.0348 - loglik: -6.6288e+02 - logprior: -1.1537e+00
Epoch 2/10
39/39 - 66s - loss: 660.7592 - loglik: -6.6082e+02 - logprior: 0.0630
Epoch 3/10
39/39 - 66s - loss: 658.6132 - loglik: -6.5889e+02 - logprior: 0.2809
Epoch 4/10
39/39 - 66s - loss: 656.8484 - loglik: -6.5702e+02 - logprior: 0.1746
Epoch 5/10
39/39 - 66s - loss: 656.5204 - loglik: -6.5681e+02 - logprior: 0.2931
Epoch 6/10
39/39 - 66s - loss: 655.6857 - loglik: -6.5610e+02 - logprior: 0.4153
Epoch 7/10
39/39 - 66s - loss: 655.5953 - loglik: -6.5609e+02 - logprior: 0.4911
Epoch 8/10
39/39 - 66s - loss: 655.7240 - loglik: -6.5615e+02 - logprior: 0.4221
Fitted a model with MAP estimate = -654.4597
Time for alignment: 1381.4152
Computed alignments with likelihoods: ['-653.4190', '-651.7928', '-654.4597']
Best model has likelihood: -651.7928
SP score = 0.8729
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f511f889910>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f523c935eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f511f41b400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50e40a2640>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.4269 - loglik: -4.0599e+02 - logprior: -3.4344e+00
Epoch 2/10
19/19 - 4s - loss: 328.8631 - loglik: -3.2780e+02 - logprior: -1.0598e+00
Epoch 3/10
19/19 - 4s - loss: 300.7498 - loglik: -2.9949e+02 - logprior: -1.2623e+00
Epoch 4/10
19/19 - 4s - loss: 295.8743 - loglik: -2.9460e+02 - logprior: -1.2762e+00
Epoch 5/10
19/19 - 4s - loss: 294.4332 - loglik: -2.9317e+02 - logprior: -1.2656e+00
Epoch 6/10
19/19 - 4s - loss: 293.1651 - loglik: -2.9188e+02 - logprior: -1.2811e+00
Epoch 7/10
19/19 - 4s - loss: 292.6230 - loglik: -2.9133e+02 - logprior: -1.2905e+00
Epoch 8/10
19/19 - 4s - loss: 292.6649 - loglik: -2.9133e+02 - logprior: -1.3327e+00
Fitted a model with MAP estimate = -291.2157
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (52, 1), (54, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (106, 1), (113, 1), (115, 1)]
discards: []
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 291.7222 - loglik: -2.8748e+02 - logprior: -4.2376e+00
Epoch 2/2
19/19 - 5s - loss: 279.1872 - loglik: -2.7790e+02 - logprior: -1.2879e+00
Fitted a model with MAP estimate = -277.4543
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  73 100 101]
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 284.4387 - loglik: -2.8042e+02 - logprior: -4.0207e+00
Epoch 2/2
19/19 - 5s - loss: 279.6889 - loglik: -2.7841e+02 - logprior: -1.2750e+00
Fitted a model with MAP estimate = -278.0171
expansions: [(0, 5)]
discards: [0 1 2 4]
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 282.3043 - loglik: -2.7906e+02 - logprior: -3.2480e+00
Epoch 2/10
19/19 - 5s - loss: 278.3644 - loglik: -2.7726e+02 - logprior: -1.1086e+00
Epoch 3/10
19/19 - 5s - loss: 277.7714 - loglik: -2.7680e+02 - logprior: -9.6801e-01
Epoch 4/10
19/19 - 5s - loss: 276.3407 - loglik: -2.7546e+02 - logprior: -8.8319e-01
Epoch 5/10
19/19 - 5s - loss: 275.9723 - loglik: -2.7512e+02 - logprior: -8.4840e-01
Epoch 6/10
19/19 - 5s - loss: 275.0091 - loglik: -2.7417e+02 - logprior: -8.3514e-01
Epoch 7/10
19/19 - 5s - loss: 275.0036 - loglik: -2.7419e+02 - logprior: -8.1195e-01
Epoch 8/10
19/19 - 5s - loss: 273.8921 - loglik: -2.7308e+02 - logprior: -8.0863e-01
Epoch 9/10
19/19 - 5s - loss: 274.1358 - loglik: -2.7331e+02 - logprior: -8.2487e-01
Fitted a model with MAP estimate = -273.9112
Time for alignment: 139.0788
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.4505 - loglik: -4.0601e+02 - logprior: -3.4453e+00
Epoch 2/10
19/19 - 4s - loss: 328.2940 - loglik: -3.2723e+02 - logprior: -1.0673e+00
Epoch 3/10
19/19 - 4s - loss: 300.1565 - loglik: -2.9884e+02 - logprior: -1.3128e+00
Epoch 4/10
19/19 - 4s - loss: 294.5311 - loglik: -2.9319e+02 - logprior: -1.3417e+00
Epoch 5/10
19/19 - 4s - loss: 292.6633 - loglik: -2.9133e+02 - logprior: -1.3291e+00
Epoch 6/10
19/19 - 4s - loss: 291.9187 - loglik: -2.9057e+02 - logprior: -1.3438e+00
Epoch 7/10
19/19 - 4s - loss: 291.5636 - loglik: -2.9022e+02 - logprior: -1.3466e+00
Epoch 8/10
19/19 - 4s - loss: 291.3415 - loglik: -2.8997e+02 - logprior: -1.3760e+00
Epoch 9/10
19/19 - 4s - loss: 290.2473 - loglik: -2.8883e+02 - logprior: -1.4152e+00
Epoch 10/10
19/19 - 4s - loss: 291.0650 - loglik: -2.8962e+02 - logprior: -1.4478e+00
Fitted a model with MAP estimate = -290.2191
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (105, 1), (113, 1), (115, 1)]
discards: []
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 291.2291 - loglik: -2.8687e+02 - logprior: -4.3607e+00
Epoch 2/2
19/19 - 5s - loss: 279.8339 - loglik: -2.7853e+02 - logprior: -1.3005e+00
Fitted a model with MAP estimate = -277.8688
expansions: [(0, 4)]
discards: [  1   2   3   4  22  49 100 101]
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 284.8531 - loglik: -2.8081e+02 - logprior: -4.0468e+00
Epoch 2/2
19/19 - 5s - loss: 279.9989 - loglik: -2.7861e+02 - logprior: -1.3851e+00
Fitted a model with MAP estimate = -278.5784
expansions: [(0, 4)]
discards: [0 1 2 3 4]
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 282.7273 - loglik: -2.7945e+02 - logprior: -3.2778e+00
Epoch 2/10
19/19 - 5s - loss: 279.1627 - loglik: -2.7809e+02 - logprior: -1.0679e+00
Epoch 3/10
19/19 - 5s - loss: 277.5830 - loglik: -2.7669e+02 - logprior: -8.9708e-01
Epoch 4/10
19/19 - 5s - loss: 276.5930 - loglik: -2.7572e+02 - logprior: -8.6990e-01
Epoch 5/10
19/19 - 5s - loss: 275.9663 - loglik: -2.7512e+02 - logprior: -8.5090e-01
Epoch 6/10
19/19 - 5s - loss: 276.0961 - loglik: -2.7524e+02 - logprior: -8.5439e-01
Fitted a model with MAP estimate = -275.1361
Time for alignment: 132.6583
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 409.5783 - loglik: -4.0615e+02 - logprior: -3.4315e+00
Epoch 2/10
19/19 - 4s - loss: 328.0862 - loglik: -3.2702e+02 - logprior: -1.0623e+00
Epoch 3/10
19/19 - 4s - loss: 298.2853 - loglik: -2.9700e+02 - logprior: -1.2829e+00
Epoch 4/10
19/19 - 4s - loss: 293.0525 - loglik: -2.9175e+02 - logprior: -1.2994e+00
Epoch 5/10
19/19 - 4s - loss: 290.7113 - loglik: -2.8941e+02 - logprior: -1.3034e+00
Epoch 6/10
19/19 - 4s - loss: 290.5170 - loglik: -2.8920e+02 - logprior: -1.3141e+00
Epoch 7/10
19/19 - 4s - loss: 289.6802 - loglik: -2.8834e+02 - logprior: -1.3381e+00
Epoch 8/10
19/19 - 4s - loss: 289.7051 - loglik: -2.8834e+02 - logprior: -1.3694e+00
Fitted a model with MAP estimate = -288.8975
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 289.8066 - loglik: -2.8558e+02 - logprior: -4.2241e+00
Epoch 2/2
19/19 - 5s - loss: 278.2339 - loglik: -2.7696e+02 - logprior: -1.2730e+00
Fitted a model with MAP estimate = -276.3324
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  49 100 101 147]
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 283.6107 - loglik: -2.7961e+02 - logprior: -3.9988e+00
Epoch 2/2
19/19 - 5s - loss: 278.4944 - loglik: -2.7723e+02 - logprior: -1.2613e+00
Fitted a model with MAP estimate = -276.9644
expansions: [(0, 5)]
discards: [0 1 2 4]
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 281.0750 - loglik: -2.7786e+02 - logprior: -3.2172e+00
Epoch 2/10
19/19 - 5s - loss: 277.5595 - loglik: -2.7648e+02 - logprior: -1.0809e+00
Epoch 3/10
19/19 - 5s - loss: 276.4137 - loglik: -2.7549e+02 - logprior: -9.2814e-01
Epoch 4/10
19/19 - 5s - loss: 275.6814 - loglik: -2.7483e+02 - logprior: -8.5378e-01
Epoch 5/10
19/19 - 5s - loss: 274.4823 - loglik: -2.7367e+02 - logprior: -8.1685e-01
Epoch 6/10
19/19 - 5s - loss: 273.9741 - loglik: -2.7316e+02 - logprior: -8.1524e-01
Epoch 7/10
19/19 - 5s - loss: 273.2372 - loglik: -2.7242e+02 - logprior: -8.1948e-01
Epoch 8/10
19/19 - 5s - loss: 273.0435 - loglik: -2.7223e+02 - logprior: -8.1630e-01
Epoch 9/10
19/19 - 5s - loss: 272.9000 - loglik: -2.7206e+02 - logprior: -8.3819e-01
Epoch 10/10
19/19 - 5s - loss: 272.8696 - loglik: -2.7200e+02 - logprior: -8.6511e-01
Fitted a model with MAP estimate = -272.5265
Time for alignment: 143.4464
Computed alignments with likelihoods: ['-273.9112', '-275.1361', '-272.5265']
Best model has likelihood: -272.5265
SP score = 0.7043
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5203689f10>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f51284b0760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52380dec10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5237757ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 68.9971 - loglik: -6.5548e+01 - logprior: -3.4496e+00
Epoch 2/10
19/19 - 1s - loss: 52.2294 - loglik: -5.0765e+01 - logprior: -1.4640e+00
Epoch 3/10
19/19 - 1s - loss: 46.7138 - loglik: -4.5187e+01 - logprior: -1.5272e+00
Epoch 4/10
19/19 - 1s - loss: 44.7058 - loglik: -4.3110e+01 - logprior: -1.5961e+00
Epoch 5/10
19/19 - 1s - loss: 44.2957 - loglik: -4.2686e+01 - logprior: -1.6097e+00
Epoch 6/10
19/19 - 1s - loss: 44.1033 - loglik: -4.2493e+01 - logprior: -1.6098e+00
Epoch 7/10
19/19 - 1s - loss: 43.9822 - loglik: -4.2362e+01 - logprior: -1.6203e+00
Epoch 8/10
19/19 - 1s - loss: 43.9123 - loglik: -4.2290e+01 - logprior: -1.6220e+00
Epoch 9/10
19/19 - 1s - loss: 43.9249 - loglik: -4.2298e+01 - logprior: -1.6271e+00
Fitted a model with MAP estimate = -43.8405
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.1321 - loglik: -4.2377e+01 - logprior: -4.7549e+00
Epoch 2/2
19/19 - 1s - loss: 42.1254 - loglik: -4.0725e+01 - logprior: -1.4001e+00
Fitted a model with MAP estimate = -41.3377
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.2024 - loglik: -4.0819e+01 - logprior: -3.3839e+00
Epoch 2/2
19/19 - 1s - loss: 41.8090 - loglik: -4.0227e+01 - logprior: -1.5820e+00
Fitted a model with MAP estimate = -41.4481
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.8997 - loglik: -4.0575e+01 - logprior: -3.3245e+00
Epoch 2/10
19/19 - 1s - loss: 41.6691 - loglik: -4.0107e+01 - logprior: -1.5624e+00
Epoch 3/10
19/19 - 1s - loss: 41.3031 - loglik: -3.9824e+01 - logprior: -1.4790e+00
Epoch 4/10
19/19 - 1s - loss: 41.0357 - loglik: -3.9581e+01 - logprior: -1.4547e+00
Epoch 5/10
19/19 - 1s - loss: 40.9614 - loglik: -3.9512e+01 - logprior: -1.4499e+00
Epoch 6/10
19/19 - 1s - loss: 40.6974 - loglik: -3.9238e+01 - logprior: -1.4592e+00
Epoch 7/10
19/19 - 1s - loss: 40.5691 - loglik: -3.9102e+01 - logprior: -1.4666e+00
Epoch 8/10
19/19 - 1s - loss: 40.5650 - loglik: -3.9104e+01 - logprior: -1.4607e+00
Epoch 9/10
19/19 - 1s - loss: 40.5141 - loglik: -3.9052e+01 - logprior: -1.4622e+00
Epoch 10/10
19/19 - 1s - loss: 40.4676 - loglik: -3.9008e+01 - logprior: -1.4593e+00
Fitted a model with MAP estimate = -40.4044
Time for alignment: 36.4089
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 68.9343 - loglik: -6.5490e+01 - logprior: -3.4439e+00
Epoch 2/10
19/19 - 1s - loss: 52.0935 - loglik: -5.0653e+01 - logprior: -1.4409e+00
Epoch 3/10
19/19 - 1s - loss: 46.8327 - loglik: -4.5466e+01 - logprior: -1.3671e+00
Epoch 4/10
19/19 - 1s - loss: 44.8467 - loglik: -4.3526e+01 - logprior: -1.3205e+00
Epoch 5/10
19/19 - 1s - loss: 44.3955 - loglik: -4.3083e+01 - logprior: -1.3124e+00
Epoch 6/10
19/19 - 1s - loss: 44.1501 - loglik: -4.2839e+01 - logprior: -1.3108e+00
Epoch 7/10
19/19 - 1s - loss: 44.1396 - loglik: -4.2828e+01 - logprior: -1.3112e+00
Epoch 8/10
19/19 - 1s - loss: 44.0320 - loglik: -4.2721e+01 - logprior: -1.3115e+00
Epoch 9/10
19/19 - 1s - loss: 44.0088 - loglik: -4.2699e+01 - logprior: -1.3101e+00
Epoch 10/10
19/19 - 1s - loss: 43.9752 - loglik: -4.2664e+01 - logprior: -1.3117e+00
Fitted a model with MAP estimate = -43.9236
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.4042 - loglik: -4.2616e+01 - logprior: -4.7881e+00
Epoch 2/2
19/19 - 1s - loss: 42.2243 - loglik: -4.0501e+01 - logprior: -1.7229e+00
Fitted a model with MAP estimate = -41.6278
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 43.9323 - loglik: -4.0667e+01 - logprior: -3.2649e+00
Epoch 2/10
19/19 - 1s - loss: 41.7614 - loglik: -4.0241e+01 - logprior: -1.5202e+00
Epoch 3/10
19/19 - 1s - loss: 41.3805 - loglik: -3.9880e+01 - logprior: -1.5002e+00
Epoch 4/10
19/19 - 1s - loss: 41.1030 - loglik: -3.9632e+01 - logprior: -1.4714e+00
Epoch 5/10
19/19 - 1s - loss: 40.9027 - loglik: -3.9438e+01 - logprior: -1.4643e+00
Epoch 6/10
19/19 - 1s - loss: 40.6769 - loglik: -3.9203e+01 - logprior: -1.4737e+00
Epoch 7/10
19/19 - 1s - loss: 40.5515 - loglik: -3.9075e+01 - logprior: -1.4766e+00
Epoch 8/10
19/19 - 1s - loss: 40.6155 - loglik: -3.9134e+01 - logprior: -1.4813e+00
Fitted a model with MAP estimate = -40.5097
Time for alignment: 29.3326
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.9327 - loglik: -6.5482e+01 - logprior: -3.4508e+00
Epoch 2/10
19/19 - 1s - loss: 51.6540 - loglik: -5.0194e+01 - logprior: -1.4604e+00
Epoch 3/10
19/19 - 1s - loss: 46.1125 - loglik: -4.4591e+01 - logprior: -1.5217e+00
Epoch 4/10
19/19 - 1s - loss: 44.6335 - loglik: -4.3029e+01 - logprior: -1.6045e+00
Epoch 5/10
19/19 - 1s - loss: 44.3019 - loglik: -4.2700e+01 - logprior: -1.6018e+00
Epoch 6/10
19/19 - 1s - loss: 44.1264 - loglik: -4.2512e+01 - logprior: -1.6144e+00
Epoch 7/10
19/19 - 1s - loss: 43.9820 - loglik: -4.2365e+01 - logprior: -1.6170e+00
Epoch 8/10
19/19 - 1s - loss: 43.9102 - loglik: -4.2289e+01 - logprior: -1.6214e+00
Epoch 9/10
19/19 - 1s - loss: 43.8175 - loglik: -4.2195e+01 - logprior: -1.6224e+00
Epoch 10/10
19/19 - 1s - loss: 43.9244 - loglik: -4.2301e+01 - logprior: -1.6237e+00
Fitted a model with MAP estimate = -43.8147
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.2057 - loglik: -4.2400e+01 - logprior: -4.8059e+00
Epoch 2/2
19/19 - 1s - loss: 42.1583 - loglik: -4.0754e+01 - logprior: -1.4040e+00
Fitted a model with MAP estimate = -41.3649
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 44.2245 - loglik: -4.0848e+01 - logprior: -3.3762e+00
Epoch 2/2
19/19 - 1s - loss: 41.7757 - loglik: -4.0194e+01 - logprior: -1.5820e+00
Fitted a model with MAP estimate = -41.4398
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.8881 - loglik: -4.0566e+01 - logprior: -3.3224e+00
Epoch 2/10
19/19 - 1s - loss: 41.6617 - loglik: -4.0099e+01 - logprior: -1.5623e+00
Epoch 3/10
19/19 - 1s - loss: 41.3232 - loglik: -3.9843e+01 - logprior: -1.4804e+00
Epoch 4/10
19/19 - 1s - loss: 41.0637 - loglik: -3.9611e+01 - logprior: -1.4532e+00
Epoch 5/10
19/19 - 1s - loss: 40.8943 - loglik: -3.9444e+01 - logprior: -1.4508e+00
Epoch 6/10
19/19 - 1s - loss: 40.7271 - loglik: -3.9269e+01 - logprior: -1.4585e+00
Epoch 7/10
19/19 - 1s - loss: 40.5839 - loglik: -3.9119e+01 - logprior: -1.4652e+00
Epoch 8/10
19/19 - 1s - loss: 40.5517 - loglik: -3.9090e+01 - logprior: -1.4613e+00
Epoch 9/10
19/19 - 1s - loss: 40.5830 - loglik: -3.9125e+01 - logprior: -1.4575e+00
Fitted a model with MAP estimate = -40.4740
Time for alignment: 34.1796
Computed alignments with likelihoods: ['-40.4044', '-40.5097', '-40.4740']
Best model has likelihood: -40.4044
SP score = 0.9005
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50fdc21070>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52379b7370>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50b11cb2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f523cf4cdf0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 734.7476 - loglik: -7.3254e+02 - logprior: -2.2075e+00
Epoch 2/10
39/39 - 23s - loss: 531.5894 - loglik: -5.2990e+02 - logprior: -1.6844e+00
Epoch 3/10
39/39 - 23s - loss: 519.2139 - loglik: -5.1742e+02 - logprior: -1.7932e+00
Epoch 4/10
39/39 - 23s - loss: 516.7667 - loglik: -5.1500e+02 - logprior: -1.7639e+00
Epoch 5/10
39/39 - 23s - loss: 515.2067 - loglik: -5.1339e+02 - logprior: -1.8187e+00
Epoch 6/10
39/39 - 23s - loss: 514.5641 - loglik: -5.1267e+02 - logprior: -1.8955e+00
Epoch 7/10
39/39 - 23s - loss: 514.1010 - loglik: -5.1215e+02 - logprior: -1.9527e+00
Epoch 8/10
39/39 - 23s - loss: 513.9719 - loglik: -5.1197e+02 - logprior: -2.0061e+00
Epoch 9/10
39/39 - 23s - loss: 513.9639 - loglik: -5.1192e+02 - logprior: -2.0445e+00
Epoch 10/10
39/39 - 23s - loss: 513.4697 - loglik: -5.1140e+02 - logprior: -2.0723e+00
Fitted a model with MAP estimate = -512.9154
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (17, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (133, 1), (134, 1), (142, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (207, 1), (209, 1), (213, 1), (214, 1), (217, 3), (230, 3), (231, 1), (243, 1), (244, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 483.5177 - loglik: -4.8106e+02 - logprior: -2.4580e+00
Epoch 2/2
39/39 - 34s - loss: 465.8962 - loglik: -4.6508e+02 - logprior: -8.1325e-01
Fitted a model with MAP estimate = -464.0403
expansions: []
discards: [  0   1  97 178 213]
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 471.6027 - loglik: -4.6985e+02 - logprior: -1.7534e+00
Epoch 2/2
39/39 - 33s - loss: 466.7214 - loglik: -4.6658e+02 - logprior: -1.4379e-01
Fitted a model with MAP estimate = -464.6499
expansions: [(0, 2)]
discards: []
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 469.2827 - loglik: -4.6762e+02 - logprior: -1.6630e+00
Epoch 2/10
39/39 - 34s - loss: 464.8052 - loglik: -4.6463e+02 - logprior: -1.7708e-01
Epoch 3/10
39/39 - 34s - loss: 462.9730 - loglik: -4.6296e+02 - logprior: -1.1066e-02
Epoch 4/10
39/39 - 34s - loss: 461.5909 - loglik: -4.6162e+02 - logprior: 0.0330
Epoch 5/10
39/39 - 34s - loss: 460.5216 - loglik: -4.6062e+02 - logprior: 0.0971
Epoch 6/10
39/39 - 34s - loss: 460.0078 - loglik: -4.6002e+02 - logprior: 0.0171
Epoch 7/10
39/39 - 34s - loss: 459.6479 - loglik: -4.5988e+02 - logprior: 0.2346
Epoch 8/10
39/39 - 34s - loss: 459.4953 - loglik: -4.5962e+02 - logprior: 0.1219
Epoch 9/10
39/39 - 34s - loss: 459.3275 - loglik: -4.5975e+02 - logprior: 0.4255
Epoch 10/10
39/39 - 34s - loss: 459.1997 - loglik: -4.5949e+02 - logprior: 0.2951
Fitted a model with MAP estimate = -458.5977
Time for alignment: 887.4916
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 735.4238 - loglik: -7.3323e+02 - logprior: -2.1954e+00
Epoch 2/10
39/39 - 23s - loss: 529.2272 - loglik: -5.2749e+02 - logprior: -1.7323e+00
Epoch 3/10
39/39 - 23s - loss: 517.3231 - loglik: -5.1552e+02 - logprior: -1.8079e+00
Epoch 4/10
39/39 - 23s - loss: 514.8440 - loglik: -5.1312e+02 - logprior: -1.7258e+00
Epoch 5/10
39/39 - 23s - loss: 513.5462 - loglik: -5.1177e+02 - logprior: -1.7809e+00
Epoch 6/10
39/39 - 23s - loss: 513.2245 - loglik: -5.1140e+02 - logprior: -1.8261e+00
Epoch 7/10
39/39 - 23s - loss: 512.9431 - loglik: -5.1107e+02 - logprior: -1.8761e+00
Epoch 8/10
39/39 - 23s - loss: 512.7583 - loglik: -5.1087e+02 - logprior: -1.8901e+00
Epoch 9/10
39/39 - 23s - loss: 512.6146 - loglik: -5.1067e+02 - logprior: -1.9437e+00
Epoch 10/10
39/39 - 23s - loss: 512.5056 - loglik: -5.1056e+02 - logprior: -1.9488e+00
Fitted a model with MAP estimate = -511.7871
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (182, 3), (188, 1), (205, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (228, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 483.7593 - loglik: -4.8130e+02 - logprior: -2.4628e+00
Epoch 2/2
39/39 - 34s - loss: 466.1913 - loglik: -4.6546e+02 - logprior: -7.2860e-01
Fitted a model with MAP estimate = -463.4413
expansions: []
discards: [  0   1  97 212 227]
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 471.2978 - loglik: -4.6959e+02 - logprior: -1.7029e+00
Epoch 2/2
39/39 - 33s - loss: 466.9628 - loglik: -4.6677e+02 - logprior: -1.8945e-01
Fitted a model with MAP estimate = -464.7040
expansions: [(0, 2)]
discards: []
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 468.9270 - loglik: -4.6746e+02 - logprior: -1.4623e+00
Epoch 2/10
39/39 - 34s - loss: 464.9144 - loglik: -4.6462e+02 - logprior: -2.9110e-01
Epoch 3/10
39/39 - 34s - loss: 462.6344 - loglik: -4.6270e+02 - logprior: 0.0700
Epoch 4/10
39/39 - 34s - loss: 461.6175 - loglik: -4.6152e+02 - logprior: -9.2700e-02
Epoch 5/10
39/39 - 34s - loss: 460.7835 - loglik: -4.6094e+02 - logprior: 0.1578
Epoch 6/10
39/39 - 34s - loss: 459.6583 - loglik: -4.5959e+02 - logprior: -6.8170e-02
Epoch 7/10
39/39 - 34s - loss: 459.8754 - loglik: -4.6017e+02 - logprior: 0.2900
Fitted a model with MAP estimate = -459.2086
Time for alignment: 786.7449
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 732.9741 - loglik: -7.3077e+02 - logprior: -2.1996e+00
Epoch 2/10
39/39 - 23s - loss: 529.5487 - loglik: -5.2787e+02 - logprior: -1.6826e+00
Epoch 3/10
39/39 - 23s - loss: 517.8848 - loglik: -5.1610e+02 - logprior: -1.7882e+00
Epoch 4/10
39/39 - 23s - loss: 515.4534 - loglik: -5.1372e+02 - logprior: -1.7296e+00
Epoch 5/10
39/39 - 23s - loss: 514.4116 - loglik: -5.1262e+02 - logprior: -1.7891e+00
Epoch 6/10
39/39 - 23s - loss: 513.2768 - loglik: -5.1141e+02 - logprior: -1.8717e+00
Epoch 7/10
39/39 - 23s - loss: 512.9186 - loglik: -5.1100e+02 - logprior: -1.9187e+00
Epoch 8/10
39/39 - 23s - loss: 512.8746 - loglik: -5.1091e+02 - logprior: -1.9666e+00
Epoch 9/10
39/39 - 23s - loss: 512.5615 - loglik: -5.1056e+02 - logprior: -2.0031e+00
Epoch 10/10
39/39 - 23s - loss: 511.9731 - loglik: -5.0992e+02 - logprior: -2.0502e+00
Fitted a model with MAP estimate = -511.6972
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 1), (112, 2), (113, 1), (132, 3), (133, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 484.3158 - loglik: -4.8178e+02 - logprior: -2.5402e+00
Epoch 2/2
39/39 - 35s - loss: 465.8675 - loglik: -4.6498e+02 - logprior: -8.9086e-01
Fitted a model with MAP estimate = -463.0567
expansions: []
discards: [  0  57  98 141 165 181 216]
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 472.2309 - loglik: -4.6959e+02 - logprior: -2.6419e+00
Epoch 2/2
39/39 - 33s - loss: 466.6637 - loglik: -4.6616e+02 - logprior: -5.0043e-01
Fitted a model with MAP estimate = -464.1950
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 468.4944 - loglik: -4.6711e+02 - logprior: -1.3806e+00
Epoch 2/10
39/39 - 34s - loss: 464.6101 - loglik: -4.6450e+02 - logprior: -1.1122e-01
Epoch 3/10
39/39 - 34s - loss: 462.6501 - loglik: -4.6261e+02 - logprior: -3.7590e-02
Epoch 4/10
39/39 - 34s - loss: 461.1608 - loglik: -4.6133e+02 - logprior: 0.1688
Epoch 5/10
39/39 - 34s - loss: 460.6734 - loglik: -4.6056e+02 - logprior: -1.1400e-01
Epoch 6/10
39/39 - 34s - loss: 459.5322 - loglik: -4.5975e+02 - logprior: 0.2156
Epoch 7/10
39/39 - 34s - loss: 459.4956 - loglik: -4.5945e+02 - logprior: -4.8178e-02
Epoch 8/10
39/39 - 34s - loss: 459.1069 - loglik: -4.5938e+02 - logprior: 0.2694
Epoch 9/10
39/39 - 34s - loss: 459.3143 - loglik: -4.5957e+02 - logprior: 0.2534
Fitted a model with MAP estimate = -458.9447
Time for alignment: 856.0603
Computed alignments with likelihoods: ['-458.5977', '-459.2086', '-458.9447']
Best model has likelihood: -458.5977
SP score = 0.9174
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f520325aee0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f50b1448eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5239304730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52393047c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 398.0535 - loglik: -3.9454e+02 - logprior: -3.5088e+00
Epoch 2/10
19/19 - 5s - loss: 326.0193 - loglik: -3.2471e+02 - logprior: -1.3064e+00
Epoch 3/10
19/19 - 5s - loss: 305.0768 - loglik: -3.0341e+02 - logprior: -1.6685e+00
Epoch 4/10
19/19 - 5s - loss: 300.2943 - loglik: -2.9875e+02 - logprior: -1.5485e+00
Epoch 5/10
19/19 - 5s - loss: 298.0355 - loglik: -2.9651e+02 - logprior: -1.5304e+00
Epoch 6/10
19/19 - 5s - loss: 297.0813 - loglik: -2.9553e+02 - logprior: -1.5515e+00
Epoch 7/10
19/19 - 5s - loss: 296.1443 - loglik: -2.9457e+02 - logprior: -1.5790e+00
Epoch 8/10
19/19 - 5s - loss: 294.9230 - loglik: -2.9331e+02 - logprior: -1.6177e+00
Epoch 9/10
19/19 - 5s - loss: 294.6613 - loglik: -2.9299e+02 - logprior: -1.6699e+00
Epoch 10/10
19/19 - 5s - loss: 293.6896 - loglik: -2.9197e+02 - logprior: -1.7220e+00
Fitted a model with MAP estimate = -292.6388
expansions: [(7, 2), (21, 1), (23, 1), (24, 1), (25, 2), (29, 2), (35, 1), (37, 2), (46, 3), (51, 2), (56, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 307.3299 - loglik: -3.0297e+02 - logprior: -4.3578e+00
Epoch 2/2
19/19 - 6s - loss: 296.3589 - loglik: -2.9392e+02 - logprior: -2.4385e+00
Fitted a model with MAP estimate = -293.6005
expansions: [(0, 2)]
discards: [  0  29  35  47  65 104 136]
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 295.7588 - loglik: -2.9262e+02 - logprior: -3.1431e+00
Epoch 2/2
19/19 - 6s - loss: 290.3428 - loglik: -2.8904e+02 - logprior: -1.3010e+00
Fitted a model with MAP estimate = -289.1265
expansions: []
discards: [ 0 56]
Fitting a model of length 137 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 297.6662 - loglik: -2.9366e+02 - logprior: -4.0024e+00
Epoch 2/10
19/19 - 6s - loss: 291.4763 - loglik: -2.8986e+02 - logprior: -1.6202e+00
Epoch 3/10
19/19 - 6s - loss: 289.6803 - loglik: -2.8836e+02 - logprior: -1.3161e+00
Epoch 4/10
19/19 - 6s - loss: 288.3163 - loglik: -2.8707e+02 - logprior: -1.2440e+00
Epoch 5/10
19/19 - 6s - loss: 288.0186 - loglik: -2.8682e+02 - logprior: -1.1985e+00
Epoch 6/10
19/19 - 6s - loss: 285.7693 - loglik: -2.8460e+02 - logprior: -1.1653e+00
Epoch 7/10
19/19 - 6s - loss: 283.8716 - loglik: -2.8275e+02 - logprior: -1.1196e+00
Epoch 8/10
19/19 - 6s - loss: 285.1601 - loglik: -2.8402e+02 - logprior: -1.1390e+00
Fitted a model with MAP estimate = -282.8085
Time for alignment: 164.6061
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 398.1523 - loglik: -3.9464e+02 - logprior: -3.5116e+00
Epoch 2/10
19/19 - 5s - loss: 327.1139 - loglik: -3.2581e+02 - logprior: -1.3029e+00
Epoch 3/10
19/19 - 5s - loss: 305.7400 - loglik: -3.0405e+02 - logprior: -1.6923e+00
Epoch 4/10
19/19 - 5s - loss: 301.4051 - loglik: -2.9987e+02 - logprior: -1.5396e+00
Epoch 5/10
19/19 - 5s - loss: 299.3057 - loglik: -2.9778e+02 - logprior: -1.5264e+00
Epoch 6/10
19/19 - 5s - loss: 298.7328 - loglik: -2.9720e+02 - logprior: -1.5284e+00
Epoch 7/10
19/19 - 5s - loss: 297.3390 - loglik: -2.9577e+02 - logprior: -1.5700e+00
Epoch 8/10
19/19 - 5s - loss: 295.1531 - loglik: -2.9354e+02 - logprior: -1.6112e+00
Epoch 9/10
19/19 - 5s - loss: 296.9331 - loglik: -2.9528e+02 - logprior: -1.6557e+00
Fitted a model with MAP estimate = -294.0467
expansions: [(7, 2), (21, 3), (22, 1), (25, 1), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Fitting a model of length 145 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 307.6048 - loglik: -3.0328e+02 - logprior: -4.3221e+00
Epoch 2/2
19/19 - 6s - loss: 296.5377 - loglik: -2.9409e+02 - logprior: -2.4459e+00
Fitted a model with MAP estimate = -293.5768
expansions: [(0, 2)]
discards: [  0  23  36  47  59  66 137]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 295.8307 - loglik: -2.9268e+02 - logprior: -3.1460e+00
Epoch 2/2
19/19 - 6s - loss: 290.7056 - loglik: -2.8939e+02 - logprior: -1.3169e+00
Fitted a model with MAP estimate = -289.0969
expansions: []
discards: [0]
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 297.2764 - loglik: -2.9326e+02 - logprior: -4.0142e+00
Epoch 2/10
19/19 - 6s - loss: 291.8169 - loglik: -2.9021e+02 - logprior: -1.6099e+00
Epoch 3/10
19/19 - 6s - loss: 289.4803 - loglik: -2.8818e+02 - logprior: -1.3052e+00
Epoch 4/10
19/19 - 6s - loss: 288.4264 - loglik: -2.8719e+02 - logprior: -1.2366e+00
Epoch 5/10
19/19 - 6s - loss: 286.2203 - loglik: -2.8502e+02 - logprior: -1.2025e+00
Epoch 6/10
19/19 - 6s - loss: 285.3612 - loglik: -2.8422e+02 - logprior: -1.1453e+00
Epoch 7/10
19/19 - 6s - loss: 285.4378 - loglik: -2.8431e+02 - logprior: -1.1327e+00
Fitted a model with MAP estimate = -283.3465
Time for alignment: 154.4618
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 397.8443 - loglik: -3.9433e+02 - logprior: -3.5149e+00
Epoch 2/10
19/19 - 5s - loss: 326.8190 - loglik: -3.2552e+02 - logprior: -1.2971e+00
Epoch 3/10
19/19 - 5s - loss: 305.0417 - loglik: -3.0337e+02 - logprior: -1.6736e+00
Epoch 4/10
19/19 - 5s - loss: 300.3105 - loglik: -2.9879e+02 - logprior: -1.5175e+00
Epoch 5/10
19/19 - 5s - loss: 298.3791 - loglik: -2.9687e+02 - logprior: -1.5057e+00
Epoch 6/10
19/19 - 5s - loss: 296.5990 - loglik: -2.9507e+02 - logprior: -1.5314e+00
Epoch 7/10
19/19 - 5s - loss: 296.9342 - loglik: -2.9536e+02 - logprior: -1.5711e+00
Fitted a model with MAP estimate = -294.2329
expansions: [(7, 2), (22, 2), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 1), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Fitting a model of length 146 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 306.6715 - loglik: -3.0235e+02 - logprior: -4.3174e+00
Epoch 2/2
19/19 - 6s - loss: 296.5432 - loglik: -2.9410e+02 - logprior: -2.4411e+00
Fitted a model with MAP estimate = -293.3885
expansions: [(0, 2)]
discards: [  0  23  30  37  48  60  67 138]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 295.7929 - loglik: -2.9263e+02 - logprior: -3.1584e+00
Epoch 2/2
19/19 - 6s - loss: 290.9033 - loglik: -2.8960e+02 - logprior: -1.3055e+00
Fitted a model with MAP estimate = -289.1311
expansions: []
discards: [0]
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 297.0279 - loglik: -2.9303e+02 - logprior: -3.9932e+00
Epoch 2/10
19/19 - 6s - loss: 292.0040 - loglik: -2.9041e+02 - logprior: -1.5968e+00
Epoch 3/10
19/19 - 6s - loss: 289.6012 - loglik: -2.8829e+02 - logprior: -1.3147e+00
Epoch 4/10
19/19 - 6s - loss: 288.7898 - loglik: -2.8756e+02 - logprior: -1.2338e+00
Epoch 5/10
19/19 - 6s - loss: 285.9617 - loglik: -2.8478e+02 - logprior: -1.1842e+00
Epoch 6/10
19/19 - 6s - loss: 286.4314 - loglik: -2.8527e+02 - logprior: -1.1645e+00
Fitted a model with MAP estimate = -284.4572
Time for alignment: 137.4032
Computed alignments with likelihoods: ['-282.8085', '-283.3465', '-284.4572']
Best model has likelihood: -282.8085
SP score = 0.3655
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5239b83d90>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52031ed250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f525e9f01c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f514a22d7c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 489.4432 - loglik: -4.8606e+02 - logprior: -3.3882e+00
Epoch 2/10
19/19 - 6s - loss: 450.7650 - loglik: -4.4962e+02 - logprior: -1.1430e+00
Epoch 3/10
19/19 - 6s - loss: 430.5829 - loglik: -4.2930e+02 - logprior: -1.2866e+00
Epoch 4/10
19/19 - 6s - loss: 425.3730 - loglik: -4.2412e+02 - logprior: -1.2573e+00
Epoch 5/10
19/19 - 6s - loss: 422.4202 - loglik: -4.2103e+02 - logprior: -1.3927e+00
Epoch 6/10
19/19 - 6s - loss: 422.0510 - loglik: -4.2056e+02 - logprior: -1.4890e+00
Epoch 7/10
19/19 - 6s - loss: 420.7652 - loglik: -4.1915e+02 - logprior: -1.6188e+00
Epoch 8/10
19/19 - 6s - loss: 418.5689 - loglik: -4.1681e+02 - logprior: -1.7574e+00
Epoch 9/10
19/19 - 6s - loss: 419.3867 - loglik: -4.1753e+02 - logprior: -1.8602e+00
Fitted a model with MAP estimate = -417.9456
expansions: [(19, 2), (20, 1), (31, 4), (32, 1), (33, 1), (48, 3), (50, 1), (54, 1), (75, 1), (77, 2), (78, 7), (79, 2), (94, 1), (97, 1), (107, 4), (108, 1)]
discards: [ 1 58 59 60]
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 428.1300 - loglik: -4.2483e+02 - logprior: -3.3050e+00
Epoch 2/2
19/19 - 8s - loss: 416.5764 - loglik: -4.1552e+02 - logprior: -1.0574e+00
Fitted a model with MAP estimate = -414.9080
expansions: []
discards: [35 57 95 96 97 98]
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 419.5668 - loglik: -4.1680e+02 - logprior: -2.7702e+00
Epoch 2/2
19/19 - 7s - loss: 415.2988 - loglik: -4.1441e+02 - logprior: -8.9217e-01
Fitted a model with MAP estimate = -414.7411
expansions: []
discards: []
Fitting a model of length 151 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 418.2057 - loglik: -4.1546e+02 - logprior: -2.7489e+00
Epoch 2/10
19/19 - 7s - loss: 416.0279 - loglik: -4.1517e+02 - logprior: -8.5464e-01
Epoch 3/10
19/19 - 7s - loss: 414.0192 - loglik: -4.1336e+02 - logprior: -6.6305e-01
Epoch 4/10
19/19 - 7s - loss: 413.6881 - loglik: -4.1301e+02 - logprior: -6.7895e-01
Epoch 5/10
19/19 - 7s - loss: 412.0807 - loglik: -4.1137e+02 - logprior: -7.1020e-01
Epoch 6/10
19/19 - 7s - loss: 410.0183 - loglik: -4.0925e+02 - logprior: -7.6620e-01
Epoch 7/10
19/19 - 7s - loss: 411.3896 - loglik: -4.1055e+02 - logprior: -8.4187e-01
Fitted a model with MAP estimate = -409.0068
Time for alignment: 191.9664
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 489.7962 - loglik: -4.8640e+02 - logprior: -3.4003e+00
Epoch 2/10
19/19 - 6s - loss: 449.7734 - loglik: -4.4862e+02 - logprior: -1.1536e+00
Epoch 3/10
19/19 - 6s - loss: 428.9565 - loglik: -4.2759e+02 - logprior: -1.3644e+00
Epoch 4/10
19/19 - 6s - loss: 423.0741 - loglik: -4.2170e+02 - logprior: -1.3742e+00
Epoch 5/10
19/19 - 6s - loss: 423.3872 - loglik: -4.2194e+02 - logprior: -1.4510e+00
Fitted a model with MAP estimate = -421.2489
expansions: [(19, 2), (20, 1), (31, 1), (32, 1), (33, 2), (47, 1), (48, 1), (49, 1), (50, 2), (54, 1), (55, 2), (56, 1), (76, 1), (78, 4), (94, 1), (102, 1), (107, 1), (108, 1), (117, 1), (118, 2)]
discards: [0]
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 424.7679 - loglik: -4.2097e+02 - logprior: -3.7961e+00
Epoch 2/2
19/19 - 8s - loss: 417.6289 - loglik: -4.1593e+02 - logprior: -1.6983e+00
Fitted a model with MAP estimate = -415.5952
expansions: [(0, 4), (99, 1)]
discards: [ 0 56]
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 418.8209 - loglik: -4.1596e+02 - logprior: -2.8658e+00
Epoch 2/2
19/19 - 8s - loss: 414.5787 - loglik: -4.1352e+02 - logprior: -1.0576e+00
Fitted a model with MAP estimate = -413.6106
expansions: [(24, 1)]
discards: [0 1 2 3]
Fitting a model of length 155 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 419.6825 - loglik: -4.1618e+02 - logprior: -3.4990e+00
Epoch 2/10
19/19 - 8s - loss: 416.0974 - loglik: -4.1456e+02 - logprior: -1.5390e+00
Epoch 3/10
19/19 - 8s - loss: 415.1983 - loglik: -4.1423e+02 - logprior: -9.6535e-01
Epoch 4/10
19/19 - 8s - loss: 413.8665 - loglik: -4.1326e+02 - logprior: -6.0554e-01
Epoch 5/10
19/19 - 8s - loss: 411.9301 - loglik: -4.1127e+02 - logprior: -6.6170e-01
Epoch 6/10
19/19 - 7s - loss: 410.8829 - loglik: -4.1019e+02 - logprior: -6.9292e-01
Epoch 7/10
19/19 - 8s - loss: 410.0470 - loglik: -4.0932e+02 - logprior: -7.2486e-01
Epoch 8/10
19/19 - 7s - loss: 407.1614 - loglik: -4.0635e+02 - logprior: -8.1624e-01
Epoch 9/10
19/19 - 8s - loss: 410.8613 - loglik: -4.0991e+02 - logprior: -9.5106e-01
Fitted a model with MAP estimate = -407.7958
Time for alignment: 184.5699
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 489.8155 - loglik: -4.8642e+02 - logprior: -3.3914e+00
Epoch 2/10
19/19 - 6s - loss: 448.2066 - loglik: -4.4707e+02 - logprior: -1.1413e+00
Epoch 3/10
19/19 - 6s - loss: 429.6967 - loglik: -4.2837e+02 - logprior: -1.3275e+00
Epoch 4/10
19/19 - 6s - loss: 426.7968 - loglik: -4.2549e+02 - logprior: -1.3100e+00
Epoch 5/10
19/19 - 6s - loss: 423.8863 - loglik: -4.2244e+02 - logprior: -1.4468e+00
Epoch 6/10
19/19 - 6s - loss: 422.2825 - loglik: -4.2074e+02 - logprior: -1.5381e+00
Epoch 7/10
19/19 - 6s - loss: 422.1662 - loglik: -4.2053e+02 - logprior: -1.6315e+00
Epoch 8/10
19/19 - 6s - loss: 419.5482 - loglik: -4.1780e+02 - logprior: -1.7461e+00
Epoch 9/10
19/19 - 6s - loss: 421.0351 - loglik: -4.1915e+02 - logprior: -1.8835e+00
Fitted a model with MAP estimate = -419.1212
expansions: [(19, 2), (20, 1), (31, 4), (32, 3), (47, 1), (48, 1), (49, 2), (55, 2), (58, 1), (73, 1), (75, 2), (78, 2), (79, 6), (93, 1), (101, 1), (104, 1), (105, 1), (106, 1), (108, 1)]
discards: [1]
Fitting a model of length 161 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 427.9316 - loglik: -4.2461e+02 - logprior: -3.3196e+00
Epoch 2/2
19/19 - 8s - loss: 417.2659 - loglik: -4.1621e+02 - logprior: -1.0560e+00
Fitted a model with MAP estimate = -415.4765
expansions: []
discards: [ 35  57  90 102 103 104]
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 420.5228 - loglik: -4.1775e+02 - logprior: -2.7712e+00
Epoch 2/2
19/19 - 8s - loss: 417.0701 - loglik: -4.1616e+02 - logprior: -9.0692e-01
Fitted a model with MAP estimate = -415.6035
expansions: []
discards: []
Fitting a model of length 155 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 419.5291 - loglik: -4.1679e+02 - logprior: -2.7423e+00
Epoch 2/10
19/19 - 8s - loss: 416.0409 - loglik: -4.1516e+02 - logprior: -8.7764e-01
Epoch 3/10
19/19 - 8s - loss: 414.3106 - loglik: -4.1364e+02 - logprior: -6.7476e-01
Epoch 4/10
19/19 - 8s - loss: 415.1213 - loglik: -4.1446e+02 - logprior: -6.6454e-01
Fitted a model with MAP estimate = -412.9808
Time for alignment: 172.7518
Computed alignments with likelihoods: ['-409.0068', '-407.7958', '-412.9808']
Best model has likelihood: -407.7958
SP score = 0.8202
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5245460640>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5238494940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f52379ba7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50a891b280>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 793.3358 - loglik: -7.9138e+02 - logprior: -1.9520e+00
Epoch 2/10
39/39 - 20s - loss: 725.0583 - loglik: -7.2436e+02 - logprior: -6.9385e-01
Epoch 3/10
39/39 - 20s - loss: 716.4957 - loglik: -7.1562e+02 - logprior: -8.7590e-01
Epoch 4/10
39/39 - 20s - loss: 713.1099 - loglik: -7.1215e+02 - logprior: -9.5504e-01
Epoch 5/10
39/39 - 20s - loss: 711.4307 - loglik: -7.1038e+02 - logprior: -1.0548e+00
Epoch 6/10
39/39 - 20s - loss: 710.1370 - loglik: -7.0895e+02 - logprior: -1.1885e+00
Epoch 7/10
39/39 - 20s - loss: 709.3681 - loglik: -7.0807e+02 - logprior: -1.2977e+00
Epoch 8/10
39/39 - 20s - loss: 708.5280 - loglik: -7.0706e+02 - logprior: -1.4639e+00
Epoch 9/10
39/39 - 20s - loss: 707.8004 - loglik: -7.0613e+02 - logprior: -1.6707e+00
Epoch 10/10
39/39 - 20s - loss: 706.7615 - loglik: -7.0485e+02 - logprior: -1.9118e+00
Fitted a model with MAP estimate = -705.6915
expansions: [(0, 4), (23, 1), (43, 1), (49, 1), (50, 1), (61, 1), (85, 2), (89, 10), (103, 1), (114, 2), (117, 2), (118, 1), (120, 1), (152, 1), (153, 1), (157, 1), (177, 5), (207, 6), (208, 1), (216, 1)]
discards: []
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 717.4108 - loglik: -7.1483e+02 - logprior: -2.5839e+00
Epoch 2/2
39/39 - 25s - loss: 703.9886 - loglik: -7.0339e+02 - logprior: -5.9742e-01
Fitted a model with MAP estimate = -701.9294
expansions: [(0, 3), (209, 1)]
discards: [  1   2 142 243 244 245 246]
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 707.4742 - loglik: -7.0501e+02 - logprior: -2.4638e+00
Epoch 2/2
39/39 - 24s - loss: 703.6699 - loglik: -7.0315e+02 - logprior: -5.2195e-01
Fitted a model with MAP estimate = -701.3627
expansions: [(0, 3)]
discards: [1 2 3 4]
Fitting a model of length 269 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 706.8550 - loglik: -7.0415e+02 - logprior: -2.7063e+00
Epoch 2/10
39/39 - 24s - loss: 703.0206 - loglik: -7.0254e+02 - logprior: -4.7566e-01
Epoch 3/10
39/39 - 25s - loss: 701.0350 - loglik: -7.0082e+02 - logprior: -2.1969e-01
Epoch 4/10
39/39 - 25s - loss: 698.5909 - loglik: -6.9841e+02 - logprior: -1.8212e-01
Epoch 5/10
39/39 - 25s - loss: 697.4155 - loglik: -6.9721e+02 - logprior: -2.0233e-01
Epoch 6/10
39/39 - 25s - loss: 696.3321 - loglik: -6.9607e+02 - logprior: -2.5934e-01
Epoch 7/10
39/39 - 25s - loss: 695.6924 - loglik: -6.9535e+02 - logprior: -3.3864e-01
Epoch 8/10
39/39 - 25s - loss: 695.1031 - loglik: -6.9467e+02 - logprior: -4.3411e-01
Epoch 9/10
39/39 - 25s - loss: 694.4438 - loglik: -6.9389e+02 - logprior: -5.5844e-01
Epoch 10/10
39/39 - 24s - loss: 693.6701 - loglik: -6.9299e+02 - logprior: -6.8442e-01
Fitted a model with MAP estimate = -692.7998
Time for alignment: 687.9685
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 793.5629 - loglik: -7.9161e+02 - logprior: -1.9520e+00
Epoch 2/10
39/39 - 20s - loss: 723.6366 - loglik: -7.2285e+02 - logprior: -7.8946e-01
Epoch 3/10
39/39 - 20s - loss: 715.5574 - loglik: -7.1456e+02 - logprior: -9.9248e-01
Epoch 4/10
39/39 - 20s - loss: 712.4761 - loglik: -7.1139e+02 - logprior: -1.0822e+00
Epoch 5/10
39/39 - 20s - loss: 709.9571 - loglik: -7.0876e+02 - logprior: -1.2007e+00
Epoch 6/10
39/39 - 20s - loss: 709.0781 - loglik: -7.0771e+02 - logprior: -1.3717e+00
Epoch 7/10
39/39 - 20s - loss: 708.2456 - loglik: -7.0669e+02 - logprior: -1.5516e+00
Epoch 8/10
39/39 - 20s - loss: 707.3979 - loglik: -7.0566e+02 - logprior: -1.7408e+00
Epoch 9/10
39/39 - 20s - loss: 707.4683 - loglik: -7.0550e+02 - logprior: -1.9704e+00
Fitted a model with MAP estimate = -705.7806
expansions: [(0, 3), (9, 2), (24, 1), (47, 2), (49, 1), (50, 1), (62, 1), (85, 4), (87, 1), (89, 5), (90, 1), (91, 1), (98, 2), (102, 1), (115, 4), (116, 2), (117, 1), (121, 1), (125, 1), (152, 2), (158, 2), (177, 1), (178, 8), (181, 1), (207, 6), (208, 1), (209, 1), (214, 1)]
discards: []
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 714.3924 - loglik: -7.1184e+02 - logprior: -2.5523e+00
Epoch 2/2
39/39 - 27s - loss: 702.1789 - loglik: -7.0150e+02 - logprior: -6.7675e-01
Fitted a model with MAP estimate = -699.4398
expansions: [(0, 3), (97, 1)]
discards: [  1   2   3   4  12  54 106 107 122 144 145 195 219 220 257 258 259 260]
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 707.0202 - loglik: -7.0458e+02 - logprior: -2.4438e+00
Epoch 2/2
39/39 - 25s - loss: 701.9247 - loglik: -7.0147e+02 - logprior: -4.5392e-01
Fitted a model with MAP estimate = -699.4063
expansions: [(96, 1), (184, 3), (245, 4)]
discards: [  0 139 187 211]
Fitting a model of length 277 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 705.4281 - loglik: -7.0311e+02 - logprior: -2.3144e+00
Epoch 2/10
39/39 - 25s - loss: 700.4260 - loglik: -7.0011e+02 - logprior: -3.1808e-01
Epoch 3/10
39/39 - 25s - loss: 698.7034 - loglik: -6.9852e+02 - logprior: -1.8475e-01
Epoch 4/10
39/39 - 25s - loss: 696.5638 - loglik: -6.9637e+02 - logprior: -1.9283e-01
Epoch 5/10
39/39 - 26s - loss: 694.6882 - loglik: -6.9445e+02 - logprior: -2.4277e-01
Epoch 6/10
39/39 - 26s - loss: 693.8897 - loglik: -6.9360e+02 - logprior: -2.8776e-01
Epoch 7/10
39/39 - 26s - loss: 693.1053 - loglik: -6.9273e+02 - logprior: -3.7560e-01
Epoch 8/10
39/39 - 25s - loss: 692.4503 - loglik: -6.9196e+02 - logprior: -4.8646e-01
Epoch 9/10
39/39 - 26s - loss: 691.6307 - loglik: -6.9100e+02 - logprior: -6.2739e-01
Epoch 10/10
39/39 - 25s - loss: 690.6658 - loglik: -6.8989e+02 - logprior: -7.7640e-01
Fitted a model with MAP estimate = -689.9825
Time for alignment: 682.9542
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 792.2241 - loglik: -7.9027e+02 - logprior: -1.9501e+00
Epoch 2/10
39/39 - 20s - loss: 725.6127 - loglik: -7.2490e+02 - logprior: -7.1207e-01
Epoch 3/10
39/39 - 20s - loss: 718.0989 - loglik: -7.1717e+02 - logprior: -9.2722e-01
Epoch 4/10
39/39 - 20s - loss: 715.3905 - loglik: -7.1441e+02 - logprior: -9.7782e-01
Epoch 5/10
39/39 - 20s - loss: 713.7418 - loglik: -7.1266e+02 - logprior: -1.0857e+00
Epoch 6/10
39/39 - 20s - loss: 712.2382 - loglik: -7.1100e+02 - logprior: -1.2406e+00
Epoch 7/10
39/39 - 20s - loss: 711.8015 - loglik: -7.1035e+02 - logprior: -1.4514e+00
Epoch 8/10
39/39 - 20s - loss: 710.3584 - loglik: -7.0870e+02 - logprior: -1.6554e+00
Epoch 9/10
39/39 - 20s - loss: 709.3738 - loglik: -7.0746e+02 - logprior: -1.9108e+00
Epoch 10/10
39/39 - 20s - loss: 709.0278 - loglik: -7.0688e+02 - logprior: -2.1481e+00
Fitted a model with MAP estimate = -707.4137
expansions: [(0, 3), (9, 1), (23, 3), (42, 1), (51, 1), (52, 3), (61, 1), (84, 4), (85, 1), (89, 11), (101, 1), (114, 3), (117, 1), (121, 1), (153, 5), (157, 2), (176, 1), (179, 5), (182, 1), (184, 1), (207, 7), (214, 1)]
discards: []
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 717.7573 - loglik: -7.1510e+02 - logprior: -2.6577e+00
Epoch 2/2
39/39 - 27s - loss: 702.5997 - loglik: -7.0193e+02 - logprior: -6.7366e-01
Fitted a model with MAP estimate = -700.0130
expansions: [(0, 3), (50, 1), (256, 3)]
discards: [  1   2   3   4  28  62  99 100 195 196 197 209 223 224 258 259]
Fitting a model of length 278 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 707.5079 - loglik: -7.0508e+02 - logprior: -2.4278e+00
Epoch 2/2
39/39 - 25s - loss: 702.5336 - loglik: -7.0207e+02 - logprior: -4.6265e-01
Fitted a model with MAP estimate = -700.2960
expansions: [(0, 3), (202, 1)]
discards: [  0   1   2 107 248 249]
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 704.9848 - loglik: -7.0345e+02 - logprior: -1.5359e+00
Epoch 2/10
39/39 - 25s - loss: 701.9043 - loglik: -7.0164e+02 - logprior: -2.6015e-01
Epoch 3/10
39/39 - 25s - loss: 700.0894 - loglik: -6.9986e+02 - logprior: -2.2443e-01
Epoch 4/10
39/39 - 25s - loss: 698.3552 - loglik: -6.9813e+02 - logprior: -2.2616e-01
Epoch 5/10
39/39 - 25s - loss: 696.0801 - loglik: -6.9582e+02 - logprior: -2.5886e-01
Epoch 6/10
39/39 - 25s - loss: 695.3137 - loglik: -6.9501e+02 - logprior: -3.0005e-01
Epoch 7/10
39/39 - 25s - loss: 694.8349 - loglik: -6.9442e+02 - logprior: -4.1852e-01
Epoch 8/10
39/39 - 26s - loss: 693.8595 - loglik: -6.9331e+02 - logprior: -5.4605e-01
Epoch 9/10
39/39 - 26s - loss: 693.0194 - loglik: -6.9233e+02 - logprior: -6.9442e-01
Epoch 10/10
39/39 - 25s - loss: 692.1634 - loglik: -6.9130e+02 - logprior: -8.6118e-01
Fitted a model with MAP estimate = -691.2811
Time for alignment: 703.0643
Computed alignments with likelihoods: ['-692.7998', '-689.9825', '-691.2811']
Best model has likelihood: -689.9825
SP score = 0.5997
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f511fdc7dc0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f526f404ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f526f404640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f521d3166d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 624.0981 - loglik: -6.2066e+02 - logprior: -3.4416e+00
Epoch 2/10
19/19 - 9s - loss: 527.1553 - loglik: -5.2601e+02 - logprior: -1.1409e+00
Epoch 3/10
19/19 - 9s - loss: 484.3957 - loglik: -4.8277e+02 - logprior: -1.6209e+00
Epoch 4/10
19/19 - 9s - loss: 476.0337 - loglik: -4.7428e+02 - logprior: -1.7512e+00
Epoch 5/10
19/19 - 9s - loss: 472.9504 - loglik: -4.7108e+02 - logprior: -1.8706e+00
Epoch 6/10
19/19 - 9s - loss: 472.0540 - loglik: -4.7013e+02 - logprior: -1.9265e+00
Epoch 7/10
19/19 - 9s - loss: 470.4736 - loglik: -4.6848e+02 - logprior: -1.9923e+00
Epoch 8/10
19/19 - 9s - loss: 470.9578 - loglik: -4.6892e+02 - logprior: -2.0389e+00
Fitted a model with MAP estimate = -467.8518
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (50, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (87, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 2), (154, 1), (155, 4), (156, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 471.8911 - loglik: -4.6799e+02 - logprior: -3.9018e+00
Epoch 2/2
19/19 - 13s - loss: 456.1387 - loglik: -4.5431e+02 - logprior: -1.8330e+00
Fitted a model with MAP estimate = -451.5064
expansions: [(3, 1)]
discards: [  0  26  85 139 141 189 200]
Fitting a model of length 239 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 459.4748 - loglik: -4.5604e+02 - logprior: -3.4339e+00
Epoch 2/2
19/19 - 12s - loss: 454.0948 - loglik: -4.5300e+02 - logprior: -1.0913e+00
Fitted a model with MAP estimate = -449.7798
expansions: [(4, 1)]
discards: [  0 193 196]
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 455.9220 - loglik: -4.5250e+02 - logprior: -3.4247e+00
Epoch 2/10
19/19 - 12s - loss: 451.3665 - loglik: -4.4981e+02 - logprior: -1.5539e+00
Epoch 3/10
19/19 - 12s - loss: 450.2985 - loglik: -4.4898e+02 - logprior: -1.3204e+00
Epoch 4/10
19/19 - 12s - loss: 448.5924 - loglik: -4.4797e+02 - logprior: -6.1755e-01
Epoch 5/10
19/19 - 12s - loss: 446.7350 - loglik: -4.4623e+02 - logprior: -5.0665e-01
Epoch 6/10
19/19 - 12s - loss: 447.7107 - loglik: -4.4716e+02 - logprior: -5.5379e-01
Fitted a model with MAP estimate = -446.2326
Time for alignment: 286.8667
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 624.2803 - loglik: -6.2084e+02 - logprior: -3.4413e+00
Epoch 2/10
19/19 - 9s - loss: 525.9208 - loglik: -5.2478e+02 - logprior: -1.1405e+00
Epoch 3/10
19/19 - 9s - loss: 484.6217 - loglik: -4.8299e+02 - logprior: -1.6284e+00
Epoch 4/10
19/19 - 9s - loss: 476.7625 - loglik: -4.7506e+02 - logprior: -1.7019e+00
Epoch 5/10
19/19 - 9s - loss: 472.8001 - loglik: -4.7098e+02 - logprior: -1.8175e+00
Epoch 6/10
19/19 - 9s - loss: 472.1150 - loglik: -4.7022e+02 - logprior: -1.8986e+00
Epoch 7/10
19/19 - 9s - loss: 471.0816 - loglik: -4.6911e+02 - logprior: -1.9759e+00
Epoch 8/10
19/19 - 9s - loss: 470.9408 - loglik: -4.6890e+02 - logprior: -2.0385e+00
Epoch 9/10
19/19 - 9s - loss: 471.0344 - loglik: -4.6888e+02 - logprior: -2.1533e+00
Fitted a model with MAP estimate = -467.6997
expansions: [(8, 1), (11, 1), (18, 1), (25, 1), (28, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (71, 1), (72, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (129, 1), (130, 1), (149, 2), (150, 1), (151, 3), (153, 2), (154, 1), (155, 4), (156, 2), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Fitting a model of length 244 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 472.1917 - loglik: -4.6825e+02 - logprior: -3.9464e+00
Epoch 2/2
19/19 - 13s - loss: 455.9049 - loglik: -4.5402e+02 - logprior: -1.8826e+00
Fitted a model with MAP estimate = -451.6350
expansions: [(3, 1)]
discards: [  0  84 138 141 188 196 199 203]
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 459.5012 - loglik: -4.5611e+02 - logprior: -3.3896e+00
Epoch 2/2
19/19 - 12s - loss: 454.1924 - loglik: -4.5312e+02 - logprior: -1.0678e+00
Fitted a model with MAP estimate = -449.6890
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 456.3504 - loglik: -4.5294e+02 - logprior: -3.4096e+00
Epoch 2/10
19/19 - 12s - loss: 451.6894 - loglik: -4.5014e+02 - logprior: -1.5496e+00
Epoch 3/10
19/19 - 12s - loss: 449.8820 - loglik: -4.4854e+02 - logprior: -1.3387e+00
Epoch 4/10
19/19 - 12s - loss: 448.6393 - loglik: -4.4799e+02 - logprior: -6.5410e-01
Epoch 5/10
19/19 - 12s - loss: 447.0768 - loglik: -4.4657e+02 - logprior: -5.0246e-01
Epoch 6/10
19/19 - 12s - loss: 447.1037 - loglik: -4.4655e+02 - logprior: -5.4927e-01
Fitted a model with MAP estimate = -446.2380
Time for alignment: 294.1429
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 624.2350 - loglik: -6.2080e+02 - logprior: -3.4397e+00
Epoch 2/10
19/19 - 9s - loss: 525.6609 - loglik: -5.2450e+02 - logprior: -1.1612e+00
Epoch 3/10
19/19 - 9s - loss: 483.5448 - loglik: -4.8188e+02 - logprior: -1.6693e+00
Epoch 4/10
19/19 - 9s - loss: 475.1661 - loglik: -4.7340e+02 - logprior: -1.7699e+00
Epoch 5/10
19/19 - 9s - loss: 473.3836 - loglik: -4.7153e+02 - logprior: -1.8547e+00
Epoch 6/10
19/19 - 9s - loss: 471.5626 - loglik: -4.6966e+02 - logprior: -1.9017e+00
Epoch 7/10
19/19 - 9s - loss: 470.7426 - loglik: -4.6877e+02 - logprior: -1.9730e+00
Epoch 8/10
19/19 - 9s - loss: 471.0574 - loglik: -4.6900e+02 - logprior: -2.0561e+00
Fitted a model with MAP estimate = -467.7557
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (46, 1), (47, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 3), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Fitting a model of length 246 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 16s - loss: 472.0790 - loglik: -4.6815e+02 - logprior: -3.9263e+00
Epoch 2/2
19/19 - 13s - loss: 456.0305 - loglik: -4.5416e+02 - logprior: -1.8709e+00
Fitted a model with MAP estimate = -451.2008
expansions: [(3, 1)]
discards: [  0  26  84 139 142 189 190 191 201 203 205]
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 459.6308 - loglik: -4.5623e+02 - logprior: -3.3996e+00
Epoch 2/2
19/19 - 12s - loss: 454.1344 - loglik: -4.5309e+02 - logprior: -1.0456e+00
Fitted a model with MAP estimate = -449.9077
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 456.4476 - loglik: -4.5304e+02 - logprior: -3.4053e+00
Epoch 2/10
19/19 - 12s - loss: 451.8148 - loglik: -4.5028e+02 - logprior: -1.5363e+00
Epoch 3/10
19/19 - 12s - loss: 449.9831 - loglik: -4.4867e+02 - logprior: -1.3099e+00
Epoch 4/10
19/19 - 12s - loss: 448.8676 - loglik: -4.4824e+02 - logprior: -6.2393e-01
Epoch 5/10
19/19 - 12s - loss: 447.8694 - loglik: -4.4737e+02 - logprior: -5.0100e-01
Epoch 6/10
19/19 - 12s - loss: 446.3859 - loglik: -4.4583e+02 - logprior: -5.5392e-01
Epoch 7/10
19/19 - 12s - loss: 446.6463 - loglik: -4.4600e+02 - logprior: -6.4369e-01
Fitted a model with MAP estimate = -445.9961
Time for alignment: 297.4254
Computed alignments with likelihoods: ['-446.2326', '-446.2380', '-445.9961']
Best model has likelihood: -445.9961
SP score = 0.7306
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f504303b280>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f504303b190>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f504303b730>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456d00>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563d0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384563a0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f5038456100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456a30>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384560d0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456400>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c10>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456130>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456880>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456c40>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384569d0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456d30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f5038456ac0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50384567c0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456be0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f52567767c0>, <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f5295324970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f50f5287760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f529519feb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f5038436790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f5038456280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 321.1527 - loglik: -3.1766e+02 - logprior: -3.4897e+00
Epoch 2/10
19/19 - 3s - loss: 277.4802 - loglik: -2.7608e+02 - logprior: -1.3990e+00
Epoch 3/10
19/19 - 3s - loss: 252.0523 - loglik: -2.5034e+02 - logprior: -1.7077e+00
Epoch 4/10
19/19 - 3s - loss: 245.6871 - loglik: -2.4396e+02 - logprior: -1.7241e+00
Epoch 5/10
19/19 - 3s - loss: 243.6974 - loglik: -2.4199e+02 - logprior: -1.7109e+00
Epoch 6/10
19/19 - 3s - loss: 242.6656 - loglik: -2.4097e+02 - logprior: -1.6945e+00
Epoch 7/10
19/19 - 3s - loss: 242.4105 - loglik: -2.4070e+02 - logprior: -1.7092e+00
Epoch 8/10
19/19 - 3s - loss: 242.2183 - loglik: -2.4050e+02 - logprior: -1.7195e+00
Epoch 9/10
19/19 - 3s - loss: 242.0164 - loglik: -2.4028e+02 - logprior: -1.7336e+00
Epoch 10/10
19/19 - 3s - loss: 241.9707 - loglik: -2.4021e+02 - logprior: -1.7656e+00
Fitted a model with MAP estimate = -241.7464
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 1), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (47, 1), (49, 1), (55, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 248.7469 - loglik: -2.4464e+02 - logprior: -4.1065e+00
Epoch 2/2
19/19 - 3s - loss: 238.2201 - loglik: -2.3631e+02 - logprior: -1.9149e+00
Fitted a model with MAP estimate = -236.7312
expansions: [(0, 2)]
discards: [ 0 15 16 37 43 86]
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 239.0305 - loglik: -2.3610e+02 - logprior: -2.9286e+00
Epoch 2/2
19/19 - 3s - loss: 234.9652 - loglik: -2.3392e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -234.0848
expansions: [(22, 1)]
discards: [ 0 19]
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.9764 - loglik: -2.3732e+02 - logprior: -3.6544e+00
Epoch 2/10
19/19 - 3s - loss: 236.4562 - loglik: -2.3538e+02 - logprior: -1.0807e+00
Epoch 3/10
19/19 - 3s - loss: 234.2202 - loglik: -2.3345e+02 - logprior: -7.7382e-01
Epoch 4/10
19/19 - 3s - loss: 233.3417 - loglik: -2.3258e+02 - logprior: -7.5822e-01
Epoch 5/10
19/19 - 3s - loss: 232.6463 - loglik: -2.3188e+02 - logprior: -7.6404e-01
Epoch 6/10
19/19 - 3s - loss: 232.1490 - loglik: -2.3121e+02 - logprior: -9.4205e-01
Epoch 7/10
19/19 - 3s - loss: 230.9459 - loglik: -2.2993e+02 - logprior: -1.0191e+00
Epoch 8/10
19/19 - 3s - loss: 231.1835 - loglik: -2.3017e+02 - logprior: -1.0109e+00
Fitted a model with MAP estimate = -230.8273
Time for alignment: 106.3442
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 321.3504 - loglik: -3.1786e+02 - logprior: -3.4909e+00
Epoch 2/10
19/19 - 3s - loss: 276.5125 - loglik: -2.7511e+02 - logprior: -1.4074e+00
Epoch 3/10
19/19 - 3s - loss: 253.5924 - loglik: -2.5187e+02 - logprior: -1.7218e+00
Epoch 4/10
19/19 - 3s - loss: 247.7498 - loglik: -2.4605e+02 - logprior: -1.6999e+00
Epoch 5/10
19/19 - 3s - loss: 245.3725 - loglik: -2.4368e+02 - logprior: -1.6959e+00
Epoch 6/10
19/19 - 3s - loss: 244.4806 - loglik: -2.4279e+02 - logprior: -1.6909e+00
Epoch 7/10
19/19 - 3s - loss: 243.3532 - loglik: -2.4165e+02 - logprior: -1.7071e+00
Epoch 8/10
19/19 - 3s - loss: 243.3692 - loglik: -2.4166e+02 - logprior: -1.7085e+00
Fitted a model with MAP estimate = -243.2563
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 2), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (55, 1), (64, 2), (65, 2), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Fitting a model of length 125 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 249.9976 - loglik: -2.4583e+02 - logprior: -4.1646e+00
Epoch 2/2
19/19 - 3s - loss: 238.5600 - loglik: -2.3648e+02 - logprior: -2.0846e+00
Fitted a model with MAP estimate = -236.5527
expansions: [(0, 2)]
discards: [ 0 15 16 29 38 44 84]
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.3776 - loglik: -2.3545e+02 - logprior: -2.9315e+00
Epoch 2/2
19/19 - 3s - loss: 234.6262 - loglik: -2.3357e+02 - logprior: -1.0582e+00
Fitted a model with MAP estimate = -233.4522
expansions: []
discards: [ 0 83]
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.3141 - loglik: -2.3662e+02 - logprior: -3.6981e+00
Epoch 2/10
19/19 - 3s - loss: 235.5227 - loglik: -2.3427e+02 - logprior: -1.2502e+00
Epoch 3/10
19/19 - 3s - loss: 233.3433 - loglik: -2.3229e+02 - logprior: -1.0519e+00
Epoch 4/10
19/19 - 3s - loss: 232.6838 - loglik: -2.3165e+02 - logprior: -1.0325e+00
Epoch 5/10
19/19 - 3s - loss: 231.8224 - loglik: -2.3080e+02 - logprior: -1.0183e+00
Epoch 6/10
19/19 - 3s - loss: 231.4757 - loglik: -2.3044e+02 - logprior: -1.0343e+00
Epoch 7/10
19/19 - 3s - loss: 230.6118 - loglik: -2.2956e+02 - logprior: -1.0500e+00
Epoch 8/10
19/19 - 3s - loss: 230.9055 - loglik: -2.2986e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -230.5048
Time for alignment: 101.5155
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 321.6054 - loglik: -3.1812e+02 - logprior: -3.4902e+00
Epoch 2/10
19/19 - 3s - loss: 277.8114 - loglik: -2.7643e+02 - logprior: -1.3821e+00
Epoch 3/10
19/19 - 3s - loss: 253.5504 - loglik: -2.5180e+02 - logprior: -1.7458e+00
Epoch 4/10
19/19 - 3s - loss: 246.8304 - loglik: -2.4509e+02 - logprior: -1.7382e+00
Epoch 5/10
19/19 - 3s - loss: 244.8736 - loglik: -2.4315e+02 - logprior: -1.7230e+00
Epoch 6/10
19/19 - 3s - loss: 243.9347 - loglik: -2.4222e+02 - logprior: -1.7120e+00
Epoch 7/10
19/19 - 3s - loss: 243.1253 - loglik: -2.4141e+02 - logprior: -1.7155e+00
Epoch 8/10
19/19 - 3s - loss: 243.0412 - loglik: -2.4130e+02 - logprior: -1.7392e+00
Epoch 9/10
19/19 - 3s - loss: 243.0757 - loglik: -2.4132e+02 - logprior: -1.7575e+00
Fitted a model with MAP estimate = -242.6140
expansions: [(17, 1), (18, 3), (19, 4), (22, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (53, 1), (55, 1), (56, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 248.8662 - loglik: -2.4472e+02 - logprior: -4.1504e+00
Epoch 2/2
19/19 - 3s - loss: 237.6504 - loglik: -2.3560e+02 - logprior: -2.0550e+00
Fitted a model with MAP estimate = -235.8609
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 37 43 86]
Fitting a model of length 117 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.7009 - loglik: -2.3581e+02 - logprior: -2.8934e+00
Epoch 2/2
19/19 - 3s - loss: 234.8567 - loglik: -2.3382e+02 - logprior: -1.0329e+00
Fitted a model with MAP estimate = -234.0216
expansions: []
discards: [0]
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 240.6281 - loglik: -2.3694e+02 - logprior: -3.6858e+00
Epoch 2/10
19/19 - 3s - loss: 236.0776 - loglik: -2.3483e+02 - logprior: -1.2513e+00
Epoch 3/10
19/19 - 3s - loss: 234.0369 - loglik: -2.3297e+02 - logprior: -1.0676e+00
Epoch 4/10
19/19 - 3s - loss: 232.9489 - loglik: -2.3190e+02 - logprior: -1.0480e+00
Epoch 5/10
19/19 - 3s - loss: 232.4036 - loglik: -2.3137e+02 - logprior: -1.0375e+00
Epoch 6/10
19/19 - 3s - loss: 231.9981 - loglik: -2.3096e+02 - logprior: -1.0403e+00
Epoch 7/10
19/19 - 3s - loss: 231.7237 - loglik: -2.3066e+02 - logprior: -1.0600e+00
Epoch 8/10
19/19 - 3s - loss: 231.1201 - loglik: -2.3006e+02 - logprior: -1.0628e+00
Epoch 9/10
19/19 - 3s - loss: 231.2049 - loglik: -2.3014e+02 - logprior: -1.0685e+00
Fitted a model with MAP estimate = -231.0240
Time for alignment: 107.3792
Computed alignments with likelihoods: ['-230.8273', '-230.5048', '-231.0240']
Best model has likelihood: -230.5048
SP score = 0.8555
