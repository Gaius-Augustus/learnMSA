{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ab05b-4bf0-48d3-bf1c-7c27050852f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install t-coffee mmseqs2 -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f067659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from learnMSA.msa_hmm import Configuration, Align, Visualize\n",
    "from learnMSA.msa_hmm.SequenceDataset import SequenceDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c715bf4-a1bb-40b3-95bf-67a61b1916fa",
   "metadata": {},
   "source": [
    "## learnMSA demo\n",
    "\n",
    "In this notebook, we will fit a number of HMM models to a dataset of unaligned sequences. \n",
    "We will then use the fitted models to align the sequences and compare the results.\n",
    "Moreover, we will visualize the best (according to an objective criterion) model and alignment.\n",
    "\n",
    "*This notebook is meant to be a demo for running learnMSA in Python code. Check the readme if you want to run learnMSA from the command line.* \n",
    "\n",
    "Change the variables in the following cell to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59efc747-cd9a-47bc-a38c-ad654566b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fasta file with unaligned sequences.\n",
    "\n",
    "train_filename = \"test/data/egf.fasta\"\n",
    "\n",
    "# Reference file with aligned sequences that have matching IDs to (potentially a subset of) the \n",
    "# sequences in the train_file.\n",
    "# Replace with empty string if no reference is available.\n",
    "ref_filename = \"test/data/egf.ref\"\n",
    "\n",
    "# The number of independently trained models.\n",
    "num_models = 10\n",
    "\n",
    "# Use sequence weights based on a rapid pre-clustering of the sequences (requires mmseqs2 to be installed)\n",
    "use_weights = True\n",
    "\n",
    "# Align long insertions with an external aligner left unaligned by the main MSA stage (requires famsa to be installed).\n",
    "align_insertions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e9512-4a21-4288-b94f-863012b5ec52",
   "metadata": {},
   "source": [
    "## Run learnMSA from Python (Training + Viterbi alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a47ebb-cc04-4303-9194-6df7d9fe7bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of 10 models on file egf.fasta\n",
      "Configuration: \n",
      "{\n",
      "num_models : 10\n",
      "transitioner : ProfileHMMTransitioner(\n",
      " transition_init=\n",
      "    {\n",
      "    begin_to_match : DefaultEntry() , match_to_end : DefaultExit() , \n",
      "    match_to_match : DefaultMatchTransition(1) , match_to_insert : DefaultMatchTransition(-1) , \n",
      "    insert_to_match : Norm(0, 0.1) , insert_to_insert : Norm(-0.5, 0.1) , \n",
      "    match_to_delete : DefaultMatchTransition(-1) , delete_to_match : Norm(0, 0.1) , \n",
      "    delete_to_delete : Norm(-0.5, 0.1) , left_flank_loop : Norm(0, 0.1) , \n",
      "    left_flank_exit : Norm(-1, 0.1) , right_flank_loop : Norm(0, 0.1) , \n",
      "    right_flank_exit : Norm(-1, 0.1) , unannotated_segment_loop : Norm(0, 0.1) , \n",
      "    unannotated_segment_exit : Norm(-1, 0.1) , end_to_unannotated_segment : Norm(-9, 0.1) , \n",
      "    end_to_right_flank : Norm(0, 0.1) , end_to_terminal : Norm(0, 0.1)\n",
      "    },\n",
      " flank_init=Const(0.0),\n",
      " prior=ProfileHMMTransitionPrior(match_comp=1, insert_comp=1, delete_comp=1, alpha_flank=7000, alpha_single=1000000000.0, alpha_global=10000.0, alpha_flank_compl=1, alpha_single_compl=1, alpha_global_compl=1),\n",
      " frozen_kernels={})\n",
      "emitter : ProfileHMMEmitter(\n",
      " emission_init=DefaultEmission(),\n",
      " insertion_init=Const(shape=(23,)),\n",
      " prior=AminoAcidPrior(comp_count=1),\n",
      " frozen_insertions=True, )\n",
      "max_surgery_runs : 4\n",
      "length_init_quantile : 0.5\n",
      "surgery_quantile : 0.5\n",
      "min_surgery_seqs : 100000.0\n",
      "len_mul : 0.8\n",
      "batch_size : <function get_adaptive_batch_size at 0x7fd82c2ec8b0>\n",
      "learning_rate : 0.1\n",
      "epochs : [10, 2, 10]\n",
      "use_prior : True\n",
      "dirichlet_mix_comp_count : 1\n",
      "use_anc_probs : True\n",
      "trainable_rate_matrices : False\n",
      "surgery_del : 0.5\n",
      "surgery_ins : 0.5\n",
      "num_rate_matrices : 1\n",
      "per_matrix_rate : False\n",
      "matrix_rate_l2 : 0.0\n",
      "shared_rate_matrix : False\n",
      "equilibrium_sample : False\n",
      "transposed : False\n",
      "encoder_initializer : [Const(-3), Const(shape=(10, 20, 20)), Const(shape=(10, 20))]\n",
      "model_criterion : AIC\n",
      "encoder_weight_extractor : None\n",
      "experimental_evolve_upper_half : False\n",
      "embedding_l2_match : 16\n",
      "embedding_l2_insert : 0\n",
      "use_shared_embedding_insertions : True\n",
      "frozen_insertions : True\n",
      "lm_name : esm2\n",
      "reduced_embedding_dim : 32\n",
      "use_finetuned_lm : False\n",
      "cluster_seq_id : 0.9\n",
      "use_language_model : False\n",
      "allow_user_keys_in_config : False\n",
      "}\n",
      "Fitting models of lengths [24 24 25 25 23 25 24 24 25 25] on 7774 sequences.\n",
      "Batch size= 512 Learning rate= 0.1\n",
      "Using sequence weights  [1.         0.33333334 0.07692308 ... 0.04347826 0.02777778 0.25      ] .\n",
      "Using 0 GPUs.\n",
      "Epoch 1/10\n",
      "Batch 9: Invalid loss, terminating training\n",
      "17/17 - 16s - loss: nan - loglik: nan - logprior: nan - 16s/epoch - 956ms/step\n",
      "expansions model 0: []\n",
      "discards model 0: []\n",
      "expansions model 1: []\n",
      "discards model 1: []\n",
      "expansions model 2: []\n",
      "discards model 2: []\n",
      "expansions model 3: []\n",
      "discards model 3: []\n",
      "expansions model 4: []\n",
      "discards model 4: []\n",
      "expansions model 5: []\n",
      "discards model 5: []\n",
      "expansions model 6: []\n",
      "discards model 6: []\n",
      "expansions model 7: []\n",
      "discards model 7: []\n",
      "expansions model 8: []\n",
      "discards model 8: []\n",
      "expansions model 9: []\n",
      "discards model 9: []\n",
      "Re-initialized the encoder parameters.\n",
      "Fitting models of lengths [24, 24, 25, 25, 23, 25, 24, 24, 25, 25] on 7774 sequences.\n",
      "Batch size= 512 Learning rate= 0.1\n",
      "Using sequence weights  [1.         0.33333334 0.07692308 ... 0.04347826 0.02777778 0.25      ] .\n",
      "Using 0 GPUs.\n"
     ]
    }
   ],
   "source": [
    "out_filename = \"test/data/interactive.alignment.fasta\"\n",
    "config = Configuration.make_default(num_models)\n",
    "with SequenceDataset(train_filename, fmt=\"fasta\") as data:\n",
    "    alignment_model = Align.run_learnMSA(data,\n",
    "                                        out_filename,\n",
    "                                        config, \n",
    "                                        sequence_weights=Align.compute_sequence_weights(train_filename, \"tmp\") if use_weights else None,\n",
    "                                        verbose=True,\n",
    "                                        align_insertions=align_insertions)\n",
    "    Visualize.print_and_plot(alignment_model, alignment_model.best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceefb9fa-1d18-48d1-991d-12bce94a7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE: 1ixa\n",
      "HERE: 1apo\n",
      "HERE: 1urk\n",
      "HERE: 1fsb\n",
      "HERE: 1esl\n",
      "HERE: 1hre\n",
      "HERE: 1epi\n",
      "HERE: 4tgf\n",
      "HERE: 1hcgb\n",
      "HERE: 1dan1\n",
      "HERE: 1dan2\n",
      "HERE: 1rfnb\n"
     ]
    }
   ],
   "source": [
    "!id_list=$(sed -n '/^>/p' {ref_filename} | sed 's/^.//') ; export MAX_N_PID_4_TCOFFEE=10000000 ; t_coffee -other_pg seq_reformat -in test/data/interactive.alignment.fasta -action +extract_seq_list ${{id_list[@]}} +rm_gap > test/data/interactive.projection.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb89f14-dd1f-4b28-8869-59ebc41a1778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "seq1       seq2          Sim   [ALL]           Tot  \n",
      "egf           12         31.1    75.3 [100.0]   [ 5182]\n"
     ]
    }
   ],
   "source": [
    "!t_coffee -other_pg aln_compare -al1 {ref_filename} -al2 test/data/interactive.projection.fasta -compare_mode sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5840b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "488d3aa71b322ef168bf72fc4d82bebaa59a8882dc4050bd9af49d22feb8fb8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
