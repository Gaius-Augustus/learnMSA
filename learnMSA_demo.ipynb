{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ab05b-4bf0-48d3-bf1c-7c27050852f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install t-coffee mmseqs2 -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be9fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 08:55:37.835678: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-11 08:55:37.852025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 08:55:37.869479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 08:55:37.874800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 08:55:37.889604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 08:55:38.486535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728636939.103628   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.139156   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.139369   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.141072   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.141219   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.141337   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.203992   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.204176   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636939.204309   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 08:55:39.204427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728636941.581011   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636941.581254   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636941.581373   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636941.581534   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636941.581652   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 08:55:41.581755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n",
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728636942.182012   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636942.182258   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636942.182378   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636942.182539   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728636942.182658   32367 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 08:55:42.182761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['emission_kernel_0', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'init_logit_0'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 35)\n",
      "(94, 2)\n",
      "(94,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728636945.218882   32488 service.cc:146] XLA service 0x555edf3042a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728636945.218907   32488 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-10-11 08:55:45.436610: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-11 08:55:46.012608: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-10-11 08:55:46.321457: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:762] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.107). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/20\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 8s/step - loss: -295734400.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728636949.629254   32488 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 555ms/step - loss: -329612000.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 08:56:00.194540: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-10-11 08:56:00.195177: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f2a1bfea060>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from learnMSA.msa_hmm.MsaHmmLayer import MsaHmmLayer\n",
    "from learnMSA.msa_hmm.MsaHmmCell import MsaHmmCell\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class PermuteSeqs(tf.keras.layers.Layer):\n",
    "    def __init__(self, perm, **kwargs):\n",
    "        super(PermuteSeqs, self).__init__(**kwargs)\n",
    "        self.perm = perm\n",
    "\n",
    "    def call(self, sequences):\n",
    "        return tf.transpose(sequences, self.perm, name=\"loglik\")\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"perm\": self.perm}\n",
    "\n",
    "\n",
    "sequences = tf.keras.Input(shape=(None,None,20), name=\"sequences\", dtype=tf.float32)\n",
    "hmm_cell = MsaHmmCell([10])\n",
    "msa_hmm_layer = MsaHmmLayer(hmm_cell)\n",
    "s = PermuteSeqs([1,0,2,3])(sequences)\n",
    "map_loss, loglik = msa_hmm_layer(s)\n",
    "model = tf.keras.Model(inputs=(sequences), outputs=(map_loss, loglik))\n",
    "model.compile(optimizer=tf.optimizers.Adam(0.1), loss=lambda _,loss: loss) \n",
    "\n",
    "data = np.zeros((100, 1, 30, 20), dtype=np.float32)\n",
    "\n",
    "model.fit(data, np.ones((100), dtype=np.float32), epochs=1, steps_per_epoch=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ec311be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       [5, 2],\n",
       "       [3, 2],\n",
       "       [1, 1],\n",
       "       [5, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0,2], [0,1], [0,0], [5,2], [3,2], [1,1], [5,0]])\n",
    "x = np.argsort([i*6+j for i,j in a])\n",
    "# -> [1,0,3,2]\n",
    "a[x][np.argsort(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f8d2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728635078.961448    8385 service.cc:146] XLA service 0x55e2e5052900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728635078.961466    8385 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-10-11 08:24:38.962095: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSum (No registered 'SparseReduceSum' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSum}}){{node SparseReduceSum}}\n",
      "The op is created at: \n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "File \"/tmp/ipykernel_8385/790610365.py\", line 22, in <module>\n",
      "File \"/tmp/ipykernel_8385/790610365.py\", line 19, in get_batch_sum\n",
      "\ttf2xla conversion failed while converting __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Detected unsupported operations when trying to compile graph __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSum (No registered 'SparseReduceSum' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSum}}){{node SparseReduceSum}}\nThe op is created at: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_8385/790610365.py\", line 22, in <module>\nFile \"/tmp/ipykernel_8385/790610365.py\", line 19, in get_batch_sum\n\ttf2xla conversion failed while converting __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_416]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39mfunction(input_signature\u001b[39m=\u001b[39m[tf\u001b[39m.\u001b[39mSparseTensorSpec(\n\u001b[1;32m     15\u001b[0m     shape\u001b[39m=\u001b[39m(\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m),\n\u001b[1;32m     16\u001b[0m     dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mfloat32)], jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_sum\u001b[39m(inputs):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# same problem with tf.sparse.reduce_max\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mreduce_sum(inputs)\n\u001b[0;32m---> 22\u001b[0m sum_out \u001b[39m=\u001b[39m get_batch_sum(inputs)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(sum_out)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSum (No registered 'SparseReduceSum' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSum}}){{node SparseReduceSum}}\nThe op is created at: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_8385/790610365.py\", line 22, in <module>\nFile \"/tmp/ipykernel_8385/790610365.py\", line 19, in get_batch_sum\n\ttf2xla conversion failed while converting __inference_get_batch_sum_416[_XlaMustCompile=true,config_proto=16962375805000911571,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_416]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "batch_size = 4\n",
    "input_shape = (3, 3)\n",
    "\n",
    "indices = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [2, 0, 0], [2, 0, 1], [3, 0, 0], [3, 0, 1]])\n",
    "inputs = tf.sparse.SparseTensor(dense_shape=(batch_size, *input_shape),\n",
    "                                indices=indices,\n",
    "                                values=[9., 1, 9, 1, 9, 1, 9, 1])\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.SparseTensorSpec(\n",
    "    shape=(4, 3, 3),\n",
    "    dtype=tf.dtypes.float32)], jit_compile=True)\n",
    "def get_batch_sum(inputs):\n",
    "    # same problem with tf.sparse.reduce_max\n",
    "    return tf.sparse.reduce_sum(inputs)\n",
    "\n",
    "\n",
    "sum_out = get_batch_sum(inputs)\n",
    "print(sum_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d8cd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(indices=tf.Tensor(\n",
       "[[0 2]\n",
       " [3 4]], shape=(2, 2), dtype=int64), values=tf.Tensor([31.  2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([ 4 10], shape=(2,), dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f067659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 09:01:23.583040: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-11 09:01:23.595036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 09:01:23.606656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 09:01:23.610111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 09:01:23.620799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 09:01:24.209030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728637285.010099   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.052370   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.052662   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.054490   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.054639   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.054762   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.132891   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.133083   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637285.133216   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 09:01:25.133341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from learnMSA.msa_hmm import Configuration, Align, Visualize\n",
    "from learnMSA.msa_hmm.SequenceDataset import SequenceDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c715bf4-a1bb-40b3-95bf-67a61b1916fa",
   "metadata": {},
   "source": [
    "## learnMSA demo\n",
    "\n",
    "In this notebook, we will fit a number of HMM models to a dataset of unaligned sequences. \n",
    "We will then use the fitted models to align the sequences and compare the results.\n",
    "Moreover, we will visualize the best (according to an objective criterion) model and alignment.\n",
    "\n",
    "*This notebook is meant to be a demo for running learnMSA in Python code. Check the readme if you want to run learnMSA from the command line.* \n",
    "\n",
    "Change the variables in the following cell to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59efc747-cd9a-47bc-a38c-ad654566b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fasta file with unaligned sequences.\n",
    "\n",
    "train_filename = \"test/data/egf.fasta\"\n",
    "\n",
    "# Reference file with aligned sequences that have matching IDs to (potentially a subset of) the \n",
    "# sequences in the train_file.\n",
    "# Replace with empty string if no reference is available.\n",
    "ref_filename = \"test/data/egf.ref\"\n",
    "\n",
    "# The number of independently trained models.\n",
    "num_models = 4\n",
    "\n",
    "# Use sequence weights based on a rapid pre-clustering of the sequences (requires mmseqs2 to be installed)\n",
    "use_weights = True\n",
    "\n",
    "# Align long insertions with an external aligner left unaligned by the main MSA stage (requires famsa to be installed).\n",
    "align_insertions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e9512-4a21-4288-b94f-863012b5ec52",
   "metadata": {},
   "source": [
    "## Run learnMSA from Python (Training + Viterbi alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a47ebb-cc04-4303-9194-6df7d9fe7bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of 4 models on file egf.fasta\n",
      "Configuration: \n",
      "{\n",
      "num_models : 4\n",
      "transitioner : ProfileHMMTransitioner(\n",
      " transition_init=\n",
      "    {\n",
      "    begin_to_match : DefaultEntry() , match_to_end : DefaultExit() , \n",
      "    match_to_match : DefaultMatchTransition(1) , match_to_insert : DefaultMatchTransition(-1) , \n",
      "    insert_to_match : Norm(0, 0.1) , insert_to_insert : Norm(-0.5, 0.1) , \n",
      "    match_to_delete : DefaultMatchTransition(-1) , delete_to_match : Norm(0, 0.1) , \n",
      "    delete_to_delete : Norm(-0.5, 0.1) , left_flank_loop : Norm(0, 0.1) , \n",
      "    left_flank_exit : Norm(-1, 0.1) , right_flank_loop : Norm(0, 0.1) , \n",
      "    right_flank_exit : Norm(-1, 0.1) , unannotated_segment_loop : Norm(0, 0.1) , \n",
      "    unannotated_segment_exit : Norm(-1, 0.1) , end_to_unannotated_segment : Norm(-9, 0.1) , \n",
      "    end_to_right_flank : Norm(0, 0.1) , end_to_terminal : Norm(0, 0.1)\n",
      "    },\n",
      " flank_init=Const(0.0),\n",
      " prior=ProfileHMMTransitionPrior(match_comp=1, insert_comp=1, delete_comp=1, alpha_flank=7000, alpha_single=1000000000.0, alpha_global=10000.0, alpha_flank_compl=1, alpha_single_compl=1, alpha_global_compl=1),\n",
      " frozen_kernels={})\n",
      "emitter : ProfileHMMEmitter(\n",
      " emission_init=EmissionInitializer(),\n",
      " insertion_init=Const(shape=(23,)),\n",
      " prior=AminoAcidPrior(comp_count=1),\n",
      " frozen_insertions=True, )\n",
      "max_surgery_runs : 4\n",
      "length_init_quantile : 0.5\n",
      "surgery_quantile : 0.5\n",
      "min_surgery_seqs : 100000.0\n",
      "len_mul : 0.8\n",
      "batch_size : functools.partial(<function get_adaptive_batch_size at 0x7f1040328540>, small_gpu=True)\n",
      "learning_rate : 0.1\n",
      "epochs : [10, 2, 10]\n",
      "crop_long_seqs : inf\n",
      "use_prior : True\n",
      "dirichlet_mix_comp_count : 1\n",
      "use_anc_probs : True\n",
      "trainable_rate_matrices : False\n",
      "trainable_distances : True\n",
      "surgery_del : 0.5\n",
      "surgery_ins : 0.5\n",
      "num_rate_matrices : 1\n",
      "per_matrix_rate : False\n",
      "matrix_rate_l2 : 0.0\n",
      "shared_rate_matrix : False\n",
      "equilibrium_sample : False\n",
      "transposed : False\n",
      "encoder_initializer : [Const(-3), Const(shape=(4, 20, 20)), Const(shape=(4, 20))]\n",
      "model_criterion : AIC\n",
      "encoder_weight_extractor : None\n",
      "experimental_evolve_upper_half : False\n",
      "cluster_seq_id : 0.9\n",
      "use_language_model : False\n",
      "frozen_insertions : True\n",
      "allow_user_keys_in_config : False\n",
      "}\n",
      "Fitting models of lengths [24 23 24 26] on 7774 sequences.\n",
      "Batch size= 256 Learning rate= 0.1\n",
      "Using sequence weights  [1.         0.33333334 0.05882353 ... 0.05555556 0.03030303 0.25      ] .\n",
      "Using 1 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728637296.061508   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637296.061758   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637296.061881   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637296.062047   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637296.062169   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 09:01:36.062273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728637297.715024   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637297.715262   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637297.715381   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637297.715543   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728637297.715666   42837 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-11 09:01:37.715768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /device:GPU:0 with 22290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['emission_kernel_0', 'emission_kernel_1', 'emission_kernel_2', 'emission_kernel_3', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'init_logit_0', 'init_logit_1', 'init_logit_2', 'init_logit_3'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728637309.293932   42964 service.cc:146] XLA service 0x7f0e9c00cb30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728637309.293965   42964 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-10-11 09:01:50.454316: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-11 09:01:51.455228: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\n",
      "\n",
      "Stack trace for op definition: \n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "File \"/tmp/ipykernel_42837/1985174582.py\", line 4, in <module>\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 209, in run_learnMSA\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 94, in fit_and_align\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Training.py\", line 322, in fit_model\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/core/function/capture/capture_container.py\", line 154, in capture_by_value\n",
      "\n",
      "\t [[{{node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator}}]]\n",
      "\ttf2xla conversion failed while converting __inference_one_step_on_data_51339[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
      "2024-10-11 09:01:51.455639: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\n",
      "\n",
      "Stack trace for op definition: \n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "File \"/tmp/ipykernel_42837/1985174582.py\", line 4, in <module>\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 209, in run_learnMSA\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 94, in fit_and_align\n",
      "File \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Training.py\", line 322, in fit_model\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\n",
      "File \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/core/function/capture/capture_container.py\", line 154, in capture_by_value\n",
      "\n",
      "\t [[{{node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator}}]]\n",
      "\ttf2xla conversion failed while converting __inference_one_step_on_data_51339[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
      "\t [[StatefulPartitionedCall]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator defined at (most recent call last):\n<stack traces unavailable>\nXLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\n\nStack trace for op definition: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_42837/1985174582.py\", line 4, in <module>\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 209, in run_learnMSA\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 94, in fit_and_align\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Training.py\", line 322, in fit_model\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/core/function/capture/capture_container.py\", line 154, in capture_by_value\n\n\t [[{{node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_51339[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_51818]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m Configuration\u001b[39m.\u001b[39mmake_default(num_models)\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m SequenceDataset(train_filename, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfasta\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m data:\n\u001b[0;32m----> 4\u001b[0m     alignment_model \u001b[39m=\u001b[39m Align\u001b[39m.\u001b[39;49mrun_learnMSA(data,\n\u001b[1;32m      5\u001b[0m                                         out_filename,\n\u001b[1;32m      6\u001b[0m                                         config, \n\u001b[1;32m      7\u001b[0m                                         sequence_weights\u001b[39m=\u001b[39;49mAlign\u001b[39m.\u001b[39;49mcompute_sequence_weights(train_filename, \u001b[39m\"\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mif\u001b[39;49;00m use_weights \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m                                         verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m                                         align_insertions\u001b[39m=\u001b[39;49malign_insertions)\n\u001b[1;32m     10\u001b[0m     Visualize\u001b[39m.\u001b[39mprint_and_plot(alignment_model, alignment_model\u001b[39m.\u001b[39mbest_model)\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Align.py:209\u001b[0m, in \u001b[0;36mrun_learnMSA\u001b[0;34m(data, out_filename, config, model_generator, batch_generator, subset_ids, align_insertions, insertion_aligner, aligner_threads, sequence_weights, clusters, verbose, initial_model_length_callback, select_best_for_comparison, logo_gif_mode, logo_dir, output_format, load_model)\u001b[0m\n\u001b[1;32m    207\u001b[0m     am \u001b[39m=\u001b[39m fit_and_align_with_logo_gif(data, config, initial_model_length_callback, logo_dir)\n\u001b[1;32m    208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     am \u001b[39m=\u001b[39m fit_and_align(data, \n\u001b[1;32m    210\u001b[0m                         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    211\u001b[0m                         model_generator\u001b[39m=\u001b[39;49mmodel_generator,\n\u001b[1;32m    212\u001b[0m                         batch_generator\u001b[39m=\u001b[39;49mbatch_generator,\n\u001b[1;32m    213\u001b[0m                         subset\u001b[39m=\u001b[39;49msubset, \n\u001b[1;32m    214\u001b[0m                         initial_model_length_callback\u001b[39m=\u001b[39;49minitial_model_length_callback,\n\u001b[1;32m    215\u001b[0m                         sequence_weights\u001b[39m=\u001b[39;49msequence_weights,\n\u001b[1;32m    216\u001b[0m                         clusters\u001b[39m=\u001b[39;49mclusters,\n\u001b[1;32m    217\u001b[0m                         verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    219\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime for alignment:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mt_a))\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Align.py:94\u001b[0m, in \u001b[0;36mfit_and_align\u001b[0;34m(data, config, model_generator, batch_generator, subset, initial_model_length_callback, sequence_weights, clusters, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m     decode_indices \u001b[39m=\u001b[39m full_length_estimate\n\u001b[1;32m     93\u001b[0m epochs_this_iteration \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m i\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m last_iteration \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m]\n\u001b[0;32m---> 94\u001b[0m model, history \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mfit_model(model_generator,\n\u001b[1;32m     95\u001b[0m                                   batch_generator,\n\u001b[1;32m     96\u001b[0m                                   data,\n\u001b[1;32m     97\u001b[0m                                   train_indices,\n\u001b[1;32m     98\u001b[0m                                   model_lengths, \n\u001b[1;32m     99\u001b[0m                                   config,\n\u001b[1;32m    100\u001b[0m                                   batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m    101\u001b[0m                                   epochs\u001b[39m=\u001b[39;49mepochs_this_iteration,\n\u001b[1;32m    102\u001b[0m                                   sequence_weights\u001b[39m=\u001b[39;49msequence_weights,\n\u001b[1;32m    103\u001b[0m                                   clusters\u001b[39m=\u001b[39;49mclusters,\n\u001b[1;32m    104\u001b[0m                                   verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    105\u001b[0m am \u001b[39m=\u001b[39m AlignmentModel(data, batch_generator, decode_indices, batch_size\u001b[39m=\u001b[39mbatch_size, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m last_iteration:\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Training.py:322\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model_generator, batch_generator, data, indices, model_lengths, config, batch_size, epochs, sequence_weights, clusters, verbose, train_callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39massert\u001b[39;00m msa_hmm_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCan not find a MsaHmmLayer in the specified model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     \u001b[39m# class CustomCallback(tf.keras.callbacks.Callback):\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m     \u001b[39m#     # def on_train_begin(self, logs=None):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39m#         msa_hmm_layer.reverse_cell.emitter[0].step_counter.assign_add(1.)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# callbacks.append(CustomCallback())\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset, \n\u001b[1;32m    323\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    324\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    325\u001b[0m                       callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    326\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mint\u001b[39;49m(verbose))\n\u001b[1;32m    327\u001b[0m tf\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39msetLevel(\u001b[39m'\u001b[39m\u001b[39mINFO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[39mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator defined at (most recent call last):\n<stack traces unavailable>\nXLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\n\nStack trace for op definition: \nFile \"<frozen runpy>\", line 198, in _run_module_as_main\nFile \"<frozen runpy>\", line 88, in _run_code\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py\", line 88, in _run\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\nFile \"/tmp/ipykernel_42837/1985174582.py\", line 4, in <module>\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 209, in run_learnMSA\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Align.py\", line 94, in fit_and_align\nFile \"/home/beckerf/brain/learnMSA/learnMSA/msa_hmm/Training.py\", line 322, in fit_model\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 70, in train_step\nFile \"/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/core/function/capture/capture_container.py\", line 154, in capture_by_value\n\n\t [[{{node gradient_tape/functional_4_1/anc_probs_layer_1/matrix_exponential/while/functional_4_1/anc_probs_layer_1/matrix_exponential/while/Less_0/accumulator}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_51339[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_51818]"
     ]
    }
   ],
   "source": [
    "out_filename = \"test/data/interactive.alignment.fasta\"\n",
    "config = Configuration.make_default(num_models)\n",
    "with SequenceDataset(train_filename, fmt=\"fasta\") as data:\n",
    "    alignment_model = Align.run_learnMSA(data,\n",
    "                                        out_filename,\n",
    "                                        config, \n",
    "                                        sequence_weights=Align.compute_sequence_weights(train_filename, \"tmp\") if use_weights else None,\n",
    "                                        verbose=True,\n",
    "                                        align_insertions=align_insertions)\n",
    "    Visualize.print_and_plot(alignment_model, alignment_model.best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d0c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "488d3aa71b322ef168bf72fc4d82bebaa59a8882dc4050bd9af49d22feb8fb8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
