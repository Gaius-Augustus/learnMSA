{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed4d5d-7804-471e-8bc6-91bd763f5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install t-coffee mmseqs2 -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb62c54-8f80-4130-9721-2bf5290f6500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 14:27:17.786059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 14:27:17.799764: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 14:27:17.804067: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 14:27:17.814508: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 14:27:18.371788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728916039.032396   90060 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-14 14:27:19.058086: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from learnMSA.msa_hmm import Configuration, Align, Visualize, Emitter, Transitioner, Initializers, Training\n",
    "from learnMSA.msa_hmm.SequenceDataset import SequenceDataset\n",
    "from learnMSA.protein_language_models import Common, EmbeddingBatchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e4cf6",
   "metadata": {},
   "source": [
    "\n",
    "# Experimental demo: learnMSA + protein language model\n",
    "\n",
    "This notebook demonstrates how to align a set of protein sequences with learnMSA supported by a large, pre-trained protein language model.\n",
    "\n",
    "This configuration of learnMSA - although a prototype - is the most accurate variant of learnMSA currently available. It is also the most computationally expensive. It is recommended to run this notebook on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ddeab2-cb0e-4d42-8992-8cf5f78017d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fasta file with unaligned sequences.\n",
    "\n",
    "train_filename = \"test/data/egf.ref\"\n",
    "\n",
    "# Reference file with aligned sequences that have matching IDs to (potentially a subset of) the \n",
    "# sequences in the train_file.\n",
    "# Replace with empty string if no reference is available.\n",
    "ref_filename = \"test/data/egf.ref\"\n",
    "\n",
    "# The number of independently trained models.\n",
    "num_models = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44f1d1",
   "metadata": {},
   "source": [
    "HMM training supported by protein embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cebcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(filename, out_filename):\n",
    "    scoring_model_config = Common.ScoringModelConfig(dim=32, lm_name=\"protT5\", activation=\"softmax\", scaled=False)\n",
    "    config = Configuration.make_default(num_models, \n",
    "                                        use_language_model=True, \n",
    "                                        scoring_model_config=scoring_model_config,\n",
    "                                        frozen_insertions=True,\n",
    "                                        num_prior_components=10,\n",
    "                                        V2_emitter=True,\n",
    "                                        V2_temperature=3.)\n",
    "    #config[\"batch_size\"] = 16\n",
    "    # we have to define a special model- and batch generator if using a language model\n",
    "    # because the emission probabilities are computed differently and the LM requires specific inputs\n",
    "    model_gen = EmbeddingBatchGenerator.make_generic_embedding_model_generator(config[\"scoring_model_config\"].dim)\n",
    "    batch_gen = EmbeddingBatchGenerator.EmbeddingBatchGenerator(config[\"scoring_model_config\"])\n",
    "    with SequenceDataset(train_filename, fmt=\"fasta\") as data:\n",
    "        config[\"crop_long_seqs\"] = int(np.ceil(3 * np.mean(data.seq_lens)))\n",
    "        alignment_model = Align.run_learnMSA(data,\n",
    "                                            out_filename,\n",
    "                                            config, \n",
    "                                            model_generator=model_gen,\n",
    "                                            batch_generator=batch_gen,\n",
    "                                            sequence_weights=Align.compute_sequence_weights(train_filename, \"tmp\", config[\"cluster_seq_id\"]),\n",
    "                                            verbose=True,\n",
    "                                            align_insertions=True)\n",
    "    return alignment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2722a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of 4 models on file egf.ref\n",
      "Configuration: \n",
      "{\n",
      "num_models : 4\n",
      "transitioner : ProfileHMMTransitioner(\n",
      " transition_init=\n",
      "    {\n",
      "    begin_to_match : DefaultEntry() , match_to_end : DefaultExit() , \n",
      "    match_to_match : DefaultMatchTransition(1) , match_to_insert : DefaultMatchTransition(-1) , \n",
      "    insert_to_match : Norm(0, 0.1) , insert_to_insert : Norm(-0.5, 0.1) , \n",
      "    match_to_delete : DefaultMatchTransition(-1) , delete_to_match : Norm(0, 0.1) , \n",
      "    delete_to_delete : Norm(-0.5, 0.1) , left_flank_loop : Norm(0, 0.1) , \n",
      "    left_flank_exit : Norm(-1, 0.1) , right_flank_loop : Norm(0, 0.1) , \n",
      "    right_flank_exit : Norm(-1, 0.1) , unannotated_segment_loop : Norm(0, 0.1) , \n",
      "    unannotated_segment_exit : Norm(-1, 0.1) , end_to_unannotated_segment : Norm(-9, 0.1) , \n",
      "    end_to_right_flank : Norm(0, 0.1) , end_to_terminal : Norm(0, 0.1)\n",
      "    },\n",
      " flank_init=Const(0.0),\n",
      " prior=ProfileHMMTransitionPrior(match_comp=1, insert_comp=1, delete_comp=1, alpha_flank=7000, alpha_single=1000000000.0, alpha_global=10000.0, alpha_flank_compl=1, alpha_single_compl=1, alpha_global_compl=1),\n",
      " frozen_kernels={})\n",
      "emitter : MvnEmitter(scoring_model_config = ScoringModelConfig(lm_name=protT5, dim=32, activation=softmax, suffix=), ProfileHMMEmitter(\n",
      " emission_init=EmissionInitializer(),\n",
      " insertion_init=EmissionInitializer(),\n",
      " prior=JointEmissionPrior(AminoAcidPrior(comp_count=1), <MvnPrior name=mvn_prior, built=False>, NullPrior()),\n",
      " frozen_insertions=True, ))\n",
      "max_surgery_runs : 4\n",
      "length_init_quantile : 0.5\n",
      "surgery_quantile : 0.5\n",
      "min_surgery_seqs : 100000.0\n",
      "len_mul : 0.8\n",
      "batch_size : functools.partial(<function get_adaptive_batch_size_with_language_model at 0x7fc36c111080>, embedding_dim=32, small_gpu=False)\n",
      "learning_rate : 0.05\n",
      "epochs : [10, 4, 20]\n",
      "crop_long_seqs : 144\n",
      "use_prior : True\n",
      "dirichlet_mix_comp_count : 1\n",
      "use_anc_probs : True\n",
      "trainable_rate_matrices : False\n",
      "trainable_distances : True\n",
      "surgery_del : 0.5\n",
      "surgery_ins : 0.5\n",
      "num_rate_matrices : 1\n",
      "per_matrix_rate : False\n",
      "matrix_rate_l2 : 0.0\n",
      "shared_rate_matrix : False\n",
      "equilibrium_sample : False\n",
      "transposed : False\n",
      "encoder_initializer : [Const(-3), Const(shape=(4, 1, 20, 20)), Const(shape=(4, 1, 20))]\n",
      "model_criterion : AIC\n",
      "encoder_weight_extractor : None\n",
      "experimental_evolve_upper_half : False\n",
      "cluster_seq_id : 0.5\n",
      "use_language_model : True\n",
      "frozen_insertions : True\n",
      "allow_user_keys_in_config : False\n",
      "scoring_model_config : ScoringModelConfig(lm_name=protT5, dim=32, activation=softmax, suffix=)\n",
      "mvn_prior_components : 10\n",
      "use_l2 : False\n",
      "L2_match : 10.0\n",
      "L2_insert : 0.0\n",
      "temperature_mode : trainable\n",
      "conditionally_independent : True\n",
      "V2_emitter : True\n",
      "V2_full_covariance : False\n",
      "V2_temperature : 3.0\n",
      "}\n",
      "Warning: The sequences in test/data/egf.ref seem to be already aligned. learnMSA will ignore any gap character.\n",
      "Warning: You are aligning 12 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing all embeddings (this may take a while).\n",
      "10% done.\n",
      "Fitting models of lengths [37 38 36 38] on 12 sequences.\n",
      "Batch size= 6 Learning rate= 0.05\n",
      "Using sequence weights  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] .\n",
      "Using 0 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728916070.757828   90060 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-14 14:27:50.758093: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_6']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['sequences', 'indices', 'embeddings']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n",
      "I0000 00:00:1728916073.662962   90060 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-14 14:27:53.663220: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['emission_kernel_0', 'emission_kernel_1', 'emission_kernel_2', 'emission_kernel_3', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'transition_kernel_begin_to_match_0', 'transition_kernel_match_to_end_1', 'transition_kernel_match_to_match_2', 'transition_kernel_match_to_insert_3', 'transition_kernel_insert_to_match_4', 'transition_kernel_insert_to_insert_5', 'transition_kernel_match_to_delete_6', 'transition_kernel_delete_to_match_7', 'transition_kernel_delete_to_delete_8', 'transition_kernel_left_flank_loop_9', 'transition_kernel_left_flank_exit_10', 'transition_kernel_unannotated_segment_loop_11', 'transition_kernel_unannotated_segment_exit_12', 'transition_kernel_end_to_unannotated_segment_15', 'transition_kernel_end_to_right_flank_16', 'transition_kernel_end_to_terminal_17', 'init_logit_0', 'init_logit_1', 'init_logit_2', 'init_logit_3'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 - 20s - 359ms/step - loss: 1970.3904\n",
      "Epoch 2/10\n",
      "57/57 - 2s - 31ms/step - loss: 268.1707\n",
      "Epoch 3/10\n",
      "57/57 - 2s - 31ms/step - loss: -5.9380e+00\n",
      "Epoch 4/10\n",
      "57/57 - 2s - 30ms/step - loss: -1.4980e+02\n",
      "Epoch 5/10\n",
      "57/57 - 2s - 30ms/step - loss: -2.6649e+02\n",
      "Epoch 6/10\n",
      "57/57 - 2s - 31ms/step - loss: -3.7674e+02\n",
      "Epoch 7/10\n",
      "57/57 - 2s - 31ms/step - loss: -4.7540e+02\n",
      "Epoch 8/10\n",
      "57/57 - 2s - 31ms/step - loss: -5.3560e+02\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alignment_model \u001b[39m=\u001b[39m align(train_filename, \u001b[39m\"\u001b[39;49m\u001b[39mtest/data/interactive.alignment.fasta\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n\u001b[1;32m      2\u001b[0m Visualize\u001b[39m.\u001b[39mprint_and_plot(alignment_model, alignment_model\u001b[39m.\u001b[39mbest_model)\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36malign\u001b[0;34m(filename, out_filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m SequenceDataset(train_filename, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfasta\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m data:\n\u001b[1;32m     16\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mcrop_long_seqs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(data\u001b[39m.\u001b[39mseq_lens)))\n\u001b[0;32m---> 17\u001b[0m     alignment_model \u001b[39m=\u001b[39m Align\u001b[39m.\u001b[39;49mrun_learnMSA(data,\n\u001b[1;32m     18\u001b[0m                                         out_filename,\n\u001b[1;32m     19\u001b[0m                                         config, \n\u001b[1;32m     20\u001b[0m                                         model_generator\u001b[39m=\u001b[39;49mmodel_gen,\n\u001b[1;32m     21\u001b[0m                                         batch_generator\u001b[39m=\u001b[39;49mbatch_gen,\n\u001b[1;32m     22\u001b[0m                                         sequence_weights\u001b[39m=\u001b[39;49mAlign\u001b[39m.\u001b[39;49mcompute_sequence_weights(train_filename, \u001b[39m\"\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m\"\u001b[39;49m, config[\u001b[39m\"\u001b[39;49m\u001b[39mcluster_seq_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m     23\u001b[0m                                         verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     24\u001b[0m                                         align_insertions\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m alignment_model\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Align.py:209\u001b[0m, in \u001b[0;36mrun_learnMSA\u001b[0;34m(data, out_filename, config, model_generator, batch_generator, subset_ids, align_insertions, insertion_aligner, aligner_threads, sequence_weights, clusters, verbose, initial_model_length_callback, select_best_for_comparison, logo_gif_mode, logo_dir, output_format, load_model)\u001b[0m\n\u001b[1;32m    207\u001b[0m     am \u001b[39m=\u001b[39m fit_and_align_with_logo_gif(data, config, initial_model_length_callback, logo_dir)\n\u001b[1;32m    208\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     am \u001b[39m=\u001b[39m fit_and_align(data, \n\u001b[1;32m    210\u001b[0m                         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    211\u001b[0m                         model_generator\u001b[39m=\u001b[39;49mmodel_generator,\n\u001b[1;32m    212\u001b[0m                         batch_generator\u001b[39m=\u001b[39;49mbatch_generator,\n\u001b[1;32m    213\u001b[0m                         subset\u001b[39m=\u001b[39;49msubset, \n\u001b[1;32m    214\u001b[0m                         initial_model_length_callback\u001b[39m=\u001b[39;49minitial_model_length_callback,\n\u001b[1;32m    215\u001b[0m                         sequence_weights\u001b[39m=\u001b[39;49msequence_weights,\n\u001b[1;32m    216\u001b[0m                         clusters\u001b[39m=\u001b[39;49mclusters,\n\u001b[1;32m    217\u001b[0m                         verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m    219\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTime for alignment:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mt_a))\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Align.py:94\u001b[0m, in \u001b[0;36mfit_and_align\u001b[0;34m(data, config, model_generator, batch_generator, subset, initial_model_length_callback, sequence_weights, clusters, verbose)\u001b[0m\n\u001b[1;32m     92\u001b[0m     decode_indices \u001b[39m=\u001b[39m full_length_estimate\n\u001b[1;32m     93\u001b[0m epochs_this_iteration \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m i\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m last_iteration \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m]\n\u001b[0;32m---> 94\u001b[0m model, history \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mfit_model(model_generator,\n\u001b[1;32m     95\u001b[0m                                   batch_generator,\n\u001b[1;32m     96\u001b[0m                                   data,\n\u001b[1;32m     97\u001b[0m                                   train_indices,\n\u001b[1;32m     98\u001b[0m                                   model_lengths, \n\u001b[1;32m     99\u001b[0m                                   config,\n\u001b[1;32m    100\u001b[0m                                   batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m    101\u001b[0m                                   epochs\u001b[39m=\u001b[39;49mepochs_this_iteration,\n\u001b[1;32m    102\u001b[0m                                   sequence_weights\u001b[39m=\u001b[39;49msequence_weights,\n\u001b[1;32m    103\u001b[0m                                   clusters\u001b[39m=\u001b[39;49mclusters,\n\u001b[1;32m    104\u001b[0m                                   verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    105\u001b[0m am \u001b[39m=\u001b[39m AlignmentModel(data, batch_generator, decode_indices, batch_size\u001b[39m=\u001b[39mbatch_size, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m last_iteration:\n",
      "File \u001b[0;32m~/brain/learnMSA/learnMSA/msa_hmm/Training.py:330\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model_generator, batch_generator, data, indices, model_lengths, config, batch_size, epochs, sequence_weights, clusters, verbose, train_callbacks)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[39massert\u001b[39;00m msa_hmm_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCan not find a MsaHmmLayer in the specified model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# class CustomCallback(tf.keras.callbacks.Callback):\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[1;32m    322\u001b[0m     \u001b[39m#     # def on_train_begin(self, logs=None):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[39m#         msa_hmm_layer.reverse_cell.emitter[0].step_counter.assign_add(1.)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[39m# callbacks.append(CustomCallback())\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset, \n\u001b[1;32m    331\u001b[0m                     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    332\u001b[0m                     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    333\u001b[0m                       callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    334\u001b[0m                     verbose \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mint\u001b[39;49m(verbose))\n\u001b[1;32m    335\u001b[0m tf\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39msetLevel(\u001b[39m'\u001b[39m\u001b[39mINFO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    336\u001b[0m \u001b[39mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1553\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1554\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1555\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1556\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1557\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1558\u001b[0m   )\n\u001b[1;32m   1559\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alignment_model = align(train_filename, \"test/data/interactive.alignment.fasta\") \n",
    "Visualize.print_and_plot(alignment_model, alignment_model.best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788bc8a-d9a8-4b72-a67a-e68fa6a1ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE: 1tme\n",
      "HERE: 2mev\n",
      "HERE: 1bbt\n",
      "HERE: 1r1a\n",
      "HERE: 4rhv\n",
      "HERE: 2plv\n"
     ]
    }
   ],
   "source": [
    "!id_list=$(sed -n '/^>/p' {ref_filename} | sed 's/^.//') ; export MAX_N_PID_4_TCOFFEE=10000000 ; t_coffee -other_pg seq_reformat -in test/data/interactive.alignment.fasta -action +extract_seq_list ${{id_list[@]}} +rm_gap > test/data/interactive.projection.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd433dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "seq1       seq2          Sim   [ALL]           Tot  \n",
      "rhv           6          33.1    67.2 [100.0]   [20998]\n"
     ]
    }
   ],
   "source": [
    "!t_coffee -other_pg aln_compare -al1 {ref_filename} -al2 test/data/interactive.projection.fasta -compare_mode sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0a055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "488d3aa71b322ef168bf72fc4d82bebaa59a8882dc4050bd9af49d22feb8fb8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
